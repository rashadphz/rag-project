{"pages": [{"page": 1, "text": "                     DOES PROGRESS ON IMAGENET TRANSFER\n                     TO REAL-WORLD DATASETS?\n                       Alex Fang                                                              Simon Kornblith\u2217\n                       University of Washington                                               Google Research, Brain Team\n                       apf1@cs.washington.edu                                                 skornblith@google.com\n                       Ludwig Schmidt\u2217\n                       University of Washington, Allen Institute for AI\n                       schmidt@cs.washington.edu\narXiv:2301.04644v1  [cs.CV]  11 Jan 2023\n                                                                    ABSTRACT\n                               Does progress on ImageNet transfer to real-world datasets? We investigate this\n                               question by evaluating ImageNet pre-trained models with varying accuracy (57% -\n                               83%) on six practical image classification datasets. In particular, we study datasets\n                               collected with the goal of solving real-world tasks (e.g., classifying images from\n                               camera traps or satellites), as opposed to web-scraped benchmarks collected for\n                               comparing models. On multiple datasets, models with higher ImageNet accuracy\n                               do not consistently yield performance improvements. For certain tasks, interven-\n                               tions such as data augmentation improve performance even when architectures\n                               do not. We hope that future benchmarks will include more diverse datasets to\n                               encourage a more comprehensive approach to improving learning algorithms.\n                     1    INTRODUCTION\n                     ImageNet is one of the most widely used datasets in machine learning. Initially, the ImageNet com-\n                     petition played a key role in re-popularizing neural networks with the success of AlexNet in 2012.\n                     Ten years later, the ImageNet dataset is still one of the main benchmarks for state-of-the-art com-\n                     puter vision models (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016; Liu\n                     et al., 2018; Howard et al., 2019; Touvron et al., 2021; Radford et al., 2021). As a result of Ima-\n                     geNet\u2019s prominence, the machine learning community has invested tremendous effort into develop-\n                     ing model architectures, training algorithms, and other methodological innovations with the goal of\n                     increasing performance on ImageNet. Comparing methods on a common task has important benefits\n                     because it ensures controlled experimental conditions and results in rigorous evaluations. But the\n                     singular focus on ImageNet also raises the question whether the community is over-optimizing for\n                     this specific dataset.\n                     As a first approximation, ImageNet has clearly encouraged effective methodological innovation be-\n                     yond ImageNet itself. For instance, the key finding from the early years of ImageNet was that\n                     large convolution neural networks (CNNs) can succeed on contemporary computer vision datasets\n                     by leveraging GPUs for training. This paradigm has led to large improvements in other computer\n                     vision tasks, and CNNs are now omnipresent in the field. Nevertheless, this clear example of trans-\n                     fer to other tasks early in the ImageNet evolution does not necessarily justify the continued focus\n                     ImageNet still receives. For instance, it is possible that early methodological innovations transferred\n                     more broadly to other tasks, but later innovations have become less generalizable. The goal of our\n                     paper is to investigate this possibility specifically for neural network architecture and their transfer\n                     to real-world data not commonly found on the Internet.\n                     When discussing the transfer of techniques developed for ImageNet to other datasets, a key ques-\n                     tion is what other datasets to consider. Currently there is no comprehensive characterization of the\n                     many machine learning datasets and transfer between them. Hence we restrict our attention to a\n                     limited but well-motivated family of datasets. In particular, we consider classification tasks derived\n                     from image data that were specifically collected with the goal of classification in mind. This is in\n                        \u2217Equal contribution\n                                                                           1", "md": "# Does Progress on ImageNet Transfer to Real-World Datasets?\n\n# Does Progress on ImageNet Transfer to Real-World Datasets?\n\nAlex Fang - University of Washington - apf1@cs.washington.edu\n\nSimon Kornblith - Google Research, Brain Team - skornblith@google.com\n\nLudwig Schmidt - University of Washington, Allen Institute for AI - schmidt@cs.washington.edu\n\narXiv:2301.04644v1 [cs.CV] 11 Jan 2023\n\n## Abstract\n\nDoes progress on ImageNet transfer to real-world datasets? We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets. In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements. For certain tasks, interventions such as data augmentation improve performance even when architectures do not. We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.\n\n## 1 Introduction\n\nImageNet is one of the most widely used datasets in machine learning. Initially, the ImageNet competition played a key role in re-popularizing neural networks with the success of AlexNet in 2012. Ten years later, the ImageNet dataset is still one of the main benchmarks for state-of-the-art computer vision models (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016; Liu et al., 2018; Howard et al., 2019; Touvron et al., 2021; Radford et al., 2021). As a result of ImageNet\u2019s prominence, the machine learning community has invested tremendous effort into developing model architectures, training algorithms, and other methodological innovations with the goal of increasing performance on ImageNet. Comparing methods on a common task has important benefits because it ensures controlled experimental conditions and results in rigorous evaluations. But the singular focus on ImageNet also raises the question whether the community is over-optimizing for this specific dataset.\n\nAs a first approximation, ImageNet has clearly encouraged effective methodological innovation beyond ImageNet itself. For instance, the key finding from the early years of ImageNet was that large convolution neural networks (CNNs) can succeed on contemporary computer vision datasets by leveraging GPUs for training. This paradigm has led to large improvements in other computer vision tasks, and CNNs are now omnipresent in the field. Nevertheless, this clear example of transfer to other tasks early in the ImageNet evolution does not necessarily justify the continued focus ImageNet still receives. For instance, it is possible that early methodological innovations transferred more broadly to other tasks, but later innovations have become less generalizable. The goal of our paper is to investigate this possibility specifically for neural network architecture and their transfer to real-world data not commonly found on the Internet.\n\nWhen discussing the transfer of techniques developed for ImageNet to other datasets, a key question is what other datasets to consider. Currently there is no comprehensive characterization of the many machine learning datasets and transfer between them. Hence we restrict our attention to a limited but well-motivated family of datasets. In particular, we consider classification tasks derived from image data that were specifically collected with the goal of classification in mind. This is in\n\n\u2217Equal contribution", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Does Progress on ImageNet Transfer to Real-World Datasets?", "md": "# Does Progress on ImageNet Transfer to Real-World Datasets?"}, {"type": "heading", "lvl": 1, "value": "Does Progress on ImageNet Transfer to Real-World Datasets?", "md": "# Does Progress on ImageNet Transfer to Real-World Datasets?"}, {"type": "text", "value": "Alex Fang - University of Washington - apf1@cs.washington.edu\n\nSimon Kornblith - Google Research, Brain Team - skornblith@google.com\n\nLudwig Schmidt - University of Washington, Allen Institute for AI - schmidt@cs.washington.edu\n\narXiv:2301.04644v1 [cs.CV] 11 Jan 2023", "md": "Alex Fang - University of Washington - apf1@cs.washington.edu\n\nSimon Kornblith - Google Research, Brain Team - skornblith@google.com\n\nLudwig Schmidt - University of Washington, Allen Institute for AI - schmidt@cs.washington.edu\n\narXiv:2301.04644v1 [cs.CV] 11 Jan 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Does progress on ImageNet transfer to real-world datasets? We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets. In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements. For certain tasks, interventions such as data augmentation improve performance even when architectures do not. We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.", "md": "Does progress on ImageNet transfer to real-world datasets? We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets. In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements. For certain tasks, interventions such as data augmentation improve performance even when architectures do not. We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms."}, {"type": "heading", "lvl": 2, "value": "1 Introduction", "md": "## 1 Introduction"}, {"type": "text", "value": "ImageNet is one of the most widely used datasets in machine learning. Initially, the ImageNet competition played a key role in re-popularizing neural networks with the success of AlexNet in 2012. Ten years later, the ImageNet dataset is still one of the main benchmarks for state-of-the-art computer vision models (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016; Liu et al., 2018; Howard et al., 2019; Touvron et al., 2021; Radford et al., 2021). As a result of ImageNet\u2019s prominence, the machine learning community has invested tremendous effort into developing model architectures, training algorithms, and other methodological innovations with the goal of increasing performance on ImageNet. Comparing methods on a common task has important benefits because it ensures controlled experimental conditions and results in rigorous evaluations. But the singular focus on ImageNet also raises the question whether the community is over-optimizing for this specific dataset.\n\nAs a first approximation, ImageNet has clearly encouraged effective methodological innovation beyond ImageNet itself. For instance, the key finding from the early years of ImageNet was that large convolution neural networks (CNNs) can succeed on contemporary computer vision datasets by leveraging GPUs for training. This paradigm has led to large improvements in other computer vision tasks, and CNNs are now omnipresent in the field. Nevertheless, this clear example of transfer to other tasks early in the ImageNet evolution does not necessarily justify the continued focus ImageNet still receives. For instance, it is possible that early methodological innovations transferred more broadly to other tasks, but later innovations have become less generalizable. The goal of our paper is to investigate this possibility specifically for neural network architecture and their transfer to real-world data not commonly found on the Internet.\n\nWhen discussing the transfer of techniques developed for ImageNet to other datasets, a key question is what other datasets to consider. Currently there is no comprehensive characterization of the many machine learning datasets and transfer between them. Hence we restrict our attention to a limited but well-motivated family of datasets. In particular, we consider classification tasks derived from image data that were specifically collected with the goal of classification in mind. This is in\n\n\u2217Equal contribution", "md": "ImageNet is one of the most widely used datasets in machine learning. Initially, the ImageNet competition played a key role in re-popularizing neural networks with the success of AlexNet in 2012. Ten years later, the ImageNet dataset is still one of the main benchmarks for state-of-the-art computer vision models (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016; Liu et al., 2018; Howard et al., 2019; Touvron et al., 2021; Radford et al., 2021). As a result of ImageNet\u2019s prominence, the machine learning community has invested tremendous effort into developing model architectures, training algorithms, and other methodological innovations with the goal of increasing performance on ImageNet. Comparing methods on a common task has important benefits because it ensures controlled experimental conditions and results in rigorous evaluations. But the singular focus on ImageNet also raises the question whether the community is over-optimizing for this specific dataset.\n\nAs a first approximation, ImageNet has clearly encouraged effective methodological innovation beyond ImageNet itself. For instance, the key finding from the early years of ImageNet was that large convolution neural networks (CNNs) can succeed on contemporary computer vision datasets by leveraging GPUs for training. This paradigm has led to large improvements in other computer vision tasks, and CNNs are now omnipresent in the field. Nevertheless, this clear example of transfer to other tasks early in the ImageNet evolution does not necessarily justify the continued focus ImageNet still receives. For instance, it is possible that early methodological innovations transferred more broadly to other tasks, but later innovations have become less generalizable. The goal of our paper is to investigate this possibility specifically for neural network architecture and their transfer to real-world data not commonly found on the Internet.\n\nWhen discussing the transfer of techniques developed for ImageNet to other datasets, a key question is what other datasets to consider. Currently there is no comprehensive characterization of the many machine learning datasets and transfer between them. Hence we restrict our attention to a limited but well-motivated family of datasets. In particular, we consider classification tasks derived from image data that were specifically collected with the goal of classification in mind. This is in\n\n\u2217Equal contribution"}]}, {"page": 2, "text": "contrast to many standard computer vision datasets \u2013 including ImageNet \u2013 where the constituent\nimages were originally collected for a different purpose, posted to the web, and later re-purposed for\nbenchmarking computer vision methods. Concretely, we study six datasets ranging from leaf dis-\nease classification over melanoma detection to categorizing animals in camera trap images. Since\nthese datasets represent real-world applications, transfer of methods from ImageNet is particularly\nrelevant.\nWe find that on four out of our six real-world datasets, ImageNet-motivated architecture improve-\nments after VGG resulted in little to no progress (see Figure 1). Specifically, when we fit a line to\ndownstream model accuracies as a function of ImageNet accuracy, the resulting slope is less than\n0.05. The two exceptions where post-VGG architectures yield larger gains are the Caltech Camera\nTraps-20 (CCT-20) (Beery et al., 2018) dataset (slope 0.11) and the Human Protein Atlas Image\nClassification (Ouyang et al., 2019) dataset (slope 0.29). On multiple other datasets, we find that\ntask-specific improvements such as data augmentations or extra training data lead to larger gains\nthan using a more recent ImageNet architecture. We evaluate on a representative testbed of 19 Im-\nageNet models, ranging from the seminal AlexNet (Krizhevsky et al., 2012) over VGG (Simonyan\n& Zisserman, 2015) and ResNets (He et al., 2016) to the more recent and higher-performing Effi-\ncientNets (Tan & Le, 2019) and ConvNexts (Liu et al., 2022) (ImageNet top-1 accuracies 56.5% to\n83.4%). Our testbed includes three Vision Transformer models to cover non-CNN architectures.\nInterestingly, our findings stand in contrast to earlier work that investigated the aforementioned\nimage classification benchmarks such as CIFAR-10 (Krizhevsky & Hinton, 2009), PASCAL VOC\n2007 (Everingham et al., 2010), and Caltech-101 (Fei-Fei et al., 2004) that were scraped from the\nInternet. On these datasets, Kornblith et al. (2019) found consistent gains in downstream task ac-\ncuracy for a similar range of architectures as we study in our work. Taken together, these findings\nindicate that ImageNet accuracy may be a good predictor for other web-scraped datasets, but less\ninformative for real-world image classification datasets that are not sourced through the web. On the\nother hand, the CCT-20 data point shows that even very recent ImageNet models do help on some\ndownstream tasks that do not rely on images from the web. Overall, our results highlight the need\nfor a more comprehensive understanding of machine learning datasets to build and evaluate broadly\nuseful data representations.\n2    RELATED WORK\nTransferability of ImageNet architectures. Although there is extensive previous work investigat-\ning the effect of architecture upon the transferability of ImageNet-pretrained models to different\ndatasets, most of this work focuses on performance on datasets collected for the purpose of bench-\nmarking. Kornblith et al. (2019) previously showed that ImageNet accuracy of different models is\nstrongly correlated with downstream accuracy on a wide variety of web-scraped object-centric com-\nputer vision benchmark tasks. Later studies have investigated the relationship between ImageNet\nand transfer accuracy for self-supervised networks (Ericsson et al., 2021; Kotar et al., 2021; Nayman\net al., 2022), adversarially trained networks (Salman et al., 2020), or networks trained with different\nloss functions (Kornblith et al., 2021), but still evaluate primarily on web-scraped benchmark tasks.\nThe Visual Task Adaptation Benchmark (VTAB) (Zhai et al., 2019) comprises a more diverse set of\ntasks, including natural and non-natural classification tasks as well as non-classification tasks, but\nnearly all consist of web-scraped or synthetic images. In the medical imaging domain, models have\nbeen extensively evaluated on real-world data, with limited gains from newer models that perform\nbetter on ImageNet (Raghu et al., 2019; Bressem et al., 2020; Ke et al., 2021).\nMost closely related to our work, Tuggener et al. (2021) investigate performance of 500 CNN archi-\ntectures on yet another set of datasets, several of which are not web-scraped, and find that accuracy\ncorrelates poorly with ImageNet accuracy when training from scratch, but correlations are higher\nwhen fine-tuning ImageNet-pretrained models. Our work differs from theirs in our focus solely on\nreal-world datasets (e.g., from Kaggle competitions) and in that we perform extensive tuning in order\nto approach the best single-model performance obtainable on these datasets whereas Tuggener et al.\n(2021) instead devote their compute budget to increasing the breadth of architectures investigated.\nTransferability of networks trained on other datasets. Other work has evaluated transferability\nof representations of networks trained on datasets beyond ImageNet. Most notably, Abnar et al.\n(2022) explore the relationship between upstream and downstream accuracy for models pretrained\non JFT and ImageNet-21K and find that, on many tasks, downstream accuracy saturates with up-\n                                                    2", "md": "# Research Paper Summary\n\n## Contrast in Transferability of ImageNet Architectures\n\nIn contrast to many standard computer vision datasets \u2013 including ImageNet \u2013 where the constituent images were originally collected for a different purpose, posted to the web, and later re-purposed for benchmarking computer vision methods. Concretely, the study focuses on six datasets ranging from leaf disease classification over melanoma detection to categorizing animals in camera trap images. Since these datasets represent real-world applications, transfer of methods from ImageNet is particularly relevant.\n\nWe find that on four out of our six real-world datasets, ImageNet-motivated architecture improvements after VGG resulted in little to no progress. Specifically, when fitting a line to downstream model accuracies as a function of ImageNet accuracy, the resulting slope is less than 0.05. The exceptions where post-VGG architectures yield larger gains are the Caltech Camera Traps-20 (CCT-20) dataset (slope 0.11) and the Human Protein Atlas Image Classification dataset (slope 0.29). On multiple other datasets, task-specific improvements such as data augmentations or extra training data lead to larger gains than using a more recent ImageNet architecture.\n\nWe evaluate on a representative testbed of 19 ImageNet models, ranging from the seminal AlexNet over VGG and ResNets to the more recent and higher-performing EfficientNets and ConvNexts. Our testbed includes three Vision Transformer models to cover non-CNN architectures.\n\n### Related Work\n\nTransferability of ImageNet architectures has been extensively studied. Kornblith et al. (2019) showed a strong correlation between ImageNet accuracy and downstream accuracy on web-scraped object-centric computer vision benchmark tasks. Later studies have explored the relationship between ImageNet and transfer accuracy for various network types.\n\nMost closely related to this work, Tuggener et al. (2021) investigated the performance of CNN architectures on datasets, finding that accuracy correlates poorly with ImageNet accuracy when training from scratch, but correlations are higher when fine-tuning ImageNet-pretrained models.\n\nOther work has evaluated the transferability of networks trained on datasets beyond ImageNet. Abnar et al. (2022) explored the relationship between upstream and downstream accuracy for models pretrained on JFT and ImageNet-21K.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Paper Summary", "md": "# Research Paper Summary"}, {"type": "heading", "lvl": 2, "value": "Contrast in Transferability of ImageNet Architectures", "md": "## Contrast in Transferability of ImageNet Architectures"}, {"type": "text", "value": "In contrast to many standard computer vision datasets \u2013 including ImageNet \u2013 where the constituent images were originally collected for a different purpose, posted to the web, and later re-purposed for benchmarking computer vision methods. Concretely, the study focuses on six datasets ranging from leaf disease classification over melanoma detection to categorizing animals in camera trap images. Since these datasets represent real-world applications, transfer of methods from ImageNet is particularly relevant.\n\nWe find that on four out of our six real-world datasets, ImageNet-motivated architecture improvements after VGG resulted in little to no progress. Specifically, when fitting a line to downstream model accuracies as a function of ImageNet accuracy, the resulting slope is less than 0.05. The exceptions where post-VGG architectures yield larger gains are the Caltech Camera Traps-20 (CCT-20) dataset (slope 0.11) and the Human Protein Atlas Image Classification dataset (slope 0.29). On multiple other datasets, task-specific improvements such as data augmentations or extra training data lead to larger gains than using a more recent ImageNet architecture.\n\nWe evaluate on a representative testbed of 19 ImageNet models, ranging from the seminal AlexNet over VGG and ResNets to the more recent and higher-performing EfficientNets and ConvNexts. Our testbed includes three Vision Transformer models to cover non-CNN architectures.", "md": "In contrast to many standard computer vision datasets \u2013 including ImageNet \u2013 where the constituent images were originally collected for a different purpose, posted to the web, and later re-purposed for benchmarking computer vision methods. Concretely, the study focuses on six datasets ranging from leaf disease classification over melanoma detection to categorizing animals in camera trap images. Since these datasets represent real-world applications, transfer of methods from ImageNet is particularly relevant.\n\nWe find that on four out of our six real-world datasets, ImageNet-motivated architecture improvements after VGG resulted in little to no progress. Specifically, when fitting a line to downstream model accuracies as a function of ImageNet accuracy, the resulting slope is less than 0.05. The exceptions where post-VGG architectures yield larger gains are the Caltech Camera Traps-20 (CCT-20) dataset (slope 0.11) and the Human Protein Atlas Image Classification dataset (slope 0.29). On multiple other datasets, task-specific improvements such as data augmentations or extra training data lead to larger gains than using a more recent ImageNet architecture.\n\nWe evaluate on a representative testbed of 19 ImageNet models, ranging from the seminal AlexNet over VGG and ResNets to the more recent and higher-performing EfficientNets and ConvNexts. Our testbed includes three Vision Transformer models to cover non-CNN architectures."}, {"type": "heading", "lvl": 3, "value": "Related Work", "md": "### Related Work"}, {"type": "text", "value": "Transferability of ImageNet architectures has been extensively studied. Kornblith et al. (2019) showed a strong correlation between ImageNet accuracy and downstream accuracy on web-scraped object-centric computer vision benchmark tasks. Later studies have explored the relationship between ImageNet and transfer accuracy for various network types.\n\nMost closely related to this work, Tuggener et al. (2021) investigated the performance of CNN architectures on datasets, finding that accuracy correlates poorly with ImageNet accuracy when training from scratch, but correlations are higher when fine-tuning ImageNet-pretrained models.\n\nOther work has evaluated the transferability of networks trained on datasets beyond ImageNet. Abnar et al. (2022) explored the relationship between upstream and downstream accuracy for models pretrained on JFT and ImageNet-21K.", "md": "Transferability of ImageNet architectures has been extensively studied. Kornblith et al. (2019) showed a strong correlation between ImageNet accuracy and downstream accuracy on web-scraped object-centric computer vision benchmark tasks. Later studies have explored the relationship between ImageNet and transfer accuracy for various network types.\n\nMost closely related to this work, Tuggener et al. (2021) investigated the performance of CNN architectures on datasets, finding that accuracy correlates poorly with ImageNet accuracy when training from scratch, but correlations are higher when fine-tuning ImageNet-pretrained models.\n\nOther work has evaluated the transferability of networks trained on datasets beyond ImageNet. Abnar et al. (2022) explored the relationship between upstream and downstream accuracy for models pretrained on JFT and ImageNet-21K."}]}, {"page": 3, "text": "             Caltech Camera Traps 20                                  APTOS 2019 Blindness                                     Human Protein Atlas\n                                                         Quadratic weighted kappa                                 0.75\n      78                                                  0.930\n      76                                                  0.925                                                   0.70\n                                                                                                                 Macro F1 score\n      74                                                  0.920                                                   0.65\n     Accuracy\n      72                                                  0.915                                                   0.60\n      70                                                  0.910                                                   0.55\n      68                                                  0.905                                                   0.50\n      66                                                  0.900                                                   0.45\n      64                                                  0.895                                                   0.40\n               60     65      70      75     80                       60      65     70      75     80                       60      65     70      75    80\n               ImageNet top-1 accuracy                                ImageNet top-1 accuracy                                ImageNet top-1 accuracy\n    0.97         SIIM-ISIC Melanoma                          89        Cassava Leaf Disease                       99.4                  EuroSAT\n    0.96                                                     88                                                   99.2\n   Area under ROC\n    0.95                                                     87                                                   99.0\n                                                            Accuracy                                             Accuracy\n    0.94                                                     86                                                   98.8\n    0.93                                                     85                                                   98.6\n                                                                                                                  98.4\n    0.92                                                     84                                                   98.2\n    0.91                                                     83                                                   98.0\n               60     65      70      75     80                       60      65     70      75     80                       60      65     70      75    80\n               ImageNet top-1 accuracy                                ImageNet top-1 accuracy                                ImageNet top-1 accuracy\n                        AlexNet                             ResNet-152                             EfficientNet B0                      ConvNext-tiny\n                        MobileNetV3-small                   DeiT-small                             EfficientNet B4                      ShuffleNetV2x0.5\n                        VGG-13 BN                           PNASNet-5                              DenseNet-121                         SqueezeNet 1.1\n                        DeiT-tiny                           Inception-ResNet v2                    ResNeXt-50-32x4d                     ViT-B/16\n                        ResNet-50                           VGG-16 BN                              ShuffleNetV2x1.0\nFigure 1: Overview of transfer performance across models from ImageNet to each of the datasets we study.\nAlthough there seems to be a strong linear trends between ImageNet accuracy and the target metrics (green),\nthese trends become less certain when we restrict the models to those above 70% ImageNet accuracy (blue).\nVersions with error bars and spline interpolation can be found in Appendix B.\nstream accuracy. However, they evaluate representational quality using linear transfer rather than\nend-to-end fine-tuning. Other studies have investigated the impact of relationships between pre-\ntraining and fine-tuning tasks (Zamir et al., 2018; Mensink et al., 2021) or the impact of scaling the\nmodel and dataset (Goyal et al., 2019; Kolesnikov et al., 2020).\nAnother direction of related work relates to the effect of pretraining data on transfer learning. Huh\net al. (2016) look into the factors that make ImageNet good for transfer learning. They find that\nfine-grained classes are not needed for good transfer performance, and that reducing the dataset size\nand number of classes only results in slight drops in transfer learning performance. Though there is\na common goal of exploring what makes transfer learning work well, our work differs from this line\nof work by focusing on the fine-tuning aspect of transfer learning.\nOther studies of external validity of benchmarks. Our study fits into a broader literature inves-\ntigating the external validity of image classification benchmarks. Early work in this area identified\nlack of diversity as a key shortcoming of the benchmarks of the time (Ponce et al., 2006; Torralba\n& Efros, 2011), a problem that was largely resolved with the introduction of the much more di-\nverse ImageNet benchmark (Deng et al., 2009; Russakovsky et al., 2015). More recent studies have\ninvestigated the extent to which ImageNet classification accuracy correlates with accuracy on out-\nof-distribution (OOD) data (Recht et al., 2019; Taori et al., 2020) or accuracy as measured using\nhigher-quality human labels (Shankar et al., 2020; Tsipras et al., 2020; Beyer et al., 2020).\nAs in previous studies of OOD generalization, transfer learning involves generalization to test sets\nthat differ in distribution from the (pre-)training data. However, there are also key differences be-\ntween transfer learning and OOD generalization. First, in transfer learning, additional training data\nfrom the target task is used to adapt the model, while OOD evaluations usually apply trained models\nto a new distribution without any adaptation. Second, OOD evaluations usually focus on settings\nwith a shared class space so that evaluations without adaptation are possible. In contrast, transfer\nlearning evaluation generally involves downstream tasks with classes different from those in the pre-\ntraining dataset. These differences between transfer learning and OOD generalization are not only\nconceptual but also lead to different empirical phenomena. Miller et al. (2021) has shown that in-\n                                                                                 3", "md": "# Document\n\n## Caltech Camera Traps 20\n\n### APTOS 2019 Blindness\n\n#### Human Protein Atlas\n\n| |Quadratic weighted kappa|\n|---|---|\n|78|0.930|\n|76|0.925|0.70|\n|74|0.920|0.65|\n|Accuracy|0.915|0.60|\n|72|0.910|0.55|\n|70|0.905|0.50|\n|68|0.900|0.45|\n|66|0.895|0.40|\n\n| |ImageNet top-1 accuracy|\n|---|---|\n|0.97|SIIM-ISIC Melanoma|89|Cassava Leaf Disease|99.4|EuroSAT|\n|0.96| |88| |99.2| |\n|Area under ROC| |87| |99.0| |\n| |Accuracy|86| |98.8| |\n| | |85| |98.6| |\n| | |84| |98.2| |\n| | |83| |98.0| |\n\n### ImageNet top-1 accuracy\n\n- AlexNet\n- ResNet-152\n- EfficientNet B0\n- ConvNext-tiny\n- MobileNetV3-small\n- DeiT-small\n- EfficientNet B4\n- ShuffleNetV2x0.5\n- VGG-13 BN\n- PNASNet-5\n- DenseNet-121\n- SqueezeNet 1.1\n- DeiT-tiny\n- Inception-ResNet v2\n- ResNeXt-50-32x4d\n- ViT-B/16\n- ResNet-50\n- VGG-16 BN\n- ShuffleNetV2x1.0\n\nFigure 1: Overview of transfer performance across models from ImageNet to each of the datasets we study.\nAlthough there seems to be a strong linear trends between ImageNet accuracy and the target metrics (green),\nthese trends become less certain when we restrict the models to those above 70% ImageNet accuracy (blue).\nVersions with error bars and spline interpolation can be found in Appendix B.\n\nStream accuracy. However, they evaluate representational quality using linear transfer rather than\nend-to-end fine-tuning. Other studies have investigated the impact of relationships between pre-\ntraining and fine-tuning tasks (Zamir et al., 2018; Mensink et al., 2021) or the impact of scaling the\nmodel and dataset (Goyal et al., 2019; Kolesnikov et al., 2020).\n\nAnother direction of related work relates to the effect of pretraining data on transfer learning. Huh\net al. (2016) look into the factors that make ImageNet good for transfer learning. They find that\nfine-grained classes are not needed for good transfer performance, and that reducing the dataset size\nand number of classes only results in slight drops in transfer learning performance. Though there is\na common goal of exploring what makes transfer learning work well, our work differs from this line\nof work by focusing on the fine-tuning aspect of transfer learning.\n\nOther studies of external validity of benchmarks. Our study fits into a broader literature inves-\ntigating the external validity of image classification benchmarks. Early work in this area identified\nlack of diversity as a key shortcoming of the benchmarks of the time (Ponce et al., 2006; Torralba\n& Efros, 2011), a problem that was largely resolved with the introduction of the much more di-\nverse ImageNet benchmark (Deng et al., 2009; Russakovsky et al., 2015). More recent studies have\ninvestigated the extent to which ImageNet classification accuracy correlates with accuracy on out-\nof-distribution (OOD) data (Recht et al., 2019; Taori et al., 2020) or accuracy as measured using\nhigher-quality human labels (Shankar et al., 2020; Tsipras et al., 2020; Beyer et al., 2020).\n\nAs in previous studies of OOD generalization, transfer learning involves generalization to test sets\nthat differ in distribution from the (pre-)training data. However, there are also key differences be-\ntween transfer learning and OOD generalization. First, in transfer learning, additional training data\nfrom the target task is used to adapt the model, while OOD evaluations usually apply trained models\nto a new distribution without any adaptation. Second, OOD evaluations usually focus on settings\nwith a shared class space so that evaluations without adaptation are possible. In contrast, transfer\nlearning evaluation generally involves downstream tasks with classes different from those in the pre-\ntraining dataset. These differences between transfer learning and OOD generalization are not only\nconceptual but also lead to different empirical phenomena. Miller et al. (2021) has shown that in-", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Caltech Camera Traps 20", "md": "## Caltech Camera Traps 20"}, {"type": "heading", "lvl": 3, "value": "APTOS 2019 Blindness", "md": "### APTOS 2019 Blindness"}, {"type": "heading", "lvl": 4, "value": "Human Protein Atlas", "md": "#### Human Protein Atlas"}, {"type": "table", "rows": [["", "Quadratic weighted kappa"], ["78", "0.930"], ["76", "0.925", "0.70"], ["74", "0.920", "0.65"], ["Accuracy", "0.915", "0.60"], ["72", "0.910", "0.55"], ["70", "0.905", "0.50"], ["68", "0.900", "0.45"], ["66", "0.895", "0.40"]], "md": "| |Quadratic weighted kappa|\n|---|---|\n|78|0.930|\n|76|0.925|0.70|\n|74|0.920|0.65|\n|Accuracy|0.915|0.60|\n|72|0.910|0.55|\n|70|0.905|0.50|\n|68|0.900|0.45|\n|66|0.895|0.40|", "isPerfectTable": false, "csv": "\"\",\"Quadratic weighted kappa\"\n\"78\",\"0.930\"\n\"76\",\"0.925\",\"0.70\"\n\"74\",\"0.920\",\"0.65\"\n\"Accuracy\",\"0.915\",\"0.60\"\n\"72\",\"0.910\",\"0.55\"\n\"70\",\"0.905\",\"0.50\"\n\"68\",\"0.900\",\"0.45\"\n\"66\",\"0.895\",\"0.40\""}, {"type": "table", "rows": [["", "ImageNet top-1 accuracy"], ["0.97", "SIIM-ISIC Melanoma", "89", "Cassava Leaf Disease", "99.4", "EuroSAT"], ["0.96", "", "88", "", "99.2", ""], ["Area under ROC", "", "87", "", "99.0", ""], ["", "Accuracy", "86", "", "98.8", ""], ["", "", "85", "", "98.6", ""], ["", "", "84", "", "98.2", ""], ["", "", "83", "", "98.0", ""]], "md": "| |ImageNet top-1 accuracy|\n|---|---|\n|0.97|SIIM-ISIC Melanoma|89|Cassava Leaf Disease|99.4|EuroSAT|\n|0.96| |88| |99.2| |\n|Area under ROC| |87| |99.0| |\n| |Accuracy|86| |98.8| |\n| | |85| |98.6| |\n| | |84| |98.2| |\n| | |83| |98.0| |", "isPerfectTable": false, "csv": "\"\",\"ImageNet top-1 accuracy\"\n\"0.97\",\"SIIM-ISIC Melanoma\",\"89\",\"Cassava Leaf Disease\",\"99.4\",\"EuroSAT\"\n\"0.96\",\"\",\"88\",\"\",\"99.2\",\"\"\n\"Area under ROC\",\"\",\"87\",\"\",\"99.0\",\"\"\n\"\",\"Accuracy\",\"86\",\"\",\"98.8\",\"\"\n\"\",\"\",\"85\",\"\",\"98.6\",\"\"\n\"\",\"\",\"84\",\"\",\"98.2\",\"\"\n\"\",\"\",\"83\",\"\",\"98.0\",\"\""}, {"type": "heading", "lvl": 3, "value": "ImageNet top-1 accuracy", "md": "### ImageNet top-1 accuracy"}, {"type": "text", "value": "- AlexNet\n- ResNet-152\n- EfficientNet B0\n- ConvNext-tiny\n- MobileNetV3-small\n- DeiT-small\n- EfficientNet B4\n- ShuffleNetV2x0.5\n- VGG-13 BN\n- PNASNet-5\n- DenseNet-121\n- SqueezeNet 1.1\n- DeiT-tiny\n- Inception-ResNet v2\n- ResNeXt-50-32x4d\n- ViT-B/16\n- ResNet-50\n- VGG-16 BN\n- ShuffleNetV2x1.0\n\nFigure 1: Overview of transfer performance across models from ImageNet to each of the datasets we study.\nAlthough there seems to be a strong linear trends between ImageNet accuracy and the target metrics (green),\nthese trends become less certain when we restrict the models to those above 70% ImageNet accuracy (blue).\nVersions with error bars and spline interpolation can be found in Appendix B.\n\nStream accuracy. However, they evaluate representational quality using linear transfer rather than\nend-to-end fine-tuning. Other studies have investigated the impact of relationships between pre-\ntraining and fine-tuning tasks (Zamir et al., 2018; Mensink et al., 2021) or the impact of scaling the\nmodel and dataset (Goyal et al., 2019; Kolesnikov et al., 2020).\n\nAnother direction of related work relates to the effect of pretraining data on transfer learning. Huh\net al. (2016) look into the factors that make ImageNet good for transfer learning. They find that\nfine-grained classes are not needed for good transfer performance, and that reducing the dataset size\nand number of classes only results in slight drops in transfer learning performance. Though there is\na common goal of exploring what makes transfer learning work well, our work differs from this line\nof work by focusing on the fine-tuning aspect of transfer learning.\n\nOther studies of external validity of benchmarks. Our study fits into a broader literature inves-\ntigating the external validity of image classification benchmarks. Early work in this area identified\nlack of diversity as a key shortcoming of the benchmarks of the time (Ponce et al., 2006; Torralba\n& Efros, 2011), a problem that was largely resolved with the introduction of the much more di-\nverse ImageNet benchmark (Deng et al., 2009; Russakovsky et al., 2015). More recent studies have\ninvestigated the extent to which ImageNet classification accuracy correlates with accuracy on out-\nof-distribution (OOD) data (Recht et al., 2019; Taori et al., 2020) or accuracy as measured using\nhigher-quality human labels (Shankar et al., 2020; Tsipras et al., 2020; Beyer et al., 2020).\n\nAs in previous studies of OOD generalization, transfer learning involves generalization to test sets\nthat differ in distribution from the (pre-)training data. However, there are also key differences be-\ntween transfer learning and OOD generalization. First, in transfer learning, additional training data\nfrom the target task is used to adapt the model, while OOD evaluations usually apply trained models\nto a new distribution without any adaptation. Second, OOD evaluations usually focus on settings\nwith a shared class space so that evaluations without adaptation are possible. In contrast, transfer\nlearning evaluation generally involves downstream tasks with classes different from those in the pre-\ntraining dataset. These differences between transfer learning and OOD generalization are not only\nconceptual but also lead to different empirical phenomena. Miller et al. (2021) has shown that in-", "md": "- AlexNet\n- ResNet-152\n- EfficientNet B0\n- ConvNext-tiny\n- MobileNetV3-small\n- DeiT-small\n- EfficientNet B4\n- ShuffleNetV2x0.5\n- VGG-13 BN\n- PNASNet-5\n- DenseNet-121\n- SqueezeNet 1.1\n- DeiT-tiny\n- Inception-ResNet v2\n- ResNeXt-50-32x4d\n- ViT-B/16\n- ResNet-50\n- VGG-16 BN\n- ShuffleNetV2x1.0\n\nFigure 1: Overview of transfer performance across models from ImageNet to each of the datasets we study.\nAlthough there seems to be a strong linear trends between ImageNet accuracy and the target metrics (green),\nthese trends become less certain when we restrict the models to those above 70% ImageNet accuracy (blue).\nVersions with error bars and spline interpolation can be found in Appendix B.\n\nStream accuracy. However, they evaluate representational quality using linear transfer rather than\nend-to-end fine-tuning. Other studies have investigated the impact of relationships between pre-\ntraining and fine-tuning tasks (Zamir et al., 2018; Mensink et al., 2021) or the impact of scaling the\nmodel and dataset (Goyal et al., 2019; Kolesnikov et al., 2020).\n\nAnother direction of related work relates to the effect of pretraining data on transfer learning. Huh\net al. (2016) look into the factors that make ImageNet good for transfer learning. They find that\nfine-grained classes are not needed for good transfer performance, and that reducing the dataset size\nand number of classes only results in slight drops in transfer learning performance. Though there is\na common goal of exploring what makes transfer learning work well, our work differs from this line\nof work by focusing on the fine-tuning aspect of transfer learning.\n\nOther studies of external validity of benchmarks. Our study fits into a broader literature inves-\ntigating the external validity of image classification benchmarks. Early work in this area identified\nlack of diversity as a key shortcoming of the benchmarks of the time (Ponce et al., 2006; Torralba\n& Efros, 2011), a problem that was largely resolved with the introduction of the much more di-\nverse ImageNet benchmark (Deng et al., 2009; Russakovsky et al., 2015). More recent studies have\ninvestigated the extent to which ImageNet classification accuracy correlates with accuracy on out-\nof-distribution (OOD) data (Recht et al., 2019; Taori et al., 2020) or accuracy as measured using\nhigher-quality human labels (Shankar et al., 2020; Tsipras et al., 2020; Beyer et al., 2020).\n\nAs in previous studies of OOD generalization, transfer learning involves generalization to test sets\nthat differ in distribution from the (pre-)training data. However, there are also key differences be-\ntween transfer learning and OOD generalization. First, in transfer learning, additional training data\nfrom the target task is used to adapt the model, while OOD evaluations usually apply trained models\nto a new distribution without any adaptation. Second, OOD evaluations usually focus on settings\nwith a shared class space so that evaluations without adaptation are possible. In contrast, transfer\nlearning evaluation generally involves downstream tasks with classes different from those in the pre-\ntraining dataset. These differences between transfer learning and OOD generalization are not only\nconceptual but also lead to different empirical phenomena. Miller et al. (2021) has shown that in-"}]}, {"page": 4, "text": "distribution accuracy improvements often directly yield out-of-distribution accuracy improvements\nas well. This is the opposite of our main experimental finding that ImageNet improvements do not\ndirectly yield performance improvements on many real-world downstream tasks. Hence our work\ndemonstrates an important difference between OOD generalization and transfer learning.\n3    DATASETS\nAs mentioned in the introduction, a key choice in any transfer study is the set of target tasks on which\nto evaluate model performance. Before we introduce our suite of target tasks, we first describe three\ncriteria that guided our dataset selection: (i) diverse data sources, (ii) relevance to an application,\nand (iii) availability of well-tuned baseline models for comparison.\n3.1   SELECTION CRITERIA\nPrior work has already investigated transfer of ImageNet architectures to many downstream\ndatasets (Donahue et al., 2014; Sharif Razavian et al., 2014; Chatfield et al., 2014; Simonyan &\nZisserman, 2015). The 12 datasets used by Kornblith et al. (2019) often serve as a standard evalu-\nation suite (e.g., in (Salman et al., 2020; Ericsson et al., 2021; Radford et al., 2021)). While these\ndatasets are an informative starting point, they are all object-centric natural image datasets, and do\nnot represent the entire range of image classification problems. There are many applications of com-\nputer vision; the Kaggle website alone lists more than 1,500 datasets as of May 2022. To understand\ntransfer from ImageNet more broadly, we selected six datasets guided by the following criteria.\nDiverse data sources. Since collecting data is an expensive process, machine learning researchers\noften rely on web scraping to gather data when assembling a new benchmark. This practice has led to\nseveral image classification datasets with different label spaces such as food dishes, bird species, car\nmodels, or other everyday objects. However, the data sources underlying these seemingly different\ntasks are actually often similar. Specifically, we surveyed the 12 datasets from Kornblith et al.\n(2019) and found that all of these datasets were harvested from the web, often via keyword searches\nin Flickr, Google image search, or other search engines (see Appendix K). This narrow range of\ndata sources limits the external validity of existing transfer learning experiments. To get a broader\nunderstanding of transfer from ImageNet, we focus on scientific, commercial, and medical image\nclassification datasets that were not originally scraped from the web.\nApplication relevance. In addition to the data source, the classification task posed on a given set of\nimages also affects how relevant the resulting problem is for real-world applications. For instance,\nit would be possible to start with real-world satellite imagery that shows multiple building types\nper image, but only label one of the building types for the purpose of benchmarking (e.g., to avoid\nhigh annotation costs). The resulting task may then be of limited value for an actual application\ninvolving the satellite images that requires all buildings to be annotated. We aim to avoid such\npitfalls by limiting our attention to classification tasks that were assembled by domain experts with\na specific application in mind.\nAvailability of baselines. If methodological progress does not transfer from ImageNet to a given\ntarget task, we should expect that, as models perform better on ImageNet, accuracy on the target\ntask saturates. However, observing such a trend in an experiment is not sufficient to reach a conclu-\nsion regarding transfer because there is an alternative explanation for this empirical phenomenon.\nBesides a lack of transfer, the target task could also simply be easier than the source task so that\nmodels with sub-optimal source task accuracy already approach the Bayes error rate. As an illus-\ntrative example, consider MNIST as a target task for ImageNet transfer. A model with mediocre\nImageNet accuracy is already sufficient to get 99% accuracy on MNIST, but this finding does not\nmean that better ImageNet models are insufficient to improve MNIST accuracy \u2014 the models have\nalready hit the MNIST performance ceiling.\nMore interesting failures of transfer occur when ImageNet architectures plateau on the target task,\nbut it is still possible to improve accuracy beyond what the best ImageNet architecture can achieve\nwithout target task-specific modifications. In order to make such comparisons, well-tuned base-\nlines for the target task are essential. If improving ImageNet accuracy alone is insufficient to reach\nthese well-tuned baselines, we can indeed conclude that architecture transfer to this target task is\nlimited. In our experiments, we use multiple datasets from Kaggle competitions since the resulting\nleaderboards offer well-tuned baselines arising from a competitive process.\n                                                   4", "md": "distribution accuracy improvements often directly yield out-of-distribution accuracy improvements\nas well. This is the opposite of our main experimental finding that ImageNet improvements do not\ndirectly yield performance improvements on many real-world downstream tasks. Hence our work\ndemonstrates an important difference between OOD generalization and transfer learning.\n\n### DATASETS\n\nAs mentioned in the introduction, a key choice in any transfer study is the set of target tasks on which\nto evaluate model performance. Before we introduce our suite of target tasks, we first describe three\ncriteria that guided our dataset selection: (i) diverse data sources, (ii) relevance to an application,\nand (iii) availability of well-tuned baseline models for comparison.\n\n#### SELECTION CRITERIA\n\nPrior work has already investigated transfer of ImageNet architectures to many downstream\ndatasets (Donahue et al., 2014; Sharif Razavian et al., 2014; Chatfield et al., 2014; Simonyan &\nZisserman, 2015). The 12 datasets used by Kornblith et al. (2019) often serve as a standard evaluation suite (e.g., in (Salman et al., 2020; Ericsson et al., 2021; Radford et al., 2021)). While these\ndatasets are an informative starting point, they are all object-centric natural image datasets, and do\nnot represent the entire range of image classification problems. There are many applications of computer vision; the Kaggle website alone lists more than 1,500 datasets as of May 2022. To understand\ntransfer from ImageNet more broadly, we selected six datasets guided by the following criteria.\n\nDiverse data sources. Since collecting data is an expensive process, machine learning researchers\noften rely on web scraping to gather data when assembling a new benchmark. This practice has led to\nseveral image classification datasets with different label spaces such as food dishes, bird species, car\nmodels, or other everyday objects. However, the data sources underlying these seemingly different\ntasks are actually often similar. Specifically, we surveyed the 12 datasets from Kornblith et al.\n(2019) and found that all of these datasets were harvested from the web, often via keyword searches\nin Flickr, Google image search, or other search engines (see Appendix K). This narrow range of\ndata sources limits the external validity of existing transfer learning experiments. To get a broader\nunderstanding of transfer from ImageNet, we focus on scientific, commercial, and medical image\nclassification datasets that were not originally scraped from the web.\n\nApplication relevance. In addition to the data source, the classification task posed on a given set of\nimages also affects how relevant the resulting problem is for real-world applications. For instance,\nit would be possible to start with real-world satellite imagery that shows multiple building types\nper image, but only label one of the building types for the purpose of benchmarking (e.g., to avoid\nhigh annotation costs). The resulting task may then be of limited value for an actual application\ninvolving the satellite images that requires all buildings to be annotated. We aim to avoid such\npitfalls by limiting our attention to classification tasks that were assembled by domain experts with\na specific application in mind.\n\nAvailability of baselines. If methodological progress does not transfer from ImageNet to a given\ntarget task, we should expect that, as models perform better on ImageNet, accuracy on the target\ntask saturates. However, observing such a trend in an experiment is not sufficient to reach a conclusion\nregarding transfer because there is an alternative explanation for this empirical phenomenon.\nBesides a lack of transfer, the target task could also simply be easier than the source task so that\nmodels with sub-optimal source task accuracy already approach the Bayes error rate. As an illustrative\nexample, consider MNIST as a target task for ImageNet transfer. A model with mediocre\nImageNet accuracy is already sufficient to get 99% accuracy on MNIST, but this finding does not\nmean that better ImageNet models are insufficient to improve MNIST accuracy \u2014 the models have\nalready hit the MNIST performance ceiling.\n\nMore interesting failures of transfer occur when ImageNet architectures plateau on the target task,\nbut it is still possible to improve accuracy beyond what the best ImageNet architecture can achieve\nwithout target task-specific modifications. In order to make such comparisons, well-tuned baselines for the target task are essential. If improving ImageNet accuracy alone is insufficient to reach\nthese well-tuned baselines, we can indeed conclude that architecture transfer to this target task is\nlimited. In our experiments, we use multiple datasets from Kaggle competitions since the resulting\nleaderboards offer well-tuned baselines arising from a competitive process.", "images": [], "items": [{"type": "text", "value": "distribution accuracy improvements often directly yield out-of-distribution accuracy improvements\nas well. This is the opposite of our main experimental finding that ImageNet improvements do not\ndirectly yield performance improvements on many real-world downstream tasks. Hence our work\ndemonstrates an important difference between OOD generalization and transfer learning.", "md": "distribution accuracy improvements often directly yield out-of-distribution accuracy improvements\nas well. This is the opposite of our main experimental finding that ImageNet improvements do not\ndirectly yield performance improvements on many real-world downstream tasks. Hence our work\ndemonstrates an important difference between OOD generalization and transfer learning."}, {"type": "heading", "lvl": 3, "value": "DATASETS", "md": "### DATASETS"}, {"type": "text", "value": "As mentioned in the introduction, a key choice in any transfer study is the set of target tasks on which\nto evaluate model performance. Before we introduce our suite of target tasks, we first describe three\ncriteria that guided our dataset selection: (i) diverse data sources, (ii) relevance to an application,\nand (iii) availability of well-tuned baseline models for comparison.", "md": "As mentioned in the introduction, a key choice in any transfer study is the set of target tasks on which\nto evaluate model performance. Before we introduce our suite of target tasks, we first describe three\ncriteria that guided our dataset selection: (i) diverse data sources, (ii) relevance to an application,\nand (iii) availability of well-tuned baseline models for comparison."}, {"type": "heading", "lvl": 4, "value": "SELECTION CRITERIA", "md": "#### SELECTION CRITERIA"}, {"type": "text", "value": "Prior work has already investigated transfer of ImageNet architectures to many downstream\ndatasets (Donahue et al., 2014; Sharif Razavian et al., 2014; Chatfield et al., 2014; Simonyan &\nZisserman, 2015). The 12 datasets used by Kornblith et al. (2019) often serve as a standard evaluation suite (e.g., in (Salman et al., 2020; Ericsson et al., 2021; Radford et al., 2021)). While these\ndatasets are an informative starting point, they are all object-centric natural image datasets, and do\nnot represent the entire range of image classification problems. There are many applications of computer vision; the Kaggle website alone lists more than 1,500 datasets as of May 2022. To understand\ntransfer from ImageNet more broadly, we selected six datasets guided by the following criteria.\n\nDiverse data sources. Since collecting data is an expensive process, machine learning researchers\noften rely on web scraping to gather data when assembling a new benchmark. This practice has led to\nseveral image classification datasets with different label spaces such as food dishes, bird species, car\nmodels, or other everyday objects. However, the data sources underlying these seemingly different\ntasks are actually often similar. Specifically, we surveyed the 12 datasets from Kornblith et al.\n(2019) and found that all of these datasets were harvested from the web, often via keyword searches\nin Flickr, Google image search, or other search engines (see Appendix K). This narrow range of\ndata sources limits the external validity of existing transfer learning experiments. To get a broader\nunderstanding of transfer from ImageNet, we focus on scientific, commercial, and medical image\nclassification datasets that were not originally scraped from the web.\n\nApplication relevance. In addition to the data source, the classification task posed on a given set of\nimages also affects how relevant the resulting problem is for real-world applications. For instance,\nit would be possible to start with real-world satellite imagery that shows multiple building types\nper image, but only label one of the building types for the purpose of benchmarking (e.g., to avoid\nhigh annotation costs). The resulting task may then be of limited value for an actual application\ninvolving the satellite images that requires all buildings to be annotated. We aim to avoid such\npitfalls by limiting our attention to classification tasks that were assembled by domain experts with\na specific application in mind.\n\nAvailability of baselines. If methodological progress does not transfer from ImageNet to a given\ntarget task, we should expect that, as models perform better on ImageNet, accuracy on the target\ntask saturates. However, observing such a trend in an experiment is not sufficient to reach a conclusion\nregarding transfer because there is an alternative explanation for this empirical phenomenon.\nBesides a lack of transfer, the target task could also simply be easier than the source task so that\nmodels with sub-optimal source task accuracy already approach the Bayes error rate. As an illustrative\nexample, consider MNIST as a target task for ImageNet transfer. A model with mediocre\nImageNet accuracy is already sufficient to get 99% accuracy on MNIST, but this finding does not\nmean that better ImageNet models are insufficient to improve MNIST accuracy \u2014 the models have\nalready hit the MNIST performance ceiling.\n\nMore interesting failures of transfer occur when ImageNet architectures plateau on the target task,\nbut it is still possible to improve accuracy beyond what the best ImageNet architecture can achieve\nwithout target task-specific modifications. In order to make such comparisons, well-tuned baselines for the target task are essential. If improving ImageNet accuracy alone is insufficient to reach\nthese well-tuned baselines, we can indeed conclude that architecture transfer to this target task is\nlimited. In our experiments, we use multiple datasets from Kaggle competitions since the resulting\nleaderboards offer well-tuned baselines arising from a competitive process.", "md": "Prior work has already investigated transfer of ImageNet architectures to many downstream\ndatasets (Donahue et al., 2014; Sharif Razavian et al., 2014; Chatfield et al., 2014; Simonyan &\nZisserman, 2015). The 12 datasets used by Kornblith et al. (2019) often serve as a standard evaluation suite (e.g., in (Salman et al., 2020; Ericsson et al., 2021; Radford et al., 2021)). While these\ndatasets are an informative starting point, they are all object-centric natural image datasets, and do\nnot represent the entire range of image classification problems. There are many applications of computer vision; the Kaggle website alone lists more than 1,500 datasets as of May 2022. To understand\ntransfer from ImageNet more broadly, we selected six datasets guided by the following criteria.\n\nDiverse data sources. Since collecting data is an expensive process, machine learning researchers\noften rely on web scraping to gather data when assembling a new benchmark. This practice has led to\nseveral image classification datasets with different label spaces such as food dishes, bird species, car\nmodels, or other everyday objects. However, the data sources underlying these seemingly different\ntasks are actually often similar. Specifically, we surveyed the 12 datasets from Kornblith et al.\n(2019) and found that all of these datasets were harvested from the web, often via keyword searches\nin Flickr, Google image search, or other search engines (see Appendix K). This narrow range of\ndata sources limits the external validity of existing transfer learning experiments. To get a broader\nunderstanding of transfer from ImageNet, we focus on scientific, commercial, and medical image\nclassification datasets that were not originally scraped from the web.\n\nApplication relevance. In addition to the data source, the classification task posed on a given set of\nimages also affects how relevant the resulting problem is for real-world applications. For instance,\nit would be possible to start with real-world satellite imagery that shows multiple building types\nper image, but only label one of the building types for the purpose of benchmarking (e.g., to avoid\nhigh annotation costs). The resulting task may then be of limited value for an actual application\ninvolving the satellite images that requires all buildings to be annotated. We aim to avoid such\npitfalls by limiting our attention to classification tasks that were assembled by domain experts with\na specific application in mind.\n\nAvailability of baselines. If methodological progress does not transfer from ImageNet to a given\ntarget task, we should expect that, as models perform better on ImageNet, accuracy on the target\ntask saturates. However, observing such a trend in an experiment is not sufficient to reach a conclusion\nregarding transfer because there is an alternative explanation for this empirical phenomenon.\nBesides a lack of transfer, the target task could also simply be easier than the source task so that\nmodels with sub-optimal source task accuracy already approach the Bayes error rate. As an illustrative\nexample, consider MNIST as a target task for ImageNet transfer. A model with mediocre\nImageNet accuracy is already sufficient to get 99% accuracy on MNIST, but this finding does not\nmean that better ImageNet models are insufficient to improve MNIST accuracy \u2014 the models have\nalready hit the MNIST performance ceiling.\n\nMore interesting failures of transfer occur when ImageNet architectures plateau on the target task,\nbut it is still possible to improve accuracy beyond what the best ImageNet architecture can achieve\nwithout target task-specific modifications. In order to make such comparisons, well-tuned baselines for the target task are essential. If improving ImageNet accuracy alone is insufficient to reach\nthese well-tuned baselines, we can indeed conclude that architecture transfer to this target task is\nlimited. In our experiments, we use multiple datasets from Kaggle competitions since the resulting\nleaderboards offer well-tuned baselines arising from a competitive process."}]}, {"page": 5, "text": "3.2      DATASETS STUDIED\n               Table 1: We examine a variety of real-world datasets that cover different types of tasks.\n                      Dataset                      # of classes     Train size    Eval size     Eval metric           Kaggle\n                      Caltech Camera Traps              15           14,071        15,215       Accuracy\n                      APTOS 2019 Blindness               5            2,930          732        Quadratic                \u0013\n                                                                                                weighted kappa\n                      Human Protein Atlas               28           22,582         5,664       Macro F1 score           \u0013\n                      SIIM-ISIC Melanoma                 2           46,372        11,592       Area under ROC           \u0013\n                      Cassava Leaf Disease               5           17,118         4,279       Accuracy                 \u0013\n                      EuroSAT                           10           21,600         5,400       Accuracy\n   Caltech Camera            APTOS 2019           Human Protein Atlas     SIIM-ISIC Melanoma       Cassava Leaf Disease           EuroSAT\n       Traps-20           Blindness Detection     Image Classification        Classification           Classification\n                                       Figure 2: Sample images from each of the datasets.\nThe datasets studied in this work are practical and cover a variety of applications. We choose four\nof the most popular image classification competitions on Kaggle, as measured by number of com-\npetitors, teams, and submissions. Each of these competitions is funded by an organization with the\ngoal of advancing performance on that real-world task. Additionally, we supplement these datasets\nwith Caltech Camera Traps (Beery et al., 2018) and EuroSAT (Helber et al., 2019) to broaden the\ntypes of applications studied. Details for each dataset can be found in Table 1 1.\n4      MAIN EXPERIMENTS\nWe run our experiments across 19 model architectures, including both CNNs and Vision Transform-\ners (ViT and DeiT). They range from 57% to 83% ImageNet top-1 accuracy, allowing us to observe\nthe relationship between ImageNet performance and target dataset performance. In order to get the\nbest performance out of each architecture, we do extensive hyperparameter tuning over learning\nrate, weight decay, optimizer, and learning schedule. Details about our experiment setup can be\nfound in Appendix C. We now present our results for each of the datasets we investigated. Figure 1\nsummarizes our results across all datasets, with additional statistics in Table 2. Appendix A contains\ncomplete results for all datasets across the hyperparameter grids.\n4.1      CALTECH CAMERA TRAPS                                             Table 2: We summarize the blue regression lines from\n                                                                          Figure 1, calculated on models above 70% ImageNet\nBeery et al. (2018) created Caltech Camera                                accuracy, with their correlation and slope. Slope is cal-\nTraps-20 (CCT-20) using images taken from                                 culated so that all metrics have a range from 0 to 100.\ncamera traps deployed to monitor animal pop-\nulations. The images contain 15 different ani-                                 Dataset                             Correlation         Slope\nmal classes, as well as an empty class that we                                 Caltech Camera Traps                     0.17            0.11\nremove for our experiments 2. The dataset con-                                 APTOS 2019 Blindness                     0.06            0.01\ntains two sets of validation and test sets which                               Human Protein Atlas                      0.26            0.29\ndiffer by whether they come from locations that                                SIIM-ISIC Melanoma                       0.44            0.05\nare the same as or different from the training set                             Cassava Leaf Disease                     0.12            0.02\nlocations. While one of the goals of the dataset                               EuroSAT                                  0.05            0.00\nis to study generalization to new environments,\nhere we only study the sets from the same locations. Although CCT-20 is not a Kaggle competition,\nit is a subset of the iWildCam Challenge 2018, whose yearly editions have been hosted on Kaggle.\nWe see in Figure 1 (top-left) an overall positive trend between ImageNet performance and CCT-\n20 performance. The overall trend is unsurprising, given the number of animal classes present in\nImageNet. But despite the drastic reduction in the number of classes when compared to ImageNet,\n     1Dataset download links and PyTorch datasets and splits can be found at https://github.com/\nmlfoundations/imagenet-applications-transfer.\n     2Empty class is removed for the classification experiments in Table 1 of Beery et al. (2018)\n                                                                         5", "md": "# Datasets Studied\n\n## Datasets Studied\n\n|Dataset|# of classes|Train size|Eval size|Eval metric|Kaggle|\n|---|---|---|---|---|---|\n|Caltech Camera Traps|15|14,071|15,215|Accuracy| |\n|APTOS 2019 Blindness|5|2,930|732|Quadratic weighted kappa| |\n|Human Protein Atlas|28|22,582|5,664|Macro F1 score| |\n|SIIM-ISIC Melanoma|2|46,372|11,592|Area under ROC| |\n|Cassava Leaf Disease|5|17,118|4,279|Accuracy| |\n|EuroSAT|10|21,600|5,400|Accuracy| |\n\nCaltech Camera Traps, APTOS 2019 Blindness, Human Protein Atlas, SIIM-ISIC Melanoma, Cassava Leaf Disease, EuroSAT\n\nCaltech Camera Traps-20, APTOS 2019 Blindness Detection, Human Protein Atlas Image Classification, SIIM-ISIC Melanoma Classification, Cassava Leaf Disease Classification, EuroSAT Classification\n\n## Main Experiments\n\nWe run our experiments across 19 model architectures, including both CNNs and Vision Transformers (ViT and DeiT). They range from 57% to 83% ImageNet top-1 accuracy, allowing us to observe the relationship between ImageNet performance and target dataset performance. In order to get the best performance out of each architecture, we do extensive hyperparameter tuning over learning rate, weight decay, optimizer, and learning schedule. Details about our experiment setup can be found in Appendix C. We now present our results for each of the datasets we investigated. Figure 1 summarizes our results across all datasets, with additional statistics in Table 2. Appendix A contains complete results for all datasets across the hyperparameter grids.\n\n### Caltech Camera Traps\n\nBeery et al. (2018) created Caltech Camera Traps-20 (CCT-20) using images taken from camera traps deployed to monitor animal populations. The images contain 15 different animal classes, as well as an empty class that we remove for our experiments. The dataset contains two sets of validation and test sets which differ by whether they come from locations that are the same as or different from the training set locations. While one of the goals of the dataset is to study generalization to new environments, here we only study the sets from the same locations. Although CCT-20 is not a Kaggle competition, it is a subset of the iWildCam Challenge 2018, whose yearly editions have been hosted on Kaggle. We see in Figure 1 (top-left) an overall positive trend between ImageNet performance and CCT-20 performance. The overall trend is unsurprising, given the number of animal classes present in ImageNet. But despite the drastic reduction in the number of classes when compared to ImageNet.\n\n## Appendix\n\n1. Dataset download links and PyTorch datasets and splits can be found at https://github.com/mlfoundations/imagenet-applications-transfer.\n\n2. Empty class is removed for the classification experiments in Table 1 of Beery et al. (2018)", "images": [{"name": "page-5-2.jpg", "height": 58, "width": 58, "x": 242, "y": 207}, {"name": "page-5-1.jpg", "height": 38, "width": 58, "x": 176, "y": 226}, {"name": "page-5-3.jpg", "height": 58, "width": 58, "x": 307, "y": 207}, {"name": "page-5-0.jpg", "height": 42, "width": 58, "x": 110, "y": 223}, {"name": "page-5-5.jpg", "height": 58, "width": 58, "x": 439, "y": 207}, {"name": "page-5-4.jpg", "height": 43, "width": 58, "x": 373, "y": 222}], "items": [{"type": "heading", "lvl": 1, "value": "Datasets Studied", "md": "# Datasets Studied"}, {"type": "heading", "lvl": 2, "value": "Datasets Studied", "md": "## Datasets Studied"}, {"type": "table", "rows": [["Dataset", "# of classes", "Train size", "Eval size", "Eval metric", "Kaggle"], ["Caltech Camera Traps", "15", "14,071", "15,215", "Accuracy", ""], ["APTOS 2019 Blindness", "5", "2,930", "732", "Quadratic weighted kappa", ""], ["Human Protein Atlas", "28", "22,582", "5,664", "Macro F1 score", ""], ["SIIM-ISIC Melanoma", "2", "46,372", "11,592", "Area under ROC", ""], ["Cassava Leaf Disease", "5", "17,118", "4,279", "Accuracy", ""], ["EuroSAT", "10", "21,600", "5,400", "Accuracy", ""]], "md": "|Dataset|# of classes|Train size|Eval size|Eval metric|Kaggle|\n|---|---|---|---|---|---|\n|Caltech Camera Traps|15|14,071|15,215|Accuracy| |\n|APTOS 2019 Blindness|5|2,930|732|Quadratic weighted kappa| |\n|Human Protein Atlas|28|22,582|5,664|Macro F1 score| |\n|SIIM-ISIC Melanoma|2|46,372|11,592|Area under ROC| |\n|Cassava Leaf Disease|5|17,118|4,279|Accuracy| |\n|EuroSAT|10|21,600|5,400|Accuracy| |", "isPerfectTable": true, "csv": "\"Dataset\",\"# of classes\",\"Train size\",\"Eval size\",\"Eval metric\",\"Kaggle\"\n\"Caltech Camera Traps\",\"15\",\"14,071\",\"15,215\",\"Accuracy\",\"\"\n\"APTOS 2019 Blindness\",\"5\",\"2,930\",\"732\",\"Quadratic weighted kappa\",\"\"\n\"Human Protein Atlas\",\"28\",\"22,582\",\"5,664\",\"Macro F1 score\",\"\"\n\"SIIM-ISIC Melanoma\",\"2\",\"46,372\",\"11,592\",\"Area under ROC\",\"\"\n\"Cassava Leaf Disease\",\"5\",\"17,118\",\"4,279\",\"Accuracy\",\"\"\n\"EuroSAT\",\"10\",\"21,600\",\"5,400\",\"Accuracy\",\"\""}, {"type": "text", "value": "Caltech Camera Traps, APTOS 2019 Blindness, Human Protein Atlas, SIIM-ISIC Melanoma, Cassava Leaf Disease, EuroSAT\n\nCaltech Camera Traps-20, APTOS 2019 Blindness Detection, Human Protein Atlas Image Classification, SIIM-ISIC Melanoma Classification, Cassava Leaf Disease Classification, EuroSAT Classification", "md": "Caltech Camera Traps, APTOS 2019 Blindness, Human Protein Atlas, SIIM-ISIC Melanoma, Cassava Leaf Disease, EuroSAT\n\nCaltech Camera Traps-20, APTOS 2019 Blindness Detection, Human Protein Atlas Image Classification, SIIM-ISIC Melanoma Classification, Cassava Leaf Disease Classification, EuroSAT Classification"}, {"type": "heading", "lvl": 2, "value": "Main Experiments", "md": "## Main Experiments"}, {"type": "text", "value": "We run our experiments across 19 model architectures, including both CNNs and Vision Transformers (ViT and DeiT). They range from 57% to 83% ImageNet top-1 accuracy, allowing us to observe the relationship between ImageNet performance and target dataset performance. In order to get the best performance out of each architecture, we do extensive hyperparameter tuning over learning rate, weight decay, optimizer, and learning schedule. Details about our experiment setup can be found in Appendix C. We now present our results for each of the datasets we investigated. Figure 1 summarizes our results across all datasets, with additional statistics in Table 2. Appendix A contains complete results for all datasets across the hyperparameter grids.", "md": "We run our experiments across 19 model architectures, including both CNNs and Vision Transformers (ViT and DeiT). They range from 57% to 83% ImageNet top-1 accuracy, allowing us to observe the relationship between ImageNet performance and target dataset performance. In order to get the best performance out of each architecture, we do extensive hyperparameter tuning over learning rate, weight decay, optimizer, and learning schedule. Details about our experiment setup can be found in Appendix C. We now present our results for each of the datasets we investigated. Figure 1 summarizes our results across all datasets, with additional statistics in Table 2. Appendix A contains complete results for all datasets across the hyperparameter grids."}, {"type": "heading", "lvl": 3, "value": "Caltech Camera Traps", "md": "### Caltech Camera Traps"}, {"type": "text", "value": "Beery et al. (2018) created Caltech Camera Traps-20 (CCT-20) using images taken from camera traps deployed to monitor animal populations. The images contain 15 different animal classes, as well as an empty class that we remove for our experiments. The dataset contains two sets of validation and test sets which differ by whether they come from locations that are the same as or different from the training set locations. While one of the goals of the dataset is to study generalization to new environments, here we only study the sets from the same locations. Although CCT-20 is not a Kaggle competition, it is a subset of the iWildCam Challenge 2018, whose yearly editions have been hosted on Kaggle. We see in Figure 1 (top-left) an overall positive trend between ImageNet performance and CCT-20 performance. The overall trend is unsurprising, given the number of animal classes present in ImageNet. But despite the drastic reduction in the number of classes when compared to ImageNet.", "md": "Beery et al. (2018) created Caltech Camera Traps-20 (CCT-20) using images taken from camera traps deployed to monitor animal populations. The images contain 15 different animal classes, as well as an empty class that we remove for our experiments. The dataset contains two sets of validation and test sets which differ by whether they come from locations that are the same as or different from the training set locations. While one of the goals of the dataset is to study generalization to new environments, here we only study the sets from the same locations. Although CCT-20 is not a Kaggle competition, it is a subset of the iWildCam Challenge 2018, whose yearly editions have been hosted on Kaggle. We see in Figure 1 (top-left) an overall positive trend between ImageNet performance and CCT-20 performance. The overall trend is unsurprising, given the number of animal classes present in ImageNet. But despite the drastic reduction in the number of classes when compared to ImageNet."}, {"type": "heading", "lvl": 2, "value": "Appendix", "md": "## Appendix"}, {"type": "text", "value": "1. Dataset download links and PyTorch datasets and splits can be found at https://github.com/mlfoundations/imagenet-applications-transfer.\n\n2. Empty class is removed for the classification experiments in Table 1 of Beery et al. (2018)", "md": "1. Dataset download links and PyTorch datasets and splits can be found at https://github.com/mlfoundations/imagenet-applications-transfer.\n\n2. Empty class is removed for the classification experiments in Table 1 of Beery et al. (2018)"}]}, {"page": 6, "text": "CCT-20 has its own set of challenges. Animals are often pictured at difficult angles, and sometimes\nare not even visible in the image because a sequence of frames triggered by activity all have the\nsame label. Despite these challenges, an even higher performing model still does better on this task\n- we train a CLIP ViT L/14-336px model (85.4% ImageNet top-1) with additional augmentation to\nachieve 83.4% accuracy on CCT-20.\n4.2    APTOS 2019 BLINDNESS DETECTION\nThis dataset was created for a Kaggle competition run by the Asia Pacific Tele-Ophthalmology\nSociety (APTOS) with the goal of advancing medical screening for diabetic retinopathy in rural\nareas (Asia Pacific Tele-Ophthalmology Society, 2019). Images are taken using fundus photography\nand vary in terms of clinic source, camera used, and time taken. Images are labeled by clinicians on\na scale of 0 to 4 for the severity of diabetic retinopathy. Given the scaled nature of the labels, the\ncompetition uses quadratic weighted kappa (QWK) as the evaluation metric. We create a local 80%\nto 20% random class-balanced train/validation split, as the competition test labels are hidden.\nWe find that models after VGG do not show significant improvement. Similar to in CCT-20, DeiT\nand EfficientNets performs slightly worse, while deeper models from the same architecture slightly\nhelp performance. We also find that accuracy has a similar trend as QWK, despite it being an inferior\nmetric in the context of this dataset.\nWhen performance stagnates, one might ask whether we have reached a performance limit for our\nclass of models on the dataset. To answer this question, we compare with the Kaggle leaderboard\u2019s\ntop submissions. The top Kaggle submission achieves 0.936 QWK on the private leaderboard (85%\nof the test set) (Xu, 2019). They do this by using additional augmentation, using external data,\ntraining on L1-loss, replacing the final pooling layer with generalized mean pooling, and ensembling\na variety of models trained with different input sizes. The external data consists of 88,702 images\nfrom the 2015 Diabetic Retinopathy Detection Kaggle competition.\nEven though performance saturates with architecture, we find that additional data augmentation and\nother interventions still improve accuracy. We submitted our ResNet-50 and ResNet-152 models\nwith additional interventions, along with an Inception-ResNet v2 (Szegedy et al., 2017b) model with\nhyperparameter tuning. We find that increasing color and affine augmentation by itself can account\nfor a 0.03 QWK point improvement. Once we train on 512 input size, additional augmentation, and\nadditional data, our ResNet-50 and Inception-ResNet v2 both achieve 0.896 QWK on the private\nleaderboard, while ResNet-152 achieves 0.890 QWK, once again suggesting that better ImageNet\narchitectures by themselves do not lead to increased performance on this task.\nAs a comparison, the ensemble from the top leaderboard entry included a single model Inception-\nResNet v2 trained with additional interventions that achieves 0.927 QWK. We submitted the original\nmodels we trained to Kaggle as well, finding that the new models trained with additional interven-\ntions do at least 0.03 QWK points better. See Appendix F for additional experimental details. Both\nthis result and the gap between our models and the top leaderboard models show that there exist\ninterventions that do improve task performance.\n4.3    HUMAN PROTEIN ATLAS IMAGE CLASSIFICATION\nThe Human Protein Atlas runs the Human Protein Atlas Image Classification competition on Kaggle\nto build an automated tool for identifying and locating proteins from high-throughput microscopy\nimages (Ouyang et al., 2019). Images can contain multiple of the 28 different proteins, so the\ncompetition uses the macro F1 score. Given the multi-label nature of the problem, this requires\nthresholding for prediction. We use a 73% / 18% / 9% train / validation / test-validation split created\nby a previous competitor (Park, 2019). We report results on the validation split, as we find that the\nthresholds selected for the larger validation split generalize well to the smaller test-validation split.\nWe find a slightly positive trend between task performance and ImageNet performance, even when\nignoring AlexNet and MobileNet. This is surprising because ImageNet is quite visually distinct from\nhuman protein slides. These results suggest that models with more parameters help with downstream\nperformance, especially for tasks that have a lot of room for improvement.\nSpecific challenges for this dataset are extreme class imbalance, multi-label thresholding, and gen-\neralization from the training data to the test set. Competitors were able to improve performance\nbeyond the baselines we found by using external data as well as techniques such as data cleaning,\n                                                     6", "md": "# Document\n\n## CCT-20\n\nCCT-20 has its own set of challenges. Animals are often pictured at difficult angles, and sometimes are not even visible in the image because a sequence of frames triggered by activity all have the same label. Despite these challenges, an even higher performing model still does better on this task - we train a CLIP ViT L/14-336px model (85.4% ImageNet top-1) with additional augmentation to achieve 83.4% accuracy on CCT-20.\n\n## APTOS 2019 BLINDNESS DETECTION\n\nThis dataset was created for a Kaggle competition run by the Asia Pacific Tele-Ophthalmology Society (APTOS) with the goal of advancing medical screening for diabetic retinopathy in rural areas (Asia Pacific Tele-Ophthalmology Society, 2019). Images are taken using fundus photography and vary in terms of clinic source, camera used, and time taken. Images are labeled by clinicians on a scale of 0 to 4 for the severity of diabetic retinopathy. Given the scaled nature of the labels, the competition uses quadratic weighted kappa (QWK) as the evaluation metric. We create a local 80% to 20% random class-balanced train/validation split, as the competition test labels are hidden. We find that models after VGG do not show significant improvement. Similar to in CCT-20, DeiT and EfficientNets performs slightly worse, while deeper models from the same architecture slightly help performance. We also find that accuracy has a similar trend as QWK, despite it being an inferior metric in the context of this dataset.\n\n### Performance Comparison\n\nWhen performance stagnates, one might ask whether we have reached a performance limit for our class of models on the dataset. To answer this question, we compare with the Kaggle leaderboard\u2019s top submissions. The top Kaggle submission achieves 0.936 QWK on the private leaderboard (85% of the test set) (Xu, 2019). They do this by using additional augmentation, using external data, training on L1-loss, replacing the final pooling layer with generalized mean pooling, and ensembling a variety of models trained with different input sizes. The external data consists of 88,702 images from the 2015 Diabetic Retinopathy Detection Kaggle competition.\n\n### Improving Accuracy\n\nEven though performance saturates with architecture, we find that additional data augmentation and other interventions still improve accuracy. We submitted our ResNet-50 and ResNet-152 models with additional interventions, along with an Inception-ResNet v2 (Szegedy et al., 2017b) model with hyperparameter tuning. We find that increasing color and affine augmentation by itself can account for a 0.03 QWK point improvement. Once we train on 512 input size, additional augmentation, and additional data, our ResNet-50 and Inception-ResNet v2 both achieve 0.896 QWK on the private leaderboard, while ResNet-152 achieves 0.890 QWK, once again suggesting that better ImageNet architectures by themselves do not lead to increased performance on this task.\n\n### Model Ensemble\n\nAs a comparison, the ensemble from the top leaderboard entry included a single model Inception-ResNet v2 trained with additional interventions that achieves 0.927 QWK. We submitted the original models we trained to Kaggle as well, finding that the new models trained with additional interventions do at least 0.03 QWK points better. See Appendix F for additional experimental details. Both this result and the gap between our models and the top leaderboard models show that there exist interventions that do improve task performance.\n\n## HUMAN PROTEIN ATLAS IMAGE CLASSIFICATION\n\nThe Human Protein Atlas runs the Human Protein Atlas Image Classification competition on Kaggle to build an automated tool for identifying and locating proteins from high-throughput microscopy images (Ouyang et al., 2019). Images can contain multiple of the 28 different proteins, so the competition uses the macro F1 score. Given the multi-label nature of the problem, this requires thresholding for prediction. We use a 73% / 18% / 9% train / validation / test-validation split created by a previous competitor (Park, 2019). We report results on the validation split, as we find that the thresholds selected for the larger validation split generalize well to the smaller test-validation split.\n\n### Challenges and Improvements\n\nSpecific challenges for this dataset are extreme class imbalance, multi-label thresholding, and generalization from the training data to the test set. Competitors were able to improve performance beyond the baselines we found by using external data as well as techniques such as data cleaning.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "CCT-20", "md": "## CCT-20"}, {"type": "text", "value": "CCT-20 has its own set of challenges. Animals are often pictured at difficult angles, and sometimes are not even visible in the image because a sequence of frames triggered by activity all have the same label. Despite these challenges, an even higher performing model still does better on this task - we train a CLIP ViT L/14-336px model (85.4% ImageNet top-1) with additional augmentation to achieve 83.4% accuracy on CCT-20.", "md": "CCT-20 has its own set of challenges. Animals are often pictured at difficult angles, and sometimes are not even visible in the image because a sequence of frames triggered by activity all have the same label. Despite these challenges, an even higher performing model still does better on this task - we train a CLIP ViT L/14-336px model (85.4% ImageNet top-1) with additional augmentation to achieve 83.4% accuracy on CCT-20."}, {"type": "heading", "lvl": 2, "value": "APTOS 2019 BLINDNESS DETECTION", "md": "## APTOS 2019 BLINDNESS DETECTION"}, {"type": "text", "value": "This dataset was created for a Kaggle competition run by the Asia Pacific Tele-Ophthalmology Society (APTOS) with the goal of advancing medical screening for diabetic retinopathy in rural areas (Asia Pacific Tele-Ophthalmology Society, 2019). Images are taken using fundus photography and vary in terms of clinic source, camera used, and time taken. Images are labeled by clinicians on a scale of 0 to 4 for the severity of diabetic retinopathy. Given the scaled nature of the labels, the competition uses quadratic weighted kappa (QWK) as the evaluation metric. We create a local 80% to 20% random class-balanced train/validation split, as the competition test labels are hidden. We find that models after VGG do not show significant improvement. Similar to in CCT-20, DeiT and EfficientNets performs slightly worse, while deeper models from the same architecture slightly help performance. We also find that accuracy has a similar trend as QWK, despite it being an inferior metric in the context of this dataset.", "md": "This dataset was created for a Kaggle competition run by the Asia Pacific Tele-Ophthalmology Society (APTOS) with the goal of advancing medical screening for diabetic retinopathy in rural areas (Asia Pacific Tele-Ophthalmology Society, 2019). Images are taken using fundus photography and vary in terms of clinic source, camera used, and time taken. Images are labeled by clinicians on a scale of 0 to 4 for the severity of diabetic retinopathy. Given the scaled nature of the labels, the competition uses quadratic weighted kappa (QWK) as the evaluation metric. We create a local 80% to 20% random class-balanced train/validation split, as the competition test labels are hidden. We find that models after VGG do not show significant improvement. Similar to in CCT-20, DeiT and EfficientNets performs slightly worse, while deeper models from the same architecture slightly help performance. We also find that accuracy has a similar trend as QWK, despite it being an inferior metric in the context of this dataset."}, {"type": "heading", "lvl": 3, "value": "Performance Comparison", "md": "### Performance Comparison"}, {"type": "text", "value": "When performance stagnates, one might ask whether we have reached a performance limit for our class of models on the dataset. To answer this question, we compare with the Kaggle leaderboard\u2019s top submissions. The top Kaggle submission achieves 0.936 QWK on the private leaderboard (85% of the test set) (Xu, 2019). They do this by using additional augmentation, using external data, training on L1-loss, replacing the final pooling layer with generalized mean pooling, and ensembling a variety of models trained with different input sizes. The external data consists of 88,702 images from the 2015 Diabetic Retinopathy Detection Kaggle competition.", "md": "When performance stagnates, one might ask whether we have reached a performance limit for our class of models on the dataset. To answer this question, we compare with the Kaggle leaderboard\u2019s top submissions. The top Kaggle submission achieves 0.936 QWK on the private leaderboard (85% of the test set) (Xu, 2019). They do this by using additional augmentation, using external data, training on L1-loss, replacing the final pooling layer with generalized mean pooling, and ensembling a variety of models trained with different input sizes. The external data consists of 88,702 images from the 2015 Diabetic Retinopathy Detection Kaggle competition."}, {"type": "heading", "lvl": 3, "value": "Improving Accuracy", "md": "### Improving Accuracy"}, {"type": "text", "value": "Even though performance saturates with architecture, we find that additional data augmentation and other interventions still improve accuracy. We submitted our ResNet-50 and ResNet-152 models with additional interventions, along with an Inception-ResNet v2 (Szegedy et al., 2017b) model with hyperparameter tuning. We find that increasing color and affine augmentation by itself can account for a 0.03 QWK point improvement. Once we train on 512 input size, additional augmentation, and additional data, our ResNet-50 and Inception-ResNet v2 both achieve 0.896 QWK on the private leaderboard, while ResNet-152 achieves 0.890 QWK, once again suggesting that better ImageNet architectures by themselves do not lead to increased performance on this task.", "md": "Even though performance saturates with architecture, we find that additional data augmentation and other interventions still improve accuracy. We submitted our ResNet-50 and ResNet-152 models with additional interventions, along with an Inception-ResNet v2 (Szegedy et al., 2017b) model with hyperparameter tuning. We find that increasing color and affine augmentation by itself can account for a 0.03 QWK point improvement. Once we train on 512 input size, additional augmentation, and additional data, our ResNet-50 and Inception-ResNet v2 both achieve 0.896 QWK on the private leaderboard, while ResNet-152 achieves 0.890 QWK, once again suggesting that better ImageNet architectures by themselves do not lead to increased performance on this task."}, {"type": "heading", "lvl": 3, "value": "Model Ensemble", "md": "### Model Ensemble"}, {"type": "text", "value": "As a comparison, the ensemble from the top leaderboard entry included a single model Inception-ResNet v2 trained with additional interventions that achieves 0.927 QWK. We submitted the original models we trained to Kaggle as well, finding that the new models trained with additional interventions do at least 0.03 QWK points better. See Appendix F for additional experimental details. Both this result and the gap between our models and the top leaderboard models show that there exist interventions that do improve task performance.", "md": "As a comparison, the ensemble from the top leaderboard entry included a single model Inception-ResNet v2 trained with additional interventions that achieves 0.927 QWK. We submitted the original models we trained to Kaggle as well, finding that the new models trained with additional interventions do at least 0.03 QWK points better. See Appendix F for additional experimental details. Both this result and the gap between our models and the top leaderboard models show that there exist interventions that do improve task performance."}, {"type": "heading", "lvl": 2, "value": "HUMAN PROTEIN ATLAS IMAGE CLASSIFICATION", "md": "## HUMAN PROTEIN ATLAS IMAGE CLASSIFICATION"}, {"type": "text", "value": "The Human Protein Atlas runs the Human Protein Atlas Image Classification competition on Kaggle to build an automated tool for identifying and locating proteins from high-throughput microscopy images (Ouyang et al., 2019). Images can contain multiple of the 28 different proteins, so the competition uses the macro F1 score. Given the multi-label nature of the problem, this requires thresholding for prediction. We use a 73% / 18% / 9% train / validation / test-validation split created by a previous competitor (Park, 2019). We report results on the validation split, as we find that the thresholds selected for the larger validation split generalize well to the smaller test-validation split.", "md": "The Human Protein Atlas runs the Human Protein Atlas Image Classification competition on Kaggle to build an automated tool for identifying and locating proteins from high-throughput microscopy images (Ouyang et al., 2019). Images can contain multiple of the 28 different proteins, so the competition uses the macro F1 score. Given the multi-label nature of the problem, this requires thresholding for prediction. We use a 73% / 18% / 9% train / validation / test-validation split created by a previous competitor (Park, 2019). We report results on the validation split, as we find that the thresholds selected for the larger validation split generalize well to the smaller test-validation split."}, {"type": "heading", "lvl": 3, "value": "Challenges and Improvements", "md": "### Challenges and Improvements"}, {"type": "text", "value": "Specific challenges for this dataset are extreme class imbalance, multi-label thresholding, and generalization from the training data to the test set. Competitors were able to improve performance beyond the baselines we found by using external data as well as techniques such as data cleaning.", "md": "Specific challenges for this dataset are extreme class imbalance, multi-label thresholding, and generalization from the training data to the test set. Competitors were able to improve performance beyond the baselines we found by using external data as well as techniques such as data cleaning."}]}, {"page": 7, "text": "additional training augmentation, test time augmentation, ensembling, and oversampling (Dai, 2019;\nPark, 2019; Shugaev, 2019). Additionally, some competitors modified commonly-used architectures\nby substituting pooling layers or incorporating attention (Park, 2019; Zheng, 2019). Uniquely, the\nfirst place solution used metric learning on top of a single DenseNet121 (Dai, 2019). These tech-\nniques may be useful when applied to other datasets, but are rarely used in a typical workflow.\n4.4   SIIM-ISIC MELANOMA CLASSIFICATION\nThe Society for Imaging Informatics in Medicine (SIIM) and the International Skin Imaging Collab-\noration (ISIC) jointly ran this Kaggle competition for identifying Melanoma (SIIM & ISIC, 2020),\na serious type of skin cancer. Competitors use images of skin lesions to predict the probability that\neach observed image is malignant. Images come from the ISIC Archive, which is publicly available\nand contains images from a variety of countries. The competition provided 33,126 training images,\nplus an additional 25,331 images from previous competitions. We split the combined data into an\n80% to 20% class-balanced and year-balanced train/validation split. Given the imbalanced nature of\nthe data (8.8% positive), the competition uses area under ROC curve as the evaluation metric.\nWe find only a weak positive correlation (0.44) between ImageNet performance and task perfor-\nmance, with a regression line with a normalized slope of close to zero (0.05). But if we instead look\nat classification accuracy, Appendix H shows that there is a stronger trend for transfer than that of\narea under ROC curve, as model task accuracy more closely follows the same order as ImageNet\nperformance. This difference shows that characterizing the relationship between better ImageNet\nmodels and better transfer performance is reliant on the evaluation metric as well. We use a rela-\ntively simple setup to measure the impact of ImageNet models on task performance, but we know we\ncan achieve better results with additional strategies. The top two Kaggle solutions used models with\ndifferent input size, ensembling, cross-validation and a significant variety of training augmentation\nto create a stable model that generalized to the hidden test set (Ha et al., 2020; Pan, 2020).\n4.5   CASSAVA LEAF DISEASE CLASSIFICATION\nThe Makerere Artificial Intelligence Lab is an academic research group focused on applications\nthat benefit the developing world. Their goal in creating the Cassava Leaf Disease Classification\nKaggle competition (Makerere University AI Lab, 2021) was to give farmers access to methods\nfor diagnosing plant diseases, which could allow farmers to prevent these diseases from spreading,\nincreasing crop yield. Images were taken with an inexpensive camera and labeled by agricultural\nexperts. Each image was classified as healthy or as one of four different diseases. We report results\nusing a 80%/20% random class-balanced train/validation split of the provided training data.\nOnce we ignore models below 70% ImageNet accuracy, the relationship between the performance on\nthe two datasets has both a weak positive correlation (0.12) and a near-zero normalized slope (0.02).\nWhile these are natural images similar to portions of ImageNet, it is notable that ImageNet contains\nvery few plant classes (e.g., buckeye, hip, rapeseed). Yet based on a dataset\u2019s perceived similarity to\nImageNet, it is surprising that leaf disease classification is not positively correlated with ImageNet,\nwhile the microscopy image based Human Protein Atlas competition is. Our results are supported\nby Kaggle competitors: the first place solution found that on the private leaderboard, EfficientNet\nB4 (Tan & Le, 2019), MobileNet, and ViT (Dosovitskiy et al., 2021b) achieve 89.5%, 89.4%, and\n88.8% respectively (Hanke, 2021). Their ensemble achieves 91.3% on the private leaderboard.\n4.6   EUROSAT\nHelber et al. (2019) created EuroSAT from Sentinel-2 satellite images to classify land use and land\ncover. Past work has improved performance on the dataset through additional training time tech-\nniques (Naushad et al., 2021) and using 13 spectral bands (Yassine et al., 2021). We use RGB\nimages and keep our experimental setup consistent to compare across a range of models. Since\nthere is no set train/test split, we create a 80%/20% class-balanced split.\nAll models over 60% ImageNet accuracy achieve over 98.5% EuroSAT accuracy, and the majority\nof our models achieve over 99.0% EuroSAT accuracy. There are certain tasks where using better\nImageNet models does not improve performance, and this would be the extreme case where perfor-\nmance saturation is close to being achieved. While it is outside the scope of this study, a next step\nwould be to investigate the remaining errors and find other methods to reduce this last bit of error.\n                                                   7", "md": "# Document\n\n## SIIM-ISIC MELANOMA CLASSIFICATION\n\nThe Society for Imaging Informatics in Medicine (SIIM) and the International Skin Imaging Collaboration (ISIC) jointly ran this Kaggle competition for identifying Melanoma (SIIM & ISIC, 2020), a serious type of skin cancer. Competitors use images of skin lesions to predict the probability that each observed image is malignant. Images come from the ISIC Archive, which is publicly available and contains images from a variety of countries. The competition provided 33,126 training images, plus an additional 25,331 images from previous competitions. We split the combined data into an 80% to 20% class-balanced and year-balanced train/validation split. Given the imbalanced nature of the data (8.8% positive), the competition uses area under ROC curve as the evaluation metric.\n\nWe find only a weak positive correlation (0.44) between ImageNet performance and task performance, with a regression line with a normalized slope of close to zero (0.05). But if we instead look at classification accuracy, Appendix H shows that there is a stronger trend for transfer than that of area under ROC curve, as model task accuracy more closely follows the same order as ImageNet performance. This difference shows that characterizing the relationship between better ImageNet models and better transfer performance is reliant on the evaluation metric as well. We use a relatively simple setup to measure the impact of ImageNet models on task performance, but we know we can achieve better results with additional strategies. The top two Kaggle solutions used models with different input size, ensembling, cross-validation and a significant variety of training augmentation to create a stable model that generalized to the hidden test set (Ha et al., 2020; Pan, 2020).\n\n## CASSAVA LEAF DISEASE CLASSIFICATION\n\nThe Makerere Artificial Intelligence Lab is an academic research group focused on applications that benefit the developing world. Their goal in creating the Cassava Leaf Disease Classification Kaggle competition (Makerere University AI Lab, 2021) was to give farmers access to methods for diagnosing plant diseases, which could allow farmers to prevent these diseases from spreading, increasing crop yield. Images were taken with an inexpensive camera and labeled by agricultural experts. Each image was classified as healthy or as one of four different diseases. We report results using a 80%/20% random class-balanced train/validation split of the provided training data.\n\nOnce we ignore models below 70% ImageNet accuracy, the relationship between the performance on the two datasets has both a weak positive correlation (0.12) and a near-zero normalized slope (0.02). While these are natural images similar to portions of ImageNet, it is notable that ImageNet contains very few plant classes (e.g., buckeye, hip, rapeseed). Yet based on a dataset\u2019s perceived similarity to ImageNet, it is surprising that leaf disease classification is not positively correlated with ImageNet, while the microscopy image based Human Protein Atlas competition is. Our results are supported by Kaggle competitors: the first place solution found that on the private leaderboard, EfficientNet B4 (Tan & Le, 2019), MobileNet, and ViT (Dosovitskiy et al., 2021b) achieve 89.5%, 89.4%, and 88.8% respectively (Hanke, 2021). Their ensemble achieves 91.3% on the private leaderboard.\n\n## EUROSAT\n\nHelber et al. (2019) created EuroSAT from Sentinel-2 satellite images to classify land use and land cover. Past work has improved performance on the dataset through additional training time techniques (Naushad et al., 2021) and using 13 spectral bands (Yassine et al., 2021). We use RGB images and keep our experimental setup consistent to compare across a range of models. Since there is no set train/test split, we create a 80%/20% class-balanced split.\n\nAll models over 60% ImageNet accuracy achieve over 98.5% EuroSAT accuracy, and the majority of our models achieve over 99.0% EuroSAT accuracy. There are certain tasks where using better ImageNet models does not improve performance, and this would be the extreme case where performance saturation is close to being achieved. While it is outside the scope of this study, a next step would be to investigate the remaining errors and find other methods to reduce this last bit of error.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "SIIM-ISIC MELANOMA CLASSIFICATION", "md": "## SIIM-ISIC MELANOMA CLASSIFICATION"}, {"type": "text", "value": "The Society for Imaging Informatics in Medicine (SIIM) and the International Skin Imaging Collaboration (ISIC) jointly ran this Kaggle competition for identifying Melanoma (SIIM & ISIC, 2020), a serious type of skin cancer. Competitors use images of skin lesions to predict the probability that each observed image is malignant. Images come from the ISIC Archive, which is publicly available and contains images from a variety of countries. The competition provided 33,126 training images, plus an additional 25,331 images from previous competitions. We split the combined data into an 80% to 20% class-balanced and year-balanced train/validation split. Given the imbalanced nature of the data (8.8% positive), the competition uses area under ROC curve as the evaluation metric.\n\nWe find only a weak positive correlation (0.44) between ImageNet performance and task performance, with a regression line with a normalized slope of close to zero (0.05). But if we instead look at classification accuracy, Appendix H shows that there is a stronger trend for transfer than that of area under ROC curve, as model task accuracy more closely follows the same order as ImageNet performance. This difference shows that characterizing the relationship between better ImageNet models and better transfer performance is reliant on the evaluation metric as well. We use a relatively simple setup to measure the impact of ImageNet models on task performance, but we know we can achieve better results with additional strategies. The top two Kaggle solutions used models with different input size, ensembling, cross-validation and a significant variety of training augmentation to create a stable model that generalized to the hidden test set (Ha et al., 2020; Pan, 2020).", "md": "The Society for Imaging Informatics in Medicine (SIIM) and the International Skin Imaging Collaboration (ISIC) jointly ran this Kaggle competition for identifying Melanoma (SIIM & ISIC, 2020), a serious type of skin cancer. Competitors use images of skin lesions to predict the probability that each observed image is malignant. Images come from the ISIC Archive, which is publicly available and contains images from a variety of countries. The competition provided 33,126 training images, plus an additional 25,331 images from previous competitions. We split the combined data into an 80% to 20% class-balanced and year-balanced train/validation split. Given the imbalanced nature of the data (8.8% positive), the competition uses area under ROC curve as the evaluation metric.\n\nWe find only a weak positive correlation (0.44) between ImageNet performance and task performance, with a regression line with a normalized slope of close to zero (0.05). But if we instead look at classification accuracy, Appendix H shows that there is a stronger trend for transfer than that of area under ROC curve, as model task accuracy more closely follows the same order as ImageNet performance. This difference shows that characterizing the relationship between better ImageNet models and better transfer performance is reliant on the evaluation metric as well. We use a relatively simple setup to measure the impact of ImageNet models on task performance, but we know we can achieve better results with additional strategies. The top two Kaggle solutions used models with different input size, ensembling, cross-validation and a significant variety of training augmentation to create a stable model that generalized to the hidden test set (Ha et al., 2020; Pan, 2020)."}, {"type": "heading", "lvl": 2, "value": "CASSAVA LEAF DISEASE CLASSIFICATION", "md": "## CASSAVA LEAF DISEASE CLASSIFICATION"}, {"type": "text", "value": "The Makerere Artificial Intelligence Lab is an academic research group focused on applications that benefit the developing world. Their goal in creating the Cassava Leaf Disease Classification Kaggle competition (Makerere University AI Lab, 2021) was to give farmers access to methods for diagnosing plant diseases, which could allow farmers to prevent these diseases from spreading, increasing crop yield. Images were taken with an inexpensive camera and labeled by agricultural experts. Each image was classified as healthy or as one of four different diseases. We report results using a 80%/20% random class-balanced train/validation split of the provided training data.\n\nOnce we ignore models below 70% ImageNet accuracy, the relationship between the performance on the two datasets has both a weak positive correlation (0.12) and a near-zero normalized slope (0.02). While these are natural images similar to portions of ImageNet, it is notable that ImageNet contains very few plant classes (e.g., buckeye, hip, rapeseed). Yet based on a dataset\u2019s perceived similarity to ImageNet, it is surprising that leaf disease classification is not positively correlated with ImageNet, while the microscopy image based Human Protein Atlas competition is. Our results are supported by Kaggle competitors: the first place solution found that on the private leaderboard, EfficientNet B4 (Tan & Le, 2019), MobileNet, and ViT (Dosovitskiy et al., 2021b) achieve 89.5%, 89.4%, and 88.8% respectively (Hanke, 2021). Their ensemble achieves 91.3% on the private leaderboard.", "md": "The Makerere Artificial Intelligence Lab is an academic research group focused on applications that benefit the developing world. Their goal in creating the Cassava Leaf Disease Classification Kaggle competition (Makerere University AI Lab, 2021) was to give farmers access to methods for diagnosing plant diseases, which could allow farmers to prevent these diseases from spreading, increasing crop yield. Images were taken with an inexpensive camera and labeled by agricultural experts. Each image was classified as healthy or as one of four different diseases. We report results using a 80%/20% random class-balanced train/validation split of the provided training data.\n\nOnce we ignore models below 70% ImageNet accuracy, the relationship between the performance on the two datasets has both a weak positive correlation (0.12) and a near-zero normalized slope (0.02). While these are natural images similar to portions of ImageNet, it is notable that ImageNet contains very few plant classes (e.g., buckeye, hip, rapeseed). Yet based on a dataset\u2019s perceived similarity to ImageNet, it is surprising that leaf disease classification is not positively correlated with ImageNet, while the microscopy image based Human Protein Atlas competition is. Our results are supported by Kaggle competitors: the first place solution found that on the private leaderboard, EfficientNet B4 (Tan & Le, 2019), MobileNet, and ViT (Dosovitskiy et al., 2021b) achieve 89.5%, 89.4%, and 88.8% respectively (Hanke, 2021). Their ensemble achieves 91.3% on the private leaderboard."}, {"type": "heading", "lvl": 2, "value": "EUROSAT", "md": "## EUROSAT"}, {"type": "text", "value": "Helber et al. (2019) created EuroSAT from Sentinel-2 satellite images to classify land use and land cover. Past work has improved performance on the dataset through additional training time techniques (Naushad et al., 2021) and using 13 spectral bands (Yassine et al., 2021). We use RGB images and keep our experimental setup consistent to compare across a range of models. Since there is no set train/test split, we create a 80%/20% class-balanced split.\n\nAll models over 60% ImageNet accuracy achieve over 98.5% EuroSAT accuracy, and the majority of our models achieve over 99.0% EuroSAT accuracy. There are certain tasks where using better ImageNet models does not improve performance, and this would be the extreme case where performance saturation is close to being achieved. While it is outside the scope of this study, a next step would be to investigate the remaining errors and find other methods to reduce this last bit of error.", "md": "Helber et al. (2019) created EuroSAT from Sentinel-2 satellite images to classify land use and land cover. Past work has improved performance on the dataset through additional training time techniques (Naushad et al., 2021) and using 13 spectral bands (Yassine et al., 2021). We use RGB images and keep our experimental setup consistent to compare across a range of models. Since there is no set train/test split, we create a 80%/20% class-balanced split.\n\nAll models over 60% ImageNet accuracy achieve over 98.5% EuroSAT accuracy, and the majority of our models achieve over 99.0% EuroSAT accuracy. There are certain tasks where using better ImageNet models does not improve performance, and this would be the extreme case where performance saturation is close to being achieved. While it is outside the scope of this study, a next step would be to investigate the remaining errors and find other methods to reduce this last bit of error."}]}, {"page": 8, "text": "5    ADDITIONAL STUDIES\n5.1    AUGMENTATION ABLATIONS\nIn our main experiments, we keep augmentation simple to minimize confounding factors when com-\nparing models. However, it is possible pre-training and fine-tuning with different combinations of\naugmentations may have different results. This is an important point because different architectures\nmay have different inductive biases and often use different augmentation strategies at pre-training\ntime. To investigate these effects, we run additional experiments on CCT-20 and APTOS to explore\nthe effect of data augmentation on transfer. Specifically, we take ResNet-50 models pre-trained with\nstandard crop and flip augmentation, AugMix (Hendrycks et al., 2020), and RandAugment (Cubuk\net al., 2020), and then fine-tune on our default augmentation, AugMix, and RandAugment. We also\nstudy DeiT-tiny and Deit-small models by fine-tuning on the same three augmentations mentioned\nabove. We choose to examine DeiT models because they are pre-trained using RandAugment and\nRandErasing (Zhong et al., 2020). We increase the number of epochs we fine-tune on from 30 to 50\nto account for augmentation. Our experimental results are found in Appendix G.\nIn our ResNet-50 experiments, both AugMix and RandAugment improve performance on ImageNet,\nbut while pre-training with RandAugment improves performance on downstream tasks, pre-training\nwith AugMix does not. Furthermore, fine-tuning with RandAugment usually yields additional per-\nformance gains when compared to our default fine-tuning augmentation, no matter which pre-trained\nmodel is used. For DeiT models, we found that additional augmentation did not significantly in-\ncrease performance on the downstream tasks. Thus, as with architectures, augmentation strategies\nthat improve accuracy on ImageNet do not always improve accuracy on real-world tasks.\n5.2    CLIP MODELS\nA natural follow-up to our experiments is to change the source of pre-training data. We exam-\nine CLIP models from Radford et al. (2021), which use diverse pre-training data and achieve high\nperformance on a variety of downstream datasets. We fine-tune CLIP models on each of our down-\nstream datasets by linear probing then fine-tuning (LP-FT) (Kumar et al., 2022).3 Our results are\nvisualized by the purple stars in Appendix I Figure 8. We see that by using a model that takes larger\nimages we can do better than all previous models, and even without the larger images, ViT-L/14\ndoes better on four out of the six datasets. While across all CLIP models the change in pre-training\ndata increases performance for CCT-20, the effect on the other datasets is more complicated. When\ncontrolling for architecture changes by only looking at ResNet-50 and ViT/B16, we see that the ad-\nditional pre-training data helps for CCT-20, HPA, and Cassava, the former two corresponding to the\ndatasets that empirically benefit most from using better ImageNet models. Additional results can be\nfound in Appendix I, while additional fine-tuning details can be found in Appendix J.\n6    DISCUSSION\nAlternative explanations for saturation. Whereas Kornblith et al. (2019) reported a high degree\nof correlation between ImageNet and transfer accuracy, we find that better ImageNet models do not\nconsistently transfer better on our real-world tasks. We believe these differences are related to the\ntasks themselves. Here, we rule out alternative hypotheses for our findings.\nComparison of datasets statistics suggests that the number of classes and dataset size also do not\nexplain the differences from Kornblith et al. (2019). The datasets we study range from two to 28\nclasses. Although most of the datasets studied in Kornblith et al. (2019) have more classes, CIFAR-\n10 has 10. In Appendix E, we replicate CIFAR-10 results from Kornblith et al. (2019) using our\nexperimental setup, finding a strong correlation between ImageNet accuracy and transfer accuracy.\nThus, the number of classes is likely not the determining factor. Training set sizes are similar\nbetween our study and that of Kornblith et al. (2019) and thus also do not seem to play a major role.\nA third hypothesis is that it is parameter count, rather than ImageNet accuracy, that drives trends.\nWe see that VGG BN models appear to outperform their ImageNet accuracy on multiple datasets,\nand they are among the largest models by parameter count. However, in Appendix L, we find that\nmodel size is also not a good indicator of improved transfer performance on real world datasets.\n   3We use LP-FT because, in past experiments, we have found that LP-FT makes hyperparameter tuning\neasier for CLIP models, but does not significantly alter performance when using optimal hyperparameters.\n                                                     8", "md": "# Additional Studies\n\n## 5 Additional Studies\n\n### 5.1 Augmentation Ablations\n\nIn our main experiments, we keep augmentation simple to minimize confounding factors when comparing models. However, it is possible pre-training and fine-tuning with different combinations of augmentations may have different results. This is an important point because different architectures may have different inductive biases and often use different augmentation strategies at pre-training time. To investigate these effects, we run additional experiments on CCT-20 and APTOS to explore the effect of data augmentation on transfer. Specifically, we take ResNet-50 models pre-trained with standard crop and flip augmentation, AugMix (Hendrycks et al., 2020), and RandAugment (Cubuk et al., 2020), and then fine-tune on our default augmentation, AugMix, and RandAugment. We also study DeiT-tiny and Deit-small models by fine-tuning on the same three augmentations mentioned above. We choose to examine DeiT models because they are pre-trained using RandAugment and RandErasing (Zhong et al., 2020). We increase the number of epochs we fine-tune on from 30 to 50 to account for augmentation. Our experimental results are found in Appendix G.\n\nIn our ResNet-50 experiments, both AugMix and RandAugment improve performance on ImageNet, but while pre-training with RandAugment improves performance on downstream tasks, pre-training with AugMix does not. Furthermore, fine-tuning with RandAugment usually yields additional performance gains when compared to our default fine-tuning augmentation, no matter which pre-trained model is used. For DeiT models, we found that additional augmentation did not significantly increase performance on the downstream tasks. Thus, as with architectures, augmentation strategies that improve accuracy on ImageNet do not always improve accuracy on real-world tasks.\n\n### 5.2 Clip Models\n\nA natural follow-up to our experiments is to change the source of pre-training data. We examine CLIP models from Radford et al. (2021), which use diverse pre-training data and achieve high performance on a variety of downstream datasets. We fine-tune CLIP models on each of our downstream datasets by linear probing then fine-tuning (LP-FT) (Kumar et al., 2022). Our results are visualized by the purple stars in Appendix I Figure 8. We see that by using a model that takes larger images we can do better than all previous models, and even without the larger images, ViT-L/14 does better on four out of the six datasets. While across all CLIP models the change in pre-training data increases performance for CCT-20, the effect on the other datasets is more complicated. When controlling for architecture changes by only looking at ResNet-50 and ViT/B16, we see that the additional pre-training data helps for CCT-20, HPA, and Cassava, the former two corresponding to the datasets that empirically benefit most from using better ImageNet models. Additional results can be found in Appendix I, while additional fine-tuning details can be found in Appendix J.\n\n## 6 Discussion\n\nAlternative explanations for saturation. Whereas Kornblith et al. (2019) reported a high degree of correlation between ImageNet and transfer accuracy, we find that better ImageNet models do not consistently transfer better on our real-world tasks. We believe these differences are related to the tasks themselves. Here, we rule out alternative hypotheses for our findings.\n\nComparison of datasets statistics suggests that the number of classes and dataset size also do not explain the differences from Kornblith et al. (2019). The datasets we study range from two to 28 classes. Although most of the datasets studied in Kornblith et al. (2019) have more classes, CIFAR-10 has 10. In Appendix E, we replicate CIFAR-10 results from Kornblith et al. (2019) using our experimental setup, finding a strong correlation between ImageNet accuracy and transfer accuracy. Thus, the number of classes is likely not the determining factor. Training set sizes are similar between our study and that of Kornblith et al. (2019) and thus also do not seem to play a major role.\n\nA third hypothesis is that it is parameter count, rather than ImageNet accuracy, that drives trends. We see that VGG BN models appear to outperform their ImageNet accuracy on multiple datasets, and they are among the largest models by parameter count. However, in Appendix L, we find that model size is also not a good indicator of improved transfer performance on real world datasets.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Studies", "md": "# Additional Studies"}, {"type": "heading", "lvl": 2, "value": "5 Additional Studies", "md": "## 5 Additional Studies"}, {"type": "heading", "lvl": 3, "value": "5.1 Augmentation Ablations", "md": "### 5.1 Augmentation Ablations"}, {"type": "text", "value": "In our main experiments, we keep augmentation simple to minimize confounding factors when comparing models. However, it is possible pre-training and fine-tuning with different combinations of augmentations may have different results. This is an important point because different architectures may have different inductive biases and often use different augmentation strategies at pre-training time. To investigate these effects, we run additional experiments on CCT-20 and APTOS to explore the effect of data augmentation on transfer. Specifically, we take ResNet-50 models pre-trained with standard crop and flip augmentation, AugMix (Hendrycks et al., 2020), and RandAugment (Cubuk et al., 2020), and then fine-tune on our default augmentation, AugMix, and RandAugment. We also study DeiT-tiny and Deit-small models by fine-tuning on the same three augmentations mentioned above. We choose to examine DeiT models because they are pre-trained using RandAugment and RandErasing (Zhong et al., 2020). We increase the number of epochs we fine-tune on from 30 to 50 to account for augmentation. Our experimental results are found in Appendix G.\n\nIn our ResNet-50 experiments, both AugMix and RandAugment improve performance on ImageNet, but while pre-training with RandAugment improves performance on downstream tasks, pre-training with AugMix does not. Furthermore, fine-tuning with RandAugment usually yields additional performance gains when compared to our default fine-tuning augmentation, no matter which pre-trained model is used. For DeiT models, we found that additional augmentation did not significantly increase performance on the downstream tasks. Thus, as with architectures, augmentation strategies that improve accuracy on ImageNet do not always improve accuracy on real-world tasks.", "md": "In our main experiments, we keep augmentation simple to minimize confounding factors when comparing models. However, it is possible pre-training and fine-tuning with different combinations of augmentations may have different results. This is an important point because different architectures may have different inductive biases and often use different augmentation strategies at pre-training time. To investigate these effects, we run additional experiments on CCT-20 and APTOS to explore the effect of data augmentation on transfer. Specifically, we take ResNet-50 models pre-trained with standard crop and flip augmentation, AugMix (Hendrycks et al., 2020), and RandAugment (Cubuk et al., 2020), and then fine-tune on our default augmentation, AugMix, and RandAugment. We also study DeiT-tiny and Deit-small models by fine-tuning on the same three augmentations mentioned above. We choose to examine DeiT models because they are pre-trained using RandAugment and RandErasing (Zhong et al., 2020). We increase the number of epochs we fine-tune on from 30 to 50 to account for augmentation. Our experimental results are found in Appendix G.\n\nIn our ResNet-50 experiments, both AugMix and RandAugment improve performance on ImageNet, but while pre-training with RandAugment improves performance on downstream tasks, pre-training with AugMix does not. Furthermore, fine-tuning with RandAugment usually yields additional performance gains when compared to our default fine-tuning augmentation, no matter which pre-trained model is used. For DeiT models, we found that additional augmentation did not significantly increase performance on the downstream tasks. Thus, as with architectures, augmentation strategies that improve accuracy on ImageNet do not always improve accuracy on real-world tasks."}, {"type": "heading", "lvl": 3, "value": "5.2 Clip Models", "md": "### 5.2 Clip Models"}, {"type": "text", "value": "A natural follow-up to our experiments is to change the source of pre-training data. We examine CLIP models from Radford et al. (2021), which use diverse pre-training data and achieve high performance on a variety of downstream datasets. We fine-tune CLIP models on each of our downstream datasets by linear probing then fine-tuning (LP-FT) (Kumar et al., 2022). Our results are visualized by the purple stars in Appendix I Figure 8. We see that by using a model that takes larger images we can do better than all previous models, and even without the larger images, ViT-L/14 does better on four out of the six datasets. While across all CLIP models the change in pre-training data increases performance for CCT-20, the effect on the other datasets is more complicated. When controlling for architecture changes by only looking at ResNet-50 and ViT/B16, we see that the additional pre-training data helps for CCT-20, HPA, and Cassava, the former two corresponding to the datasets that empirically benefit most from using better ImageNet models. Additional results can be found in Appendix I, while additional fine-tuning details can be found in Appendix J.", "md": "A natural follow-up to our experiments is to change the source of pre-training data. We examine CLIP models from Radford et al. (2021), which use diverse pre-training data and achieve high performance on a variety of downstream datasets. We fine-tune CLIP models on each of our downstream datasets by linear probing then fine-tuning (LP-FT) (Kumar et al., 2022). Our results are visualized by the purple stars in Appendix I Figure 8. We see that by using a model that takes larger images we can do better than all previous models, and even without the larger images, ViT-L/14 does better on four out of the six datasets. While across all CLIP models the change in pre-training data increases performance for CCT-20, the effect on the other datasets is more complicated. When controlling for architecture changes by only looking at ResNet-50 and ViT/B16, we see that the additional pre-training data helps for CCT-20, HPA, and Cassava, the former two corresponding to the datasets that empirically benefit most from using better ImageNet models. Additional results can be found in Appendix I, while additional fine-tuning details can be found in Appendix J."}, {"type": "heading", "lvl": 2, "value": "6 Discussion", "md": "## 6 Discussion"}, {"type": "text", "value": "Alternative explanations for saturation. Whereas Kornblith et al. (2019) reported a high degree of correlation between ImageNet and transfer accuracy, we find that better ImageNet models do not consistently transfer better on our real-world tasks. We believe these differences are related to the tasks themselves. Here, we rule out alternative hypotheses for our findings.\n\nComparison of datasets statistics suggests that the number of classes and dataset size also do not explain the differences from Kornblith et al. (2019). The datasets we study range from two to 28 classes. Although most of the datasets studied in Kornblith et al. (2019) have more classes, CIFAR-10 has 10. In Appendix E, we replicate CIFAR-10 results from Kornblith et al. (2019) using our experimental setup, finding a strong correlation between ImageNet accuracy and transfer accuracy. Thus, the number of classes is likely not the determining factor. Training set sizes are similar between our study and that of Kornblith et al. (2019) and thus also do not seem to play a major role.\n\nA third hypothesis is that it is parameter count, rather than ImageNet accuracy, that drives trends. We see that VGG BN models appear to outperform their ImageNet accuracy on multiple datasets, and they are among the largest models by parameter count. However, in Appendix L, we find that model size is also not a good indicator of improved transfer performance on real world datasets.", "md": "Alternative explanations for saturation. Whereas Kornblith et al. (2019) reported a high degree of correlation between ImageNet and transfer accuracy, we find that better ImageNet models do not consistently transfer better on our real-world tasks. We believe these differences are related to the tasks themselves. Here, we rule out alternative hypotheses for our findings.\n\nComparison of datasets statistics suggests that the number of classes and dataset size also do not explain the differences from Kornblith et al. (2019). The datasets we study range from two to 28 classes. Although most of the datasets studied in Kornblith et al. (2019) have more classes, CIFAR-10 has 10. In Appendix E, we replicate CIFAR-10 results from Kornblith et al. (2019) using our experimental setup, finding a strong correlation between ImageNet accuracy and transfer accuracy. Thus, the number of classes is likely not the determining factor. Training set sizes are similar between our study and that of Kornblith et al. (2019) and thus also do not seem to play a major role.\n\nA third hypothesis is that it is parameter count, rather than ImageNet accuracy, that drives trends. We see that VGG BN models appear to outperform their ImageNet accuracy on multiple datasets, and they are among the largest models by parameter count. However, in Appendix L, we find that model size is also not a good indicator of improved transfer performance on real world datasets."}]}, {"page": 9, "text": "Differences between web-scraped datasets and real-world images We conjecture that it is possi-\nble to perform well on most, if not all, web-scraped target datasets simply by collecting a very large\namount of data from the Internet and training a very large model on it. Web-scraped target datasets\nare by definition within the distribution of data collected from the web, and a sufficiently large model\ncan learn that distribution. In support of this conjecture, recent models such as CLIP (Radford et al.,\n2021), ALIGN (Jia et al., 2021), ViT-G (Zhai et al., 2022), BASIC (Pham et al., 2021), and CoCa (Yu\net al., 2022) are trained on very large web-scraped datasets and achieve high accuracy on a variety of\nweb-scraped benchmarks. However, this strategy may not be effective for non-web-scraped datasets,\nwhere there is no guarantee that we will train on data that is close in distribution to the target data,\neven if we train on the entire web. Thus, it makes sense to distinguish these two types of datasets.\nThere are clear differences in image distribution between the non-web-scraped datasets we consider\nand web-scraped datasets considered by previous work. In Figure 3 and Appendix M, we compute\nFr\u00b4\n  echet inception distance (FID) (Heusel et al.,\n2017) between ImageNet and each of the datasets\nwe study in this work as well as the ones found in\nKornblith et al. (2019). The real-world datasets are\nfurther away from ImageNet than those found in Ko-\nrnblith et al. (2019), implying that there is a large\namount of distribution shift between web-scraped\ndatasets and real-world datasets. However, FID is\nonly a proxy measure and may not capture all fac-                                       4300 4 48\ntors that lead to differences in transferability.\nWhereas web-scraped data is cheap to acquire, real-\nworld data can be more expensive. Ideally, progress            Figure 3: FID scores vs ImageNet for the datasets\nin computer vision architectures should improve per-           we study in this work (red), and the web-scraped\nformance not just on web-scraped data, but also on             datasets studied by Kornblith et al. (2019) (blue).\nreal-world tasks. Our results suggest that the latter has not happened. Gains in ImageNet accuracy\nover the last decade have primarily come from improving and scaling architectures, and past work\nhas shown that these gains generally transfer to other web-scraped datasets, regardless of size (Sun\net al., 2017; Kornblith et al., 2019; Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020).\nHowever, we find that improvements arising from architecture generally do not transfer to non-web-\nscraped tasks. Nonetheless, data augmentation and other tweaks can provide further gains on these\ntasks.\nRecommendations towards better benchmarking. While it is unclear whether researchers have\nover-optimized for ImageNet, our work suggests that researchers should explicitly search for meth-\nods that improve accuracy on real-world non-web-scraped datasets, rather than assuming that meth-\nods that improve accuracy on ImageNet will provide meaningful improvements on real-world\ndatasets as well. Just as there are methods that improve accuracy on ImageNet but not on the tasks\nwe investigate, there may be methods that improve accuracy on our tasks but not ImageNet. The\nKaggle community provides some evidence for the existence of such methods; Kaggle submissions\noften explore architectural improvements that are less common in traditional ImageNet pre-trained\nmodels. To measure such improvements on real-world problems, we suggest simply using the aver-\nage accuracy across our tasks as a benchmark for future representation learning research.\nFurther analysis of our results shows consistencies in the accuracies of different models across the\nnon-web-scraped datasets, suggesting that accuracy improvements on these datasets may translate\nto other datasets. For each dataset, we use linear regression to predict model accuracies on the target\ndataset as a linear combination of ImageNet accuracy and accuracy averaged across the other real-\nworld datasets. We perform an F-test to determine whether the average accuracy on other real-world\ndatasets explains significant variance beyond that explained by ImageNet accuracy. We find that this\nF-test is significant on all datasets except EuroSAT, where accuracy may be very close to ceiling\n(see further analysis in Appendix N.1). Additionally, in Appendix N.2 we compare the Spearman\nrank correlation (i.e., the Pearson correlation between ranks) between each dataset and the accuracy\naveraged across the other real-world datasets to the Spearman correlation between each dataset and\nImageNet. We find that the correlation with the average over real-world datasets is higher than\nthe correlation with ImageNet and statistically significant for CCT-20, APTOS, HPA, and Cassava.\nThus, there is some signal in the average accuracy across the datasets that we investigate that is not\ncaptured by ImageNet top-1 accuracy.\n                                                        9", "md": "Differences between web-scraped datasets and real-world images We conjecture that it is possible to perform well on most, if not all, web-scraped target datasets simply by collecting a very large amount of data from the Internet and training a very large model on it. Web-scraped target datasets are by definition within the distribution of data collected from the web, and a sufficiently large model can learn that distribution. In support of this conjecture, recent models such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), ViT-G (Zhai et al., 2022), BASIC (Pham et al., 2021), and CoCa (Yu et al., 2022) are trained on very large web-scraped datasets and achieve high accuracy on a variety of web-scraped benchmarks. However, this strategy may not be effective for non-web-scraped datasets, where there is no guarantee that we will train on data that is close in distribution to the target data, even if we train on the entire web. Thus, it makes sense to distinguish these two types of datasets. There are clear differences in image distribution between the non-web-scraped datasets we consider and web-scraped datasets considered by previous work. In Figure 3 and Appendix M, we compute Fr\u00e9chet inception distance (FID) (Heusel et al., 2017) between ImageNet and each of the datasets we study in this work as well as the ones found in Kornblith et al. (2019). The real-world datasets are further away from ImageNet than those found in Kornblith et al. (2019), implying that there is a large amount of distribution shift between web-scraped datasets and real-world datasets. However, FID is only a proxy measure and may not capture all factors that lead to differences in transferability.\n\nWhereas web-scraped data is cheap to acquire, real-world data can be more expensive. Ideally, progress in computer vision architectures should improve performance not just on web-scraped data, but also on real-world tasks. Our results suggest that the latter has not happened. Gains in ImageNet accuracy over the last decade have primarily come from improving and scaling architectures, and past work has shown that these gains generally transfer to other web-scraped datasets, regardless of size (Sun et al., 2017; Kornblith et al., 2019; Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). However, we find that improvements arising from architecture generally do not transfer to non-web-scraped tasks. Nonetheless, data augmentation and other tweaks can provide further gains on these tasks.\n\nRecommendations towards better benchmarking. While it is unclear whether researchers have over-optimized for ImageNet, our work suggests that researchers should explicitly search for methods that improve accuracy on real-world non-web-scraped datasets, rather than assuming that methods that improve accuracy on ImageNet will provide meaningful improvements on real-world datasets as well. Just as there are methods that improve accuracy on ImageNet but not on the tasks we investigate, there may be methods that improve accuracy on our tasks but not ImageNet. The Kaggle community provides some evidence for the existence of such methods; Kaggle submissions often explore architectural improvements that are less common in traditional ImageNet pre-trained models. To measure such improvements on real-world problems, we suggest simply using the average accuracy across our tasks as a benchmark for future representation learning research.\n\nFurther analysis of our results shows consistencies in the accuracies of different models across the non-web-scraped datasets, suggesting that accuracy improvements on these datasets may translate to other datasets. For each dataset, we use linear regression to predict model accuracies on the target dataset as a linear combination of ImageNet accuracy and accuracy averaged across the other real-world datasets. We perform an F-test to determine whether the average accuracy on other real-world datasets explains significant variance beyond that explained by ImageNet accuracy. We find that this F-test is significant on all datasets except EuroSAT, where accuracy may be very close to ceiling (see further analysis in Appendix N.1). Additionally, in Appendix N.2 we compare the Spearman rank correlation (i.e., the Pearson correlation between ranks) between each dataset and the accuracy averaged across the other real-world datasets to the Spearman correlation between each dataset and ImageNet. We find that the correlation with the average over real-world datasets is higher than the correlation with ImageNet and statistically significant for CCT-20, APTOS, HPA, and Cassava. Thus, there is some signal in the average accuracy across the datasets that we investigate that is not captured by ImageNet top-1 accuracy.", "images": [{"name": "page-9-0.jpg", "height": 124, "width": 179, "x": 325, "y": 227}], "items": [{"type": "text", "value": "Differences between web-scraped datasets and real-world images We conjecture that it is possible to perform well on most, if not all, web-scraped target datasets simply by collecting a very large amount of data from the Internet and training a very large model on it. Web-scraped target datasets are by definition within the distribution of data collected from the web, and a sufficiently large model can learn that distribution. In support of this conjecture, recent models such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), ViT-G (Zhai et al., 2022), BASIC (Pham et al., 2021), and CoCa (Yu et al., 2022) are trained on very large web-scraped datasets and achieve high accuracy on a variety of web-scraped benchmarks. However, this strategy may not be effective for non-web-scraped datasets, where there is no guarantee that we will train on data that is close in distribution to the target data, even if we train on the entire web. Thus, it makes sense to distinguish these two types of datasets. There are clear differences in image distribution between the non-web-scraped datasets we consider and web-scraped datasets considered by previous work. In Figure 3 and Appendix M, we compute Fr\u00e9chet inception distance (FID) (Heusel et al., 2017) between ImageNet and each of the datasets we study in this work as well as the ones found in Kornblith et al. (2019). The real-world datasets are further away from ImageNet than those found in Kornblith et al. (2019), implying that there is a large amount of distribution shift between web-scraped datasets and real-world datasets. However, FID is only a proxy measure and may not capture all factors that lead to differences in transferability.\n\nWhereas web-scraped data is cheap to acquire, real-world data can be more expensive. Ideally, progress in computer vision architectures should improve performance not just on web-scraped data, but also on real-world tasks. Our results suggest that the latter has not happened. Gains in ImageNet accuracy over the last decade have primarily come from improving and scaling architectures, and past work has shown that these gains generally transfer to other web-scraped datasets, regardless of size (Sun et al., 2017; Kornblith et al., 2019; Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). However, we find that improvements arising from architecture generally do not transfer to non-web-scraped tasks. Nonetheless, data augmentation and other tweaks can provide further gains on these tasks.\n\nRecommendations towards better benchmarking. While it is unclear whether researchers have over-optimized for ImageNet, our work suggests that researchers should explicitly search for methods that improve accuracy on real-world non-web-scraped datasets, rather than assuming that methods that improve accuracy on ImageNet will provide meaningful improvements on real-world datasets as well. Just as there are methods that improve accuracy on ImageNet but not on the tasks we investigate, there may be methods that improve accuracy on our tasks but not ImageNet. The Kaggle community provides some evidence for the existence of such methods; Kaggle submissions often explore architectural improvements that are less common in traditional ImageNet pre-trained models. To measure such improvements on real-world problems, we suggest simply using the average accuracy across our tasks as a benchmark for future representation learning research.\n\nFurther analysis of our results shows consistencies in the accuracies of different models across the non-web-scraped datasets, suggesting that accuracy improvements on these datasets may translate to other datasets. For each dataset, we use linear regression to predict model accuracies on the target dataset as a linear combination of ImageNet accuracy and accuracy averaged across the other real-world datasets. We perform an F-test to determine whether the average accuracy on other real-world datasets explains significant variance beyond that explained by ImageNet accuracy. We find that this F-test is significant on all datasets except EuroSAT, where accuracy may be very close to ceiling (see further analysis in Appendix N.1). Additionally, in Appendix N.2 we compare the Spearman rank correlation (i.e., the Pearson correlation between ranks) between each dataset and the accuracy averaged across the other real-world datasets to the Spearman correlation between each dataset and ImageNet. We find that the correlation with the average over real-world datasets is higher than the correlation with ImageNet and statistically significant for CCT-20, APTOS, HPA, and Cassava. Thus, there is some signal in the average accuracy across the datasets that we investigate that is not captured by ImageNet top-1 accuracy.", "md": "Differences between web-scraped datasets and real-world images We conjecture that it is possible to perform well on most, if not all, web-scraped target datasets simply by collecting a very large amount of data from the Internet and training a very large model on it. Web-scraped target datasets are by definition within the distribution of data collected from the web, and a sufficiently large model can learn that distribution. In support of this conjecture, recent models such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), ViT-G (Zhai et al., 2022), BASIC (Pham et al., 2021), and CoCa (Yu et al., 2022) are trained on very large web-scraped datasets and achieve high accuracy on a variety of web-scraped benchmarks. However, this strategy may not be effective for non-web-scraped datasets, where there is no guarantee that we will train on data that is close in distribution to the target data, even if we train on the entire web. Thus, it makes sense to distinguish these two types of datasets. There are clear differences in image distribution between the non-web-scraped datasets we consider and web-scraped datasets considered by previous work. In Figure 3 and Appendix M, we compute Fr\u00e9chet inception distance (FID) (Heusel et al., 2017) between ImageNet and each of the datasets we study in this work as well as the ones found in Kornblith et al. (2019). The real-world datasets are further away from ImageNet than those found in Kornblith et al. (2019), implying that there is a large amount of distribution shift between web-scraped datasets and real-world datasets. However, FID is only a proxy measure and may not capture all factors that lead to differences in transferability.\n\nWhereas web-scraped data is cheap to acquire, real-world data can be more expensive. Ideally, progress in computer vision architectures should improve performance not just on web-scraped data, but also on real-world tasks. Our results suggest that the latter has not happened. Gains in ImageNet accuracy over the last decade have primarily come from improving and scaling architectures, and past work has shown that these gains generally transfer to other web-scraped datasets, regardless of size (Sun et al., 2017; Kornblith et al., 2019; Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). However, we find that improvements arising from architecture generally do not transfer to non-web-scraped tasks. Nonetheless, data augmentation and other tweaks can provide further gains on these tasks.\n\nRecommendations towards better benchmarking. While it is unclear whether researchers have over-optimized for ImageNet, our work suggests that researchers should explicitly search for methods that improve accuracy on real-world non-web-scraped datasets, rather than assuming that methods that improve accuracy on ImageNet will provide meaningful improvements on real-world datasets as well. Just as there are methods that improve accuracy on ImageNet but not on the tasks we investigate, there may be methods that improve accuracy on our tasks but not ImageNet. The Kaggle community provides some evidence for the existence of such methods; Kaggle submissions often explore architectural improvements that are less common in traditional ImageNet pre-trained models. To measure such improvements on real-world problems, we suggest simply using the average accuracy across our tasks as a benchmark for future representation learning research.\n\nFurther analysis of our results shows consistencies in the accuracies of different models across the non-web-scraped datasets, suggesting that accuracy improvements on these datasets may translate to other datasets. For each dataset, we use linear regression to predict model accuracies on the target dataset as a linear combination of ImageNet accuracy and accuracy averaged across the other real-world datasets. We perform an F-test to determine whether the average accuracy on other real-world datasets explains significant variance beyond that explained by ImageNet accuracy. We find that this F-test is significant on all datasets except EuroSAT, where accuracy may be very close to ceiling (see further analysis in Appendix N.1). Additionally, in Appendix N.2 we compare the Spearman rank correlation (i.e., the Pearson correlation between ranks) between each dataset and the accuracy averaged across the other real-world datasets to the Spearman correlation between each dataset and ImageNet. We find that the correlation with the average over real-world datasets is higher than the correlation with ImageNet and statistically significant for CCT-20, APTOS, HPA, and Cassava. Thus, there is some signal in the average accuracy across the datasets that we investigate that is not captured by ImageNet top-1 accuracy."}]}, {"page": 10, "text": "Where do our findings leave ImageNet? We suspect that most of the methodological innovations\nthat help on ImageNet are useful for some real-world tasks, and in that sense it has been a successful\nbenchmark. However, the innovations that improve performance on industrial web-scraped datasets\nsuch as JFT (Sun et al., 2017) or IG-3.5B-17k (Mahajan et al., 2018) (e.g., model scaling) may be\nalmost entirely disjoint from the innovations that help with the non-web-scraped real-world tasks\nstudied here (e.g., data augmentation strategies). We hope that future benchmarks will include more\ndiverse datasets to encourage a more comprehensive approach to improving learning algorithms.\n7    ACKNOWLEDGEMENTS\nWe would like to thank Samuel Ainsworth, Sara Beery, Gabriel Ilharco, Pieter-Jan Kindermans,\nSarah Pratt, Matthew Wallingford, Ross Wightman, and Mitchell Wortsman for valuable conversa-\ntions while working on this project. We would especially like to thank Sarah Pratt for help with\nearly experimentation and brainstorming.\nWe would also like to thank Hyak computing cluster at the University of Washington and the Google\nTPU Research Cloud program for access to compute resources that allowed us to run our experi-\nments.\nThis work is in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML),\nOpen Philanthropy, Google, and the Allen Institute for AI.\nREFERENCES\nSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of\n   large scale pre-training. In International Conference on Learning Representations, 2022. URL\n   https://openreview.net/forum?id=V3C8p78sDa.\nAsia Pacific Tele-Ophthalmology Society. Aptos 2019 blindness detection, 2019. URL https:\n   //www.kaggle.com/competitions/aptos2019-blindness-detection/\n   overview.\nSara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Vittorio Ferrari,\n   Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 -\n   15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI,\n   volume 11220 of Lecture Notes in Computer Science, pp. 472\u2013489. Springer, 2018. doi: 10.1007/\n   978-3-030-01270-0\\ 28.         URL https://doi.org/10.1007/978-3-030-01270-0_\n   28.\nLucas Beyer, Olivier J H\u00b4   enaff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00a8        aron van den Oord. Are\n   we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\nKeno K Bressem, Lisa C Adams, Christoph Erxleben, Bernd Hamm, Stefan M Niehues, and Janis L\n   Vahldiek. Comparing different deep learning architectures for classification of chest radiographs.\n   Scientific reports, 10(1):1\u201316, 2020.\nKen Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the\n   details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014.\nEkin    Dogus    Cubuk,     Barret    Zoph,    Jonathon     Shlens,    and   Quoc    Le.       Randaugment:\n   Practical   automated     data    augmentation     with    a   reduced    search    space.        In   Hugo\n   Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien\n   Lin (eds.), Advances in Neural Information Processing Systems 33:                     Annual Conference\n   on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\n   virtual,  2020.       URL https://proceedings.neurips.cc/paper/2020/hash/\n   d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html.\nShubin     Dai.         A    cnn    classifier   and    a   metric    learning    model,     1st    place   so-\n   lution,       2019.                  URL        https://www.kaggle.com/competitions/\n   human-protein-atlas-image-classification/discussion/78109.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\n   erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\n   pp. 248\u2013255. Ieee, 2009.\n                                                      10", "md": "# Research Findings on ImageNet\n\n## Where do our findings leave ImageNet?\n\nWe suspect that most of the methodological innovations that help on ImageNet are useful for some real-world tasks, and in that sense it has been a successful benchmark. However, the innovations that improve performance on industrial web-scraped datasets such as JFT (Sun et al., 2017) or IG-3.5B-17k (Mahajan et al., 2018) (e.g., model scaling) may be almost entirely disjoint from the innovations that help with the non-web-scraped real-world tasks studied here (e.g., data augmentation strategies). We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.\n\n### ACKNOWLEDGEMENTS\n\nWe would like to thank Samuel Ainsworth, Sara Beery, Gabriel Ilharco, Pieter-Jan Kindermans, Sarah Pratt, Matthew Wallingford, Ross Wightman, and Mitchell Wortsman for valuable conversations while working on this project. We would especially like to thank Sarah Pratt for help with early experimentation and brainstorming.\n\nWe would also like to thank Hyak computing cluster at the University of Washington and the Google TPU Research Cloud program for access to compute resources that allowed us to run our experiments.\n\nThis work is in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML), Open Philanthropy, Google, and the Allen Institute for AI.\n\n### REFERENCES\n\n- Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of large scale pre-training. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=V3C8p78sDa.\n- Asia Pacific Tele-Ophthalmology Society. Aptos 2019 blindness detection, 2019. URL https://www.kaggle.com/competitions/aptos2019-blindness-detection/overview.\n- Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI, volume 11220 of Lecture Notes in Computer Science, pp. 472\u2013489. Springer, 2018. doi: 10.1007/978-3-030-01270-0\\ 28. URL https://doi.org/10.1007/978-3-030-01270-0_28.\n- Lucas Beyer, Olivier J H\u00b4 enaff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00a8 aron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n- Keno K Bressem, Lisa C Adams, Christoph Erxleben, Bernd Hamm, Stefan M Niehues, and Janis L Vahldiek. Comparing different deep learning architectures for classification of chest radiographs. Scientific reports, 10(1):1\u201316, 2020.\n- Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014.\n- Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html.\n- Shubin Dai. A cnn classifier and a metric learning model, 1st place solution, 2019. URL https://www.kaggle.com/competitions/human-protein-atlas-image-classification/discussion/78109.\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Findings on ImageNet", "md": "# Research Findings on ImageNet"}, {"type": "heading", "lvl": 2, "value": "Where do our findings leave ImageNet?", "md": "## Where do our findings leave ImageNet?"}, {"type": "text", "value": "We suspect that most of the methodological innovations that help on ImageNet are useful for some real-world tasks, and in that sense it has been a successful benchmark. However, the innovations that improve performance on industrial web-scraped datasets such as JFT (Sun et al., 2017) or IG-3.5B-17k (Mahajan et al., 2018) (e.g., model scaling) may be almost entirely disjoint from the innovations that help with the non-web-scraped real-world tasks studied here (e.g., data augmentation strategies). We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.", "md": "We suspect that most of the methodological innovations that help on ImageNet are useful for some real-world tasks, and in that sense it has been a successful benchmark. However, the innovations that improve performance on industrial web-scraped datasets such as JFT (Sun et al., 2017) or IG-3.5B-17k (Mahajan et al., 2018) (e.g., model scaling) may be almost entirely disjoint from the innovations that help with the non-web-scraped real-world tasks studied here (e.g., data augmentation strategies). We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms."}, {"type": "heading", "lvl": 3, "value": "ACKNOWLEDGEMENTS", "md": "### ACKNOWLEDGEMENTS"}, {"type": "text", "value": "We would like to thank Samuel Ainsworth, Sara Beery, Gabriel Ilharco, Pieter-Jan Kindermans, Sarah Pratt, Matthew Wallingford, Ross Wightman, and Mitchell Wortsman for valuable conversations while working on this project. We would especially like to thank Sarah Pratt for help with early experimentation and brainstorming.\n\nWe would also like to thank Hyak computing cluster at the University of Washington and the Google TPU Research Cloud program for access to compute resources that allowed us to run our experiments.\n\nThis work is in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML), Open Philanthropy, Google, and the Allen Institute for AI.", "md": "We would like to thank Samuel Ainsworth, Sara Beery, Gabriel Ilharco, Pieter-Jan Kindermans, Sarah Pratt, Matthew Wallingford, Ross Wightman, and Mitchell Wortsman for valuable conversations while working on this project. We would especially like to thank Sarah Pratt for help with early experimentation and brainstorming.\n\nWe would also like to thank Hyak computing cluster at the University of Washington and the Google TPU Research Cloud program for access to compute resources that allowed us to run our experiments.\n\nThis work is in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML), Open Philanthropy, Google, and the Allen Institute for AI."}, {"type": "heading", "lvl": 3, "value": "REFERENCES", "md": "### REFERENCES"}, {"type": "text", "value": "- Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of large scale pre-training. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=V3C8p78sDa.\n- Asia Pacific Tele-Ophthalmology Society. Aptos 2019 blindness detection, 2019. URL https://www.kaggle.com/competitions/aptos2019-blindness-detection/overview.\n- Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI, volume 11220 of Lecture Notes in Computer Science, pp. 472\u2013489. Springer, 2018. doi: 10.1007/978-3-030-01270-0\\ 28. URL https://doi.org/10.1007/978-3-030-01270-0_28.\n- Lucas Beyer, Olivier J H\u00b4 enaff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00a8 aron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n- Keno K Bressem, Lisa C Adams, Christoph Erxleben, Bernd Hamm, Stefan M Niehues, and Janis L Vahldiek. Comparing different deep learning architectures for classification of chest radiographs. Scientific reports, 10(1):1\u201316, 2020.\n- Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014.\n- Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html.\n- Shubin Dai. A cnn classifier and a metric learning model, 1st place solution, 2019. URL https://www.kaggle.com/competitions/human-protein-atlas-image-classification/discussion/78109.\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.", "md": "- Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of large scale pre-training. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=V3C8p78sDa.\n- Asia Pacific Tele-Ophthalmology Society. Aptos 2019 blindness detection, 2019. URL https://www.kaggle.com/competitions/aptos2019-blindness-detection/overview.\n- Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI, volume 11220 of Lecture Notes in Computer Science, pp. 472\u2013489. Springer, 2018. doi: 10.1007/978-3-030-01270-0\\ 28. URL https://doi.org/10.1007/978-3-030-01270-0_28.\n- Lucas Beyer, Olivier J H\u00b4 enaff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00a8 aron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n- Keno K Bressem, Lisa C Adams, Christoph Erxleben, Bernd Hamm, Stefan M Niehues, and Janis L Vahldiek. Comparing different deep learning architectures for classification of chest radiographs. Scientific reports, 10(1):1\u201316, 2020.\n- Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014.\n- Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html.\n- Shubin Dai. A cnn classifier and a metric learning model, 1st place solution, 2019. URL https://www.kaggle.com/competitions/human-protein-atlas-image-classification/discussion/78109.\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009."}]}, {"page": 11, "text": "Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor\n   Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Inter-\n   national conference on machine learning, pp. 647\u2013655. PMLR, 2014.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\n   Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\n   reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition\n   at scale.  In 9th International Conference on Learning Representations, ICLR 2021, Virtual\n   Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/\n   forum?id=YicbFdNTTy.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\n   Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\n   reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition\n   at scale.  In 9th International Conference on Learning Representations, ICLR 2021, Virtual\n   Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview.net/\n   forum?id=YicbFdNTTy.\nLinus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models\n   transfer? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\n   tion, pp. 5414\u20135423, 2021.\nMark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zis-\n   serman.    The pascal visual object classes (VOC) challenge.           Int. J. Comput. Vis., 88(2):\n   303\u2013338, 2010. doi: 10.1007/s11263-009-0275-4. URL https://doi.org/10.1007/\n   s11263-009-0275-4.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\n   examples: An incremental bayesian approach tested on 101 object categories. In IEEE Conference\n   on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2004, Washington,\n   DC, USA, June 27 - July 2, 2004, pp. 178. IEEE Computer Society, 2004. doi: 10.1109/CVPR.\n   2004.383. URL https://doi.org/10.1109/CVPR.2004.383.\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-\n   supervised visual representation learning. In Proceedings of the ieee/cvf International Conference\n   on computer vision, pp. 6391\u20136400, 2019.\nQishen Ha, Bo Liu, and Fuxu Liu. Identifying melanoma images using efficientnet ensemble: Win-\n   ning solution to the SIIM-ISIC melanoma classification challenge. CoRR, abs/2010.05351, 2020.\n   URL https://arxiv.org/abs/2010.05351.\nJannis Hanke. 1st place solution, 2021. URL https://www.kaggle.com/competitions/\n   cassava-leaf-disease-classification/discussion/221957.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.               Deep residual learning for image\n   recognition.   In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n   2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016. doi:\n   10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset\n   and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl.\n   Earth Obs. Remote. Sens., 12(7):2217\u20132226, 2019. doi: 10.1109/JSTARS.2019.2918242. URL\n   https://doi.org/10.1109/JSTARS.2019.2918242.\nDan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-\n   narayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In\n   8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\n   April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=\n   S1gmrxHFvB.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\n   Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Isabelle\n                                                   11", "md": "# References\n\n# References\n\n- Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. *Decaf: A deep convolutional activation feature for generic visual recognition.* In International conference on machine learning, pp. 647\u2013655. PMLR, 2014.\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-reit, and Neil Houlsby. *An image is worth 16x16 words: Transformers for image recognition at scale.* In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=YicbFdNTTy.\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-reit, and Neil Houlsby. *An image is worth 16x16 words: Transformers for image recognition at scale.* In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview.net/forum?id=YicbFdNTTy.\n- Linus Ericsson, Henry Gouk, and Timothy M Hospedales. *How well do self-supervised models transfer?* In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5414\u20135423, 2021.\n- Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. *The pascal visual object classes (VOC) challenge.* Int. J. Comput. Vis., 88(2): 303\u2013338, 2010. doi: 10.1007/s11263-009-0275-4. URL https://doi.org/10.1007/s11263-009-0275-4.\n- Li Fei-Fei, Rob Fergus, and Pietro Perona. *Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.* In IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2004, Washington, DC, USA, June 27 - July 2, 2004, pp. 178. IEEE Computer Society, 2004. doi: 10.1109/CVPR.2004.383. URL https://doi.org/10.1109/CVPR.2004.383.\n- Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. *Scaling and benchmarking self-supervised visual representation learning.* In Proceedings of the ieee/cvf International Conference on computer vision, pp. 6391\u20136400, 2019.\n- Qishen Ha, Bo Liu, and Fuxu Liu. *Identifying melanoma images using efficientnet ensemble: Winning solution to the SIIM-ISIC melanoma classification challenge.* CoRR, abs/2010.05351, 2020. URL https://arxiv.org/abs/2010.05351.\n- Jannis Hanke. *1st place solution, 2021.* URL https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957.\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. *Deep residual learning for image recognition.* In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\n- Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. *Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.* IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217\u20132226, 2019. doi: 10.1109/JSTARS.2019.2918242. URL https://doi.org/10.1109/JSTARS.2019.2918242.\n- Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. *Augmix: A simple data processing method to improve robustness and uncertainty.* In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=S1gmrxHFvB.\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. *Gans trained by a two time-scale update rule converge to a local nash equilibrium.* In Isabelle.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. *Decaf: A deep convolutional activation feature for generic visual recognition.* In International conference on machine learning, pp. 647\u2013655. PMLR, 2014.\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-reit, and Neil Houlsby. *An image is worth 16x16 words: Transformers for image recognition at scale.* In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=YicbFdNTTy.\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-reit, and Neil Houlsby. *An image is worth 16x16 words: Transformers for image recognition at scale.* In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview.net/forum?id=YicbFdNTTy.\n- Linus Ericsson, Henry Gouk, and Timothy M Hospedales. *How well do self-supervised models transfer?* In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5414\u20135423, 2021.\n- Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. *The pascal visual object classes (VOC) challenge.* Int. J. Comput. Vis., 88(2): 303\u2013338, 2010. doi: 10.1007/s11263-009-0275-4. URL https://doi.org/10.1007/s11263-009-0275-4.\n- Li Fei-Fei, Rob Fergus, and Pietro Perona. *Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.* In IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2004, Washington, DC, USA, June 27 - July 2, 2004, pp. 178. IEEE Computer Society, 2004. doi: 10.1109/CVPR.2004.383. URL https://doi.org/10.1109/CVPR.2004.383.\n- Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. *Scaling and benchmarking self-supervised visual representation learning.* In Proceedings of the ieee/cvf International Conference on computer vision, pp. 6391\u20136400, 2019.\n- Qishen Ha, Bo Liu, and Fuxu Liu. *Identifying melanoma images using efficientnet ensemble: Winning solution to the SIIM-ISIC melanoma classification challenge.* CoRR, abs/2010.05351, 2020. URL https://arxiv.org/abs/2010.05351.\n- Jannis Hanke. *1st place solution, 2021.* URL https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957.\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. *Deep residual learning for image recognition.* In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\n- Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. *Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.* IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217\u20132226, 2019. doi: 10.1109/JSTARS.2019.2918242. URL https://doi.org/10.1109/JSTARS.2019.2918242.\n- Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. *Augmix: A simple data processing method to improve robustness and uncertainty.* In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=S1gmrxHFvB.\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. *Gans trained by a two time-scale update rule converge to a local nash equilibrium.* In Isabelle.", "md": "- Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. *Decaf: A deep convolutional activation feature for generic visual recognition.* In International conference on machine learning, pp. 647\u2013655. PMLR, 2014.\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-reit, and Neil Houlsby. *An image is worth 16x16 words: Transformers for image recognition at scale.* In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=YicbFdNTTy.\n- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-reit, and Neil Houlsby. *An image is worth 16x16 words: Transformers for image recognition at scale.* In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview.net/forum?id=YicbFdNTTy.\n- Linus Ericsson, Henry Gouk, and Timothy M Hospedales. *How well do self-supervised models transfer?* In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5414\u20135423, 2021.\n- Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. *The pascal visual object classes (VOC) challenge.* Int. J. Comput. Vis., 88(2): 303\u2013338, 2010. doi: 10.1007/s11263-009-0275-4. URL https://doi.org/10.1007/s11263-009-0275-4.\n- Li Fei-Fei, Rob Fergus, and Pietro Perona. *Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.* In IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2004, Washington, DC, USA, June 27 - July 2, 2004, pp. 178. IEEE Computer Society, 2004. doi: 10.1109/CVPR.2004.383. URL https://doi.org/10.1109/CVPR.2004.383.\n- Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. *Scaling and benchmarking self-supervised visual representation learning.* In Proceedings of the ieee/cvf International Conference on computer vision, pp. 6391\u20136400, 2019.\n- Qishen Ha, Bo Liu, and Fuxu Liu. *Identifying melanoma images using efficientnet ensemble: Winning solution to the SIIM-ISIC melanoma classification challenge.* CoRR, abs/2010.05351, 2020. URL https://arxiv.org/abs/2010.05351.\n- Jannis Hanke. *1st place solution, 2021.* URL https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957.\n- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. *Deep residual learning for image recognition.* In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770\u2013778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.\n- Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. *Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.* IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217\u20132226, 2019. doi: 10.1109/JSTARS.2019.2918242. URL https://doi.org/10.1109/JSTARS.2019.2918242.\n- Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. *Augmix: A simple data processing method to improve robustness and uncertainty.* In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=S1gmrxHFvB.\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. *Gans trained by a two time-scale update rule converge to a local nash equilibrium.* In Isabelle."}]}, {"page": 12, "text": "   Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-\n   wanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30:\n   Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\n   Beach, CA, USA, pp. 6626\u20136637, 2017.         URL https://proceedings.neurips.cc/\n   paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html.\nAndrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen, Weijun\n   Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Search-\n   ing for mobilenetv3. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV\n   2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 1314\u20131324. IEEE, 2019. doi:\n   10.1109/ICCV.2019.00140. URL https://doi.org/10.1109/ICCV.2019.00140.\nGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected\n   convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition,\n   CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 2261\u20132269. IEEE Computer Society,\n   2017. doi: 10.1109/CVPR.2017.243. URL https://doi.org/10.1109/CVPR.2017.\n   243.\nMinyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer\n   learning? CoRR, abs/1608.08614, 2016. URL http://arxiv.org/abs/1608.08614.\nForrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt\n   Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size.\n   CoRR, abs/1602.07360, 2016. URL http://arxiv.org/abs/1602.07360.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\n   Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\n   with noisy text supervision. In International Conference on Machine Learning, pp. 4904\u20134916.\n   PMLR, 2021.\nAlexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y Ng, and Pranav Rajpurkar. Chextrans-\n   fer: performance and parameter efficiency of imagenet models for chest x-ray interpretation. In\n   Proceedings of the Conference on Health, Inference, and Learning, pp. 116\u2013124, 2021.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\n   and Neil Houlsby. Big transfer (bit): General visual representation learning. In European confer-\n   ence on computer vision, pp. 491\u2013507. Springer, 2020.\nSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In\n   Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661\u2013\n   2671, 2019.\nSimon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions\n   lead to less transferable features? Advances in Neural Information Processing Systems, 34, 2021.\nKlemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting\n   contrastive self-supervised representation learning pipelines. In Proceedings of the IEEE/CVF\n   International Conference on Computer Vision, pp. 9949\u20139959, 2021.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-\n   nical report, University of Toronto, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep con-\n   volutional neural networks.    In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C.\n   Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger (eds.), Advances in Neural Information Pro-\n   cessing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012.\n   Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp.\n   1106\u20131114, 2012. URL https://proceedings.neurips.cc/paper/2012/hash/\n   c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-\n   tuning can distort pretrained features and underperform out-of-distribution. In International Con-\n   ference on Learning Representations, 2022.      URL https://openreview.net/forum?\n   id=UYneFzXSJWh.\n                                                 12", "md": "- Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6626\u20136637, 2017. URL Link.\n- Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen, Weijun Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Searching for mobilenetv3. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 1314\u20131324. IEEE, 2019. doi: 10.1109/ICCV.2019.00140. URL Link.\n- Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 2261\u20132269. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.243. URL Link.\n- Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer learning? CoRR, abs/1608.08614, 2016. URL Link.\n- Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. CoRR, abs/1602.07360, 2016. URL Link.\n- Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904\u20134916. PMLR, 2021.\n- Alexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y Ng, and Pranav Rajpurkar. Chextransfer: performance and parameter efficiency of imagenet models for chest x-ray interpretation. In Proceedings of the Conference on Health, Inference, and Learning, pp. 116\u2013124, 2021.\n- Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European conference on computer vision, pp. 491\u2013507. Springer, 2020.\n- Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661\u20132671, 2019.\n- Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions lead to less transferable features? Advances in Neural Information Processing Systems, 34, 2021.\n- Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting contrastive self-supervised representation learning pipelines. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9949\u20139959, 2021.\n- Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1106\u20131114, 2012. URL Link.\n- Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. URL Link.", "images": [], "items": [{"type": "text", "value": "- Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6626\u20136637, 2017. URL Link.\n- Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen, Weijun Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Searching for mobilenetv3. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 1314\u20131324. IEEE, 2019. doi: 10.1109/ICCV.2019.00140. URL Link.\n- Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 2261\u20132269. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.243. URL Link.\n- Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer learning? CoRR, abs/1608.08614, 2016. URL Link.\n- Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. CoRR, abs/1602.07360, 2016. URL Link.\n- Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904\u20134916. PMLR, 2021.\n- Alexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y Ng, and Pranav Rajpurkar. Chextransfer: performance and parameter efficiency of imagenet models for chest x-ray interpretation. In Proceedings of the Conference on Health, Inference, and Learning, pp. 116\u2013124, 2021.\n- Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European conference on computer vision, pp. 491\u2013507. Springer, 2020.\n- Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661\u20132671, 2019.\n- Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions lead to less transferable features? Advances in Neural Information Processing Systems, 34, 2021.\n- Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting contrastive self-supervised representation learning pipelines. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9949\u20139959, 2021.\n- Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1106\u20131114, 2012. URL Link.\n- Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. URL Link.", "md": "- Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6626\u20136637, 2017. URL Link.\n- Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen, Weijun Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Searching for mobilenetv3. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 1314\u20131324. IEEE, 2019. doi: 10.1109/ICCV.2019.00140. URL Link.\n- Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 2261\u20132269. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.243. URL Link.\n- Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer learning? CoRR, abs/1608.08614, 2016. URL Link.\n- Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. CoRR, abs/1602.07360, 2016. URL Link.\n- Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904\u20134916. PMLR, 2021.\n- Alexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y Ng, and Pranav Rajpurkar. Chextransfer: performance and parameter efficiency of imagenet models for chest x-ray interpretation. In Proceedings of the Conference on Health, Inference, and Learning, pp. 116\u2013124, 2021.\n- Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European conference on computer vision, pp. 491\u2013507. Springer, 2020.\n- Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661\u20132671, 2019.\n- Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions lead to less transferable features? Advances in Neural Information Processing Systems, 34, 2021.\n- Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting contrastive self-supervised representation learning pipelines. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9949\u20139959, 2021.\n- Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L\u00b4eon Bottou, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1106\u20131114, 2012. URL Link.\n- Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. URL Link."}]}, {"page": 13, "text": "Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L.\n   Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Vittorio\n   Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV\n   2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part\n   I, volume 11205 of Lecture Notes in Computer Science, pp. 19\u201335. Springer, 2018. doi: 10.1007/\n   978-3-030-01246-5\\ 2. URL https://doi.org/10.1007/978-3-030-01246-5_2.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\n   A convnet for the 2020s. CoRR, abs/2201.03545, 2022. URL https://arxiv.org/abs/\n   2201.03545.\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet V2: practical guidelines\n   for efficient CNN architecture design. In Vittorio Ferrari, Martial Hebert, Cristian Sminchis-\n   escu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Mu-\n   nich, Germany, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes\n   in Computer Science, pp. 122\u2013138. Springer, 2018. doi: 10.1007/978-3-030-01264-9\\ 8. URL\n   https://doi.org/10.1007/978-3-030-01264-9_8.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\n   Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised\n   pretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181\u2013\n   196, 2018.\nMakerere University AI Lab.       Cassava leaf disease classification, 2021.     URL https://\n   www.kaggle.com/competitions/cassava-leaf-disease-classification/\n   overview.\nThomas Mensink, Jasper Uijlings, Alina Kuznetsova, Michael Gygli, and Vittorio Ferrari. Factors\n   of influence for transfer learning across diverse appearance domains and task types. IEEE Trans-\n   actions on Pattern Analysis and Machine Intelligence, pp. 1\u20131, 2021. doi: 10.1109/TPAMI.2021.\n   3129870.\nJohn Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,\n   Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correla-\n   tion between out-of-distribution and in-distribution generalization. In Marina Meila and Tong\n   Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML\n   2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Re-\n   search, pp. 7721\u20137735. PMLR, 2021. URL http://proceedings.mlr.press/v139/\n   miller21b.html.\nRaoof Naushad, Tarunpreet Kaur, and Ebrahim Ghaderpour. Deep transfer learning for land use\n   and land cover classification: A comparative study. Sensors, 21(23):8083, 2021. doi: 10.3390/\n   s21238083. URL https://doi.org/10.3390/s21238083.\nNiv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse imagenet models\n   transfer better. arXiv preprint arXiv:2204.09134, 2022.\nWei Ouyang, Casper F. Winsnes, Martin Hjelmare, Anthony J. Cesnik, Lovisa \u02da      Akesson, Hao Xu,\n   Devin P. Sullivan, Shubin Dai, Jun Lan, Park Jinmo, Shaikat M. Galib, Christof Henkel, Kevin\n   Hwang, Dmytro Poplavskiy, Bojan Tunguz, Russel D. Wolfinger, Yinzheng Gu, Chuanpeng Li,\n   Jinbin Xie, Dmitry Buslov, Sergei Fironov, Alexander Kiselev, Dmytro Panchenko, Xuan Cao,\n   Runmin Wei, Yuanhao Wu, Xun Zhu, Kuan-Lun Tseng, Zhifeng Gao, Cheng Ju, Xiaohan Yi,\n   Hongdong Zheng, Constantin Kappel, and Emma Lundberg. Analysis of the human protein at-\n   las image classification competition. Nature Methods, 16(12):1254\u20131261, 2019. doi: 10.1038/\n   s41592-019-0658-6. URL https://doi.org/10.1038/s41592-019-0658-6.\nIan Pan.      [2nd place] solution overview, 2020.        URL https://www.kaggle.com/\n   competitions/siim-isic-melanoma-classification/discussion/\n   175324.\nJinmo Park.    3rd place solution with code., 2019.     URL https://www.kaggle.com/c/\n   human-protein-atlas-image-classification/discussion/77320.\n                                                 13", "md": "# References\n\n# References\n\nChenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L.\nYuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Vittorio\nFerrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV\n2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part\nI, volume 11205 of Lecture Notes in Computer Science, pp. 19\u201335. Springer, 2018. doi: 10.1007/\n978-3-030-01246-5\\ 2. URL https://doi.org/10.1007/978-3-030-01246-5_2.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\nA convnet for the 2020s. CoRR, abs/2201.03545, 2022. URL https://arxiv.org/abs/2201.03545.\n\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet V2: practical guidelines\nfor efficient CNN architecture design. In Vittorio Ferrari, Martial Hebert, Cristian Sminchis-\nescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Mu-\nnich, Germany, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes\nin Computer Science, pp. 122\u2013138. Springer, 2018. doi: 10.1007/978-3-030-01264-9\\ 8. URL\nhttps://doi.org/10.1007/978-3-030-01264-9_8.\n\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised\npretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181\u2013\n196, 2018.\n\nMakerere University AI Lab. Cassava leaf disease classification, 2021. URL https://www.kaggle.com/competitions/cassava-leaf-disease-classification/overview.\n\nThomas Mensink, Jasper Uijlings, Alina Kuznetsova, Michael Gygli, and Vittorio Ferrari. Factors\nof influence for transfer learning across diverse appearance domains and task types. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u20131, 2021. doi: 10.1109/TPAMI.2021.\n3129870.\n\nJohn Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,\nPercy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7721\u20137735. PMLR, 2021. URL http://proceedings.mlr.press/v139/miller21b.html.\n\nRaoof Naushad, Tarunpreet Kaur, and Ebrahim Ghaderpour. Deep transfer learning for land use\nand land cover classification: A comparative study. Sensors, 21(23):8083, 2021. doi: 10.3390/\ns21238083. URL https://doi.org/10.3390/s21238083.\n\nNiv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse imagenet models\ntransfer better. arXiv preprint arXiv:2204.09134, 2022.\n\nWei Ouyang, Casper F. Winsnes, Martin Hjelmare, Anthony J. Cesnik, Lovisa \u02da Akesson, Hao Xu,\nDevin P. Sullivan, Shubin Dai, Jun Lan, Park Jinmo, Shaikat M. Galib, Christof Henkel, Kevin\nHwang, Dmytro Poplavskiy, Bojan Tunguz, Russel D. Wolfinger, Yinzheng Gu, Chuanpeng Li,\nJinbin Xie, Dmitry Buslov, Sergei Fironov, Alexander Kiselev, Dmytro Panchenko, Xuan Cao,\nRunmin Wei, Yuanhao Wu, Xun Zhu, Kuan-Lun Tseng, Zhifeng Gao, Cheng Ju, Xiaohan Yi,\nHongdong Zheng, Constantin Kappel, and Emma Lundberg. Analysis of the human protein atlas image classification competition. Nature Methods, 16(12):1254\u20131261, 2019. doi: 10.1038/\ns41592-019-0658-6. URL https://doi.org/10.1038/s41592-019-0658-6.\n\nIan Pan. [2nd place] solution overview, 2020. URL https://www.kaggle.com/competitions/siim-isic-melanoma-classification/discussion/175324.\n\nJinmo Park. 3rd place solution with code., 2019. URL https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77320.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L.\nYuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Vittorio\nFerrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV\n2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part\nI, volume 11205 of Lecture Notes in Computer Science, pp. 19\u201335. Springer, 2018. doi: 10.1007/\n978-3-030-01246-5\\ 2. URL https://doi.org/10.1007/978-3-030-01246-5_2.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\nA convnet for the 2020s. CoRR, abs/2201.03545, 2022. URL https://arxiv.org/abs/2201.03545.\n\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet V2: practical guidelines\nfor efficient CNN architecture design. In Vittorio Ferrari, Martial Hebert, Cristian Sminchis-\nescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Mu-\nnich, Germany, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes\nin Computer Science, pp. 122\u2013138. Springer, 2018. doi: 10.1007/978-3-030-01264-9\\ 8. URL\nhttps://doi.org/10.1007/978-3-030-01264-9_8.\n\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised\npretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181\u2013\n196, 2018.\n\nMakerere University AI Lab. Cassava leaf disease classification, 2021. URL https://www.kaggle.com/competitions/cassava-leaf-disease-classification/overview.\n\nThomas Mensink, Jasper Uijlings, Alina Kuznetsova, Michael Gygli, and Vittorio Ferrari. Factors\nof influence for transfer learning across diverse appearance domains and task types. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u20131, 2021. doi: 10.1109/TPAMI.2021.\n3129870.\n\nJohn Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,\nPercy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7721\u20137735. PMLR, 2021. URL http://proceedings.mlr.press/v139/miller21b.html.\n\nRaoof Naushad, Tarunpreet Kaur, and Ebrahim Ghaderpour. Deep transfer learning for land use\nand land cover classification: A comparative study. Sensors, 21(23):8083, 2021. doi: 10.3390/\ns21238083. URL https://doi.org/10.3390/s21238083.\n\nNiv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse imagenet models\ntransfer better. arXiv preprint arXiv:2204.09134, 2022.\n\nWei Ouyang, Casper F. Winsnes, Martin Hjelmare, Anthony J. Cesnik, Lovisa \u02da Akesson, Hao Xu,\nDevin P. Sullivan, Shubin Dai, Jun Lan, Park Jinmo, Shaikat M. Galib, Christof Henkel, Kevin\nHwang, Dmytro Poplavskiy, Bojan Tunguz, Russel D. Wolfinger, Yinzheng Gu, Chuanpeng Li,\nJinbin Xie, Dmitry Buslov, Sergei Fironov, Alexander Kiselev, Dmytro Panchenko, Xuan Cao,\nRunmin Wei, Yuanhao Wu, Xun Zhu, Kuan-Lun Tseng, Zhifeng Gao, Cheng Ju, Xiaohan Yi,\nHongdong Zheng, Constantin Kappel, and Emma Lundberg. Analysis of the human protein atlas image classification competition. Nature Methods, 16(12):1254\u20131261, 2019. doi: 10.1038/\ns41592-019-0658-6. URL https://doi.org/10.1038/s41592-019-0658-6.\n\nIan Pan. [2nd place] solution overview, 2020. URL https://www.kaggle.com/competitions/siim-isic-melanoma-classification/discussion/175324.\n\nJinmo Park. 3rd place solution with code., 2019. URL https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77320.", "md": "Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L.\nYuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Vittorio\nFerrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV\n2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part\nI, volume 11205 of Lecture Notes in Computer Science, pp. 19\u201335. Springer, 2018. doi: 10.1007/\n978-3-030-01246-5\\ 2. URL https://doi.org/10.1007/978-3-030-01246-5_2.\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\nA convnet for the 2020s. CoRR, abs/2201.03545, 2022. URL https://arxiv.org/abs/2201.03545.\n\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet V2: practical guidelines\nfor efficient CNN architecture design. In Vittorio Ferrari, Martial Hebert, Cristian Sminchis-\nescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Mu-\nnich, Germany, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes\nin Computer Science, pp. 122\u2013138. Springer, 2018. doi: 10.1007/978-3-030-01264-9\\ 8. URL\nhttps://doi.org/10.1007/978-3-030-01264-9_8.\n\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised\npretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181\u2013\n196, 2018.\n\nMakerere University AI Lab. Cassava leaf disease classification, 2021. URL https://www.kaggle.com/competitions/cassava-leaf-disease-classification/overview.\n\nThomas Mensink, Jasper Uijlings, Alina Kuznetsova, Michael Gygli, and Vittorio Ferrari. Factors\nof influence for transfer learning across diverse appearance domains and task types. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u20131, 2021. doi: 10.1109/TPAMI.2021.\n3129870.\n\nJohn Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,\nPercy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7721\u20137735. PMLR, 2021. URL http://proceedings.mlr.press/v139/miller21b.html.\n\nRaoof Naushad, Tarunpreet Kaur, and Ebrahim Ghaderpour. Deep transfer learning for land use\nand land cover classification: A comparative study. Sensors, 21(23):8083, 2021. doi: 10.3390/\ns21238083. URL https://doi.org/10.3390/s21238083.\n\nNiv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse imagenet models\ntransfer better. arXiv preprint arXiv:2204.09134, 2022.\n\nWei Ouyang, Casper F. Winsnes, Martin Hjelmare, Anthony J. Cesnik, Lovisa \u02da Akesson, Hao Xu,\nDevin P. Sullivan, Shubin Dai, Jun Lan, Park Jinmo, Shaikat M. Galib, Christof Henkel, Kevin\nHwang, Dmytro Poplavskiy, Bojan Tunguz, Russel D. Wolfinger, Yinzheng Gu, Chuanpeng Li,\nJinbin Xie, Dmitry Buslov, Sergei Fironov, Alexander Kiselev, Dmytro Panchenko, Xuan Cao,\nRunmin Wei, Yuanhao Wu, Xun Zhu, Kuan-Lun Tseng, Zhifeng Gao, Cheng Ju, Xiaohan Yi,\nHongdong Zheng, Constantin Kappel, and Emma Lundberg. Analysis of the human protein atlas image classification competition. Nature Methods, 16(12):1254\u20131261, 2019. doi: 10.1038/\ns41592-019-0658-6. URL https://doi.org/10.1038/s41592-019-0658-6.\n\nIan Pan. [2nd place] solution overview, 2020. URL https://www.kaggle.com/competitions/siim-isic-melanoma-classification/discussion/175324.\n\nJinmo Park. 3rd place solution with code., 2019. URL https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77320."}]}, {"page": 14, "text": "Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingx-\n   ing Tan, and Quoc V Le.           Combined scaling for zero-shot transfer learning.              arXiv preprint\n   arXiv:2111.10050, 2021.\nJean Ponce, Tamara L Berg, Mark Everingham, David A Forsyth, Martial Hebert, Svetlana Lazeb-\n   nik, Marcin Marszalek, Cordelia Schmid, Bryan C Russell, Antonio Torralba, et al. Dataset issues\n   in object recognition. In Toward category-level object recognition, pp. 29\u201348. Springer, 2006.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n   Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n   models from natural language supervision. In International Conference on Machine Learning,\n   pp. 8748\u20138763. PMLR, 2021.\nMaithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding\n   transfer learning for medical imaging. Advances in neural information processing systems, 32,\n   2019.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers\n   generalize to imagenet?         In International Conference on Machine Learning, pp. 5389\u20135400.\n   PMLR, 2019.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\n   Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\n   recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.\nHadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adver-\n   sarially robust imagenet models transfer better?             Advances in Neural Information Processing\n   Systems, 33:3533\u20133545, 2020.\nVaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, and Ludwig\n   Schmidt. Evaluating machine accuracy on imagenet. In International Conference on Machine\n   Learning, pp. 8634\u20138644. PMLR, 2020.\nAli Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-\n   the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on\n   computer vision and pattern recognition workshops, pp. 806\u2013813, 2014.\nMaxim         Shugaev.                    Pretrained       resnet34        with       rgby       (0.460       public\n   lb),         2019.                      URL         https://www.kaggle.com/code/iafoss/\n   pretrained-resnet34-with-rgby-0-460-public-lb/notebook.\nSIIM and ISIC. Siim-isic melanoma classification, 2020. URL https://www.kaggle.com/\n   competitions/siim-isic-melanoma-classification/overview.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n   recognition. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning\n   Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-\n   ings, 2015. URL http://arxiv.org/abs/1409.1556.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas\n   Beyer. How to train your vit? data, augmentation, and regularization in vision transformers.\n   CoRR, abs/2106.10270, 2021. URL https://arxiv.org/abs/2106.10270.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\n   fectiveness of data in deep learning era. In Proceedings of the IEEE international conference on\n   computer vision, pp. 843\u2013852, 2017.\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi.                          Inception-v4,\n   inception-resnet and the impact of residual connections on learning. In Satinder Singh and Shaul\n   Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,\n   February 4-9, 2017, San Francisco, California, USA, pp. 4278\u20134284. AAAI Press, 2017a. URL\n   http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.\n                                                         14", "md": "# References\n\n## References\n\n- Pham, H., Dai, Z., Ghiasi, G., Liu, H., Yu, A. W., Luong, M.-T., Tan, M., & Le, Q. V. (2021). Combined scaling for zero-shot transfer learning. *arXiv preprint*, arXiv:2111.10050.\n- Ponce, J., Berg, T. L., Everingham, M., Forsyth, D. A., Hebert, M., Lazebnik, S., Marszalek, M., Schmid, C., Russell, B. C., Torralba, A., et al. (2006). Dataset issues in object recognition. In *Toward category-level object recognition*, pp. 29\u201348. Springer.\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In *International Conference on Machine Learning*, pp. 8748\u20138763. PMLR.\n- Raghu, M., Zhang, C., Kleinberg, J., & Bengio, S. (2019). Transfusion: Understanding transfer learning for medical imaging. *Advances in neural information processing systems*, 32.\n- Recht, B., Roelofs, R., Schmidt, L., & Shankar, V. (2019). Do imagenet classifiers generalize to imagenet? In *International Conference on Machine Learning*, pp. 5389\u20135400. PMLR.\n- Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015). Imagenet large scale visual recognition challenge. *International journal of computer vision*, 115(3), 211\u2013252.\n- Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., & Madry, A. (2020). Do adversarially robust imagenet models transfer better? *Advances in Neural Information Processing Systems*, 33, 3533\u20133545.\n- Shankar, V., Roelofs, R., Mania, H., Fang, A., Recht, B., & Schmidt, L. (2020). Evaluating machine accuracy on imagenet. In *International Conference on Machine Learning*, pp. 8634\u20138644. PMLR.\n- Razavian, A. S., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition workshops*, pp. 806\u2013813.\n- Shugaev, M. (2019). Pretrained resnet34 with rgby (0.460 public lb). URL: https://www.kaggle.com/code/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb/notebook.\n- SIIM and ISIC. (2020). Siim-isic melanoma classification. URL: https://www.kaggle.com/competitions/siim-isic-melanoma-classification/overview.\n- Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. In Bengio, Y., & LeCun, Y. (Eds.), *3rd International Conference on Learning Representations, ICLR 2015*, Conference Track Proceedings. URL: http://arxiv.org/abs/1409.1556.\n- Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., & Beyer, L. (2021). How to train your vit? data, augmentation, and regularization in vision transformers. *CoRR*, abs/2106.10270. URL: https://arxiv.org/abs/2106.10270.\n- Sun, C., Shrivastava, A., Singh, S., & Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. In *Proceedings of the IEEE international conference on computer vision*, pp. 843\u2013852.\n- Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. In Singh, S., & Markovitch, S. (Eds.), *Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence*, pp. 4278\u20134284. AAAI Press. URL: http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- Pham, H., Dai, Z., Ghiasi, G., Liu, H., Yu, A. W., Luong, M.-T., Tan, M., & Le, Q. V. (2021). Combined scaling for zero-shot transfer learning. *arXiv preprint*, arXiv:2111.10050.\n- Ponce, J., Berg, T. L., Everingham, M., Forsyth, D. A., Hebert, M., Lazebnik, S., Marszalek, M., Schmid, C., Russell, B. C., Torralba, A., et al. (2006). Dataset issues in object recognition. In *Toward category-level object recognition*, pp. 29\u201348. Springer.\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In *International Conference on Machine Learning*, pp. 8748\u20138763. PMLR.\n- Raghu, M., Zhang, C., Kleinberg, J., & Bengio, S. (2019). Transfusion: Understanding transfer learning for medical imaging. *Advances in neural information processing systems*, 32.\n- Recht, B., Roelofs, R., Schmidt, L., & Shankar, V. (2019). Do imagenet classifiers generalize to imagenet? In *International Conference on Machine Learning*, pp. 5389\u20135400. PMLR.\n- Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015). Imagenet large scale visual recognition challenge. *International journal of computer vision*, 115(3), 211\u2013252.\n- Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., & Madry, A. (2020). Do adversarially robust imagenet models transfer better? *Advances in Neural Information Processing Systems*, 33, 3533\u20133545.\n- Shankar, V., Roelofs, R., Mania, H., Fang, A., Recht, B., & Schmidt, L. (2020). Evaluating machine accuracy on imagenet. In *International Conference on Machine Learning*, pp. 8634\u20138644. PMLR.\n- Razavian, A. S., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition workshops*, pp. 806\u2013813.\n- Shugaev, M. (2019). Pretrained resnet34 with rgby (0.460 public lb). URL: https://www.kaggle.com/code/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb/notebook.\n- SIIM and ISIC. (2020). Siim-isic melanoma classification. URL: https://www.kaggle.com/competitions/siim-isic-melanoma-classification/overview.\n- Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. In Bengio, Y., & LeCun, Y. (Eds.), *3rd International Conference on Learning Representations, ICLR 2015*, Conference Track Proceedings. URL: http://arxiv.org/abs/1409.1556.\n- Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., & Beyer, L. (2021). How to train your vit? data, augmentation, and regularization in vision transformers. *CoRR*, abs/2106.10270. URL: https://arxiv.org/abs/2106.10270.\n- Sun, C., Shrivastava, A., Singh, S., & Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. In *Proceedings of the IEEE international conference on computer vision*, pp. 843\u2013852.\n- Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. In Singh, S., & Markovitch, S. (Eds.), *Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence*, pp. 4278\u20134284. AAAI Press. URL: http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.", "md": "- Pham, H., Dai, Z., Ghiasi, G., Liu, H., Yu, A. W., Luong, M.-T., Tan, M., & Le, Q. V. (2021). Combined scaling for zero-shot transfer learning. *arXiv preprint*, arXiv:2111.10050.\n- Ponce, J., Berg, T. L., Everingham, M., Forsyth, D. A., Hebert, M., Lazebnik, S., Marszalek, M., Schmid, C., Russell, B. C., Torralba, A., et al. (2006). Dataset issues in object recognition. In *Toward category-level object recognition*, pp. 29\u201348. Springer.\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In *International Conference on Machine Learning*, pp. 8748\u20138763. PMLR.\n- Raghu, M., Zhang, C., Kleinberg, J., & Bengio, S. (2019). Transfusion: Understanding transfer learning for medical imaging. *Advances in neural information processing systems*, 32.\n- Recht, B., Roelofs, R., Schmidt, L., & Shankar, V. (2019). Do imagenet classifiers generalize to imagenet? In *International Conference on Machine Learning*, pp. 5389\u20135400. PMLR.\n- Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015). Imagenet large scale visual recognition challenge. *International journal of computer vision*, 115(3), 211\u2013252.\n- Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., & Madry, A. (2020). Do adversarially robust imagenet models transfer better? *Advances in Neural Information Processing Systems*, 33, 3533\u20133545.\n- Shankar, V., Roelofs, R., Mania, H., Fang, A., Recht, B., & Schmidt, L. (2020). Evaluating machine accuracy on imagenet. In *International Conference on Machine Learning*, pp. 8634\u20138644. PMLR.\n- Razavian, A. S., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition workshops*, pp. 806\u2013813.\n- Shugaev, M. (2019). Pretrained resnet34 with rgby (0.460 public lb). URL: https://www.kaggle.com/code/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb/notebook.\n- SIIM and ISIC. (2020). Siim-isic melanoma classification. URL: https://www.kaggle.com/competitions/siim-isic-melanoma-classification/overview.\n- Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. In Bengio, Y., & LeCun, Y. (Eds.), *3rd International Conference on Learning Representations, ICLR 2015*, Conference Track Proceedings. URL: http://arxiv.org/abs/1409.1556.\n- Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., & Beyer, L. (2021). How to train your vit? data, augmentation, and regularization in vision transformers. *CoRR*, abs/2106.10270. URL: https://arxiv.org/abs/2106.10270.\n- Sun, C., Shrivastava, A., Singh, S., & Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. In *Proceedings of the IEEE international conference on computer vision*, pp. 843\u2013852.\n- Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. In Singh, S., & Markovitch, S. (Eds.), *Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence*, pp. 4278\u20134284. AAAI Press. URL: http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806."}]}, {"page": 15, "text": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi.                 Inception-v4,\n  inception-resnet and the impact of residual connections on learning. In Satinder Singh and Shaul\n  Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,\n  February 4-9, 2017, San Francisco, California, USA, pp. 4278\u20134284. AAAI Press, 2017b. URL\n  http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.\nMingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural\n  networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th\n  International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-\n  fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 6105\u20136114. PMLR,\n  2019. URL http://proceedings.mlr.press/v97/tan19a.html.\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig\n  Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances\n  in Neural Information Processing Systems, 33:18583\u201318599, 2020.\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521\u20131528.\n  IEEE, 2011.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\n  Herv\u00b4 e J\u00b4\n           egou. Training data-efficient image transformers & distillation through attention. In Ma-\n  rina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine\n  Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine\n  Learning Research, pp. 10347\u201310357. PMLR, 2021. URL http://proceedings.mlr.\n  press/v139/touvron21a.html.\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. From\n  imagenet to image classification: Contextualizing progress on benchmarks. In International Con-\n  ference on Machine Learning, pp. 9625\u20139635. PMLR, 2020.\nLukas Tuggener, J\u00a8 urgen Schmidhuber, and Thilo Stadelmann. Is it enough to optimize cnn architec-\n  tures on imagenet? arXiv preprint arXiv:2103.09108, 2021.\nRoss Wightman, Hugo Touvron, and Herv\u00b4         e J\u00b4egou.   Resnet strikes back: An improved training\n  procedure in timm. CoRR, abs/2110.00476, 2021. URL https://arxiv.org/abs/2110.\n  00476.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student\n  improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision\n  and pattern recognition, pp. 10687\u201310698, 2020.\nSaining Xie, Ross B. Girshick, Piotr Doll\u00b4     ar, Zhuowen Tu, and Kaiming He. Aggregated resid-\n  ual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision\n  and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 5987\u20135995.\n  IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.634. URL https://doi.org/10.\n  1109/CVPR.2017.634.\nGuanshuo Xu.       1st place solution summary, 2019.           URL https://www.kaggle.com/\n  competitions/aptos2019-blindness-detection/discussion/108065.\nH. Yassine, K. Tout, and M. Jaber.        Improving Lulc Classification from Satellite Imagery Us-\n  ing Deep Learning - Eurosat Dataset.           ISPRS - International Archives of the Photogram-\n  metry, Remote Sensing and Spatial Information Sciences, 43B3:369\u2013376, June 2021.                     doi:\n  10.5194/isprs-archives-XLIII-B3-2021-369-2021.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui\n  Wu.      Coca:    Contrastive captioners are image-text foundation models.               arXiv preprint\n  arXiv:2205.01917, 2022.\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\n  Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE con-\n  ference on computer vision and pattern recognition, pp. 3712\u20133722, 2018.\n                                                    15", "md": "# References\n\n# References\n\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Satinder Singh and Shaul Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pp. 4278\u20134284. AAAI Press, 2017b. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.\n\nMingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 6105\u20136114. PMLR, 2019. URL http://proceedings.mlr.press/v97/tan19a.html.\n\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583\u201318599, 2020.\n\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521\u20131528. IEEE, 2011.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00b4 e J\u00b4 egou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 10347\u201310357. PMLR, 2021. URL http://proceedings.mlr.press/v139/touvron21a.html.\n\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. From imagenet to image classification: Contextualizing progress on benchmarks. In International Conference on Machine Learning, pp. 9625\u20139635. PMLR, 2020.\n\nLukas Tuggener, J\u00a8 urgen Schmidhuber, and Thilo Stadelmann. Is it enough to optimize cnn architectures on imagenet? arXiv preprint arXiv:2103.09108, 2021.\n\nRoss Wightman, Hugo Touvron, and Herv\u00b4 e J\u00b4egou. Resnet strikes back: An improved training procedure in timm. CoRR, abs/2110.00476, 2021. URL https://arxiv.org/abs/2110.00476.\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10687\u201310698, 2020.\n\nSaining Xie, Ross B. Girshick, Piotr Doll\u00b4 ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 5987\u20135995. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.634. URL https://doi.org/10.1109/CVPR.2017.634.\n\nGuanshuo Xu. 1st place solution summary, 2019. URL https://www.kaggle.com/competitions/aptos2019-blindness-detection/discussion/108065.\n\nH. Yassine, K. Tout, and M. Jaber. Improving Lulc Classification from Satellite Imagery Using Deep Learning - Eurosat Dataset. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 43B3:369\u2013376, June 2021. doi: 10.5194/isprs-archives-XLIII-B3-2021-369-2021.\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\n\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712\u20133722, 2018.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Satinder Singh and Shaul Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pp. 4278\u20134284. AAAI Press, 2017b. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.\n\nMingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 6105\u20136114. PMLR, 2019. URL http://proceedings.mlr.press/v97/tan19a.html.\n\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583\u201318599, 2020.\n\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521\u20131528. IEEE, 2011.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00b4 e J\u00b4 egou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 10347\u201310357. PMLR, 2021. URL http://proceedings.mlr.press/v139/touvron21a.html.\n\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. From imagenet to image classification: Contextualizing progress on benchmarks. In International Conference on Machine Learning, pp. 9625\u20139635. PMLR, 2020.\n\nLukas Tuggener, J\u00a8 urgen Schmidhuber, and Thilo Stadelmann. Is it enough to optimize cnn architectures on imagenet? arXiv preprint arXiv:2103.09108, 2021.\n\nRoss Wightman, Hugo Touvron, and Herv\u00b4 e J\u00b4egou. Resnet strikes back: An improved training procedure in timm. CoRR, abs/2110.00476, 2021. URL https://arxiv.org/abs/2110.00476.\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10687\u201310698, 2020.\n\nSaining Xie, Ross B. Girshick, Piotr Doll\u00b4 ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 5987\u20135995. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.634. URL https://doi.org/10.1109/CVPR.2017.634.\n\nGuanshuo Xu. 1st place solution summary, 2019. URL https://www.kaggle.com/competitions/aptos2019-blindness-detection/discussion/108065.\n\nH. Yassine, K. Tout, and M. Jaber. Improving Lulc Classification from Satellite Imagery Using Deep Learning - Eurosat Dataset. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 43B3:369\u2013376, June 2021. doi: 10.5194/isprs-archives-XLIII-B3-2021-369-2021.\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\n\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712\u20133722, 2018.", "md": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Satinder Singh and Shaul Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pp. 4278\u20134284. AAAI Press, 2017b. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.\n\nMingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 6105\u20136114. PMLR, 2019. URL http://proceedings.mlr.press/v97/tan19a.html.\n\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583\u201318599, 2020.\n\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521\u20131528. IEEE, 2011.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00b4 e J\u00b4 egou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 10347\u201310357. PMLR, 2021. URL http://proceedings.mlr.press/v139/touvron21a.html.\n\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. From imagenet to image classification: Contextualizing progress on benchmarks. In International Conference on Machine Learning, pp. 9625\u20139635. PMLR, 2020.\n\nLukas Tuggener, J\u00a8 urgen Schmidhuber, and Thilo Stadelmann. Is it enough to optimize cnn architectures on imagenet? arXiv preprint arXiv:2103.09108, 2021.\n\nRoss Wightman, Hugo Touvron, and Herv\u00b4 e J\u00b4egou. Resnet strikes back: An improved training procedure in timm. CoRR, abs/2110.00476, 2021. URL https://arxiv.org/abs/2110.00476.\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10687\u201310698, 2020.\n\nSaining Xie, Ross B. Girshick, Piotr Doll\u00b4 ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 5987\u20135995. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.634. URL https://doi.org/10.1109/CVPR.2017.634.\n\nGuanshuo Xu. 1st place solution summary, 2019. URL https://www.kaggle.com/competitions/aptos2019-blindness-detection/discussion/108065.\n\nH. Yassine, K. Tout, and M. Jaber. Improving Lulc Classification from Satellite Imagery Using Deep Learning - Eurosat Dataset. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 43B3:369\u2013376, June 2021. doi: 10.5194/isprs-archives-XLIII-B3-2021-369-2021.\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\n\nAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712\u20133722, 2018."}]}, {"page": 16, "text": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\n   Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The\n   visual task adaptation benchmark. 2019.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\n   In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n   12104\u201312113, 2022.\nKevin     Zheng.           39th    solution-attention    gated    resnet18    (single    model    with-\n   out    cv),     2019.              URL      https://www.kaggle.com/competitions/\n   human-protein-atlas-image-classification/discussion/77637.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data aug-\n   mentation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The\n   Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The\n   Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New\n   York, NY, USA, February 7-12, 2020, pp. 13001\u201313008. AAAI Press, 2020.               URL https:\n   //ojs.aaai.org/index.php/AAAI/article/view/7000.\n                                                   16", "md": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation benchmark. 2019.\n\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104\u201312113, 2022.\n\nKevin Zheng. 39th solution-attention gated resnet18 (single model without cv), 2019. URL https://www.kaggle.com/competitions/human-protein-atlas-image-classification/discussion/77637.\n\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 13001\u201313008. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/7000.", "images": [], "items": [{"type": "text", "value": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation benchmark. 2019.\n\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104\u201312113, 2022.\n\nKevin Zheng. 39th solution-attention gated resnet18 (single model without cv), 2019. URL https://www.kaggle.com/competitions/human-protein-atlas-image-classification/discussion/77637.\n\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 13001\u201313008. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/7000.", "md": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation benchmark. 2019.\n\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104\u201312113, 2022.\n\nKevin Zheng. 39th solution-attention gated resnet18 (single model without cv), 2019. URL https://www.kaggle.com/competitions/human-protein-atlas-image-classification/discussion/77637.\n\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 13001\u201313008. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/7000."}]}, {"page": 17, "text": "Appendix\nA     DETAILED EXPERIMENT RESULTS\nTable 3: For each ImageNet pre-trained model, we provide the best performing model when fine-tuned on each\ndataset across our hyperparameter grid\n  Model                     ImageNet top-1      CCT20      APTOS        HPA       Melanoma       Cassava    EuroSAT\n  AlexNet                         56.5           63.59      0.8835     0.3846       0.9283        82.58       97.93\n  SqueezeNet 1.1                  58.2           66.36      0.9021     0.3972       0.9073        85.15       98.07\n  ShuffleNetV2x0.5                60.6           66.37      0.9227     0.5867       0.9289        85.64       98.56\n  MobileNet V3 small              67.7           66.01      0.9230     0.6108       0.9455        85.81       99.15\n  ShuffleNetV2x1.0                69.4           69.27      0.9202     0.6202       0.9418        87.33       98.91\n  VGG-13 BN                       71.6           75.06      0.9268     0.6794       0.9529        88.99       98.85\n  DeiT-tiny                       72.2           68.77      0.9130     0.5777       0.9510        86.25       99.11\n  VGG-16 BN                       73.4           75.93      0.9287     0.6791       0.9531        88.45       98.93\n  DenseNet-121                    74.4           74.66      0.9287     0.7019       0.9514        87.80       99.06\n  ResNet-50                       76.1           73.96      0.9215     0.6718       0.9524        87.75       99.19\n  ResNeXt-50-32x4d                77.6           73.73      0.9212     0.6906       0.9588        88.15       99.24\n  EfficientNet B0                 77.7           71.02      0.9195     0.6942       0.9456        87.63       98.80\n  ResNet-152                      78.3           74.05      0.9228     0.6732       0.9562        87.75       99.15\n  ViT-B/16                        78.7           72.07      0.9262     0.5852       0.9600        86.63       99.28\n  DeiT-small                      79.9           71.41      0.9205     0.6148       0.9583        87.19       99.20\n  Inception-ResNet v2             80.4           70.68      0.9168     0.6882       0.9483        87.84       98.93\n  ConvNext-tiny                   82.5           78.51      0.9297     0.6992       0.9628        88.89       99.11\n  PNASNet-5 large                 82.9           75.21      0.9271     0.6941       0.9584        87.77       99.17\n  EfficientNet B4                 83.4           73.49      0.9211     0.6954       0.9552        88.36       98.70\nSee the following link for experiment results across hyperparameters: https://docs.google.\ncom/spreadsheets/d/1aDeuTH0V1Kid_JMRUt3sF1N76LUCAMDQ007Ykjo3Z4U/\nedit?usp=sharing.\n                                                     17", "md": "# Experiment Results\n\n## Appendix\n\n### DETAILED EXPERIMENT RESULTS\n\n|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|\n|---|---|---|---|---|---|---|---|\n|AlexNet|56.5|63.59|0.8835|0.3846|0.9283|82.58|97.93|\n|SqueezeNet 1.1|58.2|66.36|0.9021|0.3972|0.9073|85.15|98.07|\n|ShuffleNetV2x0.5|60.6|66.37|0.9227|0.5867|0.9289|85.64|98.56|\n|MobileNet V3 small|67.7|66.01|0.9230|0.6108|0.9455|85.81|99.15|\n|ShuffleNetV2x1.0|69.4|69.27|0.9202|0.6202|0.9418|87.33|98.91|\n|VGG-13 BN|71.6|75.06|0.9268|0.6794|0.9529|88.99|98.85|\n|DeiT-tiny|72.2|68.77|0.9130|0.5777|0.9510|86.25|99.11|\n|VGG-16 BN|73.4|75.93|0.9287|0.6791|0.9531|88.45|98.93|\n|DenseNet-121|74.4|74.66|0.9287|0.7019|0.9514|87.80|99.06|\n|ResNet-50|76.1|73.96|0.9215|0.6718|0.9524|87.75|99.19|\n|ResNeXt-50-32x4d|77.6|73.73|0.9212|0.6906|0.9588|88.15|99.24|\n|EfficientNet B0|77.7|71.02|0.9195|0.6942|0.9456|87.63|98.80|\n|ResNet-152|78.3|74.05|0.9228|0.6732|0.9562|87.75|99.15|\n|ViT-B/16|78.7|72.07|0.9262|0.5852|0.9600|86.63|99.28|\n|DeiT-small|79.9|71.41|0.9205|0.6148|0.9583|87.19|99.20|\n|Inception-ResNet v2|80.4|70.68|0.9168|0.6882|0.9483|87.84|98.93|\n|ConvNext-tiny|82.5|78.51|0.9297|0.6992|0.9628|88.89|99.11|\n|PNASNet-5 large|82.9|75.21|0.9271|0.6941|0.9584|87.77|99.17|\n|EfficientNet B4|83.4|73.49|0.9211|0.6954|0.9552|88.36|98.70|\n\nSee the following link for experiment results across hyperparameters: Experiment Results.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Experiment Results", "md": "# Experiment Results"}, {"type": "heading", "lvl": 2, "value": "Appendix", "md": "## Appendix"}, {"type": "heading", "lvl": 3, "value": "DETAILED EXPERIMENT RESULTS", "md": "### DETAILED EXPERIMENT RESULTS"}, {"type": "table", "rows": [["Model", "ImageNet top-1", "CCT20", "APTOS", "HPA", "Melanoma", "Cassava", "EuroSAT"], ["AlexNet", "56.5", "63.59", "0.8835", "0.3846", "0.9283", "82.58", "97.93"], ["SqueezeNet 1.1", "58.2", "66.36", "0.9021", "0.3972", "0.9073", "85.15", "98.07"], ["ShuffleNetV2x0.5", "60.6", "66.37", "0.9227", "0.5867", "0.9289", "85.64", "98.56"], ["MobileNet V3 small", "67.7", "66.01", "0.9230", "0.6108", "0.9455", "85.81", "99.15"], ["ShuffleNetV2x1.0", "69.4", "69.27", "0.9202", "0.6202", "0.9418", "87.33", "98.91"], ["VGG-13 BN", "71.6", "75.06", "0.9268", "0.6794", "0.9529", "88.99", "98.85"], ["DeiT-tiny", "72.2", "68.77", "0.9130", "0.5777", "0.9510", "86.25", "99.11"], ["VGG-16 BN", "73.4", "75.93", "0.9287", "0.6791", "0.9531", "88.45", "98.93"], ["DenseNet-121", "74.4", "74.66", "0.9287", "0.7019", "0.9514", "87.80", "99.06"], ["ResNet-50", "76.1", "73.96", "0.9215", "0.6718", "0.9524", "87.75", "99.19"], ["ResNeXt-50-32x4d", "77.6", "73.73", "0.9212", "0.6906", "0.9588", "88.15", "99.24"], ["EfficientNet B0", "77.7", "71.02", "0.9195", "0.6942", "0.9456", "87.63", "98.80"], ["ResNet-152", "78.3", "74.05", "0.9228", "0.6732", "0.9562", "87.75", "99.15"], ["ViT-B/16", "78.7", "72.07", "0.9262", "0.5852", "0.9600", "86.63", "99.28"], ["DeiT-small", "79.9", "71.41", "0.9205", "0.6148", "0.9583", "87.19", "99.20"], ["Inception-ResNet v2", "80.4", "70.68", "0.9168", "0.6882", "0.9483", "87.84", "98.93"], ["ConvNext-tiny", "82.5", "78.51", "0.9297", "0.6992", "0.9628", "88.89", "99.11"], ["PNASNet-5 large", "82.9", "75.21", "0.9271", "0.6941", "0.9584", "87.77", "99.17"], ["EfficientNet B4", "83.4", "73.49", "0.9211", "0.6954", "0.9552", "88.36", "98.70"]], "md": "|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|\n|---|---|---|---|---|---|---|---|\n|AlexNet|56.5|63.59|0.8835|0.3846|0.9283|82.58|97.93|\n|SqueezeNet 1.1|58.2|66.36|0.9021|0.3972|0.9073|85.15|98.07|\n|ShuffleNetV2x0.5|60.6|66.37|0.9227|0.5867|0.9289|85.64|98.56|\n|MobileNet V3 small|67.7|66.01|0.9230|0.6108|0.9455|85.81|99.15|\n|ShuffleNetV2x1.0|69.4|69.27|0.9202|0.6202|0.9418|87.33|98.91|\n|VGG-13 BN|71.6|75.06|0.9268|0.6794|0.9529|88.99|98.85|\n|DeiT-tiny|72.2|68.77|0.9130|0.5777|0.9510|86.25|99.11|\n|VGG-16 BN|73.4|75.93|0.9287|0.6791|0.9531|88.45|98.93|\n|DenseNet-121|74.4|74.66|0.9287|0.7019|0.9514|87.80|99.06|\n|ResNet-50|76.1|73.96|0.9215|0.6718|0.9524|87.75|99.19|\n|ResNeXt-50-32x4d|77.6|73.73|0.9212|0.6906|0.9588|88.15|99.24|\n|EfficientNet B0|77.7|71.02|0.9195|0.6942|0.9456|87.63|98.80|\n|ResNet-152|78.3|74.05|0.9228|0.6732|0.9562|87.75|99.15|\n|ViT-B/16|78.7|72.07|0.9262|0.5852|0.9600|86.63|99.28|\n|DeiT-small|79.9|71.41|0.9205|0.6148|0.9583|87.19|99.20|\n|Inception-ResNet v2|80.4|70.68|0.9168|0.6882|0.9483|87.84|98.93|\n|ConvNext-tiny|82.5|78.51|0.9297|0.6992|0.9628|88.89|99.11|\n|PNASNet-5 large|82.9|75.21|0.9271|0.6941|0.9584|87.77|99.17|\n|EfficientNet B4|83.4|73.49|0.9211|0.6954|0.9552|88.36|98.70|", "isPerfectTable": true, "csv": "\"Model\",\"ImageNet top-1\",\"CCT20\",\"APTOS\",\"HPA\",\"Melanoma\",\"Cassava\",\"EuroSAT\"\n\"AlexNet\",\"56.5\",\"63.59\",\"0.8835\",\"0.3846\",\"0.9283\",\"82.58\",\"97.93\"\n\"SqueezeNet 1.1\",\"58.2\",\"66.36\",\"0.9021\",\"0.3972\",\"0.9073\",\"85.15\",\"98.07\"\n\"ShuffleNetV2x0.5\",\"60.6\",\"66.37\",\"0.9227\",\"0.5867\",\"0.9289\",\"85.64\",\"98.56\"\n\"MobileNet V3 small\",\"67.7\",\"66.01\",\"0.9230\",\"0.6108\",\"0.9455\",\"85.81\",\"99.15\"\n\"ShuffleNetV2x1.0\",\"69.4\",\"69.27\",\"0.9202\",\"0.6202\",\"0.9418\",\"87.33\",\"98.91\"\n\"VGG-13 BN\",\"71.6\",\"75.06\",\"0.9268\",\"0.6794\",\"0.9529\",\"88.99\",\"98.85\"\n\"DeiT-tiny\",\"72.2\",\"68.77\",\"0.9130\",\"0.5777\",\"0.9510\",\"86.25\",\"99.11\"\n\"VGG-16 BN\",\"73.4\",\"75.93\",\"0.9287\",\"0.6791\",\"0.9531\",\"88.45\",\"98.93\"\n\"DenseNet-121\",\"74.4\",\"74.66\",\"0.9287\",\"0.7019\",\"0.9514\",\"87.80\",\"99.06\"\n\"ResNet-50\",\"76.1\",\"73.96\",\"0.9215\",\"0.6718\",\"0.9524\",\"87.75\",\"99.19\"\n\"ResNeXt-50-32x4d\",\"77.6\",\"73.73\",\"0.9212\",\"0.6906\",\"0.9588\",\"88.15\",\"99.24\"\n\"EfficientNet B0\",\"77.7\",\"71.02\",\"0.9195\",\"0.6942\",\"0.9456\",\"87.63\",\"98.80\"\n\"ResNet-152\",\"78.3\",\"74.05\",\"0.9228\",\"0.6732\",\"0.9562\",\"87.75\",\"99.15\"\n\"ViT-B/16\",\"78.7\",\"72.07\",\"0.9262\",\"0.5852\",\"0.9600\",\"86.63\",\"99.28\"\n\"DeiT-small\",\"79.9\",\"71.41\",\"0.9205\",\"0.6148\",\"0.9583\",\"87.19\",\"99.20\"\n\"Inception-ResNet v2\",\"80.4\",\"70.68\",\"0.9168\",\"0.6882\",\"0.9483\",\"87.84\",\"98.93\"\n\"ConvNext-tiny\",\"82.5\",\"78.51\",\"0.9297\",\"0.6992\",\"0.9628\",\"88.89\",\"99.11\"\n\"PNASNet-5 large\",\"82.9\",\"75.21\",\"0.9271\",\"0.6941\",\"0.9584\",\"87.77\",\"99.17\"\n\"EfficientNet B4\",\"83.4\",\"73.49\",\"0.9211\",\"0.6954\",\"0.9552\",\"88.36\",\"98.70\""}, {"type": "text", "value": "See the following link for experiment results across hyperparameters: Experiment Results.", "md": "See the following link for experiment results across hyperparameters: Experiment Results."}]}, {"page": 18, "text": "B         MAIN FIGURE VARIATIONS\n                        Caltech Camera Traps 20                                    APTOS 2019 Blindness                                      Human Protein Atlas\n                                                                      Quadratic weighted kappa                                 0.75\n               78                                                      0.94                                                    0.70\n               76                                                                                                             Macro F1 score\n               74                                                                                                              0.65\n              Accuracy                                                 0.92                                                    0.60\n               72                                                                                                              0.55\n               70                                                      0.90                                                    0.50\n               68\n               66                                                      0.88                                                    0.45\n               64                                                                                                              0.40\n                                                                                                                               0.35\n                  55     60      65      70      75     80      85          55     60      65      70      75   80      85          55     60      65      70     75    80  85\n                         ImageNet top-1 accuracy                                   ImageNet top-1 accuracy                                 ImageNet top-1 accuracy\n              0.97         SIIM-ISIC Melanoma                            90         Cassava Leaf Disease                      99.50                   EuroSAT\n              0.96                                                                                                            99.25\n             Area under ROC                                              88                                                   99.00\n              0.95\n                                                                        Accuracy                                             Accuracy\n              0.94                                                       86                                                   98.75\n              0.93                                                                                                            98.50\n              0.92                                                       84                                                   98.25\n                                                                                                                              98.00\n              0.91                                                       82                                                   97.75\n              0.90                                                                                                            97.50\n                  55     60      65      70      75     80      85          55     60      65      70      75   80      85          55     60      65      70     75    80  85\n                         ImageNet top-1 accuracy                                   ImageNet top-1 accuracy                                 ImageNet top-1 accuracy\n                                  AlexNet                                ResNet-152                             EfficientNet B0                       ConvNext-tiny\n                                  MobileNetV3-small                      DeiT-small                             EfficientNet B4                       ShuffleNetV2x0.5\n                                  VGG-13 BN                              PNASNet-5                              DenseNet-121                          SqueezeNet 1.1\n                                  DeiT-tiny                              Inception-ResNet v2                    ResNeXt-50-32x4d                      ViT-B/16\n                                  ResNet-50                              VGG-16 BN                              ShuffleNetV2x1.0\nFigure 4: Figure 1 with error bars. Green is linear trend of all models, while blue is linear trend for models\nabove 70% ImageNet accuracy. We use 95% confidence intervals computed with Clopper-Pearson for accuracy\nmetrics and bootstrap with 10,000 trials for other metrics.\n              80.0      Caltech Camera Traps 20                                    APTOS 2019 Blindness                                      Human Protein Atlas\n                                                                      Quadratic weighted kappa                                   0.7\n              77.5                                                     0.94\n                                                                                                                                 0.6\n              75.0                                                                                                              Macro F1 score\n             Accuracy                                                  0.92\n              72.5                                                                                                               0.5\n              70.0                                                     0.90                                                      0.4\n              67.5                                                                                                               0.3\n              65.0                                                     0.88                                                      0.2\n              62.5\n                  55     60      65      70      75     80      85          55     60      65      70      75   80      85          55     60      65      70     75    80  85\n                         ImageNet top-1 accuracy                                   ImageNet top-1 accuracy                                 ImageNet top-1 accuracy\n              0.97         SIIM-ISIC Melanoma                            90         Cassava Leaf Disease                      99.50                   EuroSAT\n              0.96                                                                                                            99.25\n             Area under ROC                                              88\n              0.95                                                                                                            99.00\n              0.94                                                      Accuracy                                             Accuracy\n                                                                                                                              98.75\n              0.93                                                       86                                                   98.50\n              0.92                                                                                                            98.25\n              0.91                                                       84                                                   98.00\n              0.90                                                       82                                                   97.75\n              0.89                                                                                                            97.50\n                  55     60      65      70      75     80      85          55     60      65      70      75   80      85          55     60      65      70     75    80  85\n                         ImageNet top-1 accuracy                                   ImageNet top-1 accuracy                                 ImageNet top-1 accuracy\n                                  AlexNet                                ResNet-152                             EfficientNet B0                       ConvNext-tiny\n                                  MobileNetV3-small                      DeiT-small                             EfficientNet B4                       ShuffleNetV2x0.5\n                                  VGG-13 BN                              PNASNet-5                              DenseNet-121                          SqueezeNet 1.1\n                                  DeiT-tiny                              Inception-ResNet v2                    ResNeXt-50-32x4d                      ViT-B/16\n                                  ResNet-50                              VGG-16 BN                              ShuffleNetV2x1.0\n                                    Figure 5: Figure 4 with spline interpolation fits instead of linear fits.\n                                                                                              18", "md": "# Main Figure Variations\n\n## Main Figure Variations\n\n| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|Quadratic weighted kappa|0.75|0.94|0.70|\n| | | |Macro F1 score|\n|Accuracy|0.92|0.60|0.65|\n\nImageNet top-1 accuracy\n\n| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|SIIM-ISIC Melanoma|0.97|90|99.50|\n|Area under ROC|88| |99.00|\n|Accuracy|86| |98.75|\n\nImageNet top-1 accuracy\n\n| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|AlexNet| | |ConvNext-tiny|\n|MobileNetV3-small| | |ShuffleNetV2x0.5|\n|VGG-13 BN| | |SqueezeNet 1.1|\n|DeiT-tiny| | |ViT-B/16|\n|ResNet-50| | | |\n\nFigure 4: Figure 1 with error bars. Green is linear trend of all models, while blue is linear trend for models above 70% ImageNet accuracy. We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.\n\n| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|Quadratic weighted kappa|0.7|0.94|0.6|\n| | | |Macro F1 score|\n|Accuracy|0.92|0.4|0.5|\n\nImageNet top-1 accuracy\n\n| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|SIIM-ISIC Melanoma|0.97|90|99.50|\n|Area under ROC|88| |99.00|\n|Accuracy|86| |98.75|\n\nImageNet top-1 accuracy\n\n| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|AlexNet| | |ConvNext-tiny|\n|MobileNetV3-small| | |ShuffleNetV2x0.5|\n|VGG-13 BN| | |SqueezeNet 1.1|\n|DeiT-tiny| | |ViT-B/16|\n|ResNet-50| | | |\n\nFigure 5: Figure 4 with spline interpolation fits instead of linear fits.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Main Figure Variations", "md": "# Main Figure Variations"}, {"type": "heading", "lvl": 2, "value": "Main Figure Variations", "md": "## Main Figure Variations"}, {"type": "table", "rows": [["", "Caltech Camera Traps 20", "APTOS 2019 Blindness", "Human Protein Atlas"], ["Quadratic weighted kappa", "0.75", "0.94", "0.70"], ["", "", "", "Macro F1 score"], ["Accuracy", "0.92", "0.60", "0.65"]], "md": "| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|Quadratic weighted kappa|0.75|0.94|0.70|\n| | | |Macro F1 score|\n|Accuracy|0.92|0.60|0.65|", "isPerfectTable": true, "csv": "\"\",\"Caltech Camera Traps 20\",\"APTOS 2019 Blindness\",\"Human Protein Atlas\"\n\"Quadratic weighted kappa\",\"0.75\",\"0.94\",\"0.70\"\n\"\",\"\",\"\",\"Macro F1 score\"\n\"Accuracy\",\"0.92\",\"0.60\",\"0.65\""}, {"type": "text", "value": "ImageNet top-1 accuracy", "md": "ImageNet top-1 accuracy"}, {"type": "table", "rows": [["", "Caltech Camera Traps 20", "APTOS 2019 Blindness", "Human Protein Atlas"], ["SIIM-ISIC Melanoma", "0.97", "90", "99.50"], ["Area under ROC", "88", "", "99.00"], ["Accuracy", "86", "", "98.75"]], "md": "| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|SIIM-ISIC Melanoma|0.97|90|99.50|\n|Area under ROC|88| |99.00|\n|Accuracy|86| |98.75|", "isPerfectTable": true, "csv": "\"\",\"Caltech Camera Traps 20\",\"APTOS 2019 Blindness\",\"Human Protein Atlas\"\n\"SIIM-ISIC Melanoma\",\"0.97\",\"90\",\"99.50\"\n\"Area under ROC\",\"88\",\"\",\"99.00\"\n\"Accuracy\",\"86\",\"\",\"98.75\""}, {"type": "text", "value": "ImageNet top-1 accuracy", "md": "ImageNet top-1 accuracy"}, {"type": "table", "rows": [["", "Caltech Camera Traps 20", "APTOS 2019 Blindness", "Human Protein Atlas"], ["AlexNet", "", "", "ConvNext-tiny"], ["MobileNetV3-small", "", "", "ShuffleNetV2x0.5"], ["VGG-13 BN", "", "", "SqueezeNet 1.1"], ["DeiT-tiny", "", "", "ViT-B/16"], ["ResNet-50", "", "", ""]], "md": "| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|AlexNet| | |ConvNext-tiny|\n|MobileNetV3-small| | |ShuffleNetV2x0.5|\n|VGG-13 BN| | |SqueezeNet 1.1|\n|DeiT-tiny| | |ViT-B/16|\n|ResNet-50| | | |", "isPerfectTable": true, "csv": "\"\",\"Caltech Camera Traps 20\",\"APTOS 2019 Blindness\",\"Human Protein Atlas\"\n\"AlexNet\",\"\",\"\",\"ConvNext-tiny\"\n\"MobileNetV3-small\",\"\",\"\",\"ShuffleNetV2x0.5\"\n\"VGG-13 BN\",\"\",\"\",\"SqueezeNet 1.1\"\n\"DeiT-tiny\",\"\",\"\",\"ViT-B/16\"\n\"ResNet-50\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 4: Figure 1 with error bars. Green is linear trend of all models, while blue is linear trend for models above 70% ImageNet accuracy. We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.", "md": "Figure 4: Figure 1 with error bars. Green is linear trend of all models, while blue is linear trend for models above 70% ImageNet accuracy. We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics."}, {"type": "table", "rows": [["", "Caltech Camera Traps 20", "APTOS 2019 Blindness", "Human Protein Atlas"], ["Quadratic weighted kappa", "0.7", "0.94", "0.6"], ["", "", "", "Macro F1 score"], ["Accuracy", "0.92", "0.4", "0.5"]], "md": "| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|Quadratic weighted kappa|0.7|0.94|0.6|\n| | | |Macro F1 score|\n|Accuracy|0.92|0.4|0.5|", "isPerfectTable": true, "csv": "\"\",\"Caltech Camera Traps 20\",\"APTOS 2019 Blindness\",\"Human Protein Atlas\"\n\"Quadratic weighted kappa\",\"0.7\",\"0.94\",\"0.6\"\n\"\",\"\",\"\",\"Macro F1 score\"\n\"Accuracy\",\"0.92\",\"0.4\",\"0.5\""}, {"type": "text", "value": "ImageNet top-1 accuracy", "md": "ImageNet top-1 accuracy"}, {"type": "table", "rows": [["", "Caltech Camera Traps 20", "APTOS 2019 Blindness", "Human Protein Atlas"], ["SIIM-ISIC Melanoma", "0.97", "90", "99.50"], ["Area under ROC", "88", "", "99.00"], ["Accuracy", "86", "", "98.75"]], "md": "| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|SIIM-ISIC Melanoma|0.97|90|99.50|\n|Area under ROC|88| |99.00|\n|Accuracy|86| |98.75|", "isPerfectTable": true, "csv": "\"\",\"Caltech Camera Traps 20\",\"APTOS 2019 Blindness\",\"Human Protein Atlas\"\n\"SIIM-ISIC Melanoma\",\"0.97\",\"90\",\"99.50\"\n\"Area under ROC\",\"88\",\"\",\"99.00\"\n\"Accuracy\",\"86\",\"\",\"98.75\""}, {"type": "text", "value": "ImageNet top-1 accuracy", "md": "ImageNet top-1 accuracy"}, {"type": "table", "rows": [["", "Caltech Camera Traps 20", "APTOS 2019 Blindness", "Human Protein Atlas"], ["AlexNet", "", "", "ConvNext-tiny"], ["MobileNetV3-small", "", "", "ShuffleNetV2x0.5"], ["VGG-13 BN", "", "", "SqueezeNet 1.1"], ["DeiT-tiny", "", "", "ViT-B/16"], ["ResNet-50", "", "", ""]], "md": "| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|AlexNet| | |ConvNext-tiny|\n|MobileNetV3-small| | |ShuffleNetV2x0.5|\n|VGG-13 BN| | |SqueezeNet 1.1|\n|DeiT-tiny| | |ViT-B/16|\n|ResNet-50| | | |", "isPerfectTable": true, "csv": "\"\",\"Caltech Camera Traps 20\",\"APTOS 2019 Blindness\",\"Human Protein Atlas\"\n\"AlexNet\",\"\",\"\",\"ConvNext-tiny\"\n\"MobileNetV3-small\",\"\",\"\",\"ShuffleNetV2x0.5\"\n\"VGG-13 BN\",\"\",\"\",\"SqueezeNet 1.1\"\n\"DeiT-tiny\",\"\",\"\",\"ViT-B/16\"\n\"ResNet-50\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 5: Figure 4 with spline interpolation fits instead of linear fits.", "md": "Figure 5: Figure 4 with spline interpolation fits instead of linear fits."}]}, {"page": 19, "text": "C       EXPERIMENT SETUP\nC.1       MODELS\nTable 4: We examine the effectiveness of transfer learning from a number of models pretrained on ImageNet,\nincluding both CNNs and Vision Transformers.\n                 Model                                                         ImageNet top-1       # params  Year Released\n                 AlexNet (Krizhevsky et al., 2012)                                   56.5             61M          2012\n                 SqueezeNet 1.1 (Iandola et al., 2016)                               58.2             1.2M         2016\n                 ShuffleNetV2x0.5 (Ma et al., 2018)                                  60.6             1.4M         2018\n                 MobileNet V3 small (Howard et al., 2019)                            67.7             2.5M         2019\n                 ShuffleNetV2x1.0 (Ma et al., 2018)                                  69.4             2.3M         2018\n                 VGG-13 BN (Simonyan & Zisserman, 2015)                              71.6            133M       2014/2015\n                 DeiT-tiny (Touvron et al., 2021)                                    72.2             5.7M         2020\n                 VGG-16 BN (Simonyan & Zisserman, 2015)                              73.4            138M       2014/2015\n                 DenseNet-121 (Huang et al., 2017)                                   74.4             8.0M         2016\n                 ResNet-50 (He et al., 2016)                                         76.1             26M          2015\n                 ResNeXt-50-32x4d (Xie et al., 2017)                                 77.6             25M          2016\n                 EfficientNet B0 (Tan & Le, 2019)                                    77.7             5.3M         2019\n                 ResNet-152 (He et al., 2016)                                        78.3             60M          2015\n                 ViT-B/16 (Dosovitskiy et al., 2021a; Steiner et al., 2021)          78.7            304M          2020\n                 DeiT-small (Touvron et al., 2021)                                   79.9             22M          2020\n                 Inception-ResNet v2 (Szegedy et al., 2017a)                         80.4             56M          2016\n                 ConvNext-tiny (Liu et al., 2022)                                    82.5             29M          2022\n                 PNASNet-5 large (Liu et al., 2018)                                  82.9             86M          2017\n                 EfficientNet B4 (Tan & Le, 2019)                                    83.4             19M          2019\nWe examine 19 model architectures in this work that cover a diverse range of accuracies on ImageNet\nin order to observe the relationship between ImageNet performance and target dataset performance.\nIn addition to the commonly used CNNs, we also include data-efficient image transformers (DeiT)\ndue to the recent increase in usage of Vision Transformers. Additional model details are in Table 4.\nC.2       HYPERPARAMETER GRID\nHyperparameter tuning is a key part of neural network training, as using suboptimal hyperparameters\ncan lead to suboptimal performance. Furthermore, the correct hyperparameters vary across both\nmodels and training data. To get the best performance out of each model, we train each model\non AdamW with a cosine decay learning rate schedule, SGD with a cosine decay learning rate\nschedule, and SGD with a multi-step decay learning rate schedule. We also grid search for optimal\ninitial learning rate and weight decay combinations, searching logarithmically between 10\u22121 to\n10\u22124 for SGD learning rate, 10\u22122 to 10\u22125 for AdamW learning rate, and 10\u22123 to 10\u22126 as well as\n0 for weight decay. All models are pretrained on ImageNet and then fine-tuned on the downstream\ntask. Additional training details for each dataset can be found in Appendix D. We also run our\nhyperparameter grid on CIFAR-10 in Appendix E to verify that we find a strong relationship between\nImageNet and CIFAR-10 accuracy as previously reported by Kornblith et al. (2019).\nD       TRAINING DETAILS BY DATASET (IMAGENET MODELS)\nExperiments on Cassava Leaf Disease, SIIM-ISIC Melanoma, and EuroSAT datasets were ran on\nTPU v2-8s, while all other datasets were ran on NVIDIA A40s.\nAll experiments were ran with mini-batch size of 128.\nFor SGD experiments, we use Nesterov momentum, set momentum to 0.9, and try learning rates of\n1e-1, 1e-2, 1e-3, and 1e-4. For AdamW experiments, we try learning rates of 1e-2, 1e-3, 1e-4, 1e-5.\nFor all experiments, we try weight decays of 1e-3, 1e-4, 1e-5, 1e-6, and 0.\nFor all experiments, we use weights that are pretrained on ImageNet. AlexNet, DenseNet, Mo-\nbileNet, ResNet, ResNext, ShuffleNet, SqueezeNet and VGG models are from torchvision, while\nConvNext, DeiT, EfficientNet, InceptionResNet, and PNASNet models are from timm. Addition-\nally, we normalize images to ImageNet\u2019s mean and standard deviation.\nFor EuroSAT we random resize crop to 224 with area at least 0.65.\nFor all other datasets, we random resize crop with area at least 0.65 to 224 for DeiT models, and 256\nfor all other models. Additionally, we use horizontal flips. For Human Protein Atlas, Cassava Leaf\nDisease, and SIIM-ISIC Melanoma, we also use vertical flips.\n                                                                       19", "md": "# Experiment Setup\n\n## MODELS\n\n|Model|ImageNet top-1|# params|Year Released|\n|---|---|---|---|\n|AlexNet (Krizhevsky et al., 2012)|56.5|61M|2012|\n|SqueezeNet 1.1 (Iandola et al., 2016)|58.2|1.2M|2016|\n|ShuffleNetV2x0.5 (Ma et al., 2018)|60.6|1.4M|2018|\n|MobileNet V3 small (Howard et al., 2019)|67.7|2.5M|2019|\n|ShuffleNetV2x1.0 (Ma et al., 2018)|69.4|2.3M|2018|\n|VGG-13 BN (Simonyan & Zisserman, 2015)|71.6|133M|2014/2015|\n|DeiT-tiny (Touvron et al., 2021)|72.2|5.7M|2020|\n|VGG-16 BN (Simonyan & Zisserman, 2015)|73.4|138M|2014/2015|\n|DenseNet-121 (Huang et al., 2017)|74.4|8.0M|2016|\n|ResNet-50 (He et al., 2016)|76.1|26M|2015|\n|ResNeXt-50-32x4d (Xie et al., 2017)|77.6|25M|2016|\n|EfficientNet B0 (Tan & Le, 2019)|77.7|5.3M|2019|\n|ResNet-152 (He et al., 2016)|78.3|60M|2015|\n|ViT-B/16 (Dosovitskiy et al., 2021a; Steiner et al., 2021)|78.7|304M|2020|\n|DeiT-small (Touvron et al., 2021)|79.9|22M|2020|\n|Inception-ResNet v2 (Szegedy et al., 2017a)|80.4|56M|2016|\n|ConvNext-tiny (Liu et al., 2022)|82.5|29M|2022|\n|PNASNet-5 large (Liu et al., 2018)|82.9|86M|2017|\n|EfficientNet B4 (Tan & Le, 2019)|83.4|19M|2019|\n\nWe examine 19 model architectures in this work that cover a diverse range of accuracies on ImageNet\nin order to observe the relationship between ImageNet performance and target dataset performance.\nIn addition to the commonly used CNNs, we also include data-efficient image transformers (DeiT)\ndue to the recent increase in usage of Vision Transformers. Additional model details are in Table 4.\n\n## HYPERPARAMETER GRID\n\nHyperparameter tuning is a key part of neural network training, as using suboptimal hyperparameters\ncan lead to suboptimal performance. Furthermore, the correct hyperparameters vary across both\nmodels and training data. To get the best performance out of each model, we train each model\non AdamW with a cosine decay learning rate schedule, SGD with a cosine decay learning rate\nschedule, and SGD with a multi-step decay learning rate schedule. We also grid search for optimal\ninitial learning rate and weight decay combinations, searching logarithmically between $$10^{-1}$$ to\n$$10^{-4}$$ for SGD learning rate, $$10^{-2}$$ to $$10^{-5}$$ for AdamW learning rate, and $$10^{-3}$$ to $$10^{-6}$$ as well as\n0 for weight decay. All models are pretrained on ImageNet and then fine-tuned on the downstream\ntask. Additional training details for each dataset can be found in Appendix D. We also run our\nhyperparameter grid on CIFAR-10 in Appendix E to verify that we find a strong relationship between\nImageNet and CIFAR-10 accuracy as previously reported by Kornblith et al. (2019).\n\n## TRAINING DETAILS BY DATASET (IMAGENET MODELS)\n\nExperiments on Cassava Leaf Disease, SIIM-ISIC Melanoma, and EuroSAT datasets were ran on\nTPU v2-8s, while all other datasets were ran on NVIDIA A40s.\n\nAll experiments were ran with mini-batch size of 128.\n\nFor SGD experiments, we use Nesterov momentum, set momentum to 0.9, and try learning rates of\n$$1e-1$$, $$1e-2$$, $$1e-3$$, and $$1e-4$$. For AdamW experiments, we try learning rates of $$1e-2$$, $$1e-3$$, $$1e-4$$, $$1e-5$$.\n\nFor all experiments, we try weight decays of $$1e-3$$, $$1e-4$$, $$1e-5$$, $$1e-6$$, and 0.\n\nFor all experiments, we use weights that are pretrained on ImageNet. AlexNet, DenseNet, MobileNet, ResNet, ResNext, ShuffleNet, SqueezeNet and VGG models are from torchvision, while\nConvNext, DeiT, EfficientNet, InceptionResNet, and PNASNet models are from timm. Additionally, we normalize images to ImageNet\u2019s mean and standard deviation.\n\nFor EuroSAT we random resize crop to 224 with area at least 0.65.\n\nFor all other datasets, we random resize crop with area at least 0.65 to 224 for DeiT models, and 256\nfor all other models. Additionally, we use horizontal flips. For Human Protein Atlas, Cassava Leaf\nDisease, and SIIM-ISIC Melanoma, we also use vertical flips.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Experiment Setup", "md": "# Experiment Setup"}, {"type": "heading", "lvl": 2, "value": "MODELS", "md": "## MODELS"}, {"type": "table", "rows": [["Model", "ImageNet top-1", "# params", "Year Released"], ["AlexNet (Krizhevsky et al., 2012)", "56.5", "61M", "2012"], ["SqueezeNet 1.1 (Iandola et al., 2016)", "58.2", "1.2M", "2016"], ["ShuffleNetV2x0.5 (Ma et al., 2018)", "60.6", "1.4M", "2018"], ["MobileNet V3 small (Howard et al., 2019)", "67.7", "2.5M", "2019"], ["ShuffleNetV2x1.0 (Ma et al., 2018)", "69.4", "2.3M", "2018"], ["VGG-13 BN (Simonyan & Zisserman, 2015)", "71.6", "133M", "2014/2015"], ["DeiT-tiny (Touvron et al., 2021)", "72.2", "5.7M", "2020"], ["VGG-16 BN (Simonyan & Zisserman, 2015)", "73.4", "138M", "2014/2015"], ["DenseNet-121 (Huang et al., 2017)", "74.4", "8.0M", "2016"], ["ResNet-50 (He et al., 2016)", "76.1", "26M", "2015"], ["ResNeXt-50-32x4d (Xie et al., 2017)", "77.6", "25M", "2016"], ["EfficientNet B0 (Tan & Le, 2019)", "77.7", "5.3M", "2019"], ["ResNet-152 (He et al., 2016)", "78.3", "60M", "2015"], ["ViT-B/16 (Dosovitskiy et al., 2021a; Steiner et al., 2021)", "78.7", "304M", "2020"], ["DeiT-small (Touvron et al., 2021)", "79.9", "22M", "2020"], ["Inception-ResNet v2 (Szegedy et al., 2017a)", "80.4", "56M", "2016"], ["ConvNext-tiny (Liu et al., 2022)", "82.5", "29M", "2022"], ["PNASNet-5 large (Liu et al., 2018)", "82.9", "86M", "2017"], ["EfficientNet B4 (Tan & Le, 2019)", "83.4", "19M", "2019"]], "md": "|Model|ImageNet top-1|# params|Year Released|\n|---|---|---|---|\n|AlexNet (Krizhevsky et al., 2012)|56.5|61M|2012|\n|SqueezeNet 1.1 (Iandola et al., 2016)|58.2|1.2M|2016|\n|ShuffleNetV2x0.5 (Ma et al., 2018)|60.6|1.4M|2018|\n|MobileNet V3 small (Howard et al., 2019)|67.7|2.5M|2019|\n|ShuffleNetV2x1.0 (Ma et al., 2018)|69.4|2.3M|2018|\n|VGG-13 BN (Simonyan & Zisserman, 2015)|71.6|133M|2014/2015|\n|DeiT-tiny (Touvron et al., 2021)|72.2|5.7M|2020|\n|VGG-16 BN (Simonyan & Zisserman, 2015)|73.4|138M|2014/2015|\n|DenseNet-121 (Huang et al., 2017)|74.4|8.0M|2016|\n|ResNet-50 (He et al., 2016)|76.1|26M|2015|\n|ResNeXt-50-32x4d (Xie et al., 2017)|77.6|25M|2016|\n|EfficientNet B0 (Tan & Le, 2019)|77.7|5.3M|2019|\n|ResNet-152 (He et al., 2016)|78.3|60M|2015|\n|ViT-B/16 (Dosovitskiy et al., 2021a; Steiner et al., 2021)|78.7|304M|2020|\n|DeiT-small (Touvron et al., 2021)|79.9|22M|2020|\n|Inception-ResNet v2 (Szegedy et al., 2017a)|80.4|56M|2016|\n|ConvNext-tiny (Liu et al., 2022)|82.5|29M|2022|\n|PNASNet-5 large (Liu et al., 2018)|82.9|86M|2017|\n|EfficientNet B4 (Tan & Le, 2019)|83.4|19M|2019|", "isPerfectTable": true, "csv": "\"Model\",\"ImageNet top-1\",\"# params\",\"Year Released\"\n\"AlexNet (Krizhevsky et al., 2012)\",\"56.5\",\"61M\",\"2012\"\n\"SqueezeNet 1.1 (Iandola et al., 2016)\",\"58.2\",\"1.2M\",\"2016\"\n\"ShuffleNetV2x0.5 (Ma et al., 2018)\",\"60.6\",\"1.4M\",\"2018\"\n\"MobileNet V3 small (Howard et al., 2019)\",\"67.7\",\"2.5M\",\"2019\"\n\"ShuffleNetV2x1.0 (Ma et al., 2018)\",\"69.4\",\"2.3M\",\"2018\"\n\"VGG-13 BN (Simonyan & Zisserman, 2015)\",\"71.6\",\"133M\",\"2014/2015\"\n\"DeiT-tiny (Touvron et al., 2021)\",\"72.2\",\"5.7M\",\"2020\"\n\"VGG-16 BN (Simonyan & Zisserman, 2015)\",\"73.4\",\"138M\",\"2014/2015\"\n\"DenseNet-121 (Huang et al., 2017)\",\"74.4\",\"8.0M\",\"2016\"\n\"ResNet-50 (He et al., 2016)\",\"76.1\",\"26M\",\"2015\"\n\"ResNeXt-50-32x4d (Xie et al., 2017)\",\"77.6\",\"25M\",\"2016\"\n\"EfficientNet B0 (Tan & Le, 2019)\",\"77.7\",\"5.3M\",\"2019\"\n\"ResNet-152 (He et al., 2016)\",\"78.3\",\"60M\",\"2015\"\n\"ViT-B/16 (Dosovitskiy et al., 2021a; Steiner et al., 2021)\",\"78.7\",\"304M\",\"2020\"\n\"DeiT-small (Touvron et al., 2021)\",\"79.9\",\"22M\",\"2020\"\n\"Inception-ResNet v2 (Szegedy et al., 2017a)\",\"80.4\",\"56M\",\"2016\"\n\"ConvNext-tiny (Liu et al., 2022)\",\"82.5\",\"29M\",\"2022\"\n\"PNASNet-5 large (Liu et al., 2018)\",\"82.9\",\"86M\",\"2017\"\n\"EfficientNet B4 (Tan & Le, 2019)\",\"83.4\",\"19M\",\"2019\""}, {"type": "text", "value": "We examine 19 model architectures in this work that cover a diverse range of accuracies on ImageNet\nin order to observe the relationship between ImageNet performance and target dataset performance.\nIn addition to the commonly used CNNs, we also include data-efficient image transformers (DeiT)\ndue to the recent increase in usage of Vision Transformers. Additional model details are in Table 4.", "md": "We examine 19 model architectures in this work that cover a diverse range of accuracies on ImageNet\nin order to observe the relationship between ImageNet performance and target dataset performance.\nIn addition to the commonly used CNNs, we also include data-efficient image transformers (DeiT)\ndue to the recent increase in usage of Vision Transformers. Additional model details are in Table 4."}, {"type": "heading", "lvl": 2, "value": "HYPERPARAMETER GRID", "md": "## HYPERPARAMETER GRID"}, {"type": "text", "value": "Hyperparameter tuning is a key part of neural network training, as using suboptimal hyperparameters\ncan lead to suboptimal performance. Furthermore, the correct hyperparameters vary across both\nmodels and training data. To get the best performance out of each model, we train each model\non AdamW with a cosine decay learning rate schedule, SGD with a cosine decay learning rate\nschedule, and SGD with a multi-step decay learning rate schedule. We also grid search for optimal\ninitial learning rate and weight decay combinations, searching logarithmically between $$10^{-1}$$ to\n$$10^{-4}$$ for SGD learning rate, $$10^{-2}$$ to $$10^{-5}$$ for AdamW learning rate, and $$10^{-3}$$ to $$10^{-6}$$ as well as\n0 for weight decay. All models are pretrained on ImageNet and then fine-tuned on the downstream\ntask. Additional training details for each dataset can be found in Appendix D. We also run our\nhyperparameter grid on CIFAR-10 in Appendix E to verify that we find a strong relationship between\nImageNet and CIFAR-10 accuracy as previously reported by Kornblith et al. (2019).", "md": "Hyperparameter tuning is a key part of neural network training, as using suboptimal hyperparameters\ncan lead to suboptimal performance. Furthermore, the correct hyperparameters vary across both\nmodels and training data. To get the best performance out of each model, we train each model\non AdamW with a cosine decay learning rate schedule, SGD with a cosine decay learning rate\nschedule, and SGD with a multi-step decay learning rate schedule. We also grid search for optimal\ninitial learning rate and weight decay combinations, searching logarithmically between $$10^{-1}$$ to\n$$10^{-4}$$ for SGD learning rate, $$10^{-2}$$ to $$10^{-5}$$ for AdamW learning rate, and $$10^{-3}$$ to $$10^{-6}$$ as well as\n0 for weight decay. All models are pretrained on ImageNet and then fine-tuned on the downstream\ntask. Additional training details for each dataset can be found in Appendix D. We also run our\nhyperparameter grid on CIFAR-10 in Appendix E to verify that we find a strong relationship between\nImageNet and CIFAR-10 accuracy as previously reported by Kornblith et al. (2019)."}, {"type": "heading", "lvl": 2, "value": "TRAINING DETAILS BY DATASET (IMAGENET MODELS)", "md": "## TRAINING DETAILS BY DATASET (IMAGENET MODELS)"}, {"type": "text", "value": "Experiments on Cassava Leaf Disease, SIIM-ISIC Melanoma, and EuroSAT datasets were ran on\nTPU v2-8s, while all other datasets were ran on NVIDIA A40s.\n\nAll experiments were ran with mini-batch size of 128.\n\nFor SGD experiments, we use Nesterov momentum, set momentum to 0.9, and try learning rates of\n$$1e-1$$, $$1e-2$$, $$1e-3$$, and $$1e-4$$. For AdamW experiments, we try learning rates of $$1e-2$$, $$1e-3$$, $$1e-4$$, $$1e-5$$.\n\nFor all experiments, we try weight decays of $$1e-3$$, $$1e-4$$, $$1e-5$$, $$1e-6$$, and 0.\n\nFor all experiments, we use weights that are pretrained on ImageNet. AlexNet, DenseNet, MobileNet, ResNet, ResNext, ShuffleNet, SqueezeNet and VGG models are from torchvision, while\nConvNext, DeiT, EfficientNet, InceptionResNet, and PNASNet models are from timm. Additionally, we normalize images to ImageNet\u2019s mean and standard deviation.\n\nFor EuroSAT we random resize crop to 224 with area at least 0.65.\n\nFor all other datasets, we random resize crop with area at least 0.65 to 224 for DeiT models, and 256\nfor all other models. Additionally, we use horizontal flips. For Human Protein Atlas, Cassava Leaf\nDisease, and SIIM-ISIC Melanoma, we also use vertical flips.", "md": "Experiments on Cassava Leaf Disease, SIIM-ISIC Melanoma, and EuroSAT datasets were ran on\nTPU v2-8s, while all other datasets were ran on NVIDIA A40s.\n\nAll experiments were ran with mini-batch size of 128.\n\nFor SGD experiments, we use Nesterov momentum, set momentum to 0.9, and try learning rates of\n$$1e-1$$, $$1e-2$$, $$1e-3$$, and $$1e-4$$. For AdamW experiments, we try learning rates of $$1e-2$$, $$1e-3$$, $$1e-4$$, $$1e-5$$.\n\nFor all experiments, we try weight decays of $$1e-3$$, $$1e-4$$, $$1e-5$$, $$1e-6$$, and 0.\n\nFor all experiments, we use weights that are pretrained on ImageNet. AlexNet, DenseNet, MobileNet, ResNet, ResNext, ShuffleNet, SqueezeNet and VGG models are from torchvision, while\nConvNext, DeiT, EfficientNet, InceptionResNet, and PNASNet models are from timm. Additionally, we normalize images to ImageNet\u2019s mean and standard deviation.\n\nFor EuroSAT we random resize crop to 224 with area at least 0.65.\n\nFor all other datasets, we random resize crop with area at least 0.65 to 224 for DeiT models, and 256\nfor all other models. Additionally, we use horizontal flips. For Human Protein Atlas, Cassava Leaf\nDisease, and SIIM-ISIC Melanoma, we also use vertical flips."}]}, {"page": 20, "text": "For SIIM-ISIC Melanoma, we train for 10 epochs, and for the step scheduler decay with factor 0.1\nat 5 epochs.\nFor all other datasets, we train for 30 epochs, and for the step scheduler decay with factor 0.1 at 15,\n20, and 25 epochs.\nE      CIFAR-10 ON HYPERPARAMETER GRID\n                          99                                      CIFAR-10\n                          98\n                          97\n                          Accuracy\n                          96\n                          95\n                          94\n                          93\n                             55            60           65            70            75            80            85\n                                                         ImageNet top-1 accuracy\n              AlexNet                         ResNet-50                Inception-ResNet v2              DenseNet-121\n              MobileNetV3-small               ResNet-152               VGG-16 BN                        ResNeXt-50-32x4d\n              VGG-13 BN                       DeiT-small               EfficientNet B0                  ShuffleNetV2x1.0\n              DeiT-tiny                       PNASNet-5                EfficientNet B4                  ConvNext-tiny\nFigure 6: Transfer performance across models from ImageNet to CIFAR-10. Green linear trend is computed\nacross all models, while blue linear trend is restricted to models above 70% ImageNet accuracy. We use 95%\nconfidence intervals computed with Clopper-Pearson.\nF      APTOS 2019 BLINDNESS DETECTION ABLATIONS\nScores presented are submissions to the Kaggle leaderboard. All scores are evaluated with quadratic\nweighted kappa. Within each entry, we first present the private leaderboard score, then the pub-\nlic leaderboard score. The private leaderboard represents 85% of the test data, while the public\nleaderboard is the remaining 15%.\nModels used here are trained using AdamW with a cosine scheduler. We random resize crop to 512,\nuse random rotations, and use color jitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1).\nWe train on all the available training data, no longer using the local train/validation split mentioned\nin the main text. This includes both the training data in the 2019 competition, as well as data from a\nprior 2015 diabetic retinopathy competition.\n  Table 5: Comparing various models with additional interventions by evaluating on the Kaggle leaderboard.\n                                        lr \\wd             1.00E-04                  1.00E-05                 1.00E-06\n       ResNet-50                       1.00E-03        0.8610 / 0.6317          0.8570 / 0.6180          0.8548 / 0.6646\n                                       1.00E-04        0.8952 / 0.7531          0.8918 / 0.7204          0.8961 / 0.7547\n       ResNet-152                      1.00E-03        0.8658 / 0.6812          0.8686 / 0.6612          0.8640 / 0.6554\n                                       1.00E-04        0.8898 / 0.7164          0.8836 / 0.6946          0.8859 / 0.6947\n       Inception-Resnet-v2             1.00E-03        0.8933 / 0.7748          0.8905 / 0.7565          0.8960 / 0.7585\n                                       1.00E-04        0.8897 / 0.7210          0.8929 / 0.7420          0.8944 / 0.7439\n                                                                 20", "md": "For SIIM-ISIC Melanoma, we train for 10 epochs, and for the step scheduler decay with factor 0.1 at 5 epochs.\n\nFor all other datasets, we train for 30 epochs, and for the step scheduler decay with factor 0.1 at 15, 20, and 25 epochs.\n\nCIFAR-10 ON HYPERPARAMETER GRID\n\n55\n60\n65\n70\n75\n80\n85\n\nAlexNet\n\nResNet-50\n\nInception-ResNet v2\n\nDenseNet-121\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n& 55 & 60 & 65 & 70 & 75 & 80 & 85 \\\\\n\\hline\n\\text{AlexNet} & & & & & & & \\\\\n\\hline\n\\text{ResNet-50} & & & & & & & \\\\\n\\hline\n\\text{Inception-ResNet v2} & & & & & & & \\\\\n\\hline\n\\text{DenseNet-121} & & & & & & & \\\\\n\\hline\n\\end{array}\n$$", "images": [], "items": [{"type": "text", "value": "For SIIM-ISIC Melanoma, we train for 10 epochs, and for the step scheduler decay with factor 0.1 at 5 epochs.\n\nFor all other datasets, we train for 30 epochs, and for the step scheduler decay with factor 0.1 at 15, 20, and 25 epochs.\n\nCIFAR-10 ON HYPERPARAMETER GRID\n\n55\n60\n65\n70\n75\n80\n85\n\nAlexNet\n\nResNet-50\n\nInception-ResNet v2\n\nDenseNet-121\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n& 55 & 60 & 65 & 70 & 75 & 80 & 85 \\\\\n\\hline\n\\text{AlexNet} & & & & & & & \\\\\n\\hline\n\\text{ResNet-50} & & & & & & & \\\\\n\\hline\n\\text{Inception-ResNet v2} & & & & & & & \\\\\n\\hline\n\\text{DenseNet-121} & & & & & & & \\\\\n\\hline\n\\end{array}\n$$", "md": "For SIIM-ISIC Melanoma, we train for 10 epochs, and for the step scheduler decay with factor 0.1 at 5 epochs.\n\nFor all other datasets, we train for 30 epochs, and for the step scheduler decay with factor 0.1 at 15, 20, and 25 epochs.\n\nCIFAR-10 ON HYPERPARAMETER GRID\n\n55\n60\n65\n70\n75\n80\n85\n\nAlexNet\n\nResNet-50\n\nInception-ResNet v2\n\nDenseNet-121\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n& 55 & 60 & 65 & 70 & 75 & 80 & 85 \\\\\n\\hline\n\\text{AlexNet} & & & & & & & \\\\\n\\hline\n\\text{ResNet-50} & & & & & & & \\\\\n\\hline\n\\text{Inception-ResNet v2} & & & & & & & \\\\\n\\hline\n\\text{DenseNet-121} & & & & & & & \\\\\n\\hline\n\\end{array}\n$$"}]}, {"page": 21, "text": "Table 6: Comparing the effect of augmentation on Kaggle leaderboard scores. More augmentation is as de-\nscribed earlier in this section. Less augmentation only uses random resize crop with at least 0.65 area and\nhorizontal flips.\n                                     lr \\wd               1.00E-04                   1.00E-05                    1.00E-06\n               ResNet-50           1.00E-03          0.8669 / 0.6405             0.8520 / 0.6013            0.8613 / 0.6269\n               less aug            1.00E-04          0.8525 / 0.6115             0.8570 / 0.6431            0.8483 / 0.6147\n                                   1.00E-05          0.8186 / 0.5071             0.8287 / 0.5647            0.8288 / 0.5328\n               ResNet-50           1.00E-03          0.8440 / 0.6432             0.8547 / 0.6856            0.8524 / 0.7125\n               more aug            1.00E-04          0.8948 / 0.7490             0.8972 / 0.7693            0.8999 / 0.7758\n                                   1.00E-05          0.8724 / 0.7370             0.8685 / 0.7567            0.8623 / 0.7376\nG       AUGMENTATION ABLATION DETAILS\nTable 7:      We examine the effect of pre-training augmentation and fine-tuning augmentation on downstream\ntransfer performance. The model specifies the architecture and pre-training augmentation, while each column\nspecifies the downstream task and fine-tuning augmentation. We find that augmentation strategies that improve\nImageNet accuracy do not always improve accuracy on downstream tasks. Pre-trained augmentation models\nare from Wightman et al. (2021).\n      Model                ImageNet           CCT-20          CCT-20           CCT-20           APTOS           APTOS            APTOS\n                               Acc           Base Aug         AugMix          RandAug          Base Aug         AugMix          RandAug\n      ResNet-50                76.1            72.02            72.24           73.57            0.9210          0.9212          0.9250\n      ResNet-50                77.5            71.63            71.53           72.39            0.9239          0.9152          0.9222\n      w/ AugMix\n      ResNet-50                78.8            72.94            73.54           73.76            0.9190          0.9204          0.9302\n      w/ RandAug\n      Deit-tiny                72.2            66.57            66.47           66.95            0.9153          0.9197          0.9172\n      Deit-small               79.9            70.65            69.72           70.07            0.9293          0.9212          0.9277\nH       MELANOMA METRIC COMPARISON\n                      0.97      SIIM-ISIC Melanoma ROC                       96.5       SIIM-ISIC Melanoma Acc\n                      0.96                                                   96.0\n                     Area under ROC\n                      0.95                                                   95.5\n                                                                            Accuracy\n                      0.94                                                   95.0\n                      0.93                                                   94.5\n                      0.92                                                   94.0\n                      0.91                                                   93.5\n                      0.90                                                   93.0\n                          55      60     65     70      75     80     85         55      60     65     70      75     80      85\n                                 ImageNet top-1 accuracy                                ImageNet top-1 accuracy\n         AlexNet                              ResNet-152                             EfficientNet B0                     ShuffleNetV2x1.0\n         MobileNetV3-small                    DeiT-small                             EfficientNet B4                     ConvNext-tiny\n         VGG-13 BN                            PNASNet-5                              DenseNet-121                        ShuffleNetV2x0.5\n         DeiT-tiny                            Inception-ResNet v2                    ResNeXt-50-32x4d                    SqueezeNet 1.1\n         ResNet-50                            VGG-16 BN\nFigure 7: Comparing transfer performance from ImageNet to Melanoma when using different metrics. Green\nlinear trend is computed across all models, while blue linear trend is restricted to models above 70% ImageNet\naccuracy. Using accuracy implies that better ImageNet models transfer better; however, ROC is a better metric\nfor this task.\n                                                                       21", "md": "# OCR Text\n\n## Table 6: Comparing the effect of augmentation on Kaggle leaderboard scores\n\nMore augmentation is as described earlier in this section. Less augmentation only uses random resize crop with at least 0.65 area and horizontal flips.\n\n|lr \\ wd|1.00E-04|1.00E-05|1.00E-06|\n|---|---|---|---|\n|ResNet-50|1.00E-03|0.8669 / 0.6405|0.8520 / 0.6013|0.8613 / 0.6269|\n|less aug|1.00E-04|0.8525 / 0.6115|0.8570 / 0.6431|0.8483 / 0.6147|\n| |1.00E-05|0.8186 / 0.5071|0.8287 / 0.5647|0.8288 / 0.5328|\n|ResNet-50|1.00E-03|0.8440 / 0.6432|0.8547 / 0.6856|0.8524 / 0.7125|\n|more aug|1.00E-04|0.8948 / 0.7490|0.8972 / 0.7693|0.8999 / 0.7758|\n| |1.00E-05|0.8724 / 0.7370|0.8685 / 0.7567|0.8623 / 0.7376|\n\n## AUGMENTATION ABLATION DETAILS\n\n### Table 7: Examining the effect of pre-training augmentation and fine-tuning augmentation on downstream transfer performance\n\nThe model specifies the architecture and pre-training augmentation, while each column specifies the downstream task and fine-tuning augmentation. We find that augmentation strategies that improve ImageNet accuracy do not always improve accuracy on downstream tasks. Pre-trained augmentation models are from Wightman et al. (2021).\n\n|Model|ImageNet Acc|CCT-20 Base Aug|CCT-20 AugMix|CCT-20 RandAug|APTOS Base Aug|APTOS AugMix|APTOS RandAug|\n|---|---|---|---|---|---|---|---|\n|ResNet-50|76.1|72.02|72.24|73.57|0.9210|0.9212|0.9250|\n|ResNet-50 w/ AugMix|77.5|71.63|71.53|72.39|0.9239|0.9152|0.9222|\n|ResNet-50 w/ RandAug|78.8|72.94|73.54|73.76|0.9190|0.9204|0.9302|\n|Deit-tiny|72.2|66.57|66.47|66.95|0.9153|0.9197|0.9172|\n|Deit-small|79.9|70.65|69.72|70.07|0.9293|0.9212|0.9277|\n\n## MELANOMA METRIC COMPARISON\n\nSIIM-ISIC Melanoma ROC: 0.97, SIIM-ISIC Melanoma Acc: 96.5\n\nArea under ROC:\n\n0.96: 96.0\n\n0.95: 95.5\n\n0.94: 95.0\n\n0.93: 94.5\n\n0.92: 94.0\n\n0.91: 93.5\n\n0.90: 93.0\n\nImageNet top-1 accuracy: 55 60 65 70 75 80 85\n\nAlexNet, MobileNetV3-small, VGG-13 BN, DeiT-tiny, ResNet-50\n\nResNet-152, PNASNet-5, Inception-ResNet v2, VGG-16 BN\n\nEfficientNet B0, DenseNet-121, ResNeXt-50-32x4d, ShuffleNetV2x1.0\n\nEfficientNet B4, ShuffleNetV2x0.5, SqueezeNet 1.1, ConvNext-tiny\n\n## Figure 7: Comparing transfer performance from ImageNet to Melanoma using different metrics\n\nGreen linear trend is computed across all models, while blue linear trend is restricted to models above 70% ImageNet accuracy. Using accuracy implies that better ImageNet models transfer better; however, ROC is a better metric for this task.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "heading", "lvl": 2, "value": "Table 6: Comparing the effect of augmentation on Kaggle leaderboard scores", "md": "## Table 6: Comparing the effect of augmentation on Kaggle leaderboard scores"}, {"type": "text", "value": "More augmentation is as described earlier in this section. Less augmentation only uses random resize crop with at least 0.65 area and horizontal flips.", "md": "More augmentation is as described earlier in this section. Less augmentation only uses random resize crop with at least 0.65 area and horizontal flips."}, {"type": "table", "rows": [["lr \\ wd", "1.00E-04", "1.00E-05", "1.00E-06"], ["ResNet-50", "1.00E-03", "0.8669 / 0.6405", "0.8520 / 0.6013", "0.8613 / 0.6269"], ["less aug", "1.00E-04", "0.8525 / 0.6115", "0.8570 / 0.6431", "0.8483 / 0.6147"], ["", "1.00E-05", "0.8186 / 0.5071", "0.8287 / 0.5647", "0.8288 / 0.5328"], ["ResNet-50", "1.00E-03", "0.8440 / 0.6432", "0.8547 / 0.6856", "0.8524 / 0.7125"], ["more aug", "1.00E-04", "0.8948 / 0.7490", "0.8972 / 0.7693", "0.8999 / 0.7758"], ["", "1.00E-05", "0.8724 / 0.7370", "0.8685 / 0.7567", "0.8623 / 0.7376"]], "md": "|lr \\ wd|1.00E-04|1.00E-05|1.00E-06|\n|---|---|---|---|\n|ResNet-50|1.00E-03|0.8669 / 0.6405|0.8520 / 0.6013|0.8613 / 0.6269|\n|less aug|1.00E-04|0.8525 / 0.6115|0.8570 / 0.6431|0.8483 / 0.6147|\n| |1.00E-05|0.8186 / 0.5071|0.8287 / 0.5647|0.8288 / 0.5328|\n|ResNet-50|1.00E-03|0.8440 / 0.6432|0.8547 / 0.6856|0.8524 / 0.7125|\n|more aug|1.00E-04|0.8948 / 0.7490|0.8972 / 0.7693|0.8999 / 0.7758|\n| |1.00E-05|0.8724 / 0.7370|0.8685 / 0.7567|0.8623 / 0.7376|", "isPerfectTable": false, "csv": "\"lr \\ wd\",\"1.00E-04\",\"1.00E-05\",\"1.00E-06\"\n\"ResNet-50\",\"1.00E-03\",\"0.8669 / 0.6405\",\"0.8520 / 0.6013\",\"0.8613 / 0.6269\"\n\"less aug\",\"1.00E-04\",\"0.8525 / 0.6115\",\"0.8570 / 0.6431\",\"0.8483 / 0.6147\"\n\"\",\"1.00E-05\",\"0.8186 / 0.5071\",\"0.8287 / 0.5647\",\"0.8288 / 0.5328\"\n\"ResNet-50\",\"1.00E-03\",\"0.8440 / 0.6432\",\"0.8547 / 0.6856\",\"0.8524 / 0.7125\"\n\"more aug\",\"1.00E-04\",\"0.8948 / 0.7490\",\"0.8972 / 0.7693\",\"0.8999 / 0.7758\"\n\"\",\"1.00E-05\",\"0.8724 / 0.7370\",\"0.8685 / 0.7567\",\"0.8623 / 0.7376\""}, {"type": "heading", "lvl": 2, "value": "AUGMENTATION ABLATION DETAILS", "md": "## AUGMENTATION ABLATION DETAILS"}, {"type": "heading", "lvl": 3, "value": "Table 7: Examining the effect of pre-training augmentation and fine-tuning augmentation on downstream transfer performance", "md": "### Table 7: Examining the effect of pre-training augmentation and fine-tuning augmentation on downstream transfer performance"}, {"type": "text", "value": "The model specifies the architecture and pre-training augmentation, while each column specifies the downstream task and fine-tuning augmentation. We find that augmentation strategies that improve ImageNet accuracy do not always improve accuracy on downstream tasks. Pre-trained augmentation models are from Wightman et al. (2021).", "md": "The model specifies the architecture and pre-training augmentation, while each column specifies the downstream task and fine-tuning augmentation. We find that augmentation strategies that improve ImageNet accuracy do not always improve accuracy on downstream tasks. Pre-trained augmentation models are from Wightman et al. (2021)."}, {"type": "table", "rows": [["Model", "ImageNet Acc", "CCT-20 Base Aug", "CCT-20 AugMix", "CCT-20 RandAug", "APTOS Base Aug", "APTOS AugMix", "APTOS RandAug"], ["ResNet-50", "76.1", "72.02", "72.24", "73.57", "0.9210", "0.9212", "0.9250"], ["ResNet-50 w/ AugMix", "77.5", "71.63", "71.53", "72.39", "0.9239", "0.9152", "0.9222"], ["ResNet-50 w/ RandAug", "78.8", "72.94", "73.54", "73.76", "0.9190", "0.9204", "0.9302"], ["Deit-tiny", "72.2", "66.57", "66.47", "66.95", "0.9153", "0.9197", "0.9172"], ["Deit-small", "79.9", "70.65", "69.72", "70.07", "0.9293", "0.9212", "0.9277"]], "md": "|Model|ImageNet Acc|CCT-20 Base Aug|CCT-20 AugMix|CCT-20 RandAug|APTOS Base Aug|APTOS AugMix|APTOS RandAug|\n|---|---|---|---|---|---|---|---|\n|ResNet-50|76.1|72.02|72.24|73.57|0.9210|0.9212|0.9250|\n|ResNet-50 w/ AugMix|77.5|71.63|71.53|72.39|0.9239|0.9152|0.9222|\n|ResNet-50 w/ RandAug|78.8|72.94|73.54|73.76|0.9190|0.9204|0.9302|\n|Deit-tiny|72.2|66.57|66.47|66.95|0.9153|0.9197|0.9172|\n|Deit-small|79.9|70.65|69.72|70.07|0.9293|0.9212|0.9277|", "isPerfectTable": true, "csv": "\"Model\",\"ImageNet Acc\",\"CCT-20 Base Aug\",\"CCT-20 AugMix\",\"CCT-20 RandAug\",\"APTOS Base Aug\",\"APTOS AugMix\",\"APTOS RandAug\"\n\"ResNet-50\",\"76.1\",\"72.02\",\"72.24\",\"73.57\",\"0.9210\",\"0.9212\",\"0.9250\"\n\"ResNet-50 w/ AugMix\",\"77.5\",\"71.63\",\"71.53\",\"72.39\",\"0.9239\",\"0.9152\",\"0.9222\"\n\"ResNet-50 w/ RandAug\",\"78.8\",\"72.94\",\"73.54\",\"73.76\",\"0.9190\",\"0.9204\",\"0.9302\"\n\"Deit-tiny\",\"72.2\",\"66.57\",\"66.47\",\"66.95\",\"0.9153\",\"0.9197\",\"0.9172\"\n\"Deit-small\",\"79.9\",\"70.65\",\"69.72\",\"70.07\",\"0.9293\",\"0.9212\",\"0.9277\""}, {"type": "heading", "lvl": 2, "value": "MELANOMA METRIC COMPARISON", "md": "## MELANOMA METRIC COMPARISON"}, {"type": "text", "value": "SIIM-ISIC Melanoma ROC: 0.97, SIIM-ISIC Melanoma Acc: 96.5\n\nArea under ROC:\n\n0.96: 96.0\n\n0.95: 95.5\n\n0.94: 95.0\n\n0.93: 94.5\n\n0.92: 94.0\n\n0.91: 93.5\n\n0.90: 93.0\n\nImageNet top-1 accuracy: 55 60 65 70 75 80 85\n\nAlexNet, MobileNetV3-small, VGG-13 BN, DeiT-tiny, ResNet-50\n\nResNet-152, PNASNet-5, Inception-ResNet v2, VGG-16 BN\n\nEfficientNet B0, DenseNet-121, ResNeXt-50-32x4d, ShuffleNetV2x1.0\n\nEfficientNet B4, ShuffleNetV2x0.5, SqueezeNet 1.1, ConvNext-tiny", "md": "SIIM-ISIC Melanoma ROC: 0.97, SIIM-ISIC Melanoma Acc: 96.5\n\nArea under ROC:\n\n0.96: 96.0\n\n0.95: 95.5\n\n0.94: 95.0\n\n0.93: 94.5\n\n0.92: 94.0\n\n0.91: 93.5\n\n0.90: 93.0\n\nImageNet top-1 accuracy: 55 60 65 70 75 80 85\n\nAlexNet, MobileNetV3-small, VGG-13 BN, DeiT-tiny, ResNet-50\n\nResNet-152, PNASNet-5, Inception-ResNet v2, VGG-16 BN\n\nEfficientNet B0, DenseNet-121, ResNeXt-50-32x4d, ShuffleNetV2x1.0\n\nEfficientNet B4, ShuffleNetV2x0.5, SqueezeNet 1.1, ConvNext-tiny"}, {"type": "heading", "lvl": 2, "value": "Figure 7: Comparing transfer performance from ImageNet to Melanoma using different metrics", "md": "## Figure 7: Comparing transfer performance from ImageNet to Melanoma using different metrics"}, {"type": "text", "value": "Green linear trend is computed across all models, while blue linear trend is restricted to models above 70% ImageNet accuracy. Using accuracy implies that better ImageNet models transfer better; however, ROC is a better metric for this task.", "md": "Green linear trend is computed across all models, while blue linear trend is restricted to models above 70% ImageNet accuracy. Using accuracy implies that better ImageNet models transfer better; however, ROC is a better metric for this task."}]}, {"page": 22, "text": "I      CLIP EXPERIMENT DETAILS\n             Caltech Camera Traps 20                                  APTOS 2019 Blindness                                     Human Protein Atlas\n                                                          Quadratic weighted kappa                                0.75\n      80                                                   0.94                                                   0.70\n                                                                                                                 Macro F1 score\n                                                                                                                  0.65\n     Accuracy                                              0.92                                                   0.60\n      75\n                                                                                                                  0.55\n      70                                                   0.90                                                   0.50\n                                                           0.88                                                   0.45\n      65                                                                                                          0.40\n                                                                                                                  0.35\n        55     60     65     70     75    80     85            55     60     65     70     75     80    85            55     60     65     70     75    80     85\n               ImageNet top-1 accuracy                                ImageNet top-1 accuracy                                ImageNet top-1 accuracy\n    0.98         SIIM-ISIC Melanoma                          90        Cassava Leaf Disease                                             EuroSAT\n    0.97                                                                                                          99.5\n   Area under ROC\n    0.96                                                     88                                                   99.0\n    0.95                                                    Accuracy                                             Accuracy\n    0.94                                                     86                                                   98.5\n    0.93\n    0.92                                                     84                                                   98.0\n    0.91\n    0.90                                                     82                                                   97.5\n        55     60     65     70     75    80     85            55     60     65     70     75     80    85            55     60     65     70     75    80     85\n               ImageNet top-1 accuracy                                ImageNet top-1 accuracy                                ImageNet top-1 accuracy\n                          AlexNet                             PNASNet-5                              ResNeXt-50-32x4d                     CLIP-RN50\n                          MobileNetV3-small                   Inception-ResNet v2                    ShuffleNetV2x1.0                     CLIP-RN101\n                          VGG-13 BN                           VGG-16 BN                              ConvNext-tiny                        CLIP-B32\n                          DeiT-tiny                           EfficientNet B0                        ShuffleNetV2x0.5                     CLIP-B16\n                          ResNet-50                           EfficientNet B4                        SqueezeNet 1.1                       CLIP-L14\n                          ResNet-152                          DenseNet-121                           ViT-B/16                             CLIP-L14@336\n                          DeiT-small\nFigure 8:        Figure 4 with CLIP models overlaid (purple stars). The best CLIP models do better than all the\nImageNet models, but when looking across all CLIP models, the patterns are more complicated.\nTable 8: For each CLIP pre-trained model, we provide the best performing model when fine-tuned on each\ndataset across our LP-FT hyperparameter grid\n   Model                           ImageNet top-1                 CCT20            APTOS              HPA            Melanoma               Cassava          EuroSAT\n   CLIP-RN50                                73.3                   74.45            0.9135           0.7053             0.9350                87.89             98.80\n   CLIP-RN101                               75.7                   75.19            0.9235           0.6909             0.9378                87.68             99.11\n   CLIP-B32                                 76.1                   70.57            0.9137           0.5338             0.9546                86.28             99.26\n   CLIP-B16                                 80.2                   77.81            0.9213           0.6365             0.9619                87.82             99.24\n   CLIP-L14                                 83.9                   79.99            0.9330           0.6687             0.9717                88.82             99.33\n   CLIP-L14@336                             85.4                   83.17            0.9337           0.7131             0.9738                89.24             99.48\nTable 9: We directly compare models pre-trained on ImageNet with models pre-trained on OpenAI\u2019s CLIP\ndata. Specifically, we look at ResNet 50 and ViT B/16.\n   Model                       ImageNet top-1                 CCT20            APTOS               HPA           Melanoma               Cassava          EuroSAT\n   IN-ResNet-50                          76.1                   73.96           0.9215           0.6718              0.9524               87.75             99.19\n   CLIP-RN50                             73.3                   74.45           0.9135           0.7053              0.9350               87.89             98.80\n   IN-ViT-B/16                           78.7                   72.07           0.9262           0.5852              0.9600               86.63             99.28\n   CLIP-B16                              80.2                   77.81           0.9213           0.6365              0.9619               87.82             99.24\nJ       CLIP FINE-TUNING DETAILS\nWe fine-tune by running a linear probe, followed by end-to-end fine-tuning on the best model from\nthe first part. We keep total epochs consistent with the previous models, with a third of the epochs\ngoing toward linear probing. We use AdamW with a cosine decay schedule. During the linear probe,\nwe search over 10\u22121, 10\u22122, and 10\u22123 learning rates, and during fine-tuning, we search over 10\u22124,\n10\u22125, and 10\u22126 learning rates. For both parts, we search over 10\u22123 to 10\u22126 and 0 for weight decay.\n                                                                                22", "md": "# CLIP Experiment Details\n\n## CLIP Experiment Details\n\n| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|Quadratic weighted kappa|0.75|0.94|0.70|\n|Macro F1 score| |0.65| |\n|Accuracy|0.92|0.60| |\n\n| |ImageNet top-1 accuracy|ImageNet top-1 accuracy|ImageNet top-1 accuracy|\n|---|---|---|---|\n|SIIM-ISIC Melanoma|0.98|90|Cassava Leaf Disease|\n| |0.97| |EuroSAT|\n|Area under ROC|0.96|88|99.0|\n|Accuracy|86|98.5| |\n\n|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|\n|---|---|---|---|---|---|---|---|\n|CLIP-RN50|73.3|74.45|0.9135|0.7053|0.9350|87.89|98.80|\n|CLIP-RN101|75.7|75.19|0.9235|0.6909|0.9378|87.68|99.11|\n|CLIP-B32|76.1|70.57|0.9137|0.5338|0.9546|86.28|99.26|\n|CLIP-B16|80.2|77.81|0.9213|0.6365|0.9619|87.82|99.24|\n|CLIP-L14|83.9|79.99|0.9330|0.6687|0.9717|88.82|99.33|\n|CLIP-L14@336|85.4|83.17|0.9337|0.7131|0.9738|89.24|99.48|\n\n|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|\n|---|---|---|---|---|---|---|---|\n|IN-ResNet-50|76.1|73.96|0.9215|0.6718|0.9524|87.75|99.19|\n|CLIP-RN50|73.3|74.45|0.9135|0.7053|0.9350|87.89|98.80|\n|IN-ViT-B/16|78.7|72.07|0.9262|0.5852|0.9600|86.63|99.28|\n|CLIP-B16|80.2|77.81|0.9213|0.6365|0.9619|87.82|99.24|\n\n### CLIP Fine-Tuning Details\n\nWe fine-tune by running a linear probe, followed by end-to-end fine-tuning on the best model from the first part. We keep total epochs consistent with the previous models, with a third of the epochs going toward linear probing. We use AdamW with a cosine decay schedule. During the linear probe, we search over 10-1, 10-2, and 10-3 learning rates, and during fine-tuning, we search over 10-4, 10-5, and 10-6 learning rates. For both parts, we search over 10-3 to 10-6 and 0 for weight decay.\n\n22", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "CLIP Experiment Details", "md": "# CLIP Experiment Details"}, {"type": "heading", "lvl": 2, "value": "CLIP Experiment Details", "md": "## CLIP Experiment Details"}, {"type": "table", "rows": [["", "Caltech Camera Traps 20", "APTOS 2019 Blindness", "Human Protein Atlas"], ["Quadratic weighted kappa", "0.75", "0.94", "0.70"], ["Macro F1 score", "", "0.65", ""], ["Accuracy", "0.92", "0.60", ""]], "md": "| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|Quadratic weighted kappa|0.75|0.94|0.70|\n|Macro F1 score| |0.65| |\n|Accuracy|0.92|0.60| |", "isPerfectTable": true, "csv": "\"\",\"Caltech Camera Traps 20\",\"APTOS 2019 Blindness\",\"Human Protein Atlas\"\n\"Quadratic weighted kappa\",\"0.75\",\"0.94\",\"0.70\"\n\"Macro F1 score\",\"\",\"0.65\",\"\"\n\"Accuracy\",\"0.92\",\"0.60\",\"\""}, {"type": "table", "rows": [["", "ImageNet top-1 accuracy", "ImageNet top-1 accuracy", "ImageNet top-1 accuracy"], ["SIIM-ISIC Melanoma", "0.98", "90", "Cassava Leaf Disease"], ["", "0.97", "", "EuroSAT"], ["Area under ROC", "0.96", "88", "99.0"], ["Accuracy", "86", "98.5", ""]], "md": "| |ImageNet top-1 accuracy|ImageNet top-1 accuracy|ImageNet top-1 accuracy|\n|---|---|---|---|\n|SIIM-ISIC Melanoma|0.98|90|Cassava Leaf Disease|\n| |0.97| |EuroSAT|\n|Area under ROC|0.96|88|99.0|\n|Accuracy|86|98.5| |", "isPerfectTable": true, "csv": "\"\",\"ImageNet top-1 accuracy\",\"ImageNet top-1 accuracy\",\"ImageNet top-1 accuracy\"\n\"SIIM-ISIC Melanoma\",\"0.98\",\"90\",\"Cassava Leaf Disease\"\n\"\",\"0.97\",\"\",\"EuroSAT\"\n\"Area under ROC\",\"0.96\",\"88\",\"99.0\"\n\"Accuracy\",\"86\",\"98.5\",\"\""}, {"type": "table", "rows": [["Model", "ImageNet top-1", "CCT20", "APTOS", "HPA", "Melanoma", "Cassava", "EuroSAT"], ["CLIP-RN50", "73.3", "74.45", "0.9135", "0.7053", "0.9350", "87.89", "98.80"], ["CLIP-RN101", "75.7", "75.19", "0.9235", "0.6909", "0.9378", "87.68", "99.11"], ["CLIP-B32", "76.1", "70.57", "0.9137", "0.5338", "0.9546", "86.28", "99.26"], ["CLIP-B16", "80.2", "77.81", "0.9213", "0.6365", "0.9619", "87.82", "99.24"], ["CLIP-L14", "83.9", "79.99", "0.9330", "0.6687", "0.9717", "88.82", "99.33"], ["CLIP-L14@336", "85.4", "83.17", "0.9337", "0.7131", "0.9738", "89.24", "99.48"]], "md": "|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|\n|---|---|---|---|---|---|---|---|\n|CLIP-RN50|73.3|74.45|0.9135|0.7053|0.9350|87.89|98.80|\n|CLIP-RN101|75.7|75.19|0.9235|0.6909|0.9378|87.68|99.11|\n|CLIP-B32|76.1|70.57|0.9137|0.5338|0.9546|86.28|99.26|\n|CLIP-B16|80.2|77.81|0.9213|0.6365|0.9619|87.82|99.24|\n|CLIP-L14|83.9|79.99|0.9330|0.6687|0.9717|88.82|99.33|\n|CLIP-L14@336|85.4|83.17|0.9337|0.7131|0.9738|89.24|99.48|", "isPerfectTable": true, "csv": "\"Model\",\"ImageNet top-1\",\"CCT20\",\"APTOS\",\"HPA\",\"Melanoma\",\"Cassava\",\"EuroSAT\"\n\"CLIP-RN50\",\"73.3\",\"74.45\",\"0.9135\",\"0.7053\",\"0.9350\",\"87.89\",\"98.80\"\n\"CLIP-RN101\",\"75.7\",\"75.19\",\"0.9235\",\"0.6909\",\"0.9378\",\"87.68\",\"99.11\"\n\"CLIP-B32\",\"76.1\",\"70.57\",\"0.9137\",\"0.5338\",\"0.9546\",\"86.28\",\"99.26\"\n\"CLIP-B16\",\"80.2\",\"77.81\",\"0.9213\",\"0.6365\",\"0.9619\",\"87.82\",\"99.24\"\n\"CLIP-L14\",\"83.9\",\"79.99\",\"0.9330\",\"0.6687\",\"0.9717\",\"88.82\",\"99.33\"\n\"CLIP-L14@336\",\"85.4\",\"83.17\",\"0.9337\",\"0.7131\",\"0.9738\",\"89.24\",\"99.48\""}, {"type": "table", "rows": [["Model", "ImageNet top-1", "CCT20", "APTOS", "HPA", "Melanoma", "Cassava", "EuroSAT"], ["IN-ResNet-50", "76.1", "73.96", "0.9215", "0.6718", "0.9524", "87.75", "99.19"], ["CLIP-RN50", "73.3", "74.45", "0.9135", "0.7053", "0.9350", "87.89", "98.80"], ["IN-ViT-B/16", "78.7", "72.07", "0.9262", "0.5852", "0.9600", "86.63", "99.28"], ["CLIP-B16", "80.2", "77.81", "0.9213", "0.6365", "0.9619", "87.82", "99.24"]], "md": "|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|\n|---|---|---|---|---|---|---|---|\n|IN-ResNet-50|76.1|73.96|0.9215|0.6718|0.9524|87.75|99.19|\n|CLIP-RN50|73.3|74.45|0.9135|0.7053|0.9350|87.89|98.80|\n|IN-ViT-B/16|78.7|72.07|0.9262|0.5852|0.9600|86.63|99.28|\n|CLIP-B16|80.2|77.81|0.9213|0.6365|0.9619|87.82|99.24|", "isPerfectTable": true, "csv": "\"Model\",\"ImageNet top-1\",\"CCT20\",\"APTOS\",\"HPA\",\"Melanoma\",\"Cassava\",\"EuroSAT\"\n\"IN-ResNet-50\",\"76.1\",\"73.96\",\"0.9215\",\"0.6718\",\"0.9524\",\"87.75\",\"99.19\"\n\"CLIP-RN50\",\"73.3\",\"74.45\",\"0.9135\",\"0.7053\",\"0.9350\",\"87.89\",\"98.80\"\n\"IN-ViT-B/16\",\"78.7\",\"72.07\",\"0.9262\",\"0.5852\",\"0.9600\",\"86.63\",\"99.28\"\n\"CLIP-B16\",\"80.2\",\"77.81\",\"0.9213\",\"0.6365\",\"0.9619\",\"87.82\",\"99.24\""}, {"type": "heading", "lvl": 3, "value": "CLIP Fine-Tuning Details", "md": "### CLIP Fine-Tuning Details"}, {"type": "text", "value": "We fine-tune by running a linear probe, followed by end-to-end fine-tuning on the best model from the first part. We keep total epochs consistent with the previous models, with a third of the epochs going toward linear probing. We use AdamW with a cosine decay schedule. During the linear probe, we search over 10-1, 10-2, and 10-3 learning rates, and during fine-tuning, we search over 10-4, 10-5, and 10-6 learning rates. For both parts, we search over 10-3 to 10-6 and 0 for weight decay.\n\n22", "md": "We fine-tune by running a linear probe, followed by end-to-end fine-tuning on the best model from the first part. We keep total epochs consistent with the previous models, with a third of the epochs going toward linear probing. We use AdamW with a cosine decay schedule. During the linear probe, we search over 10-1, 10-2, and 10-3 learning rates, and during fine-tuning, we search over 10-4, 10-5, and 10-6 learning rates. For both parts, we search over 10-3 to 10-6 and 0 for weight decay.\n\n22"}]}, {"page": 23, "text": "K        CREATION INFORMATION FOR DATASETS STUDIED IN KORNBLITH ET AL.\n         (2019)\n         Table 10: We find that the 12 datasets studied in Kornblith et al. (2019) come from web scraping.\n   Dataset                                     Origin                                  Additional information\n   Food-101                                    foodspotting.com                        Users upload an image of their food and anno-\n                                                                                       tate the type of food; categories chosen by pop-\n                                                                                       ularity\n   CIFAR-10                                    TinyImages                              Web crawl\n   CIFAR-100                                   TinyImages                              Web crawl\n   Birdsnap                                    Flickr                                  Also used MTurk\n   SUN397                                      Web search engines                      Also used WordNet\n   Stanford Cars                               Flickr,              Google,            Also used MTurk\n                                               Bing\n   FGVC Aircraft                               airliners.net                           Images taken by 10 photographers\n   Pascal VOC 2007 Cls.                        Flickr                                  N/A\n   Describable Textures                        Google and Flickr                       Also used MTurk\n   Oxford-IIT Pets                             Flickr, Google,                         Catster and Dogster are social websites for col-\n                                               Catster, Dogster                        lecting and discussing pet images\n   Caltech-101                                 Google                                  97 categories chosen from Webster Collegiate\n                                                                                       Dictionary categories associated with a drawing\n   Oxford 102 Flowers                          Mostly collected                        A small number of images acquired by the pa-\n                                               from web                                per authors taking the pictures\nL       RELATIONSHIP BETWEEN MODEL SIZE AND TRANSFER PERFORMANCE\n             Caltech Camera Traps 20                                     APTOS 2019 Blindness                                        Human Protein Atlas\n   80.0                                                     Quadratic weighted kappa                                    0.8\n   77.5                                                      0.94                                                       0.7\n   75.0                                                                                                                Macro F1 score\n  Accuracy                                                   0.92\n   72.5                                                                                                                 0.6\n   70.0                                                      0.90\n   67.5                                                                                                                 0.5\n   65.0                                                      0.88                                                       0.4\n   62.5  0     20    40     60    80   100    120   140            0     20    40    60     80   100    120   140            0     20    40    60     80   100  120  140\n            # of parameters (in millions)                             # of parameters (in millions)                             # of parameters (in millions)\n                 SIIM-ISIC Melanoma                            90         Cassava Leaf Disease                        99.50                   EuroSAT\n   0.97                                                                                                               99.25\n   0.96\n  Area under ROC                                               88                                                     99.00\n   0.95                                                       Accuracy                                               Accuracy\n                                                                                                                      98.75\n   0.94                                                        86                                                     98.50\n   0.93                                                                                                               98.25\n   0.92                                                        84                                                     98.00\n   0.91                                                        82                                                     97.75\n   0.90                                                                                                               97.50\n         0     20    40     60    80   100    120   140            0     20    40    60     80   100    120   140            0     20    40    60     80   100  120  140\n            # of parameters (in millions)                             # of parameters (in millions)                             # of parameters (in millions)\n                        AlexNet                                ResNet-152                              EfficientNet B0                        ShuffleNetV2x1.0\n                        MobileNetV3-small                      DeiT-small                              EfficientNet B4                        ConvNext-tiny\n                        VGG-13 BN                              PNASNet-5                               DenseNet-121                           ShuffleNetV2x0.5\n                        DeiT-tiny                              Inception-ResNet v2                     ResNeXt-50-32x4d                       SqueezeNet 1.1\n                        ResNet-50                              VGG-16 BN\nFigure 9: We compare model size with downstream transfer performance. Again we use separate trend lines\nfor all models (green) and only those above 70% ImageNet accuracy (blue). We use 95% confidence intervals\ncomputed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.\n                                                                                    23", "md": "# Creation Information for Datasets Studied in Kornblith et al. (2019)\n\n## Creation Information for Datasets Studied in Kornblith et al. (2019)\n\nTable 10: We find that the 12 datasets studied in Kornblith et al. (2019) come from web scraping.\n\n|Dataset|Origin|Additional information|\n|---|---|---|\n|Food-101|foodspotting.com|Users upload an image of their food and annotate the type of food; categories chosen by popularity|\n|CIFAR-10|TinyImages|Web crawl|\n|CIFAR-100|TinyImages|Web crawl|\n|Birdsnap|Flickr|Also used MTurk|\n|SUN397|Web search engines|Also used WordNet|\n|Stanford Cars|Flickr, Google, Bing|Also used MTurk|\n|FGVC Aircraft|airliners.net|Images taken by 10 photographers|\n|Pascal VOC 2007 Cls.|Flickr|N/A|\n|Describable Textures|Google and Flickr|Also used MTurk|\n|Oxford-IIT Pets|Flickr, Google, Catster, Dogster|Catster and Dogster are social websites for collecting and discussing pet images|\n|Caltech-101|Google|97 categories chosen from Webster Collegiate Dictionary categories associated with a drawing|\n|Oxford 102 Flowers|Mostly collected from web|A small number of images acquired by the paper authors taking the pictures|\n\n## Relationship Between Model Size and Transfer Performance\n\nCaltech Camera Traps 20\n\n| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|80.0|Quadratic weighted kappa| |0.8|\n|77.5|0.94| |0.7|\n|75.0| | |Macro F1 score|\n|Accuracy|0.92| | |\n|72.5| | |0.6|\n|70.0|0.90| | |\n|67.5| | |0.5|\n|65.0|0.88| |0.4|\n|62.5|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|\n|# of parameters (in millions)|SIIM-ISIC Melanoma|90|Cassava Leaf Disease|99.50|EuroSAT|\n|0.97| | | |99.25|\n|0.96| | |Area under ROC|88| |99.00|\n|0.95|Accuracy| | |98.75|\n|0.94| | |86| |98.50|\n|0.93| | | |98.25|\n|0.92| | |84| |98.00|\n|0.91| | |82| |97.75|\n|0.90| | | |97.50|\n|0.89|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|\n|# of parameters (in millions)|AlexNet|ResNet-152|EfficientNet B0|ShuffleNetV2x1.0|\n| |MobileNetV3-small|DeiT-small|EfficientNet B4|ConvNext-tiny|\n| |VGG-13 BN|PNASNet-5|DenseNet-121|ShuffleNetV2x0.5|\n| |DeiT-tiny|Inception-ResNet v2|ResNeXt-50-32x4d|SqueezeNet 1.1|\n| |ResNet-50|VGG-16 BN| | |\n\nFigure 9: We compare model size with downstream transfer performance. Again we use separate trend lines for all models (green) and only those above 70% ImageNet accuracy (blue). We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Creation Information for Datasets Studied in Kornblith et al. (2019)", "md": "# Creation Information for Datasets Studied in Kornblith et al. (2019)"}, {"type": "heading", "lvl": 2, "value": "Creation Information for Datasets Studied in Kornblith et al. (2019)", "md": "## Creation Information for Datasets Studied in Kornblith et al. (2019)"}, {"type": "text", "value": "Table 10: We find that the 12 datasets studied in Kornblith et al. (2019) come from web scraping.", "md": "Table 10: We find that the 12 datasets studied in Kornblith et al. (2019) come from web scraping."}, {"type": "table", "rows": [["Dataset", "Origin", "Additional information"], ["Food-101", "foodspotting.com", "Users upload an image of their food and annotate the type of food; categories chosen by popularity"], ["CIFAR-10", "TinyImages", "Web crawl"], ["CIFAR-100", "TinyImages", "Web crawl"], ["Birdsnap", "Flickr", "Also used MTurk"], ["SUN397", "Web search engines", "Also used WordNet"], ["Stanford Cars", "Flickr, Google, Bing", "Also used MTurk"], ["FGVC Aircraft", "airliners.net", "Images taken by 10 photographers"], ["Pascal VOC 2007 Cls.", "Flickr", "N/A"], ["Describable Textures", "Google and Flickr", "Also used MTurk"], ["Oxford-IIT Pets", "Flickr, Google, Catster, Dogster", "Catster and Dogster are social websites for collecting and discussing pet images"], ["Caltech-101", "Google", "97 categories chosen from Webster Collegiate Dictionary categories associated with a drawing"], ["Oxford 102 Flowers", "Mostly collected from web", "A small number of images acquired by the paper authors taking the pictures"]], "md": "|Dataset|Origin|Additional information|\n|---|---|---|\n|Food-101|foodspotting.com|Users upload an image of their food and annotate the type of food; categories chosen by popularity|\n|CIFAR-10|TinyImages|Web crawl|\n|CIFAR-100|TinyImages|Web crawl|\n|Birdsnap|Flickr|Also used MTurk|\n|SUN397|Web search engines|Also used WordNet|\n|Stanford Cars|Flickr, Google, Bing|Also used MTurk|\n|FGVC Aircraft|airliners.net|Images taken by 10 photographers|\n|Pascal VOC 2007 Cls.|Flickr|N/A|\n|Describable Textures|Google and Flickr|Also used MTurk|\n|Oxford-IIT Pets|Flickr, Google, Catster, Dogster|Catster and Dogster are social websites for collecting and discussing pet images|\n|Caltech-101|Google|97 categories chosen from Webster Collegiate Dictionary categories associated with a drawing|\n|Oxford 102 Flowers|Mostly collected from web|A small number of images acquired by the paper authors taking the pictures|", "isPerfectTable": true, "csv": "\"Dataset\",\"Origin\",\"Additional information\"\n\"Food-101\",\"foodspotting.com\",\"Users upload an image of their food and annotate the type of food; categories chosen by popularity\"\n\"CIFAR-10\",\"TinyImages\",\"Web crawl\"\n\"CIFAR-100\",\"TinyImages\",\"Web crawl\"\n\"Birdsnap\",\"Flickr\",\"Also used MTurk\"\n\"SUN397\",\"Web search engines\",\"Also used WordNet\"\n\"Stanford Cars\",\"Flickr, Google, Bing\",\"Also used MTurk\"\n\"FGVC Aircraft\",\"airliners.net\",\"Images taken by 10 photographers\"\n\"Pascal VOC 2007 Cls.\",\"Flickr\",\"N/A\"\n\"Describable Textures\",\"Google and Flickr\",\"Also used MTurk\"\n\"Oxford-IIT Pets\",\"Flickr, Google, Catster, Dogster\",\"Catster and Dogster are social websites for collecting and discussing pet images\"\n\"Caltech-101\",\"Google\",\"97 categories chosen from Webster Collegiate Dictionary categories associated with a drawing\"\n\"Oxford 102 Flowers\",\"Mostly collected from web\",\"A small number of images acquired by the paper authors taking the pictures\""}, {"type": "heading", "lvl": 2, "value": "Relationship Between Model Size and Transfer Performance", "md": "## Relationship Between Model Size and Transfer Performance"}, {"type": "text", "value": "Caltech Camera Traps 20", "md": "Caltech Camera Traps 20"}, {"type": "table", "rows": [["", "Caltech Camera Traps 20", "APTOS 2019 Blindness", "Human Protein Atlas"], ["80.0", "Quadratic weighted kappa", "", "0.8"], ["77.5", "0.94", "", "0.7"], ["75.0", "", "", "Macro F1 score"], ["Accuracy", "0.92", "", ""], ["72.5", "", "", "0.6"], ["70.0", "0.90", "", ""], ["67.5", "", "", "0.5"], ["65.0", "0.88", "", "0.4"], ["62.5", "0", "20", "40", "60", "80", "100", "120", "140", "0", "20", "40", "60", "80", "100", "120", "140", "0", "20", "40", "60", "80", "100", "120", "140"], ["# of parameters (in millions)", "SIIM-ISIC Melanoma", "90", "Cassava Leaf Disease", "99.50", "EuroSAT"], ["0.97", "", "", "", "99.25"], ["0.96", "", "", "Area under ROC", "88", "", "99.00"], ["0.95", "Accuracy", "", "", "98.75"], ["0.94", "", "", "86", "", "98.50"], ["0.93", "", "", "", "98.25"], ["0.92", "", "", "84", "", "98.00"], ["0.91", "", "", "82", "", "97.75"], ["0.90", "", "", "", "97.50"], ["0.89", "0", "20", "40", "60", "80", "100", "120", "140", "0", "20", "40", "60", "80", "100", "120", "140", "0", "20", "40", "60", "80", "100", "120", "140"], ["# of parameters (in millions)", "AlexNet", "ResNet-152", "EfficientNet B0", "ShuffleNetV2x1.0"], ["", "MobileNetV3-small", "DeiT-small", "EfficientNet B4", "ConvNext-tiny"], ["", "VGG-13 BN", "PNASNet-5", "DenseNet-121", "ShuffleNetV2x0.5"], ["", "DeiT-tiny", "Inception-ResNet v2", "ResNeXt-50-32x4d", "SqueezeNet 1.1"], ["", "ResNet-50", "VGG-16 BN", "", ""]], "md": "| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|\n|---|---|---|---|\n|80.0|Quadratic weighted kappa| |0.8|\n|77.5|0.94| |0.7|\n|75.0| | |Macro F1 score|\n|Accuracy|0.92| | |\n|72.5| | |0.6|\n|70.0|0.90| | |\n|67.5| | |0.5|\n|65.0|0.88| |0.4|\n|62.5|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|\n|# of parameters (in millions)|SIIM-ISIC Melanoma|90|Cassava Leaf Disease|99.50|EuroSAT|\n|0.97| | | |99.25|\n|0.96| | |Area under ROC|88| |99.00|\n|0.95|Accuracy| | |98.75|\n|0.94| | |86| |98.50|\n|0.93| | | |98.25|\n|0.92| | |84| |98.00|\n|0.91| | |82| |97.75|\n|0.90| | | |97.50|\n|0.89|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|\n|# of parameters (in millions)|AlexNet|ResNet-152|EfficientNet B0|ShuffleNetV2x1.0|\n| |MobileNetV3-small|DeiT-small|EfficientNet B4|ConvNext-tiny|\n| |VGG-13 BN|PNASNet-5|DenseNet-121|ShuffleNetV2x0.5|\n| |DeiT-tiny|Inception-ResNet v2|ResNeXt-50-32x4d|SqueezeNet 1.1|\n| |ResNet-50|VGG-16 BN| | |", "isPerfectTable": false, "csv": "\"\",\"Caltech Camera Traps 20\",\"APTOS 2019 Blindness\",\"Human Protein Atlas\"\n\"80.0\",\"Quadratic weighted kappa\",\"\",\"0.8\"\n\"77.5\",\"0.94\",\"\",\"0.7\"\n\"75.0\",\"\",\"\",\"Macro F1 score\"\n\"Accuracy\",\"0.92\",\"\",\"\"\n\"72.5\",\"\",\"\",\"0.6\"\n\"70.0\",\"0.90\",\"\",\"\"\n\"67.5\",\"\",\"\",\"0.5\"\n\"65.0\",\"0.88\",\"\",\"0.4\"\n\"62.5\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"140\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"140\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"140\"\n\"# of parameters (in millions)\",\"SIIM-ISIC Melanoma\",\"90\",\"Cassava Leaf Disease\",\"99.50\",\"EuroSAT\"\n\"0.97\",\"\",\"\",\"\",\"99.25\"\n\"0.96\",\"\",\"\",\"Area under ROC\",\"88\",\"\",\"99.00\"\n\"0.95\",\"Accuracy\",\"\",\"\",\"98.75\"\n\"0.94\",\"\",\"\",\"86\",\"\",\"98.50\"\n\"0.93\",\"\",\"\",\"\",\"98.25\"\n\"0.92\",\"\",\"\",\"84\",\"\",\"98.00\"\n\"0.91\",\"\",\"\",\"82\",\"\",\"97.75\"\n\"0.90\",\"\",\"\",\"\",\"97.50\"\n\"0.89\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"140\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"140\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"140\"\n\"# of parameters (in millions)\",\"AlexNet\",\"ResNet-152\",\"EfficientNet B0\",\"ShuffleNetV2x1.0\"\n\"\",\"MobileNetV3-small\",\"DeiT-small\",\"EfficientNet B4\",\"ConvNext-tiny\"\n\"\",\"VGG-13 BN\",\"PNASNet-5\",\"DenseNet-121\",\"ShuffleNetV2x0.5\"\n\"\",\"DeiT-tiny\",\"Inception-ResNet v2\",\"ResNeXt-50-32x4d\",\"SqueezeNet 1.1\"\n\"\",\"ResNet-50\",\"VGG-16 BN\",\"\",\"\""}, {"type": "text", "value": "Figure 9: We compare model size with downstream transfer performance. Again we use separate trend lines for all models (green) and only those above 70% ImageNet accuracy (blue). We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.", "md": "Figure 9: We compare model size with downstream transfer performance. Again we use separate trend lines for all models (green) and only those above 70% ImageNet accuracy (blue). We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics."}]}, {"page": 24, "text": "M      FID SCORE DETAILS\nTable 11: We calculate FID scores between the ImageNet validation set and each of the datasets we study, as\nwell as between the ImageNet validation set and each of the datasets in Kornblith et al. (2019). We found that\ndataset size affects FID score, so we take a 3,662 subset of each downstream dataset. Note that 3,662 is the size\nof APTOS, which is the smallest dataset.\n                                      Dataset                        FID\n                                      CCT-20                       162.69\n                                      APTOS                        196.24\n                                      HPA                          230.70\n                                      Cassava                      179.24\n                                      Melanoma                     186.34\n                                      EuroSAT                      151.85\n                                      Food-101                     108.35\n                                      CIFAR-10                     132.53\n                                      CIFAR-100                    120.72\n                                      Birdsnap                      94.08\n                                      SUN397                        62.95\n                                      Stanford Cars                143.35\n                                      FGVC Aircraft                183.35\n                                      Pascal VOC 2007 Cls.          39.84\n                                      Describable Textures          89.13\n                                      Oxford-IIT Pets               77.27\n                                      Caltech-101                   50.77\n                                      Oxford 102 Flowers           140.21\nN     PREDICTIVE POWER OF ACCURACY ON NON-WEB-SCRAPED DATASETS ON\n      NOVEL DATASETS\nWe observe that, on many non-web-scraped datasets, accuracy correlates only weakly with Ima-\ngeNet accuracy. It is thus worth asking whether other predictors might correlate better. In this\nsection, we examine the extent to which accuracy on a given non-web-scraped target dataset can be\npredicted from the accuracy on the other non-web-scraped target datasets.\nN.1     F-TEST\nWe can further measure the extent to which the averages of the fi          ve other datasets beyond the pre-\ndictive power provided by ImageNet by using F-tests. For each target task, we fit a linear regression\nmodel that predicts accuracy as either ImageNet accuracy or the average accuracy on the other fi               ve\nnon-web-scraped datasets, and a second linear regression model that predicts accuracy as a func-\ntion of both ImageNet accuracy and the average accuracy on the other fi            ve datasets. Since the first\nmodel is nested within the second, the second model must explain at least as much variance as the\nfirst. The F-test measures whether the increase in explained variance is significant. For these ex-\nperiments, we logit-transform accuracy values and standardize them to zero mean and unit variance\nbefore computing the averages, as in the middle column of Table 13.\nResults are shown in Table 12. The average accuracy across the other fi          ve datasets explains variance\nbeyond that explained by ImageNet accuracy alone on fi           ve of the six datasets. The only exception\nis EuroSAT, where the range of accuracies is low (most models get \u223c99%) and a significant fraction\nof the variance among models may correspond to noise. By contrast, ImageNet accuracy explains\nvariance beyond the average accuracy only on two datasets (APTOS and Melanoma). These results\nindicate that there are patterns in how well different models transfer to non-web-scraped data that\nare not captured by ImageNet accuracy alone, but are captured by the accuracy on other non-web-\nscraped datasets.\n                                                       24", "md": "# FID Scores and Predictive Power of Accuracy\n\n## Table 11: FID Scores\n\n| Dataset              | FID   |\n|----------------------|-------|\n| CCT-20               | 162.69|\n| APTOS                | 196.24|\n| HPA                  | 230.70|\n| Cassava              | 179.24|\n| Melanoma             | 186.34|\n| EuroSAT              | 151.85|\n| Food-101             | 108.35|\n| CIFAR-10             | 132.53|\n| CIFAR-100            | 120.72|\n| Birdsnap             | 94.08 |\n| SUN397               | 62.95 |\n| Stanford Cars        | 143.35|\n| FGVC Aircraft        | 183.35|\n| Pascal VOC 2007 Cls. | 39.84 |\n| Describable Textures  | 89.13 |\n| Oxford-IIT Pets      | 77.27 |\n| Caltech-101          | 50.77 |\n| Oxford 102 Flowers   | 140.21|\n\n## Predictive Power of Accuracy on Non-Web-Scraped Datasets on Novel Datasets\n\nWe observe that, on many non-web-scraped datasets, accuracy correlates only weakly with ImageNet accuracy. It is thus worth asking whether other predictors might correlate better. In this section, we examine the extent to which accuracy on a given non-web-scraped target dataset can be predicted from the accuracy on the other non-web-scraped target datasets.\n\n### N.1 F-TEST\n\nWe can further measure the extent to which the averages of the five other datasets beyond the predictive power provided by ImageNet by using F-tests. For each target task, we fit a linear regression model that predicts accuracy as either ImageNet accuracy or the average accuracy on the other five non-web-scraped datasets, and a second linear regression model that predicts accuracy as a function of both ImageNet accuracy and the average accuracy on the other five datasets. Since the first model is nested within the second, the second model must explain at least as much variance as the first. The F-test measures whether the increase in explained variance is significant. For these experiments, we logit-transform accuracy values and standardize them to zero mean and unit variance before computing the averages, as in the middle column of Table 13.\n\nResults are shown in Table 12. The average accuracy across the other five datasets explains variance beyond that explained by ImageNet accuracy alone on five of the six datasets. The only exception is EuroSAT, where the range of accuracies is low (most models get approximately 99%) and a significant fraction of the variance among models may correspond to noise. By contrast, ImageNet accuracy explains variance beyond the average accuracy only on two datasets (APTOS and Melanoma). These results indicate that there are patterns in how well different models transfer to non-web-scraped data that are not captured by ImageNet accuracy alone, but are captured by the accuracy on other non-web-scraped datasets.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "FID Scores and Predictive Power of Accuracy", "md": "# FID Scores and Predictive Power of Accuracy"}, {"type": "heading", "lvl": 2, "value": "Table 11: FID Scores", "md": "## Table 11: FID Scores"}, {"type": "table", "rows": [["Dataset", "FID"], ["CCT-20", "162.69"], ["APTOS", "196.24"], ["HPA", "230.70"], ["Cassava", "179.24"], ["Melanoma", "186.34"], ["EuroSAT", "151.85"], ["Food-101", "108.35"], ["CIFAR-10", "132.53"], ["CIFAR-100", "120.72"], ["Birdsnap", "94.08"], ["SUN397", "62.95"], ["Stanford Cars", "143.35"], ["FGVC Aircraft", "183.35"], ["Pascal VOC 2007 Cls.", "39.84"], ["Describable Textures", "89.13"], ["Oxford-IIT Pets", "77.27"], ["Caltech-101", "50.77"], ["Oxford 102 Flowers", "140.21"]], "md": "| Dataset              | FID   |\n|----------------------|-------|\n| CCT-20               | 162.69|\n| APTOS                | 196.24|\n| HPA                  | 230.70|\n| Cassava              | 179.24|\n| Melanoma             | 186.34|\n| EuroSAT              | 151.85|\n| Food-101             | 108.35|\n| CIFAR-10             | 132.53|\n| CIFAR-100            | 120.72|\n| Birdsnap             | 94.08 |\n| SUN397               | 62.95 |\n| Stanford Cars        | 143.35|\n| FGVC Aircraft        | 183.35|\n| Pascal VOC 2007 Cls. | 39.84 |\n| Describable Textures  | 89.13 |\n| Oxford-IIT Pets      | 77.27 |\n| Caltech-101          | 50.77 |\n| Oxford 102 Flowers   | 140.21|", "isPerfectTable": true, "csv": "\"Dataset\",\"FID\"\n\"CCT-20\",\"162.69\"\n\"APTOS\",\"196.24\"\n\"HPA\",\"230.70\"\n\"Cassava\",\"179.24\"\n\"Melanoma\",\"186.34\"\n\"EuroSAT\",\"151.85\"\n\"Food-101\",\"108.35\"\n\"CIFAR-10\",\"132.53\"\n\"CIFAR-100\",\"120.72\"\n\"Birdsnap\",\"94.08\"\n\"SUN397\",\"62.95\"\n\"Stanford Cars\",\"143.35\"\n\"FGVC Aircraft\",\"183.35\"\n\"Pascal VOC 2007 Cls.\",\"39.84\"\n\"Describable Textures\",\"89.13\"\n\"Oxford-IIT Pets\",\"77.27\"\n\"Caltech-101\",\"50.77\"\n\"Oxford 102 Flowers\",\"140.21\""}, {"type": "heading", "lvl": 2, "value": "Predictive Power of Accuracy on Non-Web-Scraped Datasets on Novel Datasets", "md": "## Predictive Power of Accuracy on Non-Web-Scraped Datasets on Novel Datasets"}, {"type": "text", "value": "We observe that, on many non-web-scraped datasets, accuracy correlates only weakly with ImageNet accuracy. It is thus worth asking whether other predictors might correlate better. In this section, we examine the extent to which accuracy on a given non-web-scraped target dataset can be predicted from the accuracy on the other non-web-scraped target datasets.", "md": "We observe that, on many non-web-scraped datasets, accuracy correlates only weakly with ImageNet accuracy. It is thus worth asking whether other predictors might correlate better. In this section, we examine the extent to which accuracy on a given non-web-scraped target dataset can be predicted from the accuracy on the other non-web-scraped target datasets."}, {"type": "heading", "lvl": 3, "value": "N.1 F-TEST", "md": "### N.1 F-TEST"}, {"type": "text", "value": "We can further measure the extent to which the averages of the five other datasets beyond the predictive power provided by ImageNet by using F-tests. For each target task, we fit a linear regression model that predicts accuracy as either ImageNet accuracy or the average accuracy on the other five non-web-scraped datasets, and a second linear regression model that predicts accuracy as a function of both ImageNet accuracy and the average accuracy on the other five datasets. Since the first model is nested within the second, the second model must explain at least as much variance as the first. The F-test measures whether the increase in explained variance is significant. For these experiments, we logit-transform accuracy values and standardize them to zero mean and unit variance before computing the averages, as in the middle column of Table 13.\n\nResults are shown in Table 12. The average accuracy across the other five datasets explains variance beyond that explained by ImageNet accuracy alone on five of the six datasets. The only exception is EuroSAT, where the range of accuracies is low (most models get approximately 99%) and a significant fraction of the variance among models may correspond to noise. By contrast, ImageNet accuracy explains variance beyond the average accuracy only on two datasets (APTOS and Melanoma). These results indicate that there are patterns in how well different models transfer to non-web-scraped data that are not captured by ImageNet accuracy alone, but are captured by the accuracy on other non-web-scraped datasets.", "md": "We can further measure the extent to which the averages of the five other datasets beyond the predictive power provided by ImageNet by using F-tests. For each target task, we fit a linear regression model that predicts accuracy as either ImageNet accuracy or the average accuracy on the other five non-web-scraped datasets, and a second linear regression model that predicts accuracy as a function of both ImageNet accuracy and the average accuracy on the other five datasets. Since the first model is nested within the second, the second model must explain at least as much variance as the first. The F-test measures whether the increase in explained variance is significant. For these experiments, we logit-transform accuracy values and standardize them to zero mean and unit variance before computing the averages, as in the middle column of Table 13.\n\nResults are shown in Table 12. The average accuracy across the other five datasets explains variance beyond that explained by ImageNet accuracy alone on five of the six datasets. The only exception is EuroSAT, where the range of accuracies is low (most models get approximately 99%) and a significant fraction of the variance among models may correspond to noise. By contrast, ImageNet accuracy explains variance beyond the average accuracy only on two datasets (APTOS and Melanoma). These results indicate that there are patterns in how well different models transfer to non-web-scraped data that are not captured by ImageNet accuracy alone, but are captured by the accuracy on other non-web-scraped datasets."}]}, {"page": 25, "text": "Table 12: Results of the F-test described in Section N.1. \u201c+Avg. across datasets\u201d tests whether a model that\nincludes both ImageNet accuracy and the average accuracy across the 5 other datasets explains more variance\nthan a model that includes only ImageNet accuracy. \u201c+ImageNet\u201d tests whether a model that includes both\npredictors explains more variance than a model that includes only the average accuracy across the 5 other\ndatasets. In addition to F and p values, we report adjusted R2 for all models. p-values < 0.05 are bold-faced.\n                      +Avg. across datasets           +ImageNet                Adj. R2               Adj. R2              Adj. R2\n     Dataset        F (1, 16)        p-value     F (1, 16)     p-value     (ImageNet-only)        (Average-only)      (Both predictors)\n     CCT-20                8.2          0.01          0.69         0.42                 0.56                 0.70                  0.69\n     APTOS                31.0      0.00004             4.6      0.047                  0.34                 0.71                  0.76\n     HPA                  11.8         0.003          0.84         0.37                 0.60                 0.76                  0.76\n     Melanoma              5.8          0.03            7.8        0.01                 0.74                 0.71                  0.79\n     Cassava              13.2         0.002          0.14         0.71                 0.55                 0.75                  0.74\n     EuroSAT               2.9          0.11          0.72         0.41                 0.43                 0.52                  0.49\nN.2      SPEARMAN CORRELATION\nTable 13: We measure the Spearman correlation between each dataset with either the average of the 5 other\ndatasets we study, or with ImageNet. Normalization is done by logit transforming accuracies, and then stan-\ndardizing to zero mean and unit variance. The results suggest that using additional datasets is more predictive\nof model performance than just using ImageNet.\n                                     Avg of 5 others                    Avg of 5 others                    ImageNet\n                                     (unnormalized)                       (normalized)\n              Dataset                \u03c1            p-value               \u03c1            p-value              \u03c1         p-value\n              CCT-20             0.8684           0.0000            0.9263           0.0000           0.5825         0.0089\n              APTOS              0.7205           0.0005            0.6950           0.0010           0.3010         0.2105\n              HPA                0.7351           0.0003            0.6825           0.0013           0.6491         0.0026\n              Melanoma           0.6561           0.0023            0.7807           0.0000           0.7667         0.0001\n              Cassava            0.8872           0.0000            0.7442           0.0003           0.5222         0.0218\n              EuroSAT            0.3030           0.2073            0.3821           0.1065           0.4734         0.0406\n                                                                    25", "md": "|Dataset|+Avg. across datasets| |+ImageNet|Adj. R2|\n|---|---|---|---|---|\n| |F (1, 16)|p-value|F (1, 16)|p-value|(ImageNet-only)|(Average-only)|(Both predictors)|\n|CCT-20|8.2|0.01|0.69|0.42|0.56|0.70|0.69|\n|APTOS|31.0|0.00004|4.6|0.047|0.34|0.71|0.76|\n|HPA|11.8|0.003|0.84|0.37|0.60|0.76|0.76|\n|Melanoma|5.8|0.03|7.8|0.01|0.74|0.71|0.79|\n|Cassava|13.2|0.002|0.14|0.71|0.55|0.75|0.74|\n|EuroSAT|2.9|0.11|0.72|0.41|0.43|0.52|0.49|\n\nN.2 SPEARMAN CORRELATION\n\n|Dataset|Avg of 5 others (unnormalized)|Avg of 5 others (normalized)|ImageNet|\n|---|---|---|---|\n| |\u03c1|p-value|\u03c1|p-value|\u03c1|p-value|\n|CCT-20|0.8684|0.0000|0.9263|0.0000|0.5825|0.0089|\n|APTOS|0.7205|0.0005|0.6950|0.0010|0.3010|0.2105|\n|HPA|0.7351|0.0003|0.6825|0.0013|0.6491|0.0026|\n|Melanoma|0.6561|0.0023|0.7807|0.0000|0.7667|0.0001|\n|Cassava|0.8872|0.0000|0.7442|0.0003|0.5222|0.0218|\n|EuroSAT|0.3030|0.2073|0.3821|0.1065|0.4734|0.0406|", "images": [], "items": [{"type": "table", "rows": [["Dataset", "+Avg. across datasets", "", "+ImageNet", "Adj. R2"], ["", "F (1, 16)", "p-value", "F (1, 16)", "p-value", "(ImageNet-only)", "(Average-only)", "(Both predictors)"], ["CCT-20", "8.2", "0.01", "0.69", "0.42", "0.56", "0.70", "0.69"], ["APTOS", "31.0", "0.00004", "4.6", "0.047", "0.34", "0.71", "0.76"], ["HPA", "11.8", "0.003", "0.84", "0.37", "0.60", "0.76", "0.76"], ["Melanoma", "5.8", "0.03", "7.8", "0.01", "0.74", "0.71", "0.79"], ["Cassava", "13.2", "0.002", "0.14", "0.71", "0.55", "0.75", "0.74"], ["EuroSAT", "2.9", "0.11", "0.72", "0.41", "0.43", "0.52", "0.49"]], "md": "|Dataset|+Avg. across datasets| |+ImageNet|Adj. R2|\n|---|---|---|---|---|\n| |F (1, 16)|p-value|F (1, 16)|p-value|(ImageNet-only)|(Average-only)|(Both predictors)|\n|CCT-20|8.2|0.01|0.69|0.42|0.56|0.70|0.69|\n|APTOS|31.0|0.00004|4.6|0.047|0.34|0.71|0.76|\n|HPA|11.8|0.003|0.84|0.37|0.60|0.76|0.76|\n|Melanoma|5.8|0.03|7.8|0.01|0.74|0.71|0.79|\n|Cassava|13.2|0.002|0.14|0.71|0.55|0.75|0.74|\n|EuroSAT|2.9|0.11|0.72|0.41|0.43|0.52|0.49|", "isPerfectTable": false, "csv": "\"Dataset\",\"+Avg. across datasets\",\"\",\"+ImageNet\",\"Adj. R2\"\n\"\",\"F (1, 16)\",\"p-value\",\"F (1, 16)\",\"p-value\",\"(ImageNet-only)\",\"(Average-only)\",\"(Both predictors)\"\n\"CCT-20\",\"8.2\",\"0.01\",\"0.69\",\"0.42\",\"0.56\",\"0.70\",\"0.69\"\n\"APTOS\",\"31.0\",\"0.00004\",\"4.6\",\"0.047\",\"0.34\",\"0.71\",\"0.76\"\n\"HPA\",\"11.8\",\"0.003\",\"0.84\",\"0.37\",\"0.60\",\"0.76\",\"0.76\"\n\"Melanoma\",\"5.8\",\"0.03\",\"7.8\",\"0.01\",\"0.74\",\"0.71\",\"0.79\"\n\"Cassava\",\"13.2\",\"0.002\",\"0.14\",\"0.71\",\"0.55\",\"0.75\",\"0.74\"\n\"EuroSAT\",\"2.9\",\"0.11\",\"0.72\",\"0.41\",\"0.43\",\"0.52\",\"0.49\""}, {"type": "text", "value": "N.2 SPEARMAN CORRELATION", "md": "N.2 SPEARMAN CORRELATION"}, {"type": "table", "rows": [["Dataset", "Avg of 5 others (unnormalized)", "Avg of 5 others (normalized)", "ImageNet"], ["", "\u03c1", "p-value", "\u03c1", "p-value", "\u03c1", "p-value"], ["CCT-20", "0.8684", "0.0000", "0.9263", "0.0000", "0.5825", "0.0089"], ["APTOS", "0.7205", "0.0005", "0.6950", "0.0010", "0.3010", "0.2105"], ["HPA", "0.7351", "0.0003", "0.6825", "0.0013", "0.6491", "0.0026"], ["Melanoma", "0.6561", "0.0023", "0.7807", "0.0000", "0.7667", "0.0001"], ["Cassava", "0.8872", "0.0000", "0.7442", "0.0003", "0.5222", "0.0218"], ["EuroSAT", "0.3030", "0.2073", "0.3821", "0.1065", "0.4734", "0.0406"]], "md": "|Dataset|Avg of 5 others (unnormalized)|Avg of 5 others (normalized)|ImageNet|\n|---|---|---|---|\n| |\u03c1|p-value|\u03c1|p-value|\u03c1|p-value|\n|CCT-20|0.8684|0.0000|0.9263|0.0000|0.5825|0.0089|\n|APTOS|0.7205|0.0005|0.6950|0.0010|0.3010|0.2105|\n|HPA|0.7351|0.0003|0.6825|0.0013|0.6491|0.0026|\n|Melanoma|0.6561|0.0023|0.7807|0.0000|0.7667|0.0001|\n|Cassava|0.8872|0.0000|0.7442|0.0003|0.5222|0.0218|\n|EuroSAT|0.3030|0.2073|0.3821|0.1065|0.4734|0.0406|", "isPerfectTable": false, "csv": "\"Dataset\",\"Avg of 5 others (unnormalized)\",\"Avg of 5 others (normalized)\",\"ImageNet\"\n\"\",\"\u03c1\",\"p-value\",\"\u03c1\",\"p-value\",\"\u03c1\",\"p-value\"\n\"CCT-20\",\"0.8684\",\"0.0000\",\"0.9263\",\"0.0000\",\"0.5825\",\"0.0089\"\n\"APTOS\",\"0.7205\",\"0.0005\",\"0.6950\",\"0.0010\",\"0.3010\",\"0.2105\"\n\"HPA\",\"0.7351\",\"0.0003\",\"0.6825\",\"0.0013\",\"0.6491\",\"0.0026\"\n\"Melanoma\",\"0.6561\",\"0.0023\",\"0.7807\",\"0.0000\",\"0.7667\",\"0.0001\"\n\"Cassava\",\"0.8872\",\"0.0000\",\"0.7442\",\"0.0003\",\"0.5222\",\"0.0218\"\n\"EuroSAT\",\"0.3030\",\"0.2073\",\"0.3821\",\"0.1065\",\"0.4734\",\"0.0406\""}]}, {"page": 26, "text": "O         PRE-TRAINING AUGMENTATION DETAILS\nTable 14: For each ImageNet pre-trained model, we provide the augmentation strategy used during pre-training\ntime.\n  Model                                       Augmentation\n  AlexNet                                     Resize + Crop + Flip\n  SqueezeNet 1.1                              Resize + Crop + Flip\n  ShuffleNetV2x0.5                            AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix\n  MobileNet V3 small                          AutoAugment (ImageNet/Default)+ RandErasing\n  ShuffleNetV2x1.0                            AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix\n  VGG-13 BN                                   Resize + Crop + Flip\n  DeiT-tiny                                   RandAugment + RandErasing\n  VGG-16 BN                                   Resize + Crop + Flip\n  DenseNet-121                                Resize + Crop + Flip\n  ResNet-50                                   Resize + Crop + Flip\n  ResNeXt-50-32x4d                            Resize + Crop + Flip\n  EfficientNet B0                             RandAugment\n  ResNet-152                                  Resize + Crop + Flip\n  ViT-B/16                                    RandAugment + MixUp\n  DeiT-small                                  RandAugment + RandErasing\n  Inception-ResNet v2                         Inception Preprocessing (Color Distort + Resize + Crop + Flip)\n  ConvNext-tiny                               AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix\n  PNASNet-5 large                             Whiten + Resize + Crop + Flip\n  EfficientNet B4                             RandAugment\n                      Caltech Camera Traps 20                                  APTOS 2019 Blindness                                    Human Protein Atlas\n                                                                   Quadratic weighted kappa                                0.75\n              78                                                    0.94                                                   0.70\n              76                                                                                                          Macro F1 score\n              74                                                                                                           0.65\n             Accuracy                                               0.92                                                   0.60\n              72\n              70                                                    0.90                                                   0.55\n              68                                                                                                           0.50\n              66                                                    0.88                                                   0.45\n              64                                                                                                           0.40\n                                                                                                                           0.35\n                55      60     65     70      75     80      85         55     60     65      70     75     80      85         55     60     65      70     75    80  85\n                       ImageNet top-1 accuracy                                ImageNet top-1 accuracy                                ImageNet top-1 accuracy\n            0.97          SIIM-ISIC Melanoma                         90        Cassava Leaf Disease                       99.50                 EuroSAT\n            0.96                                                                                                          99.25\n           Area under ROC                                            88                                                   99.00\n            0.95\n                                                                    Accuracy                                             Accuracy\n            0.94                                                     86                                                   98.75\n            0.93                                                                                                          98.50\n            0.92                                                     84                                                   98.25\n                                                                                                                          98.00\n            0.91                                                     82                                                   97.75\n            0.90                                                                                                          97.50\n                55      60     65     70      75     80      85         55     60     65      70     75     80      85         55     60     65      70     75    80  85\n                       ImageNet top-1 accuracy                                ImageNet top-1 accuracy                                ImageNet top-1 accuracy\n                                AlexNet                              ResNet-152                             EfficientNet B0                     ConvNext-tiny\n                                MobileNetV3-small                    DeiT-small                             EfficientNet B4                     ShuffleNetV2x0.5\n                                VGG-13 BN                            PNASNet-5                              DenseNet-121                        SqueezeNet 1.1\n                                DeiT-tiny                            Inception-ResNet v2                    ResNeXt-50-32x4d                    ViT-B/16\n                                ResNet-50                            VGG-16 BN                              ShuffleNetV2x1.0\nFigure 10: Figure 1 with points colored by general pre-training augmentation strategy. Cyan points use simple\naugmentation (resize, crops, flips, etc.), and red points use automatic augmentation (RandAugment, AutoAug-\nment, TrivialAugmentWide).\n                                                                                         26", "md": "# Pre-training Augmentation Details\n\n## Pre-training Augmentation Details\n\n### Table 14: Pre-training Augmentation Strategy\n\n|Model|Augmentation|\n|---|---|\n|AlexNet|Resize + Crop + Flip|\n|SqueezeNet 1.1|Resize + Crop + Flip|\n|ShuffleNetV2x0.5|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|\n|MobileNet V3 small|AutoAugment (ImageNet/Default) + RandErasing|\n|ShuffleNetV2x1.0|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|\n|VGG-13 BN|Resize + Crop + Flip|\n|DeiT-tiny|RandAugment + RandErasing|\n|VGG-16 BN|Resize + Crop + Flip|\n|DenseNet-121|Resize + Crop + Flip|\n|ResNet-50|Resize + Crop + Flip|\n|ResNeXt-50-32x4d|Resize + Crop + Flip|\n|EfficientNet B0|RandAugment|\n|ResNet-152|Resize + Crop + Flip|\n|ViT-B/16|RandAugment + MixUp|\n|DeiT-small|RandAugment + RandErasing|\n|Inception-ResNet v2|Inception Preprocessing (Color Distort + Resize + Crop + Flip)|\n|ConvNext-tiny|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|\n|PNASNet-5 large|Whiten + Resize + Crop + Flip|\n|EfficientNet B4|RandAugment|\n\n### Caltech Camera Traps 20, APTOS 2019 Blindness, Human Protein Atlas\n\n| |Quadratic weighted kappa|Macro F1 score|Accuracy|\n|---|---|---|---|\n|78|0.75|0.70|0.92|\n|76|0.94| |0.60|\n|74| |0.65| |\n|72|0.90| |0.55|\n|70| |0.50| |\n|68|0.88| |0.45|\n|66| |0.40| |\n| | |0.35| |\n\n### ImageNet Top-1 Accuracy\n\n| |ImageNet top-1 accuracy|ImageNet top-1 accuracy|ImageNet top-1 accuracy|\n|---|---|---|---|\n| |SIIM-ISIC Melanoma|Cassava Leaf Disease|EuroSAT|\n|0.97|90|99.50| |\n|0.96| |99.25| |\n|Area under ROC|88|99.00| |\n|0.95| | | |\n|Accuracy|86|98.75| |\n|0.93| |98.50| |\n|0.92|84|98.25| |\n| | |98.00| |\n|0.90|82|97.75| |\n|0.89| |97.50| |\n\n### Models with General Pre-training Augmentation Strategy\n\n- AlexNet\n- ResNet-152\n- EfficientNet B0\n- ConvNext-tiny\n- MobileNetV3-small\n- DeiT-small\n- EfficientNet B4\n- ShuffleNetV2x0.5\n- VGG-13 BN\n- PNASNet-5\n- DenseNet-121\n- SqueezeNet 1.1\n- DeiT-tiny\n- Inception-ResNet v2\n- ResNeXt-50-32x4d\n- ViT-B/16\n- ResNet-50\n- VGG-16 BN\n- ShuffleNetV2x1.0\n\n### Figure 10: Pre-training Augmentation Strategy\n\nFigure 1 with points colored by general pre-training augmentation strategy. Cyan points use simple augmentation (resize, crops, flips, etc.), and red points use automatic augmentation (RandAugment, AutoAugment, TrivialAugmentWide).\n\n26", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Pre-training Augmentation Details", "md": "# Pre-training Augmentation Details"}, {"type": "heading", "lvl": 2, "value": "Pre-training Augmentation Details", "md": "## Pre-training Augmentation Details"}, {"type": "heading", "lvl": 3, "value": "Table 14: Pre-training Augmentation Strategy", "md": "### Table 14: Pre-training Augmentation Strategy"}, {"type": "table", "rows": [["Model", "Augmentation"], ["AlexNet", "Resize + Crop + Flip"], ["SqueezeNet 1.1", "Resize + Crop + Flip"], ["ShuffleNetV2x0.5", "AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix"], ["MobileNet V3 small", "AutoAugment (ImageNet/Default) + RandErasing"], ["ShuffleNetV2x1.0", "AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix"], ["VGG-13 BN", "Resize + Crop + Flip"], ["DeiT-tiny", "RandAugment + RandErasing"], ["VGG-16 BN", "Resize + Crop + Flip"], ["DenseNet-121", "Resize + Crop + Flip"], ["ResNet-50", "Resize + Crop + Flip"], ["ResNeXt-50-32x4d", "Resize + Crop + Flip"], ["EfficientNet B0", "RandAugment"], ["ResNet-152", "Resize + Crop + Flip"], ["ViT-B/16", "RandAugment + MixUp"], ["DeiT-small", "RandAugment + RandErasing"], ["Inception-ResNet v2", "Inception Preprocessing (Color Distort + Resize + Crop + Flip)"], ["ConvNext-tiny", "AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix"], ["PNASNet-5 large", "Whiten + Resize + Crop + Flip"], ["EfficientNet B4", "RandAugment"]], "md": "|Model|Augmentation|\n|---|---|\n|AlexNet|Resize + Crop + Flip|\n|SqueezeNet 1.1|Resize + Crop + Flip|\n|ShuffleNetV2x0.5|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|\n|MobileNet V3 small|AutoAugment (ImageNet/Default) + RandErasing|\n|ShuffleNetV2x1.0|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|\n|VGG-13 BN|Resize + Crop + Flip|\n|DeiT-tiny|RandAugment + RandErasing|\n|VGG-16 BN|Resize + Crop + Flip|\n|DenseNet-121|Resize + Crop + Flip|\n|ResNet-50|Resize + Crop + Flip|\n|ResNeXt-50-32x4d|Resize + Crop + Flip|\n|EfficientNet B0|RandAugment|\n|ResNet-152|Resize + Crop + Flip|\n|ViT-B/16|RandAugment + MixUp|\n|DeiT-small|RandAugment + RandErasing|\n|Inception-ResNet v2|Inception Preprocessing (Color Distort + Resize + Crop + Flip)|\n|ConvNext-tiny|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|\n|PNASNet-5 large|Whiten + Resize + Crop + Flip|\n|EfficientNet B4|RandAugment|", "isPerfectTable": true, "csv": "\"Model\",\"Augmentation\"\n\"AlexNet\",\"Resize + Crop + Flip\"\n\"SqueezeNet 1.1\",\"Resize + Crop + Flip\"\n\"ShuffleNetV2x0.5\",\"AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix\"\n\"MobileNet V3 small\",\"AutoAugment (ImageNet/Default) + RandErasing\"\n\"ShuffleNetV2x1.0\",\"AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix\"\n\"VGG-13 BN\",\"Resize + Crop + Flip\"\n\"DeiT-tiny\",\"RandAugment + RandErasing\"\n\"VGG-16 BN\",\"Resize + Crop + Flip\"\n\"DenseNet-121\",\"Resize + Crop + Flip\"\n\"ResNet-50\",\"Resize + Crop + Flip\"\n\"ResNeXt-50-32x4d\",\"Resize + Crop + Flip\"\n\"EfficientNet B0\",\"RandAugment\"\n\"ResNet-152\",\"Resize + Crop + Flip\"\n\"ViT-B/16\",\"RandAugment + MixUp\"\n\"DeiT-small\",\"RandAugment + RandErasing\"\n\"Inception-ResNet v2\",\"Inception Preprocessing (Color Distort + Resize + Crop + Flip)\"\n\"ConvNext-tiny\",\"AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix\"\n\"PNASNet-5 large\",\"Whiten + Resize + Crop + Flip\"\n\"EfficientNet B4\",\"RandAugment\""}, {"type": "heading", "lvl": 3, "value": "Caltech Camera Traps 20, APTOS 2019 Blindness, Human Protein Atlas", "md": "### Caltech Camera Traps 20, APTOS 2019 Blindness, Human Protein Atlas"}, {"type": "table", "rows": [["", "Quadratic weighted kappa", "Macro F1 score", "Accuracy"], ["78", "0.75", "0.70", "0.92"], ["76", "0.94", "", "0.60"], ["74", "", "0.65", ""], ["72", "0.90", "", "0.55"], ["70", "", "0.50", ""], ["68", "0.88", "", "0.45"], ["66", "", "0.40", ""], ["", "", "0.35", ""]], "md": "| |Quadratic weighted kappa|Macro F1 score|Accuracy|\n|---|---|---|---|\n|78|0.75|0.70|0.92|\n|76|0.94| |0.60|\n|74| |0.65| |\n|72|0.90| |0.55|\n|70| |0.50| |\n|68|0.88| |0.45|\n|66| |0.40| |\n| | |0.35| |", "isPerfectTable": true, "csv": "\"\",\"Quadratic weighted kappa\",\"Macro F1 score\",\"Accuracy\"\n\"78\",\"0.75\",\"0.70\",\"0.92\"\n\"76\",\"0.94\",\"\",\"0.60\"\n\"74\",\"\",\"0.65\",\"\"\n\"72\",\"0.90\",\"\",\"0.55\"\n\"70\",\"\",\"0.50\",\"\"\n\"68\",\"0.88\",\"\",\"0.45\"\n\"66\",\"\",\"0.40\",\"\"\n\"\",\"\",\"0.35\",\"\""}, {"type": "heading", "lvl": 3, "value": "ImageNet Top-1 Accuracy", "md": "### ImageNet Top-1 Accuracy"}, {"type": "table", "rows": [["", "ImageNet top-1 accuracy", "ImageNet top-1 accuracy", "ImageNet top-1 accuracy"], ["", "SIIM-ISIC Melanoma", "Cassava Leaf Disease", "EuroSAT"], ["0.97", "90", "99.50", ""], ["0.96", "", "99.25", ""], ["Area under ROC", "88", "99.00", ""], ["0.95", "", "", ""], ["Accuracy", "86", "98.75", ""], ["0.93", "", "98.50", ""], ["0.92", "84", "98.25", ""], ["", "", "98.00", ""], ["0.90", "82", "97.75", ""], ["0.89", "", "97.50", ""]], "md": "| |ImageNet top-1 accuracy|ImageNet top-1 accuracy|ImageNet top-1 accuracy|\n|---|---|---|---|\n| |SIIM-ISIC Melanoma|Cassava Leaf Disease|EuroSAT|\n|0.97|90|99.50| |\n|0.96| |99.25| |\n|Area under ROC|88|99.00| |\n|0.95| | | |\n|Accuracy|86|98.75| |\n|0.93| |98.50| |\n|0.92|84|98.25| |\n| | |98.00| |\n|0.90|82|97.75| |\n|0.89| |97.50| |", "isPerfectTable": true, "csv": "\"\",\"ImageNet top-1 accuracy\",\"ImageNet top-1 accuracy\",\"ImageNet top-1 accuracy\"\n\"\",\"SIIM-ISIC Melanoma\",\"Cassava Leaf Disease\",\"EuroSAT\"\n\"0.97\",\"90\",\"99.50\",\"\"\n\"0.96\",\"\",\"99.25\",\"\"\n\"Area under ROC\",\"88\",\"99.00\",\"\"\n\"0.95\",\"\",\"\",\"\"\n\"Accuracy\",\"86\",\"98.75\",\"\"\n\"0.93\",\"\",\"98.50\",\"\"\n\"0.92\",\"84\",\"98.25\",\"\"\n\"\",\"\",\"98.00\",\"\"\n\"0.90\",\"82\",\"97.75\",\"\"\n\"0.89\",\"\",\"97.50\",\"\""}, {"type": "heading", "lvl": 3, "value": "Models with General Pre-training Augmentation Strategy", "md": "### Models with General Pre-training Augmentation Strategy"}, {"type": "text", "value": "- AlexNet\n- ResNet-152\n- EfficientNet B0\n- ConvNext-tiny\n- MobileNetV3-small\n- DeiT-small\n- EfficientNet B4\n- ShuffleNetV2x0.5\n- VGG-13 BN\n- PNASNet-5\n- DenseNet-121\n- SqueezeNet 1.1\n- DeiT-tiny\n- Inception-ResNet v2\n- ResNeXt-50-32x4d\n- ViT-B/16\n- ResNet-50\n- VGG-16 BN\n- ShuffleNetV2x1.0", "md": "- AlexNet\n- ResNet-152\n- EfficientNet B0\n- ConvNext-tiny\n- MobileNetV3-small\n- DeiT-small\n- EfficientNet B4\n- ShuffleNetV2x0.5\n- VGG-13 BN\n- PNASNet-5\n- DenseNet-121\n- SqueezeNet 1.1\n- DeiT-tiny\n- Inception-ResNet v2\n- ResNeXt-50-32x4d\n- ViT-B/16\n- ResNet-50\n- VGG-16 BN\n- ShuffleNetV2x1.0"}, {"type": "heading", "lvl": 3, "value": "Figure 10: Pre-training Augmentation Strategy", "md": "### Figure 10: Pre-training Augmentation Strategy"}, {"type": "text", "value": "Figure 1 with points colored by general pre-training augmentation strategy. Cyan points use simple augmentation (resize, crops, flips, etc.), and red points use automatic augmentation (RandAugment, AutoAugment, TrivialAugmentWide).\n\n26", "md": "Figure 1 with points colored by general pre-training augmentation strategy. Cyan points use simple augmentation (resize, crops, flips, etc.), and red points use automatic augmentation (RandAugment, AutoAugment, TrivialAugmentWide).\n\n26"}]}], "job_id": "8af6201e-b9c7-4c2c-a4a7-2145dbe2f605", "file_path": "./corpus/2301.04644.pdf"}
