{"pages": [{"page": 1, "text": "                      Cluster-aware Semi-supervised Learning: Relational\n                        Knowledge Distillation Provably Learns Clustering\n                                             Yijun Dong\u2217                                       Kevin Miller\u2217\n                             Courant Institute of Mathematical Sciences              Oden Institute for Computational\n                                         New York University                               Engineering & Science\narXiv:2307.11030v2  [stat.ML]  23 Oct 2023  New York, NY                               University of Texas at Austin\n                                         yd1319@nyu.edu                                          Austin, TX\n                                                                                        ksmiller@utexas.edu\n                                                 Qi Lei                                        Rachel Ward\n                             Courant Institute of Mathematical Sciences              Oden Institute for Computational\n                                      & Center of Data Science                             Engineering & Science\n                                         New York University                           University of Texas at Austin\n                                            New York, NY                                         Austin, TX\n                                          ql518@nyu.edu                               rward@math.utexas.edu\n                                                                     Abstract\n                               Despite the empirical success and practical significance of (relational) knowledge\n                               distillation that matches (the relations of) features between teacher and student\n                               models, the corresponding theoretical interpretations remain limited for various\n                               knowledge distillation paradigms. In this work, we take an initial step toward a\n                               theoretical understanding of relational knowledge distillation (RKD), with a focus\n                               on semi-supervised classification problems. We start by casting RKD as spectral\n                               clustering on a population-induced graph unveiled by a teacher model. Via a notion\n                               of clustering error that quantifies the discrepancy between the predicted and ground\n                               truth clusterings, we illustrate that RKD over the population provably leads to\n                               low clustering error. Moreover, we provide a sample complexity bound for RKD\n                               with limited unlabeled samples. For semi-supervised learning, we further demon-\n                               strate the label efficiency of RKD through a general framework of cluster-aware\n                               semi-supervised learning that assumes low clustering errors. Finally, by unifying\n                               data augmentation consistency regularization into this cluster-aware framework,\n                               we show that despite the common effect of learning accurate clusterings, RKD\n                               facilitates a \u201cglobal\u201d perspective through spectral clustering, whereas consistency\n                               regularization focuses on a \u201clocal\u201d perspective via expansion.\n                     1    Introduction\n                     The immense volume of training data is a vital driving force behind the unparalleled power of modern\n                     deep neural networks. Nevertheless, data collection can be quite resource-intensive; in particular,\n                     data labeling may be prohibitively costly. With an aim to lessen the workload associated with data\n                     labeling, semi-supervised learning capitalizes on more easily accessible unlabeled data via diverse\n                     approaches of pseudo-labeling. For instance, data augmentation consistency regularization [Sohn\n                     et al., 2020, Berthelot et al., 2019] generates pseudo-labels from the current model predictions on\n                     carefully designed data augmentations. Alternatively, knowledge distillation [Hinton et al., 2015] can\n                     be leveraged to gauge representations of unlabeled samples via pretrained teacher models.\n                        \u2217Equal contribution.\n                     37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering\n\n# Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering\n\nYijun Dong* Courant Institute of Mathematical Sciences, New York University\n\nKevin Miller* Oden Institute for Computational Engineering & Science, University of Texas at Austin\n\nQi Lei Courant Institute of Mathematical Sciences & Center of Data Science, New York University\n\nRachel Ward Oden Institute for Computational Engineering & Science, University of Texas at Austin\n\n## Abstract\n\nDespite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning that assumes low clustering errors. Finally, by unifying data augmentation consistency regularization into this cluster-aware framework, we show that despite the common effect of learning accurate clusterings, RKD facilitates a \u201cglobal\u201d perspective through spectral clustering, whereas consistency regularization focuses on a \u201clocal\u201d perspective via expansion.\n\n## Introduction\n\nThe immense volume of training data is a vital driving force behind the unparalleled power of modern deep neural networks. Nevertheless, data collection can be quite resource-intensive; in particular, data labeling may be prohibitively costly. With an aim to lessen the workload associated with data labeling, semi-supervised learning capitalizes on more easily accessible unlabeled data via diverse approaches of pseudo-labeling. For instance, data augmentation consistency regularization [Sohn et al., 2020, Berthelot et al., 2019] generates pseudo-labels from the current model predictions on carefully designed data augmentations. Alternatively, knowledge distillation [Hinton et al., 2015] can be leveraged to gauge representations of unlabeled samples via pretrained teacher models.\n\n*Equal contribution.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering", "md": "# Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering"}, {"type": "heading", "lvl": 1, "value": "Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering", "md": "# Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering"}, {"type": "text", "value": "Yijun Dong* Courant Institute of Mathematical Sciences, New York University\n\nKevin Miller* Oden Institute for Computational Engineering & Science, University of Texas at Austin\n\nQi Lei Courant Institute of Mathematical Sciences & Center of Data Science, New York University\n\nRachel Ward Oden Institute for Computational Engineering & Science, University of Texas at Austin", "md": "Yijun Dong* Courant Institute of Mathematical Sciences, New York University\n\nKevin Miller* Oden Institute for Computational Engineering & Science, University of Texas at Austin\n\nQi Lei Courant Institute of Mathematical Sciences & Center of Data Science, New York University\n\nRachel Ward Oden Institute for Computational Engineering & Science, University of Texas at Austin"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning that assumes low clustering errors. Finally, by unifying data augmentation consistency regularization into this cluster-aware framework, we show that despite the common effect of learning accurate clusterings, RKD facilitates a \u201cglobal\u201d perspective through spectral clustering, whereas consistency regularization focuses on a \u201clocal\u201d perspective via expansion.", "md": "Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning that assumes low clustering errors. Finally, by unifying data augmentation consistency regularization into this cluster-aware framework, we show that despite the common effect of learning accurate clusterings, RKD facilitates a \u201cglobal\u201d perspective through spectral clustering, whereas consistency regularization focuses on a \u201clocal\u201d perspective via expansion."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "The immense volume of training data is a vital driving force behind the unparalleled power of modern deep neural networks. Nevertheless, data collection can be quite resource-intensive; in particular, data labeling may be prohibitively costly. With an aim to lessen the workload associated with data labeling, semi-supervised learning capitalizes on more easily accessible unlabeled data via diverse approaches of pseudo-labeling. For instance, data augmentation consistency regularization [Sohn et al., 2020, Berthelot et al., 2019] generates pseudo-labels from the current model predictions on carefully designed data augmentations. Alternatively, knowledge distillation [Hinton et al., 2015] can be leveraged to gauge representations of unlabeled samples via pretrained teacher models.\n\n*Equal contribution.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "The immense volume of training data is a vital driving force behind the unparalleled power of modern deep neural networks. Nevertheless, data collection can be quite resource-intensive; in particular, data labeling may be prohibitively costly. With an aim to lessen the workload associated with data labeling, semi-supervised learning capitalizes on more easily accessible unlabeled data via diverse approaches of pseudo-labeling. For instance, data augmentation consistency regularization [Sohn et al., 2020, Berthelot et al., 2019] generates pseudo-labels from the current model predictions on carefully designed data augmentations. Alternatively, knowledge distillation [Hinton et al., 2015] can be leveraged to gauge representations of unlabeled samples via pretrained teacher models.\n\n*Equal contribution.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "Figure 1: The complementary perspectives of RKD and DAC regarding the data. RKD learns the\npairwise relations among data (e.g., edge wxx\u2032 that characterizes the similarity between x and x\u2032)\nover the population \u201cglobally\u201d via spectral clustering, as illustrated in Section 4. Alternatively,\nDAC discovers the \u201clocal\u201d clustering structure through the expansion of neighborhoods based on\noverlapping augmentation sets (e.g., A(x) \u2229        A (x\u2032\u2032) \u0338= \u2205), as elaborated in Section 6.\nMeanwhile, as the scale of training state-of-the-art models explodes, computational costs and limited\ndata are becoming significant obstacles to learning from scratch. In this context, (data-free) knowledge\ndistillation [Liu et al., 2021, Wang and Yoon, 2021] (where pretrained models are taken as teacher\noracles without access to their associated training data) is taking an increasingly crucial role in\nlearning efficiently from powerful existing models.\nDespite the substantial progresses in knowledge distillation (KD) in practice, its theoretical under-\nstanding remains limited, possibly due to the abundant diversity of KD strategies. Most existing\ntheoretical analyses of KD [Ji and Zhu, 2020, Allen-Zhu and Li, 2020, Harutyunyan et al., 2023,\nHsu et al., 2021, Wang et al., 2022] focus on feature matching between the teacher and student\nmodels [Hinton et al., 2015], while a much broader spectrum of KD algorithms [Liu et al., 2019, Park\net al., 2019, Chen et al., 2021, Qian et al., 2022] demonstrates appealing performance in practice.\nRecently, relational knowledge distillation (RKD) [Park et al., 2019, Liu et al., 2019] has achieved\nremarkable empirical successes by matching the inter-feature relationships (instead of features them-\nselves) between the teacher and student models. This drives us to focus on RKD and motivates our\nanalysis with the following question:\n   What perspective of the data does relational knowledge distillation learn, and how efficiently?\n(RKD learns spectral clustering.)          In light of the connection between RKD and instance relation-\nship graphs (IRG) [Liu et al., 2019]2, intuitively, RKD learns the ground truth geometry through a\ngraph induced by the teacher model. We formalize this intuition from a spectral clustering perspective\n(Section 4). Specifically, for a multi-class classification problem, we introduce a notion of clustering\nerror (Definition 3.1) that measures the difference between the predicted and ground truth class\npartitions. Through low clustering error guarantees, we illustrate that RKD over the population\nlearns the ground truth partition (presumably unveiled by the teacher model) via spectral clustering\n(Section 4.1). Moreover, RKD on sufficiently many unlabeled samples learns a similar partition with\nlow clustering error (Section 4.2). By appealing to standard generalization analysis [Wainwright,\n2019, Bartlett and Mendelson, 2003, Wei and Ma, 2019] for unbiased estimates of the parameterized\nRKD loss, we show that the unlabeled sample complexity of RKD is dominated by a polynomial term\nin the model complexity (Theorem 4.2, Theorem 4.3).\nAs one of the most influential and classical unsupervised learning strategies, clustering remains an\nessential aspect of modern learning algorithms with weak/no supervision. For example, contrastive\nlearning [Caron et al., 2020, Grill et al., 2020, Chen and He, 2021] has proven to learn good\nrepresentations without any labels by encouraging (spectral) clustering [HaoChen et al., 2021, Lee\net al., 2021, Parulekar et al., 2023]. However, the effects of clustering in the related schemes of semi-\nsupervised learning (SSL) are less well investigated. Therefore, we pose a follow-up question:\n   How does cluster-aware semi-supervised learning improve label efficiency and generalization?\n    2An IRG is a graph with samples as vertices and sample relationships, characterized by their mutual distances\nin a feature space, as edges. RKD encourages the student model to learn such IRGs from the teacher model.\n                                                        2", "md": "# Complementary Perspectives of RKD and DAC\n\n## Figure 1: The complementary perspectives of RKD and DAC regarding the data\n\nRKD learns the pairwise relations among data (e.g., edge $$w_{xx'}$$ that characterizes the similarity between x and x') over the population \"globally\" via spectral clustering, as illustrated in Section 4. Alternatively, DAC discovers the \"local\" clustering structure through the expansion of neighborhoods based on overlapping augmentation sets (e.g., $$A(x) \\cap A(x'') \\neq \\emptyset$$), as elaborated in Section 6.\n\nMeanwhile, as the scale of training state-of-the-art models explodes, computational costs and limited data are becoming significant obstacles to learning from scratch. In this context, (data-free) knowledge distillation (where pretrained models are taken as teacher oracles without access to their associated training data) is taking an increasingly crucial role in learning efficiently from powerful existing models.\n\nDespite the substantial progresses in knowledge distillation (KD) in practice, its theoretical understanding remains limited, possibly due to the abundant diversity of KD strategies. Most existing theoretical analyses of KD focus on feature matching between the teacher and student models, while a much broader spectrum of KD algorithms demonstrates appealing performance in practice.\n\nRecently, relational knowledge distillation (RKD) has achieved remarkable empirical successes by matching the inter-feature relationships (instead of features themselves) between the teacher and student models. This drives us to focus on RKD and motivates our analysis with the following question:\n\nWhat perspective of the data does relational knowledge distillation learn, and how efficiently?\n\nIn light of the connection between RKD and instance relationship graphs (IRG), intuitively, RKD learns the ground truth geometry through a graph induced by the teacher model. We formalize this intuition from a spectral clustering perspective. Specifically, for a multi-class classification problem, we introduce a notion of clustering error that measures the difference between the predicted and ground truth class partitions. Through low clustering error guarantees, we illustrate that RKD over the population learns the ground truth partition via spectral clustering. Moreover, RKD on sufficiently many unlabeled samples learns a similar partition with low clustering error. By appealing to standard generalization analysis for unbiased estimates of the parameterized RKD loss, we show that the unlabeled sample complexity of RKD is dominated by a polynomial term in the model complexity.\n\nAs one of the most influential and classical unsupervised learning strategies, clustering remains an essential aspect of modern learning algorithms with weak/no supervision. However, the effects of clustering in the related schemes of semi-supervised learning (SSL) are less well investigated. Therefore, we pose a follow-up question:\n\nHow does cluster-aware semi-supervised learning improve label efficiency and generalization?\n\nAn IRG is a graph with samples as vertices and sample relationships, characterized by their mutual distances in a feature space, as edges. RKD encourages the student model to learn such IRGs from the teacher model.", "images": [{"name": "page-2-0.jpg", "height": 121, "width": 198, "x": 207, "y": 72}], "items": [{"type": "heading", "lvl": 1, "value": "Complementary Perspectives of RKD and DAC", "md": "# Complementary Perspectives of RKD and DAC"}, {"type": "heading", "lvl": 2, "value": "Figure 1: The complementary perspectives of RKD and DAC regarding the data", "md": "## Figure 1: The complementary perspectives of RKD and DAC regarding the data"}, {"type": "text", "value": "RKD learns the pairwise relations among data (e.g., edge $$w_{xx'}$$ that characterizes the similarity between x and x') over the population \"globally\" via spectral clustering, as illustrated in Section 4. Alternatively, DAC discovers the \"local\" clustering structure through the expansion of neighborhoods based on overlapping augmentation sets (e.g., $$A(x) \\cap A(x'') \\neq \\emptyset$$), as elaborated in Section 6.\n\nMeanwhile, as the scale of training state-of-the-art models explodes, computational costs and limited data are becoming significant obstacles to learning from scratch. In this context, (data-free) knowledge distillation (where pretrained models are taken as teacher oracles without access to their associated training data) is taking an increasingly crucial role in learning efficiently from powerful existing models.\n\nDespite the substantial progresses in knowledge distillation (KD) in practice, its theoretical understanding remains limited, possibly due to the abundant diversity of KD strategies. Most existing theoretical analyses of KD focus on feature matching between the teacher and student models, while a much broader spectrum of KD algorithms demonstrates appealing performance in practice.\n\nRecently, relational knowledge distillation (RKD) has achieved remarkable empirical successes by matching the inter-feature relationships (instead of features themselves) between the teacher and student models. This drives us to focus on RKD and motivates our analysis with the following question:\n\nWhat perspective of the data does relational knowledge distillation learn, and how efficiently?\n\nIn light of the connection between RKD and instance relationship graphs (IRG), intuitively, RKD learns the ground truth geometry through a graph induced by the teacher model. We formalize this intuition from a spectral clustering perspective. Specifically, for a multi-class classification problem, we introduce a notion of clustering error that measures the difference between the predicted and ground truth class partitions. Through low clustering error guarantees, we illustrate that RKD over the population learns the ground truth partition via spectral clustering. Moreover, RKD on sufficiently many unlabeled samples learns a similar partition with low clustering error. By appealing to standard generalization analysis for unbiased estimates of the parameterized RKD loss, we show that the unlabeled sample complexity of RKD is dominated by a polynomial term in the model complexity.\n\nAs one of the most influential and classical unsupervised learning strategies, clustering remains an essential aspect of modern learning algorithms with weak/no supervision. However, the effects of clustering in the related schemes of semi-supervised learning (SSL) are less well investigated. Therefore, we pose a follow-up question:\n\nHow does cluster-aware semi-supervised learning improve label efficiency and generalization?\n\nAn IRG is a graph with samples as vertices and sample relationships, characterized by their mutual distances in a feature space, as edges. RKD encourages the student model to learn such IRGs from the teacher model.", "md": "RKD learns the pairwise relations among data (e.g., edge $$w_{xx'}$$ that characterizes the similarity between x and x') over the population \"globally\" via spectral clustering, as illustrated in Section 4. Alternatively, DAC discovers the \"local\" clustering structure through the expansion of neighborhoods based on overlapping augmentation sets (e.g., $$A(x) \\cap A(x'') \\neq \\emptyset$$), as elaborated in Section 6.\n\nMeanwhile, as the scale of training state-of-the-art models explodes, computational costs and limited data are becoming significant obstacles to learning from scratch. In this context, (data-free) knowledge distillation (where pretrained models are taken as teacher oracles without access to their associated training data) is taking an increasingly crucial role in learning efficiently from powerful existing models.\n\nDespite the substantial progresses in knowledge distillation (KD) in practice, its theoretical understanding remains limited, possibly due to the abundant diversity of KD strategies. Most existing theoretical analyses of KD focus on feature matching between the teacher and student models, while a much broader spectrum of KD algorithms demonstrates appealing performance in practice.\n\nRecently, relational knowledge distillation (RKD) has achieved remarkable empirical successes by matching the inter-feature relationships (instead of features themselves) between the teacher and student models. This drives us to focus on RKD and motivates our analysis with the following question:\n\nWhat perspective of the data does relational knowledge distillation learn, and how efficiently?\n\nIn light of the connection between RKD and instance relationship graphs (IRG), intuitively, RKD learns the ground truth geometry through a graph induced by the teacher model. We formalize this intuition from a spectral clustering perspective. Specifically, for a multi-class classification problem, we introduce a notion of clustering error that measures the difference between the predicted and ground truth class partitions. Through low clustering error guarantees, we illustrate that RKD over the population learns the ground truth partition via spectral clustering. Moreover, RKD on sufficiently many unlabeled samples learns a similar partition with low clustering error. By appealing to standard generalization analysis for unbiased estimates of the parameterized RKD loss, we show that the unlabeled sample complexity of RKD is dominated by a polynomial term in the model complexity.\n\nAs one of the most influential and classical unsupervised learning strategies, clustering remains an essential aspect of modern learning algorithms with weak/no supervision. However, the effects of clustering in the related schemes of semi-supervised learning (SSL) are less well investigated. Therefore, we pose a follow-up question:\n\nHow does cluster-aware semi-supervised learning improve label efficiency and generalization?\n\nAn IRG is a graph with samples as vertices and sample relationships, characterized by their mutual distances in a feature space, as edges. RKD encourages the student model to learn such IRGs from the teacher model."}]}, {"page": 3, "text": "(Cluster-aware SSL is label-efficient.)     We formulate a cluster-aware SSL framework (Section 5)\nwhere assuming a low clustering error (Definition 3.1) endowed by learning from unlabeled data,\nthe label complexity scales linearly in the number of clusters, up to a logarithmic factor (Theo-\nrem 5.1).\nSince clustering is a rather generic notion that can be facilitated from various perspectives, to better\nunderstand spectral clustering as learned by RKD, we compare the clustering mechanism of RKD\nwith that of another common example of cluster-aware SSL\u2014data augmentation consistency (DAC)\nregularization, while exploring the following question:\n      What are the similarities and discrepancies between different cluster-aware strategies?\n(RKD and DAC learn clustering from complementary perspectives.)                  We unify the existing\nanalysis [Yang et al., 2023] tailored for DAC regularization [Sohn et al., 2020] into the cluster-aware\nSSL framework (Section 6). In particular, while facilitating clustering similarly to RKD, DAC reduces\nthe clustering error through an expansion-based mechanism characterized by \u201clocal\u201d neighborhoods\ninduced by data augmentations (Theorem 6.2). This serves as a complementary perspective to RKD\nwhich approximates spectral clustering on a graph Laplacian that reflects the underlying \u201cglobal\u201d\nstructure of the population (Figure 1).\n2    Related Works\nRelational knowledge distillation.     Since the seminal work of Hinton et al. [2015], knowledge dis-\ntillation has become a foundational method for time and memory-efficient deep learning. Recent years\nhave seen different KD strategies. Some works learn to directly match the output (response) [Hinton\net al., 2015, Chen et al., 2017, Ba and Caruana, 2014] or intermediate layers (features) [Romero et al.,\n2014, Zagoruyko and Komodakis, 2016a, Kim et al., 2018] of the teacher network. More recently,\nPark et al. [2019] introduced the idea of relational knowledge distillation (RKD), and concurrent\nwork [Liu et al., 2019] introduced the \u201cinstance relationship graph\u201d (IRG). They essentially presented\nsimilar ideas to learn the inter-feature relationship between samples instead of matching models\u2019\nresponses or features individually. A comprehensive survey of relational (relation-based) KD can be\nfound in [Gou et al., 2021].\nDespite the empirical success stories, the theoretical understanding of KD remained limited. Prior\ntheoretical work is mostly constrained to linear classifier/nets [Phuong and Lampert, 2019, Ji and\nZhu, 2020] or neural tangent kernel (NTK) analysis [Harutyunyan et al., 2023]. Hsu et al. [2021]\nleverages knowledge distillation to improve the sample complexity analysis of large neural networks\nwith potentially vacuous generalization bounds.\nData augmentation consistency regularization.           Data augmentation serves as an implicit or\nexplicit regularization to improve sample efficiency and avoid overfitting. Initially, people applied\nsimple transformations to labeled samples and add them to the training set [Krizhevsky et al., 2017,\nSimard et al., 2002, Simonyan and Zisserman, 2014, He et al., 2016, Cubuk et al., 2018]. Traditional\ntransformations preserve image semantics, including (random) perturbations, distortions, scales,\ncrops, rotations, and horizontal flips. Subsequent augmentations may alter the labels jointly with the\ninput samples, e.g. Mixup [Zhang et al., 2017], Cutout [DeVries and Taylor, 2017], and Cutmix [Yun\net al., 2019]. Theoretical works have demonstrated different functionalities of data augmentation\nsuch as incorporating invariances [Mei et al., 2021], amplifying strong features [Shen et al., 2022], or\nreducing variance during the learning procedure [Chen et al., 2020].\nRecent practices explicitly add consistency regularization to enforce prediction or representation\nsimilarities [Laine and Aila, 2016, Bachman et al., 2014, Sohn et al., 2020]. Not only does this\nway incorporates unlabeled samples, but Yang et al. [2023] also theoretically demonstrated other\nbenefits: explicit consistency regularization reduces sample complexity more significantly and handles\nmisspecification better than simply adding augmented data into the training set.\nGraph-based learning.      Although we leverage graphs to model the underlying clustering structure\nof the population, the RKD algorithm that we study applies to generic data distributions and does\nnot involve any graph constructions over the training data. The latter is crucial for modern learning\nsettings as graph construction can be prohibitively expensive on common large-scale datasets.\nNevertheless, assuming that explicit graph construction is affordable/available, there exists a rich\nhistory of nonparametric graph-based semi-supervised classification models for transductive learn-\n                                                   3", "md": "(Cluster-aware SSL is label-efficient.) We formulate a cluster-aware SSL framework (Section 5) where assuming a low clustering error (Definition 3.1) endowed by learning from unlabeled data, the label complexity scales linearly in the number of clusters, up to a logarithmic factor (Theorem 5.1). Since clustering is a rather generic notion that can be facilitated from various perspectives, to better understand spectral clustering as learned by RKD, we compare the clustering mechanism of RKD with that of another common example of cluster-aware SSL\u2014data augmentation consistency (DAC) regularization, while exploring the following question:\n\nWhat are the similarities and discrepancies between different cluster-aware strategies? (RKD and DAC learn clustering from complementary perspectives.) We unify the existing analysis [Yang et al., 2023] tailored for DAC regularization [Sohn et al., 2020] into the cluster-aware SSL framework (Section 6). In particular, while facilitating clustering similarly to RKD, DAC reduces the clustering error through an expansion-based mechanism characterized by \u201clocal\u201d neighborhoods induced by data augmentations (Theorem 6.2). This serves as a complementary perspective to RKD which approximates spectral clustering on a graph Laplacian that reflects the underlying \u201cglobal\u201d structure of the population (Figure 1).\n\n## Related Works\n\nRelational knowledge distillation. Since the seminal work of Hinton et al. [2015], knowledge distillation has become a foundational method for time and memory-efficient deep learning. Recent years have seen different KD strategies. Some works learn to directly match the output (response) [Hinton et al., 2015, Chen et al., 2017, Ba and Caruana, 2014] or intermediate layers (features) [Romero et al., 2014, Zagoruyko and Komodakis, 2016a, Kim et al., 2018]. More recently, Park et al. [2019] introduced the idea of relational knowledge distillation (RKD), and concurrent work [Liu et al., 2019] introduced the \u201cinstance relationship graph\u201d (IRG). They essentially presented similar ideas to learn the inter-feature relationship between samples instead of matching models\u2019 responses or features individually. A comprehensive survey of relational (relation-based) KD can be found in [Gou et al., 2021].\n\nDespite the empirical success stories, the theoretical understanding of KD remained limited. Prior theoretical work is mostly constrained to linear classifier/nets [Phuong and Lampert, 2019, Ji and Zhu, 2020] or neural tangent kernel (NTK) analysis [Harutyunyan et al., 2023]. Hsu et al. [2021] leverages knowledge distillation to improve the sample complexity analysis of large neural networks with potentially vacuous generalization bounds.\n\nData augmentation consistency regularization. Data augmentation serves as an implicit or explicit regularization to improve sample efficiency and avoid overfitting. Initially, people applied simple transformations to labeled samples and add them to the training set [Krizhevsky et al., 2017, Simard et al., 2002, Simonyan and Zisserman, 2014, He et al., 2016, Cubuk et al., 2018]. Traditional transformations preserve image semantics, including (random) perturbations, distortions, scales, crops, rotations, and horizontal flips. Subsequent augmentations may alter the labels jointly with the input samples, e.g. Mixup [Zhang et al., 2017], Cutout [DeVries and Taylor, 2017], and Cutmix [Yun et al., 2019]. Theoretical works have demonstrated different functionalities of data augmentation such as incorporating invariances [Mei et al., 2021], amplifying strong features [Shen et al., 2022], or reducing variance during the learning procedure [Chen et al., 2020].\n\nRecent practices explicitly add consistency regularization to enforce prediction or representation similarities [Laine and Aila, 2016, Bachman et al., 2014, Sohn et al., 2020]. Not only does this way incorporates unlabeled samples, but Yang et al. [2023] also theoretically demonstrated other benefits: explicit consistency regularization reduces sample complexity more significantly and handles misspecification better than simply adding augmented data into the training set.\n\nGraph-based learning. Although we leverage graphs to model the underlying clustering structure of the population, the RKD algorithm that we study applies to generic data distributions and does not involve any graph constructions over the training data. The latter is crucial for modern learning settings as graph construction can be prohibitively expensive on common large-scale datasets. Nevertheless, assuming that explicit graph construction is affordable/available, there exists a rich history of nonparametric graph-based semi-supervised classification models for transductive learning.", "images": [], "items": [{"type": "text", "value": "(Cluster-aware SSL is label-efficient.) We formulate a cluster-aware SSL framework (Section 5) where assuming a low clustering error (Definition 3.1) endowed by learning from unlabeled data, the label complexity scales linearly in the number of clusters, up to a logarithmic factor (Theorem 5.1). Since clustering is a rather generic notion that can be facilitated from various perspectives, to better understand spectral clustering as learned by RKD, we compare the clustering mechanism of RKD with that of another common example of cluster-aware SSL\u2014data augmentation consistency (DAC) regularization, while exploring the following question:\n\nWhat are the similarities and discrepancies between different cluster-aware strategies? (RKD and DAC learn clustering from complementary perspectives.) We unify the existing analysis [Yang et al., 2023] tailored for DAC regularization [Sohn et al., 2020] into the cluster-aware SSL framework (Section 6). In particular, while facilitating clustering similarly to RKD, DAC reduces the clustering error through an expansion-based mechanism characterized by \u201clocal\u201d neighborhoods induced by data augmentations (Theorem 6.2). This serves as a complementary perspective to RKD which approximates spectral clustering on a graph Laplacian that reflects the underlying \u201cglobal\u201d structure of the population (Figure 1).", "md": "(Cluster-aware SSL is label-efficient.) We formulate a cluster-aware SSL framework (Section 5) where assuming a low clustering error (Definition 3.1) endowed by learning from unlabeled data, the label complexity scales linearly in the number of clusters, up to a logarithmic factor (Theorem 5.1). Since clustering is a rather generic notion that can be facilitated from various perspectives, to better understand spectral clustering as learned by RKD, we compare the clustering mechanism of RKD with that of another common example of cluster-aware SSL\u2014data augmentation consistency (DAC) regularization, while exploring the following question:\n\nWhat are the similarities and discrepancies between different cluster-aware strategies? (RKD and DAC learn clustering from complementary perspectives.) We unify the existing analysis [Yang et al., 2023] tailored for DAC regularization [Sohn et al., 2020] into the cluster-aware SSL framework (Section 6). In particular, while facilitating clustering similarly to RKD, DAC reduces the clustering error through an expansion-based mechanism characterized by \u201clocal\u201d neighborhoods induced by data augmentations (Theorem 6.2). This serves as a complementary perspective to RKD which approximates spectral clustering on a graph Laplacian that reflects the underlying \u201cglobal\u201d structure of the population (Figure 1)."}, {"type": "heading", "lvl": 2, "value": "Related Works", "md": "## Related Works"}, {"type": "text", "value": "Relational knowledge distillation. Since the seminal work of Hinton et al. [2015], knowledge distillation has become a foundational method for time and memory-efficient deep learning. Recent years have seen different KD strategies. Some works learn to directly match the output (response) [Hinton et al., 2015, Chen et al., 2017, Ba and Caruana, 2014] or intermediate layers (features) [Romero et al., 2014, Zagoruyko and Komodakis, 2016a, Kim et al., 2018]. More recently, Park et al. [2019] introduced the idea of relational knowledge distillation (RKD), and concurrent work [Liu et al., 2019] introduced the \u201cinstance relationship graph\u201d (IRG). They essentially presented similar ideas to learn the inter-feature relationship between samples instead of matching models\u2019 responses or features individually. A comprehensive survey of relational (relation-based) KD can be found in [Gou et al., 2021].\n\nDespite the empirical success stories, the theoretical understanding of KD remained limited. Prior theoretical work is mostly constrained to linear classifier/nets [Phuong and Lampert, 2019, Ji and Zhu, 2020] or neural tangent kernel (NTK) analysis [Harutyunyan et al., 2023]. Hsu et al. [2021] leverages knowledge distillation to improve the sample complexity analysis of large neural networks with potentially vacuous generalization bounds.\n\nData augmentation consistency regularization. Data augmentation serves as an implicit or explicit regularization to improve sample efficiency and avoid overfitting. Initially, people applied simple transformations to labeled samples and add them to the training set [Krizhevsky et al., 2017, Simard et al., 2002, Simonyan and Zisserman, 2014, He et al., 2016, Cubuk et al., 2018]. Traditional transformations preserve image semantics, including (random) perturbations, distortions, scales, crops, rotations, and horizontal flips. Subsequent augmentations may alter the labels jointly with the input samples, e.g. Mixup [Zhang et al., 2017], Cutout [DeVries and Taylor, 2017], and Cutmix [Yun et al., 2019]. Theoretical works have demonstrated different functionalities of data augmentation such as incorporating invariances [Mei et al., 2021], amplifying strong features [Shen et al., 2022], or reducing variance during the learning procedure [Chen et al., 2020].\n\nRecent practices explicitly add consistency regularization to enforce prediction or representation similarities [Laine and Aila, 2016, Bachman et al., 2014, Sohn et al., 2020]. Not only does this way incorporates unlabeled samples, but Yang et al. [2023] also theoretically demonstrated other benefits: explicit consistency regularization reduces sample complexity more significantly and handles misspecification better than simply adding augmented data into the training set.\n\nGraph-based learning. Although we leverage graphs to model the underlying clustering structure of the population, the RKD algorithm that we study applies to generic data distributions and does not involve any graph constructions over the training data. The latter is crucial for modern learning settings as graph construction can be prohibitively expensive on common large-scale datasets. Nevertheless, assuming that explicit graph construction is affordable/available, there exists a rich history of nonparametric graph-based semi-supervised classification models for transductive learning.", "md": "Relational knowledge distillation. Since the seminal work of Hinton et al. [2015], knowledge distillation has become a foundational method for time and memory-efficient deep learning. Recent years have seen different KD strategies. Some works learn to directly match the output (response) [Hinton et al., 2015, Chen et al., 2017, Ba and Caruana, 2014] or intermediate layers (features) [Romero et al., 2014, Zagoruyko and Komodakis, 2016a, Kim et al., 2018]. More recently, Park et al. [2019] introduced the idea of relational knowledge distillation (RKD), and concurrent work [Liu et al., 2019] introduced the \u201cinstance relationship graph\u201d (IRG). They essentially presented similar ideas to learn the inter-feature relationship between samples instead of matching models\u2019 responses or features individually. A comprehensive survey of relational (relation-based) KD can be found in [Gou et al., 2021].\n\nDespite the empirical success stories, the theoretical understanding of KD remained limited. Prior theoretical work is mostly constrained to linear classifier/nets [Phuong and Lampert, 2019, Ji and Zhu, 2020] or neural tangent kernel (NTK) analysis [Harutyunyan et al., 2023]. Hsu et al. [2021] leverages knowledge distillation to improve the sample complexity analysis of large neural networks with potentially vacuous generalization bounds.\n\nData augmentation consistency regularization. Data augmentation serves as an implicit or explicit regularization to improve sample efficiency and avoid overfitting. Initially, people applied simple transformations to labeled samples and add them to the training set [Krizhevsky et al., 2017, Simard et al., 2002, Simonyan and Zisserman, 2014, He et al., 2016, Cubuk et al., 2018]. Traditional transformations preserve image semantics, including (random) perturbations, distortions, scales, crops, rotations, and horizontal flips. Subsequent augmentations may alter the labels jointly with the input samples, e.g. Mixup [Zhang et al., 2017], Cutout [DeVries and Taylor, 2017], and Cutmix [Yun et al., 2019]. Theoretical works have demonstrated different functionalities of data augmentation such as incorporating invariances [Mei et al., 2021], amplifying strong features [Shen et al., 2022], or reducing variance during the learning procedure [Chen et al., 2020].\n\nRecent practices explicitly add consistency regularization to enforce prediction or representation similarities [Laine and Aila, 2016, Bachman et al., 2014, Sohn et al., 2020]. Not only does this way incorporates unlabeled samples, but Yang et al. [2023] also theoretically demonstrated other benefits: explicit consistency regularization reduces sample complexity more significantly and handles misspecification better than simply adding augmented data into the training set.\n\nGraph-based learning. Although we leverage graphs to model the underlying clustering structure of the population, the RKD algorithm that we study applies to generic data distributions and does not involve any graph constructions over the training data. The latter is crucial for modern learning settings as graph construction can be prohibitively expensive on common large-scale datasets. Nevertheless, assuming that explicit graph construction is affordable/available, there exists a rich history of nonparametric graph-based semi-supervised classification models for transductive learning."}]}, {"page": 4, "text": " ing [Zhu et al., 2003, Bertozzi and Merkurjev, 2019, Zhou et al., 2003, Belkin et al., 2004, Belkin and\n Niyogi, 2004]. These methods have seen success in achieving high accuracy results in the low-label\n rate regime [Calder et al., 2020, Bertozzi and Merkurjev, 2019]. Furthermore, assuming inherited\n graph structures of data, recent progress in graph neural networks (GNN) [Zhou et al., 2020] and\n graph convolutional networks (GCN) [Welling and Kipf, 2016] has provided a connection between\n these classical graph-based methods and powerful deep learning models for a variety of problem\n settings [Zhou et al., 2020].\n 3      Problem Setup\n Notations.           For any event e, let 1 {e} = 1 if e is true, and 1 {e} = 0 otherwise. For any positive in-\n tegers n, K \u2208          N, we denote [n] = {1, . . . , n} and \u2206K \u225c                              (p1, . . . , pK) \u2208         [0, 1]K        K   k=1 pk = 1           .\n For a finite set X           , |X   | denotes the size of X               , and Unif (X          ) denotes the uniform distribution over\n X (i.e., Unif(x) = 1/ |X                   | for all x \u2208         X   ). For a distribution P over X and any n \u2208                                 N, P n rep-\n resents the joint distribution over X n such that {xi}i\u2208[n] \u223c                                       P n is a set of n i.i.d. samples. Given\n any matrix M \u2208               Rn\u00d7k (n \u2265           k without loss of generality), let \u03c3 (M) = {\u03c3i (M) | i \u2208                                       [k]} be the\n singular values of M with \u03c31 (M) \u2265                              \u00b7 \u00b7 \u00b7 \u2265     \u03c3k (M). While for any symmetric M \u2208                                   Rn\u00d7n, let\n \u03bb (M) = {\u03bbi (M) | i \u2208                   [n]} be the eigenvalues of M such that \u03bb1 (M) \u2264                                 \u00b7 \u00b7 \u00b7 \u2264    \u03bbn (M). Given any\n labeling function y : X \u2192                   [K], let \u2212y :\u2192X \u2192             {0, 1}K be its one-hot encoding.\n 3.1      Spectral Clustering on Population-induced Graph\n We consider a K-class classification problem over an unknown data distribution P : X \u00d7[K] \u2192                                                               [0, 1]\n (while overloading P for the probability measure over X without ambiguity). Let y\u2217                                                                   : X \u2192\n [K] be the ground truth classification that introduced a natural partition {Xk}k\u2208[K] where Xk \u225c\n {x \u2208     X | y\u2217     (x) = k} such that  K                k=1 Xk = X and Xk \u2229                    Xk\u2032 = \u2205        for all k \u0338= k\u2032. Meanwhile, we\n specify a function class F \u2282                       f : X \u2192        RK        to learn from where each f \u2208                      F is associated with a\n prediction function yf (x) \u225c                    argmaxk\u2208[K] f (x)k.\n We characterize the geometry of the data population3 X with an undirected weighted graph GX =\n(X   , W(X       )) such that: (i) For every vertex pair x, x\u2032 \u2208                            X  , the edge weight wxx\u2032 \u2265                   0 characterizes\n the similarity between (x, x\u2032). For example, wxx\u2032 between a pair of samples from the same class\n is larger than that between a pair from different classes (ii) P                                      (x) = wx \u225c              x\u2032\u2208X wxx\u2032 such that\n     x\u2208X         x\u2032\u2208X wxx\u2032 = 1, intuitively implying that a more representative instance x (corresponding to\n a larger wx) has a higher probability of being sampled. We remark that such population-induced graph\n GX is reminiscent of the population augmentation graph for studying contrastive learning4 [HaoChen\n et al., 2021].\n Let W (X         ) with Wxx\u2032 (X              ) = wxx\u2032 be the weighted adjacency matrix of GX ; let D (X                                              ) be the\n diagonal matrix of the weighted degrees {wx}x\u2208X . The (normalized) graph Laplacian takes the form\n L (X    ) = I \u2212       W(X       ) where W(X             ) \u225c     D (X     )\u22121/2 W (X           ) D (X     )\u22121/2 is the normalized adjacency\n matrix. Then, the spectral clustering [von Luxburg, 2007] on GX can be expressed as a (rank-K)                           1\n Nystr\u00f6m approximation of W(X                          ) in terms of the weighted outputs D(X                           ) 2 f (X     ):\n                                                                                                1                                    1  2\n                   FL(X) \u225c         argmin           R (f) \u225c        W(X         ) \u2212   D(X      ) 2 f (X    ) f (X     )\u22a4   D(X      )2   F      .              (1)\n                                      f\u2208F\n To quantify the alignment between the ground truth class partition and the clustering predicted by\n f \u2208  3 F, we introduce the notion of clustering error.\n       For demonstration purposes, we assume an arbitrarily large, but finite, population |X                                      | < \u221e      (as in HaoChen\n et al. [2021], e.g., X = Fd where F is the set of all floating-point numbers with finite precision). Meanwhile,\n our analyses can be generalized to any compact set X via functional analysis tools (e.g., replacing the adjacency\n matrix W(X          ) with an adjacency operator).\n      4The key difference between the population augmentation graph [HaoChen et al., 2021] and the population-\n induced graph described here is that for analyzing RKD, we consider a graph over the original, instead of the\n augmented, population. It\u2019s also worth highlighting that we introduce such graph structure only for the sake of\n analysis, while our data distribution is generic. Therefore, we focus on learning from a general function class F\n without constraining to graph neural networks.\n                                                                                 4", "md": "ing [Zhu et al., 2003, Bertozzi and Merkurjev, 2019, Zhou et al., 2003, Belkin et al., 2004, Belkin and\nNiyogi, 2004]. These methods have seen success in achieving high accuracy results in the low-label\nrate regime [Calder et al., 2020, Bertozzi and Merkurjev, 2019]. Furthermore, assuming inherited\ngraph structures of data, recent progress in graph neural networks (GNN) [Zhou et al., 2020] and\ngraph convolutional networks (GCN) [Welling and Kipf, 2016] has provided a connection between\nthese classical graph-based methods and powerful deep learning models for a variety of problem\nsettings [Zhou et al., 2020].\n\n### Problem Setup\n\nNotations. For any event e, let $$1\\{e\\} = 1$$ if e is true, and $$1\\{e\\} = 0$$ otherwise. For any positive integers n, K \u2208 N, we denote [n] = {1, . . . , n} and $$\\Delta_K \\triangleq (p_1, . . . , p_K) \\in [0, 1]^K$$ such that $$\\sum_{k=1}^{K} p_k = 1$$. For a finite set X, |X| denotes the size of X, and Unif(X) denotes the uniform distribution over X (i.e., Unif(x) = 1/|X| for all x \u2208 X). For a distribution P over X and any n \u2208 N, $$P^n$$ represents the joint distribution over X^n such that $$\\{x_i\\}_{i\\in[n]} \\sim P^n$$ is a set of n i.i.d. samples. Given any matrix $$M \\in \\mathbb{R}^{n \\times k}$$ (n \u2265 k without loss of generality), let $$\\sigma(M) = \\{\\sigma_i(M) | i \\in [k]\\}$$ be the singular values of M with $$\\sigma_1(M) \\geq \\cdots \\geq \\sigma_k(M)$$. While for any symmetric $$M \\in \\mathbb{R}^{n \\times n}$$, let $$\\lambda(M) = \\{\\lambda_i(M) | i \\in [n]\\}$$ be the eigenvalues of M such that $$\\lambda_1(M) \\leq \\cdots \\leq \\lambda_n(M)$$. Given any labeling function $$y : X \\rightarrow [K]$$, let $$-y : X \\rightarrow \\{0, 1\\}^K$$ be its one-hot encoding.\n\n### Spectral Clustering on Population-induced Graph\n\nWe consider a K-class classification problem over an unknown data distribution P : X \u00d7 [K] \u2192 [0, 1] (while overloading P for the probability measure over X without ambiguity). Let $$y^* : X \\rightarrow [K]$$ be the ground truth classification that introduced a natural partition {Xk}k\u2208[K] where $$X_k \\triangleq \\{x \\in X | y^*(x) = k\\}$$ such that $$\\sum_{k=1}^{K} X_k = X$$ and $$X_k \\cap X_{k'} = \\emptyset$$ for all $$k \\neq k'$$.\n\nMeanwhile, we specify a function class $$F \\subset f : X \\rightarrow \\mathbb{R}^K$$ to learn from where each $$f \\in F$$ is associated with a prediction function $$y_f(x) \\triangleq \\text{argmax}_{k \\in [K]} f(x)_k$$.\n\nWe characterize the geometry of the data population X with an undirected weighted graph $$G_X = (X, W(X))$$ such that: (i) For every vertex pair x, x' \u2208 X, the edge weight $$w_{xx'} \\geq 0$$ characterizes the similarity between (x, x'). For example, $$w_{xx'}$$ between a pair of samples from the same class is larger than that between a pair from different classes (ii) $$P(x) = w_x \\triangleq \\sum_{x' \\in X} w_{xx'}$$ such that $$\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} = 1$$, intuitively implying that a more representative instance x (corresponding to a larger wx) has a higher probability of being sampled. We remark that such population-induced graph $$G_X$$ is reminiscent of the population augmentation graph for studying contrastive learning [HaoChen et al., 2021].\n\nLet $$W(X)$$ with $$W_{xx'}(X) = w_{xx'}$$ be the weighted adjacency matrix of $$G_X$$; let $$D(X)$$ be the diagonal matrix of the weighted degrees {$$w_x$$}x\u2208X. The (normalized) graph Laplacian takes the form $$L(X) = I - W(X)$$ where $$W(X) \\triangleq D(X)^{-1/2} W(X) D(X)^{-1/2}$$ is the normalized adjacency matrix. Then, the spectral clustering [von Luxburg, 2007] on $$G_X$$ can be expressed as a (rank-K) Nystr\u00f6m approximation of $$W(X)$$ in terms of the weighted outputs $$D(X)^{1/2} f(X)$$:\n\n$$\\begin{aligned} F_L(X) &\\triangleq \\text{argmin}_{f \\in F} R(f) \\triangleq \\|W(X) - D(X)^{1/2} f(X) f(X)^\\top D(X)^{1/2}\\|_F. \\end{aligned}$$ (1)\n\nTo quantify the alignment between the ground truth class partition and the clustering predicted by $$f \\in F$$, we introduce the notion of clustering error.\n\nFor demonstration purposes, we assume an arbitrarily large, but finite, population $$|X| < \\infty$$ (as in HaoChen et al. [2021], e.g., $$X = F^d$$ where F is the set of all floating-point numbers with finite precision). Meanwhile, our analyses can be generalized to any compact set X via functional analysis tools (e.g., replacing the adjacency matrix $$W(X)$$ with an adjacency operator).\n\nThe key difference between the population augmentation graph [HaoChen et al., 2021] and the population-induced graph described here is that for analyzing RKD, we consider a graph over the original, instead of the augmented, population. It\u2019s also worth highlighting that we introduce such graph structure only for the sake of analysis, while our data distribution is generic. Therefore, we focus on learning from a general function class F without constraining to graph neural networks.", "images": [], "items": [{"type": "text", "value": "ing [Zhu et al., 2003, Bertozzi and Merkurjev, 2019, Zhou et al., 2003, Belkin et al., 2004, Belkin and\nNiyogi, 2004]. These methods have seen success in achieving high accuracy results in the low-label\nrate regime [Calder et al., 2020, Bertozzi and Merkurjev, 2019]. Furthermore, assuming inherited\ngraph structures of data, recent progress in graph neural networks (GNN) [Zhou et al., 2020] and\ngraph convolutional networks (GCN) [Welling and Kipf, 2016] has provided a connection between\nthese classical graph-based methods and powerful deep learning models for a variety of problem\nsettings [Zhou et al., 2020].", "md": "ing [Zhu et al., 2003, Bertozzi and Merkurjev, 2019, Zhou et al., 2003, Belkin et al., 2004, Belkin and\nNiyogi, 2004]. These methods have seen success in achieving high accuracy results in the low-label\nrate regime [Calder et al., 2020, Bertozzi and Merkurjev, 2019]. Furthermore, assuming inherited\ngraph structures of data, recent progress in graph neural networks (GNN) [Zhou et al., 2020] and\ngraph convolutional networks (GCN) [Welling and Kipf, 2016] has provided a connection between\nthese classical graph-based methods and powerful deep learning models for a variety of problem\nsettings [Zhou et al., 2020]."}, {"type": "heading", "lvl": 3, "value": "Problem Setup", "md": "### Problem Setup"}, {"type": "text", "value": "Notations. For any event e, let $$1\\{e\\} = 1$$ if e is true, and $$1\\{e\\} = 0$$ otherwise. For any positive integers n, K \u2208 N, we denote [n] = {1, . . . , n} and $$\\Delta_K \\triangleq (p_1, . . . , p_K) \\in [0, 1]^K$$ such that $$\\sum_{k=1}^{K} p_k = 1$$. For a finite set X, |X| denotes the size of X, and Unif(X) denotes the uniform distribution over X (i.e., Unif(x) = 1/|X| for all x \u2208 X). For a distribution P over X and any n \u2208 N, $$P^n$$ represents the joint distribution over X^n such that $$\\{x_i\\}_{i\\in[n]} \\sim P^n$$ is a set of n i.i.d. samples. Given any matrix $$M \\in \\mathbb{R}^{n \\times k}$$ (n \u2265 k without loss of generality), let $$\\sigma(M) = \\{\\sigma_i(M) | i \\in [k]\\}$$ be the singular values of M with $$\\sigma_1(M) \\geq \\cdots \\geq \\sigma_k(M)$$. While for any symmetric $$M \\in \\mathbb{R}^{n \\times n}$$, let $$\\lambda(M) = \\{\\lambda_i(M) | i \\in [n]\\}$$ be the eigenvalues of M such that $$\\lambda_1(M) \\leq \\cdots \\leq \\lambda_n(M)$$. Given any labeling function $$y : X \\rightarrow [K]$$, let $$-y : X \\rightarrow \\{0, 1\\}^K$$ be its one-hot encoding.", "md": "Notations. For any event e, let $$1\\{e\\} = 1$$ if e is true, and $$1\\{e\\} = 0$$ otherwise. For any positive integers n, K \u2208 N, we denote [n] = {1, . . . , n} and $$\\Delta_K \\triangleq (p_1, . . . , p_K) \\in [0, 1]^K$$ such that $$\\sum_{k=1}^{K} p_k = 1$$. For a finite set X, |X| denotes the size of X, and Unif(X) denotes the uniform distribution over X (i.e., Unif(x) = 1/|X| for all x \u2208 X). For a distribution P over X and any n \u2208 N, $$P^n$$ represents the joint distribution over X^n such that $$\\{x_i\\}_{i\\in[n]} \\sim P^n$$ is a set of n i.i.d. samples. Given any matrix $$M \\in \\mathbb{R}^{n \\times k}$$ (n \u2265 k without loss of generality), let $$\\sigma(M) = \\{\\sigma_i(M) | i \\in [k]\\}$$ be the singular values of M with $$\\sigma_1(M) \\geq \\cdots \\geq \\sigma_k(M)$$. While for any symmetric $$M \\in \\mathbb{R}^{n \\times n}$$, let $$\\lambda(M) = \\{\\lambda_i(M) | i \\in [n]\\}$$ be the eigenvalues of M such that $$\\lambda_1(M) \\leq \\cdots \\leq \\lambda_n(M)$$. Given any labeling function $$y : X \\rightarrow [K]$$, let $$-y : X \\rightarrow \\{0, 1\\}^K$$ be its one-hot encoding."}, {"type": "heading", "lvl": 3, "value": "Spectral Clustering on Population-induced Graph", "md": "### Spectral Clustering on Population-induced Graph"}, {"type": "text", "value": "We consider a K-class classification problem over an unknown data distribution P : X \u00d7 [K] \u2192 [0, 1] (while overloading P for the probability measure over X without ambiguity). Let $$y^* : X \\rightarrow [K]$$ be the ground truth classification that introduced a natural partition {Xk}k\u2208[K] where $$X_k \\triangleq \\{x \\in X | y^*(x) = k\\}$$ such that $$\\sum_{k=1}^{K} X_k = X$$ and $$X_k \\cap X_{k'} = \\emptyset$$ for all $$k \\neq k'$$.\n\nMeanwhile, we specify a function class $$F \\subset f : X \\rightarrow \\mathbb{R}^K$$ to learn from where each $$f \\in F$$ is associated with a prediction function $$y_f(x) \\triangleq \\text{argmax}_{k \\in [K]} f(x)_k$$.\n\nWe characterize the geometry of the data population X with an undirected weighted graph $$G_X = (X, W(X))$$ such that: (i) For every vertex pair x, x' \u2208 X, the edge weight $$w_{xx'} \\geq 0$$ characterizes the similarity between (x, x'). For example, $$w_{xx'}$$ between a pair of samples from the same class is larger than that between a pair from different classes (ii) $$P(x) = w_x \\triangleq \\sum_{x' \\in X} w_{xx'}$$ such that $$\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} = 1$$, intuitively implying that a more representative instance x (corresponding to a larger wx) has a higher probability of being sampled. We remark that such population-induced graph $$G_X$$ is reminiscent of the population augmentation graph for studying contrastive learning [HaoChen et al., 2021].\n\nLet $$W(X)$$ with $$W_{xx'}(X) = w_{xx'}$$ be the weighted adjacency matrix of $$G_X$$; let $$D(X)$$ be the diagonal matrix of the weighted degrees {$$w_x$$}x\u2208X. The (normalized) graph Laplacian takes the form $$L(X) = I - W(X)$$ where $$W(X) \\triangleq D(X)^{-1/2} W(X) D(X)^{-1/2}$$ is the normalized adjacency matrix. Then, the spectral clustering [von Luxburg, 2007] on $$G_X$$ can be expressed as a (rank-K) Nystr\u00f6m approximation of $$W(X)$$ in terms of the weighted outputs $$D(X)^{1/2} f(X)$$:\n\n$$\\begin{aligned} F_L(X) &\\triangleq \\text{argmin}_{f \\in F} R(f) \\triangleq \\|W(X) - D(X)^{1/2} f(X) f(X)^\\top D(X)^{1/2}\\|_F. \\end{aligned}$$ (1)\n\nTo quantify the alignment between the ground truth class partition and the clustering predicted by $$f \\in F$$, we introduce the notion of clustering error.\n\nFor demonstration purposes, we assume an arbitrarily large, but finite, population $$|X| < \\infty$$ (as in HaoChen et al. [2021], e.g., $$X = F^d$$ where F is the set of all floating-point numbers with finite precision). Meanwhile, our analyses can be generalized to any compact set X via functional analysis tools (e.g., replacing the adjacency matrix $$W(X)$$ with an adjacency operator).\n\nThe key difference between the population augmentation graph [HaoChen et al., 2021] and the population-induced graph described here is that for analyzing RKD, we consider a graph over the original, instead of the augmented, population. It\u2019s also worth highlighting that we introduce such graph structure only for the sake of analysis, while our data distribution is generic. Therefore, we focus on learning from a general function class F without constraining to graph neural networks.", "md": "We consider a K-class classification problem over an unknown data distribution P : X \u00d7 [K] \u2192 [0, 1] (while overloading P for the probability measure over X without ambiguity). Let $$y^* : X \\rightarrow [K]$$ be the ground truth classification that introduced a natural partition {Xk}k\u2208[K] where $$X_k \\triangleq \\{x \\in X | y^*(x) = k\\}$$ such that $$\\sum_{k=1}^{K} X_k = X$$ and $$X_k \\cap X_{k'} = \\emptyset$$ for all $$k \\neq k'$$.\n\nMeanwhile, we specify a function class $$F \\subset f : X \\rightarrow \\mathbb{R}^K$$ to learn from where each $$f \\in F$$ is associated with a prediction function $$y_f(x) \\triangleq \\text{argmax}_{k \\in [K]} f(x)_k$$.\n\nWe characterize the geometry of the data population X with an undirected weighted graph $$G_X = (X, W(X))$$ such that: (i) For every vertex pair x, x' \u2208 X, the edge weight $$w_{xx'} \\geq 0$$ characterizes the similarity between (x, x'). For example, $$w_{xx'}$$ between a pair of samples from the same class is larger than that between a pair from different classes (ii) $$P(x) = w_x \\triangleq \\sum_{x' \\in X} w_{xx'}$$ such that $$\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} = 1$$, intuitively implying that a more representative instance x (corresponding to a larger wx) has a higher probability of being sampled. We remark that such population-induced graph $$G_X$$ is reminiscent of the population augmentation graph for studying contrastive learning [HaoChen et al., 2021].\n\nLet $$W(X)$$ with $$W_{xx'}(X) = w_{xx'}$$ be the weighted adjacency matrix of $$G_X$$; let $$D(X)$$ be the diagonal matrix of the weighted degrees {$$w_x$$}x\u2208X. The (normalized) graph Laplacian takes the form $$L(X) = I - W(X)$$ where $$W(X) \\triangleq D(X)^{-1/2} W(X) D(X)^{-1/2}$$ is the normalized adjacency matrix. Then, the spectral clustering [von Luxburg, 2007] on $$G_X$$ can be expressed as a (rank-K) Nystr\u00f6m approximation of $$W(X)$$ in terms of the weighted outputs $$D(X)^{1/2} f(X)$$:\n\n$$\\begin{aligned} F_L(X) &\\triangleq \\text{argmin}_{f \\in F} R(f) \\triangleq \\|W(X) - D(X)^{1/2} f(X) f(X)^\\top D(X)^{1/2}\\|_F. \\end{aligned}$$ (1)\n\nTo quantify the alignment between the ground truth class partition and the clustering predicted by $$f \\in F$$, we introduce the notion of clustering error.\n\nFor demonstration purposes, we assume an arbitrarily large, but finite, population $$|X| < \\infty$$ (as in HaoChen et al. [2021], e.g., $$X = F^d$$ where F is the set of all floating-point numbers with finite precision). Meanwhile, our analyses can be generalized to any compact set X via functional analysis tools (e.g., replacing the adjacency matrix $$W(X)$$ with an adjacency operator).\n\nThe key difference between the population augmentation graph [HaoChen et al., 2021] and the population-induced graph described here is that for analyzing RKD, we consider a graph over the original, instead of the augmented, population. It\u2019s also worth highlighting that we introduce such graph structure only for the sake of analysis, while our data distribution is generic. Therefore, we focus on learning from a general function class F without constraining to graph neural networks."}]}, {"page": 5, "text": "Definition 3.1 (Clustering error). Given any f \u2208                               F, we define the majority labeling\n                                yf (x) \u225c       argmax        Px\u2032\u223cP (x) [y\u2217(x\u2032) = k | yf                (x\u2032) = yf       (x)] ,                             (2)\n                                                 k\u2208[K]\nalong with the minority subsets associated with f: M (f) \u225c                                          {x \u2208     X |    yf  (x) \u0338= y\u2217      (x)} such that\nP (M(f)) quantifies the clustering error of f. For any F\u2032 \u2286                                       F, let \u00b5 (F\u2032) \u225c           supf\u2208F\u2032 P (M (f)).\nIntuitively, M(f) characterizes the difference between K-partition of X by the ground truth y\u2217                                                           and\nby the prediction function yf associated with f; while \u00b5 (F\u2032) quantifies the worse-case clustering\nerror of all functions f \u2208                 F\u2032. In Section 4.1, we will demonstrate that spectral clustering on GX\n(Equation (1)) leads to provably low clustering error \u00b5                                  FL(X)        .\n3.2      Relational Knowledge Distillation\nFor RKD, we assume access to a proper teacher model \u03c8 : X \u2192                           w xx\u2032               W (for a latent feature space W)\nthat induces a graph-revealing kernel: k\u03c8 (x, x\u2032) =                                 wxw    x\u2032 . Then, the spectral clustering on GX in\nEquation (1) can be interpreted as the population RKD loss:\n                                    R (f) = Ex,x\u2032\u223cP (x)2                   k\u03c8 (x, x\u2032) \u2212         f(x)\u22a4f(x\u2032)          2     .\nWhile with only limited unlabeled samples Xu =                                   xuj     j \u2208    [N]      \u223c   P  (x)N in practice, we consider\nthe analogous empirical RKD loss:\n                                                                                                                                               \uf8fc\n                                         \uf8f1                           N/2                                                                       \uf8fd\n          FL(Xu) \u225c          argmin       \uf8f2  RXu (f) \u225c            2            f    xu2i\u22121    \u22a4   f (xu  2i) \u2212     k\u03c8     xu2i\u22121, xu   2i   2   \uf8fe   ,      (3)\n                               f\u2208F       \uf8f3                      N    i=1\nwhere     RXu(f) serves as an unbiased estimate for R(f) (Proposition D.1).\nFurther, for semi-supervised setting with a small set of labeled samples (X, y) = {(xi, yi)}i\u2208[n] \u223c\nP (x, y)n (usually n \u226a                    N) independent of Xu, let \u2113                       : RK \u00d7 [K] \u2192                 {0, 1} be the zero-one\nloss: \u2113     (f(x), y) = 1 {yf               (x) \u0338= y}.          We denote E (f) \u225c                   E(x,y)\u223cP [\u2113        (f(x), y)] and            E (f) \u225c\nn1  n   i=1 \u2113   (f(xi), yi) as the population and empirical losses, respectively, and consider a proper\nlearning setting with the ground truth f\u2217                          \u225c    argminf\u2208F E (f). Then, SSL with RKD aims to find\nminf\u2208F        E(f)+      RXu(f). Alternatively, for an overparameterized setting5 with                                       argminf\u2208F           E (f)      \u2229\nFL(Xu) \u0338= \u2205, SSL with RKD can be formulated as: minf\u2208FL(Xu)                                              E (f).\n4      Relational Knowledge Distillation as Spectral Clustering\nIn this section, we show that minimizing either the population RKD loss (Section 4.1) or the empirical\nRKD loss (Section 4.2) leads to low clustering errors \u00b5                                FL(X)         and \u00b5      FL(Xu)        , respectively.\n4.1      Relational Knowledge Distillation over Population\nStarting with minimization of the population RKD loss f \u2208                                            FL(X) = argminf\u2208F R (f) (Equa-\ntion (1)), let \u03bb (L (X           )) =      \u03bb1, . . . , \u03bb|X|       be the eigenvalues of the graph Laplacian in the ascending\norder, 0 = \u03bb1 \u2264            \u00b7 \u00b7 \u00b7 \u2264   \u03bb|X| with an arbitrary breaking of ties.\nA key assumption of RKD is that the teacher model \u03c8 (with the corresponding graph-revealing kernel\nk\u03c8) is well aligned with the underlying ground truth partition, formalized as below:\nAssumption 4.1 (Appropriate teacher models). For GX unveiled through the teacher model k\u03c8 (\u00b7, \u00b7),\nwe assume \u03bbK+1 > 0 (i.e., GX has at most K disconnected components); while the ground truth            k\u2208[K]       x\u2208Xk       x\u2032 /\nclasses {Xk}k\u2208[K] are well-separated by GX such that \u03b1 \u225c                                                                         \u2208Xk wxx\u2032       \u226a    1 (i.e.,\nthe fraction of weights of inter-class edges is sufficiently small).                                      2  x\u2208X         x\u2032\u2208X wxx\u2032\n     5Overparameterization is a ubiquitous scenario when learning with neural networks: f \u2208                                         F is parameterized\nby \u03b8 \u2208     \u0398 with dim(\u0398) > N + n such that the minimization problem has infinitely many solutions.\n                                                                              5", "md": "# Math Equations and Text\n\n## Definition 3.1 (Clustering error)\n\nGiven any \\( f \\in F \\), we define the majority labeling\n\n$$\ny_f(x) \\triangleq \\arg\\max_{k \\in [K]} P_{x' \\sim P(x)}[y^*(x') = k | y_f(x') = y_f(x)]\n$$\nalong with the minority subsets associated with \\( f \\): \\( M(f) \\triangleq \\{x \\in X | y_f(x) \\neq y^*(x)\\} \\) such that \\( P(M(f)) \\) quantifies the clustering error of \\( f \\). For any \\( F' \\subseteq F \\), let \\( \\mu(F') \\triangleq \\sup_{f \\in F'} P(M(f)) \\).\n\nIntuitively, \\( M(f) \\) characterizes the difference between \\( K \\)-partition of \\( X \\) by the ground truth \\( y^* \\) and by the prediction function \\( y_f \\) associated with \\( f \\); while \\( \\mu(F') \\) quantifies the worst-case clustering error of all functions \\( f \\in F' \\). In Section 4.1, we will demonstrate that spectral clustering on \\( G_X \\) (Equation (1)) leads to provably low clustering error \\( \\mu_{FL}(X) \\).\n\n## 3.2 Relational Knowledge Distillation\n\nFor RKD, we assume access to a proper teacher model \\( \\psi : X \\rightarrow W \\) that induces a graph-revealing kernel: \\( k_\\psi(x, x') = wxw_{x'} \\). Then, the spectral clustering on \\( G_X \\) in Equation (1) can be interpreted as the population RKD loss:\n\n$$\nR(f) = \\mathbb{E}_{x,x' \\sim P(x)}[k_\\psi(x, x') - f(x)^Tf(x')]^2\n$$\nWhile with only limited unlabeled samples \\( X_u = \\{x_{u_j}\\}_{j \\in [N]} \\sim P(x)^N \\) in practice, we consider the analogous empirical RKD loss:\n\n$$\n\\begin{cases}\nFL(X_u) \\triangleq \\arg\\min_{f \\in F} \\left( R_{X_u}(f) = \\frac{1}{N} \\sum_{i=1}^{N/2} [f(x_{u_{2i-1}})^Tf(x_{u_{2i}}) - k_\\psi(x_{u_{2i-1}}, x_{u_{2i}})^2] \\right)\n\\end{cases}\n$$\nwhere \\( R_{X_u}(f) \\) serves as an unbiased estimate for \\( R(f) \\) (Proposition D.1).\n\nFurther, for semi-supervised setting with a small set of labeled samples \\( (X, y) = \\{(x_i, y_i)\\}_{i \\in [n]} \\sim P(x, y)^n \\) (usually \\( n \\ll N \\)) independent of \\( X_u \\), let \\( \\ell : RK \\times [K] \\rightarrow \\{0, 1\\} \\) be the zero-one loss: \\( \\ell(f(x), y) = 1\\{y_f(x) \\neq y\\} \\). We denote \\( E(f) \\triangleq \\mathbb{E}_{(x,y) \\sim P}[\\ell(f(x), y)] \\) and \\( E(f) \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} \\ell(f(x_i), y_i) \\) as the population and empirical losses, respectively, and consider a proper learning setting with the ground truth \\( f^* \\triangleq \\arg\\min_{f \\in F} E(f) \\). Then, SSL with RKD aims to find \\( \\min_{f \\in F} E(f) + R_{X_u}(f) \\). Alternatively, for an overparameterized setting with \\( \\arg\\min_{f \\in F} E(f) \\cap FL(X_u) \\neq \\emptyset \\), SSL with RKD can be formulated as: \\( \\min_{f \\in FL(X_u)} E(f) \\).\n\n## 4 Relational Knowledge Distillation as Spectral Clustering\n\nIn this section, we show that minimizing either the population RKD loss (Section 4.1) or the empirical RKD loss (Section 4.2) leads to low clustering errors \\( \\mu_{FL}(X) \\) and \\( \\mu_{FL}(X_u) \\), respectively.\n\n## 4.1 Relational Knowledge Distillation over Population\n\nStarting with minimization of the population RKD loss \\( f \\in FL(X) = \\arg\\min_{f \\in F} R(f) \\) (Equation (1)), let \\( \\lambda(L(X)) = \\lambda_1, ..., \\lambda|X| \\) be the eigenvalues of the graph Laplacian in the ascending order, \\( 0 = \\lambda_1 \\leq ... \\leq \\lambda|X| \\) with an arbitrary breaking of ties.\n\nA key assumption of RKD is that the teacher model \\( \\psi \\) (with the corresponding graph-revealing kernel \\( k_\\psi \\)) is well aligned with the underlying ground truth partition, formalized as below:\n\nAssumption 4.1 (Appropriate teacher models). For \\( G_X \\) unveiled through the teacher model \\( k_\\psi(\\cdot, \\cdot) \\), we assume \\( \\lambda_{K+1} > 0 \\) (i.e., \\( G_X \\) has at most \\( K \\) disconnected components); while the ground truth classes \\( \\{X_k\\}_{k \\in [K]} \\) are well-separated by \\( G_X \\) such that \\( \\alpha \\triangleq \\frac{\\sum_{x \\in X_k} \\sum_{x' \\in X} w_{xx'}}{\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'}} \\ll 1 \\) (i.e., the fraction of weights of inter-class edges is sufficiently small).\n\n5Overparameterization is a ubiquitous scenario when learning with neural networks: \\( f \\in F \\) is parameterized by \\( \\theta \\in \\Theta \\) with \\( \\text{dim}(\\Theta) > N + n \\) such that the minimization problem has infinitely many solutions.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Definition 3.1 (Clustering error)", "md": "## Definition 3.1 (Clustering error)"}, {"type": "text", "value": "Given any \\( f \\in F \\), we define the majority labeling\n\n$$\ny_f(x) \\triangleq \\arg\\max_{k \\in [K]} P_{x' \\sim P(x)}[y^*(x') = k | y_f(x') = y_f(x)]\n$$\nalong with the minority subsets associated with \\( f \\): \\( M(f) \\triangleq \\{x \\in X | y_f(x) \\neq y^*(x)\\} \\) such that \\( P(M(f)) \\) quantifies the clustering error of \\( f \\). For any \\( F' \\subseteq F \\), let \\( \\mu(F') \\triangleq \\sup_{f \\in F'} P(M(f)) \\).\n\nIntuitively, \\( M(f) \\) characterizes the difference between \\( K \\)-partition of \\( X \\) by the ground truth \\( y^* \\) and by the prediction function \\( y_f \\) associated with \\( f \\); while \\( \\mu(F') \\) quantifies the worst-case clustering error of all functions \\( f \\in F' \\). In Section 4.1, we will demonstrate that spectral clustering on \\( G_X \\) (Equation (1)) leads to provably low clustering error \\( \\mu_{FL}(X) \\).", "md": "Given any \\( f \\in F \\), we define the majority labeling\n\n$$\ny_f(x) \\triangleq \\arg\\max_{k \\in [K]} P_{x' \\sim P(x)}[y^*(x') = k | y_f(x') = y_f(x)]\n$$\nalong with the minority subsets associated with \\( f \\): \\( M(f) \\triangleq \\{x \\in X | y_f(x) \\neq y^*(x)\\} \\) such that \\( P(M(f)) \\) quantifies the clustering error of \\( f \\). For any \\( F' \\subseteq F \\), let \\( \\mu(F') \\triangleq \\sup_{f \\in F'} P(M(f)) \\).\n\nIntuitively, \\( M(f) \\) characterizes the difference between \\( K \\)-partition of \\( X \\) by the ground truth \\( y^* \\) and by the prediction function \\( y_f \\) associated with \\( f \\); while \\( \\mu(F') \\) quantifies the worst-case clustering error of all functions \\( f \\in F' \\). In Section 4.1, we will demonstrate that spectral clustering on \\( G_X \\) (Equation (1)) leads to provably low clustering error \\( \\mu_{FL}(X) \\)."}, {"type": "heading", "lvl": 2, "value": "3.2 Relational Knowledge Distillation", "md": "## 3.2 Relational Knowledge Distillation"}, {"type": "text", "value": "For RKD, we assume access to a proper teacher model \\( \\psi : X \\rightarrow W \\) that induces a graph-revealing kernel: \\( k_\\psi(x, x') = wxw_{x'} \\). Then, the spectral clustering on \\( G_X \\) in Equation (1) can be interpreted as the population RKD loss:\n\n$$\nR(f) = \\mathbb{E}_{x,x' \\sim P(x)}[k_\\psi(x, x') - f(x)^Tf(x')]^2\n$$\nWhile with only limited unlabeled samples \\( X_u = \\{x_{u_j}\\}_{j \\in [N]} \\sim P(x)^N \\) in practice, we consider the analogous empirical RKD loss:\n\n$$\n\\begin{cases}\nFL(X_u) \\triangleq \\arg\\min_{f \\in F} \\left( R_{X_u}(f) = \\frac{1}{N} \\sum_{i=1}^{N/2} [f(x_{u_{2i-1}})^Tf(x_{u_{2i}}) - k_\\psi(x_{u_{2i-1}}, x_{u_{2i}})^2] \\right)\n\\end{cases}\n$$\nwhere \\( R_{X_u}(f) \\) serves as an unbiased estimate for \\( R(f) \\) (Proposition D.1).\n\nFurther, for semi-supervised setting with a small set of labeled samples \\( (X, y) = \\{(x_i, y_i)\\}_{i \\in [n]} \\sim P(x, y)^n \\) (usually \\( n \\ll N \\)) independent of \\( X_u \\), let \\( \\ell : RK \\times [K] \\rightarrow \\{0, 1\\} \\) be the zero-one loss: \\( \\ell(f(x), y) = 1\\{y_f(x) \\neq y\\} \\). We denote \\( E(f) \\triangleq \\mathbb{E}_{(x,y) \\sim P}[\\ell(f(x), y)] \\) and \\( E(f) \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} \\ell(f(x_i), y_i) \\) as the population and empirical losses, respectively, and consider a proper learning setting with the ground truth \\( f^* \\triangleq \\arg\\min_{f \\in F} E(f) \\). Then, SSL with RKD aims to find \\( \\min_{f \\in F} E(f) + R_{X_u}(f) \\). Alternatively, for an overparameterized setting with \\( \\arg\\min_{f \\in F} E(f) \\cap FL(X_u) \\neq \\emptyset \\), SSL with RKD can be formulated as: \\( \\min_{f \\in FL(X_u)} E(f) \\).", "md": "For RKD, we assume access to a proper teacher model \\( \\psi : X \\rightarrow W \\) that induces a graph-revealing kernel: \\( k_\\psi(x, x') = wxw_{x'} \\). Then, the spectral clustering on \\( G_X \\) in Equation (1) can be interpreted as the population RKD loss:\n\n$$\nR(f) = \\mathbb{E}_{x,x' \\sim P(x)}[k_\\psi(x, x') - f(x)^Tf(x')]^2\n$$\nWhile with only limited unlabeled samples \\( X_u = \\{x_{u_j}\\}_{j \\in [N]} \\sim P(x)^N \\) in practice, we consider the analogous empirical RKD loss:\n\n$$\n\\begin{cases}\nFL(X_u) \\triangleq \\arg\\min_{f \\in F} \\left( R_{X_u}(f) = \\frac{1}{N} \\sum_{i=1}^{N/2} [f(x_{u_{2i-1}})^Tf(x_{u_{2i}}) - k_\\psi(x_{u_{2i-1}}, x_{u_{2i}})^2] \\right)\n\\end{cases}\n$$\nwhere \\( R_{X_u}(f) \\) serves as an unbiased estimate for \\( R(f) \\) (Proposition D.1).\n\nFurther, for semi-supervised setting with a small set of labeled samples \\( (X, y) = \\{(x_i, y_i)\\}_{i \\in [n]} \\sim P(x, y)^n \\) (usually \\( n \\ll N \\)) independent of \\( X_u \\), let \\( \\ell : RK \\times [K] \\rightarrow \\{0, 1\\} \\) be the zero-one loss: \\( \\ell(f(x), y) = 1\\{y_f(x) \\neq y\\} \\). We denote \\( E(f) \\triangleq \\mathbb{E}_{(x,y) \\sim P}[\\ell(f(x), y)] \\) and \\( E(f) \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} \\ell(f(x_i), y_i) \\) as the population and empirical losses, respectively, and consider a proper learning setting with the ground truth \\( f^* \\triangleq \\arg\\min_{f \\in F} E(f) \\). Then, SSL with RKD aims to find \\( \\min_{f \\in F} E(f) + R_{X_u}(f) \\). Alternatively, for an overparameterized setting with \\( \\arg\\min_{f \\in F} E(f) \\cap FL(X_u) \\neq \\emptyset \\), SSL with RKD can be formulated as: \\( \\min_{f \\in FL(X_u)} E(f) \\)."}, {"type": "heading", "lvl": 2, "value": "4 Relational Knowledge Distillation as Spectral Clustering", "md": "## 4 Relational Knowledge Distillation as Spectral Clustering"}, {"type": "text", "value": "In this section, we show that minimizing either the population RKD loss (Section 4.1) or the empirical RKD loss (Section 4.2) leads to low clustering errors \\( \\mu_{FL}(X) \\) and \\( \\mu_{FL}(X_u) \\), respectively.", "md": "In this section, we show that minimizing either the population RKD loss (Section 4.1) or the empirical RKD loss (Section 4.2) leads to low clustering errors \\( \\mu_{FL}(X) \\) and \\( \\mu_{FL}(X_u) \\), respectively."}, {"type": "heading", "lvl": 2, "value": "4.1 Relational Knowledge Distillation over Population", "md": "## 4.1 Relational Knowledge Distillation over Population"}, {"type": "text", "value": "Starting with minimization of the population RKD loss \\( f \\in FL(X) = \\arg\\min_{f \\in F} R(f) \\) (Equation (1)), let \\( \\lambda(L(X)) = \\lambda_1, ..., \\lambda|X| \\) be the eigenvalues of the graph Laplacian in the ascending order, \\( 0 = \\lambda_1 \\leq ... \\leq \\lambda|X| \\) with an arbitrary breaking of ties.\n\nA key assumption of RKD is that the teacher model \\( \\psi \\) (with the corresponding graph-revealing kernel \\( k_\\psi \\)) is well aligned with the underlying ground truth partition, formalized as below:\n\nAssumption 4.1 (Appropriate teacher models). For \\( G_X \\) unveiled through the teacher model \\( k_\\psi(\\cdot, \\cdot) \\), we assume \\( \\lambda_{K+1} > 0 \\) (i.e., \\( G_X \\) has at most \\( K \\) disconnected components); while the ground truth classes \\( \\{X_k\\}_{k \\in [K]} \\) are well-separated by \\( G_X \\) such that \\( \\alpha \\triangleq \\frac{\\sum_{x \\in X_k} \\sum_{x' \\in X} w_{xx'}}{\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'}} \\ll 1 \\) (i.e., the fraction of weights of inter-class edges is sufficiently small).\n\n5Overparameterization is a ubiquitous scenario when learning with neural networks: \\( f \\in F \\) is parameterized by \\( \\theta \\in \\Theta \\) with \\( \\text{dim}(\\Theta) > N + n \\) such that the minimization problem has infinitely many solutions.", "md": "Starting with minimization of the population RKD loss \\( f \\in FL(X) = \\arg\\min_{f \\in F} R(f) \\) (Equation (1)), let \\( \\lambda(L(X)) = \\lambda_1, ..., \\lambda|X| \\) be the eigenvalues of the graph Laplacian in the ascending order, \\( 0 = \\lambda_1 \\leq ... \\leq \\lambda|X| \\) with an arbitrary breaking of ties.\n\nA key assumption of RKD is that the teacher model \\( \\psi \\) (with the corresponding graph-revealing kernel \\( k_\\psi \\)) is well aligned with the underlying ground truth partition, formalized as below:\n\nAssumption 4.1 (Appropriate teacher models). For \\( G_X \\) unveiled through the teacher model \\( k_\\psi(\\cdot, \\cdot) \\), we assume \\( \\lambda_{K+1} > 0 \\) (i.e., \\( G_X \\) has at most \\( K \\) disconnected components); while the ground truth classes \\( \\{X_k\\}_{k \\in [K]} \\) are well-separated by \\( G_X \\) such that \\( \\alpha \\triangleq \\frac{\\sum_{x \\in X_k} \\sum_{x' \\in X} w_{xx'}}{\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'}} \\ll 1 \\) (i.e., the fraction of weights of inter-class edges is sufficiently small).\n\n5Overparameterization is a ubiquitous scenario when learning with neural networks: \\( f \\in F \\) is parameterized by \\( \\theta \\in \\Theta \\) with \\( \\text{dim}(\\Theta) > N + n \\) such that the minimization problem has infinitely many solutions."}]}, {"page": 6, "text": " In particular, \u03b1 \u2208        [0, 1] reflects the extent to which the ground truth classes {Xk}k\u2208[K] are separated\n from each other, as quantified by the fraction of edge weights between nodes of disparate classes. In\n the ideal case, \u03b1 = 0 when the ground truth classes are perfectly separated such that every Xk is a\n connected component of GX , while \u03bbK+1 > \u03bbK = 0.\n Meanwhile, to reduce the generic function class F down to a class of reasonable prediction functions,\n we introduce the following regularity conditions on the boundedness and margin:\n Assumption 4.2 (\u03b2-skeleton boundedness and \u03b3-margin). For any f \u2208                                        F with P (M(f) \u2229             Xk) \u2264\n P (Xk) /2 for all k \u2208           [K], we assume there exists a skeleton subset S = [s1; . . . ; sK] \u2208                             X K with6\n sk = argmax         x\u2208X\\M(f) f(x)k such that yf (sk) = k for every k \u2208                           [K] and rank (f(S)) = K.\n    (i) We say that f is \u03b2-skeleton bounded if \u03c31 (f(S)) \u2264                          \u03b2 for some reasonably small \u03b2.\n   (ii) Let the k-th margin of f be \u03b3k \u225c                   f (sk)k \u2212      maxx\u2208M(f):yf (x)\u0338=k f (x)k. We say that f has a\n         \u03b3-margin if mink\u2208[K] \u03b3k > \u03b3 for some sufficiently large \u03b3 > 0.\n Intuitively, S can be viewed as a set of K samples where f makes the \u201cmost confident\u201d prediction in\n each class. Assumption 4.2 ensures the boundedness of predictions made on such a skeleton subset\n \u2225f (S)\u22252 \u2264        \u03b2, as well as a margin \u03b3 by which the \u201cmost confident\u201d prediction of f in each class sk\n can be separated from the minority samples predicted to lie in other classes {x \u2208                                   M(f)|yf(x) \u0338= k}.\n As a toy example, when yf : X \u2192                   [K] is surjective, and the columns in f (X                   ) \u2208   R|X|\u00d7K consist of\n identity vectors of the predicted clusters f (x)                  k = 1 {yf (x) = k}, Assumption 4.2 is satisfied with\n \u03b2 = 1 and \u03b3 = 1. Meanwhile, the following Remark 4.1 highlights that although Assumption 4.2\n cannot be guaranteed with spectral clustering alone, it is generally satisfied in the semi-supervised\n learning setting with additional supervision/regularization (cf. Example C.1).\n Remark 4.1 (Limitation of spectral clustering alone). Notice that for a generic function class F,\n spectral clustering alone is not sufficient to guarantee Assumption 4.2, especially the existence of\n a skeleton subset S or a large enough margin \u03b3. As counter-exemplified in Example C.1, Equa-\n tion (1) can suffer from large clustering error \u00b5                      FL(X)       when applied alone and failing to satisfy\nAssumption 4.2.\nTo learn predictions with accurate clustering, RKD requires either (i) additional supervision/regular-\n ization in end-to-end settings like semi-supervised learning or (ii) further fine-tuning in unsupervised\n representation learning settings like contrastive learning [HaoChen et al., 2021]. For end-to-end\n learning in practice, RKD is generally coupled with standard KD (i.e., feature matching) and weak\n supervision [Park et al., 2019, Liu et al., 2021, Ma et al., 2022], both of which help the learned\n function f satisfy Assumption 4.2 with a reasonable margin \u03b3.\n Throughout this work, we assume F is sufficiently regularized (with weak supervision in Section 5 and\n consistency regularization in Section 6) such that Assumption 4.2 always holds. Under Assumption 4.1\n and Assumption 4.2, the clustering error of RKD over the population \u00b5                                   FL(X)       is guaranteed to be\n small given a good teacher model \u03c8 that leads to a negligible \u03b1:\n Theorem 4.1 (RKD over population, proof in Appendix C.1). Under Assumption 4.1 and Assump-\n tion 4.2 for every f \u2208          FL(X), the clustering error with the population (Equation (1)) satisfies\n                                            \u00b5    FL(X)       \u2264  2 max      \u03b22\u03b32 , 1     \u00b7     \u03b1     .\n                                                                                           \u03bbK+1\n Theorem 4.1 suggests that the clustering error over the population is negligible under mild regularity\n assumptions (i.e., Assumption 4.2) when (i) the ground truth classes are well-separated by GX\n revealed through the teacher model k\u03c8 (\u00b7, \u00b7) (i.e., \u03b1 \u226a                        1 in Assumption 4.1) and (ii) the (K + 1)th\n eigenvalue \u03bbK+1 of the graph Laplacian L(X                        ) is not too small. As we review in Appendix C.3, the\n existing result Lemma C.4 [Louis and Makarychev, 2014] unveils the connection between \u03bbK+1 and\n the sparsest K-partition (Definition C.1) of GX . Intuitively, a reasonably large \u03bbK+1 implies that the\n partition of the K ground truth classes is the \u201conly\u201d partition of GX into K components by removing\n a sparse set of edges (Corollary C.5).\n     6Recall the majority labeling (Equation (2)) and the associated minority subset M (f) which intuitively\n describes the difference between clustering of the y\u2217                and that of yf. We denote f(x) = [f(x)1, . . . , f(x)K].\n                                                                       6", "md": "In particular, \\( \\alpha \\in [0, 1] \\) reflects the extent to which the ground truth classes {Xk}k\u2208[K] are separated\nfrom each other, as quantified by the fraction of edge weights between nodes of disparate classes. In\nthe ideal case, \\( \\alpha = 0 \\) when the ground truth classes are perfectly separated such that every Xk is a\nconnected component of GX, while \\( \\lambda_{K+1} > \\lambda_{K} = 0 \\).\n\nMeanwhile, to reduce the generic function class F down to a class of reasonable prediction functions,\nwe introduce the following regularity conditions on the boundedness and margin:\n\nAssumption 4.2 (\u03b2-skeleton boundedness and \u03b3-margin). For any \\( f \\in F \\) with \\( P(M(f) \\cap Xk) \\leq\nP(Xk)/2 \\) for all \\( k \\in [K] \\), we assume there exists a skeleton subset S = [s1; . . . ; sK] \\in XK with6\n\\( sk = \\arg\\max_{x \\in X \\ M(f)} f(x)k \\) such that \\( yf(sk) = k \\) for every \\( k \\in [K] \\) and \\( \\text{rank}(f(S)) = K \\).\n\n1. We say that \\( f \\) is \u03b2-skeleton bounded if \\( \\sigma_1(f(S)) \\leq \\beta \\) for some reasonably small \\( \\beta \\).\n2. Let the k-th margin of \\( f \\) be \\( \\gamma_k \\triangleq f(sk)k - \\max_{x \\in M(f):yf(x) \\neq k} f(x)k \\). We say that \\( f \\) has a\n\\( \\gamma \\)-margin if \\( \\min_{k \\in [K]} \\gamma_k > \\gamma \\) for some sufficiently large \\( \\gamma > 0 \\).\n\nIntuitively, S can be viewed as a set of K samples where \\( f \\) makes the \u201cmost confident\u201d prediction in\neach class. Assumption 4.2 ensures the boundedness of predictions made on such a skeleton subset\n\\( \\|f(S)\\|_2 \\leq \\beta \\), as well as a margin \\( \\gamma \\) by which the \u201cmost confident\u201d prediction of \\( f \\) in each class \\( sk \\)\ncan be separated from the minority samples predicted to lie in other classes {x \\in M(f)|yf(x) \\neq k}.\n\nAs a toy example, when \\( yf : X \\rightarrow [K] \\) is surjective, and the columns in \\( f(X) \\in \\mathbb{R}^{|X| \\times K} \\) consist of\nidentity vectors of the predicted clusters \\( f(x)k = 1\\{yf(x) = k\\} \\), Assumption 4.2 is satisfied with\n\\( \\beta = 1 \\) and \\( \\gamma = 1 \\). Meanwhile, the following Remark 4.1 highlights that although Assumption 4.2\ncannot be guaranteed with spectral clustering alone, it is generally satisfied in the semi-supervised\nlearning setting with additional supervision/regularization (cf. Example C.1).\n\nRemark 4.1 (Limitation of spectral clustering alone). Notice that for a generic function class F,\nspectral clustering alone is not sufficient to guarantee Assumption 4.2, especially the existence of\na skeleton subset S or a large enough margin \\( \\gamma \\). As counter-exemplified in Example C.1, Equation (1) can suffer from large clustering error \\( \\mu_{FL}(X) \\)\nwhen applied alone and failing to satisfy Assumption 4.2.\n\nTo learn predictions with accurate clustering, RKD requires either (i) additional supervision/regularization in end-to-end settings like semi-supervised learning or (ii) further fine-tuning in unsupervised\nrepresentation learning settings like contrastive learning [HaoChen et al., 2021]. For end-to-end\nlearning in practice, RKD is generally coupled with standard KD (i.e., feature matching) and weak\nsupervision [Park et al., 2019, Liu et al., 2021, Ma et al., 2022], both of which help the learned\nfunction \\( f \\) satisfy Assumption 4.2 with a reasonable margin \\( \\gamma \\).\n\nThroughout this work, we assume F is sufficiently regularized (with weak supervision in Section 5 and\nconsistency regularization in Section 6) such that Assumption 4.2 always holds. Under Assumption 4.1\nand Assumption 4.2, the clustering error of RKD over the population \\( \\mu_{FL}(X) \\) is guaranteed to be\nsmall given a good teacher model \\( \\psi \\) that leads to a negligible \\( \\alpha \\):\n\nTheorem 4.1 (RKD over population, proof in Appendix C.1). Under Assumption 4.1 and Assumption 4.2 for every \\( f \\in FL(X) \\), the clustering error with the population (Equation (1)) satisfies\n\n$$\n\\mu_{FL}(X) \\leq 2 \\max \\left( \\beta^2\\gamma^2, 1 \\right) \\cdot \\frac{\\alpha}{\\lambda_{K+1}}\n$$\n\nTheorem 4.1 suggests that the clustering error over the population is negligible under mild regularity\nassumptions (i.e., Assumption 4.2) when (i) the ground truth classes are well-separated by GX\nrevealed through the teacher model \\( k\\psi(\\cdot, \\cdot) \\) (i.e., \\( \\alpha \\ll 1 \\) in Assumption 4.1) and (ii) the \\( (K + 1) \\)th\neigenvalue \\( \\lambda_{K+1} \\) of the graph Laplacian L(X) is not too small. As we review in Appendix C.3, the\nexisting result Lemma C.4 [Louis and Makarychev, 2014] unveils the connection between \\( \\lambda_{K+1} \\) and\nthe sparsest K-partition (Definition C.1) of GX. Intuitively, a reasonably large \\( \\lambda_{K+1} \\) implies that the\npartition of the K ground truth classes is the \u201conly\u201d partition of GX into K components by removing\na sparse set of edges (Corollary C.5).\n\n6Recall the majority labeling (Equation (2)) and the associated minority subset M(f) which intuitively\ndescribes the difference between clustering of the \\( y^* \\) and that of \\( yf \\). We denote \\( f(x) = [f(x)1, . . . , f(x)K] \\).", "images": [], "items": [{"type": "text", "value": "In particular, \\( \\alpha \\in [0, 1] \\) reflects the extent to which the ground truth classes {Xk}k\u2208[K] are separated\nfrom each other, as quantified by the fraction of edge weights between nodes of disparate classes. In\nthe ideal case, \\( \\alpha = 0 \\) when the ground truth classes are perfectly separated such that every Xk is a\nconnected component of GX, while \\( \\lambda_{K+1} > \\lambda_{K} = 0 \\).\n\nMeanwhile, to reduce the generic function class F down to a class of reasonable prediction functions,\nwe introduce the following regularity conditions on the boundedness and margin:\n\nAssumption 4.2 (\u03b2-skeleton boundedness and \u03b3-margin). For any \\( f \\in F \\) with \\( P(M(f) \\cap Xk) \\leq\nP(Xk)/2 \\) for all \\( k \\in [K] \\), we assume there exists a skeleton subset S = [s1; . . . ; sK] \\in XK with6\n\\( sk = \\arg\\max_{x \\in X \\ M(f)} f(x)k \\) such that \\( yf(sk) = k \\) for every \\( k \\in [K] \\) and \\( \\text{rank}(f(S)) = K \\).\n\n1. We say that \\( f \\) is \u03b2-skeleton bounded if \\( \\sigma_1(f(S)) \\leq \\beta \\) for some reasonably small \\( \\beta \\).\n2. Let the k-th margin of \\( f \\) be \\( \\gamma_k \\triangleq f(sk)k - \\max_{x \\in M(f):yf(x) \\neq k} f(x)k \\). We say that \\( f \\) has a\n\\( \\gamma \\)-margin if \\( \\min_{k \\in [K]} \\gamma_k > \\gamma \\) for some sufficiently large \\( \\gamma > 0 \\).\n\nIntuitively, S can be viewed as a set of K samples where \\( f \\) makes the \u201cmost confident\u201d prediction in\neach class. Assumption 4.2 ensures the boundedness of predictions made on such a skeleton subset\n\\( \\|f(S)\\|_2 \\leq \\beta \\), as well as a margin \\( \\gamma \\) by which the \u201cmost confident\u201d prediction of \\( f \\) in each class \\( sk \\)\ncan be separated from the minority samples predicted to lie in other classes {x \\in M(f)|yf(x) \\neq k}.\n\nAs a toy example, when \\( yf : X \\rightarrow [K] \\) is surjective, and the columns in \\( f(X) \\in \\mathbb{R}^{|X| \\times K} \\) consist of\nidentity vectors of the predicted clusters \\( f(x)k = 1\\{yf(x) = k\\} \\), Assumption 4.2 is satisfied with\n\\( \\beta = 1 \\) and \\( \\gamma = 1 \\). Meanwhile, the following Remark 4.1 highlights that although Assumption 4.2\ncannot be guaranteed with spectral clustering alone, it is generally satisfied in the semi-supervised\nlearning setting with additional supervision/regularization (cf. Example C.1).\n\nRemark 4.1 (Limitation of spectral clustering alone). Notice that for a generic function class F,\nspectral clustering alone is not sufficient to guarantee Assumption 4.2, especially the existence of\na skeleton subset S or a large enough margin \\( \\gamma \\). As counter-exemplified in Example C.1, Equation (1) can suffer from large clustering error \\( \\mu_{FL}(X) \\)\nwhen applied alone and failing to satisfy Assumption 4.2.\n\nTo learn predictions with accurate clustering, RKD requires either (i) additional supervision/regularization in end-to-end settings like semi-supervised learning or (ii) further fine-tuning in unsupervised\nrepresentation learning settings like contrastive learning [HaoChen et al., 2021]. For end-to-end\nlearning in practice, RKD is generally coupled with standard KD (i.e., feature matching) and weak\nsupervision [Park et al., 2019, Liu et al., 2021, Ma et al., 2022], both of which help the learned\nfunction \\( f \\) satisfy Assumption 4.2 with a reasonable margin \\( \\gamma \\).\n\nThroughout this work, we assume F is sufficiently regularized (with weak supervision in Section 5 and\nconsistency regularization in Section 6) such that Assumption 4.2 always holds. Under Assumption 4.1\nand Assumption 4.2, the clustering error of RKD over the population \\( \\mu_{FL}(X) \\) is guaranteed to be\nsmall given a good teacher model \\( \\psi \\) that leads to a negligible \\( \\alpha \\):\n\nTheorem 4.1 (RKD over population, proof in Appendix C.1). Under Assumption 4.1 and Assumption 4.2 for every \\( f \\in FL(X) \\), the clustering error with the population (Equation (1)) satisfies\n\n$$\n\\mu_{FL}(X) \\leq 2 \\max \\left( \\beta^2\\gamma^2, 1 \\right) \\cdot \\frac{\\alpha}{\\lambda_{K+1}}\n$$\n\nTheorem 4.1 suggests that the clustering error over the population is negligible under mild regularity\nassumptions (i.e., Assumption 4.2) when (i) the ground truth classes are well-separated by GX\nrevealed through the teacher model \\( k\\psi(\\cdot, \\cdot) \\) (i.e., \\( \\alpha \\ll 1 \\) in Assumption 4.1) and (ii) the \\( (K + 1) \\)th\neigenvalue \\( \\lambda_{K+1} \\) of the graph Laplacian L(X) is not too small. As we review in Appendix C.3, the\nexisting result Lemma C.4 [Louis and Makarychev, 2014] unveils the connection between \\( \\lambda_{K+1} \\) and\nthe sparsest K-partition (Definition C.1) of GX. Intuitively, a reasonably large \\( \\lambda_{K+1} \\) implies that the\npartition of the K ground truth classes is the \u201conly\u201d partition of GX into K components by removing\na sparse set of edges (Corollary C.5).\n\n6Recall the majority labeling (Equation (2)) and the associated minority subset M(f) which intuitively\ndescribes the difference between clustering of the \\( y^* \\) and that of \\( yf \\). We denote \\( f(x) = [f(x)1, . . . , f(x)K] \\).", "md": "In particular, \\( \\alpha \\in [0, 1] \\) reflects the extent to which the ground truth classes {Xk}k\u2208[K] are separated\nfrom each other, as quantified by the fraction of edge weights between nodes of disparate classes. In\nthe ideal case, \\( \\alpha = 0 \\) when the ground truth classes are perfectly separated such that every Xk is a\nconnected component of GX, while \\( \\lambda_{K+1} > \\lambda_{K} = 0 \\).\n\nMeanwhile, to reduce the generic function class F down to a class of reasonable prediction functions,\nwe introduce the following regularity conditions on the boundedness and margin:\n\nAssumption 4.2 (\u03b2-skeleton boundedness and \u03b3-margin). For any \\( f \\in F \\) with \\( P(M(f) \\cap Xk) \\leq\nP(Xk)/2 \\) for all \\( k \\in [K] \\), we assume there exists a skeleton subset S = [s1; . . . ; sK] \\in XK with6\n\\( sk = \\arg\\max_{x \\in X \\ M(f)} f(x)k \\) such that \\( yf(sk) = k \\) for every \\( k \\in [K] \\) and \\( \\text{rank}(f(S)) = K \\).\n\n1. We say that \\( f \\) is \u03b2-skeleton bounded if \\( \\sigma_1(f(S)) \\leq \\beta \\) for some reasonably small \\( \\beta \\).\n2. Let the k-th margin of \\( f \\) be \\( \\gamma_k \\triangleq f(sk)k - \\max_{x \\in M(f):yf(x) \\neq k} f(x)k \\). We say that \\( f \\) has a\n\\( \\gamma \\)-margin if \\( \\min_{k \\in [K]} \\gamma_k > \\gamma \\) for some sufficiently large \\( \\gamma > 0 \\).\n\nIntuitively, S can be viewed as a set of K samples where \\( f \\) makes the \u201cmost confident\u201d prediction in\neach class. Assumption 4.2 ensures the boundedness of predictions made on such a skeleton subset\n\\( \\|f(S)\\|_2 \\leq \\beta \\), as well as a margin \\( \\gamma \\) by which the \u201cmost confident\u201d prediction of \\( f \\) in each class \\( sk \\)\ncan be separated from the minority samples predicted to lie in other classes {x \\in M(f)|yf(x) \\neq k}.\n\nAs a toy example, when \\( yf : X \\rightarrow [K] \\) is surjective, and the columns in \\( f(X) \\in \\mathbb{R}^{|X| \\times K} \\) consist of\nidentity vectors of the predicted clusters \\( f(x)k = 1\\{yf(x) = k\\} \\), Assumption 4.2 is satisfied with\n\\( \\beta = 1 \\) and \\( \\gamma = 1 \\). Meanwhile, the following Remark 4.1 highlights that although Assumption 4.2\ncannot be guaranteed with spectral clustering alone, it is generally satisfied in the semi-supervised\nlearning setting with additional supervision/regularization (cf. Example C.1).\n\nRemark 4.1 (Limitation of spectral clustering alone). Notice that for a generic function class F,\nspectral clustering alone is not sufficient to guarantee Assumption 4.2, especially the existence of\na skeleton subset S or a large enough margin \\( \\gamma \\). As counter-exemplified in Example C.1, Equation (1) can suffer from large clustering error \\( \\mu_{FL}(X) \\)\nwhen applied alone and failing to satisfy Assumption 4.2.\n\nTo learn predictions with accurate clustering, RKD requires either (i) additional supervision/regularization in end-to-end settings like semi-supervised learning or (ii) further fine-tuning in unsupervised\nrepresentation learning settings like contrastive learning [HaoChen et al., 2021]. For end-to-end\nlearning in practice, RKD is generally coupled with standard KD (i.e., feature matching) and weak\nsupervision [Park et al., 2019, Liu et al., 2021, Ma et al., 2022], both of which help the learned\nfunction \\( f \\) satisfy Assumption 4.2 with a reasonable margin \\( \\gamma \\).\n\nThroughout this work, we assume F is sufficiently regularized (with weak supervision in Section 5 and\nconsistency regularization in Section 6) such that Assumption 4.2 always holds. Under Assumption 4.1\nand Assumption 4.2, the clustering error of RKD over the population \\( \\mu_{FL}(X) \\) is guaranteed to be\nsmall given a good teacher model \\( \\psi \\) that leads to a negligible \\( \\alpha \\):\n\nTheorem 4.1 (RKD over population, proof in Appendix C.1). Under Assumption 4.1 and Assumption 4.2 for every \\( f \\in FL(X) \\), the clustering error with the population (Equation (1)) satisfies\n\n$$\n\\mu_{FL}(X) \\leq 2 \\max \\left( \\beta^2\\gamma^2, 1 \\right) \\cdot \\frac{\\alpha}{\\lambda_{K+1}}\n$$\n\nTheorem 4.1 suggests that the clustering error over the population is negligible under mild regularity\nassumptions (i.e., Assumption 4.2) when (i) the ground truth classes are well-separated by GX\nrevealed through the teacher model \\( k\\psi(\\cdot, \\cdot) \\) (i.e., \\( \\alpha \\ll 1 \\) in Assumption 4.1) and (ii) the \\( (K + 1) \\)th\neigenvalue \\( \\lambda_{K+1} \\) of the graph Laplacian L(X) is not too small. As we review in Appendix C.3, the\nexisting result Lemma C.4 [Louis and Makarychev, 2014] unveils the connection between \\( \\lambda_{K+1} \\) and\nthe sparsest K-partition (Definition C.1) of GX. Intuitively, a reasonably large \\( \\lambda_{K+1} \\) implies that the\npartition of the K ground truth classes is the \u201conly\u201d partition of GX into K components by removing\na sparse set of edges (Corollary C.5).\n\n6Recall the majority labeling (Equation (2)) and the associated minority subset M(f) which intuitively\ndescribes the difference between clustering of the \\( y^* \\) and that of \\( yf \\). We denote \\( f(x) = [f(x)1, . . . , f(x)K] \\)."}]}, {"page": 7, "text": "4.2      Relational Knowledge Distillation on Unlabeled Samples\nWe now turn our attention to a more practical scenario with limited unlabeled samples and bound the\nclustering error \u00b5            FL(Xu)         granted by minimizing the empirical RKD loss (Equation (3)).\nTo cope with F \u220b              f : X \u2192        RK, we recall the notion of Rademacher complexity for vector-valued\nfunctions from Maurer [2016]. Let Rad(\u03c1) be the Rademacher distribution (i.e., with uniform\nprobability 1      2 over {\u22121, 1}) and \u03c1 \u223c                  RadN\u00d7K be a random matrix with i.i.d. Rademacher entries.\nGiven any N \u2208             N, with fk(x) denoting the k-th entry of f(x) \u2208                               RK, we define\n                                      RN (F) \u225c          EXu\u223cP (x)N              sup    N1    N    \u03c1ik \u00b7 fk (xu     i )                                    (4)\n                                                           \u03c1\u223cRadN\u00d7K            f\u2208F          i=1\nas the Rademacher complexity of the vector-valued function class F \u220b                                              f : X \u2192         RK.\nSince the empirical RKD loss is an unbiased estimate of its population correspondence (Proposi-\ntion D.1), we can leverage the standard generalization analysis in terms of Rademacher complexity to\nprovide an unlabeled sample complexity for RKD.\nTheorem 4.2 (Unlabeled sample complexity of RKD, proof in Appendix D.2). Assume there exist\nBf   , Bk  \u03c8 > 0 such that \u2225f(x)\u22252                2 \u2264    Bf and k\u03c8 (x, x\u2032) \u2264               Bk\u03c8 for all x, x\u2032 \u2208             X   , f \u2208     F. Given any\nf |Xu \u2208     FL(Xu), f|X \u2208             FL(X), \u03b4 \u2208         (0, 1), with probability at least 1 \u2212                    \u03b4/2 over Xu \u223c             P  (x)N    ,\n       R     f                                        2Bf       Bf + Bk       \u03c8    RN/2 (F) + 2              Bk\u03c8 + Bf          2       log (4/\u03b4)      .\n               |Xu     \u2212   R     f|X      \u2264   16                                                                                            N\nTheorem 4.2 implies that the unlabeled sample complexity of RKD is dominated by the Rademacher\ncomplexity of F. In Appendix D.3, we further concretize Theorem 4.2 by instantiating RN/2 (F)\nvia existing Rademacher complexity bounds for neural networks [Golowich et al., 2018].\nWith \u2206       \u2192    0 as N increasing (Theorem 4.2), the clustering error of minimizing the empirical RKD\nloss can be upper bounded as follows.\nTheorem 4.3 (RKD on unlabeled samples, proof in Appendix D.4). Under Assumption 4.1 and\nAssumption 4.2 for every f \u2208                   FL(Xu), given any \u03b4 \u2208                 (0, 1), if there exists \u2206            < (1 \u2212       \u03bbK)2 such that\nR    f |Xu      \u2264   R     f|X     + \u2206     for all f|Xu \u2208          FL(Xu) and f|X \u2208               FL(X) with probability at least 1 \u2212                     \u03b4/2\nover Xu \u223c           P  (x)N    , then error of clustering with the empirical graph (Equation (3)) satisfies the    2\nfollows: for any K0 \u2208                [K] such that \u03bbK0 < \u03bbK+1 and CK0 \u225c                               (1\u2212\u03bbK0)\u2206      \u2212(1\u2212\u03bbK)2         = O(1),\n                  \u00b5    FL(Xu)          \u2264   2 max       \u03b22\u03b32 , 1      \u00b7        \u03b1       +      (1 + (K \u2212          K0) CK0) \u2206                 .\n                                                                           \u03bbK+1           (1 \u2212     \u03bbK0)2 \u2212        (1 \u2212    \u03bbK+1)2\nTheorem 4.3 ensures that given sufficient unlabeled samples (proportional to the Rademacher com-\nplexity of F) such that \u2206                \u2192    0, the clustering error \u00b5               FL(Xu)         from empirical RKD is nearly as low\nas \u00b5     FL(X)        from population RKD, up to an additional error term that scales linearly in \u2206.\n5      Label Efficiency of Cluster-aware Semi-supervised Learning\nIn this section, we demonstrate the label efficiency of learning from a function class with low\nclustering error (Definition 3.1), like the ones endowed by RKD discussed in Section 4.\nSpecifically, given any cluster-aware function subclass F\u2032 \u2286                                            F with low clustering error (e.g.,\nFL(X) and FL(Xu)), for a set of n i.i.d. labeled samples (X, y) \u223c                                          P  (x, y)n, we have the following\ngeneralization guarantee:\nTheorem 5.1 (Label complexity, proof in Appendix B.1). Given any cluster-aware F\u2032 \u2286                                                                F with\n\u00b5(F\u2032) \u226a         1, assuming that (X, y) contains at least one sample per class, for any \u03b4 \u2208                                                  (0, 1), with\nprobability at least 1 \u2212              \u03b4/2 over (X, y) \u223c              P  (x, y)n,      f \u2208    argminf\u2208F\u2032           E(f) satisfies\n                            E     f    \u2212   E (f\u2217) \u2264        4     2K log(2n)           + 2\u00b5 (F\u2032) +              2 log (4/\u03b4)        ,                       (5)\ne.g., conditioned on Xu,                f |Xu \u2208      argminf\u2208FL(Xu)      n    7 E(f) satisfies Equation (5) with \u00b5     n                 FL(Xu)        .", "md": "# Relational Knowledge Distillation\n\n## Relational Knowledge Distillation on Unlabeled Samples\n\nWe now turn our attention to a more practical scenario with limited unlabeled samples and bound the clustering error &mu; FL(Xu) granted by minimizing the empirical RKD loss (Equation (3)).\n\nTo cope with F &ni; f : X &rarr; RK, we recall the notion of Rademacher complexity for vector-valued functions from Maurer [2016]. Let Rad(&rho;) be the Rademacher distribution (i.e., with uniform probability 1/2 over {-1, 1}) and &rho; &sim; RadN&times;K be a random matrix with i.i.d. Rademacher entries.\n\nGiven any N &isin; N, with fk(x) denoting the k-th entry of f(x) &isin; RK, we define\n\n$$\nRN(F) \\triangleq \\mathbb{E}xu&sim;P(x)N \\sup&rho;&sim;RadN&times;K \\left[ \\frac{1}{N} \\sumi=1 &rho;ik \\cdot fk(xui) \\right] \\quad (4)\n$$\nas the Rademacher complexity of the vector-valued function class F &ni; f : X &rarr; RK.\n\nSince the empirical RKD loss is an unbiased estimate of its population correspondence (Proposition D.1), we can leverage the standard generalization analysis in terms of Rademacher complexity to provide an unlabeled sample complexity for RKD.\n\nTheorem 4.2 (Unlabeled sample complexity of RKD, proof in Appendix D.2). Assume there exist Bf, Bk, &psi; > 0 such that ||f(x)||22 &le; Bf and k&psi;(x, x') &le; Bk&psi; for all x, x' &isin; X, f &isin; F. Given any f|Xu &isin; FL(Xu), f|X &isin; FL(X), &delta; &isin; (0, 1), with probability at least 1 - &delta;/2 over Xu &sim; P(x)N,\n\n$$\n\\left| R_f|Xu - R_f|X \\right| &le; \\frac{16}{N}\n$$\nTheorem 4.2 implies that the unlabeled sample complexity of RKD is dominated by the Rademacher complexity of F. In Appendix D.3, we further concretize Theorem 4.2 by instantiating RN/2(F) via existing Rademacher complexity bounds for neural networks [Golowich et al., 2018].\n\nWith &Delta; &rarr; 0 as N increasing (Theorem 4.2), the clustering error of minimizing the empirical RKD loss can be upper bounded as follows.\n\nTheorem 4.3 (RKD on unlabeled samples, proof in Appendix D.4). Under Assumption 4.1 and Assumption 4.2 for every f &isin; FL(Xu), given any &delta; &isin; (0, 1), if there exists &Delta; &lt; (1 - &lambda;K)2 such that\n\n$$\nR_f|Xu &le; R_f|X + &Delta; \\text{ for all } f|Xu &isin; FL(Xu), f|X &isin; FL(X) \\text{ with probability at least 1 - &delta;/2 over Xu &sim; P(x)N$$\nthen error of clustering with the empirical graph (Equation (3)) satisfies the following: for any K0 &isin; [K] such that &lambda;K0 &lt; &lambda;K+1 and CK0 &triangleq (1 - &lambda;K0)&Delta;-(1 - &lambda;K)2 = O(1),\n\n$$\n\\mu FL(Xu</sub) &le; 2 \\max(\\beta2\\gamma2, 1) \\cdot \\alpha + (1 + (K - K0)CK0)\\Delta\n$$\nTheorem 4.3 ensures that given sufficient unlabeled samples (proportional to the Rademacher complexity of F) such that &Delta; &rarr; 0, the clustering error &mu FL(Xu</sub) from empirical RKD is nearly as low as &mu FL(X) from population RKD, up to an additional error term that scales linearly in &Delta;.\n\n## Label Efficiency of Cluster-aware Semi-supervised Learning\n\nIn this section, we demonstrate the label efficiency of learning from a function class with low clustering error (Definition 3.1), like the ones endowed by RKD discussed in Section 4.\n\nSpecifically, given any cluster-aware function subclass F' &sube; F with low clustering error (e.g., FL(X) and FL(Xu)), for a set of n i.i.d. labeled samples (X, y) &sim; P(x, y)n, we have the following generalization guarantee:\n\nTheorem 5.1 (Label complexity, proof in Appendix B.1). Given any cluster-aware F' &sube; F with &mu;(F') &ll; 1, assuming that (X, y) contains at least one sample per class, for any &delta; &isin; (0, 1), with probability at least 1 - &delta;/2 over (X, y) &sim; P(x, y)n, f &isin; argminf&isin;F' E(f) satisfies\n\n$$\nE_f - E(f^*) &le; 4\\sqrt{2K\\log(2n)} + 2\\mu(F') + 2\\log(4/\\delta) \\quad (5)\n$$\ne.g., conditioned on Xu, f|Xu &isin; argminf&isin;FL(Xu) n &rarr; E(f) satisfies Equation (5) with &mu;n FL(Xu).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Relational Knowledge Distillation", "md": "# Relational Knowledge Distillation"}, {"type": "heading", "lvl": 2, "value": "Relational Knowledge Distillation on Unlabeled Samples", "md": "## Relational Knowledge Distillation on Unlabeled Samples"}, {"type": "text", "value": "We now turn our attention to a more practical scenario with limited unlabeled samples and bound the clustering error &mu; FL(Xu) granted by minimizing the empirical RKD loss (Equation (3)).\n\nTo cope with F &ni; f : X &rarr; RK, we recall the notion of Rademacher complexity for vector-valued functions from Maurer [2016]. Let Rad(&rho;) be the Rademacher distribution (i.e., with uniform probability 1/2 over {-1, 1}) and &rho; &sim; RadN&times;K be a random matrix with i.i.d. Rademacher entries.\n\nGiven any N &isin; N, with fk(x) denoting the k-th entry of f(x) &isin; RK, we define\n\n$$\nRN(F) \\triangleq \\mathbb{E}xu&sim;P(x)N \\sup&rho;&sim;RadN&times;K \\left[ \\frac{1}{N} \\sumi=1 &rho;ik \\cdot fk(xui) \\right] \\quad (4)\n$$\nas the Rademacher complexity of the vector-valued function class F &ni; f : X &rarr; RK.\n\nSince the empirical RKD loss is an unbiased estimate of its population correspondence (Proposition D.1), we can leverage the standard generalization analysis in terms of Rademacher complexity to provide an unlabeled sample complexity for RKD.\n\nTheorem 4.2 (Unlabeled sample complexity of RKD, proof in Appendix D.2). Assume there exist Bf, Bk, &psi; > 0 such that ||f(x)||22 &le; Bf and k&psi;(x, x') &le; Bk&psi; for all x, x' &isin; X, f &isin; F. Given any f|Xu &isin; FL(Xu), f|X &isin; FL(X), &delta; &isin; (0, 1), with probability at least 1 - &delta;/2 over Xu &sim; P(x)N,\n\n$$\n\\left| R_f|Xu - R_f|X \\right| &le; \\frac{16}{N}\n$$\nTheorem 4.2 implies that the unlabeled sample complexity of RKD is dominated by the Rademacher complexity of F. In Appendix D.3, we further concretize Theorem 4.2 by instantiating RN/2(F) via existing Rademacher complexity bounds for neural networks [Golowich et al., 2018].\n\nWith &Delta; &rarr; 0 as N increasing (Theorem 4.2), the clustering error of minimizing the empirical RKD loss can be upper bounded as follows.\n\nTheorem 4.3 (RKD on unlabeled samples, proof in Appendix D.4). Under Assumption 4.1 and Assumption 4.2 for every f &isin; FL(Xu), given any &delta; &isin; (0, 1), if there exists &Delta; &lt; (1 - &lambda;K)2 such that\n\n$$\nR_f|Xu &le; R_f|X + &Delta; \\text{ for all } f|Xu &isin; FL(Xu), f|X &isin; FL(X) \\text{ with probability at least 1 - &delta;/2 over Xu &sim; P(x)N$$\nthen error of clustering with the empirical graph (Equation (3)) satisfies the following: for any K0 &isin; [K] such that &lambda;K0 &lt; &lambda;K+1 and CK0 &triangleq (1 - &lambda;K0)&Delta;-(1 - &lambda;K)2 = O(1),\n\n$$\n\\mu FL(Xu</sub) &le; 2 \\max(\\beta2\\gamma2, 1) \\cdot \\alpha + (1 + (K - K0)CK0)\\Delta\n$$\nTheorem 4.3 ensures that given sufficient unlabeled samples (proportional to the Rademacher complexity of F) such that &Delta; &rarr; 0, the clustering error &mu FL(Xu</sub) from empirical RKD is nearly as low as &mu FL(X) from population RKD, up to an additional error term that scales linearly in &Delta;.", "md": "We now turn our attention to a more practical scenario with limited unlabeled samples and bound the clustering error &mu; FL(Xu) granted by minimizing the empirical RKD loss (Equation (3)).\n\nTo cope with F &ni; f : X &rarr; RK, we recall the notion of Rademacher complexity for vector-valued functions from Maurer [2016]. Let Rad(&rho;) be the Rademacher distribution (i.e., with uniform probability 1/2 over {-1, 1}) and &rho; &sim; RadN&times;K be a random matrix with i.i.d. Rademacher entries.\n\nGiven any N &isin; N, with fk(x) denoting the k-th entry of f(x) &isin; RK, we define\n\n$$\nRN(F) \\triangleq \\mathbb{E}xu&sim;P(x)N \\sup&rho;&sim;RadN&times;K \\left[ \\frac{1}{N} \\sumi=1 &rho;ik \\cdot fk(xui) \\right] \\quad (4)\n$$\nas the Rademacher complexity of the vector-valued function class F &ni; f : X &rarr; RK.\n\nSince the empirical RKD loss is an unbiased estimate of its population correspondence (Proposition D.1), we can leverage the standard generalization analysis in terms of Rademacher complexity to provide an unlabeled sample complexity for RKD.\n\nTheorem 4.2 (Unlabeled sample complexity of RKD, proof in Appendix D.2). Assume there exist Bf, Bk, &psi; > 0 such that ||f(x)||22 &le; Bf and k&psi;(x, x') &le; Bk&psi; for all x, x' &isin; X, f &isin; F. Given any f|Xu &isin; FL(Xu), f|X &isin; FL(X), &delta; &isin; (0, 1), with probability at least 1 - &delta;/2 over Xu &sim; P(x)N,\n\n$$\n\\left| R_f|Xu - R_f|X \\right| &le; \\frac{16}{N}\n$$\nTheorem 4.2 implies that the unlabeled sample complexity of RKD is dominated by the Rademacher complexity of F. In Appendix D.3, we further concretize Theorem 4.2 by instantiating RN/2(F) via existing Rademacher complexity bounds for neural networks [Golowich et al., 2018].\n\nWith &Delta; &rarr; 0 as N increasing (Theorem 4.2), the clustering error of minimizing the empirical RKD loss can be upper bounded as follows.\n\nTheorem 4.3 (RKD on unlabeled samples, proof in Appendix D.4). Under Assumption 4.1 and Assumption 4.2 for every f &isin; FL(Xu), given any &delta; &isin; (0, 1), if there exists &Delta; &lt; (1 - &lambda;K)2 such that\n\n$$\nR_f|Xu &le; R_f|X + &Delta; \\text{ for all } f|Xu &isin; FL(Xu), f|X &isin; FL(X) \\text{ with probability at least 1 - &delta;/2 over Xu &sim; P(x)N$$\nthen error of clustering with the empirical graph (Equation (3)) satisfies the following: for any K0 &isin; [K] such that &lambda;K0 &lt; &lambda;K+1 and CK0 &triangleq (1 - &lambda;K0)&Delta;-(1 - &lambda;K)2 = O(1),\n\n$$\n\\mu FL(Xu</sub) &le; 2 \\max(\\beta2\\gamma2, 1) \\cdot \\alpha + (1 + (K - K0)CK0)\\Delta\n$$\nTheorem 4.3 ensures that given sufficient unlabeled samples (proportional to the Rademacher complexity of F) such that &Delta; &rarr; 0, the clustering error &mu FL(Xu</sub) from empirical RKD is nearly as low as &mu FL(X) from population RKD, up to an additional error term that scales linearly in &Delta;."}, {"type": "heading", "lvl": 2, "value": "Label Efficiency of Cluster-aware Semi-supervised Learning", "md": "## Label Efficiency of Cluster-aware Semi-supervised Learning"}, {"type": "text", "value": "In this section, we demonstrate the label efficiency of learning from a function class with low clustering error (Definition 3.1), like the ones endowed by RKD discussed in Section 4.\n\nSpecifically, given any cluster-aware function subclass F' &sube; F with low clustering error (e.g., FL(X) and FL(Xu)), for a set of n i.i.d. labeled samples (X, y) &sim; P(x, y)n, we have the following generalization guarantee:\n\nTheorem 5.1 (Label complexity, proof in Appendix B.1). Given any cluster-aware F' &sube; F with &mu;(F') &ll; 1, assuming that (X, y) contains at least one sample per class, for any &delta; &isin; (0, 1), with probability at least 1 - &delta;/2 over (X, y) &sim; P(x, y)n, f &isin; argminf&isin;F' E(f) satisfies\n\n$$\nE_f - E(f^*) &le; 4\\sqrt{2K\\log(2n)} + 2\\mu(F') + 2\\log(4/\\delta) \\quad (5)\n$$\ne.g., conditioned on Xu, f|Xu &isin; argminf&isin;FL(Xu) n &rarr; E(f) satisfies Equation (5) with &mu;n FL(Xu).", "md": "In this section, we demonstrate the label efficiency of learning from a function class with low clustering error (Definition 3.1), like the ones endowed by RKD discussed in Section 4.\n\nSpecifically, given any cluster-aware function subclass F' &sube; F with low clustering error (e.g., FL(X) and FL(Xu)), for a set of n i.i.d. labeled samples (X, y) &sim; P(x, y)n, we have the following generalization guarantee:\n\nTheorem 5.1 (Label complexity, proof in Appendix B.1). Given any cluster-aware F' &sube; F with &mu;(F') &ll; 1, assuming that (X, y) contains at least one sample per class, for any &delta; &isin; (0, 1), with probability at least 1 - &delta;/2 over (X, y) &sim; P(x, y)n, f &isin; argminf&isin;F' E(f) satisfies\n\n$$\nE_f - E(f^*) &le; 4\\sqrt{2K\\log(2n)} + 2\\mu(F') + 2\\log(4/\\delta) \\quad (5)\n$$\ne.g., conditioned on Xu, f|Xu &isin; argminf&isin;FL(Xu) n &rarr; E(f) satisfies Equation (5) with &mu;n FL(Xu)."}]}, {"page": 8, "text": "Theorem 5.1 implies that with low clustering error (e.g., endowed by unsupervised methods like\nRKD in Theorem 4.1/Theorem 4.3, DAC in Theorem 6.2), the labeled sample complexity is as low as\nO (K), linear in the number of clusters and asymptotically optimal7 up to a logarithmic factor.\nRemark 5.1 (Class imbalance, elaborated in Appendix B.2). In Theorem 5.1, while the label\ncomplexity scales as n =      O(K) with i.i.d. sampling, we meanwhile assume that (X, y) contains at\nleast one labeled sample per class. Intuitively, when the K classes are balanced (i.e., |Xk| = |X| /K\nfor all k \u2208  [K]), an analogy to the coupon collector problem implies that n = O (K log(K)) i.i.d.\nlabeled samples are sufficient in expectation. Nevertheless, with class imbalance, the label complexity\ncan be much worse (e.g., when |XK| \u226a            |Xk| for all k \u2208    [K \u2212   1], collecting one label K takes\n1/P (XK) \u226b       K labeled samples). In Appendix B.2, we show that such label inefficiency can be\ncircumvented by leveraging a cluster-aware prediction f \u2208             F\u2032 and drawing O (log(K)) labeled\nsamples uniformly from each of the K predicted clusters, instead of i.i.d. from the entire population.\nRemark 5.2 (Coreset selection). While Theorem 5.1 yields an asymptotically optimal label complexity\nguarantee with i.i.d. sampling, it is of significant practical value to be more judicious in selecting\nlabeled samples, especially for the low-label-rate regime (e.g., n = O(K)). Therefore, in the\nexperiments (Appendix A), we further investigate\u2014alongside i.i.d. sampling8\u2014a popular coreset\nselection method [Bilmes, 2022, Krause and Golovin, 2014], where the coreset (X, y) is statistically\nmore representative of the data distribution P.\nIn particular, coreset selection can be recast as a facility location problem [Krause and Golovin,\n2014] characterized by the teacher model: maxX\u2282X                x\u2032\u2208X maxx\u2208X k\u03c8(x, x\u2032), whose optimizers\ncan be approximated heuristically via the stochastic greedy [Mirzasoleiman et al., 2015] submodular\noptimization algorithm [Schreiber et al., 2020]. Intuitively, the facility location objective encourages\nthe coreset X to be representative of the entire population X in a pairwise similarity sense, and the\ncoreset approximation identified by the stochastic greedy method is nearly optimal (in the facility\nlocation objective) up to a multiplicative constant [Mirzasoleiman et al., 2015].\n6    Data Augmentation Consistency Regularization as Clustering\nIn light of the broadness of the notion of clustering, investigating the discrepancy between different\ntypes of cluster-awareness is crucial for understanding the effect of spectral clustering brought by\nRKD beyond the low clustering error demonstrated in Section 4. In this section, we leverage the\nexisting theoretical tools from Wei and Ma [2019], Cai et al. [2021] to unify DAC regularization into\nthe cluster-aware semi-supervised learning framework in Theorem 5.1. Specifically, we demonstrate\nthe effect of DAC as a \u201clocal\u201d expansion-based clustering [Yang et al., 2023], which works in\ncomplement with the \u201cglobal\u201d spectral clustering facilitated by RKD.\nStart by recalling the formal notion of expansion-based data augmentation (Definition 6.1) and data\naugmentation consistency (DAC) error (Definition 6.2) from Wei et al. [2021]:\nDefinition 6.1 (Expansion-based data augmentation ([Wei et al., 2021] Definition 3.1)). For any\nx \u2208   X, we consider a set of class-invariant data augmentations A (x) \u2282                 X such that {x} \u228a\nA(x) \u2286      Xy\u2217(x). We say x, x\u2032 \u2208       X lie in neighborhoods of each other if their augmentation\nsets have a non-empty intersection: NB (x) \u225c             {x\u2032 \u2208   X | A(x) \u2229    A(x\u2032) \u0338= \u2205} for x \u2208      X; and\nNB(S) \u225c      x\u2208S NB (x) for S \u2286       X. Then, we quantify strength of such data augmentations via the\nc-expansion property (c > 1): for any S \u2286        X such that P (S \u2229     Xk) \u2264    P(Xk)/2 \u2200    k \u2208  [K],\n                     P (NB(S) \u2229     Xk) > min {c \u00b7 P (S \u2229      Xk) , P(Xk)} \u2200     k \u2208  [K].\nDefinition 6.2 (DAC error [Wei et al., 2021] (3.3)). For any f \u2208          F and F\u2032 \u2286     F, let\n             \u03bd (f) \u225c   Px\u223cP (x) [\u2203  x\u2032 \u2208  A(x) s.t. yf(x) \u0338= yf(x\u2032)] ,       \u03bd (F\u2032) \u225c   sup\n    7                                                                                   f\u2208F\u2032 \u03bd(f).\n     Notice that assuming at least one labeled sample per class (i.e., n \u2265K) is necessary for any generalization\nguarantees. Otherwise, depending on the missing classes, the excess risk in Equation (5) can be arbitrarily bad.\n    8For the low-label-rate experiments, to ensure consistent performance, we follow the common practice [Calder\net al., 2020] and slightly deviate from the assumption by sampling i.i.d. from each ground truth class instead.\n                                                        8", "md": "# Math Equations and Remarks\n\n## Theorem 5.1\n\nImplies that with low clustering error (e.g., endowed by unsupervised methods like RKD in Theorem 4.1/Theorem 4.3, DAC in Theorem 6.2), the labeled sample complexity is as low as O(K), linear in the number of clusters and asymptotically optimal up to a logarithmic factor.\n\n### Remark 5.1 (Class imbalance, elaborated in Appendix B.2)\n\nIn Theorem 5.1, while the label complexity scales as n = O(K) with i.i.d. sampling, we meanwhile assume that (X, y) contains at least one labeled sample per class. Intuitively, when the K classes are balanced (i.e., |Xk| = |X| / K for all k \u2208 [K]), an analogy to the coupon collector problem implies that n = O(K log(K)) i.i.d. labeled samples are sufficient in expectation. Nevertheless, with class imbalance, the label complexity can be much worse (e.g., when |XK| \u226a |Xk| for all k \u2208 [K - 1], collecting one label K takes 1/P(XK) \u226b K labeled samples).\n\n### Remark 5.2 (Coreset selection)\n\nWhile Theorem 5.1 yields an asymptotically optimal label complexity guarantee with i.i.d. sampling, it is of significant practical value to be more judicious in selecting labeled samples, especially for the low-label-rate regime (e.g., n = O(K)). Therefore, in the experiments (Appendix A), we further investigate\u2014alongside i.i.d. sampling\u2014a popular coreset selection method [Bilmes, 2022, Krause and Golovin, 2014], where the coreset (X, y) is statistically more representative of the data distribution P.\n\n### Data Augmentation Consistency Regularization as Clustering\n\nIn light of the broadness of the notion of clustering, investigating the discrepancy between different types of cluster-awareness is crucial for understanding the effect of spectral clustering brought by RKD beyond the low clustering error demonstrated in Section 4. In this section, we leverage the existing theoretical tools from Wei and Ma [2019], Cai et al. [2021] to unify DAC regularization into the cluster-aware semi-supervised learning framework in Theorem 5.1. Specifically, we demonstrate the effect of DAC as a \u201clocal\u201d expansion-based clustering [Yang et al., 2023], which works in complement with the \u201cglobal\u201d spectral clustering facilitated by RKD.\n\n### Definition 6.1 (Expansion-based data augmentation)\n\nFor any $$x \\in X$$, we consider a set of class-invariant data augmentations $$A(x) \\subset X$$ such that {x} \u228a $$A(x) \\subseteq Xy^*(x)$$. We say $$x, x' \\in X$$ lie in neighborhoods of each other if their augmentation sets have a non-empty intersection: $$NB(x) = \\{x' \\in X | A(x) \\cap A(x') \\neq \\emptyset\\}$$ for $$x \\in X$$; and $$NB(S) = \\{x \\in S | NB(x)\\}$$ for $$S \\subseteq X$$. Then, we quantify strength of such data augmentations via the c-expansion property ($$c > 1$$): for any $$S \\subseteq X$$ such that $$P(S \\cap X_k) \\leq P(X_k)/2$$ for all $$k \\in [K]$$, $$P(NB(S) \\cap X_k) > \\min\\{c \\cdot P(S \\cap X_k), P(X_k)\\}$$ for all $$k \\in [K]$$.\n\n### Definition 6.2 (DAC error)\n\nFor any $$f \\in F$$ and $$F' \\subseteq F$$, let $$\\nu(f) = P_{x \\sim P(x)} [\\exists x' \\in A(x) \\text{ s.t. } yf(x) \\neq yf(x')]$$, $$\\nu(F') = \\sup_{f \\in F'} \\nu(f)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Remarks", "md": "# Math Equations and Remarks"}, {"type": "heading", "lvl": 2, "value": "Theorem 5.1", "md": "## Theorem 5.1"}, {"type": "text", "value": "Implies that with low clustering error (e.g., endowed by unsupervised methods like RKD in Theorem 4.1/Theorem 4.3, DAC in Theorem 6.2), the labeled sample complexity is as low as O(K), linear in the number of clusters and asymptotically optimal up to a logarithmic factor.", "md": "Implies that with low clustering error (e.g., endowed by unsupervised methods like RKD in Theorem 4.1/Theorem 4.3, DAC in Theorem 6.2), the labeled sample complexity is as low as O(K), linear in the number of clusters and asymptotically optimal up to a logarithmic factor."}, {"type": "heading", "lvl": 3, "value": "Remark 5.1 (Class imbalance, elaborated in Appendix B.2)", "md": "### Remark 5.1 (Class imbalance, elaborated in Appendix B.2)"}, {"type": "text", "value": "In Theorem 5.1, while the label complexity scales as n = O(K) with i.i.d. sampling, we meanwhile assume that (X, y) contains at least one labeled sample per class. Intuitively, when the K classes are balanced (i.e., |Xk| = |X| / K for all k \u2208 [K]), an analogy to the coupon collector problem implies that n = O(K log(K)) i.i.d. labeled samples are sufficient in expectation. Nevertheless, with class imbalance, the label complexity can be much worse (e.g., when |XK| \u226a |Xk| for all k \u2208 [K - 1], collecting one label K takes 1/P(XK) \u226b K labeled samples).", "md": "In Theorem 5.1, while the label complexity scales as n = O(K) with i.i.d. sampling, we meanwhile assume that (X, y) contains at least one labeled sample per class. Intuitively, when the K classes are balanced (i.e., |Xk| = |X| / K for all k \u2208 [K]), an analogy to the coupon collector problem implies that n = O(K log(K)) i.i.d. labeled samples are sufficient in expectation. Nevertheless, with class imbalance, the label complexity can be much worse (e.g., when |XK| \u226a |Xk| for all k \u2208 [K - 1], collecting one label K takes 1/P(XK) \u226b K labeled samples)."}, {"type": "heading", "lvl": 3, "value": "Remark 5.2 (Coreset selection)", "md": "### Remark 5.2 (Coreset selection)"}, {"type": "text", "value": "While Theorem 5.1 yields an asymptotically optimal label complexity guarantee with i.i.d. sampling, it is of significant practical value to be more judicious in selecting labeled samples, especially for the low-label-rate regime (e.g., n = O(K)). Therefore, in the experiments (Appendix A), we further investigate\u2014alongside i.i.d. sampling\u2014a popular coreset selection method [Bilmes, 2022, Krause and Golovin, 2014], where the coreset (X, y) is statistically more representative of the data distribution P.", "md": "While Theorem 5.1 yields an asymptotically optimal label complexity guarantee with i.i.d. sampling, it is of significant practical value to be more judicious in selecting labeled samples, especially for the low-label-rate regime (e.g., n = O(K)). Therefore, in the experiments (Appendix A), we further investigate\u2014alongside i.i.d. sampling\u2014a popular coreset selection method [Bilmes, 2022, Krause and Golovin, 2014], where the coreset (X, y) is statistically more representative of the data distribution P."}, {"type": "heading", "lvl": 3, "value": "Data Augmentation Consistency Regularization as Clustering", "md": "### Data Augmentation Consistency Regularization as Clustering"}, {"type": "text", "value": "In light of the broadness of the notion of clustering, investigating the discrepancy between different types of cluster-awareness is crucial for understanding the effect of spectral clustering brought by RKD beyond the low clustering error demonstrated in Section 4. In this section, we leverage the existing theoretical tools from Wei and Ma [2019], Cai et al. [2021] to unify DAC regularization into the cluster-aware semi-supervised learning framework in Theorem 5.1. Specifically, we demonstrate the effect of DAC as a \u201clocal\u201d expansion-based clustering [Yang et al., 2023], which works in complement with the \u201cglobal\u201d spectral clustering facilitated by RKD.", "md": "In light of the broadness of the notion of clustering, investigating the discrepancy between different types of cluster-awareness is crucial for understanding the effect of spectral clustering brought by RKD beyond the low clustering error demonstrated in Section 4. In this section, we leverage the existing theoretical tools from Wei and Ma [2019], Cai et al. [2021] to unify DAC regularization into the cluster-aware semi-supervised learning framework in Theorem 5.1. Specifically, we demonstrate the effect of DAC as a \u201clocal\u201d expansion-based clustering [Yang et al., 2023], which works in complement with the \u201cglobal\u201d spectral clustering facilitated by RKD."}, {"type": "heading", "lvl": 3, "value": "Definition 6.1 (Expansion-based data augmentation)", "md": "### Definition 6.1 (Expansion-based data augmentation)"}, {"type": "text", "value": "For any $$x \\in X$$, we consider a set of class-invariant data augmentations $$A(x) \\subset X$$ such that {x} \u228a $$A(x) \\subseteq Xy^*(x)$$. We say $$x, x' \\in X$$ lie in neighborhoods of each other if their augmentation sets have a non-empty intersection: $$NB(x) = \\{x' \\in X | A(x) \\cap A(x') \\neq \\emptyset\\}$$ for $$x \\in X$$; and $$NB(S) = \\{x \\in S | NB(x)\\}$$ for $$S \\subseteq X$$. Then, we quantify strength of such data augmentations via the c-expansion property ($$c > 1$$): for any $$S \\subseteq X$$ such that $$P(S \\cap X_k) \\leq P(X_k)/2$$ for all $$k \\in [K]$$, $$P(NB(S) \\cap X_k) > \\min\\{c \\cdot P(S \\cap X_k), P(X_k)\\}$$ for all $$k \\in [K]$$.", "md": "For any $$x \\in X$$, we consider a set of class-invariant data augmentations $$A(x) \\subset X$$ such that {x} \u228a $$A(x) \\subseteq Xy^*(x)$$. We say $$x, x' \\in X$$ lie in neighborhoods of each other if their augmentation sets have a non-empty intersection: $$NB(x) = \\{x' \\in X | A(x) \\cap A(x') \\neq \\emptyset\\}$$ for $$x \\in X$$; and $$NB(S) = \\{x \\in S | NB(x)\\}$$ for $$S \\subseteq X$$. Then, we quantify strength of such data augmentations via the c-expansion property ($$c > 1$$): for any $$S \\subseteq X$$ such that $$P(S \\cap X_k) \\leq P(X_k)/2$$ for all $$k \\in [K]$$, $$P(NB(S) \\cap X_k) > \\min\\{c \\cdot P(S \\cap X_k), P(X_k)\\}$$ for all $$k \\in [K]$$."}, {"type": "heading", "lvl": 3, "value": "Definition 6.2 (DAC error)", "md": "### Definition 6.2 (DAC error)"}, {"type": "text", "value": "For any $$f \\in F$$ and $$F' \\subseteq F$$, let $$\\nu(f) = P_{x \\sim P(x)} [\\exists x' \\in A(x) \\text{ s.t. } yf(x) \\neq yf(x')]$$, $$\\nu(F') = \\sup_{f \\in F'} \\nu(f)$$.", "md": "For any $$f \\in F$$ and $$F' \\subseteq F$$, let $$\\nu(f) = P_{x \\sim P(x)} [\\exists x' \\in A(x) \\text{ s.t. } yf(x) \\neq yf(x')]$$, $$\\nu(F') = \\sup_{f \\in F'} \\nu(f)$$."}]}, {"page": 9, "text": "We adopt the theoretical framework in Wei et al. [2021] for generic neural networks with smooth\n activation function \u03d5 and consider\n         F =        f(x) = Ap\u03d5 (\u00b7 \u00b7 \u00b7 A2\u03d5(A1x) \u00b7 \u00b7 \u00b7 )                                                                 max             .\n                                                                        A\u03b9 \u2208     Rd\u03b9\u00d7d\u03b9\u22121 \u2200       \u03b9 \u2208   [p], d =     \u03b9=0,\u00b7\u00b7\u00b7 ,p d\u03b9\nWith such function class F of p-layer neural networks with maximum width d and weights {A\u03b9}p                                               \u03b9=1,\n we recall the notion of all-layer margin from Wei et al. [2021] Appendix C.2. By decomposing\n f = f2p\u22121 \u25e6        \u00b7 \u00b7 \u00b7 \u25e6f1 such that f2\u03b9\u22121(z) = A\u03b9z for all \u03b9 \u2208                    [p] and f2\u03b9(z) = \u03d5(z) for all \u03b9 \u2208               [p \u2212   1],\n we consider a perturbation \u03b4 = (\u03b41, \u00b7 \u00b7 \u00b7 , \u03b42p\u22121) (where \u03b42\u03b9\u22121, \u03b42\u03b9 \u2208                             Rd\u03b9) to each layer of f:\n                       f1 (x, \u03b4) = f1 (x, \u03b41) = f1(x) + \u03b41 \u2225x\u22252 ,\n                       f\u03b9 (x, \u03b4) = f\u03b9 (x, \u03b41, \u00b7 \u00b7 \u00b7 , \u03b4\u03b9) = f\u03b9 (f\u03b9\u22121 (x, \u03b4)) + \u03b4\u03b9 \u2225f\u03b9\u22121 (x, \u03b4)\u22252\n such that f(x, \u03b4) = f2p\u22121 (x, \u03b4). Then, the all-layer margin m : F \u00d7 X \u00d7 [K] \u2192                                        R\u22650 is defined as\n the minimum norm of \u03b4 that is sufficient to perturb the classification of f(x):\n                          m (f, x, y) \u225c        min     2p\u22121      \u2225\u03b4\u03b9\u22252 2     s.t.    argmax      f (x, \u03b4)k \u0338= y.                            (6)\n                                                 \u03b4        \u03b9=1                          k\u2208[K]\n Moreover, Wei et al. [2021] introduced the robust margin for the expansion-based data augmentation\n A (Definition 6.1), which intuitively quantifies the worst-case preservation of the label-relevant\n information in the augmentations of x, measured with respect to f:\n                                            mA (f, x) \u225c           min                                                                       (7)\n                                                                x\u2032\u2208A(x) m (f, x\u2032, yf(x)) .\nWe say the expansion-based data augmentation A is margin-robust with respect to the ground truth\n f\u2217  if minx\u2208X mA (f\u2217, x) > 0 is reasonably large.\n Then, we cast DAC regularization9 as reducing the function class F via constraints on the robust\n margins at the N unlabeled samples Xu: for some 0 < \u03c4 < minx\u2208X mA (f\u2217, x),\n                                       F\u03c4Xu \u225c     {f \u2208     F | mA (f, xu      i ) \u2265   \u03c4 \u2200   i \u2208  [N]} .                                     (8)\n Leverage the existing unlabeled sample complexity bound for DAC error from Wei et al. [2021]:\n Proposition 6.1 (Unlabeled sample complexity of DAC ([Wei et al., 2021] Theorem 3.7)). Given\n any \u03b4 \u2208     (0, 1), with probability at least 1 \u2212              \u03b4/2 over Xu,\n                     \u03bd (F\u03c4            O    p    \u03b9=1   \u221a\u221a d \u2225A\u03b9\u2225F          + O           log (1/\u03b4) + p log (N)              ,\n                           Xu) \u2264                     \u03c4    N                                           N\n where    O (\u00b7) suppresses polylogarithmic factors in N and d.\n The clustering error of Equation (8) is as low as its DAC error, up to a constant factor that depends on\n the augmentation strength, characterized by the c-expansion (Definition 6.1):\n Theorem 6.2 (Expansion-based clustering, proof in Appendix E.3). For Equation (8) with A ad-\n mitting c-expansion (Definition 6.1), the clustering error is upper bounded by the DAC error in\n Proposition 6.1:\n                                           \u00b5 (F\u03c4  Xu) \u2264     max        c \u22122  1, 2     \u00b7 \u03bd (F\u03c4 Xu) .\n In Theorem 6.2, (i) c characterizes the augmentation strength (i.e., the perturbation on label-irrelevant\n information), whereas (ii) \u03c4 quantifies the margin robustness (i.e., the preservation of label-relevant\n information) for the expansion-based data augmentation. Ideally, we would like both c and \u03c4 to\n be reasonably large, while there exist trade-offs between the augmentation strength and margin\n robustness (e.g., overly strong augmentations inevitably perturb label-relevant information).\n     9In Appendix E.2, we further bridge the conceptual notion of DAC regularization in Equation (8) with\n common DAC regularization algorithms in practice like FixMatch [Sohn et al., 2020].\n                                                                       9", "md": "We adopt the theoretical framework in Wei et al. [2021] for generic neural networks with smooth activation function \u03d5 and consider\n\n$$\nF = f(x) = A^p\\phi(A^{p-1}\\phi(\\ldots A^2\\phi(A^1x) \\ldots)) \\quad \\text{max}\n$$\n\n$$\nA^{(\\iota)} \\in \\mathbb{R}^{d_{\\iota} \\times d_{\\iota-1}} \\quad \\forall \\iota \\in [p], \\quad d = \\sum_{\\iota=0}^{p} d_{\\iota}\n$$\n\nWith such function class F of p-layer neural networks with maximum width d and weights $\\{A^{\\iota}\\}_{\\iota=1}$, we recall the notion of all-layer margin from Wei et al. [2021] Appendix C.2. By decomposing\n\n$$\nf = f_{2p-1} \\circ \\ldots \\circ f_1\n$$\n\nsuch that $f_{2\\iota-1}(z) = A^{\\iota}z$ for all $\\iota \\in [p]$ and $f_{2\\iota}(z) = \\phi(z)$ for all $\\iota \\in [p-1]$, we consider a perturbation $\\delta = (\\delta_1, \\ldots, \\delta_{2p-1})$ (where $\\delta_{2\\iota-1}, \\delta_{2\\iota} \\in \\mathbb{R}^{d_{\\iota}}$) to each layer of $f$:\n\n$$\nf_1(x, \\delta) = f_1(x, \\delta_1) = f_1(x) + \\delta_1 \\|x\\|_2,\n$$\n\n$$\nf_{\\iota}(x, \\delta) = f_{\\iota}(x, \\delta_1, \\ldots, \\delta_{\\iota}) = f_{\\iota}(f_{\\iota-1}(x, \\delta)) + \\delta_{\\iota} \\|f_{\\iota-1}(x, \\delta)\\|_2\n$$\n\nsuch that $f(x, \\delta) = f_{2p-1}(x, \\delta)$. Then, the all-layer margin $m : F \\times X \\times [K] \\rightarrow \\mathbb{R}_{\\geq 0}$ is defined as the minimum norm of $\\delta$ that is sufficient to perturb the classification of $f(x)$:\n\n$$\nm(f, x, y) \\triangleq \\min_{\\delta} \\sum_{\\iota=1}^{2p-1} \\|\\delta_{\\iota}\\|_2^2 \\quad \\text{s.t.} \\quad \\arg\\max_k f(x, \\delta)_k \\neq y. \\quad (6)\n$$\n\nMoreover, Wei et al. [2021] introduced the robust margin for the expansion-based data augmentation $A$ (Definition 6.1), which intuitively quantifies the worst-case preservation of the label-relevant information in the augmentations of $x$, measured with respect to $f$:\n\n$$\nm_A(f, x) \\triangleq \\min_{x' \\in A(x)} m(f, x', y_{f(x)}). \\quad (7)\n$$\n\nWe say the expansion-based data augmentation $A$ is margin-robust with respect to the ground truth $f^*$ if $\\min_{x \\in X} m_A(f^*, x) > 0$ is reasonably large.\n\nThen, we cast DAC regularization as reducing the function class $F$ via constraints on the robust margins at the $N$ unlabeled samples $X_u$: for some $0 < \\tau < \\min_{x \\in X} m_A(f^*, x)$,\n\n$$\nF_{\\tau X_u} \\triangleq \\{f \\in F \\,|\\, m_A(f, x_{u_i}) \\geq \\tau \\, \\forall i \\in [N]\\}. \\quad (8)\n$$\n\nLeverage the existing unlabeled sample complexity bound for DAC error from Wei et al. [2021]:\n\nProposition 6.1 (Unlabeled sample complexity of DAC ([Wei et al., 2021] Theorem 3.7)). Given any $\\delta \\in (0, 1)$, with probability at least $1 - \\delta/2$ over $X_u$,\n\n$$\n\\nu(F_{\\tau X_u}) \\leq \\tau \\cdot O\\left(p \\sum_{\\iota=1}^{p} \\sqrt{d} \\|A^{\\iota}\\|_F + O\\log\\left(\\frac{1}{\\delta}\\right) + p\\log(N)\\right),\n$$\n\nwhere $O(\\cdot)$ suppresses polylogarithmic factors in $N$ and $d$.\n\nThe clustering error of Equation (8) is as low as its DAC error, up to a constant factor that depends on the augmentation strength, characterized by the c-expansion (Definition 6.1):\n\nTheorem 6.2 (Expansion-based clustering, proof in Appendix E.3). For Equation (8) with $A$ admitting c-expansion (Definition 6.1), the clustering error is upper bounded by the DAC error in Proposition 6.1:\n\n$$\n\\mu(F_{\\tau X_u}) \\leq \\max\\left(c^{-2}, 2\\right) \\cdot \\nu(F_{\\tau X_u}).\n$$\n\nIn Theorem 6.2, (i) $c$ characterizes the augmentation strength (i.e., the perturbation on label-irrelevant information), whereas (ii) $\\tau$ quantifies the margin robustness (i.e., the preservation of label-relevant information) for the expansion-based data augmentation. Ideally, we would like both $c$ and $\\tau$ to be reasonably large, while there exist trade-offs between the augmentation strength and margin robustness (e.g., overly strong augmentations inevitably perturb label-relevant information).\n\nIn Appendix E.2, we further bridge the conceptual notion of DAC regularization in Equation (8) with common DAC regularization algorithms in practice like FixMatch [Sohn et al., 2020].", "images": [], "items": [{"type": "text", "value": "We adopt the theoretical framework in Wei et al. [2021] for generic neural networks with smooth activation function \u03d5 and consider\n\n$$\nF = f(x) = A^p\\phi(A^{p-1}\\phi(\\ldots A^2\\phi(A^1x) \\ldots)) \\quad \\text{max}\n$$\n\n$$\nA^{(\\iota)} \\in \\mathbb{R}^{d_{\\iota} \\times d_{\\iota-1}} \\quad \\forall \\iota \\in [p], \\quad d = \\sum_{\\iota=0}^{p} d_{\\iota}\n$$\n\nWith such function class F of p-layer neural networks with maximum width d and weights $\\{A^{\\iota}\\}_{\\iota=1}$, we recall the notion of all-layer margin from Wei et al. [2021] Appendix C.2. By decomposing\n\n$$\nf = f_{2p-1} \\circ \\ldots \\circ f_1\n$$\n\nsuch that $f_{2\\iota-1}(z) = A^{\\iota}z$ for all $\\iota \\in [p]$ and $f_{2\\iota}(z) = \\phi(z)$ for all $\\iota \\in [p-1]$, we consider a perturbation $\\delta = (\\delta_1, \\ldots, \\delta_{2p-1})$ (where $\\delta_{2\\iota-1}, \\delta_{2\\iota} \\in \\mathbb{R}^{d_{\\iota}}$) to each layer of $f$:\n\n$$\nf_1(x, \\delta) = f_1(x, \\delta_1) = f_1(x) + \\delta_1 \\|x\\|_2,\n$$\n\n$$\nf_{\\iota}(x, \\delta) = f_{\\iota}(x, \\delta_1, \\ldots, \\delta_{\\iota}) = f_{\\iota}(f_{\\iota-1}(x, \\delta)) + \\delta_{\\iota} \\|f_{\\iota-1}(x, \\delta)\\|_2\n$$\n\nsuch that $f(x, \\delta) = f_{2p-1}(x, \\delta)$. Then, the all-layer margin $m : F \\times X \\times [K] \\rightarrow \\mathbb{R}_{\\geq 0}$ is defined as the minimum norm of $\\delta$ that is sufficient to perturb the classification of $f(x)$:\n\n$$\nm(f, x, y) \\triangleq \\min_{\\delta} \\sum_{\\iota=1}^{2p-1} \\|\\delta_{\\iota}\\|_2^2 \\quad \\text{s.t.} \\quad \\arg\\max_k f(x, \\delta)_k \\neq y. \\quad (6)\n$$\n\nMoreover, Wei et al. [2021] introduced the robust margin for the expansion-based data augmentation $A$ (Definition 6.1), which intuitively quantifies the worst-case preservation of the label-relevant information in the augmentations of $x$, measured with respect to $f$:\n\n$$\nm_A(f, x) \\triangleq \\min_{x' \\in A(x)} m(f, x', y_{f(x)}). \\quad (7)\n$$\n\nWe say the expansion-based data augmentation $A$ is margin-robust with respect to the ground truth $f^*$ if $\\min_{x \\in X} m_A(f^*, x) > 0$ is reasonably large.\n\nThen, we cast DAC regularization as reducing the function class $F$ via constraints on the robust margins at the $N$ unlabeled samples $X_u$: for some $0 < \\tau < \\min_{x \\in X} m_A(f^*, x)$,\n\n$$\nF_{\\tau X_u} \\triangleq \\{f \\in F \\,|\\, m_A(f, x_{u_i}) \\geq \\tau \\, \\forall i \\in [N]\\}. \\quad (8)\n$$\n\nLeverage the existing unlabeled sample complexity bound for DAC error from Wei et al. [2021]:\n\nProposition 6.1 (Unlabeled sample complexity of DAC ([Wei et al., 2021] Theorem 3.7)). Given any $\\delta \\in (0, 1)$, with probability at least $1 - \\delta/2$ over $X_u$,\n\n$$\n\\nu(F_{\\tau X_u}) \\leq \\tau \\cdot O\\left(p \\sum_{\\iota=1}^{p} \\sqrt{d} \\|A^{\\iota}\\|_F + O\\log\\left(\\frac{1}{\\delta}\\right) + p\\log(N)\\right),\n$$\n\nwhere $O(\\cdot)$ suppresses polylogarithmic factors in $N$ and $d$.\n\nThe clustering error of Equation (8) is as low as its DAC error, up to a constant factor that depends on the augmentation strength, characterized by the c-expansion (Definition 6.1):\n\nTheorem 6.2 (Expansion-based clustering, proof in Appendix E.3). For Equation (8) with $A$ admitting c-expansion (Definition 6.1), the clustering error is upper bounded by the DAC error in Proposition 6.1:\n\n$$\n\\mu(F_{\\tau X_u}) \\leq \\max\\left(c^{-2}, 2\\right) \\cdot \\nu(F_{\\tau X_u}).\n$$\n\nIn Theorem 6.2, (i) $c$ characterizes the augmentation strength (i.e., the perturbation on label-irrelevant information), whereas (ii) $\\tau$ quantifies the margin robustness (i.e., the preservation of label-relevant information) for the expansion-based data augmentation. Ideally, we would like both $c$ and $\\tau$ to be reasonably large, while there exist trade-offs between the augmentation strength and margin robustness (e.g., overly strong augmentations inevitably perturb label-relevant information).\n\nIn Appendix E.2, we further bridge the conceptual notion of DAC regularization in Equation (8) with common DAC regularization algorithms in practice like FixMatch [Sohn et al., 2020].", "md": "We adopt the theoretical framework in Wei et al. [2021] for generic neural networks with smooth activation function \u03d5 and consider\n\n$$\nF = f(x) = A^p\\phi(A^{p-1}\\phi(\\ldots A^2\\phi(A^1x) \\ldots)) \\quad \\text{max}\n$$\n\n$$\nA^{(\\iota)} \\in \\mathbb{R}^{d_{\\iota} \\times d_{\\iota-1}} \\quad \\forall \\iota \\in [p], \\quad d = \\sum_{\\iota=0}^{p} d_{\\iota}\n$$\n\nWith such function class F of p-layer neural networks with maximum width d and weights $\\{A^{\\iota}\\}_{\\iota=1}$, we recall the notion of all-layer margin from Wei et al. [2021] Appendix C.2. By decomposing\n\n$$\nf = f_{2p-1} \\circ \\ldots \\circ f_1\n$$\n\nsuch that $f_{2\\iota-1}(z) = A^{\\iota}z$ for all $\\iota \\in [p]$ and $f_{2\\iota}(z) = \\phi(z)$ for all $\\iota \\in [p-1]$, we consider a perturbation $\\delta = (\\delta_1, \\ldots, \\delta_{2p-1})$ (where $\\delta_{2\\iota-1}, \\delta_{2\\iota} \\in \\mathbb{R}^{d_{\\iota}}$) to each layer of $f$:\n\n$$\nf_1(x, \\delta) = f_1(x, \\delta_1) = f_1(x) + \\delta_1 \\|x\\|_2,\n$$\n\n$$\nf_{\\iota}(x, \\delta) = f_{\\iota}(x, \\delta_1, \\ldots, \\delta_{\\iota}) = f_{\\iota}(f_{\\iota-1}(x, \\delta)) + \\delta_{\\iota} \\|f_{\\iota-1}(x, \\delta)\\|_2\n$$\n\nsuch that $f(x, \\delta) = f_{2p-1}(x, \\delta)$. Then, the all-layer margin $m : F \\times X \\times [K] \\rightarrow \\mathbb{R}_{\\geq 0}$ is defined as the minimum norm of $\\delta$ that is sufficient to perturb the classification of $f(x)$:\n\n$$\nm(f, x, y) \\triangleq \\min_{\\delta} \\sum_{\\iota=1}^{2p-1} \\|\\delta_{\\iota}\\|_2^2 \\quad \\text{s.t.} \\quad \\arg\\max_k f(x, \\delta)_k \\neq y. \\quad (6)\n$$\n\nMoreover, Wei et al. [2021] introduced the robust margin for the expansion-based data augmentation $A$ (Definition 6.1), which intuitively quantifies the worst-case preservation of the label-relevant information in the augmentations of $x$, measured with respect to $f$:\n\n$$\nm_A(f, x) \\triangleq \\min_{x' \\in A(x)} m(f, x', y_{f(x)}). \\quad (7)\n$$\n\nWe say the expansion-based data augmentation $A$ is margin-robust with respect to the ground truth $f^*$ if $\\min_{x \\in X} m_A(f^*, x) > 0$ is reasonably large.\n\nThen, we cast DAC regularization as reducing the function class $F$ via constraints on the robust margins at the $N$ unlabeled samples $X_u$: for some $0 < \\tau < \\min_{x \\in X} m_A(f^*, x)$,\n\n$$\nF_{\\tau X_u} \\triangleq \\{f \\in F \\,|\\, m_A(f, x_{u_i}) \\geq \\tau \\, \\forall i \\in [N]\\}. \\quad (8)\n$$\n\nLeverage the existing unlabeled sample complexity bound for DAC error from Wei et al. [2021]:\n\nProposition 6.1 (Unlabeled sample complexity of DAC ([Wei et al., 2021] Theorem 3.7)). Given any $\\delta \\in (0, 1)$, with probability at least $1 - \\delta/2$ over $X_u$,\n\n$$\n\\nu(F_{\\tau X_u}) \\leq \\tau \\cdot O\\left(p \\sum_{\\iota=1}^{p} \\sqrt{d} \\|A^{\\iota}\\|_F + O\\log\\left(\\frac{1}{\\delta}\\right) + p\\log(N)\\right),\n$$\n\nwhere $O(\\cdot)$ suppresses polylogarithmic factors in $N$ and $d$.\n\nThe clustering error of Equation (8) is as low as its DAC error, up to a constant factor that depends on the augmentation strength, characterized by the c-expansion (Definition 6.1):\n\nTheorem 6.2 (Expansion-based clustering, proof in Appendix E.3). For Equation (8) with $A$ admitting c-expansion (Definition 6.1), the clustering error is upper bounded by the DAC error in Proposition 6.1:\n\n$$\n\\mu(F_{\\tau X_u}) \\leq \\max\\left(c^{-2}, 2\\right) \\cdot \\nu(F_{\\tau X_u}).\n$$\n\nIn Theorem 6.2, (i) $c$ characterizes the augmentation strength (i.e., the perturbation on label-irrelevant information), whereas (ii) $\\tau$ quantifies the margin robustness (i.e., the preservation of label-relevant information) for the expansion-based data augmentation. Ideally, we would like both $c$ and $\\tau$ to be reasonably large, while there exist trade-offs between the augmentation strength and margin robustness (e.g., overly strong augmentations inevitably perturb label-relevant information).\n\nIn Appendix E.2, we further bridge the conceptual notion of DAC regularization in Equation (8) with common DAC regularization algorithms in practice like FixMatch [Sohn et al., 2020]."}]}, {"page": 10, "text": "Remark 6.1 (Clustering with DAC v.s. RKD). As Figure 1 demonstrated, in contrast to RKD that\nunveils the spectral clustering of a population-induced graph from a \u201cglobal\u201d perspective, DAC\nalternatively learns a \u201clocal\u201d clustering structure through the expansion of neighborhoods NB (\u00b7)\ncharacterized by sets of data augmentations A(\u00b7) (Definition 6.1).\nTherefore, DAC and RKD provide complementary perspectives for each other. On one hand, the \u201clocal\u201d\nclustering structure revealed by DAC effectively ensures a reasonable margin in Assumption 4.2\n(Remark 4.1). On the other hand, when the augmentation strength c is insufficient (i.e., A(\u00b7) is\nnot expansive enough) for DAC to achieve a low clustering error \u00b5 (F\u03c4         Xu) (Theorem 6.2), the\nsupplementary \u201cglobal\u201d perspective of RKD connects the non-overlapping neighborhoods in the\nsame classes and brings better classification accuracies, as we empirically verified in Appendix A.\n7   Discussions, Limitations, and Future Directions\nThis work provides a theoretical analysis of relational knowledge distillation (RKD) in the semi-\nsupervised classification setting from a clustering perspective. Through a cluster-aware semi-\nsupervised learning (SSL) framework characterized by a notion of low clustering error, we demon-\nstrate that RKD achieves a nearly optimal label complexity (linear in the number of clusters) by\nlearning the underlying geometry of the population as revealed by the teacher model via spectral\nclustering. With a unified view of data augmentation consistency (DAC) regularization in the cluster-\naware SSL framework, we further illustrate the complementary \u201cglobal\u201d and \u201clocal\u201d perspectives of\nclustering learned by RKD and DAC, respectively.\nAs an appealing future direction, domain adaptation in knowledge distillation is a common scenario\nin practice where the training data of the teacher and student models come from different distributions.\nAlthough distributional shifts can implicitly reflect the alignment between the ground truth and the\nteacher model (Assumption 4.1) in our analysis, an explicit and specialized study on RKD in the\ndomain adaptation setting may lead to better theoretical guarantees and deeper insights.\nAnother avenue for future exploration involves RKD on different (hidden) layers of the neural\nnetwork. In this work, we consider RKD on the output layer following the standard practice [Park\net al., 2019], whereas RKD on additional hidden layers has been shown to further improve the\nperformance [Liu et al., 2019]. While the analysis in Section 4 remains valid for hidden-layer features\nof low dimensions (\u2248    K), Assumption 4.1 tends to fail (i.e., \u03b1 can be large) for high-dimensional\nfeatures, which may require separate and careful consideration.\nAcknowledgement\nYD was supported in part by the Office of Naval Research N00014-18-1-2354, NSF DMS-1952735,\nDOE ASCR DE-SC0022251, and UT Austin Graduate School Summer Fellowship. KM was sup-\nported by the Peter J. O\u2019Donnell Jr. Postdoctoral Fellowship at the Oden Institute at the University of\nTexas, Austin. QL acknowledges the support of NYU Research Catalyst Prize and Whitehead Fellow-\nship for Junior Faculty in Biomedical and Biological Sciences. RW was supported in part by AFOSR\nMURI FA9550-19-1-0005, NSF DMS-1952735, NSF IFML grant 2019844, NSF DMS-N2109155,\nand NSF 2217033. The authors wish to thank IFML for generously providing computational resources,\nas well as the anonymous reviewers for their helpful comments and suggestions.\nReferences\nZ. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation\n  in deep learning. arXiv preprint arXiv:2012.09816, 2020.\nJ. Ba and R. Caruana. Do deep nets really need to be deep?           Advances in neural information\n  processing systems, 27, 2014.\nP. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. Advances in neural\n  information processing systems, 27, 2014.\nP. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural\n  results. J. Mach. Learn. Res., 3(null):463\u2013482, Mar. 2003. ISSN 1532-4435.\nM. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Machine learning, 56:\n  209\u2013239, 2004.\n                                                  10", "md": "Remark 6.1 (Clustering with DAC v.s. RKD). As Figure 1 demonstrated, in contrast to RKD that\nunveils the spectral clustering of a population-induced graph from a \u201cglobal\u201d perspective, DAC\nalternatively learns a \u201clocal\u201d clustering structure through the expansion of neighborhoods NB (\u00b7)\ncharacterized by sets of data augmentations A(\u00b7) (Definition 6.1).\nTherefore, DAC and RKD provide complementary perspectives for each other. On one hand, the \u201clocal\u201d\nclustering structure revealed by DAC effectively ensures a reasonable margin in Assumption 4.2\n(Remark 4.1). On the other hand, when the augmentation strength c is insufficient (i.e., A(\u00b7) is\nnot expansive enough) for DAC to achieve a low clustering error \u00b5 (F\u03c4         Xu) (Theorem 6.2), the\nsupplementary \u201cglobal\u201d perspective of RKD connects the non-overlapping neighborhoods in the\nsame classes and brings better classification accuracies, as we empirically verified in Appendix A.\n\n7   Discussions, Limitations, and Future Directions\nThis work provides a theoretical analysis of relational knowledge distillation (RKD) in the semi-\nsupervised classification setting from a clustering perspective. Through a cluster-aware semi-\nsupervised learning (SSL) framework characterized by a notion of low clustering error, we demon-\nstrate that RKD achieves a nearly optimal label complexity (linear in the number of clusters) by\nlearning the underlying geometry of the population as revealed by the teacher model via spectral\nclustering. With a unified view of data augmentation consistency (DAC) regularization in the cluster-\naware SSL framework, we further illustrate the complementary \u201cglobal\u201d and \u201clocal\u201d perspectives of\nclustering learned by RKD and DAC, respectively.\nAs an appealing future direction, domain adaptation in knowledge distillation is a common scenario\nin practice where the training data of the teacher and student models come from different distributions.\nAlthough distributional shifts can implicitly reflect the alignment between the ground truth and the\nteacher model (Assumption 4.1) in our analysis, an explicit and specialized study on RKD in the\ndomain adaptation setting may lead to better theoretical guarantees and deeper insights.\nAnother avenue for future exploration involves RKD on different (hidden) layers of the neural\nnetwork. In this work, we consider RKD on the output layer following the standard practice [Park\net al., 2019], whereas RKD on additional hidden layers has been shown to further improve the\nperformance [Liu et al., 2019]. While the analysis in Section 4 remains valid for hidden-layer features\nof low dimensions ($$ \\approx K $$), Assumption 4.1 tends to fail (i.e., \u03b1 can be large) for high-dimensional\nfeatures, which may require separate and careful consideration.\n\nAcknowledgement\nYD was supported in part by the Office of Naval Research N00014-18-1-2354, NSF DMS-1952735,\nDOE ASCR DE-SC0022251, and UT Austin Graduate School Summer Fellowship. KM was sup-\nported by the Peter J. O\u2019Donnell Jr. Postdoctoral Fellowship at the Oden Institute at the University of\nTexas, Austin. QL acknowledges the support of NYU Research Catalyst Prize and Whitehead Fellow-\nship for Junior Faculty in Biomedical and Biological Sciences. RW was supported in part by AFOSR\nMURI FA9550-19-1-0005, NSF DMS-1952735, NSF IFML grant 2019844, NSF DMS-N2109155,\nand NSF 2217033. The authors wish to thank IFML for generously providing computational resources,\nas well as the anonymous reviewers for their helpful comments and suggestions.\n\nReferences\nZ. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation\nin deep learning. arXiv preprint arXiv:2012.09816, 2020.\nJ. Ba and R. Caruana. Do deep nets really need to be deep?           Advances in neural information\nprocessing systems, 27, 2014.\nP. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. Advances in neural\ninformation processing systems, 27, 2014.\nP. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural\nresults. J. Mach. Learn. Res., 3(null):463\u2013482, Mar. 2003. ISSN 1532-4435.\nM. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Machine learning, 56:\n209\u2013239, 2004.", "images": [], "items": [{"type": "text", "value": "Remark 6.1 (Clustering with DAC v.s. RKD). As Figure 1 demonstrated, in contrast to RKD that\nunveils the spectral clustering of a population-induced graph from a \u201cglobal\u201d perspective, DAC\nalternatively learns a \u201clocal\u201d clustering structure through the expansion of neighborhoods NB (\u00b7)\ncharacterized by sets of data augmentations A(\u00b7) (Definition 6.1).\nTherefore, DAC and RKD provide complementary perspectives for each other. On one hand, the \u201clocal\u201d\nclustering structure revealed by DAC effectively ensures a reasonable margin in Assumption 4.2\n(Remark 4.1). On the other hand, when the augmentation strength c is insufficient (i.e., A(\u00b7) is\nnot expansive enough) for DAC to achieve a low clustering error \u00b5 (F\u03c4         Xu) (Theorem 6.2), the\nsupplementary \u201cglobal\u201d perspective of RKD connects the non-overlapping neighborhoods in the\nsame classes and brings better classification accuracies, as we empirically verified in Appendix A.\n\n7   Discussions, Limitations, and Future Directions\nThis work provides a theoretical analysis of relational knowledge distillation (RKD) in the semi-\nsupervised classification setting from a clustering perspective. Through a cluster-aware semi-\nsupervised learning (SSL) framework characterized by a notion of low clustering error, we demon-\nstrate that RKD achieves a nearly optimal label complexity (linear in the number of clusters) by\nlearning the underlying geometry of the population as revealed by the teacher model via spectral\nclustering. With a unified view of data augmentation consistency (DAC) regularization in the cluster-\naware SSL framework, we further illustrate the complementary \u201cglobal\u201d and \u201clocal\u201d perspectives of\nclustering learned by RKD and DAC, respectively.\nAs an appealing future direction, domain adaptation in knowledge distillation is a common scenario\nin practice where the training data of the teacher and student models come from different distributions.\nAlthough distributional shifts can implicitly reflect the alignment between the ground truth and the\nteacher model (Assumption 4.1) in our analysis, an explicit and specialized study on RKD in the\ndomain adaptation setting may lead to better theoretical guarantees and deeper insights.\nAnother avenue for future exploration involves RKD on different (hidden) layers of the neural\nnetwork. In this work, we consider RKD on the output layer following the standard practice [Park\net al., 2019], whereas RKD on additional hidden layers has been shown to further improve the\nperformance [Liu et al., 2019]. While the analysis in Section 4 remains valid for hidden-layer features\nof low dimensions ($$ \\approx K $$), Assumption 4.1 tends to fail (i.e., \u03b1 can be large) for high-dimensional\nfeatures, which may require separate and careful consideration.\n\nAcknowledgement\nYD was supported in part by the Office of Naval Research N00014-18-1-2354, NSF DMS-1952735,\nDOE ASCR DE-SC0022251, and UT Austin Graduate School Summer Fellowship. KM was sup-\nported by the Peter J. O\u2019Donnell Jr. Postdoctoral Fellowship at the Oden Institute at the University of\nTexas, Austin. QL acknowledges the support of NYU Research Catalyst Prize and Whitehead Fellow-\nship for Junior Faculty in Biomedical and Biological Sciences. RW was supported in part by AFOSR\nMURI FA9550-19-1-0005, NSF DMS-1952735, NSF IFML grant 2019844, NSF DMS-N2109155,\nand NSF 2217033. The authors wish to thank IFML for generously providing computational resources,\nas well as the anonymous reviewers for their helpful comments and suggestions.\n\nReferences\nZ. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation\nin deep learning. arXiv preprint arXiv:2012.09816, 2020.\nJ. Ba and R. Caruana. Do deep nets really need to be deep?           Advances in neural information\nprocessing systems, 27, 2014.\nP. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. Advances in neural\ninformation processing systems, 27, 2014.\nP. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural\nresults. J. Mach. Learn. Res., 3(null):463\u2013482, Mar. 2003. ISSN 1532-4435.\nM. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Machine learning, 56:\n209\u2013239, 2004.", "md": "Remark 6.1 (Clustering with DAC v.s. RKD). As Figure 1 demonstrated, in contrast to RKD that\nunveils the spectral clustering of a population-induced graph from a \u201cglobal\u201d perspective, DAC\nalternatively learns a \u201clocal\u201d clustering structure through the expansion of neighborhoods NB (\u00b7)\ncharacterized by sets of data augmentations A(\u00b7) (Definition 6.1).\nTherefore, DAC and RKD provide complementary perspectives for each other. On one hand, the \u201clocal\u201d\nclustering structure revealed by DAC effectively ensures a reasonable margin in Assumption 4.2\n(Remark 4.1). On the other hand, when the augmentation strength c is insufficient (i.e., A(\u00b7) is\nnot expansive enough) for DAC to achieve a low clustering error \u00b5 (F\u03c4         Xu) (Theorem 6.2), the\nsupplementary \u201cglobal\u201d perspective of RKD connects the non-overlapping neighborhoods in the\nsame classes and brings better classification accuracies, as we empirically verified in Appendix A.\n\n7   Discussions, Limitations, and Future Directions\nThis work provides a theoretical analysis of relational knowledge distillation (RKD) in the semi-\nsupervised classification setting from a clustering perspective. Through a cluster-aware semi-\nsupervised learning (SSL) framework characterized by a notion of low clustering error, we demon-\nstrate that RKD achieves a nearly optimal label complexity (linear in the number of clusters) by\nlearning the underlying geometry of the population as revealed by the teacher model via spectral\nclustering. With a unified view of data augmentation consistency (DAC) regularization in the cluster-\naware SSL framework, we further illustrate the complementary \u201cglobal\u201d and \u201clocal\u201d perspectives of\nclustering learned by RKD and DAC, respectively.\nAs an appealing future direction, domain adaptation in knowledge distillation is a common scenario\nin practice where the training data of the teacher and student models come from different distributions.\nAlthough distributional shifts can implicitly reflect the alignment between the ground truth and the\nteacher model (Assumption 4.1) in our analysis, an explicit and specialized study on RKD in the\ndomain adaptation setting may lead to better theoretical guarantees and deeper insights.\nAnother avenue for future exploration involves RKD on different (hidden) layers of the neural\nnetwork. In this work, we consider RKD on the output layer following the standard practice [Park\net al., 2019], whereas RKD on additional hidden layers has been shown to further improve the\nperformance [Liu et al., 2019]. While the analysis in Section 4 remains valid for hidden-layer features\nof low dimensions ($$ \\approx K $$), Assumption 4.1 tends to fail (i.e., \u03b1 can be large) for high-dimensional\nfeatures, which may require separate and careful consideration.\n\nAcknowledgement\nYD was supported in part by the Office of Naval Research N00014-18-1-2354, NSF DMS-1952735,\nDOE ASCR DE-SC0022251, and UT Austin Graduate School Summer Fellowship. KM was sup-\nported by the Peter J. O\u2019Donnell Jr. Postdoctoral Fellowship at the Oden Institute at the University of\nTexas, Austin. QL acknowledges the support of NYU Research Catalyst Prize and Whitehead Fellow-\nship for Junior Faculty in Biomedical and Biological Sciences. RW was supported in part by AFOSR\nMURI FA9550-19-1-0005, NSF DMS-1952735, NSF IFML grant 2019844, NSF DMS-N2109155,\nand NSF 2217033. The authors wish to thank IFML for generously providing computational resources,\nas well as the anonymous reviewers for their helpful comments and suggestions.\n\nReferences\nZ. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation\nin deep learning. arXiv preprint arXiv:2012.09816, 2020.\nJ. Ba and R. Caruana. Do deep nets really need to be deep?           Advances in neural information\nprocessing systems, 27, 2014.\nP. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. Advances in neural\ninformation processing systems, 27, 2014.\nP. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural\nresults. J. Mach. Learn. Res., 3(null):463\u2013482, Mar. 2003. ISSN 1532-4435.\nM. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Machine learning, 56:\n209\u2013239, 2004."}]}, {"page": 11, "text": "M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs.\n   In Learning Theory: 17th Annual Conference on Learning Theory, COLT 2004, Banff, Canada,\n   July 1-4, 2004. Proceedings 17, pages 624\u2013638. Springer, 2004.\nD. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel. Mixmatch: A\n   holistic approach to semi-supervised learning. Advances in neural information processing systems,\n   32, 2019.\nA. L. Bertozzi and E. Merkurjev. Graph-based optimization approaches for machine learning,\n   uncertainty quantification and networks. In Handbook of Numerical Analysis, volume 20, pages\n   503\u2013531. Elsevier, 2019.\nJ. Bilmes. Submodularity In Machine Learning and Artificial Intelligence. Arxiv, abs/2202.00132,\n   Jan 2022.\nT. Cai, R. Gao, J. Lee, and Q. Lei. A theory of label propagation for subpopulation shift. In\n   International Conference on Machine Learning, pages 1170\u20131182. PMLR, 2021.\nJ. Calder, B. Cook, M. Thorpe, and D. Slep\u02c7   cev. Poisson learning: Graph-based semi-supervised\n   learning at very low label rates. In Proceedings of the 37th International Conference on Machine\n   Learning, pages 1306\u20131316. Proceedings of Machine Learning Research, Nov. 2020.\nM. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of\n   visual features by contrasting cluster assignments. 2020.\nB. Chen, P. Li, Z. Yan, B. Wang, and L. Zhang. Deep metric learning with graph consistency. In\n   Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 982\u2013990, 2021.\nG. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learning effi      cient object detection models\n   with knowledge distillation. Advances in neural information processing systems, 30, 2017.\nS. Chen, E. Dobriban, and J. H. Lee. A group-theoretic framework for data augmentation. The\n   Journal of Machine Learning Research, 21(1):9885\u20139955, 2020.\nX. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the\n   IEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.\nE. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation\n   policies from data. arXiv preprint arXiv:1805.09501, 2018.\nE. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation\n   with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and\n   pattern recognition workshops, pages 702\u2013703, 2020.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\n   image database. In 2009 IEEE conference on computer vision and pattern recognition, pages\n   248\u2013255. Ieee, 2009.\nT. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout.\n   arXiv preprint arXiv:1708.04552, 2017.\nC. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,\n   1(3):211\u2013218, Sep 1936. ISSN 1860-0980.\nN. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.\n   In Conference On Learning Theory, pages 297\u2013299. PMLR, 2018.\nG. H. Golub and C. F. Van Loan. Matrix computations. JHU press, Baltimore, MD, USA, 2013.\nJ. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. International Journal of\n   Computer Vision, 129:1789\u20131819, 2021.\nJ.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,\n   Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised\n   learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.\n                                                  11", "md": "# References\n\n# References\n\n- M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs.\nIn Learning Theory: 17th Annual Conference on Learning Theory, COLT 2004, Banff, Canada,\nJuly 1-4, 2004. Proceedings 17, pages 624\u2013638. Springer, 2004.\n- D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel. Mixmatch: A\nholistic approach to semi-supervised learning. Advances in neural information processing systems,\n32, 2019.\n- A. L. Bertozzi and E. Merkurjev. Graph-based optimization approaches for machine learning,\nuncertainty quantification and networks. In Handbook of Numerical Analysis, volume 20, pages\n503\u2013531. Elsevier, 2019.\n- J. Bilmes. Submodularity In Machine Learning and Artificial Intelligence. Arxiv, abs/2202.00132,\nJan 2022.\n- T. Cai, R. Gao, J. Lee, and Q. Lei. A theory of label propagation for subpopulation shift. In\nInternational Conference on Machine Learning, pages 1170\u20131182. PMLR, 2021.\n- J. Calder, B. Cook, M. Thorpe, and D. Slep\u02c7   cev. Poisson learning: Graph-based semi-supervised\nlearning at very low label rates. In Proceedings of the 37th International Conference on Machine\nLearning, pages 1306\u20131316. Proceedings of Machine Learning Research, Nov. 2020.\n- M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of\nvisual features by contrasting cluster assignments. 2020.\n- B. Chen, P. Li, Z. Yan, B. Wang, and L. Zhang. Deep metric learning with graph consistency. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 982\u2013990, 2021.\n- G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learning effi      cient object detection models\nwith knowledge distillation. Advances in neural information processing systems, 30, 2017.\n- S. Chen, E. Dobriban, and J. H. Lee. A group-theoretic framework for data augmentation. The\nJournal of Machine Learning Research, 21(1):9885\u20139955, 2020.\n- X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.\n- E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation\npolicies from data. arXiv preprint arXiv:1805.09501, 2018.\n- E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition workshops, pages 702\u2013703, 2020.\n- J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages\n248\u2013255. Ieee, 2009.\n- T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout.\narXiv preprint arXiv:1708.04552, 2017.\n- C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,\n1(3):211\u2013218, Sep 1936. ISSN 1860-0980.\n- N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.\nIn Conference On Learning Theory, pages 297\u2013299. PMLR, 2018.\n- G. H. Golub and C. F. Van Loan. Matrix computations. JHU press, Baltimore, MD, USA, 2013.\n- J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. International Journal of\nComputer Vision, 129:1789\u20131819, 2021.\n- J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,\nZ. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised\nlearning. Advances in neural information processing systems, 33:21271\u201321284, 2020.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs.\nIn Learning Theory: 17th Annual Conference on Learning Theory, COLT 2004, Banff, Canada,\nJuly 1-4, 2004. Proceedings 17, pages 624\u2013638. Springer, 2004.\n- D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel. Mixmatch: A\nholistic approach to semi-supervised learning. Advances in neural information processing systems,\n32, 2019.\n- A. L. Bertozzi and E. Merkurjev. Graph-based optimization approaches for machine learning,\nuncertainty quantification and networks. In Handbook of Numerical Analysis, volume 20, pages\n503\u2013531. Elsevier, 2019.\n- J. Bilmes. Submodularity In Machine Learning and Artificial Intelligence. Arxiv, abs/2202.00132,\nJan 2022.\n- T. Cai, R. Gao, J. Lee, and Q. Lei. A theory of label propagation for subpopulation shift. In\nInternational Conference on Machine Learning, pages 1170\u20131182. PMLR, 2021.\n- J. Calder, B. Cook, M. Thorpe, and D. Slep\u02c7   cev. Poisson learning: Graph-based semi-supervised\nlearning at very low label rates. In Proceedings of the 37th International Conference on Machine\nLearning, pages 1306\u20131316. Proceedings of Machine Learning Research, Nov. 2020.\n- M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of\nvisual features by contrasting cluster assignments. 2020.\n- B. Chen, P. Li, Z. Yan, B. Wang, and L. Zhang. Deep metric learning with graph consistency. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 982\u2013990, 2021.\n- G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learning effi      cient object detection models\nwith knowledge distillation. Advances in neural information processing systems, 30, 2017.\n- S. Chen, E. Dobriban, and J. H. Lee. A group-theoretic framework for data augmentation. The\nJournal of Machine Learning Research, 21(1):9885\u20139955, 2020.\n- X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.\n- E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation\npolicies from data. arXiv preprint arXiv:1805.09501, 2018.\n- E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition workshops, pages 702\u2013703, 2020.\n- J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages\n248\u2013255. Ieee, 2009.\n- T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout.\narXiv preprint arXiv:1708.04552, 2017.\n- C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,\n1(3):211\u2013218, Sep 1936. ISSN 1860-0980.\n- N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.\nIn Conference On Learning Theory, pages 297\u2013299. PMLR, 2018.\n- G. H. Golub and C. F. Van Loan. Matrix computations. JHU press, Baltimore, MD, USA, 2013.\n- J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. International Journal of\nComputer Vision, 129:1789\u20131819, 2021.\n- J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,\nZ. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised\nlearning. Advances in neural information processing systems, 33:21271\u201321284, 2020.", "md": "- M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs.\nIn Learning Theory: 17th Annual Conference on Learning Theory, COLT 2004, Banff, Canada,\nJuly 1-4, 2004. Proceedings 17, pages 624\u2013638. Springer, 2004.\n- D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel. Mixmatch: A\nholistic approach to semi-supervised learning. Advances in neural information processing systems,\n32, 2019.\n- A. L. Bertozzi and E. Merkurjev. Graph-based optimization approaches for machine learning,\nuncertainty quantification and networks. In Handbook of Numerical Analysis, volume 20, pages\n503\u2013531. Elsevier, 2019.\n- J. Bilmes. Submodularity In Machine Learning and Artificial Intelligence. Arxiv, abs/2202.00132,\nJan 2022.\n- T. Cai, R. Gao, J. Lee, and Q. Lei. A theory of label propagation for subpopulation shift. In\nInternational Conference on Machine Learning, pages 1170\u20131182. PMLR, 2021.\n- J. Calder, B. Cook, M. Thorpe, and D. Slep\u02c7   cev. Poisson learning: Graph-based semi-supervised\nlearning at very low label rates. In Proceedings of the 37th International Conference on Machine\nLearning, pages 1306\u20131316. Proceedings of Machine Learning Research, Nov. 2020.\n- M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of\nvisual features by contrasting cluster assignments. 2020.\n- B. Chen, P. Li, Z. Yan, B. Wang, and L. Zhang. Deep metric learning with graph consistency. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 982\u2013990, 2021.\n- G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learning effi      cient object detection models\nwith knowledge distillation. Advances in neural information processing systems, 30, 2017.\n- S. Chen, E. Dobriban, and J. H. Lee. A group-theoretic framework for data augmentation. The\nJournal of Machine Learning Research, 21(1):9885\u20139955, 2020.\n- X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 15750\u201315758, 2021.\n- E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation\npolicies from data. arXiv preprint arXiv:1805.09501, 2018.\n- E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition workshops, pages 702\u2013703, 2020.\n- J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages\n248\u2013255. Ieee, 2009.\n- T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout.\narXiv preprint arXiv:1708.04552, 2017.\n- C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,\n1(3):211\u2013218, Sep 1936. ISSN 1860-0980.\n- N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.\nIn Conference On Learning Theory, pages 297\u2013299. PMLR, 2018.\n- G. H. Golub and C. F. Van Loan. Matrix computations. JHU press, Baltimore, MD, USA, 2013.\n- J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. International Journal of\nComputer Vision, 129:1789\u20131819, 2021.\n- J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,\nZ. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised\nlearning. Advances in neural information processing systems, 33:21271\u201321284, 2020."}]}, {"page": 12, "text": "J. Z. HaoChen, C. Wei, A. Gaidon, and T. Ma. Provable guarantees for self-supervised deep learning\n  with spectral contrastive loss. Advances in Neural Information Processing Systems, 34:5000\u20135011,\n   2021.\nH. Harutyunyan, A. S. Rawat, A. K. Menon, S. Kim, and S. Kumar. Supervision complexity and its\n   role in knowledge distillation. arXiv preprint arXiv:2301.12245, 2023.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\n   of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\nG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint\n   arXiv:1503.02531, 2015.\nD. Hsu, Z. Ji, M. Telgarsky, and L. Wang. Generalization bounds via distillation. arXiv preprint\n   arXiv:2104.05641, 2021.\nG. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\n   networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n   pages 4700\u20134708, 2017.\nG. Ji and Z. Zhu. Knowledge distillation in wide neural networks: Risk bound, data effi   ciency and\n   imperfect teacher. Advances in Neural Information Processing Systems, 33:20823\u201320833, 2020.\nJ. Kim, S. Park, and N. Kwak. Paraphrasing complex network: Network compression via factor\n   transfer. Advances in neural information processing systems, 31, 2018.\nA. Krause and D. Golovin. Submodular function maximization. In Tractability: Practical Approaches\n   to Hard Problems. Cambridge University Press, February 2014.\nA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical\n   Report 0, University of Toronto, Toronto, Ontario, 2009.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\n   networks. Communications of the ACM, 60(6):84\u201390, 2017.\nS. Laine and T. Aila.      Temporal ensembling for semi-supervised learning.          arXiv preprint\n   arXiv:1610.02242, 2016.\nM. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer\n   Science & Business Media, 2013.\nJ. D. Lee, Q. Lei, N. Saunshi, and J. Zhuo. Predicting what you already know helps: Provable\n   self-supervised learning. Advances in Neural Information Processing Systems, 34:309\u2013323, 2021.\nY. Liu, J. Cao, B. Li, C. Yuan, W. Hu, Y. Li, and Y. Duan. Knowledge distillation via instance\n   relationship graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n   Recognition, pages 7096\u20137104, 2019.\nY. Liu, W. Zhang, J. Wang, and J. Wang. Data-free knowledge transfer: A survey. arXiv preprint\n   arXiv:2112.15278, 2021.\nA. Louis and K. Makarychev. Approximation algorithm for sparsest k-partitioning. In Proceedings of\n   the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 1244\u20131255. SIAM,\n   2014.\nY. Ma, Y. Chen, and Z. Akata. Distilling knowledge from self-supervised teacher by embedding\n   graph alignment. arXiv preprint arXiv:2211.13264, 2022.\nA. Maurer. A vector-contraction inequality for rademacher complexities. In International Conference\n   on Algorithmic Learning Theory, pages 3\u201317. Springer, 2016.\nS. Mei, T. Misiakiewicz, and A. Montanari. Learning with invariances in random features and kernel\n   models. In Conference on Learning Theory, pages 3351\u20133418. PMLR, 2021.\n                                                 12", "md": "# References\n\n# References\n\n- J. Z. HaoChen, C. Wei, A. Gaidon, and T. Ma. Provable guarantees for self-supervised deep learning\nwith spectral contrastive loss. Advances in Neural Information Processing Systems, 34:5000\u20135011,\n2021.\n- H. Harutyunyan, A. S. Rawat, A. K. Menon, S. Kim, and S. Kumar. Supervision complexity and its\nrole in knowledge distillation. arXiv preprint arXiv:2301.12245, 2023.\n- K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n- G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\n- D. Hsu, Z. Ji, M. Telgarsky, and L. Wang. Generalization bounds via distillation. arXiv preprint\narXiv:2104.05641, 2021.\n- G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700\u20134708, 2017.\n- G. Ji and Z. Zhu. Knowledge distillation in wide neural networks: Risk bound, data efficiency and\nimperfect teacher. Advances in Neural Information Processing Systems, 33:20823\u201320833, 2020.\n- J. Kim, S. Park, and N. Kwak. Paraphrasing complex network: Network compression via factor\ntransfer. Advances in neural information processing systems, 31, 2018.\n- A. Krause and D. Golovin. Submodular function maximization. In Tractability: Practical Approaches\nto Hard Problems. Cambridge University Press, February 2014.\n- A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical\nReport 0, University of Toronto, Toronto, Ontario, 2009.\n- A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\nnetworks. Communications of the ACM, 60(6):84\u201390, 2017.\n- S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. arXiv preprint\narXiv:1610.02242, 2016.\n- M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer\nScience & Business Media, 2013.\n- J. D. Lee, Q. Lei, N. Saunshi, and J. Zhuo. Predicting what you already know helps: Provable\nself-supervised learning. Advances in Neural Information Processing Systems, 34:309\u2013323, 2021.\n- Y. Liu, J. Cao, B. Li, C. Yuan, W. Hu, Y. Li, and Y. Duan. Knowledge distillation via instance\nrelationship graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7096\u20137104, 2019.\n- Y. Liu, W. Zhang, J. Wang, and J. Wang. Data-free knowledge transfer: A survey. arXiv preprint\narXiv:2112.15278, 2021.\n- A. Louis and K. Makarychev. Approximation algorithm for sparsest k-partitioning. In Proceedings of\nthe twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 1244\u20131255. SIAM,\n2014.\n- Y. Ma, Y. Chen, and Z. Akata. Distilling knowledge from self-supervised teacher by embedding\ngraph alignment. arXiv preprint arXiv:2211.13264, 2022.\n- A. Maurer. A vector-contraction inequality for Rademacher complexities. In International Conference\non Algorithmic Learning Theory, pages 3\u201317. Springer, 2016.\n- S. Mei, T. Misiakiewicz, and A. Montanari. Learning with invariances in random features and kernel\nmodels. In Conference on Learning Theory, pages 3351\u20133418. PMLR, 2021.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- J. Z. HaoChen, C. Wei, A. Gaidon, and T. Ma. Provable guarantees for self-supervised deep learning\nwith spectral contrastive loss. Advances in Neural Information Processing Systems, 34:5000\u20135011,\n2021.\n- H. Harutyunyan, A. S. Rawat, A. K. Menon, S. Kim, and S. Kumar. Supervision complexity and its\nrole in knowledge distillation. arXiv preprint arXiv:2301.12245, 2023.\n- K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n- G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\n- D. Hsu, Z. Ji, M. Telgarsky, and L. Wang. Generalization bounds via distillation. arXiv preprint\narXiv:2104.05641, 2021.\n- G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700\u20134708, 2017.\n- G. Ji and Z. Zhu. Knowledge distillation in wide neural networks: Risk bound, data efficiency and\nimperfect teacher. Advances in Neural Information Processing Systems, 33:20823\u201320833, 2020.\n- J. Kim, S. Park, and N. Kwak. Paraphrasing complex network: Network compression via factor\ntransfer. Advances in neural information processing systems, 31, 2018.\n- A. Krause and D. Golovin. Submodular function maximization. In Tractability: Practical Approaches\nto Hard Problems. Cambridge University Press, February 2014.\n- A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical\nReport 0, University of Toronto, Toronto, Ontario, 2009.\n- A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\nnetworks. Communications of the ACM, 60(6):84\u201390, 2017.\n- S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. arXiv preprint\narXiv:1610.02242, 2016.\n- M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer\nScience & Business Media, 2013.\n- J. D. Lee, Q. Lei, N. Saunshi, and J. Zhuo. Predicting what you already know helps: Provable\nself-supervised learning. Advances in Neural Information Processing Systems, 34:309\u2013323, 2021.\n- Y. Liu, J. Cao, B. Li, C. Yuan, W. Hu, Y. Li, and Y. Duan. Knowledge distillation via instance\nrelationship graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7096\u20137104, 2019.\n- Y. Liu, W. Zhang, J. Wang, and J. Wang. Data-free knowledge transfer: A survey. arXiv preprint\narXiv:2112.15278, 2021.\n- A. Louis and K. Makarychev. Approximation algorithm for sparsest k-partitioning. In Proceedings of\nthe twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 1244\u20131255. SIAM,\n2014.\n- Y. Ma, Y. Chen, and Z. Akata. Distilling knowledge from self-supervised teacher by embedding\ngraph alignment. arXiv preprint arXiv:2211.13264, 2022.\n- A. Maurer. A vector-contraction inequality for Rademacher complexities. In International Conference\non Algorithmic Learning Theory, pages 3\u201317. Springer, 2016.\n- S. Mei, T. Misiakiewicz, and A. Montanari. Learning with invariances in random features and kernel\nmodels. In Conference on Learning Theory, pages 3351\u20133418. PMLR, 2021.", "md": "- J. Z. HaoChen, C. Wei, A. Gaidon, and T. Ma. Provable guarantees for self-supervised deep learning\nwith spectral contrastive loss. Advances in Neural Information Processing Systems, 34:5000\u20135011,\n2021.\n- H. Harutyunyan, A. S. Rawat, A. K. Menon, S. Kim, and S. Kumar. Supervision complexity and its\nrole in knowledge distillation. arXiv preprint arXiv:2301.12245, 2023.\n- K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n- G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\n- D. Hsu, Z. Ji, M. Telgarsky, and L. Wang. Generalization bounds via distillation. arXiv preprint\narXiv:2104.05641, 2021.\n- G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700\u20134708, 2017.\n- G. Ji and Z. Zhu. Knowledge distillation in wide neural networks: Risk bound, data efficiency and\nimperfect teacher. Advances in Neural Information Processing Systems, 33:20823\u201320833, 2020.\n- J. Kim, S. Park, and N. Kwak. Paraphrasing complex network: Network compression via factor\ntransfer. Advances in neural information processing systems, 31, 2018.\n- A. Krause and D. Golovin. Submodular function maximization. In Tractability: Practical Approaches\nto Hard Problems. Cambridge University Press, February 2014.\n- A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical\nReport 0, University of Toronto, Toronto, Ontario, 2009.\n- A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\nnetworks. Communications of the ACM, 60(6):84\u201390, 2017.\n- S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. arXiv preprint\narXiv:1610.02242, 2016.\n- M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer\nScience & Business Media, 2013.\n- J. D. Lee, Q. Lei, N. Saunshi, and J. Zhuo. Predicting what you already know helps: Provable\nself-supervised learning. Advances in Neural Information Processing Systems, 34:309\u2013323, 2021.\n- Y. Liu, J. Cao, B. Li, C. Yuan, W. Hu, Y. Li, and Y. Duan. Knowledge distillation via instance\nrelationship graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7096\u20137104, 2019.\n- Y. Liu, W. Zhang, J. Wang, and J. Wang. Data-free knowledge transfer: A survey. arXiv preprint\narXiv:2112.15278, 2021.\n- A. Louis and K. Makarychev. Approximation algorithm for sparsest k-partitioning. In Proceedings of\nthe twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages 1244\u20131255. SIAM,\n2014.\n- Y. Ma, Y. Chen, and Z. Akata. Distilling knowledge from self-supervised teacher by embedding\ngraph alignment. arXiv preprint arXiv:2211.13264, 2022.\n- A. Maurer. A vector-contraction inequality for Rademacher complexities. In International Conference\non Algorithmic Learning Theory, pages 3\u201317. Springer, 2016.\n- S. Mei, T. Misiakiewicz, and A. Montanari. Learning with invariances in random features and kernel\nmodels. In Conference on Learning Theory, pages 3351\u20133418. PMLR, 2021."}]}, {"page": 13, "text": "B. Mirzasoleiman. Big Data Summarization Using Submodular Functions. PhD thesis, ETH Zurich,\n   2017.\nB. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondr\u00e1k, and A. Krause. Lazier than lazy greedy.\n   In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015.\nW. Park, D. Kim, Y. Lu, and M. Cho. Relational knowledge distillation. In Proceedings of the\n   IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3967\u20133976, 2019.\nA. Parulekar, L. Collins, K. Shanmugam, A. Mokhtari, and S. Shakkottai. Infonce loss provably\n   learns cluster-preserving representations. arXiv preprint arXiv:2302.07920, 2023.\nH. Phan. huyvnphan/pytorch_cifar10, jan 2021.\nM. Phuong and C. Lampert.        Towards understanding knowledge distillation.      In International\n   Conference on Machine Learning, pages 5142\u20135151. PMLR, 2019.\nQ. Qian, H. Li, and J. Hu. Improved knowledge distillation via full kernel matrix transfer. In\n   Proceedings of the 2022 SIAM International Conference on Data Mining (SDM), pages 612\u2013620.\n   SIAM, 2022.\nA. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin\n   deep nets. arXiv preprint arXiv:1412.6550, 2014.\nJ. Schreiber, J. Bilmes, and W. S. Noble. apricot: Submodular selection for data summarization in\n   python. Journal of Machine Learning Research, 21(161):1\u20136, 2020.\nR. Shen, S. Bubeck, and S. Gunasekar. Data augmentation as feature manipulation. In International\n   Conference on Machine Learning, pages 19773\u201319808. PMLR, 2022.\nP. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern\n   recognition\u2014tangent distance and tangent propagation. In Neural networks: tricks of the trade,\n   pages 239\u2013274. Springer, 2002.\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.\n   arXiv preprint arXiv:1409.1556, 2014.\nK. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin,\n   and C.-L. Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\n  Advances in neural information processing systems, 33:596\u2013608, 2020.\nU. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395\u2013416, Dec.\n   2007. ISSN 1573-1375.\nM. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in\n   Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.\nH. Wang, S. Lohit, M. J. Jones, and Y. Fu. What makes a\" good\" data augmentation in knowledge\n   distillation-a statistical perspective. In Advances in Neural Information Processing Systems, 2022.\nL. Wang and K.-J. Yoon. Knowledge distillation and student-teacher learning for visual intelligence:\n   A review and new outlooks. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n   2021.\nC. Wei and T. Ma.      Data-dependent sample complexity of deep neural networks via lipschitz\n   augmentation. Advances in Neural Information Processing Systems, 32, 2019.\nC. Wei, K. Shen, Y. Chen, and T. Ma. Theoretical analysis of self-training with deep networks on\n   unlabeled data. In International Conference on Learning Representations, 2021.\nM. Welling and T. N. Kipf. Semi-supervised classification with graph convolutional networks. In J.\n   International Conference on Learning Representations (ICLR 2017), 2016.\nS. Yang, Y. Dong, R. Ward, I. S. Dhillon, S. Sanghavi, and Q. Lei. Sample effi        ciency of data\n   augmentation consistency regularization. In International Conference on Artificial Intelligence\n   and Statistics, pages 3825\u20133853. PMLR, 2023.\n                                                 13", "md": "- B. Mirzasoleiman. *Big Data Summarization Using Submodular Functions*. PhD thesis, ETH Zurich, 2017.\n- B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondr\u00e1k, and A. Krause. *Lazier than lazy greedy. In Proceedings of the AAAI Conference on Artificial Intelligence*, volume 29, 2015.\n- W. Park, D. Kim, Y. Lu, and M. Cho. *Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3967\u20133976, 2019.\n- A. Parulekar, L. Collins, K. Shanmugam, A. Mokhtari, and S. Shakkottai. *Infonce loss provably learns cluster-preserving representations*. arXiv preprint arXiv:2302.07920, 2023.\n- H. Phan. *huyvnphan/pytorch_cifar10*, jan 2021.\n- M. Phuong and C. Lampert. *Towards understanding knowledge distillation. In International Conference on Machine Learning*, pages 5142\u20135151. PMLR, 2019.\n- Q. Qian, H. Li, and J. Hu. *Improved knowledge distillation via full kernel matrix transfer. In Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)*, pages 612\u2013620. SIAM, 2022.\n- A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. *Fitnets: Hints for thin deep nets*. arXiv preprint arXiv:1412.6550, 2014.\n- J. Schreiber, J. Bilmes, and W. S. Noble. *apricot: Submodular selection for data summarization in python. Journal of Machine Learning Research*, 21(161):1\u20136, 2020.\n- R. Shen, S. Bubeck, and S. Gunasekar. *Data augmentation as feature manipulation. In International Conference on Machine Learning*, pages 19773\u201319808. PMLR, 2022.\n- P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. *Transformation invariance in pattern recognition\u2014tangent distance and tangent propagation. In Neural networks: tricks of the trade*, pages 239\u2013274. Springer, 2002.\n- K. Simonyan and A. Zisserman. *Very deep convolutional networks for large-scale image recognition*. arXiv preprint arXiv:1409.1556, 2014.\n- K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li. *Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems*, 33:596\u2013608, 2020.\n- U. von Luxburg. *A tutorial on spectral clustering. Statistics and Computing*, 17(4):395\u2013416, Dec. 2007. ISSN 1573-1375.\n- M. J. Wainwright. *High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics*. Cambridge University Press, 2019.\n- H. Wang, S. Lohit, M. J. Jones, and Y. Fu. *What makes a\" good\" data augmentation in knowledge distillation-a statistical perspective. In Advances in Neural Information Processing Systems*, 2022.\n- L. Wang and K.-J. Yoon. *Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2021.\n- C. Wei and T. Ma. *Data-dependent sample complexity of deep neural networks via lipschitz augmentation. Advances in Neural Information Processing Systems*, 32, 2019.\n- C. Wei, K. Shen, Y. Chen, and T. Ma. *Theoretical analysis of self-training with deep networks on unlabeled data. In International Conference on Learning Representations*, 2021.\n- M. Welling and T. N. Kipf. *Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR 2017)*, 2016.\n- S. Yang, Y. Dong, R. Ward, I. S. Dhillon, S. Sanghavi, and Q. Lei. *Sample efficiency of data augmentation consistency regularization. In International Conference on Artificial Intelligence and Statistics*, pages 3825\u20133853. PMLR, 2023.", "images": [], "items": [{"type": "text", "value": "- B. Mirzasoleiman. *Big Data Summarization Using Submodular Functions*. PhD thesis, ETH Zurich, 2017.\n- B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondr\u00e1k, and A. Krause. *Lazier than lazy greedy. In Proceedings of the AAAI Conference on Artificial Intelligence*, volume 29, 2015.\n- W. Park, D. Kim, Y. Lu, and M. Cho. *Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3967\u20133976, 2019.\n- A. Parulekar, L. Collins, K. Shanmugam, A. Mokhtari, and S. Shakkottai. *Infonce loss provably learns cluster-preserving representations*. arXiv preprint arXiv:2302.07920, 2023.\n- H. Phan. *huyvnphan/pytorch_cifar10*, jan 2021.\n- M. Phuong and C. Lampert. *Towards understanding knowledge distillation. In International Conference on Machine Learning*, pages 5142\u20135151. PMLR, 2019.\n- Q. Qian, H. Li, and J. Hu. *Improved knowledge distillation via full kernel matrix transfer. In Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)*, pages 612\u2013620. SIAM, 2022.\n- A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. *Fitnets: Hints for thin deep nets*. arXiv preprint arXiv:1412.6550, 2014.\n- J. Schreiber, J. Bilmes, and W. S. Noble. *apricot: Submodular selection for data summarization in python. Journal of Machine Learning Research*, 21(161):1\u20136, 2020.\n- R. Shen, S. Bubeck, and S. Gunasekar. *Data augmentation as feature manipulation. In International Conference on Machine Learning*, pages 19773\u201319808. PMLR, 2022.\n- P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. *Transformation invariance in pattern recognition\u2014tangent distance and tangent propagation. In Neural networks: tricks of the trade*, pages 239\u2013274. Springer, 2002.\n- K. Simonyan and A. Zisserman. *Very deep convolutional networks for large-scale image recognition*. arXiv preprint arXiv:1409.1556, 2014.\n- K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li. *Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems*, 33:596\u2013608, 2020.\n- U. von Luxburg. *A tutorial on spectral clustering. Statistics and Computing*, 17(4):395\u2013416, Dec. 2007. ISSN 1573-1375.\n- M. J. Wainwright. *High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics*. Cambridge University Press, 2019.\n- H. Wang, S. Lohit, M. J. Jones, and Y. Fu. *What makes a\" good\" data augmentation in knowledge distillation-a statistical perspective. In Advances in Neural Information Processing Systems*, 2022.\n- L. Wang and K.-J. Yoon. *Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2021.\n- C. Wei and T. Ma. *Data-dependent sample complexity of deep neural networks via lipschitz augmentation. Advances in Neural Information Processing Systems*, 32, 2019.\n- C. Wei, K. Shen, Y. Chen, and T. Ma. *Theoretical analysis of self-training with deep networks on unlabeled data. In International Conference on Learning Representations*, 2021.\n- M. Welling and T. N. Kipf. *Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR 2017)*, 2016.\n- S. Yang, Y. Dong, R. Ward, I. S. Dhillon, S. Sanghavi, and Q. Lei. *Sample efficiency of data augmentation consistency regularization. In International Conference on Artificial Intelligence and Statistics*, pages 3825\u20133853. PMLR, 2023.", "md": "- B. Mirzasoleiman. *Big Data Summarization Using Submodular Functions*. PhD thesis, ETH Zurich, 2017.\n- B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondr\u00e1k, and A. Krause. *Lazier than lazy greedy. In Proceedings of the AAAI Conference on Artificial Intelligence*, volume 29, 2015.\n- W. Park, D. Kim, Y. Lu, and M. Cho. *Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3967\u20133976, 2019.\n- A. Parulekar, L. Collins, K. Shanmugam, A. Mokhtari, and S. Shakkottai. *Infonce loss provably learns cluster-preserving representations*. arXiv preprint arXiv:2302.07920, 2023.\n- H. Phan. *huyvnphan/pytorch_cifar10*, jan 2021.\n- M. Phuong and C. Lampert. *Towards understanding knowledge distillation. In International Conference on Machine Learning*, pages 5142\u20135151. PMLR, 2019.\n- Q. Qian, H. Li, and J. Hu. *Improved knowledge distillation via full kernel matrix transfer. In Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)*, pages 612\u2013620. SIAM, 2022.\n- A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. *Fitnets: Hints for thin deep nets*. arXiv preprint arXiv:1412.6550, 2014.\n- J. Schreiber, J. Bilmes, and W. S. Noble. *apricot: Submodular selection for data summarization in python. Journal of Machine Learning Research*, 21(161):1\u20136, 2020.\n- R. Shen, S. Bubeck, and S. Gunasekar. *Data augmentation as feature manipulation. In International Conference on Machine Learning*, pages 19773\u201319808. PMLR, 2022.\n- P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. *Transformation invariance in pattern recognition\u2014tangent distance and tangent propagation. In Neural networks: tricks of the trade*, pages 239\u2013274. Springer, 2002.\n- K. Simonyan and A. Zisserman. *Very deep convolutional networks for large-scale image recognition*. arXiv preprint arXiv:1409.1556, 2014.\n- K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li. *Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems*, 33:596\u2013608, 2020.\n- U. von Luxburg. *A tutorial on spectral clustering. Statistics and Computing*, 17(4):395\u2013416, Dec. 2007. ISSN 1573-1375.\n- M. J. Wainwright. *High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics*. Cambridge University Press, 2019.\n- H. Wang, S. Lohit, M. J. Jones, and Y. Fu. *What makes a\" good\" data augmentation in knowledge distillation-a statistical perspective. In Advances in Neural Information Processing Systems*, 2022.\n- L. Wang and K.-J. Yoon. *Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2021.\n- C. Wei and T. Ma. *Data-dependent sample complexity of deep neural networks via lipschitz augmentation. Advances in Neural Information Processing Systems*, 32, 2019.\n- C. Wei, K. Shen, Y. Chen, and T. Ma. *Theoretical analysis of self-training with deep networks on unlabeled data. In International Conference on Learning Representations*, 2021.\n- M. Welling and T. N. Kipf. *Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR 2017)*, 2016.\n- S. Yang, Y. Dong, R. Ward, I. S. Dhillon, S. Sanghavi, and Q. Lei. *Sample efficiency of data augmentation consistency regularization. In International Conference on Artificial Intelligence and Statistics*, pages 3825\u20133853. PMLR, 2023."}]}, {"page": 14, "text": "S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy to train strong\n  classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on\n  computer vision, pages 6023\u20136032, 2019.\nS. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of\n  convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016a.\nS. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016b.\nH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization.\n  arXiv preprint arXiv:1710.09412, 2017.\nD. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch\u00f6lkopf.           Learning with local and global\n  consistency. In S. Thrun, L. Saul, and B. Sch\u00f6lkopf, editors, Advances in Neural Information\n  Processing Systems, volume 16. MIT Press, 2003.\nJ. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural\n  networks: A review of methods and applications. AI Open, 1:57\u201381, 2020. ISSN 2666-6510.\nX. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian fields and harmonic\n  functions. In Proceedings of the 20th International Conference on International Conference on\n  Machine Learning, pages 912\u2013919, Washington, DC, USA, Aug. 2003. AAAI Press. ISBN\n  978-1-57735-189-4.\n                                                  14", "md": "# References\n\n## References\n\n- S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.\n- S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016a.\n- S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016b.\n- H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n- D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch\u00f6lkopf. Learning with local and global consistency. In S. Thrun, L. Saul, and B. Sch\u00f6lkopf, editors, Advances in Neural Information Processing Systems, volume 16. MIT Press, 2003.\n- J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57\u201381, 2020. ISSN 2666-6510.\n- X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of the 20th International Conference on International Conference on Machine Learning, pages 912\u2013919, Washington, DC, USA, Aug. 2003. AAAI Press. ISBN 978-1-57735-189-4.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.\n- S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016a.\n- S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016b.\n- H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n- D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch\u00f6lkopf. Learning with local and global consistency. In S. Thrun, L. Saul, and B. Sch\u00f6lkopf, editors, Advances in Neural Information Processing Systems, volume 16. MIT Press, 2003.\n- J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57\u201381, 2020. ISSN 2666-6510.\n- X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of the 20th International Conference on International Conference on Machine Learning, pages 912\u2013919, Washington, DC, USA, Aug. 2003. AAAI Press. ISBN 978-1-57735-189-4.", "md": "- S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.\n- S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016a.\n- S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016b.\n- H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n- D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch\u00f6lkopf. Learning with local and global consistency. In S. Thrun, L. Saul, and B. Sch\u00f6lkopf, editors, Advances in Neural Information Processing Systems, volume 16. MIT Press, 2003.\n- J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57\u201381, 2020. ISSN 2666-6510.\n- X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of the 20th International Conference on International Conference on Machine Learning, pages 912\u2013919, Washington, DC, USA, Aug. 2003. AAAI Press. ISBN 978-1-57735-189-4."}]}, {"page": 15, "text": " A      Experiments\n In this section, we present experimental results on CIFAR-10/100 [Krizhevsky and Hinton, 2009] to\n demonstrate the efficacy of combining DAC and RKD (i.e., the \u201clocal\u201d and \u201cglobal\u201d perspectives of\n clustering) for semi-supervised learning in the low-label-rate regime. The experiment code can be\n found at https://github.com/dyjdongyijun/Semi_Supervised_Knowledge_Distillation.\nWe consider the (data-free) knowledge distillation setting, wherein the student model cannot access\n the training data of the teacher model. That is, the student models only have access to limited\n training samples, with very few labels, drawn from a potentially different data distribution than\n which the teacher model is pretrained on. More precisely, given a labeled sample set (X, y) of\n size n and an unlabeled sample set Xu of size N (n \u226a                              N)10, we train a student model (WideRes-\n Net [Zagoruyko and Komodakis, 2016b]) in the low-label-rate regime with as few as 4 labeled\n samples per class (i.e., n = 4K, or n = 40 for CIFAR-10 and n = 400 for CIFAR-100), along with\n N = 25000, 40000, and 50000 unlabeled samples (i.e., 50%, 80%, 100% of the entire CIFAR-10/100\n training set).\n Then, combining DAC and RKD, we solve the following objective in the experiments:\n                                                            + \u03bbDAC \u00b7       LDAC          + \u03bbRKD \u00b7       RXu(f),\n                            min         LCE                                  Xu (f)\n                                          (X,y)(f)\n                            f\u2208F   Supervised cross-entropy      Unsupervised DAC loss         empirical RKD loss\n where     LCE(X,y)(f) =           1   n  i=1 \u2113CE (yi|f (xi)) denotes the supervised cross-entropy loss with\n \u2113CE (y|f (x)) = \u2212           log   n softmax (f(x))y            . The DAC and RKD losses are as described in Equa-\n tion (9) and Equation (3), respectively. For the regularization hyper-parameters, we have \u03bbDAC = 1\n and \u03bbRKD = 0 for DAC (only), while \u03bbRKD = 0.001 for DAC + RKD.\n DAC via FixMatch.                 We take FixMatch [Sohn et al., 2020]\u2013a state-of-the-art algorithm in the\n low-label-rate regime\u2013as the semi-supervised learning baseline. By combining RKD with FixMatch,\n we show that RKD brings additional boosts to the classification accuracy, especially with unsuitable\n data augmentations and/or limited unlabeled data.\n Following the formulation and implementation in Sohn et al. [2020], FixMatch involves a pair of\n weak and strong data augmentations (xw, xs) for each unlabeled sample xu. For a current model f,\n the DAC loss given by FixMatch can be expressed as\n          LDAC\n            Xu (f) \u225c\n          mean           \u2113CE (yf (xw    i )|f (xs   i))    max       softmax        f (xw T i )        \u2265  \u03c4DAC, i \u2208        [N]  ,  (9)\n where T = 1 is a fixed temperature hyper-parameter. \u03c4DAC is the threshold below which the\n pseudo-label gauged on xw             i is considered \u201cwith low confidence\u201d and therefore discarded. We set\n \u03c4DAC = 0.95 for CIFAR-10 and \u03c4DAC = 0.8 for CIFAR-100.\n In FixMatch, the weak augmentations include random flips and crops, whereas the strong augmen-\n tations additionally involve RandAugment [Cubuk et al., 2020]. To investigate the scenarios with\n unsuitable data augmentations and/or limited unlabeled data, we conduct experiments with various\n (i) augmentation strength (i.e., magnitude of transformations) m = 2, 6, 10 in RandAugment [Cubuk\n et al., 2020] and (ii) unlabeled sample size N = 25000, 40000, 50000 (i.e., 50%, 80%, 100% of the\n entire CIFAR-10/100 training set).\n RKD.        We warm up with the in-distribution case in the CIFAR-10 experiments (Table 1) where the\n teacher and student models are (pre)trained on the same dataset (i.e., CIFAR-10). Concretely, the\n teacher model \u03c8 : X \u2192               W is a Densenet 161 [Huang et al., 2017, Phan, 2021] pretrained on the\n entirety of the CIFAR-10 training set (via supervised ERM).\n Taking a step further, we consider an out-of-distribution scenario in the CIFAR-100 experiments\n (Table 2) where the teacher model is pretrained on ImageNet [Deng et al., 2009] via an unsupervised\n contrastive learning method\u2013SwAV [Caron et al., 2020]. Intuitively, we choose teacher models\n    10It is worth mentioning that, in practice, we relax the assumption of the independence between (X, y) and\nXu and recycle X in Xu.\n                                                                       15", "md": "# Experiments\n\n## Experiments\n\nIn this section, we present experimental results on CIFAR-10/100 [Krizhevsky and Hinton, 2009] to demonstrate the efficacy of combining DAC and RKD (i.e., the \u201clocal\u201d and \u201cglobal\u201d perspectives of clustering) for semi-supervised learning in the low-label-rate regime. The experiment code can be found at https://github.com/dyjdongyijun/Semi_Supervised_Knowledge_Distillation.\n\nWe consider the (data-free) knowledge distillation setting, wherein the student model cannot access the training data of the teacher model. That is, the student models only have access to limited training samples, with very few labels, drawn from a potentially different data distribution than which the teacher model is pretrained on. More precisely, given a labeled sample set (X, y) of size n and an unlabeled sample set Xu of size N (n \u226a N), we train a student model (WideRes-Net [Zagoruyko and Komodakis, 2016b]) in the low-label-rate regime with as few as 4 labeled samples per class (i.e., n = 4K, or n = 40 for CIFAR-10 and n = 400 for CIFAR-100), along with N = 25000, 40000, and 50000 unlabeled samples (i.e., 50%, 80%, 100% of the entire CIFAR-10/100 training set).\n\nThen, combining DAC and RKD, we solve the following objective in the experiments:\n\n$$\n\\min_{f \\in F} LCE(X,y)(f) = LCE(X,y)(f) + \\lambda_{DAC} \\cdot LDAC_{Xu}(f) + \\lambda_{RKD} \\cdot RXu(f),\n$$\nwhere:\n\n- $LCE(X,y)(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{CE}(y_i|f(x_i))$ denotes the supervised cross-entropy loss with $\\ell_{CE}(y|f(x)) = -\\log \\text{softmax}(f(x))y$.\n- The DAC and RKD losses are as described in Equation (9) and Equation (3), respectively.\n- For the regularization hyper-parameters, we have $\\lambda_{DAC} = 1$ and $\\lambda_{RKD} = 0$ for DAC (only), while $\\lambda_{RKD} = 0.001$ for DAC + RKD.\n\nDAC via FixMatch:\n\nWe take FixMatch [Sohn et al., 2020]\u2013a state-of-the-art algorithm in the low-label-rate regime\u2013as the semi-supervised learning baseline. By combining RKD with FixMatch, we show that RKD brings additional boosts to the classification accuracy, especially with unsuitable data augmentations and/or limited unlabeled data.\n\nFollowing the formulation and implementation in Sohn et al. [2020], FixMatch involves a pair of weak and strong data augmentations ($x_w, x_s$) for each unlabeled sample $x_u$. For a current model $f$, the DAC loss given by FixMatch can be expressed as:\n\n$$\nLDAC_{Xu}(f) = \\text{mean} \\left( \\ell_{CE}(y, f(x_w^i)|f(x_s^i)) \\max(\\text{softmax} f(x_w^T i)) \\geq \\tau_{DAC}, i \\in [N] \\right),\n$$\nwhere $T = 1$ is a fixed temperature hyper-parameter. $\\tau_{DAC}$ is the threshold below which the pseudo-label gauged on $x_w^i$ is considered \u201cwith low confidence\u201d and therefore discarded. We set $\\tau_{DAC} = 0.95$ for CIFAR-10 and $\\tau_{DAC} = 0.8$ for CIFAR-100.\n\nIn FixMatch, the weak augmentations include random flips and crops, whereas the strong augmentations additionally involve RandAugment [Cubuk et al., 2020]. To investigate the scenarios with unsuitable data augmentations and/or limited unlabeled data, we conduct experiments with various:\n\n1. Augmentation strength (i.e., magnitude of transformations) $m = 2, 6, 10$ in RandAugment [Cubuk et al., 2020]\n2. Unlabeled sample size $N = 25000, 40000, 50000$ (i.e., 50%, 80%, 100% of the entire CIFAR-10/100 training set)\n\nRKD:\n\nWe warm up with the in-distribution case in the CIFAR-10 experiments (Table 1) where the teacher and student models are (pre)trained on the same dataset (i.e., CIFAR-10). Concretely, the teacher model $\\psi : X \\rightarrow W$ is a Densenet 161 [Huang et al., 2017, Phan, 2021] pretrained on the entirety of the CIFAR-10 training set (via supervised ERM).\n\nTaking a step further, we consider an out-of-distribution scenario in the CIFAR-100 experiments (Table 2) where the teacher model is pretrained on ImageNet [Deng et al., 2009] via an unsupervised contrastive learning method\u2013SwAV [Caron et al., 2020]. Intuitively, we choose teacher models.\n\nIt is worth mentioning that, in practice, we relax the assumption of the independence between $(X, y)$ and $X_u$ and recycle $X$ in $X_u$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Experiments", "md": "# Experiments"}, {"type": "heading", "lvl": 2, "value": "Experiments", "md": "## Experiments"}, {"type": "text", "value": "In this section, we present experimental results on CIFAR-10/100 [Krizhevsky and Hinton, 2009] to demonstrate the efficacy of combining DAC and RKD (i.e., the \u201clocal\u201d and \u201cglobal\u201d perspectives of clustering) for semi-supervised learning in the low-label-rate regime. The experiment code can be found at https://github.com/dyjdongyijun/Semi_Supervised_Knowledge_Distillation.\n\nWe consider the (data-free) knowledge distillation setting, wherein the student model cannot access the training data of the teacher model. That is, the student models only have access to limited training samples, with very few labels, drawn from a potentially different data distribution than which the teacher model is pretrained on. More precisely, given a labeled sample set (X, y) of size n and an unlabeled sample set Xu of size N (n \u226a N), we train a student model (WideRes-Net [Zagoruyko and Komodakis, 2016b]) in the low-label-rate regime with as few as 4 labeled samples per class (i.e., n = 4K, or n = 40 for CIFAR-10 and n = 400 for CIFAR-100), along with N = 25000, 40000, and 50000 unlabeled samples (i.e., 50%, 80%, 100% of the entire CIFAR-10/100 training set).\n\nThen, combining DAC and RKD, we solve the following objective in the experiments:\n\n$$\n\\min_{f \\in F} LCE(X,y)(f) = LCE(X,y)(f) + \\lambda_{DAC} \\cdot LDAC_{Xu}(f) + \\lambda_{RKD} \\cdot RXu(f),\n$$\nwhere:\n\n- $LCE(X,y)(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{CE}(y_i|f(x_i))$ denotes the supervised cross-entropy loss with $\\ell_{CE}(y|f(x)) = -\\log \\text{softmax}(f(x))y$.\n- The DAC and RKD losses are as described in Equation (9) and Equation (3), respectively.\n- For the regularization hyper-parameters, we have $\\lambda_{DAC} = 1$ and $\\lambda_{RKD} = 0$ for DAC (only), while $\\lambda_{RKD} = 0.001$ for DAC + RKD.\n\nDAC via FixMatch:\n\nWe take FixMatch [Sohn et al., 2020]\u2013a state-of-the-art algorithm in the low-label-rate regime\u2013as the semi-supervised learning baseline. By combining RKD with FixMatch, we show that RKD brings additional boosts to the classification accuracy, especially with unsuitable data augmentations and/or limited unlabeled data.\n\nFollowing the formulation and implementation in Sohn et al. [2020], FixMatch involves a pair of weak and strong data augmentations ($x_w, x_s$) for each unlabeled sample $x_u$. For a current model $f$, the DAC loss given by FixMatch can be expressed as:\n\n$$\nLDAC_{Xu}(f) = \\text{mean} \\left( \\ell_{CE}(y, f(x_w^i)|f(x_s^i)) \\max(\\text{softmax} f(x_w^T i)) \\geq \\tau_{DAC}, i \\in [N] \\right),\n$$\nwhere $T = 1$ is a fixed temperature hyper-parameter. $\\tau_{DAC}$ is the threshold below which the pseudo-label gauged on $x_w^i$ is considered \u201cwith low confidence\u201d and therefore discarded. We set $\\tau_{DAC} = 0.95$ for CIFAR-10 and $\\tau_{DAC} = 0.8$ for CIFAR-100.\n\nIn FixMatch, the weak augmentations include random flips and crops, whereas the strong augmentations additionally involve RandAugment [Cubuk et al., 2020]. To investigate the scenarios with unsuitable data augmentations and/or limited unlabeled data, we conduct experiments with various:\n\n1. Augmentation strength (i.e., magnitude of transformations) $m = 2, 6, 10$ in RandAugment [Cubuk et al., 2020]\n2. Unlabeled sample size $N = 25000, 40000, 50000$ (i.e., 50%, 80%, 100% of the entire CIFAR-10/100 training set)\n\nRKD:\n\nWe warm up with the in-distribution case in the CIFAR-10 experiments (Table 1) where the teacher and student models are (pre)trained on the same dataset (i.e., CIFAR-10). Concretely, the teacher model $\\psi : X \\rightarrow W$ is a Densenet 161 [Huang et al., 2017, Phan, 2021] pretrained on the entirety of the CIFAR-10 training set (via supervised ERM).\n\nTaking a step further, we consider an out-of-distribution scenario in the CIFAR-100 experiments (Table 2) where the teacher model is pretrained on ImageNet [Deng et al., 2009] via an unsupervised contrastive learning method\u2013SwAV [Caron et al., 2020]. Intuitively, we choose teacher models.\n\nIt is worth mentioning that, in practice, we relax the assumption of the independence between $(X, y)$ and $X_u$ and recycle $X$ in $X_u$.", "md": "In this section, we present experimental results on CIFAR-10/100 [Krizhevsky and Hinton, 2009] to demonstrate the efficacy of combining DAC and RKD (i.e., the \u201clocal\u201d and \u201cglobal\u201d perspectives of clustering) for semi-supervised learning in the low-label-rate regime. The experiment code can be found at https://github.com/dyjdongyijun/Semi_Supervised_Knowledge_Distillation.\n\nWe consider the (data-free) knowledge distillation setting, wherein the student model cannot access the training data of the teacher model. That is, the student models only have access to limited training samples, with very few labels, drawn from a potentially different data distribution than which the teacher model is pretrained on. More precisely, given a labeled sample set (X, y) of size n and an unlabeled sample set Xu of size N (n \u226a N), we train a student model (WideRes-Net [Zagoruyko and Komodakis, 2016b]) in the low-label-rate regime with as few as 4 labeled samples per class (i.e., n = 4K, or n = 40 for CIFAR-10 and n = 400 for CIFAR-100), along with N = 25000, 40000, and 50000 unlabeled samples (i.e., 50%, 80%, 100% of the entire CIFAR-10/100 training set).\n\nThen, combining DAC and RKD, we solve the following objective in the experiments:\n\n$$\n\\min_{f \\in F} LCE(X,y)(f) = LCE(X,y)(f) + \\lambda_{DAC} \\cdot LDAC_{Xu}(f) + \\lambda_{RKD} \\cdot RXu(f),\n$$\nwhere:\n\n- $LCE(X,y)(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{CE}(y_i|f(x_i))$ denotes the supervised cross-entropy loss with $\\ell_{CE}(y|f(x)) = -\\log \\text{softmax}(f(x))y$.\n- The DAC and RKD losses are as described in Equation (9) and Equation (3), respectively.\n- For the regularization hyper-parameters, we have $\\lambda_{DAC} = 1$ and $\\lambda_{RKD} = 0$ for DAC (only), while $\\lambda_{RKD} = 0.001$ for DAC + RKD.\n\nDAC via FixMatch:\n\nWe take FixMatch [Sohn et al., 2020]\u2013a state-of-the-art algorithm in the low-label-rate regime\u2013as the semi-supervised learning baseline. By combining RKD with FixMatch, we show that RKD brings additional boosts to the classification accuracy, especially with unsuitable data augmentations and/or limited unlabeled data.\n\nFollowing the formulation and implementation in Sohn et al. [2020], FixMatch involves a pair of weak and strong data augmentations ($x_w, x_s$) for each unlabeled sample $x_u$. For a current model $f$, the DAC loss given by FixMatch can be expressed as:\n\n$$\nLDAC_{Xu}(f) = \\text{mean} \\left( \\ell_{CE}(y, f(x_w^i)|f(x_s^i)) \\max(\\text{softmax} f(x_w^T i)) \\geq \\tau_{DAC}, i \\in [N] \\right),\n$$\nwhere $T = 1$ is a fixed temperature hyper-parameter. $\\tau_{DAC}$ is the threshold below which the pseudo-label gauged on $x_w^i$ is considered \u201cwith low confidence\u201d and therefore discarded. We set $\\tau_{DAC} = 0.95$ for CIFAR-10 and $\\tau_{DAC} = 0.8$ for CIFAR-100.\n\nIn FixMatch, the weak augmentations include random flips and crops, whereas the strong augmentations additionally involve RandAugment [Cubuk et al., 2020]. To investigate the scenarios with unsuitable data augmentations and/or limited unlabeled data, we conduct experiments with various:\n\n1. Augmentation strength (i.e., magnitude of transformations) $m = 2, 6, 10$ in RandAugment [Cubuk et al., 2020]\n2. Unlabeled sample size $N = 25000, 40000, 50000$ (i.e., 50%, 80%, 100% of the entire CIFAR-10/100 training set)\n\nRKD:\n\nWe warm up with the in-distribution case in the CIFAR-10 experiments (Table 1) where the teacher and student models are (pre)trained on the same dataset (i.e., CIFAR-10). Concretely, the teacher model $\\psi : X \\rightarrow W$ is a Densenet 161 [Huang et al., 2017, Phan, 2021] pretrained on the entirety of the CIFAR-10 training set (via supervised ERM).\n\nTaking a step further, we consider an out-of-distribution scenario in the CIFAR-100 experiments (Table 2) where the teacher model is pretrained on ImageNet [Deng et al., 2009] via an unsupervised contrastive learning method\u2013SwAV [Caron et al., 2020]. Intuitively, we choose teacher models.\n\nIt is worth mentioning that, in practice, we relax the assumption of the independence between $(X, y)$ and $X_u$ and recycle $X$ in $X_u$."}]}, {"page": 16, "text": " pretrained via SwAV as it encourages both data clustering and consistency of cluster assignment\n under data augmentations without involving supervision, which tends to provide representations that\n generalize better in a cluster-aware semi-supervised learning setting.\n Given the teacher models, the output (response) of each sample is taken as the corresponding teacher\n feature such that W = RK. In light of the empirical success of RKD based on simple kernel\n functions [Park et al., 2019, Liu et al., 2019], we focus on the (shifted) cosine kernel, k\u03c8 (x, x\u2032) =\n        \u03c8(x)\u22a4\u03c8(x\u2032)\n1 +   \u2225\u03c8(x)\u22252\u2225\u03c8(x\u2032)\u22252 , in the experiments.\nWith data augmentation (e.g., when coupling with FixMatch), we assume that weak augmentations\n(i.e., random flips and crops) bring little perturbations on responses of the teacher model (i.e., the\n teacher features), intuitively as the predictions made by a good teacher model ought to be invariance\n under weak augmentations. Thereby, we can considerably reduce memory burden and accelerate\n the RKD process by storing the teacher features \u03c8 (x) of the original data x \u2208            X offline, instead of\n inferring every augmented sample x\u2032 \u2208        A(x) with the large teacher model \u03c8 (x\u2032) online.\n Labeled data selection.       As alluded to in Remark 5.2, the success of low-label-rate (i.e., n \u226a           N)\n semi-supervised classification naturally depends on the quality of the (extremely) limited amount of\n labeled data to represent the underlying population. We perform experiments in which the selection\n of labeled data is non-adaptive (Uniform) or adaptive (StochasticGreedy). The non-adaptive setting,\n Uniform, selects labeled data uniformly in each ground truth class11. This contrasts with the strictly\n i.i.d. sampling assumption used in our theoretical results but constitutes a reasonable alternative\n that has been used in other low-label rate semi-supervised classification works (e.g., [Calder et al.,\n 2020]).\nWe also utilize a coreset selection strategy (StochasticGreedy) for identifying \u201crepresentative\u201d in-\n puts to use as labeled data in an adaptive manner. This labeled set is selected via a stochastic\n greedy [Mirzasoleiman et al., 2015] optimization problem to (approximately) minimize a facility\n location objective12. An in-depth discussion of coreset selection methods is outside the scope of the\n current work, but we refer the interested reader to the enlightening references [Bilmes, 2022, Krause\n and Golovin, 2014, Mirzasoleiman, 2017] regarding submodular optimization methods in machine\n learning.\n                                   Table 1: Top-1 accuracy on CIFAR-10.\n   Label selection     Sample sizes (n, N)      SSL algorithm                 Augmentation Strength\n                                                                       High           Medium           Weak\n                           (40, 50000)            FixMatch         89.47 \u00b1 3.83    87.89 \u00b1 1.43     81.36 \u00b1 1.41\n                                              FixMatch + RKD       89.88 \u00b1 0.41    89.10 \u00b1 1.14     84.61 \u00b1 1.23\n       Uniform             (40, 40000)            FixMatch         87.60 \u00b1 1.21    85.30 \u00b1 1.35     83.74 \u00b1 3.07\n                                              FixMatch + RKD       90.19 \u00b1 0.19    88.65 \u00b1 1.81     86.68 \u00b1 1.06\n                           (40, 25000)            FixMatch         81.73 \u00b1 1.27    81.43 \u00b1 2.98     76.84 \u00b1 1.98\n                                              FixMatch + RKD       87.06 \u00b1 2.04    87.24 \u00b1 0.39     82.75 \u00b1 2.35\n                           (40, 50000)            FixMatch         85.48 \u00b1 1.67    89.39 \u00b1 0.82     78.98 \u00b1 0.52\n                                              FixMatch + RKD       91.19 \u00b1 0.26    90.00 \u00b1 0.64     87.11 \u00b1 4.33\n  StochasticGreedy         (40, 40000)            FixMatch         84.51 \u00b1 1.34    84.88 \u00b1 6.68     70.70 \u00b1 5.32\n                                              FixMatch + RKD       85.68 \u00b1 1.19    88.31 \u00b1 1.36     85.12 \u00b1 3.49\n                           (40, 25000)            FixMatch         79.74 \u00b1 8.02    73.97 \u00b1 1.98     73.86 \u00b1 3.29\n                                              FixMatch + RKD       83.72 \u00b1 4.73    82.09 \u00b1 3.27     80.65 \u00b1 3.67\n Results.    From Table 1 and Table 2, we observe that the incorporation of RKD with FixMatch\n(i.e., adding the \u201cglobal\u201d perspective of RKD to the \u201clocal\u201d perspective of DAC) generally brings\n additional improvements to the classification accuracy, especially when the data augmentation strength\n is inappropriate and/or the number of unlabeled data is limited.\n   11For fair comparison and consistent performance among different configurations in the experiments, we use\n a separate random number generator with a fixed random seed to ensure that the same set of labeled samples is\n selected for all experiments.\n   12We use the Apricot [Schreiber et al., 2020] submodular optimization codebase in Python.\n                                                        16", "md": "# Document\n\npretrained via SwAV as it encourages both data clustering and consistency of cluster assignment under data augmentations without involving supervision, which tends to provide representations that generalize better in a cluster-aware semi-supervised learning setting. Given the teacher models, the output (response) of each sample is taken as the corresponding teacher feature such that W = RK. In light of the empirical success of RKD based on simple kernel functions [Park et al., 2019, Liu et al., 2019], we focus on the (shifted) cosine kernel, $$k_{\\psi}(x, x') = \\frac{\\psi(x)^T\\psi(x')}{1 + \\|\\psi(x)\\|^2\\|\\psi(x')\\|^2}$$, in the experiments.\n\nWith data augmentation (e.g., when coupling with FixMatch), we assume that weak augmentations (i.e., random flips and crops) bring little perturbations on responses of the teacher model (i.e., the teacher features), intuitively as the predictions made by a good teacher model ought to be invariance under weak augmentations. Thereby, we can considerably reduce memory burden and accelerate the RKD process by storing the teacher features $$\\psi(x)$$ of the original data $$x \\in X$$ offline, instead of inferring every augmented sample $$x' \\in A(x)$$ with the large teacher model $$\\psi(x')$$ online.\n\nLabeled data selection. As alluded to in Remark 5.2, the success of low-label-rate (i.e., n \u226a N) semi-supervised classification naturally depends on the quality of the (extremely) limited amount of labeled data to represent the underlying population. We perform experiments in which the selection of labeled data is non-adaptive (Uniform) or adaptive (StochasticGreedy). The non-adaptive setting, Uniform, selects labeled data uniformly in each ground truth class. This contrasts with the strictly i.i.d. sampling assumption used in our theoretical results but constitutes a reasonable alternative that has been used in other low-label rate semi-supervised classification works (e.g., [Calder et al., 2020]).\n\nWe also utilize a coreset selection strategy (StochasticGreedy) for identifying \u201crepresentative\u201d inputs to use as labeled data in an adaptive manner. This labeled set is selected via a stochastic greedy [Mirzasoleiman et al., 2015] optimization problem to (approximately) minimize a facility location objective. An in-depth discussion of coreset selection methods is outside the scope of the current work, but we refer the interested reader to the enlightening references [Bilmes, 2022, Krause and Golovin, 2014, Mirzasoleiman, 2017] regarding submodular optimization methods in machine learning.\n\n|Label selection|Sample sizes (n, N)|SSL algorithm|Augmentation Strength|\n|---|---|---|---|\n|Uniform|(40, 50000)|FixMatch|High 89.47 \u00b1 3.83 Medium 87.89 \u00b1 1.43 Weak 81.36 \u00b1 1.41|\n|(40, 40000)| |FixMatch + RKD|High 89.88 \u00b1 0.41 Medium 89.10 \u00b1 1.14 Weak 84.61 \u00b1 1.23|\n|StochasticGreedy|(40, 40000)|FixMatch|High 84.51 \u00b1 1.34 Medium 84.88 \u00b1 6.68 Weak 70.70 \u00b1 5.32|\n|(40, 25000)| |FixMatch + RKD|High 85.68 \u00b1 1.19 Medium 88.31 \u00b1 1.36 Weak 85.12 \u00b1 3.49|\n\nResults. From Table 1 and Table 2, we observe that the incorporation of RKD with FixMatch (i.e., adding the \u201cglobal\u201d perspective of RKD to the \u201clocal\u201d perspective of DAC) generally brings additional improvements to the classification accuracy, especially when the data augmentation strength is inappropriate and/or the number of unlabeled data is limited.\n\nFor fair comparison and consistent performance among different configurations in the experiments, we use a separate random number generator with a fixed random seed to ensure that the same set of labeled samples is selected for all experiments.\n\nWe use the Apricot [Schreiber et al., 2020] submodular optimization codebase in Python.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "pretrained via SwAV as it encourages both data clustering and consistency of cluster assignment under data augmentations without involving supervision, which tends to provide representations that generalize better in a cluster-aware semi-supervised learning setting. Given the teacher models, the output (response) of each sample is taken as the corresponding teacher feature such that W = RK. In light of the empirical success of RKD based on simple kernel functions [Park et al., 2019, Liu et al., 2019], we focus on the (shifted) cosine kernel, $$k_{\\psi}(x, x') = \\frac{\\psi(x)^T\\psi(x')}{1 + \\|\\psi(x)\\|^2\\|\\psi(x')\\|^2}$$, in the experiments.\n\nWith data augmentation (e.g., when coupling with FixMatch), we assume that weak augmentations (i.e., random flips and crops) bring little perturbations on responses of the teacher model (i.e., the teacher features), intuitively as the predictions made by a good teacher model ought to be invariance under weak augmentations. Thereby, we can considerably reduce memory burden and accelerate the RKD process by storing the teacher features $$\\psi(x)$$ of the original data $$x \\in X$$ offline, instead of inferring every augmented sample $$x' \\in A(x)$$ with the large teacher model $$\\psi(x')$$ online.\n\nLabeled data selection. As alluded to in Remark 5.2, the success of low-label-rate (i.e., n \u226a N) semi-supervised classification naturally depends on the quality of the (extremely) limited amount of labeled data to represent the underlying population. We perform experiments in which the selection of labeled data is non-adaptive (Uniform) or adaptive (StochasticGreedy). The non-adaptive setting, Uniform, selects labeled data uniformly in each ground truth class. This contrasts with the strictly i.i.d. sampling assumption used in our theoretical results but constitutes a reasonable alternative that has been used in other low-label rate semi-supervised classification works (e.g., [Calder et al., 2020]).\n\nWe also utilize a coreset selection strategy (StochasticGreedy) for identifying \u201crepresentative\u201d inputs to use as labeled data in an adaptive manner. This labeled set is selected via a stochastic greedy [Mirzasoleiman et al., 2015] optimization problem to (approximately) minimize a facility location objective. An in-depth discussion of coreset selection methods is outside the scope of the current work, but we refer the interested reader to the enlightening references [Bilmes, 2022, Krause and Golovin, 2014, Mirzasoleiman, 2017] regarding submodular optimization methods in machine learning.", "md": "pretrained via SwAV as it encourages both data clustering and consistency of cluster assignment under data augmentations without involving supervision, which tends to provide representations that generalize better in a cluster-aware semi-supervised learning setting. Given the teacher models, the output (response) of each sample is taken as the corresponding teacher feature such that W = RK. In light of the empirical success of RKD based on simple kernel functions [Park et al., 2019, Liu et al., 2019], we focus on the (shifted) cosine kernel, $$k_{\\psi}(x, x') = \\frac{\\psi(x)^T\\psi(x')}{1 + \\|\\psi(x)\\|^2\\|\\psi(x')\\|^2}$$, in the experiments.\n\nWith data augmentation (e.g., when coupling with FixMatch), we assume that weak augmentations (i.e., random flips and crops) bring little perturbations on responses of the teacher model (i.e., the teacher features), intuitively as the predictions made by a good teacher model ought to be invariance under weak augmentations. Thereby, we can considerably reduce memory burden and accelerate the RKD process by storing the teacher features $$\\psi(x)$$ of the original data $$x \\in X$$ offline, instead of inferring every augmented sample $$x' \\in A(x)$$ with the large teacher model $$\\psi(x')$$ online.\n\nLabeled data selection. As alluded to in Remark 5.2, the success of low-label-rate (i.e., n \u226a N) semi-supervised classification naturally depends on the quality of the (extremely) limited amount of labeled data to represent the underlying population. We perform experiments in which the selection of labeled data is non-adaptive (Uniform) or adaptive (StochasticGreedy). The non-adaptive setting, Uniform, selects labeled data uniformly in each ground truth class. This contrasts with the strictly i.i.d. sampling assumption used in our theoretical results but constitutes a reasonable alternative that has been used in other low-label rate semi-supervised classification works (e.g., [Calder et al., 2020]).\n\nWe also utilize a coreset selection strategy (StochasticGreedy) for identifying \u201crepresentative\u201d inputs to use as labeled data in an adaptive manner. This labeled set is selected via a stochastic greedy [Mirzasoleiman et al., 2015] optimization problem to (approximately) minimize a facility location objective. An in-depth discussion of coreset selection methods is outside the scope of the current work, but we refer the interested reader to the enlightening references [Bilmes, 2022, Krause and Golovin, 2014, Mirzasoleiman, 2017] regarding submodular optimization methods in machine learning."}, {"type": "table", "rows": [["Label selection", "Sample sizes (n, N)", "SSL algorithm", "Augmentation Strength"], ["Uniform", "(40, 50000)", "FixMatch", "High 89.47 \u00b1 3.83 Medium 87.89 \u00b1 1.43 Weak 81.36 \u00b1 1.41"], ["(40, 40000)", "", "FixMatch + RKD", "High 89.88 \u00b1 0.41 Medium 89.10 \u00b1 1.14 Weak 84.61 \u00b1 1.23"], ["StochasticGreedy", "(40, 40000)", "FixMatch", "High 84.51 \u00b1 1.34 Medium 84.88 \u00b1 6.68 Weak 70.70 \u00b1 5.32"], ["(40, 25000)", "", "FixMatch + RKD", "High 85.68 \u00b1 1.19 Medium 88.31 \u00b1 1.36 Weak 85.12 \u00b1 3.49"]], "md": "|Label selection|Sample sizes (n, N)|SSL algorithm|Augmentation Strength|\n|---|---|---|---|\n|Uniform|(40, 50000)|FixMatch|High 89.47 \u00b1 3.83 Medium 87.89 \u00b1 1.43 Weak 81.36 \u00b1 1.41|\n|(40, 40000)| |FixMatch + RKD|High 89.88 \u00b1 0.41 Medium 89.10 \u00b1 1.14 Weak 84.61 \u00b1 1.23|\n|StochasticGreedy|(40, 40000)|FixMatch|High 84.51 \u00b1 1.34 Medium 84.88 \u00b1 6.68 Weak 70.70 \u00b1 5.32|\n|(40, 25000)| |FixMatch + RKD|High 85.68 \u00b1 1.19 Medium 88.31 \u00b1 1.36 Weak 85.12 \u00b1 3.49|", "isPerfectTable": true, "csv": "\"Label selection\",\"Sample sizes (n, N)\",\"SSL algorithm\",\"Augmentation Strength\"\n\"Uniform\",\"(40, 50000)\",\"FixMatch\",\"High 89.47 \u00b1 3.83 Medium 87.89 \u00b1 1.43 Weak 81.36 \u00b1 1.41\"\n\"(40, 40000)\",\"\",\"FixMatch + RKD\",\"High 89.88 \u00b1 0.41 Medium 89.10 \u00b1 1.14 Weak 84.61 \u00b1 1.23\"\n\"StochasticGreedy\",\"(40, 40000)\",\"FixMatch\",\"High 84.51 \u00b1 1.34 Medium 84.88 \u00b1 6.68 Weak 70.70 \u00b1 5.32\"\n\"(40, 25000)\",\"\",\"FixMatch + RKD\",\"High 85.68 \u00b1 1.19 Medium 88.31 \u00b1 1.36 Weak 85.12 \u00b1 3.49\""}, {"type": "text", "value": "Results. From Table 1 and Table 2, we observe that the incorporation of RKD with FixMatch (i.e., adding the \u201cglobal\u201d perspective of RKD to the \u201clocal\u201d perspective of DAC) generally brings additional improvements to the classification accuracy, especially when the data augmentation strength is inappropriate and/or the number of unlabeled data is limited.\n\nFor fair comparison and consistent performance among different configurations in the experiments, we use a separate random number generator with a fixed random seed to ensure that the same set of labeled samples is selected for all experiments.\n\nWe use the Apricot [Schreiber et al., 2020] submodular optimization codebase in Python.", "md": "Results. From Table 1 and Table 2, we observe that the incorporation of RKD with FixMatch (i.e., adding the \u201cglobal\u201d perspective of RKD to the \u201clocal\u201d perspective of DAC) generally brings additional improvements to the classification accuracy, especially when the data augmentation strength is inappropriate and/or the number of unlabeled data is limited.\n\nFor fair comparison and consistent performance among different configurations in the experiments, we use a separate random number generator with a fixed random seed to ensure that the same set of labeled samples is selected for all experiments.\n\nWe use the Apricot [Schreiber et al., 2020] submodular optimization codebase in Python."}]}, {"page": 17, "text": "                                          Table 2: Top-1 accuracy on CIFAR-100.\n   Label selection          Sample sizes (n, N)            SSL algorithm                          Augmentation Strength\n                                                                                         High              Medium                Weak\n                                (400, 50000)                  FixMatch              46.85 \u00b1 1.68        45.67 \u00b1 0.51         45.84 \u00b1 0.35\n                                                         FixMatch + RKD             49.33 \u00b1 0.42        47.97 \u00b1 0.37         47.65 \u00b1 0.17\n       Uniform                  (400, 40000)                  FixMatch              44.11 \u00b1 0.51        42.98 \u00b1 0.56         42.74 \u00b1 0.42\n                                                         FixMatch + RKD             46.24 \u00b1 0.64        45.78 \u00b1 0.94         45.13 \u00b1 1.08\n                                (400, 25000)                  FixMatch              36.60 \u00b1 0.03        35.86 \u00b1 0.51         33.59 \u00b1 1.54\n                                                         FixMatch + RKD             37.87 \u00b1 0.98        36.69 \u00b1 0.41         35.69 \u00b1 1.51\n                                (400, 50000)                  FixMatch              47.45 \u00b1 0.30        48.64 \u00b1 0.35         46.79 \u00b1 1.05\n                                                         FixMatch + RKD             50.41 \u00b1 1.41        50.67 \u00b1 1.62         49.09 \u00b1 1.31\n  StochasticGreedy              (400, 40000)                  FixMatch              45.93 \u00b1 0.21        44.78 \u00b1 1.51         44.62 \u00b1 0.72\n                                                         FixMatch + RKD             47.62 \u00b1 0.67        47.28 \u00b1 0.56         46.07 \u00b1 0.86\n                                (400, 25000)                  FixMatch              39.40 \u00b1 0.31        39.01 \u00b1 0.68         38.22 \u00b1 0.42\n                                                         FixMatch + RKD             40.88 \u00b1 0.61        40.73 \u00b1 0.46         39.20 \u00b1 0.73\nTraining details.           Throughout the experiments, we used weight decay 0.0005. We train the student\nmodel via stochastic gradient descent (SGD) with Nesterov momentum 0.9 for 217 iterations (batches)\nwith a batch size 64 \u2217          8 = 29 (consisting of 64 labeled samples and 64 \u2217                        7 unlabeled samples). The\ninitial learning rate is 0.03, decaying with a cosine scheduler. The test accuracies are evaluated 27\ntimes in total evenly throughout the 217 iterations (i.e., one test evaluation after every 210 iterations)\non an EMA model with a decay rate 0.999. The average and standard deviation of the best test\naccuracy (i.e., early stopping with the maximum patience 128) are reported for each experiment over\n3 arbitrary random seeds. Both CIFAR-10/100 experiments are conducted on one NVIDIA A40\nGPU.\nB      Proofs and Discussions for Section 5\nB.1     Proof of Theorem 5.1\nLemma B.1 (Rademacher complexity with cluster-aware functions (generalization of Yang et al.\n[2023] Theorem 8)). For a fixed FL(Xu), let\n                            \u2113  \u25e6 FL(Xu) =          \u2113(f(\u00b7), \u00b7) : X \u00d7 Y \u2192            {0, 1}      f \u2208   FL(Xu)\nbe the loss function class. Then, its Rademacher complexity can be upper-bounded by\n                                                                   2K log(2n)         + 2\u00b5      FL(Xu)      .                            (10)\n                                 Rn      \u2113 \u25e6  FL(Xu)       \u2264               n\nProof of Lemma B.1. Given a set of i.i.d. samples (X, y) \u223c                             P  (x, y)n, let\n               S  (X,y)    \u2113  \u25e6 FL(Xu)        \u225c      (\u2113  (f(x1), y1) , . . . , \u2113    (f(xn), yn))          f \u2208   FL(Xu)         .\ndenote the number of distinct patterns over (X, y) in \u2113                         \u25e6 FL(Xu). Then, by Massart\u2019s finite lemma,\nthe empirical Rademacher complexity with respect to (X, y) is upper bounded by\n                              R  (X,y)    \u2113  \u25e6 FL(Xu)        \u2264    2 log        S  (X,y)  n  \u2113 \u25e6  FL(Xu)         .\n                                                                      17", "md": "|Label selection|Sample sizes (n, N)|SSL algorithm|Augmentation Strength|\n|---|---|---|---|\n|(400, 50000)|FixMatch|46.85 \u00b1 1.68|45.67 \u00b1 0.51|45.84 \u00b1 0.35|\n|FixMatch + RKD|49.33 \u00b1 0.42|47.97 \u00b1 0.37|47.65 \u00b1 0.17|\n|(400, 40000)|FixMatch|44.11 \u00b1 0.51|42.98 \u00b1 0.56|42.74 \u00b1 0.42|\n|FixMatch + RKD|46.24 \u00b1 0.64|45.78 \u00b1 0.94|45.13 \u00b1 1.08|\n|(400, 25000)|FixMatch|36.60 \u00b1 0.03|35.86 \u00b1 0.51|33.59 \u00b1 1.54|\n|FixMatch + RKD|37.87 \u00b1 0.98|36.69 \u00b1 0.41|35.69 \u00b1 1.51|\n|(400, 50000)|FixMatch|47.45 \u00b1 0.30|48.64 \u00b1 0.35|46.79 \u00b1 1.05|\n|FixMatch + RKD|50.41 \u00b1 1.41|50.67 \u00b1 1.62|49.09 \u00b1 1.31|\n|(400, 40000)|FixMatch|45.93 \u00b1 0.21|44.78 \u00b1 1.51|44.62 \u00b1 0.72|\n|FixMatch + RKD|47.62 \u00b1 0.67|47.28 \u00b1 0.56|46.07 \u00b1 0.86|\n|(400, 25000)|FixMatch|39.40 \u00b1 0.31|39.01 \u00b1 0.68|38.22 \u00b1 0.42|\n|FixMatch + RKD|40.88 \u00b1 0.61|40.73 \u00b1 0.46|39.20 \u00b1 0.73|\n\nTraining details: Throughout the experiments, we used weight decay 0.0005. We train the student model via stochastic gradient descent (SGD) with Nesterov momentum 0.9 for 217 iterations (batches) with a batch size $$64 \\times 8 = 29$$ (consisting of 64 labeled samples and $$64 \\times 7$$ unlabeled samples). The initial learning rate is 0.03, decaying with a cosine scheduler. The test accuracies are evaluated 27 times in total evenly throughout the 217 iterations (i.e., one test evaluation after every 210 iterations) on an EMA model with a decay rate 0.999. The average and standard deviation of the best test accuracy (i.e., early stopping with the maximum patience 128) are reported for each experiment over 3 arbitrary random seeds. Both CIFAR-10/100 experiments are conducted on one NVIDIA A40 GPU.\n\n## Proofs and Discussions for Section 5\n\n### Proof of Theorem 5.1\n\nLemma B.1 (Rademacher complexity with cluster-aware functions (generalization of Yang et al. [2023] Theorem 8)): For a fixed $$FL(Xu)$$, let\n\n$$\n\\ell \\circ FL(Xu) = \\ell(f(\\cdot), \\cdot) : X \\times Y \\rightarrow \\{0, 1\\}, \\quad f \\in FL(Xu)\n$$\n\nbe the loss function class. Then, its Rademacher complexity can be upper-bounded by\n\n$$\nR_n \\ell \\circ FL(Xu) \\leq \\frac{2K \\log(2n)}{n} + 2\\mu FL(Xu) \\quad (10)\n$$\n\nProof of Lemma B.1: Given a set of i.i.d. samples $$(X, y) \\sim P(x, y)^n$$, let\n\n$$\nS(X,y) \\ell \\circ FL(Xu) \\triangleq (\\ell(f(x_1), y_1), ..., \\ell(f(x_n), y_n)), \\quad f \\in FL(Xu)\n$$\n\ndenote the number of distinct patterns over $$(X, y)$$ in $$\\ell \\circ FL(Xu)$$. Then, by Massart\u2019s finite lemma, the empirical Rademacher complexity with respect to $$(X, y)$$ is upper bounded by\n\n$$\nR(X,y) \\ell \\circ FL(Xu) \\leq 2 \\log S(X,y) n \\ell \\circ FL(Xu)\n$$", "images": [], "items": [{"type": "table", "rows": [["Label selection", "Sample sizes (n, N)", "SSL algorithm", "Augmentation Strength"], ["(400, 50000)", "FixMatch", "46.85 \u00b1 1.68", "45.67 \u00b1 0.51", "45.84 \u00b1 0.35"], ["FixMatch + RKD", "49.33 \u00b1 0.42", "47.97 \u00b1 0.37", "47.65 \u00b1 0.17"], ["(400, 40000)", "FixMatch", "44.11 \u00b1 0.51", "42.98 \u00b1 0.56", "42.74 \u00b1 0.42"], ["FixMatch + RKD", "46.24 \u00b1 0.64", "45.78 \u00b1 0.94", "45.13 \u00b1 1.08"], ["(400, 25000)", "FixMatch", "36.60 \u00b1 0.03", "35.86 \u00b1 0.51", "33.59 \u00b1 1.54"], ["FixMatch + RKD", "37.87 \u00b1 0.98", "36.69 \u00b1 0.41", "35.69 \u00b1 1.51"], ["(400, 50000)", "FixMatch", "47.45 \u00b1 0.30", "48.64 \u00b1 0.35", "46.79 \u00b1 1.05"], ["FixMatch + RKD", "50.41 \u00b1 1.41", "50.67 \u00b1 1.62", "49.09 \u00b1 1.31"], ["(400, 40000)", "FixMatch", "45.93 \u00b1 0.21", "44.78 \u00b1 1.51", "44.62 \u00b1 0.72"], ["FixMatch + RKD", "47.62 \u00b1 0.67", "47.28 \u00b1 0.56", "46.07 \u00b1 0.86"], ["(400, 25000)", "FixMatch", "39.40 \u00b1 0.31", "39.01 \u00b1 0.68", "38.22 \u00b1 0.42"], ["FixMatch + RKD", "40.88 \u00b1 0.61", "40.73 \u00b1 0.46", "39.20 \u00b1 0.73"]], "md": "|Label selection|Sample sizes (n, N)|SSL algorithm|Augmentation Strength|\n|---|---|---|---|\n|(400, 50000)|FixMatch|46.85 \u00b1 1.68|45.67 \u00b1 0.51|45.84 \u00b1 0.35|\n|FixMatch + RKD|49.33 \u00b1 0.42|47.97 \u00b1 0.37|47.65 \u00b1 0.17|\n|(400, 40000)|FixMatch|44.11 \u00b1 0.51|42.98 \u00b1 0.56|42.74 \u00b1 0.42|\n|FixMatch + RKD|46.24 \u00b1 0.64|45.78 \u00b1 0.94|45.13 \u00b1 1.08|\n|(400, 25000)|FixMatch|36.60 \u00b1 0.03|35.86 \u00b1 0.51|33.59 \u00b1 1.54|\n|FixMatch + RKD|37.87 \u00b1 0.98|36.69 \u00b1 0.41|35.69 \u00b1 1.51|\n|(400, 50000)|FixMatch|47.45 \u00b1 0.30|48.64 \u00b1 0.35|46.79 \u00b1 1.05|\n|FixMatch + RKD|50.41 \u00b1 1.41|50.67 \u00b1 1.62|49.09 \u00b1 1.31|\n|(400, 40000)|FixMatch|45.93 \u00b1 0.21|44.78 \u00b1 1.51|44.62 \u00b1 0.72|\n|FixMatch + RKD|47.62 \u00b1 0.67|47.28 \u00b1 0.56|46.07 \u00b1 0.86|\n|(400, 25000)|FixMatch|39.40 \u00b1 0.31|39.01 \u00b1 0.68|38.22 \u00b1 0.42|\n|FixMatch + RKD|40.88 \u00b1 0.61|40.73 \u00b1 0.46|39.20 \u00b1 0.73|", "isPerfectTable": false, "csv": "\"Label selection\",\"Sample sizes (n, N)\",\"SSL algorithm\",\"Augmentation Strength\"\n\"(400, 50000)\",\"FixMatch\",\"46.85 \u00b1 1.68\",\"45.67 \u00b1 0.51\",\"45.84 \u00b1 0.35\"\n\"FixMatch + RKD\",\"49.33 \u00b1 0.42\",\"47.97 \u00b1 0.37\",\"47.65 \u00b1 0.17\"\n\"(400, 40000)\",\"FixMatch\",\"44.11 \u00b1 0.51\",\"42.98 \u00b1 0.56\",\"42.74 \u00b1 0.42\"\n\"FixMatch + RKD\",\"46.24 \u00b1 0.64\",\"45.78 \u00b1 0.94\",\"45.13 \u00b1 1.08\"\n\"(400, 25000)\",\"FixMatch\",\"36.60 \u00b1 0.03\",\"35.86 \u00b1 0.51\",\"33.59 \u00b1 1.54\"\n\"FixMatch + RKD\",\"37.87 \u00b1 0.98\",\"36.69 \u00b1 0.41\",\"35.69 \u00b1 1.51\"\n\"(400, 50000)\",\"FixMatch\",\"47.45 \u00b1 0.30\",\"48.64 \u00b1 0.35\",\"46.79 \u00b1 1.05\"\n\"FixMatch + RKD\",\"50.41 \u00b1 1.41\",\"50.67 \u00b1 1.62\",\"49.09 \u00b1 1.31\"\n\"(400, 40000)\",\"FixMatch\",\"45.93 \u00b1 0.21\",\"44.78 \u00b1 1.51\",\"44.62 \u00b1 0.72\"\n\"FixMatch + RKD\",\"47.62 \u00b1 0.67\",\"47.28 \u00b1 0.56\",\"46.07 \u00b1 0.86\"\n\"(400, 25000)\",\"FixMatch\",\"39.40 \u00b1 0.31\",\"39.01 \u00b1 0.68\",\"38.22 \u00b1 0.42\"\n\"FixMatch + RKD\",\"40.88 \u00b1 0.61\",\"40.73 \u00b1 0.46\",\"39.20 \u00b1 0.73\""}, {"type": "text", "value": "Training details: Throughout the experiments, we used weight decay 0.0005. We train the student model via stochastic gradient descent (SGD) with Nesterov momentum 0.9 for 217 iterations (batches) with a batch size $$64 \\times 8 = 29$$ (consisting of 64 labeled samples and $$64 \\times 7$$ unlabeled samples). The initial learning rate is 0.03, decaying with a cosine scheduler. The test accuracies are evaluated 27 times in total evenly throughout the 217 iterations (i.e., one test evaluation after every 210 iterations) on an EMA model with a decay rate 0.999. The average and standard deviation of the best test accuracy (i.e., early stopping with the maximum patience 128) are reported for each experiment over 3 arbitrary random seeds. Both CIFAR-10/100 experiments are conducted on one NVIDIA A40 GPU.", "md": "Training details: Throughout the experiments, we used weight decay 0.0005. We train the student model via stochastic gradient descent (SGD) with Nesterov momentum 0.9 for 217 iterations (batches) with a batch size $$64 \\times 8 = 29$$ (consisting of 64 labeled samples and $$64 \\times 7$$ unlabeled samples). The initial learning rate is 0.03, decaying with a cosine scheduler. The test accuracies are evaluated 27 times in total evenly throughout the 217 iterations (i.e., one test evaluation after every 210 iterations) on an EMA model with a decay rate 0.999. The average and standard deviation of the best test accuracy (i.e., early stopping with the maximum patience 128) are reported for each experiment over 3 arbitrary random seeds. Both CIFAR-10/100 experiments are conducted on one NVIDIA A40 GPU."}, {"type": "heading", "lvl": 2, "value": "Proofs and Discussions for Section 5", "md": "## Proofs and Discussions for Section 5"}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 5.1", "md": "### Proof of Theorem 5.1"}, {"type": "text", "value": "Lemma B.1 (Rademacher complexity with cluster-aware functions (generalization of Yang et al. [2023] Theorem 8)): For a fixed $$FL(Xu)$$, let\n\n$$\n\\ell \\circ FL(Xu) = \\ell(f(\\cdot), \\cdot) : X \\times Y \\rightarrow \\{0, 1\\}, \\quad f \\in FL(Xu)\n$$\n\nbe the loss function class. Then, its Rademacher complexity can be upper-bounded by\n\n$$\nR_n \\ell \\circ FL(Xu) \\leq \\frac{2K \\log(2n)}{n} + 2\\mu FL(Xu) \\quad (10)\n$$\n\nProof of Lemma B.1: Given a set of i.i.d. samples $$(X, y) \\sim P(x, y)^n$$, let\n\n$$\nS(X,y) \\ell \\circ FL(Xu) \\triangleq (\\ell(f(x_1), y_1), ..., \\ell(f(x_n), y_n)), \\quad f \\in FL(Xu)\n$$\n\ndenote the number of distinct patterns over $$(X, y)$$ in $$\\ell \\circ FL(Xu)$$. Then, by Massart\u2019s finite lemma, the empirical Rademacher complexity with respect to $$(X, y)$$ is upper bounded by\n\n$$\nR(X,y) \\ell \\circ FL(Xu) \\leq 2 \\log S(X,y) n \\ell \\circ FL(Xu)\n$$", "md": "Lemma B.1 (Rademacher complexity with cluster-aware functions (generalization of Yang et al. [2023] Theorem 8)): For a fixed $$FL(Xu)$$, let\n\n$$\n\\ell \\circ FL(Xu) = \\ell(f(\\cdot), \\cdot) : X \\times Y \\rightarrow \\{0, 1\\}, \\quad f \\in FL(Xu)\n$$\n\nbe the loss function class. Then, its Rademacher complexity can be upper-bounded by\n\n$$\nR_n \\ell \\circ FL(Xu) \\leq \\frac{2K \\log(2n)}{n} + 2\\mu FL(Xu) \\quad (10)\n$$\n\nProof of Lemma B.1: Given a set of i.i.d. samples $$(X, y) \\sim P(x, y)^n$$, let\n\n$$\nS(X,y) \\ell \\circ FL(Xu) \\triangleq (\\ell(f(x_1), y_1), ..., \\ell(f(x_n), y_n)), \\quad f \\in FL(Xu)\n$$\n\ndenote the number of distinct patterns over $$(X, y)$$ in $$\\ell \\circ FL(Xu)$$. Then, by Massart\u2019s finite lemma, the empirical Rademacher complexity with respect to $$(X, y)$$ is upper bounded by\n\n$$\nR(X,y) \\ell \\circ FL(Xu) \\leq 2 \\log S(X,y) n \\ell \\circ FL(Xu)\n$$"}]}, {"page": 18, "text": " By the concavity of             log (\u00b7), we know that,\n                           Rn     \u2113 \u25e6  FL(Xu)        =E(X,y)        R(X,y)      \u2113  \u25e6 FL(Xu)\n                                                                  \uf8ee                S  (X,y)    \u2113  \u25e6 FL(Xu)          \uf8f9\n                                                                  \uf8ef  2 log                                          \uf8fa\n                                                                  \uf8ef                                                 \uf8fa\n                                                     \u2264E(X,y)      \uf8f0                          n                      \uf8fb                     (11)\n                                                     \u2264   2 log        E(X,y)      S(X,y)       \u2113 \u25e6 FL(Xu)            .\n                                                                                       n\n Since P (M(f)) \u2264            \u00b5    FL(Xu)        \u2264   1\n                                                    2 for all f \u2208      FL(Xu), by union bound, we have\n                   E(X,y) n    S(X,y)       \u2113 \u25e6  FL(Xu)\n                \u2264    n           \u00b5   FL(Xu)      \u03b9   1 \u2212    \u00b5   FL(Xu)       n\u2212\u03b9 \u00b7               n \u2212   \u03b9 + 1             2K+\u03b9\n                    \u03b9=0     \u03b9                                                            min {K, n \u2212         \u03b9} \u2212   1\n                \u2264(2n)K         n    n \u03b9      2\u00b5    FL(Xu)       \u03b9    1 \u2212   \u00b5   FL(Xu)        n\u2212\u03b9                                          (12)\n                             \u03b9=0\n                =(2n)K  1 \u2212          \u00b5   FL(Xu)        + 2\u00b5      FL(Xu)       n\n                \u2264(2n)Ken\u00b5(FL(Xu)),\n where \u03b9 counts the number of minority samples in the n samples. Plugging in this last line into\n Equation (11) yields Equation (10).\n Proof of Theorem 5.1. Since \u2113               \u25e6  FL(Xu) is 1-bounded with the zero-one loss \u2113, Lemma F.1 implies\n that with probability at least 1 \u2212             \u03b4/2 over (X, y),\n                              E    f|Xu      \u2212  E (f\u2217) \u2264      4Rn      \u2113  \u25e6 FL(Xu)        +       2 log (4/\u03b4)     .\n                                                                                                        n\n Then, incorporating the upper bound of Rn                       \u2113 \u25e6  FL(Xu)       from Lemma B.1 completes the proof.\n B.2     Cluster-wise Labeling with Cluster-aware Predictions\n Recall the notions of majority labeling               yf(\u00b7) (Equation (2)) and minority subset M (f) (Definition 3.1).\nWe first specify the cluster-aware predictions that are useful for labeling with the following non-\n degeneracy assumption.\n Assumption B.1 ((m0, c0)-non-degenerate predictions). For any prediction function f \u2208                                            F, we say\n f is (m0, c0)-non-degenerate if the following properties are satisfied:\n    (i) yf : X \u2192         [K] is surjective (i.e., {yf (x) | x \u2208              X  } = [K])\n   (ii)   X fk    \u2265   m0 for all k \u2208        [K], where we define X f         k \u225c     {x \u2208    X |   yf(x) = k}\n  (iii) There exists some c0 \u2265              2 such that c0 \u00b7 P          M(f) \u2229       X f     \u2264  P     X f     for all k \u2208     [K].\n                                                                                       k                 k\n It is worth pointing out that although the majority labeling                           yf(\u00b7) depends on the ground truth and\n therefore remains unknown without label acquisition, under Assumption B.1, the associated partition\n   X fk   k\u2208[K] over X depends only on f and is known without labeling. This is because yf(\u00b7) and\n yf(\u00b7) induce the same partition over X when both yf(\u00b7) and                                yf(\u00b7) are non-degenerate (i.e., being\n surjective functions onto [K], a necessary condition of Assumption B.1 (i) and (ii)). We also notice\n that Assumption B.1 (iii) can be generally satisfied with a reasonably large c when learning via\n Equation (3) with a sufficiently small \u00b5                   FL(Xu)        (cf. Theorem 4.1 or Theorem 4.3). For example,\n when c0 \u00b7 \u00b5       FL(Xu)        \u2264   mink\u2208K P           X f    , Assumption B.1 (iii) is automatically satisfied.\n                                                          k\n                                                                       18", "md": "By the concavity of $$\\log(\\cdot)$$, we know that,\n\n$$\n\\begin{align*}\nRn \\cdot \\ell \\circ FL(Xu) & \\leq E(X,y) \\cdot R(X,y) \\cdot \\ell \\circ FL(Xu) \\\\\n& \\leq 2 \\log \\left( \\frac{S(X,y)}{E(X,y)} \\cdot \\ell \\circ FL(Xu) \\right) \\quad (11) \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n \\quad (12)\n\\end{align*}\n$$\n\nSince $$P(M(f)) \\leq \\mu \\cdot FL(Xu) \\leq 1/2$$ for all $$f \\in FL(Xu)$$, by union bound, we have\n\n$$\n\\begin{align*}\nE(X,y) \\cdot n \\cdot S(X,y) \\cdot \\ell \\circ FL(Xu) & \\leq n \\cdot \\sum_{\\iota=0}^{K} \\mu \\cdot FL(Xu) \\cdot (1 - \\mu \\cdot FL(Xu))^{n-\\iota} \\cdot \\binom{n}{\\iota} \\cdot \\min\\{K, n - \\iota\\} - 1 \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n\n\\end{align*}\n$$\n\nwhere $$\\iota$$ counts the number of minority samples in the n samples. Plugging in this last line into Equation (11) yields Equation (10).\n\nProof of Theorem 5.1. Since $$\\ell \\circ FL(Xu)$$ is 1-bounded with the zero-one loss $$\\ell$$, Lemma F.1 implies that with probability at least $$1 - \\delta/2$$ over $$(X, y)$$,\n\n$$\nE[f|Xu] - E(f^*) \\leq 4Rn \\cdot \\ell \\circ FL(Xu) + 2\\log\\left(\\frac{4}{\\delta}\\right) / n\n$$\nThen, incorporating the upper bound of $$Rn \\cdot \\ell \\circ FL(Xu)$$ from Lemma B.1 completes the proof.\n\n## B.2 Cluster-wise Labeling with Cluster-aware Predictions\n\nRecall the notions of majority labeling $$y_f(\\cdot)$$ (Equation (2)) and minority subset $$M(f)$$ (Definition 3.1).\n\nWe first specify the cluster-aware predictions that are useful for labeling with the following non-degeneracy assumption.\n\nAssumption B.1 ((m0, c0)-non-degenerate predictions). For any prediction function $$f \\in F$$, we say $$f$$ is $$(m0, c0)$$-non-degenerate if the following properties are satisfied:\n\n1. $y_f : X \\rightarrow [K]$ is surjective (i.e., $\\{y_f(x) | x \\in X\\} = [K]$)\n2. $|X_f^k| \\geq m0$ for all $k \\in [K]$, where we define $X_f^k \\triangleq \\{x \\in X | y_f(x) = k\\}$\n3. There exists some $c0 \\geq 2$ such that $c0 \\cdot P(M(f) \\cap X_f^k) \\leq P(X_f^k)$ for all $k \\in [K]$.\n\nIt is worth pointing out that although the majority labeling $$y_f(\\cdot)$$ depends on the ground truth and therefore remains unknown without label acquisition, under Assumption B.1, the associated partition $$\\{X_f^k\\}_{k \\in [K]}$$ over $$X$$ depends only on $$f$$ and is known without labeling. This is because $$y_f(\\cdot)$$ and $$y_f(\\cdot)$$ induce the same partition over $$X$$ when both $$y_f(\\cdot)$$ and $$y_f(\\cdot)$$ are non-degenerate (i.e., being surjective functions onto $$[K]$$, a necessary condition of Assumption B.1 (i) and (ii)). We also notice that Assumption B.1 (iii) can be generally satisfied with a reasonably large $$c$$ when learning via Equation (3) with a sufficiently small $$\\mu \\cdot FL(Xu)$$ (cf. Theorem 4.1 or Theorem 4.3). For example, when $$c0 \\cdot \\mu \\cdot FL(Xu) \\leq \\min_{k \\in K} P(X_f^k)$$, Assumption B.1 (iii) is automatically satisfied.", "images": [], "items": [{"type": "text", "value": "By the concavity of $$\\log(\\cdot)$$, we know that,\n\n$$\n\\begin{align*}\nRn \\cdot \\ell \\circ FL(Xu) & \\leq E(X,y) \\cdot R(X,y) \\cdot \\ell \\circ FL(Xu) \\\\\n& \\leq 2 \\log \\left( \\frac{S(X,y)}{E(X,y)} \\cdot \\ell \\circ FL(Xu) \\right) \\quad (11) \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n \\quad (12)\n\\end{align*}\n$$\n\nSince $$P(M(f)) \\leq \\mu \\cdot FL(Xu) \\leq 1/2$$ for all $$f \\in FL(Xu)$$, by union bound, we have\n\n$$\n\\begin{align*}\nE(X,y) \\cdot n \\cdot S(X,y) \\cdot \\ell \\circ FL(Xu) & \\leq n \\cdot \\sum_{\\iota=0}^{K} \\mu \\cdot FL(Xu) \\cdot (1 - \\mu \\cdot FL(Xu))^{n-\\iota} \\cdot \\binom{n}{\\iota} \\cdot \\min\\{K, n - \\iota\\} - 1 \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n\n\\end{align*}\n$$\n\nwhere $$\\iota$$ counts the number of minority samples in the n samples. Plugging in this last line into Equation (11) yields Equation (10).\n\nProof of Theorem 5.1. Since $$\\ell \\circ FL(Xu)$$ is 1-bounded with the zero-one loss $$\\ell$$, Lemma F.1 implies that with probability at least $$1 - \\delta/2$$ over $$(X, y)$$,\n\n$$\nE[f|Xu] - E(f^*) \\leq 4Rn \\cdot \\ell \\circ FL(Xu) + 2\\log\\left(\\frac{4}{\\delta}\\right) / n\n$$\nThen, incorporating the upper bound of $$Rn \\cdot \\ell \\circ FL(Xu)$$ from Lemma B.1 completes the proof.", "md": "By the concavity of $$\\log(\\cdot)$$, we know that,\n\n$$\n\\begin{align*}\nRn \\cdot \\ell \\circ FL(Xu) & \\leq E(X,y) \\cdot R(X,y) \\cdot \\ell \\circ FL(Xu) \\\\\n& \\leq 2 \\log \\left( \\frac{S(X,y)}{E(X,y)} \\cdot \\ell \\circ FL(Xu) \\right) \\quad (11) \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n \\quad (12)\n\\end{align*}\n$$\n\nSince $$P(M(f)) \\leq \\mu \\cdot FL(Xu) \\leq 1/2$$ for all $$f \\in FL(Xu)$$, by union bound, we have\n\n$$\n\\begin{align*}\nE(X,y) \\cdot n \\cdot S(X,y) \\cdot \\ell \\circ FL(Xu) & \\leq n \\cdot \\sum_{\\iota=0}^{K} \\mu \\cdot FL(Xu) \\cdot (1 - \\mu \\cdot FL(Xu))^{n-\\iota} \\cdot \\binom{n}{\\iota} \\cdot \\min\\{K, n - \\iota\\} - 1 \\\\\n& \\leq (2n)K \\cdot \\left( 1 - \\mu \\cdot FL(Xu) \\right) + 2\\mu \\cdot FL(Xu) \\cdot n\n\\end{align*}\n$$\n\nwhere $$\\iota$$ counts the number of minority samples in the n samples. Plugging in this last line into Equation (11) yields Equation (10).\n\nProof of Theorem 5.1. Since $$\\ell \\circ FL(Xu)$$ is 1-bounded with the zero-one loss $$\\ell$$, Lemma F.1 implies that with probability at least $$1 - \\delta/2$$ over $$(X, y)$$,\n\n$$\nE[f|Xu] - E(f^*) \\leq 4Rn \\cdot \\ell \\circ FL(Xu) + 2\\log\\left(\\frac{4}{\\delta}\\right) / n\n$$\nThen, incorporating the upper bound of $$Rn \\cdot \\ell \\circ FL(Xu)$$ from Lemma B.1 completes the proof."}, {"type": "heading", "lvl": 2, "value": "B.2 Cluster-wise Labeling with Cluster-aware Predictions", "md": "## B.2 Cluster-wise Labeling with Cluster-aware Predictions"}, {"type": "text", "value": "Recall the notions of majority labeling $$y_f(\\cdot)$$ (Equation (2)) and minority subset $$M(f)$$ (Definition 3.1).\n\nWe first specify the cluster-aware predictions that are useful for labeling with the following non-degeneracy assumption.\n\nAssumption B.1 ((m0, c0)-non-degenerate predictions). For any prediction function $$f \\in F$$, we say $$f$$ is $$(m0, c0)$$-non-degenerate if the following properties are satisfied:\n\n1. $y_f : X \\rightarrow [K]$ is surjective (i.e., $\\{y_f(x) | x \\in X\\} = [K]$)\n2. $|X_f^k| \\geq m0$ for all $k \\in [K]$, where we define $X_f^k \\triangleq \\{x \\in X | y_f(x) = k\\}$\n3. There exists some $c0 \\geq 2$ such that $c0 \\cdot P(M(f) \\cap X_f^k) \\leq P(X_f^k)$ for all $k \\in [K]$.\n\nIt is worth pointing out that although the majority labeling $$y_f(\\cdot)$$ depends on the ground truth and therefore remains unknown without label acquisition, under Assumption B.1, the associated partition $$\\{X_f^k\\}_{k \\in [K]}$$ over $$X$$ depends only on $$f$$ and is known without labeling. This is because $$y_f(\\cdot)$$ and $$y_f(\\cdot)$$ induce the same partition over $$X$$ when both $$y_f(\\cdot)$$ and $$y_f(\\cdot)$$ are non-degenerate (i.e., being surjective functions onto $$[K]$$, a necessary condition of Assumption B.1 (i) and (ii)). We also notice that Assumption B.1 (iii) can be generally satisfied with a reasonably large $$c$$ when learning via Equation (3) with a sufficiently small $$\\mu \\cdot FL(Xu)$$ (cf. Theorem 4.1 or Theorem 4.3). For example, when $$c0 \\cdot \\mu \\cdot FL(Xu) \\leq \\min_{k \\in K} P(X_f^k)$$, Assumption B.1 (iii) is automatically satisfied.", "md": "Recall the notions of majority labeling $$y_f(\\cdot)$$ (Equation (2)) and minority subset $$M(f)$$ (Definition 3.1).\n\nWe first specify the cluster-aware predictions that are useful for labeling with the following non-degeneracy assumption.\n\nAssumption B.1 ((m0, c0)-non-degenerate predictions). For any prediction function $$f \\in F$$, we say $$f$$ is $$(m0, c0)$$-non-degenerate if the following properties are satisfied:\n\n1. $y_f : X \\rightarrow [K]$ is surjective (i.e., $\\{y_f(x) | x \\in X\\} = [K]$)\n2. $|X_f^k| \\geq m0$ for all $k \\in [K]$, where we define $X_f^k \\triangleq \\{x \\in X | y_f(x) = k\\}$\n3. There exists some $c0 \\geq 2$ such that $c0 \\cdot P(M(f) \\cap X_f^k) \\leq P(X_f^k)$ for all $k \\in [K]$.\n\nIt is worth pointing out that although the majority labeling $$y_f(\\cdot)$$ depends on the ground truth and therefore remains unknown without label acquisition, under Assumption B.1, the associated partition $$\\{X_f^k\\}_{k \\in [K]}$$ over $$X$$ depends only on $$f$$ and is known without labeling. This is because $$y_f(\\cdot)$$ and $$y_f(\\cdot)$$ induce the same partition over $$X$$ when both $$y_f(\\cdot)$$ and $$y_f(\\cdot)$$ are non-degenerate (i.e., being surjective functions onto $$[K]$$, a necessary condition of Assumption B.1 (i) and (ii)). We also notice that Assumption B.1 (iii) can be generally satisfied with a reasonably large $$c$$ when learning via Equation (3) with a sufficiently small $$\\mu \\cdot FL(Xu)$$ (cf. Theorem 4.1 or Theorem 4.3). For example, when $$c0 \\cdot \\mu \\cdot FL(Xu) \\leq \\min_{k \\in K} P(X_f^k)$$, Assumption B.1 (iii) is automatically satisfied."}]}, {"page": 19, "text": "With the cluster-aware predictions in Assumption B.1, we show that O (log(K)) i.i.d. labeled\nsamples per predicted cluster are suffi                      cient to ensure at least one labeled sample per class. That is,\nthe cluster-wise labeling with (m0, c0)-non-degenerate predictions guarantees a label complexity of\nO (K log(K)) even with class imbalance.\nProposition B.2 (Cluster-wise labeling). Given any (m0, c0)-non-degenerate prediction function\nf \u2208     F (Assumption B.1) with sufficiently large m0 \u226b                                   logc0 (2K) and c0 \u2265           m     2, for each k \u2208            [K],\nacquire labels of m (m \u2264                  m0) unlabeled samples {xu                                               X f         drawn i.i.d. from X f\n                                2K                                         2K       ki}i\u2208[m] \u223c         Unif         k                                       k .\nThen, for any \u03b4 \u2208               cm0   , 1   , when m \u2265           logc0       \u03b4     , {y\u2217   (xu ki)}k\u2208[K],i\u2208[m] = [K] have at least one\n                                 0\nground truth label per class with probability at least 1 \u2212                                 \u03b4.\nProof of Proposition B.2. We first observe that\n    P    {y\u2217    (xu ki)}k\u2208[K],i\u2208[m] = [K]                 =P [\u2200      k\u2032 \u2208    [K], \u2203      k \u2208    [K], i \u2208      [m] s.t. y\u2217       (xu ki) = k\u2032]\n                                                          \u2265P [\u2200 K    k \u2208    [K], \u2203      i \u2208   [m] s.t. y\u2217       (xu ki) = k]\nwhere                                                     =   k=1    P {xu ki}i\u2208[m]\u223cUnif(X f        k ) m [\u2203   i \u2208    [m] s.t. y\u2217      (xu ki) = k] ,\n                     P [\u2203   i \u2208    [m] s.t. y\u2217      (xu ki) = k] =1 \u2212            P [\u2200   i \u2208   [m], y\u2217      (xu ki) \u0338= k]\n                                                                       =1 \u2212        Px\u223cUnif(X f      k ) [y\u2217   (x) \u0338=     yf (x)]     m\n                                                                       =1 \u2212        Px\u223cUnif(X f      k ) [x \u2208     M(f)]       m\n                                                                                 \uf8eb   P     M(f) \u2229        X fk    \uf8f6m\n                                                                       =1 \u2212      \uf8ed          P     X fk           \uf8f8       .\nSince Assumption B.1 implies P                          M(f) \u2229        X fk       P     X fk     \u2264    c1\n                                                                                                      0 for all k \u2208        [K], we have\n            P    {y\u2217   (xu                                        \u2265      1 \u2212        1   m K         \u2265    exp      \u22122K           \u2265   1 \u2212     2K    ,\n                           ki)}k\u2208[K],i\u2208[m] = [K]                                   c0                                 cm0                   cm0\nwhere the second and the third inequalities follow from the facts that 1 \u2212                                                 q \u2265     exp (\u22122q) for all\n0 \u2264    q \u2264      1\n                2 and exp (\u2212q) \u2265    2K        1 \u2212    q for all 0 \u2264         q \u2264    1, respectively. It is straightforward to observe\nthat when m \u2265             logc0        \u03b4    , we have 2K      cm    \u2264   \u03b4. With mink\u2208[K]               X fk     = m0, taking up to m \u2264                    m0\n                                                               0\nunlabeled samples for each k \u2208                     [K], we can achieve \u03b4 as small as \u03b4min = 2K                         cm0   .\n                                                                                                                        0\nC       Proofs and Discussions for Section 4.1\nIn Appendix C.1, we proceed with the proof of low clustering error guarantee (Theorem 4.1) for\nminimizing the population RKD loss (Equation (1)). In Appendix C.2, we extend the discussion in\nRemark 4.1 on the limitation of spectral clustering alone in the end-to-end setting via an illustrative toy\ncounter-example. In Appendix C.3, we clarify the connection between our results and corresponding\nspectral graph theoretic notions of sparsest k-partitions.\nC.1       Proof of Theorem 4.1\n                                                                                \u2192\nGiven any labeling function y : X \u2192                               [K], let \u2212y : X \u2192                 {0, 1}K be the one-hot encoded la-\nbeling function such that for y (X                      ) \u2208    [K]|X|, \u2212y (X\u2192         ) \u2208    {0, 1}|X|\u00d7K. Further, given f \u2208                         F, we\ndenote UX \u225c              D (X     )1/2 \u2212 \u2192                                                                                          \u2192\nUX , FX , YX \u2208             R|X|\u00d7K.       yf (X     ), FX \u225c         D (X     )1/2 f (X       ), and YX \u225c             D (X     )1/2 \u2212 y\u2217(X     ) such that\n                                                                             19", "md": "With the cluster-aware predictions in Assumption B.1, we show that O(log(K)) i.i.d. labeled samples per predicted cluster are sufficient to ensure at least one labeled sample per class. That is, the cluster-wise labeling with (m0, c0)-non-degenerate predictions guarantees a label complexity of O(K log(K)) even with class imbalance.\n\nProposition B.2 (Cluster-wise labeling). Given any (m0, c0)-non-degenerate prediction function f \u2208 F (Assumption B.1) with sufficiently large m0 \u226b logc0(2K) and c0 \u2265 m/2, for each k \u2208 [K], acquire labels of m (m \u2264 m0) unlabeled samples {xu} drawn i.i.d. from Xf, then for any \u03b4 \u2208 cm0,1, when m \u2265 logc0(\u03b4), {y\u2217(xu)} have at least one ground truth label per class with probability at least 1 - \u03b4.\n\nProof of Proposition B.2. We first observe that\n\n$$\n\\begin{align*}\n&P\\{y\u2217(xu)\\} = [K] = P[\\forall k' \\in [K], \\exists k \\in [K], i \\in [m] \\text{ s.t. } y\u2217(xu) = k'] \\\\\n&\\geq P[\\forall k \\in [K], \\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] \\\\\n&= \\prod_{k=1}^{K} P\\{xu\\} \\sim Unif(Xf_k) m [\\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] \\\\\n&P[\\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] = 1 - P[\\forall i \\in [m], y\u2217(xu) \\neq k] \\\\\n&= 1 - P_{x \\sim Unif(Xf_k)}[y\u2217(x) \\neq yf(x)]^m \\\\\n&= 1 - P_{x \\sim Unif(Xf_k)}[x \\in M(f)]^m \\\\\n&= 1 - \\left(\\frac{P[M(f) \\cap Xf_k]}{P[Xf_k]}\\right)^m\n\\end{align*}\n$$\n\nSince Assumption B.1 implies $P[M(f) \\cap Xf_k] \\leq c_0$ for all $k \\in [K]$, we have\n\n$$\nP\\{y\u2217(xu)\\} \\geq 1 - \\frac{1}{mK} \\geq \\exp(-2K) \\geq 1 - 2K/c_0\n$$\n\nIt is straightforward to observe that when $m \\geq \\log(c_0/\\delta)$, we have $2K/cm \\leq \\delta$. With $\\min_{k\\in[K]} Xf_k = m0$, taking up to $m \\leq m0$ unlabeled samples for each $k \\in [K]$, we can achieve $\\delta$ as small as $\\delta_{\\text{min}} = 2K/cm0$.\n\nProofs and Discussions for Section 4.1\n\nIn Appendix C.1, we proceed with the proof of low clustering error guarantee (Theorem 4.1) for minimizing the population RKD loss (Equation (1)). In Appendix C.2, we extend the discussion in Remark 4.1 on the limitation of spectral clustering alone in the end-to-end setting via an illustrative toy counter-example. In Appendix C.3, we clarify the connection between our results and corresponding spectral graph theoretic notions of sparsest k-partitions.\n\nC.1 Proof of Theorem 4.1\n\nGiven any labeling function $y : X \\rightarrow [K]$, let $-y : X \\rightarrow \\{0, 1\\}^K$ be the one-hot encoded labeling function such that for $y(X) \\in [K]|X|$, $-y(X) \\in \\{0, 1\\}|X| \\times K$. Further, given $f \\in F$, we denote $U_X \\triangleq D(X)^{1/2} \\rightarrow U_X, F_X \\triangleq D(X)^{1/2} f(X), Y_X \\triangleq D(X)^{1/2} - y^*(X)$ such that", "images": [], "items": [{"type": "text", "value": "With the cluster-aware predictions in Assumption B.1, we show that O(log(K)) i.i.d. labeled samples per predicted cluster are sufficient to ensure at least one labeled sample per class. That is, the cluster-wise labeling with (m0, c0)-non-degenerate predictions guarantees a label complexity of O(K log(K)) even with class imbalance.\n\nProposition B.2 (Cluster-wise labeling). Given any (m0, c0)-non-degenerate prediction function f \u2208 F (Assumption B.1) with sufficiently large m0 \u226b logc0(2K) and c0 \u2265 m/2, for each k \u2208 [K], acquire labels of m (m \u2264 m0) unlabeled samples {xu} drawn i.i.d. from Xf, then for any \u03b4 \u2208 cm0,1, when m \u2265 logc0(\u03b4), {y\u2217(xu)} have at least one ground truth label per class with probability at least 1 - \u03b4.\n\nProof of Proposition B.2. We first observe that\n\n$$\n\\begin{align*}\n&P\\{y\u2217(xu)\\} = [K] = P[\\forall k' \\in [K], \\exists k \\in [K], i \\in [m] \\text{ s.t. } y\u2217(xu) = k'] \\\\\n&\\geq P[\\forall k \\in [K], \\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] \\\\\n&= \\prod_{k=1}^{K} P\\{xu\\} \\sim Unif(Xf_k) m [\\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] \\\\\n&P[\\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] = 1 - P[\\forall i \\in [m], y\u2217(xu) \\neq k] \\\\\n&= 1 - P_{x \\sim Unif(Xf_k)}[y\u2217(x) \\neq yf(x)]^m \\\\\n&= 1 - P_{x \\sim Unif(Xf_k)}[x \\in M(f)]^m \\\\\n&= 1 - \\left(\\frac{P[M(f) \\cap Xf_k]}{P[Xf_k]}\\right)^m\n\\end{align*}\n$$\n\nSince Assumption B.1 implies $P[M(f) \\cap Xf_k] \\leq c_0$ for all $k \\in [K]$, we have\n\n$$\nP\\{y\u2217(xu)\\} \\geq 1 - \\frac{1}{mK} \\geq \\exp(-2K) \\geq 1 - 2K/c_0\n$$\n\nIt is straightforward to observe that when $m \\geq \\log(c_0/\\delta)$, we have $2K/cm \\leq \\delta$. With $\\min_{k\\in[K]} Xf_k = m0$, taking up to $m \\leq m0$ unlabeled samples for each $k \\in [K]$, we can achieve $\\delta$ as small as $\\delta_{\\text{min}} = 2K/cm0$.\n\nProofs and Discussions for Section 4.1\n\nIn Appendix C.1, we proceed with the proof of low clustering error guarantee (Theorem 4.1) for minimizing the population RKD loss (Equation (1)). In Appendix C.2, we extend the discussion in Remark 4.1 on the limitation of spectral clustering alone in the end-to-end setting via an illustrative toy counter-example. In Appendix C.3, we clarify the connection between our results and corresponding spectral graph theoretic notions of sparsest k-partitions.\n\nC.1 Proof of Theorem 4.1\n\nGiven any labeling function $y : X \\rightarrow [K]$, let $-y : X \\rightarrow \\{0, 1\\}^K$ be the one-hot encoded labeling function such that for $y(X) \\in [K]|X|$, $-y(X) \\in \\{0, 1\\}|X| \\times K$. Further, given $f \\in F$, we denote $U_X \\triangleq D(X)^{1/2} \\rightarrow U_X, F_X \\triangleq D(X)^{1/2} f(X), Y_X \\triangleq D(X)^{1/2} - y^*(X)$ such that", "md": "With the cluster-aware predictions in Assumption B.1, we show that O(log(K)) i.i.d. labeled samples per predicted cluster are sufficient to ensure at least one labeled sample per class. That is, the cluster-wise labeling with (m0, c0)-non-degenerate predictions guarantees a label complexity of O(K log(K)) even with class imbalance.\n\nProposition B.2 (Cluster-wise labeling). Given any (m0, c0)-non-degenerate prediction function f \u2208 F (Assumption B.1) with sufficiently large m0 \u226b logc0(2K) and c0 \u2265 m/2, for each k \u2208 [K], acquire labels of m (m \u2264 m0) unlabeled samples {xu} drawn i.i.d. from Xf, then for any \u03b4 \u2208 cm0,1, when m \u2265 logc0(\u03b4), {y\u2217(xu)} have at least one ground truth label per class with probability at least 1 - \u03b4.\n\nProof of Proposition B.2. We first observe that\n\n$$\n\\begin{align*}\n&P\\{y\u2217(xu)\\} = [K] = P[\\forall k' \\in [K], \\exists k \\in [K], i \\in [m] \\text{ s.t. } y\u2217(xu) = k'] \\\\\n&\\geq P[\\forall k \\in [K], \\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] \\\\\n&= \\prod_{k=1}^{K} P\\{xu\\} \\sim Unif(Xf_k) m [\\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] \\\\\n&P[\\exists i \\in [m] \\text{ s.t. } y\u2217(xu) = k] = 1 - P[\\forall i \\in [m], y\u2217(xu) \\neq k] \\\\\n&= 1 - P_{x \\sim Unif(Xf_k)}[y\u2217(x) \\neq yf(x)]^m \\\\\n&= 1 - P_{x \\sim Unif(Xf_k)}[x \\in M(f)]^m \\\\\n&= 1 - \\left(\\frac{P[M(f) \\cap Xf_k]}{P[Xf_k]}\\right)^m\n\\end{align*}\n$$\n\nSince Assumption B.1 implies $P[M(f) \\cap Xf_k] \\leq c_0$ for all $k \\in [K]$, we have\n\n$$\nP\\{y\u2217(xu)\\} \\geq 1 - \\frac{1}{mK} \\geq \\exp(-2K) \\geq 1 - 2K/c_0\n$$\n\nIt is straightforward to observe that when $m \\geq \\log(c_0/\\delta)$, we have $2K/cm \\leq \\delta$. With $\\min_{k\\in[K]} Xf_k = m0$, taking up to $m \\leq m0$ unlabeled samples for each $k \\in [K]$, we can achieve $\\delta$ as small as $\\delta_{\\text{min}} = 2K/cm0$.\n\nProofs and Discussions for Section 4.1\n\nIn Appendix C.1, we proceed with the proof of low clustering error guarantee (Theorem 4.1) for minimizing the population RKD loss (Equation (1)). In Appendix C.2, we extend the discussion in Remark 4.1 on the limitation of spectral clustering alone in the end-to-end setting via an illustrative toy counter-example. In Appendix C.3, we clarify the connection between our results and corresponding spectral graph theoretic notions of sparsest k-partitions.\n\nC.1 Proof of Theorem 4.1\n\nGiven any labeling function $y : X \\rightarrow [K]$, let $-y : X \\rightarrow \\{0, 1\\}^K$ be the one-hot encoded labeling function such that for $y(X) \\in [K]|X|$, $-y(X) \\in \\{0, 1\\}|X| \\times K$. Further, given $f \\in F$, we denote $U_X \\triangleq D(X)^{1/2} \\rightarrow U_X, F_X \\triangleq D(X)^{1/2} f(X), Y_X \\triangleq D(X)^{1/2} - y^*(X)$ such that"}]}, {"page": 20, "text": " Since L(X       ) = I \u2212       W(X      ), the classical argument with Gershgorin circle theorem [Golub and\n Van Loan, 2013, Theorem 7.2.1] implies that 0 \u227c                             L(X    ) \u227c    2I, whereas k\u03c8 being positive semi-\n definite further implies that W(X                 ) \u227d   0 and therefore 0 \u227c           L(X    ) \u227c   I (see the proof of Claim C.3).\n Consider the spectral decomposition of L (X                      ) = I \u2212     W(X      ),\n       L (X    ) =    |X|  \u03bbi \u00b7 viv\u22a4  i     s.t.    D (X     )\u22121/2 W (X        ) D (X    )\u22121/2 =       |X|  (1 \u2212    \u03bbi) \u00b7 viv\u22a4  i ,       (13)\n                     i=1                                                                              i=1\n where      vi \u2208    R|X| i\u2208[|X|] are the orthonormal eigenvectors associated with 0 = \u03bb1 \u2264                                   \u00b7 \u00b7 \u00b7 \u2264  \u03bb|X| \u2264\n1, respectively, that form a basis of R|X|.\n Lemma C.1. Given any f \u2208                   F that satisfies Assumption 4.2, we have\n        P (M(f)) = Ex\u223cP [1 {               yf(x) \u0338= y\u2217(x)}] \u2264           2 max      \u03b22\u03b32 , 1     \u00b7    min                             F .\n Proof of Lemma C.1. Without ambiguity, we overload the notation of each sample x \u2208               Z\u2208RK\u00d7K \u2225YX \u2212             FX Z\u22252  X as the\n corresponding index in [|X              |] such that, given any sample subset S = {si}i\u2208[|S|] \u2282                             X  , following\n the MATLAB notation for matrix indexing, \u03a0S \u225c                               I|X| (:, S) \u2208      R|X|\u00d7|S| where I|X| denotes the\n |X  | \u00d7 |X    | identity matrix. With respect to S, we also denote FS \u225c                                   FX (S, :) = \u03a0\u22a4         S FX and\nYS \u225c       YX (S, :) = \u03a0\u22a4        S YX .\nWe first show that for any S = [s1; . . . ; sK] \u2208                   X K that satisfies rank (FS) = K,\n                                  YX \u2212       FX F\u22121  S YS      2             min                            F .                           (14)\n                                                                F \u2264    2  Z\u2208RK\u00d7K \u2225YX \u2212            FX Z\u22252\n To see this, we start by replacing Z on the right-hand side of Equation (14) with the corresponding\n least square solution such that\n                                      min                             F =     YX \u2212       FX F\u2020   X YX     2F\n                                   Z\u2208RK\u00d7K \u2225YX \u2212             FX Z\u22252\n where F\u2020    X =      F\u22a4 X FX    \u22121 F\u22a4   X . Then, we can decompose the left-hand side of Equation (14) via the\n orthogonal projection FX F\u2020             X :\n  YX \u2212       FX F\u22121  S YS      2F =        I \u2212   FX F\u2020   X      YX \u2212      FX F\u22121  S YS        2       FX F\u2020    X    YX \u2212      FX F\u22121  S YS       2\n                                                                                              F +                                                F\n                                   =    YX \u2212       FX F\u2020   X YX     2F + \u2225(PF \u2212          PS) YX \u22252     F ,\n where PF \u225c         FX F\u2020   X = FX         F\u22a4 X FX     \u22121 F\u22a4  X and PS \u225c          FX     \u03a0\u22a4 S FX     \u22121 \u03a0\u22a4   S . By observing that\n                      PSPF = FX             \u03a0\u22a4  S FX    \u22121 \u03a0\u22a4    S FX      F\u22a4X FX     \u22121 F\u22a4   X = PF = PF PF ,\n we can upper bound \u2225(PF \u2212                 PS) YX \u22252      F such that\n                                 \u2225(PF \u2212       PS) YX \u22252     F = \u2225(PF \u2212          PS) (I \u2212      PF ) YX \u22252     F\n                                                                \u2264  \u2225(I \u2212    PF ) YX \u22252     F\n which leads to Equation (14).                                  =   YX \u2212       FX F\u2020   X YX      2F ,\n Now, we consider the sample subset S \u2208                         X K described in Assumption 4.2. For given f \u2208                               F,\n partitioning X into the majority and minority subsets, X                         \\M(f) and M(f), respectively yields\n                   YX \u2212        FX F\u22121 S YS       2F          \u2212\u2192            \u2192                              2\n               = (1 \u2212     P  (M(f))) \u00b7 Ex\u223cP (x)               y\u2217(x) \u2212      \u2212\n                                                                           y\u2217(S)\u22a4f(S)\u2212\u22a4f(x)                2    x /\u2208  M(f)\n                                                              \u2212\u2192            \u2192                              2\n                         + P   (M(f)) \u00b7 Ex\u223cP (x)               y\u2217(x) \u2212      \u2212\n                                                    \u2192            \u2192          y\u2217(S)\u22a4f(S)\u2212\u22a4f(x)                2    x \u2208   M(f)\n               \u2265P    (M(f)) \u00b7 Ex\u223cP (x)             \u2212y\u2217(x) \u2212      \u2212                              2                       .\n                                                                 y\u2217(S)\u22a4f(S)\u2212\u22a4f(x)                2    x \u2208    M(f)\n                                                                       20", "md": "# Math Equations and Text\n\nSince $$L(X) = I - W(X)$$, the classical argument with Gershgorin circle theorem [Golub and Van Loan, 2013, Theorem 7.2.1] implies that $$0 \\preceq L(X) \\preceq 2I$$, whereas $$k\\psi$$ being positive semi-definite further implies that $$W(X) \\succeq 0$$ and therefore $$0 \\preceq L(X) \\preceq I$$ (see the proof of Claim C.3).\n\nConsider the spectral decomposition of $$L(X) = I - W(X)$$:\n\n$$\n\\begin{align*}\nL(X) &= \\sum_{i=1}^{|X|} |X|\\lambda_i \\cdot v_iv_i^T \\text{ s.t. } D(X)^{-1/2}W(X)D(X)^{-1/2} = \\sum_{i=1}^{|X|} |X|(1 - \\lambda_i) \\cdot v_iv_i^T \\quad (13)\n\\end{align*}\n$$\n\nwhere $$v_i \\in \\mathbb{R}^{|X|}, i \\in [|X|]$$ are the orthonormal eigenvectors associated with $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\leq 1$$, respectively, that form a basis of $$\\mathbb{R}^{|X|}$$.\n\nLemma C.1. Given any $$f \\in \\mathcal{F}$$ that satisfies Assumption 4.2, we have:\n\n$$\n\\begin{align*}\nP(M(f)) & = \\mathbb{E}_{x \\sim P}[1\\{yf(x) \\neq y^*(x)\\}] \\leq 2\\max(\\beta^2\\gamma^2, 1) \\cdot \\min \\mathcal{F}.\n\\end{align*}\n$$\n\nProof of Lemma C.1. Without ambiguity, we overload the notation of each sample $$x \\in \\mathcal{Z} \\in \\mathbb{R}^{K \\times K} \\|YX - FXZ\\|_2X$$ as the corresponding index in $$[|X|]$$ such that, given any sample subset $$S = \\{s_i\\}_{i \\in [|S|]} \\subset X$$, following the MATLAB notation for matrix indexing, $$\\Pi_S \\triangleq I_{|X|}(:, S) \\in \\mathbb{R}^{|X| \\times |S|}$$ where $$I_{|X|}$$ denotes the $$|X| \\times |X|$$ identity matrix. With respect to $$S$$, we also denote $$F_S \\triangleq FX(S, :) = \\Pi_S^T FX$$ and $$Y_S \\triangleq YX(S, :) = \\Pi_S^T YX$$.\n\nWe first show that for any $$S = [s_1; \\ldots; s_K] \\in X^K$$ that satisfies $$\\text{rank}(F_S) = K$$:\n\n$$\n\\begin{align*}\n\\|YX - FXF_S^{-1}YS\\|_2F \\leq 2\\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_2\n\\end{align*}\n$$\n\nTo see this, we start by replacing $$Z$$ on the right-hand side of Equation (14) with the corresponding least square solution such that:\n\n$$\n\\begin{align*}\n\\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_2 = \\|YX - FXF_X^{\\dagger}YX\\|_2F\n\\end{align*}\n$$\n\nwhere $$F_X^{\\dagger} = (F_X^T FX)^{-1}F_X^T$$. Then, we can decompose the left-hand side of Equation (14) via the orthogonal projection $$FXF_X^{\\dagger}$$:\n\n$$\n\\begin{align*}\n\\|YX - FXF_S^{-1}YS\\|_2F & = \\|I - FXF_X^{\\dagger}YX - FXF_S^{-1}YS\\|_2F + \\|PF - PSYX\\|_2F \\\\\n& = \\|YX - FXF_X^{\\dagger}YX\\|_2F + \\|(PF - PS)YX\\|_2F,\n\\end{align*}\n$$\n\nwhere $$PF = FXF_X^{\\dagger} = FX(F_X^T FX)^{-1}F_X^T$$ and $$PS = FX\\Pi_S^T FX(F_X^T FX)^{-1}\\Pi_S^T$$. By observing that $$PSPF = FX\\Pi_S^T FX(F_X^T FX)^{-1}\\Pi_S^T FX(F_X^T FX)^{-1}F_X^T = PF = PF^2$$, we can upper bound $$\\|(PF - PS)YX\\|_2F$$ such that:\n\n$$\n\\begin{align*}\n\\|(PF - PS)YX\\|_2F & = \\|(PF - PS)(I - PF)YX\\|_2F \\\\\n& \\leq \\|(I - PF)YX\\|_2F \\\\\n& = \\|YX - FXF_X^{\\dagger}YX\\|_2F,\n\\end{align*}\n$$\n\nNow, we consider the sample subset $$S \\in X^K$$ described in Assumption 4.2. For given $$f \\in \\mathcal{F}$$, partitioning $$X$$ into the majority and minority subsets, $$X \\setminus M(f)$$ and $$M(f)$$, respectively yields:\n\n$$\n\\begin{align*}\n& \\|YX - FXF_S^{-1}YS\\|_2F \\rightarrow \\rightarrow 2 \\\\\n& = (1 - P(M(f))) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\notin M(f) \\\\\n& \\rightarrow \\rightarrow 2 \\\\\n& + P(M(f)) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\in M(f) \\\\\n& \\geq P(M(f)) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\in M(f).\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Since $$L(X) = I - W(X)$$, the classical argument with Gershgorin circle theorem [Golub and Van Loan, 2013, Theorem 7.2.1] implies that $$0 \\preceq L(X) \\preceq 2I$$, whereas $$k\\psi$$ being positive semi-definite further implies that $$W(X) \\succeq 0$$ and therefore $$0 \\preceq L(X) \\preceq I$$ (see the proof of Claim C.3).\n\nConsider the spectral decomposition of $$L(X) = I - W(X)$$:\n\n$$\n\\begin{align*}\nL(X) &= \\sum_{i=1}^{|X|} |X|\\lambda_i \\cdot v_iv_i^T \\text{ s.t. } D(X)^{-1/2}W(X)D(X)^{-1/2} = \\sum_{i=1}^{|X|} |X|(1 - \\lambda_i) \\cdot v_iv_i^T \\quad (13)\n\\end{align*}\n$$\n\nwhere $$v_i \\in \\mathbb{R}^{|X|}, i \\in [|X|]$$ are the orthonormal eigenvectors associated with $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\leq 1$$, respectively, that form a basis of $$\\mathbb{R}^{|X|}$$.\n\nLemma C.1. Given any $$f \\in \\mathcal{F}$$ that satisfies Assumption 4.2, we have:\n\n$$\n\\begin{align*}\nP(M(f)) & = \\mathbb{E}_{x \\sim P}[1\\{yf(x) \\neq y^*(x)\\}] \\leq 2\\max(\\beta^2\\gamma^2, 1) \\cdot \\min \\mathcal{F}.\n\\end{align*}\n$$\n\nProof of Lemma C.1. Without ambiguity, we overload the notation of each sample $$x \\in \\mathcal{Z} \\in \\mathbb{R}^{K \\times K} \\|YX - FXZ\\|_2X$$ as the corresponding index in $$[|X|]$$ such that, given any sample subset $$S = \\{s_i\\}_{i \\in [|S|]} \\subset X$$, following the MATLAB notation for matrix indexing, $$\\Pi_S \\triangleq I_{|X|}(:, S) \\in \\mathbb{R}^{|X| \\times |S|}$$ where $$I_{|X|}$$ denotes the $$|X| \\times |X|$$ identity matrix. With respect to $$S$$, we also denote $$F_S \\triangleq FX(S, :) = \\Pi_S^T FX$$ and $$Y_S \\triangleq YX(S, :) = \\Pi_S^T YX$$.\n\nWe first show that for any $$S = [s_1; \\ldots; s_K] \\in X^K$$ that satisfies $$\\text{rank}(F_S) = K$$:\n\n$$\n\\begin{align*}\n\\|YX - FXF_S^{-1}YS\\|_2F \\leq 2\\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_2\n\\end{align*}\n$$\n\nTo see this, we start by replacing $$Z$$ on the right-hand side of Equation (14) with the corresponding least square solution such that:\n\n$$\n\\begin{align*}\n\\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_2 = \\|YX - FXF_X^{\\dagger}YX\\|_2F\n\\end{align*}\n$$\n\nwhere $$F_X^{\\dagger} = (F_X^T FX)^{-1}F_X^T$$. Then, we can decompose the left-hand side of Equation (14) via the orthogonal projection $$FXF_X^{\\dagger}$$:\n\n$$\n\\begin{align*}\n\\|YX - FXF_S^{-1}YS\\|_2F & = \\|I - FXF_X^{\\dagger}YX - FXF_S^{-1}YS\\|_2F + \\|PF - PSYX\\|_2F \\\\\n& = \\|YX - FXF_X^{\\dagger}YX\\|_2F + \\|(PF - PS)YX\\|_2F,\n\\end{align*}\n$$\n\nwhere $$PF = FXF_X^{\\dagger} = FX(F_X^T FX)^{-1}F_X^T$$ and $$PS = FX\\Pi_S^T FX(F_X^T FX)^{-1}\\Pi_S^T$$. By observing that $$PSPF = FX\\Pi_S^T FX(F_X^T FX)^{-1}\\Pi_S^T FX(F_X^T FX)^{-1}F_X^T = PF = PF^2$$, we can upper bound $$\\|(PF - PS)YX\\|_2F$$ such that:\n\n$$\n\\begin{align*}\n\\|(PF - PS)YX\\|_2F & = \\|(PF - PS)(I - PF)YX\\|_2F \\\\\n& \\leq \\|(I - PF)YX\\|_2F \\\\\n& = \\|YX - FXF_X^{\\dagger}YX\\|_2F,\n\\end{align*}\n$$\n\nNow, we consider the sample subset $$S \\in X^K$$ described in Assumption 4.2. For given $$f \\in \\mathcal{F}$$, partitioning $$X$$ into the majority and minority subsets, $$X \\setminus M(f)$$ and $$M(f)$$, respectively yields:\n\n$$\n\\begin{align*}\n& \\|YX - FXF_S^{-1}YS\\|_2F \\rightarrow \\rightarrow 2 \\\\\n& = (1 - P(M(f))) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\notin M(f) \\\\\n& \\rightarrow \\rightarrow 2 \\\\\n& + P(M(f)) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\in M(f) \\\\\n& \\geq P(M(f)) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\in M(f).\n\\end{align*}\n$$", "md": "Since $$L(X) = I - W(X)$$, the classical argument with Gershgorin circle theorem [Golub and Van Loan, 2013, Theorem 7.2.1] implies that $$0 \\preceq L(X) \\preceq 2I$$, whereas $$k\\psi$$ being positive semi-definite further implies that $$W(X) \\succeq 0$$ and therefore $$0 \\preceq L(X) \\preceq I$$ (see the proof of Claim C.3).\n\nConsider the spectral decomposition of $$L(X) = I - W(X)$$:\n\n$$\n\\begin{align*}\nL(X) &= \\sum_{i=1}^{|X|} |X|\\lambda_i \\cdot v_iv_i^T \\text{ s.t. } D(X)^{-1/2}W(X)D(X)^{-1/2} = \\sum_{i=1}^{|X|} |X|(1 - \\lambda_i) \\cdot v_iv_i^T \\quad (13)\n\\end{align*}\n$$\n\nwhere $$v_i \\in \\mathbb{R}^{|X|}, i \\in [|X|]$$ are the orthonormal eigenvectors associated with $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\leq 1$$, respectively, that form a basis of $$\\mathbb{R}^{|X|}$$.\n\nLemma C.1. Given any $$f \\in \\mathcal{F}$$ that satisfies Assumption 4.2, we have:\n\n$$\n\\begin{align*}\nP(M(f)) & = \\mathbb{E}_{x \\sim P}[1\\{yf(x) \\neq y^*(x)\\}] \\leq 2\\max(\\beta^2\\gamma^2, 1) \\cdot \\min \\mathcal{F}.\n\\end{align*}\n$$\n\nProof of Lemma C.1. Without ambiguity, we overload the notation of each sample $$x \\in \\mathcal{Z} \\in \\mathbb{R}^{K \\times K} \\|YX - FXZ\\|_2X$$ as the corresponding index in $$[|X|]$$ such that, given any sample subset $$S = \\{s_i\\}_{i \\in [|S|]} \\subset X$$, following the MATLAB notation for matrix indexing, $$\\Pi_S \\triangleq I_{|X|}(:, S) \\in \\mathbb{R}^{|X| \\times |S|}$$ where $$I_{|X|}$$ denotes the $$|X| \\times |X|$$ identity matrix. With respect to $$S$$, we also denote $$F_S \\triangleq FX(S, :) = \\Pi_S^T FX$$ and $$Y_S \\triangleq YX(S, :) = \\Pi_S^T YX$$.\n\nWe first show that for any $$S = [s_1; \\ldots; s_K] \\in X^K$$ that satisfies $$\\text{rank}(F_S) = K$$:\n\n$$\n\\begin{align*}\n\\|YX - FXF_S^{-1}YS\\|_2F \\leq 2\\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_2\n\\end{align*}\n$$\n\nTo see this, we start by replacing $$Z$$ on the right-hand side of Equation (14) with the corresponding least square solution such that:\n\n$$\n\\begin{align*}\n\\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_2 = \\|YX - FXF_X^{\\dagger}YX\\|_2F\n\\end{align*}\n$$\n\nwhere $$F_X^{\\dagger} = (F_X^T FX)^{-1}F_X^T$$. Then, we can decompose the left-hand side of Equation (14) via the orthogonal projection $$FXF_X^{\\dagger}$$:\n\n$$\n\\begin{align*}\n\\|YX - FXF_S^{-1}YS\\|_2F & = \\|I - FXF_X^{\\dagger}YX - FXF_S^{-1}YS\\|_2F + \\|PF - PSYX\\|_2F \\\\\n& = \\|YX - FXF_X^{\\dagger}YX\\|_2F + \\|(PF - PS)YX\\|_2F,\n\\end{align*}\n$$\n\nwhere $$PF = FXF_X^{\\dagger} = FX(F_X^T FX)^{-1}F_X^T$$ and $$PS = FX\\Pi_S^T FX(F_X^T FX)^{-1}\\Pi_S^T$$. By observing that $$PSPF = FX\\Pi_S^T FX(F_X^T FX)^{-1}\\Pi_S^T FX(F_X^T FX)^{-1}F_X^T = PF = PF^2$$, we can upper bound $$\\|(PF - PS)YX\\|_2F$$ such that:\n\n$$\n\\begin{align*}\n\\|(PF - PS)YX\\|_2F & = \\|(PF - PS)(I - PF)YX\\|_2F \\\\\n& \\leq \\|(I - PF)YX\\|_2F \\\\\n& = \\|YX - FXF_X^{\\dagger}YX\\|_2F,\n\\end{align*}\n$$\n\nNow, we consider the sample subset $$S \\in X^K$$ described in Assumption 4.2. For given $$f \\in \\mathcal{F}$$, partitioning $$X$$ into the majority and minority subsets, $$X \\setminus M(f)$$ and $$M(f)$$, respectively yields:\n\n$$\n\\begin{align*}\n& \\|YX - FXF_S^{-1}YS\\|_2F \\rightarrow \\rightarrow 2 \\\\\n& = (1 - P(M(f))) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\notin M(f) \\\\\n& \\rightarrow \\rightarrow 2 \\\\\n& + P(M(f)) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\in M(f) \\\\\n& \\geq P(M(f)) \\cdot \\mathbb{E}_{x \\sim P}(x) \\|y^*(x) - y^*(S)^Tf(S)^Tf(x)\\|_2 x \\in M(f).\n\\end{align*}\n$$"}]}, {"page": 21, "text": "                                                                  \u2212\nSince sk        \u2208/    M(f), we have \u2212            \u2192                \u2192\n                                                 y\u2217(S)      =     yf(S) whose rows lie in the canonical bases of\n ek \u2208     RK     k\u2208[K]; whereas for x \u2208              M(f), we have y\u2217(x) \u0338=                yf(x). Therefore, in the case when\ny\u2217(x) = y\u2217(sk) for some k \u2208        \u2212\u2192      [K],  \u2192                              2\n                                    y\u2217(x) \u2212      \u2212\n                                                 y\u2217(S)\u22a4f(S)\u2212\u22a4f(x)                2\n                                   f(S)\u2212\u22a4                   \u2192        \u2192                      2\n                               =                  f(S)\u22a4\u2212    y\u2217(S)\u2212   y\u2217(x) \u2212      f(x)       2\n                               \u2265\u03c3K       f(S)\u2212\u22a4 2 \u00b7         f(x) \u2212      k\u2208[K]:y\u2217(x)=y\u2217(sk)        f(sk)     22 ,\nwhere \u03c3K         f(S)\u2212\u22a4         = 1/\u03b2, and with            yf(sk) = y\u2217(sk) = y\u2217(x) \u0338=                    yf(x) implying yf(sk) \u0338=\nyf(x),\nwe have                                    f(x) \u2212     k\u2208[K]:y\u2217(x)=y\u2217(sk)         f(sk)    22  \u2265  \u03b32,\n                                           \u2212\u2192            \u2192                              2\n                                            y\u2217(x) \u2212      \u2212\n                                                         y\u2217(S)\u22a4f(S)\u2212\u22a4f(x)                2 \u2265    \u03b32\n                                                                                                \u03b22 .\nMeanwhile, in the case when y\u2217(x) \u0338= y\u2217(sk) for all k \u2208                                  [K], we have \u2212       \u2192        \u2192\ntherefore \u2212\u2192            \u2192                              2        \u2212\u2192            \u2192            \u2192                  y\u2217(S)\u2212   y\u2217(x)2= 0, and\n           y\u2217(x) \u2212      \u2212\n                        y\u2217(S)\u22a4f(S)\u2212\u22a4f(x)                         y\u2217(x)\u22a4      \u2212y\u2217(x) \u2212      \u2212\nOverall, we\u2019ve shown that       \u2212\u2192            \u2192         2 \u2265                  2             y\u2217(S)\u22a4f(S)\u2212\u22a4f(x)        \u03b32        2,= 1.\n                Ex\u223cP (x)         y\u2217(x) \u2212      \u2212\nwhich leads to                                y\u2217(S)\u22a4f(S)\u2212\u22a4f(x)                2    x \u2208    M(f)       \u2265   min       \u03b22 , 1\n                                 P  (M(f)) \u2264         max     \u03b22\u03b32 , 1      YX \u2212       FX F\u22121  S YS      2F .                             (15)\nFinally, combining Equation (14) and Equation (15) completes the proof.\nLemma C.2. With yk \u2208               [0, 1]|X| denoting the k-th column of YX \u225c                     D (X    )1/2 \u2212 \u2192\n                                                      k\u2208[K]   y\u22a4 k L (X    ) yk = \u03b1.                             y\u2217(X    ) \u2208  [0, 1]|X|\u00d7K,\nProof of Lemma C.2. It is sufficient to observe that, by construction, for every k \u2208                                    [K],\n     y\u22a4k L (X    ) yk =y\u22a4    k yk \u2212     y\u22a4k D(X     )\u22121/2W(X         )D(X     )\u22121/2yk\n                        =  x\u2208X    wx \u00b7 1 {y\u2217(x) = k} \u2212             x\u2208X    x\u2032\u2208X    wxx\u2032 \u00b7 1 {y\u2217(x) = k} \u00b7 1 {y\u2217(x\u2032) = k}\n                        =  x\u2208X    1 {y\u2217(x) = k}            wx \u2212     x\u2032\u2208X    wxx\u2032 \u00b7 1 {y\u2217(x) = y\u2217(x\u2032)}\n                        =  x\u2208X    1 {y\u2217(x) = k}         x\u2032\u2208X    wxx\u2032 \u00b7 1 {y\u2217(x) \u0338= y\u2217(x\u2032)} .\nMeanwhile, since  x\u2208X                    x\u2032\u2208X wxx\u2032 = 1, we have \u03b1 = 1                      k\u2208[K]       x\u2208Xk        x\u2032 /\n                                                                                    2                                 \u2208Xk wxx\u2032 and\n              k\u2208[K]   y\u22a4k L (X     ) yk =    x\u2208X    x\u2032\u2208X   wxx\u2032 \u00b7 1 {y\u2217(x) \u0338= y\u2217(x\u2032)}              k\u2208[K]   1 {y\u2217(x) = k}\n                                          =  x\u2208X    x\u2032\u2208X   wxx\u2032 \u00b7 1 {y\u2217(x) \u0338= y\u2217(x\u2032)}\n                                          =1 2  k\u2208[K]   x\u2208Xk    x\u2032 /     wxx\u2032 = \u03b1.\n                                                                   \u2208Xk\n                                                                      21", "md": "# Math Equations\n\nSince \\(s_k \\notin M(f)\\), we have\n\n$$y^*(S) = yf(S)$$\nwhose rows lie in the canonical bases of\n\n$$e_k \\in \\mathbb{R}^K, \\quad k \\in [K];$$\n\nwhereas for \\(x \\in M(f)\\), we have \\(y^*(x) \\neq yf(x)\\). Therefore, in the case when\n\n$$y^*(x) = y^*(s_k) \\text{ for some } k \\in [K],$$\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\sigma_K f(S)^T - 2 \\cdot f(x) - \\sum_{k \\in [K]: y^*(x)=y^*(s_k)} f(s_k)^2,$$\nwhere \\(\\sigma_K f(S)^T = 1/\\beta\\), and with \\(yf(s_k) = y^*(s_k) = y^*(x) \\neq yf(x)\\) implying \\(yf(s_k) \\neq yf(x)\\), we have\n\n$$f(x) - \\sum_{k \\in [K]: y^*(x)=y^*(s_k)} f(s_k)^2 \\geq \\gamma^2,$$\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\frac{\\gamma^2}{\\beta^2}.$$\nMeanwhile, in the case when \\(y^*(x) \\neq y^*(s_k)\\) for all \\(k \\in [K]\\), we have\n\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\gamma^2.$$\nOverall, we've shown that\n\n$$\\mathbb{E}_{x \\sim P(x)} \\left[ y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\right]_2 \\geq \\min \\left( \\beta^2, 1 \\right),$$\n$$P(M(f)) \\leq \\max \\left( \\beta^2\\gamma^2, 1 \\right) \\left\\| YX - FXF^{-1}SY \\right\\|_2F. \\quad (15)$$\nFinally, combining Equation (14) and Equation (15) completes the proof.\n\nLemma C.2. With \\(y_k \\in [0, 1]^{|X|}\\) denoting the \\(k\\)-th column of \\(YX \\triangleq D(X)^{1/2}\\),\n\n$$\\sum_{k \\in [K]} y_k^T L(X) y_k = \\alpha, \\quad y^*(X) \\in [0, 1]^{|X| \\times K},$$\nProof of Lemma C.2. It is sufficient to observe that, by construction, for every \\(k \\in [K],\\)\n\n$$y_k^T L(X) y_k = y_k^T y_k - y_k^T D(X)^{-1/2}W(X)D(X)^{-1/2}y_k$$\n$$= \\sum_{x \\in X} w_x \\cdot \\mathbb{1} \\{ y^*(x) = k \\} - \\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) = k \\} \\cdot \\mathbb{1} \\{ y^*(x') = k \\}$$\n$$= \\sum_{x \\in X} \\mathbb{1} \\{ y^*(x) = k \\} w_x - \\sum_{x' \\in X} \\sum_{x \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) = y^*(x') \\}$$\n$$= \\sum_{x \\in X} \\mathbb{1} \\{ y^*(x) = k \\} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) \\neq y^*(x') \\}.$$\nMeanwhile, since \\(\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} = 1\\), we have \\(\\alpha = 1\\),\n\n$$\\sum_{k \\in [K]} y_k^T L(X) y_k = \\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) \\neq y^*(x') \\} = \\sum_{k \\in [K]} \\mathbb{1} \\{ y^*(x) = k \\}$$\n$$= \\sum_{x \\in X_k} \\sum_{x' \\notin X_k} w_{xx'} = \\alpha.$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Since \\(s_k \\notin M(f)\\), we have\n\n$$y^*(S) = yf(S)$$\nwhose rows lie in the canonical bases of\n\n$$e_k \\in \\mathbb{R}^K, \\quad k \\in [K];$$\n\nwhereas for \\(x \\in M(f)\\), we have \\(y^*(x) \\neq yf(x)\\). Therefore, in the case when\n\n$$y^*(x) = y^*(s_k) \\text{ for some } k \\in [K],$$\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\sigma_K f(S)^T - 2 \\cdot f(x) - \\sum_{k \\in [K]: y^*(x)=y^*(s_k)} f(s_k)^2,$$\nwhere \\(\\sigma_K f(S)^T = 1/\\beta\\), and with \\(yf(s_k) = y^*(s_k) = y^*(x) \\neq yf(x)\\) implying \\(yf(s_k) \\neq yf(x)\\), we have\n\n$$f(x) - \\sum_{k \\in [K]: y^*(x)=y^*(s_k)} f(s_k)^2 \\geq \\gamma^2,$$\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\frac{\\gamma^2}{\\beta^2}.$$\nMeanwhile, in the case when \\(y^*(x) \\neq y^*(s_k)\\) for all \\(k \\in [K]\\), we have\n\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\gamma^2.$$\nOverall, we've shown that\n\n$$\\mathbb{E}_{x \\sim P(x)} \\left[ y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\right]_2 \\geq \\min \\left( \\beta^2, 1 \\right),$$\n$$P(M(f)) \\leq \\max \\left( \\beta^2\\gamma^2, 1 \\right) \\left\\| YX - FXF^{-1}SY \\right\\|_2F. \\quad (15)$$\nFinally, combining Equation (14) and Equation (15) completes the proof.\n\nLemma C.2. With \\(y_k \\in [0, 1]^{|X|}\\) denoting the \\(k\\)-th column of \\(YX \\triangleq D(X)^{1/2}\\),\n\n$$\\sum_{k \\in [K]} y_k^T L(X) y_k = \\alpha, \\quad y^*(X) \\in [0, 1]^{|X| \\times K},$$\nProof of Lemma C.2. It is sufficient to observe that, by construction, for every \\(k \\in [K],\\)\n\n$$y_k^T L(X) y_k = y_k^T y_k - y_k^T D(X)^{-1/2}W(X)D(X)^{-1/2}y_k$$\n$$= \\sum_{x \\in X} w_x \\cdot \\mathbb{1} \\{ y^*(x) = k \\} - \\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) = k \\} \\cdot \\mathbb{1} \\{ y^*(x') = k \\}$$\n$$= \\sum_{x \\in X} \\mathbb{1} \\{ y^*(x) = k \\} w_x - \\sum_{x' \\in X} \\sum_{x \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) = y^*(x') \\}$$\n$$= \\sum_{x \\in X} \\mathbb{1} \\{ y^*(x) = k \\} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) \\neq y^*(x') \\}.$$\nMeanwhile, since \\(\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} = 1\\), we have \\(\\alpha = 1\\),\n\n$$\\sum_{k \\in [K]} y_k^T L(X) y_k = \\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) \\neq y^*(x') \\} = \\sum_{k \\in [K]} \\mathbb{1} \\{ y^*(x) = k \\}$$\n$$= \\sum_{x \\in X_k} \\sum_{x' \\notin X_k} w_{xx'} = \\alpha.$$", "md": "Since \\(s_k \\notin M(f)\\), we have\n\n$$y^*(S) = yf(S)$$\nwhose rows lie in the canonical bases of\n\n$$e_k \\in \\mathbb{R}^K, \\quad k \\in [K];$$\n\nwhereas for \\(x \\in M(f)\\), we have \\(y^*(x) \\neq yf(x)\\). Therefore, in the case when\n\n$$y^*(x) = y^*(s_k) \\text{ for some } k \\in [K],$$\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\sigma_K f(S)^T - 2 \\cdot f(x) - \\sum_{k \\in [K]: y^*(x)=y^*(s_k)} f(s_k)^2,$$\nwhere \\(\\sigma_K f(S)^T = 1/\\beta\\), and with \\(yf(s_k) = y^*(s_k) = y^*(x) \\neq yf(x)\\) implying \\(yf(s_k) \\neq yf(x)\\), we have\n\n$$f(x) - \\sum_{k \\in [K]: y^*(x)=y^*(s_k)} f(s_k)^2 \\geq \\gamma^2,$$\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\frac{\\gamma^2}{\\beta^2}.$$\nMeanwhile, in the case when \\(y^*(x) \\neq y^*(s_k)\\) for all \\(k \\in [K]\\), we have\n\n$$y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\geq \\gamma^2.$$\nOverall, we've shown that\n\n$$\\mathbb{E}_{x \\sim P(x)} \\left[ y^*(x) - y^*(S)^Tf(S)^{-1}f(x) \\right]_2 \\geq \\min \\left( \\beta^2, 1 \\right),$$\n$$P(M(f)) \\leq \\max \\left( \\beta^2\\gamma^2, 1 \\right) \\left\\| YX - FXF^{-1}SY \\right\\|_2F. \\quad (15)$$\nFinally, combining Equation (14) and Equation (15) completes the proof.\n\nLemma C.2. With \\(y_k \\in [0, 1]^{|X|}\\) denoting the \\(k\\)-th column of \\(YX \\triangleq D(X)^{1/2}\\),\n\n$$\\sum_{k \\in [K]} y_k^T L(X) y_k = \\alpha, \\quad y^*(X) \\in [0, 1]^{|X| \\times K},$$\nProof of Lemma C.2. It is sufficient to observe that, by construction, for every \\(k \\in [K],\\)\n\n$$y_k^T L(X) y_k = y_k^T y_k - y_k^T D(X)^{-1/2}W(X)D(X)^{-1/2}y_k$$\n$$= \\sum_{x \\in X} w_x \\cdot \\mathbb{1} \\{ y^*(x) = k \\} - \\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) = k \\} \\cdot \\mathbb{1} \\{ y^*(x') = k \\}$$\n$$= \\sum_{x \\in X} \\mathbb{1} \\{ y^*(x) = k \\} w_x - \\sum_{x' \\in X} \\sum_{x \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) = y^*(x') \\}$$\n$$= \\sum_{x \\in X} \\mathbb{1} \\{ y^*(x) = k \\} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) \\neq y^*(x') \\}.$$\nMeanwhile, since \\(\\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} = 1\\), we have \\(\\alpha = 1\\),\n\n$$\\sum_{k \\in [K]} y_k^T L(X) y_k = \\sum_{x \\in X} \\sum_{x' \\in X} w_{xx'} \\cdot \\mathbb{1} \\{ y^*(x) \\neq y^*(x') \\} = \\sum_{k \\in [K]} \\mathbb{1} \\{ y^*(x) = k \\}$$\n$$= \\sum_{x \\in X_k} \\sum_{x' \\notin X_k} w_{xx'} = \\alpha.$$"}]}, {"page": 22, "text": "                                                                                                                            1\nClaim C.3. For all f                     \u2208     FL(X) minimizing Equation (1), Range                               D(X     ) 2 f (X   )     =\nspan {v1, . . . , vK}.\nProof of Claim C.3. Since 0 \u227c                    L(X    ) \u227c      2I, we have \u2212I \u227c                W(X      ) \u227c     I.    Meanwhile, let\nk\u03c8 (X    , X  ) \u2208  R|X|\u00d7|X| be the population kernel matrix such that k\u03c8 (X                             , X )xx\u2032 = k\u03c8 (x, x\u2032). Then\nby defintion, k\u03c8 (X         , X  ) = D(X       )\u22121W(X        )D(X     )\u22121, and therefore\n                I \u227d    W(X      ) = D(X       )\u22121 2 W(X     )D(X     )\u22121  2 = D(X       )21k\u03c8 (X     , X ) D(X     )21\u227d     0.\nThat is, the eigenvalues of L(X              ) and those of W(X            ) satisfies\n                              \u03bbi \u225c    \u03bbi (L(X     )) = 1 \u2212      \u03bb|X|\u2212i+1        W(X      )      \u2200  i \u2208   [|X  |],\nwhile the eigenvectors are shared such that, with the spectral decomposition of L(X                                   ) in Equation (13),\n                                                 W(X      ) =    |X|  (1 \u2212   \u03bbi) \u00b7 viv\u22a4   i .\n                                                                i=1\nTherefore, the K eigenvalues of W(X                   ) of maximal modulus correspond exactly to the K eigenvalues\nof L(X     ) that are closest to 0,1the eigenvalues whose eigenspaces are relevant for spectral clustering.\nRecalling FX \u225c          D (X     ) 2 f (X   ), we can rewrite Equation (1) as\n                                                R (f) =       W(X       ) \u2212   FX F\u22a4   X   2F .\nIt follows directly from the Eckart-Young-Mirsky theorem [Eckart and Young, 1936] that Range (FX )\nis the subspace spanned by the eigenvectors VK = [v1, . . . , vK] \u2208                                   R|X|\u00d7K associated with the\nmaximum K eigenvalues of W(X                      ):                                     1\n                         FL(X) =          f \u2208    F   \u2203  Z \u2208    RK\u00d7K s.t. D (X          ) 2 f (X    ) = VKZ          .\nProof of Theorem 4.1. Recall FX \u225c                       D (X     )1/2 f (X     ), YX \u225c          D (X    )1/2 \u2212\u2192\n                                                                                                              y\u2217(X    ), and let yk \u2208\n[0, 1]|X| be the k-th column of YX . Leveraging Lemma C.1, we have for any f \u2208                                         F,\n                        P (M(f)) \u22642 max               \u03b22\u03b32 , 1        min                            F\n                                                                   Z\u2208RK\u00d7K \u2225YX \u2212             FX Z\u22252\n                                        =2 max        \u03b22\u03b32 , 1           min                   \u2225yk \u2212     FX zk\u22252    2 .\n                                                                   {zk\u2208RK}k\u2208[K]       k\u2208[K]\nRecall that the orthonormal eigenvectors                     vi \u2208    R|X| i\u2208[|X|] associated with eigenvalues 0 = \u03bb1 \u2264\n\u00b7 \u00b7 \u00b7 \u2264   \u03bb|X| of L (X        ) form a basis of R|X|. For every k \u2208                        [K], (yk \u2212        FX zk) \u2208        R|X| can be\ndecomposed as the linear combination of {vi}i\u2208[|X|] such that\n                                        \u2225yk \u2212     FX zk\u22252    2 =    |X|   v\u22a4 i (yk \u2212      FX zk)     2\n                                                                   i=1\nMeanwhile, Claim C.3 implies that, for f \u2208                       FL(X), with VK = [v1, . . . , vK] \u2208                  R|X|\u00d7K, we have\nRange (FX ) = Range (VK). That is, v\u22a4                     i FX zk = 0 for all i > K, and by taking\n                                                                            K\n                        zk =       V\u22a4 KFX      \u22121 V\u22a4   Kyk        \u21d2        i=1    v\u22a4i (yk \u2212      FX zk)     2 = 0.\nOverall, for every k \u2208           [K], there exists zk \u2208          RK such that\n           \u2225yk \u2212     FX zk\u22252    2 =      |X|      v\u22a4i yk    2 \u2264        1        |X|    \u03bbi \u00b7    v\u22a4i yk   2 \u2264     y\u22a4k L (X    ) yk   ,\n                                      i=K+1                        \u03bbK+1      i=K+1                                   \u03bbK+1\n                                                                      22", "md": "Claim C.3. For all \\( f \\in FL(X) \\) minimizing Equation (1), Range \\( D(X)2f(X) \\) = span {v1, . . . , vK}.\n\nProof of Claim C.3. Since \\( 0 \\preceq L(X) \\preceq 2I \\), we have \\( -I \\preceq W(X) \\preceq I \\). Meanwhile, let \\( k_{\\psi}(X, X') \\in \\mathbb{R}^{|X| \\times |X|} \\) be the population kernel matrix such that \\( k_{\\psi}(X, X')_{xx'} = k_{\\psi}(x, x') \\). Then by definition, \\( k_{\\psi}(X, X') = D(X)^{-1}W(X)D(X)^{-1} \\), and therefore\n\\[ I \\succeq W(X) = D(X)^{-1}2W(X)D(X)^{-1}2 = D(X)^{2}1k_{\\psi}(X, X)D(X)^{2}1 \\succeq 0. \\]\nThat is, the eigenvalues of \\( L(X) \\) and those of \\( W(X) \\) satisfy\n\\[ \\lambda_i \\triangleq \\lambda_i(L(X)) = 1 - \\lambda_{|X| - i + 1}^{W(X)} \\quad \\forall i \\in [|X|], \\]\nwhile the eigenvectors are shared such that, with the spectral decomposition of \\( L(X) \\) in Equation (13),\n\\[ W(X) = \\sum_{i=1}^{|X|} (1 - \\lambda_i) \\cdot v_iv_i^{\\top}. \\]\nTherefore, the K eigenvalues of \\( W(X) \\) of maximal modulus correspond exactly to the K eigenvalues of \\( L(X) \\) that are closest to 0, the eigenvalues whose eigenspaces are relevant for spectral clustering. Recalling \\( FX \\triangleq D(X)2f(X) \\), we can rewrite Equation (1) as\n\\[ R(f) = W(X) - FXF^{\\top}2F. \\]\nIt follows directly from the Eckart-Young-Mirsky theorem [Eckart and Young, 1936] that Range \\( FX \\) is the subspace spanned by the eigenvectors \\( VK = [v1, . . . , vK] \\in \\mathbb{R}^{|X| \\times K} \\) associated with the maximum K eigenvalues of \\( W(X) \\):\n\\[ FL(X) = \\{ f \\in F \\mid \\exists Z \\in \\mathbb{R}^{K \\times K} \\text{ s.t. } D(X)2f(X) = VKZ \\}.\n\nProof of Theorem 4.1. Recall \\( FX \\triangleq D(X)^{1/2}f(X) \\), \\( YX \\triangleq D(X)^{1/2} \\rightarrow y^*(X) \\), and let \\( y_k \\in [0, 1]^{|X|} \\) be the k-th column of \\( YX \\). Leveraging Lemma C.1, we have for any \\( f \\in F \\),\n\\[ P(M(f)) \\leq 2 \\max \\{ \\beta^2\\gamma^2, 1 \\} \\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_{2}^{2} = 2 \\max \\{ \\beta^2\\gamma^2, 1 \\} \\min_{\\{z_k\\}_{k\\in[K]}} \\|y_k - FXz_k\\|_{2}^{2}. \\]\nRecall that the orthonormal eigenvectors \\( v_i \\in \\mathbb{R}^{|X|} \\) associated with eigenvalues \\( 0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\) of \\( L(X) \\) form a basis of \\( \\mathbb{R}^{|X|} \\). For every \\( k \\in [K] \\), \\( (y_k - FXz_k) \\in \\mathbb{R}^{|X|} \\) can be decomposed as the linear combination of \\( \\{v_i\\}_{i\\in[|X|]} \\) such that\n\\[ \\|y_k - FXz_k\\|_{2}^{2} = \\sum_{i=1}^{|X|} |X| \\, v_i^{\\top}(y_k - FXz_k)^{2}. \\]\nMeanwhile, Claim C.3 implies that, for \\( f \\in FL(X) \\), with \\( VK = [v1, . . . , vK] \\in \\mathbb{R}^{|X| \\times K} \\), we have Range \\( FX \\) = Range \\( VK \\). That is, \\( v_i^{\\top}FXz_k = 0 \\) for all \\( i > K \\), and by taking\n\\[ z_k = (V_K^{\\top}FX)^{-1}V_K^{\\top}Ky_k \\Rightarrow \\sum_{i=1}^{K} v_i^{\\top}(y_k - FXz_k)^{2} = 0. \\]\nOverall, for every \\( k \\in [K] \\), there exists \\( z_k \\in \\mathbb{R}^K \\) such that\n\\[ \\|y_k - FXz_k\\|_{2}^{2} = \\sum_{i=1}^{|X|} |X| \\, v_i^{\\top}y_k^{2} \\leq \\frac{1}{\\lambda_{K+1}} \\sum_{i=K+1}^{|X|} \\lambda_i \\, v_i^{\\top}y_k^{2} \\leq y_k^{\\top}L(X)y_k, \\]\nwhere the last inequality follows from the fact that...", "images": [], "items": [{"type": "text", "value": "Claim C.3. For all \\( f \\in FL(X) \\) minimizing Equation (1), Range \\( D(X)2f(X) \\) = span {v1, . . . , vK}.\n\nProof of Claim C.3. Since \\( 0 \\preceq L(X) \\preceq 2I \\), we have \\( -I \\preceq W(X) \\preceq I \\). Meanwhile, let \\( k_{\\psi}(X, X') \\in \\mathbb{R}^{|X| \\times |X|} \\) be the population kernel matrix such that \\( k_{\\psi}(X, X')_{xx'} = k_{\\psi}(x, x') \\). Then by definition, \\( k_{\\psi}(X, X') = D(X)^{-1}W(X)D(X)^{-1} \\), and therefore\n\\[ I \\succeq W(X) = D(X)^{-1}2W(X)D(X)^{-1}2 = D(X)^{2}1k_{\\psi}(X, X)D(X)^{2}1 \\succeq 0. \\]\nThat is, the eigenvalues of \\( L(X) \\) and those of \\( W(X) \\) satisfy\n\\[ \\lambda_i \\triangleq \\lambda_i(L(X)) = 1 - \\lambda_{|X| - i + 1}^{W(X)} \\quad \\forall i \\in [|X|], \\]\nwhile the eigenvectors are shared such that, with the spectral decomposition of \\( L(X) \\) in Equation (13),\n\\[ W(X) = \\sum_{i=1}^{|X|} (1 - \\lambda_i) \\cdot v_iv_i^{\\top}. \\]\nTherefore, the K eigenvalues of \\( W(X) \\) of maximal modulus correspond exactly to the K eigenvalues of \\( L(X) \\) that are closest to 0, the eigenvalues whose eigenspaces are relevant for spectral clustering. Recalling \\( FX \\triangleq D(X)2f(X) \\), we can rewrite Equation (1) as\n\\[ R(f) = W(X) - FXF^{\\top}2F. \\]\nIt follows directly from the Eckart-Young-Mirsky theorem [Eckart and Young, 1936] that Range \\( FX \\) is the subspace spanned by the eigenvectors \\( VK = [v1, . . . , vK] \\in \\mathbb{R}^{|X| \\times K} \\) associated with the maximum K eigenvalues of \\( W(X) \\):\n\\[ FL(X) = \\{ f \\in F \\mid \\exists Z \\in \\mathbb{R}^{K \\times K} \\text{ s.t. } D(X)2f(X) = VKZ \\}.\n\nProof of Theorem 4.1. Recall \\( FX \\triangleq D(X)^{1/2}f(X) \\), \\( YX \\triangleq D(X)^{1/2} \\rightarrow y^*(X) \\), and let \\( y_k \\in [0, 1]^{|X|} \\) be the k-th column of \\( YX \\). Leveraging Lemma C.1, we have for any \\( f \\in F \\),\n\\[ P(M(f)) \\leq 2 \\max \\{ \\beta^2\\gamma^2, 1 \\} \\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_{2}^{2} = 2 \\max \\{ \\beta^2\\gamma^2, 1 \\} \\min_{\\{z_k\\}_{k\\in[K]}} \\|y_k - FXz_k\\|_{2}^{2}. \\]\nRecall that the orthonormal eigenvectors \\( v_i \\in \\mathbb{R}^{|X|} \\) associated with eigenvalues \\( 0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\) of \\( L(X) \\) form a basis of \\( \\mathbb{R}^{|X|} \\). For every \\( k \\in [K] \\), \\( (y_k - FXz_k) \\in \\mathbb{R}^{|X|} \\) can be decomposed as the linear combination of \\( \\{v_i\\}_{i\\in[|X|]} \\) such that\n\\[ \\|y_k - FXz_k\\|_{2}^{2} = \\sum_{i=1}^{|X|} |X| \\, v_i^{\\top}(y_k - FXz_k)^{2}. \\]\nMeanwhile, Claim C.3 implies that, for \\( f \\in FL(X) \\), with \\( VK = [v1, . . . , vK] \\in \\mathbb{R}^{|X| \\times K} \\), we have Range \\( FX \\) = Range \\( VK \\). That is, \\( v_i^{\\top}FXz_k = 0 \\) for all \\( i > K \\), and by taking\n\\[ z_k = (V_K^{\\top}FX)^{-1}V_K^{\\top}Ky_k \\Rightarrow \\sum_{i=1}^{K} v_i^{\\top}(y_k - FXz_k)^{2} = 0. \\]\nOverall, for every \\( k \\in [K] \\), there exists \\( z_k \\in \\mathbb{R}^K \\) such that\n\\[ \\|y_k - FXz_k\\|_{2}^{2} = \\sum_{i=1}^{|X|} |X| \\, v_i^{\\top}y_k^{2} \\leq \\frac{1}{\\lambda_{K+1}} \\sum_{i=K+1}^{|X|} \\lambda_i \\, v_i^{\\top}y_k^{2} \\leq y_k^{\\top}L(X)y_k, \\]\nwhere the last inequality follows from the fact that...", "md": "Claim C.3. For all \\( f \\in FL(X) \\) minimizing Equation (1), Range \\( D(X)2f(X) \\) = span {v1, . . . , vK}.\n\nProof of Claim C.3. Since \\( 0 \\preceq L(X) \\preceq 2I \\), we have \\( -I \\preceq W(X) \\preceq I \\). Meanwhile, let \\( k_{\\psi}(X, X') \\in \\mathbb{R}^{|X| \\times |X|} \\) be the population kernel matrix such that \\( k_{\\psi}(X, X')_{xx'} = k_{\\psi}(x, x') \\). Then by definition, \\( k_{\\psi}(X, X') = D(X)^{-1}W(X)D(X)^{-1} \\), and therefore\n\\[ I \\succeq W(X) = D(X)^{-1}2W(X)D(X)^{-1}2 = D(X)^{2}1k_{\\psi}(X, X)D(X)^{2}1 \\succeq 0. \\]\nThat is, the eigenvalues of \\( L(X) \\) and those of \\( W(X) \\) satisfy\n\\[ \\lambda_i \\triangleq \\lambda_i(L(X)) = 1 - \\lambda_{|X| - i + 1}^{W(X)} \\quad \\forall i \\in [|X|], \\]\nwhile the eigenvectors are shared such that, with the spectral decomposition of \\( L(X) \\) in Equation (13),\n\\[ W(X) = \\sum_{i=1}^{|X|} (1 - \\lambda_i) \\cdot v_iv_i^{\\top}. \\]\nTherefore, the K eigenvalues of \\( W(X) \\) of maximal modulus correspond exactly to the K eigenvalues of \\( L(X) \\) that are closest to 0, the eigenvalues whose eigenspaces are relevant for spectral clustering. Recalling \\( FX \\triangleq D(X)2f(X) \\), we can rewrite Equation (1) as\n\\[ R(f) = W(X) - FXF^{\\top}2F. \\]\nIt follows directly from the Eckart-Young-Mirsky theorem [Eckart and Young, 1936] that Range \\( FX \\) is the subspace spanned by the eigenvectors \\( VK = [v1, . . . , vK] \\in \\mathbb{R}^{|X| \\times K} \\) associated with the maximum K eigenvalues of \\( W(X) \\):\n\\[ FL(X) = \\{ f \\in F \\mid \\exists Z \\in \\mathbb{R}^{K \\times K} \\text{ s.t. } D(X)2f(X) = VKZ \\}.\n\nProof of Theorem 4.1. Recall \\( FX \\triangleq D(X)^{1/2}f(X) \\), \\( YX \\triangleq D(X)^{1/2} \\rightarrow y^*(X) \\), and let \\( y_k \\in [0, 1]^{|X|} \\) be the k-th column of \\( YX \\). Leveraging Lemma C.1, we have for any \\( f \\in F \\),\n\\[ P(M(f)) \\leq 2 \\max \\{ \\beta^2\\gamma^2, 1 \\} \\min_{Z \\in \\mathbb{R}^{K \\times K}} \\|YX - FXZ\\|_{2}^{2} = 2 \\max \\{ \\beta^2\\gamma^2, 1 \\} \\min_{\\{z_k\\}_{k\\in[K]}} \\|y_k - FXz_k\\|_{2}^{2}. \\]\nRecall that the orthonormal eigenvectors \\( v_i \\in \\mathbb{R}^{|X|} \\) associated with eigenvalues \\( 0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\) of \\( L(X) \\) form a basis of \\( \\mathbb{R}^{|X|} \\). For every \\( k \\in [K] \\), \\( (y_k - FXz_k) \\in \\mathbb{R}^{|X|} \\) can be decomposed as the linear combination of \\( \\{v_i\\}_{i\\in[|X|]} \\) such that\n\\[ \\|y_k - FXz_k\\|_{2}^{2} = \\sum_{i=1}^{|X|} |X| \\, v_i^{\\top}(y_k - FXz_k)^{2}. \\]\nMeanwhile, Claim C.3 implies that, for \\( f \\in FL(X) \\), with \\( VK = [v1, . . . , vK] \\in \\mathbb{R}^{|X| \\times K} \\), we have Range \\( FX \\) = Range \\( VK \\). That is, \\( v_i^{\\top}FXz_k = 0 \\) for all \\( i > K \\), and by taking\n\\[ z_k = (V_K^{\\top}FX)^{-1}V_K^{\\top}Ky_k \\Rightarrow \\sum_{i=1}^{K} v_i^{\\top}(y_k - FXz_k)^{2} = 0. \\]\nOverall, for every \\( k \\in [K] \\), there exists \\( z_k \\in \\mathbb{R}^K \\) such that\n\\[ \\|y_k - FXz_k\\|_{2}^{2} = \\sum_{i=1}^{|X|} |X| \\, v_i^{\\top}y_k^{2} \\leq \\frac{1}{\\lambda_{K+1}} \\sum_{i=K+1}^{|X|} \\lambda_i \\, v_i^{\\top}y_k^{2} \\leq y_k^{\\top}L(X)y_k, \\]\nwhere the last inequality follows from the fact that..."}]}, {"page": 23, "text": " and Lemma C.2 implies that\n                              min                  \u2225yk \u2212      FX zk\u22252                  y\u22a4 k L (X    ) yk   =       \u03b1    ,\n                       {zk\u2208RK}k\u2208[K]        k\u2208[K]                        2 \u2264    k\u2208[K]        \u03bbK+1               \u03bbK+1\n which completes the proof.\n C.2      Limitation of Spectral Clustering Alone\n To ground the discussion in Remark 4.1, here, we raise a toy counter-example where spectral\n clustering alone fails to satisfy Assumption 4.2 and suffers from a large clustering error. Intuitively,\n such a pitfall of spectral clustering is caused by a suboptimal choice of basis for the predictions f(X                                       )\n in Equation (1) due to the inherited rotation invariance of the minimizers of R(f).\n Example C.1 (Pitfall of spectral clustering alone). We consider a binary classification problem with\n two balanced ground truth classes {X1, X2} perfectly partitioned in GX as disconnected components.\nThen, when the prediction yf (x) \u225c                     argmaxk\u2208[K] f (x)k is made with a uniformly random break\n                                                                                    1\n of ties, Equation (1) alone suffers from \u00b5                       FL(X)        \u2265    4 in expectation, due to the violation of\nAssumption 4.2.\n Nevertheless, with as few as one labeled sample per class {(xi, i) | i = 1, 2}, weak supervision via\n the cross-entropy loss\n                                    min         LCE (f) \u225c        \u2212    2   log (softmax (f (xi))i)\n                                 f\u2208FL(X)                             i=1\n facilitates satisfaction of Assumption 4.2 with a provably large margin \u03b3. For instance, assuming\n \u2225f(x)\u22252 \u2264        \u2225f(S)\u22252 \u2264        \u03b2 and wx = 1/|X            | for all x \u2208      X (simplified for illustration purposes), for\n any f \u2208     FL(X) under weak supervision such that\n                       f \u2208      f \u2208   FL(X)        log    1 + e\u2212    \u221a  2\u03b2    \u2264   LCE(f) < log           1 + e\u2212\u03b2          ,\nAssumption 4.2 is satisfied with\n                              \u03b3 \u2265    \u2212  log    e LCE(f) \u2212      1    \u2212      2\u03b22 \u2212     log     eLCE(f) \u2212      1  2  .\n In particular, with the minimum feasible cross-entropy loss                            LCE(f) = log            1 + e\u2212    \u221a2\u03b2     , we have\n \u03b3 \u2265    \u221a  2\u03b2.\nWe remark that with DAC regularizations like FixMatch [Sohn et al., 2020] and MixMatch [Berthelot\n et al., 2019], model predictions are matched against pseudo-labels that are gauged based on data\n augmentations, usually via the cross-entropy loss. Therefore, DAC has analogous effects on the\n margins of the learned prediction functions f as the additional supervision described above.\nMoreover, when coupling Equation (1) with the conventional KD (i.e., feature matching), the learned\n prediction functions f inherit the boundedness and margin of the teacher model, which are presumably\n satisfactory.\n Rationale for Example C.1. Recall that \u03bb1 \u2264                       \u00b7 \u00b7 \u00b7 \u2264  \u03bb|X| are the eigenvalues of the graph Laplacian\n L (X   ) = I \u2212       W (X     ), and let       v1, \u00b7 \u00b7 \u00b7 , v|X|       be the associated normalized eigenvectors. For a\n binary classification with {X1, X2} perfectly separated in GX , we can write \u03bb1 = \u03bb2 = 0, while                          1\n v1, v2 are the scaled identity vectors of X1, X2, respectively, such that vi(x) =                                     \u221a  |Xi|1 {x \u2208       Xi}\n for i = 1, 2.\n By denoting V2 = [v1, v2] \u2208                R|X|\u00d72, the Eckart-Young-Mirsky theorem [Eckart and Young, 1936]\n implies that, for any orthogonal matrix Q = [q1; q2] \u2208                          R2\u00d72,\n                       f (X   ) = D (X       )\u22121 2 V2 diag (1 \u2212         \u03bb1, 1 \u2212    \u03bb2)2 Q1= D (X           )\u22121 2 V2Q\n                                                                                        1\n is a minimizer of R(f) in Equation (1). That is, f(x) =                           \u221a  wx|Xi|qi for x \u2208         Xi, i = 1, 2.\n                                                                       23", "md": "and Lemma C.2 implies that\n\n$$\n\\min_{\\{z_k\\}_{k\\in[K]}} \\left\\| y_k - FX z_k \\right\\|_2 \\quad y_k^\\top L(X) y_k = \\frac{\\alpha}{\\lambda_{K+1}}\n$$\nwhich completes the proof.\n\nC.2 Limitation of Spectral Clustering Alone\n\nTo ground the discussion in Remark 4.1, here, we raise a toy counter-example where spectral clustering alone fails to satisfy Assumption 4.2 and suffers from a large clustering error. Intuitively, such a pitfall of spectral clustering is caused by a suboptimal choice of basis for the predictions \\( f(X) \\) in Equation (1) due to the inherited rotation invariance of the minimizers of \\( R(f) \\).\n\nExample C.1 (Pitfall of spectral clustering alone). We consider a binary classification problem with two balanced ground truth classes \\(\\{X_1, X_2\\}\\) perfectly partitioned in \\( GX \\) as disconnected components. Then, when the prediction \\( y_f(x) \\triangleq \\text{argmax}_{k\\in[K]} f(x)_k \\) is made with a uniformly random break of ties, Equation (1) alone suffers from \\( \\mu \\geq 4 \\) in expectation, due to the violation of Assumption 4.2.\n\nNevertheless, with as few as one labeled sample per class \\(\\{(x_i, i) | i = 1, 2\\}\\), weak supervision via the cross-entropy loss\n\n$$\n\\min_{f\\in FL(X)} L_{CE}(f) \\triangleq -\\frac{1}{2} \\sum_{i=1}^2 \\log(\\text{softmax}(f(x_i))_i)\n$$\nfacilitates satisfaction of Assumption 4.2 with a provably large margin \\( \\gamma \\). For instance, assuming \\( \\|f(x)\\|_2 \\leq \\|f(S)\\|_2 \\leq \\beta \\) and \\( w_x = 1/|X| \\) for all \\( x\\in X \\) (simplified for illustration purposes), for any \\( f\\in FL(X) \\) under weak supervision such that\n\n$$\nf \\in f \\in FL(X) \\quad \\log(1 + e^{-\\sqrt{2\\beta}}) \\leq L_{CE}(f) < \\log(1 + e^{-\\beta}),\n$$\nAssumption 4.2 is satisfied with\n\n$$\n\\gamma \\geq -\\log(e L_{CE}(f) - 1 - 2\\beta^2 - \\log(e L_{CE}(f) - 1)^2.\n$$\nIn particular, with the minimum feasible cross-entropy loss \\( L_{CE}(f) = \\log(1 + e^{-\\sqrt{2\\beta}}) \\), we have \\( \\gamma \\geq \\sqrt{2\\beta} \\).\n\nWe remark that with DAC regularizations like FixMatch [Sohn et al., 2020] and MixMatch [Berthelot et al., 2019], model predictions are matched against pseudo-labels that are gauged based on data augmentations, usually via the cross-entropy loss. Therefore, DAC has analogous effects on the margins of the learned prediction functions \\( f \\) as the additional supervision described above.\n\nMoreover, when coupling Equation (1) with the conventional KD (i.e., feature matching), the learned prediction functions \\( f \\) inherit the boundedness and margin of the teacher model, which are presumably satisfactory.\n\nRationale for Example C.1. Recall that \\( \\lambda_1 \\leq \\ldots \\leq \\lambda|X| \\) are the eigenvalues of the graph Laplacian \\( L(X) = I - W(X) \\), and let \\( v_1, \\ldots, v|X| \\) be the associated normalized eigenvectors. For a binary classification with \\(\\{X_1, X_2\\}\\) perfectly separated in \\( GX \\), we can write \\( \\lambda_1 = \\lambda_2 = 0 \\), while \\( v_1, v_2 \\) are the scaled identity vectors of \\( X_1, X_2 \\), respectively, such that \\( v_i(x) = \\sqrt{|X_i|} \\mathbb{1} \\{x \\in X_i\\} \\) for \\( i = 1, 2 \\).\n\nBy denoting \\( V_2 = [v_1, v_2] \\in \\mathbb{R}^{|X|\\times 2} \\), the Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that, for any orthogonal matrix \\( Q = [q_1; q_2] \\in \\mathbb{R}^{2\\times 2} \\),\n\n$$\nf(X) = D(X)^{-\\frac{1}{2}} V_2 \\text{diag} \\left(1 - \\lambda_1, 1 - \\lambda_2\\right)^2 Q1= D(X)^{-\\frac{1}{2}} V_2Q\n$$\nis a minimizer of \\( R(f) \\) in Equation (1). That is, \\( f(x) = \\sqrt{w_x|X_i|} q_i \\) for \\( x \\in X_i, i = 1, 2 \\).", "images": [], "items": [{"type": "text", "value": "and Lemma C.2 implies that\n\n$$\n\\min_{\\{z_k\\}_{k\\in[K]}} \\left\\| y_k - FX z_k \\right\\|_2 \\quad y_k^\\top L(X) y_k = \\frac{\\alpha}{\\lambda_{K+1}}\n$$\nwhich completes the proof.\n\nC.2 Limitation of Spectral Clustering Alone\n\nTo ground the discussion in Remark 4.1, here, we raise a toy counter-example where spectral clustering alone fails to satisfy Assumption 4.2 and suffers from a large clustering error. Intuitively, such a pitfall of spectral clustering is caused by a suboptimal choice of basis for the predictions \\( f(X) \\) in Equation (1) due to the inherited rotation invariance of the minimizers of \\( R(f) \\).\n\nExample C.1 (Pitfall of spectral clustering alone). We consider a binary classification problem with two balanced ground truth classes \\(\\{X_1, X_2\\}\\) perfectly partitioned in \\( GX \\) as disconnected components. Then, when the prediction \\( y_f(x) \\triangleq \\text{argmax}_{k\\in[K]} f(x)_k \\) is made with a uniformly random break of ties, Equation (1) alone suffers from \\( \\mu \\geq 4 \\) in expectation, due to the violation of Assumption 4.2.\n\nNevertheless, with as few as one labeled sample per class \\(\\{(x_i, i) | i = 1, 2\\}\\), weak supervision via the cross-entropy loss\n\n$$\n\\min_{f\\in FL(X)} L_{CE}(f) \\triangleq -\\frac{1}{2} \\sum_{i=1}^2 \\log(\\text{softmax}(f(x_i))_i)\n$$\nfacilitates satisfaction of Assumption 4.2 with a provably large margin \\( \\gamma \\). For instance, assuming \\( \\|f(x)\\|_2 \\leq \\|f(S)\\|_2 \\leq \\beta \\) and \\( w_x = 1/|X| \\) for all \\( x\\in X \\) (simplified for illustration purposes), for any \\( f\\in FL(X) \\) under weak supervision such that\n\n$$\nf \\in f \\in FL(X) \\quad \\log(1 + e^{-\\sqrt{2\\beta}}) \\leq L_{CE}(f) < \\log(1 + e^{-\\beta}),\n$$\nAssumption 4.2 is satisfied with\n\n$$\n\\gamma \\geq -\\log(e L_{CE}(f) - 1 - 2\\beta^2 - \\log(e L_{CE}(f) - 1)^2.\n$$\nIn particular, with the minimum feasible cross-entropy loss \\( L_{CE}(f) = \\log(1 + e^{-\\sqrt{2\\beta}}) \\), we have \\( \\gamma \\geq \\sqrt{2\\beta} \\).\n\nWe remark that with DAC regularizations like FixMatch [Sohn et al., 2020] and MixMatch [Berthelot et al., 2019], model predictions are matched against pseudo-labels that are gauged based on data augmentations, usually via the cross-entropy loss. Therefore, DAC has analogous effects on the margins of the learned prediction functions \\( f \\) as the additional supervision described above.\n\nMoreover, when coupling Equation (1) with the conventional KD (i.e., feature matching), the learned prediction functions \\( f \\) inherit the boundedness and margin of the teacher model, which are presumably satisfactory.\n\nRationale for Example C.1. Recall that \\( \\lambda_1 \\leq \\ldots \\leq \\lambda|X| \\) are the eigenvalues of the graph Laplacian \\( L(X) = I - W(X) \\), and let \\( v_1, \\ldots, v|X| \\) be the associated normalized eigenvectors. For a binary classification with \\(\\{X_1, X_2\\}\\) perfectly separated in \\( GX \\), we can write \\( \\lambda_1 = \\lambda_2 = 0 \\), while \\( v_1, v_2 \\) are the scaled identity vectors of \\( X_1, X_2 \\), respectively, such that \\( v_i(x) = \\sqrt{|X_i|} \\mathbb{1} \\{x \\in X_i\\} \\) for \\( i = 1, 2 \\).\n\nBy denoting \\( V_2 = [v_1, v_2] \\in \\mathbb{R}^{|X|\\times 2} \\), the Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that, for any orthogonal matrix \\( Q = [q_1; q_2] \\in \\mathbb{R}^{2\\times 2} \\),\n\n$$\nf(X) = D(X)^{-\\frac{1}{2}} V_2 \\text{diag} \\left(1 - \\lambda_1, 1 - \\lambda_2\\right)^2 Q1= D(X)^{-\\frac{1}{2}} V_2Q\n$$\nis a minimizer of \\( R(f) \\) in Equation (1). That is, \\( f(x) = \\sqrt{w_x|X_i|} q_i \\) for \\( x \\in X_i, i = 1, 2 \\).", "md": "and Lemma C.2 implies that\n\n$$\n\\min_{\\{z_k\\}_{k\\in[K]}} \\left\\| y_k - FX z_k \\right\\|_2 \\quad y_k^\\top L(X) y_k = \\frac{\\alpha}{\\lambda_{K+1}}\n$$\nwhich completes the proof.\n\nC.2 Limitation of Spectral Clustering Alone\n\nTo ground the discussion in Remark 4.1, here, we raise a toy counter-example where spectral clustering alone fails to satisfy Assumption 4.2 and suffers from a large clustering error. Intuitively, such a pitfall of spectral clustering is caused by a suboptimal choice of basis for the predictions \\( f(X) \\) in Equation (1) due to the inherited rotation invariance of the minimizers of \\( R(f) \\).\n\nExample C.1 (Pitfall of spectral clustering alone). We consider a binary classification problem with two balanced ground truth classes \\(\\{X_1, X_2\\}\\) perfectly partitioned in \\( GX \\) as disconnected components. Then, when the prediction \\( y_f(x) \\triangleq \\text{argmax}_{k\\in[K]} f(x)_k \\) is made with a uniformly random break of ties, Equation (1) alone suffers from \\( \\mu \\geq 4 \\) in expectation, due to the violation of Assumption 4.2.\n\nNevertheless, with as few as one labeled sample per class \\(\\{(x_i, i) | i = 1, 2\\}\\), weak supervision via the cross-entropy loss\n\n$$\n\\min_{f\\in FL(X)} L_{CE}(f) \\triangleq -\\frac{1}{2} \\sum_{i=1}^2 \\log(\\text{softmax}(f(x_i))_i)\n$$\nfacilitates satisfaction of Assumption 4.2 with a provably large margin \\( \\gamma \\). For instance, assuming \\( \\|f(x)\\|_2 \\leq \\|f(S)\\|_2 \\leq \\beta \\) and \\( w_x = 1/|X| \\) for all \\( x\\in X \\) (simplified for illustration purposes), for any \\( f\\in FL(X) \\) under weak supervision such that\n\n$$\nf \\in f \\in FL(X) \\quad \\log(1 + e^{-\\sqrt{2\\beta}}) \\leq L_{CE}(f) < \\log(1 + e^{-\\beta}),\n$$\nAssumption 4.2 is satisfied with\n\n$$\n\\gamma \\geq -\\log(e L_{CE}(f) - 1 - 2\\beta^2 - \\log(e L_{CE}(f) - 1)^2.\n$$\nIn particular, with the minimum feasible cross-entropy loss \\( L_{CE}(f) = \\log(1 + e^{-\\sqrt{2\\beta}}) \\), we have \\( \\gamma \\geq \\sqrt{2\\beta} \\).\n\nWe remark that with DAC regularizations like FixMatch [Sohn et al., 2020] and MixMatch [Berthelot et al., 2019], model predictions are matched against pseudo-labels that are gauged based on data augmentations, usually via the cross-entropy loss. Therefore, DAC has analogous effects on the margins of the learned prediction functions \\( f \\) as the additional supervision described above.\n\nMoreover, when coupling Equation (1) with the conventional KD (i.e., feature matching), the learned prediction functions \\( f \\) inherit the boundedness and margin of the teacher model, which are presumably satisfactory.\n\nRationale for Example C.1. Recall that \\( \\lambda_1 \\leq \\ldots \\leq \\lambda|X| \\) are the eigenvalues of the graph Laplacian \\( L(X) = I - W(X) \\), and let \\( v_1, \\ldots, v|X| \\) be the associated normalized eigenvectors. For a binary classification with \\(\\{X_1, X_2\\}\\) perfectly separated in \\( GX \\), we can write \\( \\lambda_1 = \\lambda_2 = 0 \\), while \\( v_1, v_2 \\) are the scaled identity vectors of \\( X_1, X_2 \\), respectively, such that \\( v_i(x) = \\sqrt{|X_i|} \\mathbb{1} \\{x \\in X_i\\} \\) for \\( i = 1, 2 \\).\n\nBy denoting \\( V_2 = [v_1, v_2] \\in \\mathbb{R}^{|X|\\times 2} \\), the Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that, for any orthogonal matrix \\( Q = [q_1; q_2] \\in \\mathbb{R}^{2\\times 2} \\),\n\n$$\nf(X) = D(X)^{-\\frac{1}{2}} V_2 \\text{diag} \\left(1 - \\lambda_1, 1 - \\lambda_2\\right)^2 Q1= D(X)^{-\\frac{1}{2}} V_2Q\n$$\nis a minimizer of \\( R(f) \\) in Equation (1). That is, \\( f(x) = \\sqrt{w_x|X_i|} q_i \\) for \\( x \\in X_i, i = 1, 2 \\)."}]}, {"page": 24, "text": "Spectral clustering alone.                       However, by taking q1 =                       \u221a1 2 [1; 1] and q2 =              \u221a 12 [\u22121; 1], with a\nuniformly random break of ties, the corresponding f \u2208                                         FL(X) will misclassify half of x \u2208                        X1\nas X2 in expectation. Then, since P (X1) = P (X2) = 1                                         2, by Definition 3.1, we have M(f) =\n{x \u2208     X1 | yf     (x) = 2} such that \u00b5                FL(X)        \u2265    P (M(f)) = 1          4 in expectation.\nSpectral clustering in Example C.1 fails to yield low clustering error due to the potential violation of\nAssumption 4.2. For example, consider any prediction function f \u2208                                          FL(X) described in Example C.1.\n    (i) When argmin              x\u2208X\\M(f) wx \u2286                 X1, recalling sk = argmaxx\u2208X\\M(f) f(x)k and f(x) =\n                1\n          \u221a   wx|Xi|qi for x \u2208           Xi such that, since |X1| = |X2|,                               1\n                 s1 \u2208    X1,            s2 \u2208      argmax         f(x)2 = argmax                      wx |Xi|       = argmin             wx \u2286      X1,\n                                                x\u2208X\\M(f)                        x\u2208X\\M(f)                               x\u2208X\\M(f)\n          there does not exist a skeleton subset S = [s1; s2] as described in Assumption 4.2.\n   (ii) Otherwise, suppose one can find a skeleton subset S = [s1; s2] as described in Assumption 4.2.\n          Without loss of generality, by taking (argmin                          x\u2208X wx)        \u2229  M(f) \u0338= \u2205         in the problem setup, there\n          exists x \u2208       M(f) \u2282         X1 (with yf         (x) = 2) such that wx \u2264                1ws1. Therefore,         1\n                         \u03b31 \u225c     f (s1)1 \u2212                max                                                   \u2212                        \u2264   0\n                                                   x\u2208M(f):yf (x)\u0338=1 f (x)1 \u2264                    2ws1 |X1|                2wx |X1|\n          implies a trivial margin \u03b3 \u2264                  0, which violates \u03b3 > 0 in Assumption 4.2.\nWith additional weak supervision.                             We first observe that for both i \u2208                    [2],\nwhich implies that                               \u2212  log (softmax (f (xi))i) \u2264                   e LCE(f),\n        f (x1)    1 \u2212    f (x1)2 \u2265        \u2212   log     e LCE(f) \u2212        1    ,     f (x2)2 \u2212        f (x2)1 \u2265        \u2212   log     e LCE(f) \u2212        1\nwhere \u2212       log     e LCE(f) \u2212        1    > 0 since        LCE(f) < log              1 + e\u2212\u03b2         < log(2).\nSince we assume \u2225f(x)\u22252 \u2264                      \u03b2 for all x \u2208        X  , f (xi)2   1 + f (xi)2     2 \u2264    \u03b22 for both i = 1, 2, we therefore\nhave\n                   f (x1)    1 \u2212    f (x2)1 \u2265        \u2212   log     e LCE(f) \u2212        1    \u2212       2\u03b22 \u2212      log     e LCE(f) \u2212        1  2  ,\nThis similarly holds for f (x2)                 12 \u2212   f (x1)2.\nRecall now that f(x) =                    \u221a   wx|Xi|qi for x \u2208             Xi, i = 1, 2 and xi \u2208                  Xi for i = 1, 2. Assuming\nwx = 1/ |X         | for all x \u2208       X   , we have    \uf8f1 wx1\n                                           f(x) =       \uf8f2   wx2  wx f (x1) = f (x1) ,                 x \u2208    X1    .\n                                                        \uf8f3        wx f (x2) = f (x2) ,                 x \u2208    X2\nTherefore, \u03b31 \u2265            f (x1)1 \u2212        f (x2)1 and \u03b32 \u2265             f (x2)2 \u2212        f (x1)2, which together imply that\n                                \u03b3 \u2265     \u2212   log     e LCE(f) \u2212       1    \u2212       2\u03b22 \u2212       log     e LCE(f) \u2212       1  2  .\nNow examine the two extremes of log                                 1 + e\u2212     \u221a  2\u03b2      \u2264    LCE(f) < log                1 + e\u2212\u03b2         . When the\ncross-entropy loss achieves its minimum feasible value                                    LCE(f) = log              1 + e\u2212      \u221a2\u03b2     , we have that\n\u03b3 \u2265    \u221a   2\u03b2, whereas the margin can be nearly trivial \u03b3 \u2265                               \u03f5CE with \u03f5CE \u2192              0 when the cross-entropy\nloss is on the larger side             LCE(f) \u2192           log     1 + e\u2212\u03b2 .  24", "md": "# Math Equations\n\nSpectral clustering alone. However, by taking $$q_1 = \\sqrt{\\frac{1}{2}} [1; 1]$$ and $$q_2 = \\sqrt{\\frac{1}{2}} [-1; 1]$$, with a uniformly random break of ties, the corresponding $$f \\in FL(X)$$ will misclassify half of $$x \\in X_1$$ as $$X_2$$ in expectation. Then, since $$P(X_1) = P(X_2) = \\frac{1}{2}$$, by Definition 3.1, we have $$M(f) = \\{x \\in X_1 | y_f(x) = 2\\}$$ such that $$\\mu_{FL(X)} \\geq P(M(f)) = \\frac{1}{4}$$ in expectation.\n\nSpectral clustering in Example C.1 fails to yield low clustering error due to the potential violation of Assumption 4.2. For example, consider any prediction function $$f \\in FL(X)$$ described in Example C.1.\n\n1. When $\\arg\\min_{x \\in X \\backslash M(f)} w_x \\subseteq X_1$, recalling $s_k = \\arg\\max_{x \\in X \\backslash M(f)} f(x)_k$ and $f(x) = \\frac{1}{\\sqrt{w_x |X_i|}}q_i$ for $x \\in X_i$ such that, since $|X_1| = |X_2|$,\n$s_1 \\in X_1, s_2 \\in \\arg\\max f(x)_2 = \\arg\\max w_x |X_i| = \\arg\\min w_x \\subseteq X_1$,\nthere does not exist a skeleton subset $S = [s_1; s_2]$ as described in Assumption 4.2.\n2. Otherwise, suppose one can find a skeleton subset $S = [s_1; s_2]$ as described in Assumption 4.2. Without loss of generality, by taking $(\\arg\\min_{x \\in X} w_x) \\cap M(f) \\neq \\emptyset$ in the problem setup, there exists $x \\in M(f) \\subset X_1$ (with $y_f(x) = 2$) such that $w_x \\leq \\frac{1}{w_{s_1}}$. Therefore,\n$\\gamma_1 \\triangleq f(s_1)_1 - \\max_{x \\in M(f): y_f(x) \\neq 1} f(x)_1 \\leq 2w_{s_1} |X_1| - 2w_x |X_1|$ implies a trivial margin $\\gamma \\leq 0$, which violates $\\gamma > 0$ in Assumption 4.2.\n\nWith additional weak supervision. We first observe that for both $$i \\in [2]$$, which implies that $$-\\log(\\text{softmax}(f(x_i))_i) \\leq e LCE(f)$$,\n$$f(x_1)_1 - f(x_1)_2 \\geq -\\log e LCE(f) - 1$$,\n$$f(x_2)_2 - f(x_2)_1 \\geq -\\log e LCE(f) - 1$$\nwhere $$-\\log e LCE(f) - 1 > 0$$ since $$LCE(f) < \\log(1 + e^{-\\beta}) < \\log(2)$$. Since we assume $$\\|f(x)\\|_2 \\leq \\beta$$ for all $$x \\in X$$, $$f(x_i)_2 \\leq \\frac{1 + f(x_i)_2}{2} \\leq \\beta^2$$ for both $$i = 1, 2$$, we therefore have\n$$f(x_1)_1 - f(x_2)_1 \\geq -\\log e LCE(f) - 1 - 2\\beta^2 - \\log e LCE(f) - 1^2$$,\nThis similarly holds for $$f(x_2)_2 - f(x_1)_2$$.\nRecall now that $$f(x) = \\sqrt{w_x |X_i|}q_i$$ for $$x \\in X_i, i = 1, 2$$ and $$x_i \\in X_i$$ for $$i = 1, 2$$. Assuming $$w_x = \\frac{1}{|X|}$$ for all $$x \\in X$$, we have\n$$f(x) = \\begin{cases} w_{x1} & f(x) = f(x_1), x \\in X_1 \\\\ w_{x2} & f(x) = f(x_2), x \\in X_2 \\end{cases}$$.\nTherefore, $$\\gamma_1 \\geq f(x_1)_1 - f(x_2)_1$$ and $$\\gamma_2 \\geq f(x_2)_2 - f(x_1)_2$$, which together imply that\n$$\\gamma \\geq -\\log e LCE(f) - 1 - 2\\beta^2 - \\log e LCE(f) - 1^2$$.\nNow examine the two extremes of $$\\log(1 + e^{-\\sqrt{2}\\beta}) \\leq LCE(f) < \\log(1 + e^{-\\beta})$$. When the cross-entropy loss achieves its minimum feasible value $$LCE(f) = \\log(1 + e^{-\\sqrt{2}\\beta})$$, we have that\n$$\\gamma \\geq \\sqrt{2}\\beta$$, whereas the margin can be nearly trivial $$\\gamma \\geq \\epsilon_{CE}$$ with $$\\epsilon_{CE} \\rightarrow 0$$ when the cross-entropy loss is on the larger side $$LCE(f) \\rightarrow \\log(1 + e^{-\\beta})$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Spectral clustering alone. However, by taking $$q_1 = \\sqrt{\\frac{1}{2}} [1; 1]$$ and $$q_2 = \\sqrt{\\frac{1}{2}} [-1; 1]$$, with a uniformly random break of ties, the corresponding $$f \\in FL(X)$$ will misclassify half of $$x \\in X_1$$ as $$X_2$$ in expectation. Then, since $$P(X_1) = P(X_2) = \\frac{1}{2}$$, by Definition 3.1, we have $$M(f) = \\{x \\in X_1 | y_f(x) = 2\\}$$ such that $$\\mu_{FL(X)} \\geq P(M(f)) = \\frac{1}{4}$$ in expectation.\n\nSpectral clustering in Example C.1 fails to yield low clustering error due to the potential violation of Assumption 4.2. For example, consider any prediction function $$f \\in FL(X)$$ described in Example C.1.\n\n1. When $\\arg\\min_{x \\in X \\backslash M(f)} w_x \\subseteq X_1$, recalling $s_k = \\arg\\max_{x \\in X \\backslash M(f)} f(x)_k$ and $f(x) = \\frac{1}{\\sqrt{w_x |X_i|}}q_i$ for $x \\in X_i$ such that, since $|X_1| = |X_2|$,\n$s_1 \\in X_1, s_2 \\in \\arg\\max f(x)_2 = \\arg\\max w_x |X_i| = \\arg\\min w_x \\subseteq X_1$,\nthere does not exist a skeleton subset $S = [s_1; s_2]$ as described in Assumption 4.2.\n2. Otherwise, suppose one can find a skeleton subset $S = [s_1; s_2]$ as described in Assumption 4.2. Without loss of generality, by taking $(\\arg\\min_{x \\in X} w_x) \\cap M(f) \\neq \\emptyset$ in the problem setup, there exists $x \\in M(f) \\subset X_1$ (with $y_f(x) = 2$) such that $w_x \\leq \\frac{1}{w_{s_1}}$. Therefore,\n$\\gamma_1 \\triangleq f(s_1)_1 - \\max_{x \\in M(f): y_f(x) \\neq 1} f(x)_1 \\leq 2w_{s_1} |X_1| - 2w_x |X_1|$ implies a trivial margin $\\gamma \\leq 0$, which violates $\\gamma > 0$ in Assumption 4.2.\n\nWith additional weak supervision. We first observe that for both $$i \\in [2]$$, which implies that $$-\\log(\\text{softmax}(f(x_i))_i) \\leq e LCE(f)$$,\n$$f(x_1)_1 - f(x_1)_2 \\geq -\\log e LCE(f) - 1$$,\n$$f(x_2)_2 - f(x_2)_1 \\geq -\\log e LCE(f) - 1$$\nwhere $$-\\log e LCE(f) - 1 > 0$$ since $$LCE(f) < \\log(1 + e^{-\\beta}) < \\log(2)$$. Since we assume $$\\|f(x)\\|_2 \\leq \\beta$$ for all $$x \\in X$$, $$f(x_i)_2 \\leq \\frac{1 + f(x_i)_2}{2} \\leq \\beta^2$$ for both $$i = 1, 2$$, we therefore have\n$$f(x_1)_1 - f(x_2)_1 \\geq -\\log e LCE(f) - 1 - 2\\beta^2 - \\log e LCE(f) - 1^2$$,\nThis similarly holds for $$f(x_2)_2 - f(x_1)_2$$.\nRecall now that $$f(x) = \\sqrt{w_x |X_i|}q_i$$ for $$x \\in X_i, i = 1, 2$$ and $$x_i \\in X_i$$ for $$i = 1, 2$$. Assuming $$w_x = \\frac{1}{|X|}$$ for all $$x \\in X$$, we have\n$$f(x) = \\begin{cases} w_{x1} & f(x) = f(x_1), x \\in X_1 \\\\ w_{x2} & f(x) = f(x_2), x \\in X_2 \\end{cases}$$.\nTherefore, $$\\gamma_1 \\geq f(x_1)_1 - f(x_2)_1$$ and $$\\gamma_2 \\geq f(x_2)_2 - f(x_1)_2$$, which together imply that\n$$\\gamma \\geq -\\log e LCE(f) - 1 - 2\\beta^2 - \\log e LCE(f) - 1^2$$.\nNow examine the two extremes of $$\\log(1 + e^{-\\sqrt{2}\\beta}) \\leq LCE(f) < \\log(1 + e^{-\\beta})$$. When the cross-entropy loss achieves its minimum feasible value $$LCE(f) = \\log(1 + e^{-\\sqrt{2}\\beta})$$, we have that\n$$\\gamma \\geq \\sqrt{2}\\beta$$, whereas the margin can be nearly trivial $$\\gamma \\geq \\epsilon_{CE}$$ with $$\\epsilon_{CE} \\rightarrow 0$$ when the cross-entropy loss is on the larger side $$LCE(f) \\rightarrow \\log(1 + e^{-\\beta})$$.", "md": "Spectral clustering alone. However, by taking $$q_1 = \\sqrt{\\frac{1}{2}} [1; 1]$$ and $$q_2 = \\sqrt{\\frac{1}{2}} [-1; 1]$$, with a uniformly random break of ties, the corresponding $$f \\in FL(X)$$ will misclassify half of $$x \\in X_1$$ as $$X_2$$ in expectation. Then, since $$P(X_1) = P(X_2) = \\frac{1}{2}$$, by Definition 3.1, we have $$M(f) = \\{x \\in X_1 | y_f(x) = 2\\}$$ such that $$\\mu_{FL(X)} \\geq P(M(f)) = \\frac{1}{4}$$ in expectation.\n\nSpectral clustering in Example C.1 fails to yield low clustering error due to the potential violation of Assumption 4.2. For example, consider any prediction function $$f \\in FL(X)$$ described in Example C.1.\n\n1. When $\\arg\\min_{x \\in X \\backslash M(f)} w_x \\subseteq X_1$, recalling $s_k = \\arg\\max_{x \\in X \\backslash M(f)} f(x)_k$ and $f(x) = \\frac{1}{\\sqrt{w_x |X_i|}}q_i$ for $x \\in X_i$ such that, since $|X_1| = |X_2|$,\n$s_1 \\in X_1, s_2 \\in \\arg\\max f(x)_2 = \\arg\\max w_x |X_i| = \\arg\\min w_x \\subseteq X_1$,\nthere does not exist a skeleton subset $S = [s_1; s_2]$ as described in Assumption 4.2.\n2. Otherwise, suppose one can find a skeleton subset $S = [s_1; s_2]$ as described in Assumption 4.2. Without loss of generality, by taking $(\\arg\\min_{x \\in X} w_x) \\cap M(f) \\neq \\emptyset$ in the problem setup, there exists $x \\in M(f) \\subset X_1$ (with $y_f(x) = 2$) such that $w_x \\leq \\frac{1}{w_{s_1}}$. Therefore,\n$\\gamma_1 \\triangleq f(s_1)_1 - \\max_{x \\in M(f): y_f(x) \\neq 1} f(x)_1 \\leq 2w_{s_1} |X_1| - 2w_x |X_1|$ implies a trivial margin $\\gamma \\leq 0$, which violates $\\gamma > 0$ in Assumption 4.2.\n\nWith additional weak supervision. We first observe that for both $$i \\in [2]$$, which implies that $$-\\log(\\text{softmax}(f(x_i))_i) \\leq e LCE(f)$$,\n$$f(x_1)_1 - f(x_1)_2 \\geq -\\log e LCE(f) - 1$$,\n$$f(x_2)_2 - f(x_2)_1 \\geq -\\log e LCE(f) - 1$$\nwhere $$-\\log e LCE(f) - 1 > 0$$ since $$LCE(f) < \\log(1 + e^{-\\beta}) < \\log(2)$$. Since we assume $$\\|f(x)\\|_2 \\leq \\beta$$ for all $$x \\in X$$, $$f(x_i)_2 \\leq \\frac{1 + f(x_i)_2}{2} \\leq \\beta^2$$ for both $$i = 1, 2$$, we therefore have\n$$f(x_1)_1 - f(x_2)_1 \\geq -\\log e LCE(f) - 1 - 2\\beta^2 - \\log e LCE(f) - 1^2$$,\nThis similarly holds for $$f(x_2)_2 - f(x_1)_2$$.\nRecall now that $$f(x) = \\sqrt{w_x |X_i|}q_i$$ for $$x \\in X_i, i = 1, 2$$ and $$x_i \\in X_i$$ for $$i = 1, 2$$. Assuming $$w_x = \\frac{1}{|X|}$$ for all $$x \\in X$$, we have\n$$f(x) = \\begin{cases} w_{x1} & f(x) = f(x_1), x \\in X_1 \\\\ w_{x2} & f(x) = f(x_2), x \\in X_2 \\end{cases}$$.\nTherefore, $$\\gamma_1 \\geq f(x_1)_1 - f(x_2)_1$$ and $$\\gamma_2 \\geq f(x_2)_2 - f(x_1)_2$$, which together imply that\n$$\\gamma \\geq -\\log e LCE(f) - 1 - 2\\beta^2 - \\log e LCE(f) - 1^2$$.\nNow examine the two extremes of $$\\log(1 + e^{-\\sqrt{2}\\beta}) \\leq LCE(f) < \\log(1 + e^{-\\beta})$$. When the cross-entropy loss achieves its minimum feasible value $$LCE(f) = \\log(1 + e^{-\\sqrt{2}\\beta})$$, we have that\n$$\\gamma \\geq \\sqrt{2}\\beta$$, whereas the margin can be nearly trivial $$\\gamma \\geq \\epsilon_{CE}$$ with $$\\epsilon_{CE} \\rightarrow 0$$ when the cross-entropy loss is on the larger side $$LCE(f) \\rightarrow \\log(1 + e^{-\\beta})$$."}]}, {"page": 25, "text": " C.3      Connection with Sparsest k-partition\nWe now identify the connection between the eigenvalue \u03bbK+1 and spectral graph theoretic notions\n of clusteredness of the underlying graph. We recall the notion of sparsest k-partitions [Louis and\n Makarychev, 2014] based on the Dirichlet conductance of subgraphs.\n Definition C.1 (Sparsest k-partition, HaoChen et al. [2021] Definition 3.4, Louis and Makarychev\n [2014] Problem 1.1). Given a weighted undirected graph G = (X                                         , w), for any k \u2208           [|X  |], let\n S =        {Si}i\u2208[k]       \u222ai\u2208[k] Si = X          , Si \u2229     Sj = \u2205, Si \u0338= \u2205            \u2200i \u0338= j, i, j \u2208         [k]    be the set of all\n k-partitions of G. The sparsest k-partition of G is defined as\n                                                  \u03d5k \u225c          min\n                                                           {Si}i\u2208[k]\u2208S max i\u2208[k] \u03d5G(Si),\n where \u03d5G(S) \u225c                x\u2208S       x\u2032 /             /                     is the Dirichlet conductance of S \u2286                 X  .\n                                           \u2208S wxx\u2032               x\u2208S wx\n It is worth mentioning that \u03d5k is a non-decreasing function in k [HaoChen et al., 2021]. In particular,\n \u03d5k = 0 when the graph has at least k disconnected components; whereas \u03bbK+1 > 0 (Assumption 4.1)\n implies that \u03d5K+1 > 0.\n Meanwhile, Louis and Makarychev [2014] unveils the following connection between the sparsest\n k-partition of the graph and the k\u2032th smallest eigenvalue of its graph Laplacian:\n Lemma C.4 (Louis and Makarychev [2014] Proposition 1.2). Given any weighted undirected graph\n G = (X      , w), let 0 = \u03bb1 \u2264         \u00b7 \u00b7 \u00b7 \u2264  \u03bb|X| be the eigenvalues of the normalized graph Laplacian in the\n ascending order. For any k, k\u2032 \u2208               [|X   |], k < k\u2032, there exists a partition {Si}i\u2208[k] \u2208                  S of X such that,\n for all i \u2208    [k],                      \u03d5G (Si) \u2272        poly      k\u2032 \u2212k   k        log (k) \u03bbk\u2032.\n Leveraging the existing result Lemma C.4 [Louis and Makarychev, 2014] from spectral graph theory,\n we have the following corollary.\n Corollary C.5 (Clustering in terms of sparsest k-partitions). Under Assumption 4.1 and Assump-\n tion 4.2 for every f \u2208          FL(X), error of clustering with the population (Equation (1)) satisfies that, if\n \u03b1 > 0 and \u03d5k > 0 for all k \u2208               [K], then\n                            \u00b5   FL(X)       \u2272   max     \u03b22 \u03b32 , 1     \u00b7 poly      K + 1 \u2212k      k     log(k) \u03b1  \u03d52k .\nWe also notice that when \u03b1 = 0, Theorem 4.1 automatically implies that \u00b5                                     FL(X)        = 0. Intuitively,\n Corollary C.5 implies that except for the partition of the K ground truth classes {X                                  }k\u2208[K] (i.e., when\n \u03b1 > 0), GX cannot be partitioned into other K components by removing a sparse set of edges from\n GX .\n Proof of Corollary C.5. Lemma C.4 implies that for any k \u2208                             [K], there exists a partition {Si}i\u2208[k] \u2208\n S of X such that            \u03d5k \u2264     max                               K + 1 \u2212k      k        log (k) \u03bbK+1,\n and therefore                        i\u2208[k] \u03d5G1(Si) \u2272\u2272   poly poly        k            log(k) 1      ,\n                                           \u03bbK+1                    K + 1 \u2212       k               \u03d52k\n which completes the proof when recalling Theorem 4.1.\n D      Proofs and Discussions for Section 4.2\n Here, we show that            RXu(f) shares the same minimizer as the population Laplacian regularizer\n R(f) in expectation (Proposition D.1) and provide generalization bounds for                                   RXu(f) (Theorem 4.2)\n accordingly.\n                                                                       25", "md": "# Document\n\n## C.3 Connection with Sparsest k-partition\n\nWe now identify the connection between the eigenvalue $$\\lambda_{K+1}$$ and spectral graph theoretic notions of clusteredness of the underlying graph. We recall the notion of sparsest k-partitions [Louis and Makarychev, 2014] based on the Dirichlet conductance of subgraphs.\n\nDefinition C.1 (Sparsest k-partition, HaoChen et al. [2021] Definition 3.4, Louis and Makarychev [2014] Problem 1.1): Given a weighted undirected graph $$G = (X, w)$$, for any $$k \\in [|X|]$$, let\n\n$$S = \\{S_i\\}_{i\\in[k]} \\cup_{i\\in[k]} S_i = X, S_i \\cap S_j = \\emptyset, S_i \\neq \\emptyset \\ \\forall i \\neq j, i, j \\in [k]$$ be the set of all k-partitions of G. The sparsest k-partition of G is defined as\n\n$$\\phi_k \\triangleq \\min_{\\{S_i\\}_{i\\in[k]}\\in S} \\max_{i\\in[k]} \\phi_G(S_i),$$\n\nwhere $$\\phi_G(S) \\triangleq \\frac{\\sum_{x\\in S} \\sum_{x'\\in S} w_{xx'}}{\\sum_{x\\in S} w_x}$$ is the Dirichlet conductance of $$S \\subseteq X$$.\n\nIt is worth mentioning that $$\\phi_k$$ is a non-decreasing function in k [HaoChen et al., 2021]. In particular, $$\\phi_k = 0$$ when the graph has at least k disconnected components; whereas $$\\lambda_{K+1} > 0$$ (Assumption 4.1) implies that $$\\phi_{K+1} > 0$$.\n\nMeanwhile, Louis and Makarychev [2014] unveils the following connection between the sparsest k-partition of the graph and the k'th smallest eigenvalue of its graph Laplacian:\n\nLemma C.4 (Louis and Makarychev [2014] Proposition 1.2): Given any weighted undirected graph $$G = (X, w)$$, let $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|}$$ be the eigenvalues of the normalized graph Laplacian in the ascending order. For any $$k, k' \\in [|X|], k < k'$$, there exists a partition $$\\{S_i\\}_{i\\in[k]} \\in S$$ of X such that, for all $$i \\in [k]$$,\n\n$$\\phi_G(S_i) \\lesssim \\text{poly}(k' - k) \\cdot k \\cdot \\log(k) \\cdot \\lambda_{k'}.$$\n\nLeveraging the existing result Lemma C.4 [Louis and Makarychev, 2014] from spectral graph theory, we have the following corollary.\n\nCorollary C.5 (Clustering in terms of sparsest k-partitions): Under Assumption 4.1 and Assumption 4.2 for every $$f \\in FL(X)$$, error of clustering with the population (Equation (1)) satisfies that, if $$\\alpha > 0$$ and $$\\phi_k > 0$$ for all $$k \\in [K]$$, then\n\n$$\\mu_{FL(X)} \\lesssim \\max(\\beta^2 \\gamma^2, 1) \\cdot \\text{poly}(K + 1 - k) \\cdot k \\cdot \\log(k) \\cdot \\alpha \\cdot \\phi_{2k}.$$\n\nWe also notice that when $$\\alpha = 0$$, Theorem 4.1 automatically implies that $$\\mu_{FL(X)} = 0$$. Intuitively, Corollary C.5 implies that except for the partition of the K ground truth classes $$\\{X\\}_{k\\in[K]}$$ (i.e., when $$\\alpha > 0$$), GX cannot be partitioned into other K components by removing a sparse set of edges from GX.\n\nProof of Corollary C.5: Lemma C.4 implies that for any $$k \\in [K]$$, there exists a partition $$\\{S_i\\}_{i\\in[k]} \\in S$$ of X such that $$\\phi_k \\leq \\max(K + 1 - k) \\cdot k \\cdot \\log(k) \\cdot \\lambda_{K+1}$$, and therefore $$\\sum_{i\\in[k]} \\phi_{G1}(S_i) \\lesssim \\text{poly}(\\text{poly}(k) \\cdot \\log(k) \\cdot 1),$$ which completes the proof when recalling Theorem 4.1.\n\n## D Proofs and Discussions for Section 4.2\n\nHere, we show that $$R_Xu(f)$$ shares the same minimizer as the population Laplacian regularizer $$R(f)$$ in expectation (Proposition D.1) and provide generalization bounds for $$R_Xu(f)$$ (Theorem 4.2) accordingly.\n\n25", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "C.3 Connection with Sparsest k-partition", "md": "## C.3 Connection with Sparsest k-partition"}, {"type": "text", "value": "We now identify the connection between the eigenvalue $$\\lambda_{K+1}$$ and spectral graph theoretic notions of clusteredness of the underlying graph. We recall the notion of sparsest k-partitions [Louis and Makarychev, 2014] based on the Dirichlet conductance of subgraphs.\n\nDefinition C.1 (Sparsest k-partition, HaoChen et al. [2021] Definition 3.4, Louis and Makarychev [2014] Problem 1.1): Given a weighted undirected graph $$G = (X, w)$$, for any $$k \\in [|X|]$$, let\n\n$$S = \\{S_i\\}_{i\\in[k]} \\cup_{i\\in[k]} S_i = X, S_i \\cap S_j = \\emptyset, S_i \\neq \\emptyset \\ \\forall i \\neq j, i, j \\in [k]$$ be the set of all k-partitions of G. The sparsest k-partition of G is defined as\n\n$$\\phi_k \\triangleq \\min_{\\{S_i\\}_{i\\in[k]}\\in S} \\max_{i\\in[k]} \\phi_G(S_i),$$\n\nwhere $$\\phi_G(S) \\triangleq \\frac{\\sum_{x\\in S} \\sum_{x'\\in S} w_{xx'}}{\\sum_{x\\in S} w_x}$$ is the Dirichlet conductance of $$S \\subseteq X$$.\n\nIt is worth mentioning that $$\\phi_k$$ is a non-decreasing function in k [HaoChen et al., 2021]. In particular, $$\\phi_k = 0$$ when the graph has at least k disconnected components; whereas $$\\lambda_{K+1} > 0$$ (Assumption 4.1) implies that $$\\phi_{K+1} > 0$$.\n\nMeanwhile, Louis and Makarychev [2014] unveils the following connection between the sparsest k-partition of the graph and the k'th smallest eigenvalue of its graph Laplacian:\n\nLemma C.4 (Louis and Makarychev [2014] Proposition 1.2): Given any weighted undirected graph $$G = (X, w)$$, let $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|}$$ be the eigenvalues of the normalized graph Laplacian in the ascending order. For any $$k, k' \\in [|X|], k < k'$$, there exists a partition $$\\{S_i\\}_{i\\in[k]} \\in S$$ of X such that, for all $$i \\in [k]$$,\n\n$$\\phi_G(S_i) \\lesssim \\text{poly}(k' - k) \\cdot k \\cdot \\log(k) \\cdot \\lambda_{k'}.$$\n\nLeveraging the existing result Lemma C.4 [Louis and Makarychev, 2014] from spectral graph theory, we have the following corollary.\n\nCorollary C.5 (Clustering in terms of sparsest k-partitions): Under Assumption 4.1 and Assumption 4.2 for every $$f \\in FL(X)$$, error of clustering with the population (Equation (1)) satisfies that, if $$\\alpha > 0$$ and $$\\phi_k > 0$$ for all $$k \\in [K]$$, then\n\n$$\\mu_{FL(X)} \\lesssim \\max(\\beta^2 \\gamma^2, 1) \\cdot \\text{poly}(K + 1 - k) \\cdot k \\cdot \\log(k) \\cdot \\alpha \\cdot \\phi_{2k}.$$\n\nWe also notice that when $$\\alpha = 0$$, Theorem 4.1 automatically implies that $$\\mu_{FL(X)} = 0$$. Intuitively, Corollary C.5 implies that except for the partition of the K ground truth classes $$\\{X\\}_{k\\in[K]}$$ (i.e., when $$\\alpha > 0$$), GX cannot be partitioned into other K components by removing a sparse set of edges from GX.\n\nProof of Corollary C.5: Lemma C.4 implies that for any $$k \\in [K]$$, there exists a partition $$\\{S_i\\}_{i\\in[k]} \\in S$$ of X such that $$\\phi_k \\leq \\max(K + 1 - k) \\cdot k \\cdot \\log(k) \\cdot \\lambda_{K+1}$$, and therefore $$\\sum_{i\\in[k]} \\phi_{G1}(S_i) \\lesssim \\text{poly}(\\text{poly}(k) \\cdot \\log(k) \\cdot 1),$$ which completes the proof when recalling Theorem 4.1.", "md": "We now identify the connection between the eigenvalue $$\\lambda_{K+1}$$ and spectral graph theoretic notions of clusteredness of the underlying graph. We recall the notion of sparsest k-partitions [Louis and Makarychev, 2014] based on the Dirichlet conductance of subgraphs.\n\nDefinition C.1 (Sparsest k-partition, HaoChen et al. [2021] Definition 3.4, Louis and Makarychev [2014] Problem 1.1): Given a weighted undirected graph $$G = (X, w)$$, for any $$k \\in [|X|]$$, let\n\n$$S = \\{S_i\\}_{i\\in[k]} \\cup_{i\\in[k]} S_i = X, S_i \\cap S_j = \\emptyset, S_i \\neq \\emptyset \\ \\forall i \\neq j, i, j \\in [k]$$ be the set of all k-partitions of G. The sparsest k-partition of G is defined as\n\n$$\\phi_k \\triangleq \\min_{\\{S_i\\}_{i\\in[k]}\\in S} \\max_{i\\in[k]} \\phi_G(S_i),$$\n\nwhere $$\\phi_G(S) \\triangleq \\frac{\\sum_{x\\in S} \\sum_{x'\\in S} w_{xx'}}{\\sum_{x\\in S} w_x}$$ is the Dirichlet conductance of $$S \\subseteq X$$.\n\nIt is worth mentioning that $$\\phi_k$$ is a non-decreasing function in k [HaoChen et al., 2021]. In particular, $$\\phi_k = 0$$ when the graph has at least k disconnected components; whereas $$\\lambda_{K+1} > 0$$ (Assumption 4.1) implies that $$\\phi_{K+1} > 0$$.\n\nMeanwhile, Louis and Makarychev [2014] unveils the following connection between the sparsest k-partition of the graph and the k'th smallest eigenvalue of its graph Laplacian:\n\nLemma C.4 (Louis and Makarychev [2014] Proposition 1.2): Given any weighted undirected graph $$G = (X, w)$$, let $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|}$$ be the eigenvalues of the normalized graph Laplacian in the ascending order. For any $$k, k' \\in [|X|], k < k'$$, there exists a partition $$\\{S_i\\}_{i\\in[k]} \\in S$$ of X such that, for all $$i \\in [k]$$,\n\n$$\\phi_G(S_i) \\lesssim \\text{poly}(k' - k) \\cdot k \\cdot \\log(k) \\cdot \\lambda_{k'}.$$\n\nLeveraging the existing result Lemma C.4 [Louis and Makarychev, 2014] from spectral graph theory, we have the following corollary.\n\nCorollary C.5 (Clustering in terms of sparsest k-partitions): Under Assumption 4.1 and Assumption 4.2 for every $$f \\in FL(X)$$, error of clustering with the population (Equation (1)) satisfies that, if $$\\alpha > 0$$ and $$\\phi_k > 0$$ for all $$k \\in [K]$$, then\n\n$$\\mu_{FL(X)} \\lesssim \\max(\\beta^2 \\gamma^2, 1) \\cdot \\text{poly}(K + 1 - k) \\cdot k \\cdot \\log(k) \\cdot \\alpha \\cdot \\phi_{2k}.$$\n\nWe also notice that when $$\\alpha = 0$$, Theorem 4.1 automatically implies that $$\\mu_{FL(X)} = 0$$. Intuitively, Corollary C.5 implies that except for the partition of the K ground truth classes $$\\{X\\}_{k\\in[K]}$$ (i.e., when $$\\alpha > 0$$), GX cannot be partitioned into other K components by removing a sparse set of edges from GX.\n\nProof of Corollary C.5: Lemma C.4 implies that for any $$k \\in [K]$$, there exists a partition $$\\{S_i\\}_{i\\in[k]} \\in S$$ of X such that $$\\phi_k \\leq \\max(K + 1 - k) \\cdot k \\cdot \\log(k) \\cdot \\lambda_{K+1}$$, and therefore $$\\sum_{i\\in[k]} \\phi_{G1}(S_i) \\lesssim \\text{poly}(\\text{poly}(k) \\cdot \\log(k) \\cdot 1),$$ which completes the proof when recalling Theorem 4.1."}, {"type": "heading", "lvl": 2, "value": "D Proofs and Discussions for Section 4.2", "md": "## D Proofs and Discussions for Section 4.2"}, {"type": "text", "value": "Here, we show that $$R_Xu(f)$$ shares the same minimizer as the population Laplacian regularizer $$R(f)$$ in expectation (Proposition D.1) and provide generalization bounds for $$R_Xu(f)$$ (Theorem 4.2) accordingly.\n\n25", "md": "Here, we show that $$R_Xu(f)$$ shares the same minimizer as the population Laplacian regularizer $$R(f)$$ in expectation (Proposition D.1) and provide generalization bounds for $$R_Xu(f)$$ (Theorem 4.2) accordingly.\n\n25"}]}, {"page": 26, "text": " D.1     Unbiased Estimations of Population Laplacian\n Proposition D.1.          RXu(f) (Equation (3)) serves as an unbiased estimate for R(f) (Equation (1)):\n                                                R (f) = EXu\u223cP (x)N               RXu (f)        .\n Proof of Proposition D.1. We first observe that R(f) can be expanded as following:\n                                                 1                               1   2\n          R (f) \u225c       W(X       ) \u2212  D(X     ) 2 f (X   ) f (X2  )\u22a4   D(X    )2  1 F                             1   2\n                   =    D(X     )\u22121 2 W(X      )D(X     )\u22121 2   F +     D(X      ) 2 f (X   ) f (X   )\u22a4  D(X     )2    F\n                       \u2212  2 tr    W(X      )f (X    ) f (X   )\u22a4\n                   =                   w2xx\u2032                                                    f(x)\u22a4f(x\u2032)        2\n                       x\u2208X   x\u2032\u2208X     wxwx\u2032 \u2212       2wxx\u2032f(x)\u22a4f(x\u2032) + wxwx\u2032\n                   =                 wxwx\u2032 \u00b7             wxx\u2032      2   \u2212    2wxx\u2032                            f(x)\u22a4f(x\u2032)         2     .\n                       x\u2208X   x\u2032\u2208X                       wxwx\u2032              wxwx\u2032 f(x)\u22a4f(x\u2032) +\n Then, with k\u03c8 (x, x\u2032) =              w xx\u2032    RXu (f) in Equation (3), wx = P (x), and wx\u2032 = P (x\u2032), we have\n              R (f) =Ex,x\u2032\u223cP (x)2    wxw   x\u2032 ,k\u03c8 (x, x\u2032)2 \u2212       2k\u03c8 (x, x\u2032) f(x)\u22a4f(x\u2032) +                 f(x)\u22a4f(x\u2032)        2\n                        =Ex,x\u2032\u223cP (x)2           f(x)\u22a4f(x\u2032) \u2212          k\u03c8 (x, x\u2032)      2                                \uf8f9\n                                             \uf8ee      N/2                   \u22a4  f (xu                                 2   \uf8fb\n                        =EXu\u223cP (x)N          \uf8f0 N2   i=1     f   xu2i\u22121              2i) \u2212   k\u03c8     xu2i\u22121, xu  2i\n                        =EXu\u223cP (x)N           RXu (f)        .\n D.2     Proof of Theorem 4.2\n Proof of Theorem 4.2. By defining\n                                          rx,x\u2032(f) \u225c         f(x)\u22a4f(x\u2032) \u2212          k\u03c8 (x, x\u2032)      2 ,\n we have R(f) = Ex,x\u2032\u223cP (x)2 [               rx,x\u2032(f)] and       RXu (f) = 2            N/2      rxu\n                                                 N/2                               N      i=1       2i\u22121,xu 2i(f) as\n                           RXu (f) = 2      N    i=1     f   xu2i\u22121    \u22a4  f (xu  2i) \u2212   k\u03c8    xu 2i\u22121, xu  2i  2   ,\n where        xu2i\u22121, xu  2i   N/2\n                                 i=1 are i.i.d. given Xu \u223c            P  (x)N. Consider the function class\n                                         r\u00b7,\u00b7 \u25e6  F = {    r\u00b7,\u00b7(f) : X \u00d7 X \u2192            R|f \u2208     F} .\n Since 0         \u2264      k\u03c8(x, x\u2032)         \u2264      Bk\u03c8,      and by Cauchy-Schwarz inequality,                           f(x)\u22a4f(x\u2032)         \u2264\n \u2225f(x)\u22252 \u2225f(x\u2032)\u22252 \u2264             Bf, we have 0 \u2264            rx,x\u2032(f) \u2264         Bk\u03c8 + Bf        2 for all x, x\u2032 \u2208         X  , which means\n that  r\u00b7,\u00b7 \u25e6  F is     Bk\u03c8 + Bf        2-bounded. Leveraging Lemma F.1 gives that, with probability at least\n1 \u2212    \u03b4/2 over Xu,R     f|Xu     \u2212   R    f|X    \u2264   4RN/2 (                                           2     log (4/\u03b4)     .\n                                                                  r\u00b7,\u00b7 \u25e6  F) + 2       Bk\u03c8 + Bf                    N\n                                                                       26", "md": "# Unbiased Estimations of Population Laplacian\n\n## Unbiased Estimations of Population Laplacian\n\nProposition D.1. $$RXu(f)$$ (Equation (3)) serves as an unbiased estimate for $$R(f)$$ (Equation (1)):\n\n$$R(f) = EXu\u223cP(x)N \\quad RXu(f)$$.\n\nProof of Proposition D.1. We first observe that $$R(f)$$ can be expanded as following:\n\n$$\n\\begin{align*}\nR(f) & = W(X) - D(X)^{1/2}f(X)f(X)^{\\top}D(X)^{1/2} \\\\\n& = D(X)^{-1/2}W(X)D(X)^{-1/2}f + D(X)^{1/2}f(X)f(X)^{\\top}D(X)^{1/2} - 2\\text{tr}W(X)f(X)f(X)^{\\top} \\\\\n& = \\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x') - 2\\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x') + \\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x')\n\\end{align*}\n$$\n\nThen, with $$k_{\\psi}(x, x') = w_{xx'} RXu(f)$$ in Equation (3), $$w_x = P(x)$$, and $$w_{x'} = P(x')$$, we have\n\n$$\n\\begin{align*}\nR(f) & = E_{x,x'\\sim P(x)}[w_xw_{x'}k_{\\psi}(x, x')^2 - 2k_{\\psi}(x, x')f(x)^{\\top}f(x') + f(x)^{\\top}f(x')] \\\\\n& = E_{x,x'\\sim P(x)}[f(x)^{\\top}f(x') - k_{\\psi}(x, x')^2]\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n& = EXu\u223cP(x)N [N/2 \\sum_{i=1}^{N} f_{xu_{2i-1}}f_{2i}) - k_{\\psi}(xu_{2i-1}, xu_{2i})] \\\\\n& = EXu\u223cP(x)N RXu(f)\n\\end{align*}\n$$\n\n## Proof of Theorem 4.2\n\nProof of Theorem 4.2. By defining\n\n$$r_{x,x'}(f) = f(x)^{\\top}f(x') - k_{\\psi}(x, x')^2$$,\n\nwe have $$R(f) = E_{x,x'\\sim P(x)}[r_{x,x'}(f)]$$ and $$RXu(f) = 2N/2 \\sum_{i=1}^{N} f_{xu_{2i-1}}^{\\top}f(xu_{2i}) - k_{\\psi}(xu_{2i-1}, xu_{2i})^2$$,\n\nwhere $$xu_{2i-1}, xu_{2i}$$ are i.i.d. given $$Xu\u223cP(x)N$$. Consider the function class\n\n$$r_{\\cdot,\\cdot} \\circ F = \\{r_{\\cdot,\\cdot}(f) : X \\times X \\rightarrow R | f \\in F\\}$$.\n\nSince $$0 \\leq k_{\\psi}(x, x') \\leq Bk_{\\psi}$$, and by Cauchy-Schwarz inequality, $$f(x)^{\\top}f(x') \\leq \\|f(x)\\|^2 \\|f(x')\\|^2 \\leq Bf$$, we have $$0 \\leq r_{x,x'}(f) \\leq Bk_{\\psi} + Bf$$ for all $$x, x' \\in X$$, which means that $$r_{\\cdot,\\cdot} \\circ F$$ is $$Bk_{\\psi} + Bf$$-bounded. Leveraging Lemma F.1 gives that, with probability at least $$1 - \\delta/2$$ over $$Xu, Rf|Xu - Rf|X \\leq 4RN/2 (2 \\log(4/\\delta))$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Unbiased Estimations of Population Laplacian", "md": "# Unbiased Estimations of Population Laplacian"}, {"type": "heading", "lvl": 2, "value": "Unbiased Estimations of Population Laplacian", "md": "## Unbiased Estimations of Population Laplacian"}, {"type": "text", "value": "Proposition D.1. $$RXu(f)$$ (Equation (3)) serves as an unbiased estimate for $$R(f)$$ (Equation (1)):\n\n$$R(f) = EXu\u223cP(x)N \\quad RXu(f)$$.\n\nProof of Proposition D.1. We first observe that $$R(f)$$ can be expanded as following:\n\n$$\n\\begin{align*}\nR(f) & = W(X) - D(X)^{1/2}f(X)f(X)^{\\top}D(X)^{1/2} \\\\\n& = D(X)^{-1/2}W(X)D(X)^{-1/2}f + D(X)^{1/2}f(X)f(X)^{\\top}D(X)^{1/2} - 2\\text{tr}W(X)f(X)f(X)^{\\top} \\\\\n& = \\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x') - 2\\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x') + \\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x')\n\\end{align*}\n$$\n\nThen, with $$k_{\\psi}(x, x') = w_{xx'} RXu(f)$$ in Equation (3), $$w_x = P(x)$$, and $$w_{x'} = P(x')$$, we have\n\n$$\n\\begin{align*}\nR(f) & = E_{x,x'\\sim P(x)}[w_xw_{x'}k_{\\psi}(x, x')^2 - 2k_{\\psi}(x, x')f(x)^{\\top}f(x') + f(x)^{\\top}f(x')] \\\\\n& = E_{x,x'\\sim P(x)}[f(x)^{\\top}f(x') - k_{\\psi}(x, x')^2]\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n& = EXu\u223cP(x)N [N/2 \\sum_{i=1}^{N} f_{xu_{2i-1}}f_{2i}) - k_{\\psi}(xu_{2i-1}, xu_{2i})] \\\\\n& = EXu\u223cP(x)N RXu(f)\n\\end{align*}\n$$", "md": "Proposition D.1. $$RXu(f)$$ (Equation (3)) serves as an unbiased estimate for $$R(f)$$ (Equation (1)):\n\n$$R(f) = EXu\u223cP(x)N \\quad RXu(f)$$.\n\nProof of Proposition D.1. We first observe that $$R(f)$$ can be expanded as following:\n\n$$\n\\begin{align*}\nR(f) & = W(X) - D(X)^{1/2}f(X)f(X)^{\\top}D(X)^{1/2} \\\\\n& = D(X)^{-1/2}W(X)D(X)^{-1/2}f + D(X)^{1/2}f(X)f(X)^{\\top}D(X)^{1/2} - 2\\text{tr}W(X)f(X)f(X)^{\\top} \\\\\n& = \\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x') - 2\\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x') + \\sum_{x\\in X}\\sum_{x'\\in X}w_{xx'}f(x)^{\\top}f(x')\n\\end{align*}\n$$\n\nThen, with $$k_{\\psi}(x, x') = w_{xx'} RXu(f)$$ in Equation (3), $$w_x = P(x)$$, and $$w_{x'} = P(x')$$, we have\n\n$$\n\\begin{align*}\nR(f) & = E_{x,x'\\sim P(x)}[w_xw_{x'}k_{\\psi}(x, x')^2 - 2k_{\\psi}(x, x')f(x)^{\\top}f(x') + f(x)^{\\top}f(x')] \\\\\n& = E_{x,x'\\sim P(x)}[f(x)^{\\top}f(x') - k_{\\psi}(x, x')^2]\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n& = EXu\u223cP(x)N [N/2 \\sum_{i=1}^{N} f_{xu_{2i-1}}f_{2i}) - k_{\\psi}(xu_{2i-1}, xu_{2i})] \\\\\n& = EXu\u223cP(x)N RXu(f)\n\\end{align*}\n$$"}, {"type": "heading", "lvl": 2, "value": "Proof of Theorem 4.2", "md": "## Proof of Theorem 4.2"}, {"type": "text", "value": "Proof of Theorem 4.2. By defining\n\n$$r_{x,x'}(f) = f(x)^{\\top}f(x') - k_{\\psi}(x, x')^2$$,\n\nwe have $$R(f) = E_{x,x'\\sim P(x)}[r_{x,x'}(f)]$$ and $$RXu(f) = 2N/2 \\sum_{i=1}^{N} f_{xu_{2i-1}}^{\\top}f(xu_{2i}) - k_{\\psi}(xu_{2i-1}, xu_{2i})^2$$,\n\nwhere $$xu_{2i-1}, xu_{2i}$$ are i.i.d. given $$Xu\u223cP(x)N$$. Consider the function class\n\n$$r_{\\cdot,\\cdot} \\circ F = \\{r_{\\cdot,\\cdot}(f) : X \\times X \\rightarrow R | f \\in F\\}$$.\n\nSince $$0 \\leq k_{\\psi}(x, x') \\leq Bk_{\\psi}$$, and by Cauchy-Schwarz inequality, $$f(x)^{\\top}f(x') \\leq \\|f(x)\\|^2 \\|f(x')\\|^2 \\leq Bf$$, we have $$0 \\leq r_{x,x'}(f) \\leq Bk_{\\psi} + Bf$$ for all $$x, x' \\in X$$, which means that $$r_{\\cdot,\\cdot} \\circ F$$ is $$Bk_{\\psi} + Bf$$-bounded. Leveraging Lemma F.1 gives that, with probability at least $$1 - \\delta/2$$ over $$Xu, Rf|Xu - Rf|X \\leq 4RN/2 (2 \\log(4/\\delta))$$.", "md": "Proof of Theorem 4.2. By defining\n\n$$r_{x,x'}(f) = f(x)^{\\top}f(x') - k_{\\psi}(x, x')^2$$,\n\nwe have $$R(f) = E_{x,x'\\sim P(x)}[r_{x,x'}(f)]$$ and $$RXu(f) = 2N/2 \\sum_{i=1}^{N} f_{xu_{2i-1}}^{\\top}f(xu_{2i}) - k_{\\psi}(xu_{2i-1}, xu_{2i})^2$$,\n\nwhere $$xu_{2i-1}, xu_{2i}$$ are i.i.d. given $$Xu\u223cP(x)N$$. Consider the function class\n\n$$r_{\\cdot,\\cdot} \\circ F = \\{r_{\\cdot,\\cdot}(f) : X \\times X \\rightarrow R | f \\in F\\}$$.\n\nSince $$0 \\leq k_{\\psi}(x, x') \\leq Bk_{\\psi}$$, and by Cauchy-Schwarz inequality, $$f(x)^{\\top}f(x') \\leq \\|f(x)\\|^2 \\|f(x')\\|^2 \\leq Bf$$, we have $$0 \\leq r_{x,x'}(f) \\leq Bk_{\\psi} + Bf$$ for all $$x, x' \\in X$$, which means that $$r_{\\cdot,\\cdot} \\circ F$$ is $$Bk_{\\psi} + Bf$$-bounded. Leveraging Lemma F.1 gives that, with probability at least $$1 - \\delta/2$$ over $$Xu, Rf|Xu - Rf|X \\leq 4RN/2 (2 \\log(4/\\delta))$$."}]}, {"page": 27, "text": "Now, it remains to show that RN/2 (                       r\u00b7,\u00b7 \u25e6  F) \u2264       4     2Bf      Bf + Bk\u03c8            RN/2 (F). For this, we recall\nEquation (4) and observe that\n                                                      \uf8ee          2   N/2                            \u22a4   f (xu                                     2   \uf8f9\n     R  N/2 (   r\u00b7,\u00b7 \u25e6   F) =EXu\u223cP (x)N               \uf8f0sup      N           \u03c1i \u00b7    f    xu 2i\u22121               2i) \u2212    k\u03c8     xu 2i\u22121, xu   2i       \uf8fb  ,\n                                      \u03c1\u223cRad     N2       f\u2208F         i=1\nwhere with Talagrand\u2019s lemma [Ledoux and Talagrand, 2013, Theorem 4.12] for compositions with\nscalar Lipschitz functions,\n                                                                                \uf8ee           2   N/2                         \u22a4   f (xu      \uf8f9\n               R  N/2 (   r\u00b7,\u00b7 \u25e6   F) \u2264      2    Bf + Bk       \u03c8    EXu,\u03c1      \uf8f0sup       N          \u03c1i \u00b7 f     xu 2i\u22121               2i) \uf8fb   .\nSince f        xu2i\u22121    \u22a4   f (xu  2i) \u2264      1     f     xu2i\u22121      2           f\u2208F  2i)\u22252  2i=1,\n                                               2                        2 + \u2225f (xu\n                       \uf8ee          2   N/2                         \u22a4   f (xu       \uf8f9\n            EXu,\u03c1      \uf8f0sup      N          \u03c1i \u00b7 f      xu2i\u22121               2i)  \uf8fb\n                         f\u2208F          i=1\n                       \uf8ee          2   N/2                 f                 2                           \uf8f9\n         \u2264EXu,\u03c1        \uf8f0sup      N          \u03c1i \u00b7 1 2            xu2i\u22121       2 + \u2225f (xu      2i)\u22252 2    \uf8fb\n               \uf8eb         f\u2208F  \uf8ee       i=12   N/2                             2  \uf8f9                  \uf8ee          2   N/2                           \uf8f9\uf8f6\n         \u22641 2  \uf8edEXu,\u03c1         \uf8f0sup      N          \u03c1i \u00b7    f     xu2i\u22121       2 \uf8fb   + EXu,\u03c1        \uf8f0sup      N          \u03c1i \u00b7 \u2225f (xu     2i)\u22252 2 \uf8fb\uf8f8\n                                f\u2208F          i=1                                                     f\u2208F          i=1\n                       \uf8ee          2   N/2                           \uf8f9\n         =EXu,\u03c1        \uf8f0sup      N          \u03c1i \u00b7 \u2225f (xu     2i)\u22252 2 \uf8fb  .\n                         f\u2208F          i=1\nBy the vector-contraction inequality for Rademacher complexities [Maurer, 2016, Corollary 4], since\n\u2225f (x)\u22252    2 is     2     Bf     -Lipschitz in f(x), with fk(x) denoting the k-th entry of f(x) \u2208                                        RK,\n                  \uf8ee          2   N/2                           \uf8f9                                           \uf8ee           2   N/2                       \uf8f9\n       EXu,\u03c1      \uf8f0sup      N          \u03c1i \u00b7 \u2225f (xu     2i)\u22252 2 \uf8fb   \u22642       2Bf \u00b7 EXu\u223cP (x)            N2  \uf8f0sup       N          \u03c1ik \u00b7 fk (xu     i )\uf8fb\n                    f\u2208F          i=1                                                               N          f\u2208F          i=1\n                                                                                        \u03c1\u223cRad      2 \u00d7K\n                                                                   =2       2Bf    RN/2 (F) .\nOverall, we have RN/2 (               r\u00b7,\u00b7 \u25e6   F) \u2264      4     2Bf       Bf + Bk\u03c8           RN/2 (F) which completes the proof.\nD.3       Unlabeled Sample Complexity with Deep Neural Networks\nHere, we ground Theorem 4.2 by exemplifying RN/2(F) leveraging the existing generalization\nbound [Golowich et al., 2018] for deep neural networks.\nClaim D.2 (Rademacher complexity of deep neural networks [Golowich et al., 2018]). Recall the\nnotion of Rademacher complexity for a vector-valued function class F \u220b                                                       f : X \u2192            RK from\nEquation (4). Adopting the existing results in Golowich et al. [2018] and following the notations in\nSection 6, we consider F as a class of deep neural networks:\n                        F = {f(x) = Ap\u03d5 (\u00b7 \u00b7 \u00b7 A2\u03d5(A1x) \u00b7 \u00b7 \u00b7 ) | \u2225A\u03b9\u2225F \u2264                                     BF,\u03b9 \u2200      \u03b9 \u2208   [p]}\nwith weight matrices {A\u03b9 | \u03b9 \u2208                    [p]} and a 1-Lipschitz, positive-homogeneous activation function \u03d5 (\u00b7)\n(e.g., ReLU) applied entry-wisely. Let BF =  p                               \u03b9=1 BF,\u03b9 and \u2225x\u22252 \u2264                 BX for all x \u2208            X  . Then, by\nGolowich et al. [2018] Theorem 1, we have\n                                           R  N/2(F) \u2264             2     p log(2) +   \u221a  N \u221a  2    KBX BF          .\n                                                                             27", "md": "# Math Equations\n\nNow, it remains to show that $$R_{N/2}(r\\cdot,\\cdot \\circ F) \\leq \\frac{4}{2Bf} \\frac{Bf + Bk\\psi}{R_{N/2}(F)}$$. For this, we recall Equation (4) and observe that\n\n$$\nR_{N/2}(r\\cdot,\\cdot \\circ F) = \\mathbb{E}_{x_u \\sim P(x)}\\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\sum_{i=1}^{N/2} \\rho_i \\cdot f(x_u^{2i-1})^{2i} - k\\psi x_u^{2i-1}, x_u^{2i} \\right],\n$$\n\nwhere with Talagrand\u2019s lemma [Ledoux and Talagrand, 2013, Theorem 4.12] for compositions with scalar Lipschitz functions,\n\n$$\nR_{N/2}(r\\cdot,\\cdot \\circ F) \\leq 2Bf + Bk\\psi \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2i} \\right].\n$$\n\nSince $$f(x_u^{2i-1})^{T} f(x_u^{2i}) \\leq 1$$ $$f(x_u^{2i-1})^{2}$$ $$f \\in F$$ $$i=1$$,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2i} \\right] &\\leq \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot 1/2 \\cdot \\left\\| x_u^{2i-1} \\right\\|^{2} + \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] \\\\\n&\\leq 1/2 \\left( \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2} \\right] + \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] \\right) \\\\\n&= \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right].\n\\end{align*}\n$$\n\nBy the vector-contraction inequality for Rademacher complexities [Maurer, 2016, Corollary 4], since $$\\left\\| f(x) \\right\\|^{2}_{2}$$ is $$2Bf$$-Lipschitz in $$f(x)$$, with $$f_k(x)$$ denoting the $$k$$-th entry of $$f(x) \\in \\mathbb{R}^{K}$$,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] &\\leq 2 \\cdot 2Bf \\cdot \\mathbb{E}_{x_u \\sim P(x)} \\left[ \\sup_{\\substack{N} f \\in F} \\sup_{\\substack{N} \\rho_{ik} \\cdot f_k(x_u^{i})} \\right] \\\\\n&= 2 \\cdot 2Bf \\cdot R_{N/2}(F).\n\\end{align*}\n$$\n\nOverall, we have $$R_{N/2}(r\\cdot,\\cdot \\circ F) \\leq \\frac{4}{2Bf} Bf + Bk\\psi R_{N/2}(F)$$ which completes the proof.\n\nD.3 Unlabeled Sample Complexity with Deep Neural Networks\n\nHere, we ground Theorem 4.2 by exemplifying $$R_{N/2}(F)$$ leveraging the existing generalization bound [Golowich et al., 2018] for deep neural networks.\n\nClaim D.2 (Rademacher complexity of deep neural networks [Golowich et al., 2018]). Recall the notion of Rademacher complexity for a vector-valued function class $$F \\ni f : X \\rightarrow \\mathbb{R}^{K}$$ from Equation (4). Adopting the existing results in Golowich et al. [2018] and following the notations in Section 6, we consider $$F$$ as a class of deep neural networks:\n\n$$\nF = \\left\\{ f(x) = A_p \\phi \\left( \\cdots A_2 \\phi(A_1 x) \\cdots \\right) \\, \\middle| \\, \\left\\| A_{\\iota} \\right\\|_F \\leq BF_{\\iota} \\, \\forall \\, \\iota \\in [p] \\right\\}\n$$\n\nwith weight matrices $$\\{A_{\\iota} \\, | \\, \\iota \\in [p]\\}$$ and a 1-Lipschitz, positive-homogeneous activation function $$\\phi(\\cdot)$$ (e.g., ReLU) applied entry-wisely. Let $$BF = \\sum_{\\iota=1}^{p} BF_{\\iota}$$ and $$\\left\\| x \\right\\|_{2} \\leq BX$$ for all $$x \\in X$$. Then, by Golowich et al. [2018] Theorem 1, we have\n\n$$R_{N/2}(F) \\leq 2p \\log(2) + \\sqrt{N} \\sqrt{2} KBX BF / 27$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Now, it remains to show that $$R_{N/2}(r\\cdot,\\cdot \\circ F) \\leq \\frac{4}{2Bf} \\frac{Bf + Bk\\psi}{R_{N/2}(F)}$$. For this, we recall Equation (4) and observe that\n\n$$\nR_{N/2}(r\\cdot,\\cdot \\circ F) = \\mathbb{E}_{x_u \\sim P(x)}\\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\sum_{i=1}^{N/2} \\rho_i \\cdot f(x_u^{2i-1})^{2i} - k\\psi x_u^{2i-1}, x_u^{2i} \\right],\n$$\n\nwhere with Talagrand\u2019s lemma [Ledoux and Talagrand, 2013, Theorem 4.12] for compositions with scalar Lipschitz functions,\n\n$$\nR_{N/2}(r\\cdot,\\cdot \\circ F) \\leq 2Bf + Bk\\psi \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2i} \\right].\n$$\n\nSince $$f(x_u^{2i-1})^{T} f(x_u^{2i}) \\leq 1$$ $$f(x_u^{2i-1})^{2}$$ $$f \\in F$$ $$i=1$$,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2i} \\right] &\\leq \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot 1/2 \\cdot \\left\\| x_u^{2i-1} \\right\\|^{2} + \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] \\\\\n&\\leq 1/2 \\left( \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2} \\right] + \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] \\right) \\\\\n&= \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right].\n\\end{align*}\n$$\n\nBy the vector-contraction inequality for Rademacher complexities [Maurer, 2016, Corollary 4], since $$\\left\\| f(x) \\right\\|^{2}_{2}$$ is $$2Bf$$-Lipschitz in $$f(x)$$, with $$f_k(x)$$ denoting the $$k$$-th entry of $$f(x) \\in \\mathbb{R}^{K}$$,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] &\\leq 2 \\cdot 2Bf \\cdot \\mathbb{E}_{x_u \\sim P(x)} \\left[ \\sup_{\\substack{N} f \\in F} \\sup_{\\substack{N} \\rho_{ik} \\cdot f_k(x_u^{i})} \\right] \\\\\n&= 2 \\cdot 2Bf \\cdot R_{N/2}(F).\n\\end{align*}\n$$\n\nOverall, we have $$R_{N/2}(r\\cdot,\\cdot \\circ F) \\leq \\frac{4}{2Bf} Bf + Bk\\psi R_{N/2}(F)$$ which completes the proof.\n\nD.3 Unlabeled Sample Complexity with Deep Neural Networks\n\nHere, we ground Theorem 4.2 by exemplifying $$R_{N/2}(F)$$ leveraging the existing generalization bound [Golowich et al., 2018] for deep neural networks.\n\nClaim D.2 (Rademacher complexity of deep neural networks [Golowich et al., 2018]). Recall the notion of Rademacher complexity for a vector-valued function class $$F \\ni f : X \\rightarrow \\mathbb{R}^{K}$$ from Equation (4). Adopting the existing results in Golowich et al. [2018] and following the notations in Section 6, we consider $$F$$ as a class of deep neural networks:\n\n$$\nF = \\left\\{ f(x) = A_p \\phi \\left( \\cdots A_2 \\phi(A_1 x) \\cdots \\right) \\, \\middle| \\, \\left\\| A_{\\iota} \\right\\|_F \\leq BF_{\\iota} \\, \\forall \\, \\iota \\in [p] \\right\\}\n$$\n\nwith weight matrices $$\\{A_{\\iota} \\, | \\, \\iota \\in [p]\\}$$ and a 1-Lipschitz, positive-homogeneous activation function $$\\phi(\\cdot)$$ (e.g., ReLU) applied entry-wisely. Let $$BF = \\sum_{\\iota=1}^{p} BF_{\\iota}$$ and $$\\left\\| x \\right\\|_{2} \\leq BX$$ for all $$x \\in X$$. Then, by Golowich et al. [2018] Theorem 1, we have\n\n$$R_{N/2}(F) \\leq 2p \\log(2) + \\sqrt{N} \\sqrt{2} KBX BF / 27$$", "md": "Now, it remains to show that $$R_{N/2}(r\\cdot,\\cdot \\circ F) \\leq \\frac{4}{2Bf} \\frac{Bf + Bk\\psi}{R_{N/2}(F)}$$. For this, we recall Equation (4) and observe that\n\n$$\nR_{N/2}(r\\cdot,\\cdot \\circ F) = \\mathbb{E}_{x_u \\sim P(x)}\\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\sum_{i=1}^{N/2} \\rho_i \\cdot f(x_u^{2i-1})^{2i} - k\\psi x_u^{2i-1}, x_u^{2i} \\right],\n$$\n\nwhere with Talagrand\u2019s lemma [Ledoux and Talagrand, 2013, Theorem 4.12] for compositions with scalar Lipschitz functions,\n\n$$\nR_{N/2}(r\\cdot,\\cdot \\circ F) \\leq 2Bf + Bk\\psi \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2i} \\right].\n$$\n\nSince $$f(x_u^{2i-1})^{T} f(x_u^{2i}) \\leq 1$$ $$f(x_u^{2i-1})^{2}$$ $$f \\in F$$ $$i=1$$,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2i} \\right] &\\leq \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot 1/2 \\cdot \\left\\| x_u^{2i-1} \\right\\|^{2} + \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] \\\\\n&\\leq 1/2 \\left( \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot f(x_u^{2i-1})^{2} \\right] + \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] \\right) \\\\\n&= \\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right].\n\\end{align*}\n$$\n\nBy the vector-contraction inequality for Rademacher complexities [Maurer, 2016, Corollary 4], since $$\\left\\| f(x) \\right\\|^{2}_{2}$$ is $$2Bf$$-Lipschitz in $$f(x)$$, with $$f_k(x)$$ denoting the $$k$$-th entry of $$f(x) \\in \\mathbb{R}^{K}$$,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x_u,\\rho} \\left[ \\sup_{\\substack{\\rho \\sim Rad_{N/2} \\\\ f \\in F}} \\rho_i \\cdot \\left\\| f(x_u^{2i}) \\right\\|^{2} \\right] &\\leq 2 \\cdot 2Bf \\cdot \\mathbb{E}_{x_u \\sim P(x)} \\left[ \\sup_{\\substack{N} f \\in F} \\sup_{\\substack{N} \\rho_{ik} \\cdot f_k(x_u^{i})} \\right] \\\\\n&= 2 \\cdot 2Bf \\cdot R_{N/2}(F).\n\\end{align*}\n$$\n\nOverall, we have $$R_{N/2}(r\\cdot,\\cdot \\circ F) \\leq \\frac{4}{2Bf} Bf + Bk\\psi R_{N/2}(F)$$ which completes the proof.\n\nD.3 Unlabeled Sample Complexity with Deep Neural Networks\n\nHere, we ground Theorem 4.2 by exemplifying $$R_{N/2}(F)$$ leveraging the existing generalization bound [Golowich et al., 2018] for deep neural networks.\n\nClaim D.2 (Rademacher complexity of deep neural networks [Golowich et al., 2018]). Recall the notion of Rademacher complexity for a vector-valued function class $$F \\ni f : X \\rightarrow \\mathbb{R}^{K}$$ from Equation (4). Adopting the existing results in Golowich et al. [2018] and following the notations in Section 6, we consider $$F$$ as a class of deep neural networks:\n\n$$\nF = \\left\\{ f(x) = A_p \\phi \\left( \\cdots A_2 \\phi(A_1 x) \\cdots \\right) \\, \\middle| \\, \\left\\| A_{\\iota} \\right\\|_F \\leq BF_{\\iota} \\, \\forall \\, \\iota \\in [p] \\right\\}\n$$\n\nwith weight matrices $$\\{A_{\\iota} \\, | \\, \\iota \\in [p]\\}$$ and a 1-Lipschitz, positive-homogeneous activation function $$\\phi(\\cdot)$$ (e.g., ReLU) applied entry-wisely. Let $$BF = \\sum_{\\iota=1}^{p} BF_{\\iota}$$ and $$\\left\\| x \\right\\|_{2} \\leq BX$$ for all $$x \\in X$$. Then, by Golowich et al. [2018] Theorem 1, we have\n\n$$R_{N/2}(F) \\leq 2p \\log(2) + \\sqrt{N} \\sqrt{2} KBX BF / 27$$"}]}, {"page": 28, "text": "D.4     Proof of Theorem 4.3\nProof of Theorem 4.3. Let UX \u225c                   D (X    )1/2 f|Xu (X       ), FX \u225c      D (X    )1/2 f|X (X      ), and yk \u2208       [0, 1]|X|\nbe the k-th column of YX \u225c                     D (X    )1/2 \u2212 \u2192\n                                                              y\u2217(X    ). We denote PU \u225c                 UX U\u2020    X as the orthogonal\nprojection onto Range (UX ) and P\u22a5                 U \u225c     I \u2212   PU as the orthogonal complement of PU.\nRecalling from the proof of Theorem 4.1, Lemma C.1 implies that for all f|Xu \u2208                                       FL(Xu),\nP    M(f    |Xu)     \u22642 max        \u03b22\u03b32 , 1               min                           2 = 2 max        \u03b22\u03b32 , 1              P\u22a5  Uyk     22 ,\n                                                k\u2208[K]   zk\u2208RK \u2225yk \u2212         UX zk\u22252                                   k\u2208[K]\nwhere the second equality comes from taking zk = U\u2020                           X yk for every k \u2208         [K].\nConsider the spectral decomposition of L (X                     ) in Equation (13). Since the orthonormal eigenvectors\n vi \u2208     R|X| i\u2208[|X|] associated with eigenvalues 0 = \u03bb1 \u2264                            \u00b7 \u00b7 \u00b7 \u2264   \u03bb|X| \u2264      1 form a basis of R|X|,\nby denoting VK \u225c              [v1, . . . , vK] \u2208      R|X|\u00d7K and V\u22a5         K \u225c       vK+1, . . . , v|X|        \u2208   R|X|\u00d7K, for every\nk \u2208   [K],          P\u22a5  Uyk     22 =    |X|    v\u22a4i P\u22a5 Uyk     2 =     V\u22a4  KP\u22a5   Uyk    22 +       V\u22a5 K   \u22a4   P\u22a5 Uyk     2\n                                        i=1                                                                             2 .\nIt is shown in the proof of Theorem 4.1 that\n                                        V\u22a5  K  \u22a4   P\u22a5 Uyk     2                  V\u22a5 K   \u22a4   yk    2     \u03bbK+1\u03b1    ,\n                            k\u2208[K]                             2 \u2264    k\u2208[K]                        2 \u2264\nwhereas by observing that \u2225yk\u22252               2 =       x\u2208X wx \u00b7 1 {y\u2217         (x) = k} = P (Xk),                                      2\n   k\u2208[K]    V\u22a4   KP\u22a5  Uyk     22 \u2264   k\u2208[K]    \u2225yk\u22252  2  P\u22a5  UVK       2F =     P\u22a5  UVK       2F  k\u2208[K]   P (Xk) =        P\u22a5  UVK        F .\nOverall, we have            P    M(f    |Xu)     \u2264   2 max      \u03b22\u03b32 , 1           \u03b1     +    P\u22a5  UVK      2 F    .                      (16)\n                                                                                \u03bbK+1\nUpper        bounding           P\u22a5  UVK      2F .    We      first    recall     from      Equation        (13)      that    W(X      )     =\n |X|                       i . For f|X \u2208        FL(X), Eckart-Young-Mirsky theorem [Eckart and Young, 1936]\n   i=1 (1 \u2212      \u03bbi) viv\u22a4\nimplies that FX F\u22a4       X =  K      i=1 (1 \u2212     \u03bbi) viv\u22a4 2 i , and                               2\n              R    f |X    =    W(X       ) \u2212   FX F\u22a4   X   F =       i>K   (1 \u2212    \u03bbi) viv\u22a4   i   F  =   i>K    (1 \u2212   \u03bbi)2 .\nTherefore, R         f|Xu     \u2264   R    f|X     + \u2206    implies that\ni>K   (1 \u2212    \u03bbi)2 + \u2206      \u2265    W(X       ) \u2212   UX U\u22a4 2  X   F =        W(X      ) \u2212   PUW(X         )   +    PUW(X          ) \u2212  UX U\u22a4    X    2F\n                            =    P\u22a5  UW(X       ) +2PU     PU W(X      ) \u2212   UX U\u22a4    X    2F    2  since PU and P\u22a5         U are orthogonal\n                            =    P\u22a5  UW(X       )   F +             W(X      ) \u2212   UX U\u22a4    X     F\n                            \u2265    P\u22a5  UW(X       )  2F =      |X|   (1 \u2212   \u03bbi)    P\u22a5 Uvi     v\u22a4 i  2\n                                   \uf8eb   |X|   |X|             i=1                                   F          \u22a4  \uf8f6\n                            = tr   \uf8ed   i=1  j=1   (1 \u2212    \u03bbi) (1 \u2212     \u03bbj)    P\u22a5 Uvi     v\u22a4i vj     P\u22a5 Uvj       \uf8f8\n                            =   |X|   (1 \u2212   \u03bbi)2    P\u22a5  Uvi    22 .\n                                i=1\n                                                                                                                                         (17)\n                                                                      28", "md": "Proof of Theorem 4.3. Let $$U_X \\triangleq D(X)^{1/2} f|Xu(X), F_X \\triangleq D(X)^{1/2} f|X(X),$$ and $$y_k \\in [0, 1]^{|X|}$$ be the k-th column of $$Y_X \\triangleq D(X)^{1/2} \\overrightarrow{y^*(X)}$$. We denote $$P_U \\triangleq U_X U_X^\\dagger$$ as the orthogonal projection onto Range$$(U_X)$$ and $$P_\\perp^U \\triangleq I - P_U$$ as the orthogonal complement of $$P_U$$. Recalling from the proof of Theorem 4.1, Lemma C.1 implies that for all $$f|Xu \\in FL(X_u)$$,\n$$\nP_M(f|X_u) \\leq 2 \\max_{k\\in[K], z_k\\in\\mathbb{R}^K} \\frac{\\beta^2\\gamma^2}{\\min_{k\\in[K]} 2} = 2 \\max_{k\\in[K]} \\frac{\\beta^2\\gamma^2}{1} P_\\perp^U y_k^2,\n$$\nwhere the second equality comes from taking $$z_k = U_X^\\dagger y_k$$ for every $$k\\in[K]$$. Consider the spectral decomposition of $$L(X)$$ in Equation (13). Since the orthonormal eigenvectors $$v_i \\in \\mathbb{R}^{|X|}, i\\in[|X|]$$ associated with eigenvalues $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\leq 1$$ form a basis of $$\\mathbb{R}^{|X|}$$, by denoting $$V_K \\triangleq [v_1, \\ldots, v_K] \\in \\mathbb{R}^{|X|\\times K}$$ and $$V_\\perp^K \\triangleq [v_{K+1}, \\ldots, v_{|X|}] \\in \\mathbb{R}^{|X|\\times K}$$, for every $$k\\in[K]$$,\n$$\nP_\\perp^U y_k^2 = |X| \\sum_{i=1}^{i=|X|} v_i^\\top P_\\perp^U y_k^2 = V_K^\\top KP_\\perp^U y_k^2 + V_\\perp^K{}^\\top P_\\perp^U y_k^2.\n$$\nIt is shown in the proof of Theorem 4.1 that\n$$\n\\sum_{k\\in[K]} V_\\perp^K{}^\\top P_\\perp^U y_k^2 \\leq \\sum_{k\\in[K]} V_\\perp^K{}^\\top y_k^2 \\leq \\lambda_{K+1}\\alpha,\n$$\nwhereas by observing that $$\\|y_k\\|_2^2 = \\sum_{x\\in X} w_x \\cdot 1\\{y^*(x) = k\\} = P(X_k)$$,\n$$\n\\sum_{k\\in[K]} V_K^\\top KP_\\perp^U y_k^2 \\leq \\sum_{k\\in[K]} \\|y_k\\|_2^2 P_\\perp^U V_K^2F = P_\\perp^U V_K^2F \\sum_{k\\in[K]} P(X_k) = P_\\perp^U V_K^2F.\n$$\nOverall, we have\n$$\nP_M(f|X_u) \\leq 2 \\max_{k\\in[K]} \\beta^2\\gamma^2, 1 \\alpha + P_\\perp^U V_K^2F. \\quad (16)\n$$\nUpper bounding $$P_\\perp^U V_K^2F$$. We first recall from Equation (13) that $$W(X) = \\sum_{i=1}^{|X|} \\lambda_i v_i v_i^\\top$$. For $$f|X \\in FL(X)$$, Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that $$F_X F_X^\\top = \\sum_{i=1}^K (1 - \\lambda_i) v_i v_i^\\top 2$$, and\n$$\nR_{f|X} = W(X) - F_X F_X^\\top F = \\sum_{i>K} (1 - \\lambda_i) v_i v_i^\\top 2 = \\sum_{i>K} (1 - \\lambda_i)^2.\n$$\nTherefore, $$R_{f|X_u} \\leq R_{f|X} + \\Delta$$ implies that\n$$\n\\sum_{i>K} (1 - \\lambda_i)^2 + \\Delta \\geq W(X) - U_X U_X^\\top 2F = W(X) - PUW(X) + PUW(X) - U_X U_X^\\top 2F\n= P_\\perp^UW(X) + 2PU PUW(X) - U_X U_X^\\top 2F \\quad \\text{since } PU \\text{ and } P_\\perp^U \\text{ are orthogonal}\n= P_\\perp^UW(X) F + W(X) - U_X U_X^\\top F\n\\geq P_\\perp^UW(X) 2F = |X| \\sum_{i=1}^{i>K} (1 - \\lambda_i) P_\\perp^U v_i v_i^\\top 2\n= \\text{tr} \\left( \\sum_{i=1}^{i=|X|} \\sum_{j=1}^{j=|X|} (1 - \\lambda_i) (1 - \\lambda_j) P_\\perp^U v_i v_i^\\top v_j P_\\perp^U v_j \\right)\n= |X| \\sum_{i=1}^{i>K} (1 - \\lambda_i)^2 P_\\perp^U v_i 2. \\quad (17)\n$$", "images": [], "items": [{"type": "text", "value": "Proof of Theorem 4.3. Let $$U_X \\triangleq D(X)^{1/2} f|Xu(X), F_X \\triangleq D(X)^{1/2} f|X(X),$$ and $$y_k \\in [0, 1]^{|X|}$$ be the k-th column of $$Y_X \\triangleq D(X)^{1/2} \\overrightarrow{y^*(X)}$$. We denote $$P_U \\triangleq U_X U_X^\\dagger$$ as the orthogonal projection onto Range$$(U_X)$$ and $$P_\\perp^U \\triangleq I - P_U$$ as the orthogonal complement of $$P_U$$. Recalling from the proof of Theorem 4.1, Lemma C.1 implies that for all $$f|Xu \\in FL(X_u)$$,\n$$\nP_M(f|X_u) \\leq 2 \\max_{k\\in[K], z_k\\in\\mathbb{R}^K} \\frac{\\beta^2\\gamma^2}{\\min_{k\\in[K]} 2} = 2 \\max_{k\\in[K]} \\frac{\\beta^2\\gamma^2}{1} P_\\perp^U y_k^2,\n$$\nwhere the second equality comes from taking $$z_k = U_X^\\dagger y_k$$ for every $$k\\in[K]$$. Consider the spectral decomposition of $$L(X)$$ in Equation (13). Since the orthonormal eigenvectors $$v_i \\in \\mathbb{R}^{|X|}, i\\in[|X|]$$ associated with eigenvalues $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\leq 1$$ form a basis of $$\\mathbb{R}^{|X|}$$, by denoting $$V_K \\triangleq [v_1, \\ldots, v_K] \\in \\mathbb{R}^{|X|\\times K}$$ and $$V_\\perp^K \\triangleq [v_{K+1}, \\ldots, v_{|X|}] \\in \\mathbb{R}^{|X|\\times K}$$, for every $$k\\in[K]$$,\n$$\nP_\\perp^U y_k^2 = |X| \\sum_{i=1}^{i=|X|} v_i^\\top P_\\perp^U y_k^2 = V_K^\\top KP_\\perp^U y_k^2 + V_\\perp^K{}^\\top P_\\perp^U y_k^2.\n$$\nIt is shown in the proof of Theorem 4.1 that\n$$\n\\sum_{k\\in[K]} V_\\perp^K{}^\\top P_\\perp^U y_k^2 \\leq \\sum_{k\\in[K]} V_\\perp^K{}^\\top y_k^2 \\leq \\lambda_{K+1}\\alpha,\n$$\nwhereas by observing that $$\\|y_k\\|_2^2 = \\sum_{x\\in X} w_x \\cdot 1\\{y^*(x) = k\\} = P(X_k)$$,\n$$\n\\sum_{k\\in[K]} V_K^\\top KP_\\perp^U y_k^2 \\leq \\sum_{k\\in[K]} \\|y_k\\|_2^2 P_\\perp^U V_K^2F = P_\\perp^U V_K^2F \\sum_{k\\in[K]} P(X_k) = P_\\perp^U V_K^2F.\n$$\nOverall, we have\n$$\nP_M(f|X_u) \\leq 2 \\max_{k\\in[K]} \\beta^2\\gamma^2, 1 \\alpha + P_\\perp^U V_K^2F. \\quad (16)\n$$\nUpper bounding $$P_\\perp^U V_K^2F$$. We first recall from Equation (13) that $$W(X) = \\sum_{i=1}^{|X|} \\lambda_i v_i v_i^\\top$$. For $$f|X \\in FL(X)$$, Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that $$F_X F_X^\\top = \\sum_{i=1}^K (1 - \\lambda_i) v_i v_i^\\top 2$$, and\n$$\nR_{f|X} = W(X) - F_X F_X^\\top F = \\sum_{i>K} (1 - \\lambda_i) v_i v_i^\\top 2 = \\sum_{i>K} (1 - \\lambda_i)^2.\n$$\nTherefore, $$R_{f|X_u} \\leq R_{f|X} + \\Delta$$ implies that\n$$\n\\sum_{i>K} (1 - \\lambda_i)^2 + \\Delta \\geq W(X) - U_X U_X^\\top 2F = W(X) - PUW(X) + PUW(X) - U_X U_X^\\top 2F\n= P_\\perp^UW(X) + 2PU PUW(X) - U_X U_X^\\top 2F \\quad \\text{since } PU \\text{ and } P_\\perp^U \\text{ are orthogonal}\n= P_\\perp^UW(X) F + W(X) - U_X U_X^\\top F\n\\geq P_\\perp^UW(X) 2F = |X| \\sum_{i=1}^{i>K} (1 - \\lambda_i) P_\\perp^U v_i v_i^\\top 2\n= \\text{tr} \\left( \\sum_{i=1}^{i=|X|} \\sum_{j=1}^{j=|X|} (1 - \\lambda_i) (1 - \\lambda_j) P_\\perp^U v_i v_i^\\top v_j P_\\perp^U v_j \\right)\n= |X| \\sum_{i=1}^{i>K} (1 - \\lambda_i)^2 P_\\perp^U v_i 2. \\quad (17)\n$$", "md": "Proof of Theorem 4.3. Let $$U_X \\triangleq D(X)^{1/2} f|Xu(X), F_X \\triangleq D(X)^{1/2} f|X(X),$$ and $$y_k \\in [0, 1]^{|X|}$$ be the k-th column of $$Y_X \\triangleq D(X)^{1/2} \\overrightarrow{y^*(X)}$$. We denote $$P_U \\triangleq U_X U_X^\\dagger$$ as the orthogonal projection onto Range$$(U_X)$$ and $$P_\\perp^U \\triangleq I - P_U$$ as the orthogonal complement of $$P_U$$. Recalling from the proof of Theorem 4.1, Lemma C.1 implies that for all $$f|Xu \\in FL(X_u)$$,\n$$\nP_M(f|X_u) \\leq 2 \\max_{k\\in[K], z_k\\in\\mathbb{R}^K} \\frac{\\beta^2\\gamma^2}{\\min_{k\\in[K]} 2} = 2 \\max_{k\\in[K]} \\frac{\\beta^2\\gamma^2}{1} P_\\perp^U y_k^2,\n$$\nwhere the second equality comes from taking $$z_k = U_X^\\dagger y_k$$ for every $$k\\in[K]$$. Consider the spectral decomposition of $$L(X)$$ in Equation (13). Since the orthonormal eigenvectors $$v_i \\in \\mathbb{R}^{|X|}, i\\in[|X|]$$ associated with eigenvalues $$0 = \\lambda_1 \\leq \\ldots \\leq \\lambda_{|X|} \\leq 1$$ form a basis of $$\\mathbb{R}^{|X|}$$, by denoting $$V_K \\triangleq [v_1, \\ldots, v_K] \\in \\mathbb{R}^{|X|\\times K}$$ and $$V_\\perp^K \\triangleq [v_{K+1}, \\ldots, v_{|X|}] \\in \\mathbb{R}^{|X|\\times K}$$, for every $$k\\in[K]$$,\n$$\nP_\\perp^U y_k^2 = |X| \\sum_{i=1}^{i=|X|} v_i^\\top P_\\perp^U y_k^2 = V_K^\\top KP_\\perp^U y_k^2 + V_\\perp^K{}^\\top P_\\perp^U y_k^2.\n$$\nIt is shown in the proof of Theorem 4.1 that\n$$\n\\sum_{k\\in[K]} V_\\perp^K{}^\\top P_\\perp^U y_k^2 \\leq \\sum_{k\\in[K]} V_\\perp^K{}^\\top y_k^2 \\leq \\lambda_{K+1}\\alpha,\n$$\nwhereas by observing that $$\\|y_k\\|_2^2 = \\sum_{x\\in X} w_x \\cdot 1\\{y^*(x) = k\\} = P(X_k)$$,\n$$\n\\sum_{k\\in[K]} V_K^\\top KP_\\perp^U y_k^2 \\leq \\sum_{k\\in[K]} \\|y_k\\|_2^2 P_\\perp^U V_K^2F = P_\\perp^U V_K^2F \\sum_{k\\in[K]} P(X_k) = P_\\perp^U V_K^2F.\n$$\nOverall, we have\n$$\nP_M(f|X_u) \\leq 2 \\max_{k\\in[K]} \\beta^2\\gamma^2, 1 \\alpha + P_\\perp^U V_K^2F. \\quad (16)\n$$\nUpper bounding $$P_\\perp^U V_K^2F$$. We first recall from Equation (13) that $$W(X) = \\sum_{i=1}^{|X|} \\lambda_i v_i v_i^\\top$$. For $$f|X \\in FL(X)$$, Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that $$F_X F_X^\\top = \\sum_{i=1}^K (1 - \\lambda_i) v_i v_i^\\top 2$$, and\n$$\nR_{f|X} = W(X) - F_X F_X^\\top F = \\sum_{i>K} (1 - \\lambda_i) v_i v_i^\\top 2 = \\sum_{i>K} (1 - \\lambda_i)^2.\n$$\nTherefore, $$R_{f|X_u} \\leq R_{f|X} + \\Delta$$ implies that\n$$\n\\sum_{i>K} (1 - \\lambda_i)^2 + \\Delta \\geq W(X) - U_X U_X^\\top 2F = W(X) - PUW(X) + PUW(X) - U_X U_X^\\top 2F\n= P_\\perp^UW(X) + 2PU PUW(X) - U_X U_X^\\top 2F \\quad \\text{since } PU \\text{ and } P_\\perp^U \\text{ are orthogonal}\n= P_\\perp^UW(X) F + W(X) - U_X U_X^\\top F\n\\geq P_\\perp^UW(X) 2F = |X| \\sum_{i=1}^{i>K} (1 - \\lambda_i) P_\\perp^U v_i v_i^\\top 2\n= \\text{tr} \\left( \\sum_{i=1}^{i=|X|} \\sum_{j=1}^{j=|X|} (1 - \\lambda_i) (1 - \\lambda_j) P_\\perp^U v_i v_i^\\top v_j P_\\perp^U v_j \\right)\n= |X| \\sum_{i=1}^{i>K} (1 - \\lambda_i)^2 P_\\perp^U v_i 2. \\quad (17)\n$$"}]}, {"page": 29, "text": "Meanwhile, the assumption \u2206          < (1 \u2212   \u03bbK)2 implies that rank        P\u22a5 U   = |X| \u2212    K, and thus\n                     |X|  P\u22a5       2      P\u22a5 2             P\u22a5     = rank     P\u22a5     = |X| \u2212    K.                 (18)\n                             Uvi    2 =      U   F = tr       U                U\n                     i=1\nOtherwise by contradiction, suppose rank (PU) = rank (UX ) < K, then Eckart-Young-Mirsky\ntheorem [Eckart and Young, 1936] implies that\n         min        W(X) \u2212       UX U\u22a4 2X   F =    |X| (1 \u2212   \u03bbi)2 >        (1 \u2212   \u03bbi)2 + \u2206    = R    f|X   + \u2206,\n    rank(UX )<K                                   i=K                  i>K\nwhich contradicts R       f|Xu    \u2264  R   f|X    + \u2206.\nFurthermore, since \u2225vi\u22252 = 1 and P\u22a5         U is an orthogonal projection, for all i \u2208       [|X|],\n                                               0 \u2264    P\u22a5 Uvi   22 \u2264  1.                                           (19)\nFor every i \u2208    [|X|], we denote \u03bei \u225c       P\u22a5 Uvi   2                               P\u22a5 UVK     2         i=1 \u03bei can\nbe recast as a linear programming problem:    K       2. Then, upper bounding                     F =  K\n                           (P) :      max        \u03bei\n                                     \u03be\u2208R|X|  i=1\n                                     s.t.   0 \u2264   \u03bei \u2264  1,\n                                             |X| \u03bei = |X| \u2212     K,                                                (20)\n                                             i=1\n                                             |X| (1 \u2212   \u03bbi)2 \u03bei \u2264        (1 \u2212  \u03bbi)2 + \u2206,\n                                             i=1                    i>K\nwhose dual can be expressed as\n            (D) :             min          |X|  \u03c9i \u2212  (|X| \u2212   K) \u03c9s +       \u2206  +        (1 \u2212  \u03bbi)2    \u03c9\u2206\n                     (\u03c91,\u00b7\u00b7\u00b7 ,\u03c9|X|,\u03c9s,\u03c9\u2206)  i=1                                     i>K\n                                    s.t.   \u03c91, \u00b7 \u00b7 \u00b7 , \u03c9|X|, \u03c9s, \u03c9\u2206  \u2265  0,                                        (21)\n                                           \u03c9i \u2212  \u03c9s + (1 \u2212    \u03bbi)2 \u03c9\u2206    \u2265  1   \u2200  i \u2264  K,\nBy taking                                  \u03c9i \u2212  \u03c9s + (1 \u2212    \u03bbi)2 \u03c9\u2206    \u2265  0   \u2200  i > K.\n                         \u03c9i = 0     \u2200  i \u2264  K0,\n                                  (1 \u2212   \u03bbK0)2 \u2212    (1 \u2212  \u03bbi)2\n                         \u03c9i =   (1 \u2212  \u03bbK0)2 \u2212     (1 \u2212  \u03bbK+1)2       \u2200 K0 < i \u2264     K,\n                         \u03c9i = (1 \u2212      \u03bbK+1)2 \u2212     (1 \u2212  \u03bbi)2      \u2200 K + 1 \u2264     i \u2264  |X| ,\n                                (1 \u2212  \u03bbK0)2 \u2212     (1 \u2212  \u03bbK+1)2\n                                         (1 \u2212  \u03bbK+1)2\n                         \u03c9s =   (1 \u2212   \u03bbK0)2 \u2212   1(1 \u2212  \u03bbK+1)2 ,\n                         \u03c9\u2206   =  (1 \u2212   \u03bbK0)2 \u2212    (1 \u2212  \u03bbK+1)2 ,\n                                                          29", "md": "Meanwhile, the assumption $$\\Delta < (1 - \\lambda K)^2$$ implies that\n\n\\[\n\\text{rank } P^\\perp U = |X| - K, \\text{ and thus }\n\\]\n\n\\[\n|X| P^\\perp U^2 P^\\perp = \\text{rank } P^\\perp = |X| - K. \\tag{18}\n\\]\n\nOtherwise by contradiction, suppose $$\\text{rank } (PU) = \\text{rank } (UX) < K$$, then Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that\n\n\\[\n\\min W(X) - \\|UX U^T\\|_2^F = |X| (1 - \\lambda_i)^2 > (1 - \\lambda_i)^2 + \\Delta = R_{f|X} + \\Delta,\n\\]\n\nwhich contradicts $$R_{f|X} \\leq R_{f|X} + \\Delta$$.\n\nFurthermore, since $$\\|v_i\\|_2 = 1$$ and $$P^\\perp U$$ is an orthogonal projection, for all $$i \\in [|X|]$$,\n\n\\[\n0 \\leq P^\\perp U v_i^2 \\leq 1. \\tag{19}\n\\]\n\nFor every $$i \\in [|X|]$$, we denote $$\\xi_i \\triangleq P^\\perp U v_i^2$$. $$P^\\perp UVK^2 \\sum_{i=1}^{\\xi_i}$$ can be recast as a linear programming problem:\n\n\\[\n\\begin{array}{ll}\n\\text{(P):} & \\max \\sum_{i=1}^{\\xi_i} \\xi_i \\\\\n& \\xi \\in \\mathbb{R}^{|X|} \\\\\n& \\text{s.t. } 0 \\leq \\xi_i \\leq 1, \\\\\n& \\sum_{i=1}^{\\xi_i} \\xi_i = |X| - K, \\\\\n& |X| (1 - \\lambda_i)^2 \\xi_i \\leq (1 - \\lambda_i)^2 + \\Delta, \\\\\n& \\text{for } i>K.\n\\end{array}\n\\]\n\nwhose dual can be expressed as\n\n\\[\n\\begin{array}{ll}\n\\text{(D):} & \\min \\sum_{i=1}^{\\xi_i} |X| \\omega_i - (|X| - K) \\omega_s + \\Delta + (1 - \\lambda_i)^2 \\omega_\\Delta \\\\\n& (\\omega_1, \\ldots, \\omega_{|X|}, \\omega_s, \\omega_\\Delta) \\\\\n& \\text{s.t. } \\omega_1, \\ldots, \\omega_{|X|}, \\omega_s, \\omega_\\Delta \\geq 0, \\\\\n& \\omega_i - \\omega_s + (1 - \\lambda_i)^2 \\omega_\\Delta \\geq 1 \\quad \\forall i \\leq K, \\\\\n& \\omega_i - \\omega_s + (1 - \\lambda_i)^2 \\omega_\\Delta \\geq 0 \\quad \\forall i > K, \\\\\n& \\omega_i = 0 \\quad \\forall i \\leq K0, \\\\\n& (1 - \\lambda_{K0})^2 - (1 - \\lambda_i)^2 \\omega_i = (1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2 \\quad \\forall K0 < i \\leq K, \\\\\n& \\omega_i = (1 - \\lambda_{K+1})^2 - (1 - \\lambda_i)^2 \\quad \\forall K + 1 \\leq i \\leq |X|, \\\\\n& \\omega_s = (1 - \\lambda_{K0})^2 - 1(1 - \\lambda_{K+1})^2, \\\\\n& \\omega_\\Delta = (1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2.\n\\end{array}\n\\]", "images": [], "items": [{"type": "text", "value": "Meanwhile, the assumption $$\\Delta < (1 - \\lambda K)^2$$ implies that\n\n\\[\n\\text{rank } P^\\perp U = |X| - K, \\text{ and thus }\n\\]\n\n\\[", "md": "Meanwhile, the assumption $$\\Delta < (1 - \\lambda K)^2$$ implies that\n\n\\[\n\\text{rank } P^\\perp U = |X| - K, \\text{ and thus }\n\\]\n\n\\["}, {"type": "table", "rows": [["X", "P^\\perp U^2 P^\\perp = \\text{rank } P^\\perp =", "X"]], "md": "|X| P^\\perp U^2 P^\\perp = \\text{rank } P^\\perp = |X| - K. \\tag{18}", "isPerfectTable": true, "csv": "\"X\",\"P^\\perp U^2 P^\\perp = \\text{rank } P^\\perp =\",\"X\""}, {"type": "text", "value": "\\]\n\nOtherwise by contradiction, suppose $$\\text{rank } (PU) = \\text{rank } (UX) < K$$, then Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that\n\n\\[\n\\min W(X) - \\|UX U^T\\|_2^F = |X| (1 - \\lambda_i)^2 > (1 - \\lambda_i)^2 + \\Delta = R_{f|X} + \\Delta,\n\\]\n\nwhich contradicts $$R_{f|X} \\leq R_{f|X} + \\Delta$$.\n\nFurthermore, since $$\\|v_i\\|_2 = 1$$ and $$P^\\perp U$$ is an orthogonal projection, for all $$i \\in [|X|]$$,\n\n\\[\n0 \\leq P^\\perp U v_i^2 \\leq 1. \\tag{19}\n\\]\n\nFor every $$i \\in [|X|]$$, we denote $$\\xi_i \\triangleq P^\\perp U v_i^2$$. $$P^\\perp UVK^2 \\sum_{i=1}^{\\xi_i}$$ can be recast as a linear programming problem:\n\n\\[\n\\begin{array}{ll}\n\\text{(P):} & \\max \\sum_{i=1}^{\\xi_i} \\xi_i \\\\\n& \\xi \\in \\mathbb{R}^{|X|} \\\\\n& \\text{s.t. } 0 \\leq \\xi_i \\leq 1, \\\\\n& \\sum_{i=1}^{\\xi_i} \\xi_i = |X| - K, \\\\\n& |X| (1 - \\lambda_i)^2 \\xi_i \\leq (1 - \\lambda_i)^2 + \\Delta, \\\\\n& \\text{for } i>K.\n\\end{array}\n\\]\n\nwhose dual can be expressed as\n\n\\[\n\\begin{array}{ll}\n\\text{(D):} & \\min \\sum_{i=1}^{\\xi_i} |X| \\omega_i - (|X| - K) \\omega_s + \\Delta + (1 - \\lambda_i)^2 \\omega_\\Delta \\\\\n& (\\omega_1, \\ldots, \\omega_{|X|}, \\omega_s, \\omega_\\Delta) \\\\\n& \\text{s.t. } \\omega_1, \\ldots, \\omega_{|X|}, \\omega_s, \\omega_\\Delta \\geq 0, \\\\\n& \\omega_i - \\omega_s + (1 - \\lambda_i)^2 \\omega_\\Delta \\geq 1 \\quad \\forall i \\leq K, \\\\\n& \\omega_i - \\omega_s + (1 - \\lambda_i)^2 \\omega_\\Delta \\geq 0 \\quad \\forall i > K, \\\\\n& \\omega_i = 0 \\quad \\forall i \\leq K0, \\\\\n& (1 - \\lambda_{K0})^2 - (1 - \\lambda_i)^2 \\omega_i = (1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2 \\quad \\forall K0 < i \\leq K, \\\\\n& \\omega_i = (1 - \\lambda_{K+1})^2 - (1 - \\lambda_i)^2 \\quad \\forall K + 1 \\leq i \\leq |X|, \\\\\n& \\omega_s = (1 - \\lambda_{K0})^2 - 1(1 - \\lambda_{K+1})^2, \\\\\n& \\omega_\\Delta = (1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2.\n\\end{array}\n\\]", "md": "\\]\n\nOtherwise by contradiction, suppose $$\\text{rank } (PU) = \\text{rank } (UX) < K$$, then Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that\n\n\\[\n\\min W(X) - \\|UX U^T\\|_2^F = |X| (1 - \\lambda_i)^2 > (1 - \\lambda_i)^2 + \\Delta = R_{f|X} + \\Delta,\n\\]\n\nwhich contradicts $$R_{f|X} \\leq R_{f|X} + \\Delta$$.\n\nFurthermore, since $$\\|v_i\\|_2 = 1$$ and $$P^\\perp U$$ is an orthogonal projection, for all $$i \\in [|X|]$$,\n\n\\[\n0 \\leq P^\\perp U v_i^2 \\leq 1. \\tag{19}\n\\]\n\nFor every $$i \\in [|X|]$$, we denote $$\\xi_i \\triangleq P^\\perp U v_i^2$$. $$P^\\perp UVK^2 \\sum_{i=1}^{\\xi_i}$$ can be recast as a linear programming problem:\n\n\\[\n\\begin{array}{ll}\n\\text{(P):} & \\max \\sum_{i=1}^{\\xi_i} \\xi_i \\\\\n& \\xi \\in \\mathbb{R}^{|X|} \\\\\n& \\text{s.t. } 0 \\leq \\xi_i \\leq 1, \\\\\n& \\sum_{i=1}^{\\xi_i} \\xi_i = |X| - K, \\\\\n& |X| (1 - \\lambda_i)^2 \\xi_i \\leq (1 - \\lambda_i)^2 + \\Delta, \\\\\n& \\text{for } i>K.\n\\end{array}\n\\]\n\nwhose dual can be expressed as\n\n\\[\n\\begin{array}{ll}\n\\text{(D):} & \\min \\sum_{i=1}^{\\xi_i} |X| \\omega_i - (|X| - K) \\omega_s + \\Delta + (1 - \\lambda_i)^2 \\omega_\\Delta \\\\\n& (\\omega_1, \\ldots, \\omega_{|X|}, \\omega_s, \\omega_\\Delta) \\\\\n& \\text{s.t. } \\omega_1, \\ldots, \\omega_{|X|}, \\omega_s, \\omega_\\Delta \\geq 0, \\\\\n& \\omega_i - \\omega_s + (1 - \\lambda_i)^2 \\omega_\\Delta \\geq 1 \\quad \\forall i \\leq K, \\\\\n& \\omega_i - \\omega_s + (1 - \\lambda_i)^2 \\omega_\\Delta \\geq 0 \\quad \\forall i > K, \\\\\n& \\omega_i = 0 \\quad \\forall i \\leq K0, \\\\\n& (1 - \\lambda_{K0})^2 - (1 - \\lambda_i)^2 \\omega_i = (1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2 \\quad \\forall K0 < i \\leq K, \\\\\n& \\omega_i = (1 - \\lambda_{K+1})^2 - (1 - \\lambda_i)^2 \\quad \\forall K + 1 \\leq i \\leq |X|, \\\\\n& \\omega_s = (1 - \\lambda_{K0})^2 - 1(1 - \\lambda_{K+1})^2, \\\\\n& \\omega_\\Delta = (1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2.\n\\end{array}\n\\]"}]}, {"page": 30, "text": "the dual optimum can be upper bounded by\n                       K\n              (D) ==  i=1 \u03c9i + \u2206   \u00b7\u2206\u03c9\u2206                    K       (1 \u2212  \u03bbK0)2 \u2212   (1 \u2212  \u03bbi)2\n                      (1 \u2212 \u03bbK0)2 \u2212    (1 \u2212 \u03bbK+1)2 +     i=K0+1   (1 \u2212  \u03bbK0)2 \u2212   (1 \u2212  \u03bbK+1)2\n                        (1 + (K \u2212   K0) CK0) \u2206\n                   \u2264  (1 \u2212 \u03bbK0)2 \u2212    (1 \u2212 \u03bbK+1)2 ,\n                         2\ngiven CK   0 \u225c  (1\u2212\u03bbK0)  \u2206\u2212(1\u2212\u03bbK)2    = O(1).\nWith both the primal Equation (20) and the dual Equation (21) being feasible and bounded, the strong\nduality then implies that\n                  P\u22a5 UVK     2      K  \u03bei = (P) = (D) \u2264        (1 + (K \u2212    K0) CK0) \u2206\n                             F =   i=1                       (1 \u2212  \u03bbK0)2 \u2212   (1 \u2212  \u03bbK+1)2 ,\nwhich, when plugging back into Equation (16), completes the proof.\nE     Proofs and Discussions for Section 6\nE.1    Expansion-based Data Augmentation\nFor the detailed analysis, we further recall the following notion of constant expansion and its relation\nwith the concept of c-expansion in Definition 6.1 from Wei et al. [2021].\nDefinition E.1 ((q, \u03be)-constant expansion [Wei et al., 2021]). We say the expansion-based data\naugmentation (Definition 6.1) satisfies (q, \u03be)-constant expansion if for any S \u2286      X such that P(S) \u2265    q\nand P (S \u2229    Xk) \u2264   P (Xk) /2 for all k \u2208   [K],\n                                 P (NB(S)) > min {P(S), \u03be} + P(S).\nLemma E.1 (c-expansion implies constant expansion [Wei et al., 2021]). The c-expansion property\n(Definition 6.1) implies      \u03be      -constant expansion (Definition E.1) for any \u03be > 0.\n                            c\u22121, \u03be\nProof of Lemma E.1. Consider a subset S \u2286          X such that P(S) \u2265      q and P (S \u2229   Xk) \u2264   P (Xk) /2.\nLet Sk \u225c   S \u2229  Xk for every k \u2208    [K].\n   (i) If c \u2208 (1, 2), since P (Xk) \u2212    P (Sk) \u2265   P (Sk) \u2265   (c \u2212  1)P (Sk)\n                                          K\n                    P (NB (S) \\ S) \u2265     k=1  P (NB (Sk)) \u2212     P (Sk)\n                                          K\n                                       > k=1  min {(c \u2212   1)P (Sk) , P (Xk) \u2212    P (Sk)}\n                                          K\n                                       \u2265 k=1 (c \u2212  1)P(Sk) = (c \u2212     1)P(S) \u2265    (c \u2212  1)q = \u03be\n  (ii) If c \u2265  2, since c \u2212 1 \u2265  1 and P (Xk) \u2212    P (Sk) \u2265    P (Sk), we have\n                                 K                                                   K\n           P (NB (S) \\ S) >     k=1 min {(c \u2212   1)P (Sk) , P (Xk) \u2212    P (Sk)} \u2265    k=1  P (Sk) = P(S)\nOverall, we have\n                  P (NB(S)) = P (NB (S) \\ S) + P(S) > min {P(S), \u03be} + P(S).\n                                                      30", "md": "# Math Equations\n\nthe dual optimum can be upper bounded by\n\n$$\n\\begin{align*}\n(D) & = \\sum_{i=1}^{K} \\omega_i + \\Delta \\cdot \\Delta \\omega \\Delta \\\\\n& \\leq \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2} + \\sum_{i=K0+1} \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{2} \\\\\n& \\leq \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{2},\n\\end{align*}\n$$\ngiven $C_{K0} \\triangleq (1 - \\lambda_{K0}) \\Delta - (1 - \\lambda_{K})^2 = O(1)$.\nWith both the primal Equation (20) and the dual Equation (21) being feasible and bounded, the strong duality then implies that\n$$\nP^{\\perp} UVK F = \\sum_{i=1}^{K} \\xi_i = (P) = (D) \\leq \\frac{1 + (K - K0) C_{K0} \\Delta}{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2},\n$$\nwhich, when plugging back into Equation (16), completes the proof.\n\nE Proofs and Discussions for Section 6\n\nE.1 Expansion-based Data Augmentation\n\nFor the detailed analysis, we further recall the following notion of constant expansion and its relation with the concept of c-expansion in Definition 6.1 from Wei et al. [2021].\n\nDefinition E.1 ((q, \u03be)-constant expansion [Wei et al., 2021])\n\nWe say the expansion-based data augmentation (Definition 6.1) satisfies (q, \u03be)-constant expansion if for any $S \\subseteq X$ such that $P(S) \\geq q$ and $P(S \\cap X_k) \\leq P(X_k) / 2$ for all $k \\in [K]$,\n$$\nP(NB(S)) > \\min\\{P(S), \\xi\\} + P(S).\n$$\n\nLemma E.1 (c-expansion implies constant expansion [Wei et al., 2021])\n\nThe c-expansion property (Definition 6.1) implies $\\xi$-constant expansion (Definition E.1) for any $\\xi > 0$.\n\nProof of Lemma E.1\n\nConsider a subset $S \\subseteq X$ such that $P(S) \\geq q$ and $P(S \\cap X_k) \\leq P(X_k) / 2$. Let $S_k = S \\cap X_k$ for every $k \\in [K]$.\n1. If $c \\in (1, 2)$, since $P(X_k) - P(S_k) \\geq P(S_k) \\geq (c - 1)P(S_k)$,\n$\nP(NB(S) \\backslash S) \\geq \\sum_{k=1}^{K} P(NB(S_k)) - P(S_k) > \\sum_{k=1}^{K} \\min\\{(c - 1)P(S_k), P(X_k) - P(S_k)\\} \\geq \\sum_{k=1}^{K} (c - 1)P(S_k) = (c - 1)P(S) \\geq (c - 1)q = \\xi.\n$\n\nOverall, we have\n$$\nP(NB(S)) = P(NB(S) \\backslash S) + P(S) > \\min\\{P(S), \\xi\\} + P(S).\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "the dual optimum can be upper bounded by\n\n$$\n\\begin{align*}\n(D) & = \\sum_{i=1}^{K} \\omega_i + \\Delta \\cdot \\Delta \\omega \\Delta \\\\\n& \\leq \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2} + \\sum_{i=K0+1} \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{2} \\\\\n& \\leq \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{2},\n\\end{align*}\n$$\ngiven $C_{K0} \\triangleq (1 - \\lambda_{K0}) \\Delta - (1 - \\lambda_{K})^2 = O(1)$.\nWith both the primal Equation (20) and the dual Equation (21) being feasible and bounded, the strong duality then implies that\n$$\nP^{\\perp} UVK F = \\sum_{i=1}^{K} \\xi_i = (P) = (D) \\leq \\frac{1 + (K - K0) C_{K0} \\Delta}{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2},\n$$\nwhich, when plugging back into Equation (16), completes the proof.\n\nE Proofs and Discussions for Section 6\n\nE.1 Expansion-based Data Augmentation\n\nFor the detailed analysis, we further recall the following notion of constant expansion and its relation with the concept of c-expansion in Definition 6.1 from Wei et al. [2021].\n\nDefinition E.1 ((q, \u03be)-constant expansion [Wei et al., 2021])\n\nWe say the expansion-based data augmentation (Definition 6.1) satisfies (q, \u03be)-constant expansion if for any $S \\subseteq X$ such that $P(S) \\geq q$ and $P(S \\cap X_k) \\leq P(X_k) / 2$ for all $k \\in [K]$,\n$$\nP(NB(S)) > \\min\\{P(S), \\xi\\} + P(S).\n$$\n\nLemma E.1 (c-expansion implies constant expansion [Wei et al., 2021])\n\nThe c-expansion property (Definition 6.1) implies $\\xi$-constant expansion (Definition E.1) for any $\\xi > 0$.\n\nProof of Lemma E.1\n\nConsider a subset $S \\subseteq X$ such that $P(S) \\geq q$ and $P(S \\cap X_k) \\leq P(X_k) / 2$. Let $S_k = S \\cap X_k$ for every $k \\in [K]$.\n1. If $c \\in (1, 2)$, since $P(X_k) - P(S_k) \\geq P(S_k) \\geq (c - 1)P(S_k)$,\n$\nP(NB(S) \\backslash S) \\geq \\sum_{k=1}^{K} P(NB(S_k)) - P(S_k) > \\sum_{k=1}^{K} \\min\\{(c - 1)P(S_k), P(X_k) - P(S_k)\\} \\geq \\sum_{k=1}^{K} (c - 1)P(S_k) = (c - 1)P(S) \\geq (c - 1)q = \\xi.\n$\n\nOverall, we have\n$$\nP(NB(S)) = P(NB(S) \\backslash S) + P(S) > \\min\\{P(S), \\xi\\} + P(S).\n$$", "md": "the dual optimum can be upper bounded by\n\n$$\n\\begin{align*}\n(D) & = \\sum_{i=1}^{K} \\omega_i + \\Delta \\cdot \\Delta \\omega \\Delta \\\\\n& \\leq \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2} + \\sum_{i=K0+1} \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{2} \\\\\n& \\leq \\frac{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2}{2},\n\\end{align*}\n$$\ngiven $C_{K0} \\triangleq (1 - \\lambda_{K0}) \\Delta - (1 - \\lambda_{K})^2 = O(1)$.\nWith both the primal Equation (20) and the dual Equation (21) being feasible and bounded, the strong duality then implies that\n$$\nP^{\\perp} UVK F = \\sum_{i=1}^{K} \\xi_i = (P) = (D) \\leq \\frac{1 + (K - K0) C_{K0} \\Delta}{(1 - \\lambda_{K0})^2 - (1 - \\lambda_{K+1})^2},\n$$\nwhich, when plugging back into Equation (16), completes the proof.\n\nE Proofs and Discussions for Section 6\n\nE.1 Expansion-based Data Augmentation\n\nFor the detailed analysis, we further recall the following notion of constant expansion and its relation with the concept of c-expansion in Definition 6.1 from Wei et al. [2021].\n\nDefinition E.1 ((q, \u03be)-constant expansion [Wei et al., 2021])\n\nWe say the expansion-based data augmentation (Definition 6.1) satisfies (q, \u03be)-constant expansion if for any $S \\subseteq X$ such that $P(S) \\geq q$ and $P(S \\cap X_k) \\leq P(X_k) / 2$ for all $k \\in [K]$,\n$$\nP(NB(S)) > \\min\\{P(S), \\xi\\} + P(S).\n$$\n\nLemma E.1 (c-expansion implies constant expansion [Wei et al., 2021])\n\nThe c-expansion property (Definition 6.1) implies $\\xi$-constant expansion (Definition E.1) for any $\\xi > 0$.\n\nProof of Lemma E.1\n\nConsider a subset $S \\subseteq X$ such that $P(S) \\geq q$ and $P(S \\cap X_k) \\leq P(X_k) / 2$. Let $S_k = S \\cap X_k$ for every $k \\in [K]$.\n1. If $c \\in (1, 2)$, since $P(X_k) - P(S_k) \\geq P(S_k) \\geq (c - 1)P(S_k)$,\n$\nP(NB(S) \\backslash S) \\geq \\sum_{k=1}^{K} P(NB(S_k)) - P(S_k) > \\sum_{k=1}^{K} \\min\\{(c - 1)P(S_k), P(X_k) - P(S_k)\\} \\geq \\sum_{k=1}^{K} (c - 1)P(S_k) = (c - 1)P(S) \\geq (c - 1)q = \\xi.\n$\n\nOverall, we have\n$$\nP(NB(S)) = P(NB(S) \\backslash S) + P(S) > \\min\\{P(S), \\xi\\} + P(S).\n$$"}]}, {"page": 31, "text": " E.2     Data Augmentation Consistency Regularization\n To bridge Equation (8) with common DAC regularization algorithms in practice, in Example E.1,\n we instantiate FixMatch [Sohn et al., 2020] \u2013 a state-of-the-art semi-supervised learning algorithm\n that leverages DAC regularization by encouraging similar predictions of proper weak and strong data\n augmentations xw, xs \u2208            A(x) of the same sample x.\n Example E.1 (FixMatch [Sohn et al., 2020]). FixMatch minimizes the loss between the outputs of\n the strong augmentation f(xs) and the pseudo-label of the weak augmentation yf (xw) when the\n confidence of such pseudo-labeling is high (i.e., maxk\u2208[K] f (xw)k is larger than a threshold).\nTo connect FixMatch with Equation (8), we consider a fixed (unknown) margin-robust neighborhood\n F\u2032 \u2287     F\u03c4Xu \u220b     f\u2217   where the strong augmentation characterizes the worst-case (minimum) robust\n margin xs \u2208         f\u2208F\u2032 argminx\u2032\u2208A(x) m (f, x\u2032, yf(x)); while the weak augmentation preserves the\n classification yf (xw) = yf (x).\nThen, for any f \u2208         F\u2032 during learning, minimizing the cross-entropy loss between softmax (f(xs))\n                              \u2192\n and the one-hot label \u2212      yf (xw) is equivalent to enforcing a large enough margin m (f, xs, yf(x)) \u2265                              \u03c4,\n whereas m (f, x\u2032, yf(x)) \u2265               m (f, xs, yf(x)) \u2265            \u03c4 for all x\u2032 \u2208        A(x) by construction implies\n mA (f, x) \u2265       \u03c4 as required by the constraints in Equation (8).\n E.3     Proof of Theorem 6.2\n Proof of Theorem 6.2. We first recall from Lemma E.1 that c-expansion implies                                      2\u03bd(f)               -\n                                                                                                                      c\u22121 , 2\u03bd(f)\n constant expansion (Definition E.1) for any f \u2208                      F. Therefore, it is suffi         cient to show that with\n(q, 2\u03bd(f))-constant expansion (i.e., q = 2\u03bd(f)           c\u22121 ),\n                                      P (M(f)) \u2264         max {q, 2\u03bd(f)}            \u2200  f \u2208   F\u03c4Xu.                                  (22)\n For any f \u2208        F\u03c4Xu, we notice that Equation (22) is automatically satisfied when P (M(f)) < q.\n Otherwise, when P (M(f)) \u2265                    q, then along with P (S \u2229              Xk) \u2264       P (Xk) /2 for all k \u2208            [K],\n(q, 2\u03bd(f))-constant expansion implies\n                            P (NB (M(f))) > min {P (M(f)) , 2\u03bd(f)} + P (M(f)) .                                                    (23)\n Meanwhile, we observe that by union bound,\n                 P (NB (M(f)) \\ M(f)) \u2264                 2P [\u2203   x\u2032 \u2208   A(x) s.t. yf(x) \u0338= yf (x\u2032)] = 2\u03bd(f)                         (24)\n because for any x \u2208           NB (M(f)) \\ M(f), there exists x\u2032 \u2208                     M(f) such that one can find some\n x\u2032\u2032 \u2208   A(x) \u2229     A (x\u2032); and therefore exactly one of the follows must hold\n    (i) if x\u2032\u2032 \u2208    M(f), then       yf (x\u2032\u2032) \u0338= y\u2217     (x) =    yf (x) implies yf (x\u2032\u2032) \u0338= yf (x) for x\u2032\u2032 \u2208                A (x);\n   (ii) if x\u2032\u2032 / \u2208  M(f), then       yf (x\u2032\u2032) = y\u2217      (x) \u0338=   yf (x\u2032) implies yf (x\u2032\u2032) \u0338= yf (x\u2032) for x\u2032\u2032 \u2208               A (x\u2032).\n Leveraging Equation (23) and Equation (24), we have\n                          P (M(f)) \u2265P (NB (M(f))) \u2212                    P (NB (M(f)) \\ M(f))\n                                          > min {P (M(f)) , 2\u03bd(f)} + P (M(f)) \u2212                       2\u03bd(f),\n which leads to min {P (M(f)) , 2\u03bd(f)} < 2\u03bd(f) and implies P (M(f)) < 2\u03bd(f).\n F     Technical Lemmas\n Here, we briefly review the standard Rademacher-complexity-based generalization analysis based on\n McDiarmid\u2019s inequality and a classical symmetrization argument Wainwright [2019], Bartlett and\n Mendelson [2003].\n Lemma F.1 (Generalization guarantee with Rademacher complexity). Given a distribution over a\n set Z13, we consider a set of i.i.d. samples {zi}i\u2208[n] \u223c                   Zn with Z = [z1, . . . , zn] and a B-bounded\n function class H = {h : Z \u2192               R | |h(z) \u2212     h(z\u2032)| \u2264   n B \u2200    z, z\u2032 \u2208   Z}. By denoting\n         H(h) \u225c      Ez\u223cZ [h(z)]          and     HZ(h) \u225c        1        h(zi)      s.t.   EZ\u223cZn        HZ(h)       = H(h),\n                                                                 n  i=1\n    13Without ambiguity, we overload the notation Z as the set as well as the distribution over the set.\n                                                                   31", "md": "# Document\n\n## E.2 Data Augmentation Consistency Regularization\n\nTo bridge Equation (8) with common DAC regularization algorithms in practice, in Example E.1, we instantiate FixMatch [Sohn et al., 2020] \u2013 a state-of-the-art semi-supervised learning algorithm that leverages DAC regularization by encouraging similar predictions of proper weak and strong data augmentations \\(x_w, x_s \\in A(x)\\) of the same sample \\(x\\).\n\n### Example E.1 (FixMatch [Sohn et al., 2020])\n\nFixMatch minimizes the loss between the outputs of the strong augmentation \\(f(x_s)\\) and the pseudo-label of the weak augmentation \\(y_f(x_w)\\) when the confidence of such pseudo-labeling is high (i.e., \\(\\max_{k \\in [K]} f(x_w)_k\\) is larger than a threshold).\n\nTo connect FixMatch with Equation (8), we consider a fixed (unknown) margin-robust neighborhood \\(F' \\supseteq F_{\\tau}^{X_u} \\ni f^*\\) where the strong augmentation characterizes the worst-case (minimum) robust margin \\(x_s \\in \\arg\\min_{x' \\in A(x)} m(f, x', y_f(x))\\); while the weak augmentation preserves the classification \\(y_f(x_w) = y_f(x)\\).\n\nThen, for any \\(f \\in F'\\) during learning, minimizing the cross-entropy loss between softmax(\\(f(x_s)\\)) and the one-hot label \\(-y_f(x_w)\\) is equivalent to enforcing a large enough margin \\(m(f, x_s, y_f(x)) \\geq \\tau\\), whereas \\(m(f, x', y_f(x)) \\geq m(f, x_s, y_f(x)) \\geq \\tau\\) for all \\(x' \\in A(x)\\) by construction implies \\(m_A(f, x) \\geq \\tau\\) as required by the constraints in Equation (8).\n\n## E.3 Proof of Theorem 6.2\n\nProof of Theorem 6.2. We first recall from Lemma E.1 that c-expansion implies \\(2\\nu(f) \\geq c-1, 2\\nu(f)\\) constant expansion (Definition E.1) for any \\(f \\in F\\). Therefore, it is sufficient to show that with \\((q, 2\\nu(f))\\)-constant expansion (i.e., \\(q = 2\\nu(f) \\geq c-1\\)),\n\n\\[P(M(f)) \\leq \\max\\{q, 2\\nu(f)\\} \\quad \\forall f \\in F_{\\tau}^{X_u}.\\] (22)\n\nFor any \\(f \\in F_{\\tau}^{X_u}\\), we notice that Equation (22) is automatically satisfied when \\(P(M(f)) < q\\). Otherwise, when \\(P(M(f)) \\geq q\\), then along with \\(P(S \\cap X_k) \\leq P(X_k)/2\\) for all \\(k \\in [K]\\), \\((q, 2\\nu(f))\\)-constant expansion implies\n\n\\[P(NB(M(f))) > \\min\\{P(M(f)), 2\\nu(f)\\} + P(M(f)).\\] (23)\n\nMeanwhile, we observe that by union bound,\n\n\\[P(NB(M(f)) \\backslash M(f)) \\leq 2P[\\exists x' \\in A(x) \\text{ s.t. } y_f(x) \\neq y_f(x')] = 2\\nu(f)\\] (24)\n\nbecause for any \\(x \\in NB(M(f)) \\backslash M(f)\\), there exists \\(x' \\in M(f)\\) such that one can find some \\(x'' \\in A(x) \\cap A(x')\\); and therefore exactly one of the follows must hold\n\n1. if \\(x'' \\in M(f)\\), then \\(y_f(x'') \\neq y^*(x) = y_f(x)\\) implies \\(y_f(x'') \\neq y_f(x)\\) for \\(x'' \\in A(x)\\);\n2. if \\(x'' \\notin M(f)\\), then \\(y_f(x'') = y^*(x) \\neq y_f(x')\\) implies \\(y_f(x'') \\neq y_f(x')\\) for \\(x'' \\in A(x')\\).\n\nLeveraging Equation (23) and Equation (24), we have\n\n\\[P(M(f)) \\geq P(NB(M(f))) - P(NB(M(f)) \\backslash M(f))\\]\n\n\\[> \\min\\{P(M(f)), 2\\nu(f)\\} + P(M(f)) - 2\\nu(f),\\]\n\nwhich leads to \\(\\min\\{P(M(f)), 2\\nu(f)\\} < 2\\nu(f)\\) and implies \\(P(M(f)) < 2\\nu(f)\\).\n\n## F Technical Lemmas\n\nHere, we briefly review the standard Rademacher-complexity-based generalization analysis based on McDiarmid\u2019s inequality and a classical symmetrization argument Wainwright [2019], Bartlett and Mendelson [2003].\n\n### Lemma F.1 (Generalization guarantee with Rademacher complexity)\n\nGiven a distribution over a set \\(Z^{13}\\), we consider a set of i.i.d. samples \\(\\{z_i\\}_{i \\in [n]} \\sim Z^n\\) with \\(Z = [z_1, ..., z_n]\\) and a \\(B\\)-bounded function class \\(H = \\{h : Z \\rightarrow R \\,|\\, |h(z) - h(z')| \\leq nB \\, \\forall z, z' \\in Z\\}\\). By denoting\n\n\\[H(h) \\triangleq \\mathbb{E}_{z \\sim Z}[h(z)] \\quad \\text{and} \\quad H_Z(h) \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} h(z_i) \\quad \\text{s.t.} \\quad \\mathbb{E}_{Z^n} [HZ(h)] = H(h),\\]\n\nWithout ambiguity, we overload the notation \\(Z\\) as the set as well as the distribution over the set.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "E.2 Data Augmentation Consistency Regularization", "md": "## E.2 Data Augmentation Consistency Regularization"}, {"type": "text", "value": "To bridge Equation (8) with common DAC regularization algorithms in practice, in Example E.1, we instantiate FixMatch [Sohn et al., 2020] \u2013 a state-of-the-art semi-supervised learning algorithm that leverages DAC regularization by encouraging similar predictions of proper weak and strong data augmentations \\(x_w, x_s \\in A(x)\\) of the same sample \\(x\\).", "md": "To bridge Equation (8) with common DAC regularization algorithms in practice, in Example E.1, we instantiate FixMatch [Sohn et al., 2020] \u2013 a state-of-the-art semi-supervised learning algorithm that leverages DAC regularization by encouraging similar predictions of proper weak and strong data augmentations \\(x_w, x_s \\in A(x)\\) of the same sample \\(x\\)."}, {"type": "heading", "lvl": 3, "value": "Example E.1 (FixMatch [Sohn et al., 2020])", "md": "### Example E.1 (FixMatch [Sohn et al., 2020])"}, {"type": "text", "value": "FixMatch minimizes the loss between the outputs of the strong augmentation \\(f(x_s)\\) and the pseudo-label of the weak augmentation \\(y_f(x_w)\\) when the confidence of such pseudo-labeling is high (i.e., \\(\\max_{k \\in [K]} f(x_w)_k\\) is larger than a threshold).\n\nTo connect FixMatch with Equation (8), we consider a fixed (unknown) margin-robust neighborhood \\(F' \\supseteq F_{\\tau}^{X_u} \\ni f^*\\) where the strong augmentation characterizes the worst-case (minimum) robust margin \\(x_s \\in \\arg\\min_{x' \\in A(x)} m(f, x', y_f(x))\\); while the weak augmentation preserves the classification \\(y_f(x_w) = y_f(x)\\).\n\nThen, for any \\(f \\in F'\\) during learning, minimizing the cross-entropy loss between softmax(\\(f(x_s)\\)) and the one-hot label \\(-y_f(x_w)\\) is equivalent to enforcing a large enough margin \\(m(f, x_s, y_f(x)) \\geq \\tau\\), whereas \\(m(f, x', y_f(x)) \\geq m(f, x_s, y_f(x)) \\geq \\tau\\) for all \\(x' \\in A(x)\\) by construction implies \\(m_A(f, x) \\geq \\tau\\) as required by the constraints in Equation (8).", "md": "FixMatch minimizes the loss between the outputs of the strong augmentation \\(f(x_s)\\) and the pseudo-label of the weak augmentation \\(y_f(x_w)\\) when the confidence of such pseudo-labeling is high (i.e., \\(\\max_{k \\in [K]} f(x_w)_k\\) is larger than a threshold).\n\nTo connect FixMatch with Equation (8), we consider a fixed (unknown) margin-robust neighborhood \\(F' \\supseteq F_{\\tau}^{X_u} \\ni f^*\\) where the strong augmentation characterizes the worst-case (minimum) robust margin \\(x_s \\in \\arg\\min_{x' \\in A(x)} m(f, x', y_f(x))\\); while the weak augmentation preserves the classification \\(y_f(x_w) = y_f(x)\\).\n\nThen, for any \\(f \\in F'\\) during learning, minimizing the cross-entropy loss between softmax(\\(f(x_s)\\)) and the one-hot label \\(-y_f(x_w)\\) is equivalent to enforcing a large enough margin \\(m(f, x_s, y_f(x)) \\geq \\tau\\), whereas \\(m(f, x', y_f(x)) \\geq m(f, x_s, y_f(x)) \\geq \\tau\\) for all \\(x' \\in A(x)\\) by construction implies \\(m_A(f, x) \\geq \\tau\\) as required by the constraints in Equation (8)."}, {"type": "heading", "lvl": 2, "value": "E.3 Proof of Theorem 6.2", "md": "## E.3 Proof of Theorem 6.2"}, {"type": "text", "value": "Proof of Theorem 6.2. We first recall from Lemma E.1 that c-expansion implies \\(2\\nu(f) \\geq c-1, 2\\nu(f)\\) constant expansion (Definition E.1) for any \\(f \\in F\\). Therefore, it is sufficient to show that with \\((q, 2\\nu(f))\\)-constant expansion (i.e., \\(q = 2\\nu(f) \\geq c-1\\)),\n\n\\[P(M(f)) \\leq \\max\\{q, 2\\nu(f)\\} \\quad \\forall f \\in F_{\\tau}^{X_u}.\\] (22)\n\nFor any \\(f \\in F_{\\tau}^{X_u}\\), we notice that Equation (22) is automatically satisfied when \\(P(M(f)) < q\\). Otherwise, when \\(P(M(f)) \\geq q\\), then along with \\(P(S \\cap X_k) \\leq P(X_k)/2\\) for all \\(k \\in [K]\\), \\((q, 2\\nu(f))\\)-constant expansion implies\n\n\\[P(NB(M(f))) > \\min\\{P(M(f)), 2\\nu(f)\\} + P(M(f)).\\] (23)\n\nMeanwhile, we observe that by union bound,\n\n\\[P(NB(M(f)) \\backslash M(f)) \\leq 2P[\\exists x' \\in A(x) \\text{ s.t. } y_f(x) \\neq y_f(x')] = 2\\nu(f)\\] (24)\n\nbecause for any \\(x \\in NB(M(f)) \\backslash M(f)\\), there exists \\(x' \\in M(f)\\) such that one can find some \\(x'' \\in A(x) \\cap A(x')\\); and therefore exactly one of the follows must hold\n\n1. if \\(x'' \\in M(f)\\), then \\(y_f(x'') \\neq y^*(x) = y_f(x)\\) implies \\(y_f(x'') \\neq y_f(x)\\) for \\(x'' \\in A(x)\\);\n2. if \\(x'' \\notin M(f)\\), then \\(y_f(x'') = y^*(x) \\neq y_f(x')\\) implies \\(y_f(x'') \\neq y_f(x')\\) for \\(x'' \\in A(x')\\).\n\nLeveraging Equation (23) and Equation (24), we have\n\n\\[P(M(f)) \\geq P(NB(M(f))) - P(NB(M(f)) \\backslash M(f))\\]\n\n\\[> \\min\\{P(M(f)), 2\\nu(f)\\} + P(M(f)) - 2\\nu(f),\\]\n\nwhich leads to \\(\\min\\{P(M(f)), 2\\nu(f)\\} < 2\\nu(f)\\) and implies \\(P(M(f)) < 2\\nu(f)\\).", "md": "Proof of Theorem 6.2. We first recall from Lemma E.1 that c-expansion implies \\(2\\nu(f) \\geq c-1, 2\\nu(f)\\) constant expansion (Definition E.1) for any \\(f \\in F\\). Therefore, it is sufficient to show that with \\((q, 2\\nu(f))\\)-constant expansion (i.e., \\(q = 2\\nu(f) \\geq c-1\\)),\n\n\\[P(M(f)) \\leq \\max\\{q, 2\\nu(f)\\} \\quad \\forall f \\in F_{\\tau}^{X_u}.\\] (22)\n\nFor any \\(f \\in F_{\\tau}^{X_u}\\), we notice that Equation (22) is automatically satisfied when \\(P(M(f)) < q\\). Otherwise, when \\(P(M(f)) \\geq q\\), then along with \\(P(S \\cap X_k) \\leq P(X_k)/2\\) for all \\(k \\in [K]\\), \\((q, 2\\nu(f))\\)-constant expansion implies\n\n\\[P(NB(M(f))) > \\min\\{P(M(f)), 2\\nu(f)\\} + P(M(f)).\\] (23)\n\nMeanwhile, we observe that by union bound,\n\n\\[P(NB(M(f)) \\backslash M(f)) \\leq 2P[\\exists x' \\in A(x) \\text{ s.t. } y_f(x) \\neq y_f(x')] = 2\\nu(f)\\] (24)\n\nbecause for any \\(x \\in NB(M(f)) \\backslash M(f)\\), there exists \\(x' \\in M(f)\\) such that one can find some \\(x'' \\in A(x) \\cap A(x')\\); and therefore exactly one of the follows must hold\n\n1. if \\(x'' \\in M(f)\\), then \\(y_f(x'') \\neq y^*(x) = y_f(x)\\) implies \\(y_f(x'') \\neq y_f(x)\\) for \\(x'' \\in A(x)\\);\n2. if \\(x'' \\notin M(f)\\), then \\(y_f(x'') = y^*(x) \\neq y_f(x')\\) implies \\(y_f(x'') \\neq y_f(x')\\) for \\(x'' \\in A(x')\\).\n\nLeveraging Equation (23) and Equation (24), we have\n\n\\[P(M(f)) \\geq P(NB(M(f))) - P(NB(M(f)) \\backslash M(f))\\]\n\n\\[> \\min\\{P(M(f)), 2\\nu(f)\\} + P(M(f)) - 2\\nu(f),\\]\n\nwhich leads to \\(\\min\\{P(M(f)), 2\\nu(f)\\} < 2\\nu(f)\\) and implies \\(P(M(f)) < 2\\nu(f)\\)."}, {"type": "heading", "lvl": 2, "value": "F Technical Lemmas", "md": "## F Technical Lemmas"}, {"type": "text", "value": "Here, we briefly review the standard Rademacher-complexity-based generalization analysis based on McDiarmid\u2019s inequality and a classical symmetrization argument Wainwright [2019], Bartlett and Mendelson [2003].", "md": "Here, we briefly review the standard Rademacher-complexity-based generalization analysis based on McDiarmid\u2019s inequality and a classical symmetrization argument Wainwright [2019], Bartlett and Mendelson [2003]."}, {"type": "heading", "lvl": 3, "value": "Lemma F.1 (Generalization guarantee with Rademacher complexity)", "md": "### Lemma F.1 (Generalization guarantee with Rademacher complexity)"}, {"type": "text", "value": "Given a distribution over a set \\(Z^{13}\\), we consider a set of i.i.d. samples \\(\\{z_i\\}_{i \\in [n]} \\sim Z^n\\) with \\(Z = [z_1, ..., z_n]\\) and a \\(B\\)-bounded function class \\(H = \\{h : Z \\rightarrow R \\,|\\, |h(z) - h(z')| \\leq nB \\, \\forall z, z' \\in Z\\}\\). By denoting\n\n\\[H(h) \\triangleq \\mathbb{E}_{z \\sim Z}[h(z)] \\quad \\text{and} \\quad H_Z(h) \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} h(z_i) \\quad \\text{s.t.} \\quad \\mathbb{E}_{Z^n} [HZ(h)] = H(h),\\]\n\nWithout ambiguity, we overload the notation \\(Z\\) as the set as well as the distribution over the set.", "md": "Given a distribution over a set \\(Z^{13}\\), we consider a set of i.i.d. samples \\(\\{z_i\\}_{i \\in [n]} \\sim Z^n\\) with \\(Z = [z_1, ..., z_n]\\) and a \\(B\\)-bounded function class \\(H = \\{h : Z \\rightarrow R \\,|\\, |h(z) - h(z')| \\leq nB \\, \\forall z, z' \\in Z\\}\\). By denoting\n\n\\[H(h) \\triangleq \\mathbb{E}_{z \\sim Z}[h(z)] \\quad \\text{and} \\quad H_Z(h) \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} h(z_i) \\quad \\text{s.t.} \\quad \\mathbb{E}_{Z^n} [HZ(h)] = H(h),\\]\n\nWithout ambiguity, we overload the notation \\(Z\\) as the set as well as the distribution over the set."}]}, {"page": 32, "text": "then for h\u2217        \u2208  argminh\u2208H H(h) and                   h \u2208     argminh\u2208H          HZ(h), we have that with probability at least\n1 \u2212    \u03b4/2 over Z,                      H(    h) \u2212    H(h\u2217) \u2264          4Rn (H) +           2B2 log(4/\u03b4) n            .\nwhere Rn (H) is the Rademacher complexity of H,                                                                  \uf8f1  1/2,       \u03c1 = \u22121\n                Rn (H) \u225c          E Z\u223cZn             sup     1     n   \u03c1i \u00b7 h(zi)        ,     Rad(\u03c1) =          \uf8f2  1/2,       \u03c1 = 1         .\n                                     \u03c1\u223cRadn         h\u2208H      n   i=1                                             \uf8f3  0,         \u03c1 \u0338= \u00b11\nProof of Lemma F.1. Let g (z1, . . . , zn) \u225c                          g(Z) \u225c         suph\u2208H H(h) \u2212               HZ(h). Then h \u2208                 H being\nB-bounded implies that for any z \u223c                         Z and i \u2208        [n],\n                                     |g(z1, . . . , zi, . . . , zn) \u2212        g(z1, . . . , z, . . . , zn)| \u2264         Bn .\nTherefore, by McDiarmid\u2019s inequality [Bartlett and Mendelson, 2003],\n                                             P [g(Z) \u2265        E[g(Z)] + t] \u2264            exp       \u22122nt2          ,\n                                                                                                      B2\nwhere exp           \u22122nt2         \u2264   \u03b4                       B2 log(4/\u03b4)      .\n                        B2            4 when t \u2265                     2n\nMeanwhile, by a classical symmetrization argument (e.g., proof of Wainwright [2019] Theorem\n4.10), we can bound the expectation E[g(Z)]: for an independent sample set Z\u2032 \u223c                                                        Zn that is also\nindependent of Z,\n                                       E [g(Z)] =EZ              sup     H(h) \u2212        HZ(h)\n                                                                h\u2208H\n                                                     =EZ         sup     EZ\u2032     HZ\u2032(h) \u2212         HZ(h)          Z\n                                                                h\u2208H\n                                                     \u2264EZ        EZ\u2032      sup      HZ\u2032(h) \u2212         HZ(h)         Z\n                                                                         h\u2208H\n                                                     =E(Z,Z\u2032)          sup     HZ\u2032(h) \u2212         HZ(h)         ,\n                                                                       h\u2208H\nwhere in the last line we have used the law of iterated conditional expectation. Then, with Rademacher\nrandom variables \u03c1 = (\u03c11, . . . , \u03c1n) \u223c                       Radn, we have\n                                   E [g(Z)] \u2264E(Z,Z\u2032,\u03c1)                 sup     n1    n   \u03c1i \u00b7 (h(z\u2032    i) \u2212    h(zi))\n                                                                       h\u2208H         i=1\n                                                  \u22642E(Z,\u03c1)           sup     1     n   \u03c1i \u00b7 h(zi)\n                                                  \u22642Rn (H) .        h\u2208H      n   i=1\nTherefore, with probability at least 1 \u2212                       \u03b4/4 over Z, we have\n                                      sup    H(h) \u2212        HZ(h) \u2264          2Rn (H) +           B2 log(4/\u03b4)2n           ,\n                                      h\u2208H\nwhile the same tail bound holds for sup                        h\u2208H \u2212H(h) +              HZ(h) via an analogous argument.\nBy recalling the definition of                 h and h\u2217, we can decompose the excess risk H(                                h) \u2212     H(h\u2217) as\n           H(   h) \u2212     H(h\u2217) =           H(    h) \u2212    HZ(     h)     +     HZ(    h) \u2212     HZ(h\u2217)          +      HZ(h\u2217) \u2212          H(h\u2217)\n                                     \u2264     H(    h) \u2212    HZ(     h)     +     HZ(h\u2217) \u2212          H(h\u2217)\n                                                                             32", "md": "then for $$h^* \\in \\text{argmin}_{h \\in H} H(h)$$ and $$h \\in \\text{argmin}_{h \\in H} H_Z(h)$$, we have that with probability at least $$1 - \\frac{\\delta}{2}$$ over Z, $$H(h) - H(h^*) \\leq 4R_n(H) + 2B^2 \\log\\left(\\frac{4}{\\delta}\\right) n$$.\n\nwhere $$R_n(H)$$ is the Rademacher complexity of H,\n\n$$\n\\begin{array}{ll}\nR_n(H) \\triangleq & \\begin{cases}\n\\frac{1}{2}, & \\rho = -1 \\\\\nE_{Z \\sim Z^n} \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i), & \\text{Rad}(\\rho) = \\frac{1}{2}, \\rho = 1 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\end{array}\n$$\n\nProof of Lemma F.1. Let $$g(z_1, ..., z_n) \\triangleq g(Z) \\triangleq \\sup_{h \\in H} H(h) - H_Z(h)$$. Then $$h \\in H$$ being B-bounded implies that for any $$z \\sim Z$$ and $$i \\in [n]$$,\n\n$$|g(z_1, ..., z_i, ..., z_n) - g(z_1, ..., z, ..., z_n)| \\leq Bn$$.\n\nTherefore, by McDiarmid\u2019s inequality [Bartlett and Mendelson, 2003],\n\n$$P[g(Z) \\geq E[g(Z)] + t] \\leq \\exp\\left(-\\frac{2nt^2}{B^2}\\right)$$,\n\nwhere $$\\exp\\left(-\\frac{2nt^2}{B^2}\\right) \\leq \\delta \\frac{B^2 \\log\\left(\\frac{4}{\\delta}\\right)}{4}$$ when $$t \\geq 2n$$.\n\nMeanwhile, by a classical symmetrization argument (e.g., proof of Wainwright [2019] Theorem 4.10), we can bound the expectation $$E[g(Z)]$$: for an independent sample set $$Z' \\sim Z^n$$ that is also independent of Z,\n\n$$\n\\begin{align*}\nE[g(Z)] & = E_Z \\sup_{h \\in H} H(h) - H_Z(h) \\\\\n& = E_Z \\sup_{h \\in H} E_{Z'} H_Z'(h) - H_Z(h) \\\\\n& \\leq E_Z E_{Z'} \\sup_{h \\in H} H_Z'(h) - H_Z(h) \\\\\n& = E(Z,Z') \\sup_{h \\in H} H_Z'(h) - H_Z(h) \\\\\n\\end{align*}\n$$\n\nwhere in the last line we have used the law of iterated conditional expectation. Then, with Rademacher random variables $$\\rho = (\\rho_1, ..., \\rho_n) \\sim \\text{Rad}^n$$, we have\n\n$$\n\\begin{align*}\nE[g(Z)] & \\leq E(Z,Z',\\rho) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot (h(z'_i) - h(z_i)) \\\\\n& \\leq 2E(Z,\\rho) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i) \\\\\n& \\leq 2R_n(H) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i) \\\\\n\\end{align*}\n$$\n\nTherefore, with probability at least $$1 - \\frac{\\delta}{4}$$ over Z, we have\n\n$$\n\\sup_{h \\in H} H(h) - H_Z(h) \\leq 2R_n(H) + B^2 \\log\\left(\\frac{4}{\\delta}\\right)2n\n$$\n\nwhile the same tail bound holds for $$\\sup_{h \\in H} -H(h) + H_Z(h)$$ via an analogous argument.\n\nBy recalling the definition of $$h$$ and $$h^*$$, we can decompose the excess risk $$H(h) - H(h^*)$$ as\n\n$$\n\\begin{align*}\nH(h) - H(h^*) & = H(h) - H_Z(h) + H_Z(h) - H_Z(h^*) + H_Z(h^*) - H(h^*) \\\\\n& \\leq H(h) - H_Z(h) + H_Z(h^*) - H(h^*) \\\\\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "then for $$h^* \\in \\text{argmin}_{h \\in H} H(h)$$ and $$h \\in \\text{argmin}_{h \\in H} H_Z(h)$$, we have that with probability at least $$1 - \\frac{\\delta}{2}$$ over Z, $$H(h) - H(h^*) \\leq 4R_n(H) + 2B^2 \\log\\left(\\frac{4}{\\delta}\\right) n$$.\n\nwhere $$R_n(H)$$ is the Rademacher complexity of H,\n\n$$\n\\begin{array}{ll}\nR_n(H) \\triangleq & \\begin{cases}\n\\frac{1}{2}, & \\rho = -1 \\\\\nE_{Z \\sim Z^n} \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i), & \\text{Rad}(\\rho) = \\frac{1}{2}, \\rho = 1 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\end{array}\n$$\n\nProof of Lemma F.1. Let $$g(z_1, ..., z_n) \\triangleq g(Z) \\triangleq \\sup_{h \\in H} H(h) - H_Z(h)$$. Then $$h \\in H$$ being B-bounded implies that for any $$z \\sim Z$$ and $$i \\in [n]$$,\n\n$$|g(z_1, ..., z_i, ..., z_n) - g(z_1, ..., z, ..., z_n)| \\leq Bn$$.\n\nTherefore, by McDiarmid\u2019s inequality [Bartlett and Mendelson, 2003],\n\n$$P[g(Z) \\geq E[g(Z)] + t] \\leq \\exp\\left(-\\frac{2nt^2}{B^2}\\right)$$,\n\nwhere $$\\exp\\left(-\\frac{2nt^2}{B^2}\\right) \\leq \\delta \\frac{B^2 \\log\\left(\\frac{4}{\\delta}\\right)}{4}$$ when $$t \\geq 2n$$.\n\nMeanwhile, by a classical symmetrization argument (e.g., proof of Wainwright [2019] Theorem 4.10), we can bound the expectation $$E[g(Z)]$$: for an independent sample set $$Z' \\sim Z^n$$ that is also independent of Z,\n\n$$\n\\begin{align*}\nE[g(Z)] & = E_Z \\sup_{h \\in H} H(h) - H_Z(h) \\\\\n& = E_Z \\sup_{h \\in H} E_{Z'} H_Z'(h) - H_Z(h) \\\\\n& \\leq E_Z E_{Z'} \\sup_{h \\in H} H_Z'(h) - H_Z(h) \\\\\n& = E(Z,Z') \\sup_{h \\in H} H_Z'(h) - H_Z(h) \\\\\n\\end{align*}\n$$\n\nwhere in the last line we have used the law of iterated conditional expectation. Then, with Rademacher random variables $$\\rho = (\\rho_1, ..., \\rho_n) \\sim \\text{Rad}^n$$, we have\n\n$$\n\\begin{align*}\nE[g(Z)] & \\leq E(Z,Z',\\rho) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot (h(z'_i) - h(z_i)) \\\\\n& \\leq 2E(Z,\\rho) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i) \\\\\n& \\leq 2R_n(H) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i) \\\\\n\\end{align*}\n$$\n\nTherefore, with probability at least $$1 - \\frac{\\delta}{4}$$ over Z, we have\n\n$$\n\\sup_{h \\in H} H(h) - H_Z(h) \\leq 2R_n(H) + B^2 \\log\\left(\\frac{4}{\\delta}\\right)2n\n$$\n\nwhile the same tail bound holds for $$\\sup_{h \\in H} -H(h) + H_Z(h)$$ via an analogous argument.\n\nBy recalling the definition of $$h$$ and $$h^*$$, we can decompose the excess risk $$H(h) - H(h^*)$$ as\n\n$$\n\\begin{align*}\nH(h) - H(h^*) & = H(h) - H_Z(h) + H_Z(h) - H_Z(h^*) + H_Z(h^*) - H(h^*) \\\\\n& \\leq H(h) - H_Z(h) + H_Z(h^*) - H(h^*) \\\\\n\\end{align*}\n$$", "md": "then for $$h^* \\in \\text{argmin}_{h \\in H} H(h)$$ and $$h \\in \\text{argmin}_{h \\in H} H_Z(h)$$, we have that with probability at least $$1 - \\frac{\\delta}{2}$$ over Z, $$H(h) - H(h^*) \\leq 4R_n(H) + 2B^2 \\log\\left(\\frac{4}{\\delta}\\right) n$$.\n\nwhere $$R_n(H)$$ is the Rademacher complexity of H,\n\n$$\n\\begin{array}{ll}\nR_n(H) \\triangleq & \\begin{cases}\n\\frac{1}{2}, & \\rho = -1 \\\\\nE_{Z \\sim Z^n} \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i), & \\text{Rad}(\\rho) = \\frac{1}{2}, \\rho = 1 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\end{array}\n$$\n\nProof of Lemma F.1. Let $$g(z_1, ..., z_n) \\triangleq g(Z) \\triangleq \\sup_{h \\in H} H(h) - H_Z(h)$$. Then $$h \\in H$$ being B-bounded implies that for any $$z \\sim Z$$ and $$i \\in [n]$$,\n\n$$|g(z_1, ..., z_i, ..., z_n) - g(z_1, ..., z, ..., z_n)| \\leq Bn$$.\n\nTherefore, by McDiarmid\u2019s inequality [Bartlett and Mendelson, 2003],\n\n$$P[g(Z) \\geq E[g(Z)] + t] \\leq \\exp\\left(-\\frac{2nt^2}{B^2}\\right)$$,\n\nwhere $$\\exp\\left(-\\frac{2nt^2}{B^2}\\right) \\leq \\delta \\frac{B^2 \\log\\left(\\frac{4}{\\delta}\\right)}{4}$$ when $$t \\geq 2n$$.\n\nMeanwhile, by a classical symmetrization argument (e.g., proof of Wainwright [2019] Theorem 4.10), we can bound the expectation $$E[g(Z)]$$: for an independent sample set $$Z' \\sim Z^n$$ that is also independent of Z,\n\n$$\n\\begin{align*}\nE[g(Z)] & = E_Z \\sup_{h \\in H} H(h) - H_Z(h) \\\\\n& = E_Z \\sup_{h \\in H} E_{Z'} H_Z'(h) - H_Z(h) \\\\\n& \\leq E_Z E_{Z'} \\sup_{h \\in H} H_Z'(h) - H_Z(h) \\\\\n& = E(Z,Z') \\sup_{h \\in H} H_Z'(h) - H_Z(h) \\\\\n\\end{align*}\n$$\n\nwhere in the last line we have used the law of iterated conditional expectation. Then, with Rademacher random variables $$\\rho = (\\rho_1, ..., \\rho_n) \\sim \\text{Rad}^n$$, we have\n\n$$\n\\begin{align*}\nE[g(Z)] & \\leq E(Z,Z',\\rho) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot (h(z'_i) - h(z_i)) \\\\\n& \\leq 2E(Z,\\rho) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i) \\\\\n& \\leq 2R_n(H) \\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^{n} \\rho_i \\cdot h(z_i) \\\\\n\\end{align*}\n$$\n\nTherefore, with probability at least $$1 - \\frac{\\delta}{4}$$ over Z, we have\n\n$$\n\\sup_{h \\in H} H(h) - H_Z(h) \\leq 2R_n(H) + B^2 \\log\\left(\\frac{4}{\\delta}\\right)2n\n$$\n\nwhile the same tail bound holds for $$\\sup_{h \\in H} -H(h) + H_Z(h)$$ via an analogous argument.\n\nBy recalling the definition of $$h$$ and $$h^*$$, we can decompose the excess risk $$H(h) - H(h^*)$$ as\n\n$$\n\\begin{align*}\nH(h) - H(h^*) & = H(h) - H_Z(h) + H_Z(h) - H_Z(h^*) + H_Z(h^*) - H(h^*) \\\\\n& \\leq H(h) - H_Z(h) + H_Z(h^*) - H(h^*) \\\\\n\\end{align*}\n$$"}]}, {"page": 33, "text": "where    HZ(    h) \u2212    HZ(h\u2217) \u2264        0 by the basic inequality. With both                 h, h\u2217   \u2208  H, we have\n                                       H(   h) \u2212    H(h\u2217) \u2264        2 sup      H(h) \u2212      HZ(h)       ,\nand therefore                                                        h\u2208H\n    P   H(   h) \u2212    H(h\u2217) \u2265       t   \u2264   P    sup    H(h) \u2212      HZ(h) \u2265         t    + P     sup    \u2212H(h) +      HZ(h) \u2265   t  .\nOverall, with probability at least 1 \u2212          h\u2208H   \u03b4/2 over Z,                  2            h\u2208H                          2\n                                    H(   h) \u2212    H(h\u2217) \u2264        4Rn (H) +         2B2 log(4/\u03b4)n          .\n                                                                      33", "md": "where $$HZ(h) - HZ(h^*) \\leq 0$$ by the basic inequality. With both $$h, h^* \\in H$$, we have\n\n$$H(h) - H(h^*) \\leq 2 \\sup H(h) - HZ(h),$$\n\nand therefore $$\\forall h \\in H$$\n\n$$\\mathbb{P} [H(h) - H(h^*) \\geq t] \\leq \\mathbb{P} [\\sup H(h) - HZ(h) \\geq t] + \\mathbb{P} [\\sup -H(h) + HZ(h) \\geq t.$$\n\nOverall, with probability at least $$1 - \\frac{\\delta}{2}$$ over $$Z$$,\n\n$$\\forall h \\in H, H(h) - H(h^*) \\leq 4Rn(H) + 2B^2 \\log\\left(\\frac{4}{\\delta}\\right)n.$$\n\n$$33$$", "images": [], "items": [{"type": "text", "value": "where $$HZ(h) - HZ(h^*) \\leq 0$$ by the basic inequality. With both $$h, h^* \\in H$$, we have\n\n$$H(h) - H(h^*) \\leq 2 \\sup H(h) - HZ(h),$$\n\nand therefore $$\\forall h \\in H$$\n\n$$\\mathbb{P} [H(h) - H(h^*) \\geq t] \\leq \\mathbb{P} [\\sup H(h) - HZ(h) \\geq t] + \\mathbb{P} [\\sup -H(h) + HZ(h) \\geq t.$$\n\nOverall, with probability at least $$1 - \\frac{\\delta}{2}$$ over $$Z$$,\n\n$$\\forall h \\in H, H(h) - H(h^*) \\leq 4Rn(H) + 2B^2 \\log\\left(\\frac{4}{\\delta}\\right)n.$$\n\n$$33$$", "md": "where $$HZ(h) - HZ(h^*) \\leq 0$$ by the basic inequality. With both $$h, h^* \\in H$$, we have\n\n$$H(h) - H(h^*) \\leq 2 \\sup H(h) - HZ(h),$$\n\nand therefore $$\\forall h \\in H$$\n\n$$\\mathbb{P} [H(h) - H(h^*) \\geq t] \\leq \\mathbb{P} [\\sup H(h) - HZ(h) \\geq t] + \\mathbb{P} [\\sup -H(h) + HZ(h) \\geq t.$$\n\nOverall, with probability at least $$1 - \\frac{\\delta}{2}$$ over $$Z$$,\n\n$$\\forall h \\in H, H(h) - H(h^*) \\leq 4Rn(H) + 2B^2 \\log\\left(\\frac{4}{\\delta}\\right)n.$$\n\n$$33$$"}]}], "job_id": "46dec46d-d571-433a-b8d8-99f7e5d020df", "file_path": "./corpus/2307.11030.pdf"}