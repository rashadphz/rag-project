{"pages": [{"page": 1, "text": "                               Objaverse-XL: A Universe of 10M+ 3D Objects\n                               Matt Deitke     \u2020\u03c8 , Ruoshi Liu    \u03b3, Matthew Wallingford         \u03c8 , Huong Ngo     \u03c8 , Oscar Michel     \u2020 ,\n                         Aditya Kusupati       \u03c8 , Alan Fan   \u03c8 , Christian Laforte      \u03c3, Vikram Voleti     \u03c3 , Samir Yitzhak Gadre        \u03b3,\n                                  Eli VanderBilt     \u2020,Aniruddha Kembhavi           \u2020\u03c8 , Carl Vondrick     \u03b3 , Georgia Gkioxari      \u03b4 ,\n                                                   Kiana Ehsani      \u2020,\u2217 Ludwig Schmidt       \u2020\u03c8\u2113  ,\u2217Ali Farhadi    \u03c8\n                                \u2020Allen Institute for AI        \u03c8 University of Washington, Seattle            \u03b3 Columbia University\n                                              \u03c3 Stability AI      \u03b4California Institute of Technology            \u2113LAION\n                                                                \u2217Equal Senior Contribution\n                        Figure 1: Objaverse-XL includes a ginormous collection of diverse 3D objects from a variety of\n                        sources. Here, we show examples of objects in Objaverse-XL rendered in a scene.\n                                                                             Abstract\n                                  Natural language processing and 2D vision models have attained remarkable profi-\narXiv:2307.05663v1 [cs.CV] 11 Jul 2023ciency on many tasks primarily by escalating the scale of training data. However,\n                                  3D vision tasks have not seen the same progress, in part due to the challenges of\n                                  acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of\n                                  over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a\n                                  diverse set of sources, including manually designed objects, photogrammetry scans\n                                  of landmarks and everyday items, and professional scans of historic and antique\n                                  artifacts. Representing the largest scale and diversity in the realm of 3D datasets,\n                                  Objaverse-XL enables significant new possibilities for 3D vision. Our experiments\n                                  demonstrate the improvements enabled with the scale provided by Objaverse-XL.\n                                  We show that by training Zero123 on novel view synthesis, utilizing over 100\n                                  million multi-view rendered images, we achieve strong zero-shot generalization\n                                  abilities. We hope that releasing Objaverse-XL will enable further innovations in\n                                  the field of 3D vision at scale.\n                        Preprint. Under review.", "md": "# Objaverse-XL: A Universe of 10M+ 3D Objects\n\n## Objaverse-XL: A Universe of 10M+ 3D Objects\n\nMatt Deitke     \u2020\u03c8 , Ruoshi Liu    \u03b3, Matthew Wallingford         \u03c8 , Huong Ngo     \u03c8 , Oscar Michel     \u2020 ,\n\nAditya Kusupati       \u03c8 , Alan Fan   \u03c8 , Christian Laforte      \u03c3, Vikram Voleti     \u03c3 , Samir Yitzhak Gadre        \u03b3,\n\nEli VanderBilt     \u2020,Aniruddha Kembhavi           \u2020\u03c8 , Carl Vondrick     \u03b3 , Georgia Gkioxari      \u03b4 ,\n\nKiana Ehsani      \u2020,\u2217 Ludwig Schmidt       \u2020\u03c8\u2113  ,\u2217Ali Farhadi    \u03c8\n\n\u2020Allen Institute for AI        \u03c8 University of Washington, Seattle            \u03b3 Columbia University\n\n\u03c3 Stability AI      \u03b4California Institute of Technology            \u2113LAION\n\n\u2217Equal Senior Contribution\n\n### Figure 1: Objaverse-XL includes a ginormous collection of diverse 3D objects from a variety of sources.\n\nHere, we show examples of objects in Objaverse-XL rendered in a scene.\n\n#### Abstract\n\nNatural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training $$Zero123$$ on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.\n\nPreprint. Under review.", "images": [{"name": "img_p0_1", "height": 669, "width": 1188}], "items": [{"type": "heading", "lvl": 1, "value": "Objaverse-XL: A Universe of 10M+ 3D Objects", "md": "# Objaverse-XL: A Universe of 10M+ 3D Objects"}, {"type": "heading", "lvl": 2, "value": "Objaverse-XL: A Universe of 10M+ 3D Objects", "md": "## Objaverse-XL: A Universe of 10M+ 3D Objects"}, {"type": "text", "value": "Matt Deitke     \u2020\u03c8 , Ruoshi Liu    \u03b3, Matthew Wallingford         \u03c8 , Huong Ngo     \u03c8 , Oscar Michel     \u2020 ,\n\nAditya Kusupati       \u03c8 , Alan Fan   \u03c8 , Christian Laforte      \u03c3, Vikram Voleti     \u03c3 , Samir Yitzhak Gadre        \u03b3,\n\nEli VanderBilt     \u2020,Aniruddha Kembhavi           \u2020\u03c8 , Carl Vondrick     \u03b3 , Georgia Gkioxari      \u03b4 ,\n\nKiana Ehsani      \u2020,\u2217 Ludwig Schmidt       \u2020\u03c8\u2113  ,\u2217Ali Farhadi    \u03c8\n\n\u2020Allen Institute for AI        \u03c8 University of Washington, Seattle            \u03b3 Columbia University\n\n\u03c3 Stability AI      \u03b4California Institute of Technology            \u2113LAION\n\n\u2217Equal Senior Contribution", "md": "Matt Deitke     \u2020\u03c8 , Ruoshi Liu    \u03b3, Matthew Wallingford         \u03c8 , Huong Ngo     \u03c8 , Oscar Michel     \u2020 ,\n\nAditya Kusupati       \u03c8 , Alan Fan   \u03c8 , Christian Laforte      \u03c3, Vikram Voleti     \u03c3 , Samir Yitzhak Gadre        \u03b3,\n\nEli VanderBilt     \u2020,Aniruddha Kembhavi           \u2020\u03c8 , Carl Vondrick     \u03b3 , Georgia Gkioxari      \u03b4 ,\n\nKiana Ehsani      \u2020,\u2217 Ludwig Schmidt       \u2020\u03c8\u2113  ,\u2217Ali Farhadi    \u03c8\n\n\u2020Allen Institute for AI        \u03c8 University of Washington, Seattle            \u03b3 Columbia University\n\n\u03c3 Stability AI      \u03b4California Institute of Technology            \u2113LAION\n\n\u2217Equal Senior Contribution"}, {"type": "heading", "lvl": 3, "value": "Figure 1: Objaverse-XL includes a ginormous collection of diverse 3D objects from a variety of sources.", "md": "### Figure 1: Objaverse-XL includes a ginormous collection of diverse 3D objects from a variety of sources."}, {"type": "text", "value": "Here, we show examples of objects in Objaverse-XL rendered in a scene.", "md": "Here, we show examples of objects in Objaverse-XL rendered in a scene."}, {"type": "heading", "lvl": 4, "value": "Abstract", "md": "#### Abstract"}, {"type": "text", "value": "Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training $$Zero123$$ on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.\n\nPreprint. Under review.", "md": "Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training $$Zero123$$ on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.\n\nPreprint. Under review."}]}, {"page": 2, "text": "1     Introduction\nScale has been paramount to recent advances in AI. Large models have produced breakthroughs\nin language comprehension and generation [7,                   43], representation learning [50], multimodal task\ncompletion [3,      37], image generation [51,          54], and more. With an increasing number of learnable\nparameters, modern neural networks consume increasingly large volumes of data. As data has scaled\nup, the capabilities exhibited by models has dramatically increased.\nJust a few years ago, GPT-2 [49] broke data barriers by consuming roughly 30 billion language tokens\nand demonstrated promising zero shot results on NLP benchmarks. Now, models like Chinchilla [25]\nand LLaMA [57] consume trillions of web crawled tokens and easily surpass GPT-2 at benchmarks\nand capabilities. In computer vision, ImageNet [15], with 1 million images, was the gold standard\nfor representation learning until scaling to billions of images, via web crawled datasets like LAION-\n5B [55], produced powerful visual representations like CLIP [50]. Key to scaling up from millions of\ndata points to billions and beyond has been the shift from assembling datasets manually to assembling\nthem from diverse sources via the web.\nAs language and image data has scaled up, applications that require other forms of data have been\nleft behind. Notable are applications in 3D computer vision, with tasks like 3D object generation and\nreconstruction, continue to consume small handcrafted datasets. 3D datasets such as ShapeNet [9]\nrely on professional 3D designers using expensive software to create assets, making the process\ntremendously difficult to crowdsource and scale. The resulting data scarcity has become a bottleneck\nfor learning-driven methods in 3D computer vision. For instance, 3D object generation currently lags\nfar behind 2D image generation, and current 3D generation approaches often still leverage models\ntrained on large 2D datasets instead of being trained on 3D data from scratch. As demand and interest\nin AR and VR technologies goes up, scaling up 3D data is going to be increasingly crucial.\nWe introduce Objaverse-XL, a large-scale, web-crawled dataset of 3D assets.                                 Advances in 3D\nauthoring tools, demand, and photogrammetry, have substantially increased the amount of 3D data on\nthe Internet. This data is spread across numerous locations including software hosting services like\nGithub, specialized sites for 3D assets like Sketchfab, 3D printing asset sources like Thingiverse, 3D\nscanning platforms like Polycam, and specialized sites like the Smithsonian Institute. Objaverse-XL\ncrawls such sources for 3D objects, providing a significantly richer variety and quality of 3D data\nthan previously available, see Figure 1. Overall, Objaverse-XL comprises of over 10 million 3D\nobjects, representing an order of magnitude more data than the recently proposed Objaverse 1.0 [14]\nand is two orders of magnitude larger than ShapeNet.\nThe scale and diversity of assets in Objaverse-XL significantly expands the performance of state-\nof-the-art 3D models. The recently proposed Zero123 [36] model for novel view synthesis, when\npre-trained with Objaverse-XL, shows significantly better zero-shot generalization to challenging\nand complex modalities including photorealistic assets, cartoons, drawings and sketches. Similar\nimprovements are also seen with PixelNerf which is trained to synthesize novel views given a small\nset of images. On each of these tasks, scaling pre-training data continues to show improvements from\na thousand assets all the way up to 10 million, with few signs of slowing down, showing the promise\nand opportunities enabled with web scale data.\n2     Related Work\nPre-training Datasets.            Massive datasets have a prevalent role in modern, data-driven AI as they\nhave produced powerful and general representations when paired with large-scale training.                                      In\ncomputer vision, ImageNet [15], introduced nearly 14 years ago, has become the standard pre-training\ndataset of state-of-the-art visual models in object detection [53,               8], instance segmentation [24,          11] and\nmore. More recently, large image datasets, such as LAION-5B [55], have powered exciting advances\nin generative AI, such as Stable Diffusion [54], and have given rise to new general-purpose vision\nand language representations with models like CLIP [50] and Flamingo [3]. This year, SAM [31]\n                                                                2", "md": "# Document\n\n# Introduction\n\nScale has been paramount to recent advances in AI. Large models have produced breakthroughs in language comprehension and generation [7, 43], representation learning [50], multimodal task completion [3, 37], image generation [51, 54], and more. With an increasing number of learnable parameters, modern neural networks consume increasingly large volumes of data. As data has scaled up, the capabilities exhibited by models has dramatically increased.\n\nJust a few years ago, GPT-2 [49] broke data barriers by consuming roughly 30 billion language tokens and demonstrated promising zero shot results on NLP benchmarks. Now, models like Chinchilla [25] and LLaMA [57] consume trillions of web crawled tokens and easily surpass GPT-2 at benchmarks and capabilities. In computer vision, ImageNet [15], with 1 million images, was the gold standard for representation learning until scaling to billions of images, via web crawled datasets like LAION-5B [55], produced powerful visual representations like CLIP [50]. Key to scaling up from millions of data points to billions and beyond has been the shift from assembling datasets manually to assembling them from diverse sources via the web.\n\nAs language and image data has scaled up, applications that require other forms of data have been left behind. Notable are applications in 3D computer vision, with tasks like 3D object generation and reconstruction, continue to consume small handcrafted datasets. 3D datasets such as ShapeNet [9] rely on professional 3D designers using expensive software to create assets, making the process tremendously difficult to crowdsource and scale. The resulting data scarcity has become a bottleneck for learning-driven methods in 3D computer vision. For instance, 3D object generation currently lags far behind 2D image generation, and current 3D generation approaches often still leverage models trained on large 2D datasets instead of being trained on 3D data from scratch. As demand and interest in AR and VR technologies goes up, scaling up 3D data is going to be increasingly crucial.\n\nWe introduce Objaverse-XL, a large-scale, web-crawled dataset of 3D assets. Advances in 3D authoring tools, demand, and photogrammetry, have substantially increased the amount of 3D data on the Internet. This data is spread across numerous locations including software hosting services like Github, specialized sites for 3D assets like Sketchfab, 3D printing asset sources like Thingiverse, 3D scanning platforms like Polycam, and specialized sites like the Smithsonian Institute. Objaverse-XL crawls such sources for 3D objects, providing a significantly richer variety and quality of 3D data than previously available, see Figure 1. Overall, Objaverse-XL comprises of over 10 million 3D objects, representing an order of magnitude more data than the recently proposed Objaverse 1.0 [14] and is two orders of magnitude larger than ShapeNet.\n\nThe scale and diversity of assets in Objaverse-XL significantly expands the performance of state-of-the-art 3D models. The recently proposed Zero123 [36] model for novel view synthesis, when pre-trained with Objaverse-XL, shows significantly better zero-shot generalization to challenging and complex modalities including photorealistic assets, cartoons, drawings and sketches. Similar improvements are also seen with PixelNerf which is trained to synthesize novel views given a small set of images. On each of these tasks, scaling pre-training data continues to show improvements from a thousand assets all the way up to 10 million, with few signs of slowing down, showing the promise and opportunities enabled with web scale data.\n\n## Related Work\n\nPre-training Datasets. Massive datasets have a prevalent role in modern, data-driven AI as they have produced powerful and general representations when paired with large-scale training. In computer vision, ImageNet [15], introduced nearly 14 years ago, has become the standard pre-training dataset of state-of-the-art visual models in object detection [53, 8], instance segmentation [24, 11] and more. More recently, large image datasets, such as LAION-5B [55], have powered exciting advances in generative AI, such as Stable Diffusion [54], and have given rise to new general-purpose vision and language representations with models like CLIP [50] and Flamingo [3]. This year, SAM [31]", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 1, "value": "Introduction", "md": "# Introduction"}, {"type": "text", "value": "Scale has been paramount to recent advances in AI. Large models have produced breakthroughs in language comprehension and generation [7, 43], representation learning [50], multimodal task completion [3, 37], image generation [51, 54], and more. With an increasing number of learnable parameters, modern neural networks consume increasingly large volumes of data. As data has scaled up, the capabilities exhibited by models has dramatically increased.\n\nJust a few years ago, GPT-2 [49] broke data barriers by consuming roughly 30 billion language tokens and demonstrated promising zero shot results on NLP benchmarks. Now, models like Chinchilla [25] and LLaMA [57] consume trillions of web crawled tokens and easily surpass GPT-2 at benchmarks and capabilities. In computer vision, ImageNet [15], with 1 million images, was the gold standard for representation learning until scaling to billions of images, via web crawled datasets like LAION-5B [55], produced powerful visual representations like CLIP [50]. Key to scaling up from millions of data points to billions and beyond has been the shift from assembling datasets manually to assembling them from diverse sources via the web.\n\nAs language and image data has scaled up, applications that require other forms of data have been left behind. Notable are applications in 3D computer vision, with tasks like 3D object generation and reconstruction, continue to consume small handcrafted datasets. 3D datasets such as ShapeNet [9] rely on professional 3D designers using expensive software to create assets, making the process tremendously difficult to crowdsource and scale. The resulting data scarcity has become a bottleneck for learning-driven methods in 3D computer vision. For instance, 3D object generation currently lags far behind 2D image generation, and current 3D generation approaches often still leverage models trained on large 2D datasets instead of being trained on 3D data from scratch. As demand and interest in AR and VR technologies goes up, scaling up 3D data is going to be increasingly crucial.\n\nWe introduce Objaverse-XL, a large-scale, web-crawled dataset of 3D assets. Advances in 3D authoring tools, demand, and photogrammetry, have substantially increased the amount of 3D data on the Internet. This data is spread across numerous locations including software hosting services like Github, specialized sites for 3D assets like Sketchfab, 3D printing asset sources like Thingiverse, 3D scanning platforms like Polycam, and specialized sites like the Smithsonian Institute. Objaverse-XL crawls such sources for 3D objects, providing a significantly richer variety and quality of 3D data than previously available, see Figure 1. Overall, Objaverse-XL comprises of over 10 million 3D objects, representing an order of magnitude more data than the recently proposed Objaverse 1.0 [14] and is two orders of magnitude larger than ShapeNet.\n\nThe scale and diversity of assets in Objaverse-XL significantly expands the performance of state-of-the-art 3D models. The recently proposed Zero123 [36] model for novel view synthesis, when pre-trained with Objaverse-XL, shows significantly better zero-shot generalization to challenging and complex modalities including photorealistic assets, cartoons, drawings and sketches. Similar improvements are also seen with PixelNerf which is trained to synthesize novel views given a small set of images. On each of these tasks, scaling pre-training data continues to show improvements from a thousand assets all the way up to 10 million, with few signs of slowing down, showing the promise and opportunities enabled with web scale data.", "md": "Scale has been paramount to recent advances in AI. Large models have produced breakthroughs in language comprehension and generation [7, 43], representation learning [50], multimodal task completion [3, 37], image generation [51, 54], and more. With an increasing number of learnable parameters, modern neural networks consume increasingly large volumes of data. As data has scaled up, the capabilities exhibited by models has dramatically increased.\n\nJust a few years ago, GPT-2 [49] broke data barriers by consuming roughly 30 billion language tokens and demonstrated promising zero shot results on NLP benchmarks. Now, models like Chinchilla [25] and LLaMA [57] consume trillions of web crawled tokens and easily surpass GPT-2 at benchmarks and capabilities. In computer vision, ImageNet [15], with 1 million images, was the gold standard for representation learning until scaling to billions of images, via web crawled datasets like LAION-5B [55], produced powerful visual representations like CLIP [50]. Key to scaling up from millions of data points to billions and beyond has been the shift from assembling datasets manually to assembling them from diverse sources via the web.\n\nAs language and image data has scaled up, applications that require other forms of data have been left behind. Notable are applications in 3D computer vision, with tasks like 3D object generation and reconstruction, continue to consume small handcrafted datasets. 3D datasets such as ShapeNet [9] rely on professional 3D designers using expensive software to create assets, making the process tremendously difficult to crowdsource and scale. The resulting data scarcity has become a bottleneck for learning-driven methods in 3D computer vision. For instance, 3D object generation currently lags far behind 2D image generation, and current 3D generation approaches often still leverage models trained on large 2D datasets instead of being trained on 3D data from scratch. As demand and interest in AR and VR technologies goes up, scaling up 3D data is going to be increasingly crucial.\n\nWe introduce Objaverse-XL, a large-scale, web-crawled dataset of 3D assets. Advances in 3D authoring tools, demand, and photogrammetry, have substantially increased the amount of 3D data on the Internet. This data is spread across numerous locations including software hosting services like Github, specialized sites for 3D assets like Sketchfab, 3D printing asset sources like Thingiverse, 3D scanning platforms like Polycam, and specialized sites like the Smithsonian Institute. Objaverse-XL crawls such sources for 3D objects, providing a significantly richer variety and quality of 3D data than previously available, see Figure 1. Overall, Objaverse-XL comprises of over 10 million 3D objects, representing an order of magnitude more data than the recently proposed Objaverse 1.0 [14] and is two orders of magnitude larger than ShapeNet.\n\nThe scale and diversity of assets in Objaverse-XL significantly expands the performance of state-of-the-art 3D models. The recently proposed Zero123 [36] model for novel view synthesis, when pre-trained with Objaverse-XL, shows significantly better zero-shot generalization to challenging and complex modalities including photorealistic assets, cartoons, drawings and sketches. Similar improvements are also seen with PixelNerf which is trained to synthesize novel views given a small set of images. On each of these tasks, scaling pre-training data continues to show improvements from a thousand assets all the way up to 10 million, with few signs of slowing down, showing the promise and opportunities enabled with web scale data."}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "Pre-training Datasets. Massive datasets have a prevalent role in modern, data-driven AI as they have produced powerful and general representations when paired with large-scale training. In computer vision, ImageNet [15], introduced nearly 14 years ago, has become the standard pre-training dataset of state-of-the-art visual models in object detection [53, 8], instance segmentation [24, 11] and more. More recently, large image datasets, such as LAION-5B [55], have powered exciting advances in generative AI, such as Stable Diffusion [54], and have given rise to new general-purpose vision and language representations with models like CLIP [50] and Flamingo [3]. This year, SAM [31]", "md": "Pre-training Datasets. Massive datasets have a prevalent role in modern, data-driven AI as they have produced powerful and general representations when paired with large-scale training. In computer vision, ImageNet [15], introduced nearly 14 years ago, has become the standard pre-training dataset of state-of-the-art visual models in object detection [53, 8], instance segmentation [24, 11] and more. More recently, large image datasets, such as LAION-5B [55], have powered exciting advances in generative AI, such as Stable Diffusion [54], and have given rise to new general-purpose vision and language representations with models like CLIP [50] and Flamingo [3]. This year, SAM [31]"}]}, {"page": 3, "text": "                                                                                             Source                # Objects\n                                                                                             IKEA [32]                   219\n                                                                                             GSO [17]                     1K\n                                                                                             EGAD [41]                    2K\n                                                                                             OmniObject3D [63]            6K\n                                                                                             PhotoShape [46]              5K\n                                                                                             ABO [13]                     8K\n                                                                                             Thingi10K [67]             10K\n                                                                                             3d-Future [19]             10K\n                                                                                             ShapeNet [9]               51K\n                                                                                             Objaverse 1.0 [14]        800K\n                                                                                             Objaverse-XL             10.2M\n   GitHub       Thingiverse     Polycam      Objaverse 1.0 (Sketchfab)     Smithsonian     Table 1:       Number of 3D\n                                                                                            models in common datasets.\n Figure 2: t-SNE projection of CLIP L/14 embeddings on a subset of                          Objaverse-XL is over an or-\n rendered objects. Compared to Objaverse 1.0 (orange), Objaverse-XL                         der of magnitude larger than\n more densely captures the distribution of 3D assets.                                       prior datasets.\n introduced a dataset of one billion object masks used to train a model capable of segmenting any\n object from an image. In language understanding, datasets like Common Crawl [1] have culminated\n in unprecedented capabilities of large language models such as GPT-4 [43], which in turn power\n mainstream applications like ChatGPT. The impact of large datasets is undeniable. However, current\n efforts to collect massive datasets focus on image and language modalities. In this work we introduce\n and release publically a massive dataset of 3D objects, called Objaverse-XL. Given the promise\n of large datasets for 2D vision and language, we believe Objaverse-XL will accelerate research in\n large-scale training for 3D understanding.\n 3D Datasets.        Existing 3D datasets have been instrumental in yielding significant findings in 3D\n over the years. ShapeNet [9] has served as the testbed for modeling, representing and predicting\n 3D shapes in the era of deep learning. ShapeNet provides a collection of 3D shapes, in the form of\n textured CAD models labeled with semantic categories from WordNet [40]. In theory, it contains 3M\n CAD models with textures. In practice, a small subset of 51K models is used after filtering by mesh\n and texture quality. Notwithstanding its impact, ShapeNet objects are of low resolution and textures\n are often overly simplistic. Other datasets such as ABO [13], GSO [17], and OmniObjects3D [63]\n improve on the texture quality of their CAD models but are significantly smaller in size with the\n largest constituting 15K CAD models. Recently, Objaverse 1.0 [14] introduced a 3D dataset of 800K\n 3D models with high quality and diverse textures, geometry and object types, making it 15\u00d7                            larger\n than prior 3D datasets. While impressive and a step toward a large-scale 3D dataset, Objaverse\n1.0 remains several magnitudes smaller than dominant datasets in vision and language. As seen in\n Figure 2 and Table 1, Objaverse-XL extends Objaverse 1.0 to an even larger 3D dataset of                             10.2M\n unique objects from a diverse set of sources, object shapes, and categories. We discuss Objaverse-XL\n and its properties in Section 3.\n 3D Applications.         The potential of a massive 3D dataset like Objaverse-XL promises exciting novel\n applications in computer vision, graphics, augmented reality and generative AI. Reconstructing 3D\n objects from images is a longstanding problem in computer vision and graphics. Here, several methods\n explore novel representations [12,           59,  38,  39], network architectures [22,           64] and differentiable\n rendering techniques [30,       10,  52, 34,35] to predict the 3D shapes and textures of objects from images\nwith or without 3D supervision. All of the aforementioned projects experiment on the small scale\n ShapeNet. The significantly larger Objaverse-XL could pave the way to new levels of performance,\n and increase generalization to new domains in a zero-shot fashion. Over the past year, generative\nAI has made its foray into 3D. MCC [62] learns a generalizable representation with self-supervised\n learning for 3D reconstruction. DreamFusion [48] and later on Magic3D [33] demonstrated that 3D\n shapes could be generated from language prompts with the help of text-to-image models. Point-E [42]\n and Shape-E [28] also train for text-to-3D with the help of 3D models from an undisclosed source.\n Recently, Zero123 [36] introduced an image-conditioned diffusion model which generates novel\n object views and is trained on Objaverse 1.0. Stable Dreamfusion [56] replaces the text-to-image\n model in DreamFusion with the 3D-informed Zero123 and shows improved 3D generations. Recent\n findings in AI and scaling laws [29,          25] suggest that both generative and predictive models benefit\n                                                               3", "md": "Source                # Objects\nIKEA [32]                   219\nGSO [17]                     1K\nEGAD [41]                    2K\nOmniObject3D [63]            6K\nPhotoShape [46]              5K\nABO [13]                     8K\nThingi10K [67]             10K\n3d-Future [19]             10K\nShapeNet [9]               51K\nObjaverse 1.0 [14]        800K\nObjaverse-XL             10.2M\n\n| GitHub | Thingiverse | Polycam | Objaverse 1.0 (Sketchfab) | Smithsonian |\n|--------|-------------|---------|---------------------------|-------------|\n| Table 1: Number of 3D models in common datasets.\n\n$$\n\\text{Figure 2: t-SNE projection of CLIP L/14 embeddings on a subset of rendered objects. Compared to Objaverse 1.0 (orange), Objaverse-XL more densely captures the distribution of 3D assets.}\n$$\n\nObjaverse-XL is over an order of magnitude larger than prior datasets.\n\nWe introduced a dataset of one billion object masks used to train a model capable of segmenting any object from an image. In language understanding, datasets like Common Crawl [1] have culminated in unprecedented capabilities of large language models such as GPT-4 [43], which in turn power mainstream applications like ChatGPT. The impact of large datasets is undeniable. However, current efforts to collect massive datasets focus on image and language modalities. In this work we introduce and release publicly a massive dataset of 3D objects, called Objaverse-XL. Given the promise of large datasets for 2D vision and language, we believe Objaverse-XL will accelerate research in large-scale training for 3D understanding.\n\n### 3D Datasets\nExisting 3D datasets have been instrumental in yielding significant findings in 3D over the years. ShapeNet [9] has served as the testbed for modeling, representing and predicting 3D shapes in the era of deep learning. ShapeNet provides a collection of 3D shapes, in the form of textured CAD models labeled with semantic categories from WordNet [40]. In theory, it contains 3M CAD models with textures. In practice, a small subset of 51K models is used after filtering by mesh and texture quality. Notwithstanding its impact, ShapeNet objects are of low resolution and textures are often overly simplistic. Other datasets such as ABO [13], GSO [17], and OmniObjects3D [63] improve on the texture quality of their CAD models but are significantly smaller in size with the largest constituting 15K CAD models. Recently, Objaverse 1.0 [14] introduced a 3D dataset of 800K 3D models with high quality and diverse textures, geometry and object types, making it 15\u00d7 larger than prior 3D datasets. While impressive and a step toward a large-scale 3D dataset, Objaverse 1.0 remains several magnitudes smaller than dominant datasets in vision and language. As seen in Figure 2 and Table 1, Objaverse-XL extends Objaverse 1.0 to an even larger 3D dataset of 10.2M unique objects from a diverse set of sources, object shapes, and categories. We discuss Objaverse-XL and its properties in Section 3.\n\n### 3D Applications\nThe potential of a massive 3D dataset like Objaverse-XL promises exciting novel applications in computer vision, graphics, augmented reality and generative AI. Reconstructing 3D objects from images is a longstanding problem in computer vision and graphics. Here, several methods explore novel representations [12, 59, 38, 39], network architectures [22, 64] and differentiable rendering techniques [30, 10, 52, 34, 35] to predict the 3D shapes and textures of objects from images with or without 3D supervision. All of the aforementioned projects experiment on the small scale ShapeNet. The significantly larger Objaverse-XL could pave the way to new levels of performance, and increase generalization to new domains in a zero-shot fashion. Over the past year, generative AI has made its foray into 3D. MCC [62] learns a generalizable representation with self-supervised learning for 3D reconstruction. DreamFusion [48] and later on Magic3D [33] demonstrated that 3D shapes could be generated from language prompts with the help of text-to-image models. Point-E [42] and Shape-E [28] also train for text-to-3D with the help of 3D models from an undisclosed source. Recently, Zero123 [36] introduced an image-conditioned diffusion model which generates novel object views and is trained on Objaverse 1.0. Stable Dreamfusion [56] replaces the text-to-image model in DreamFusion with the 3D-informed Zero123 and shows improved 3D generations. Recent findings in AI and scaling laws [29, 25] suggest that both generative and predictive models benefit.", "images": [{"name": "img_p2_1", "height": 427, "width": 832}], "items": [{"type": "text", "value": "Source                # Objects\nIKEA [32]                   219\nGSO [17]                     1K\nEGAD [41]                    2K\nOmniObject3D [63]            6K\nPhotoShape [46]              5K\nABO [13]                     8K\nThingi10K [67]             10K\n3d-Future [19]             10K\nShapeNet [9]               51K\nObjaverse 1.0 [14]        800K\nObjaverse-XL             10.2M", "md": "Source                # Objects\nIKEA [32]                   219\nGSO [17]                     1K\nEGAD [41]                    2K\nOmniObject3D [63]            6K\nPhotoShape [46]              5K\nABO [13]                     8K\nThingi10K [67]             10K\n3d-Future [19]             10K\nShapeNet [9]               51K\nObjaverse 1.0 [14]        800K\nObjaverse-XL             10.2M"}, {"type": "table", "rows": [["GitHub", "Thingiverse", "Polycam", "Objaverse 1.0 (Sketchfab)", "Smithsonian"], []], "md": "| GitHub | Thingiverse | Polycam | Objaverse 1.0 (Sketchfab) | Smithsonian |\n|--------|-------------|---------|---------------------------|-------------|\n| Table 1: Number of 3D models in common datasets.", "isPerfectTable": false, "csv": "\"GitHub\",\"Thingiverse\",\"Polycam\",\"Objaverse 1.0 (Sketchfab)\",\"Smithsonian\"\n"}, {"type": "text", "value": "$$\n\\text{Figure 2: t-SNE projection of CLIP L/14 embeddings on a subset of rendered objects. Compared to Objaverse 1.0 (orange), Objaverse-XL more densely captures the distribution of 3D assets.}\n$$\n\nObjaverse-XL is over an order of magnitude larger than prior datasets.\n\nWe introduced a dataset of one billion object masks used to train a model capable of segmenting any object from an image. In language understanding, datasets like Common Crawl [1] have culminated in unprecedented capabilities of large language models such as GPT-4 [43], which in turn power mainstream applications like ChatGPT. The impact of large datasets is undeniable. However, current efforts to collect massive datasets focus on image and language modalities. In this work we introduce and release publicly a massive dataset of 3D objects, called Objaverse-XL. Given the promise of large datasets for 2D vision and language, we believe Objaverse-XL will accelerate research in large-scale training for 3D understanding.", "md": "$$\n\\text{Figure 2: t-SNE projection of CLIP L/14 embeddings on a subset of rendered objects. Compared to Objaverse 1.0 (orange), Objaverse-XL more densely captures the distribution of 3D assets.}\n$$\n\nObjaverse-XL is over an order of magnitude larger than prior datasets.\n\nWe introduced a dataset of one billion object masks used to train a model capable of segmenting any object from an image. In language understanding, datasets like Common Crawl [1] have culminated in unprecedented capabilities of large language models such as GPT-4 [43], which in turn power mainstream applications like ChatGPT. The impact of large datasets is undeniable. However, current efforts to collect massive datasets focus on image and language modalities. In this work we introduce and release publicly a massive dataset of 3D objects, called Objaverse-XL. Given the promise of large datasets for 2D vision and language, we believe Objaverse-XL will accelerate research in large-scale training for 3D understanding."}, {"type": "heading", "lvl": 3, "value": "3D Datasets", "md": "### 3D Datasets"}, {"type": "text", "value": "Existing 3D datasets have been instrumental in yielding significant findings in 3D over the years. ShapeNet [9] has served as the testbed for modeling, representing and predicting 3D shapes in the era of deep learning. ShapeNet provides a collection of 3D shapes, in the form of textured CAD models labeled with semantic categories from WordNet [40]. In theory, it contains 3M CAD models with textures. In practice, a small subset of 51K models is used after filtering by mesh and texture quality. Notwithstanding its impact, ShapeNet objects are of low resolution and textures are often overly simplistic. Other datasets such as ABO [13], GSO [17], and OmniObjects3D [63] improve on the texture quality of their CAD models but are significantly smaller in size with the largest constituting 15K CAD models. Recently, Objaverse 1.0 [14] introduced a 3D dataset of 800K 3D models with high quality and diverse textures, geometry and object types, making it 15\u00d7 larger than prior 3D datasets. While impressive and a step toward a large-scale 3D dataset, Objaverse 1.0 remains several magnitudes smaller than dominant datasets in vision and language. As seen in Figure 2 and Table 1, Objaverse-XL extends Objaverse 1.0 to an even larger 3D dataset of 10.2M unique objects from a diverse set of sources, object shapes, and categories. We discuss Objaverse-XL and its properties in Section 3.", "md": "Existing 3D datasets have been instrumental in yielding significant findings in 3D over the years. ShapeNet [9] has served as the testbed for modeling, representing and predicting 3D shapes in the era of deep learning. ShapeNet provides a collection of 3D shapes, in the form of textured CAD models labeled with semantic categories from WordNet [40]. In theory, it contains 3M CAD models with textures. In practice, a small subset of 51K models is used after filtering by mesh and texture quality. Notwithstanding its impact, ShapeNet objects are of low resolution and textures are often overly simplistic. Other datasets such as ABO [13], GSO [17], and OmniObjects3D [63] improve on the texture quality of their CAD models but are significantly smaller in size with the largest constituting 15K CAD models. Recently, Objaverse 1.0 [14] introduced a 3D dataset of 800K 3D models with high quality and diverse textures, geometry and object types, making it 15\u00d7 larger than prior 3D datasets. While impressive and a step toward a large-scale 3D dataset, Objaverse 1.0 remains several magnitudes smaller than dominant datasets in vision and language. As seen in Figure 2 and Table 1, Objaverse-XL extends Objaverse 1.0 to an even larger 3D dataset of 10.2M unique objects from a diverse set of sources, object shapes, and categories. We discuss Objaverse-XL and its properties in Section 3."}, {"type": "heading", "lvl": 3, "value": "3D Applications", "md": "### 3D Applications"}, {"type": "text", "value": "The potential of a massive 3D dataset like Objaverse-XL promises exciting novel applications in computer vision, graphics, augmented reality and generative AI. Reconstructing 3D objects from images is a longstanding problem in computer vision and graphics. Here, several methods explore novel representations [12, 59, 38, 39], network architectures [22, 64] and differentiable rendering techniques [30, 10, 52, 34, 35] to predict the 3D shapes and textures of objects from images with or without 3D supervision. All of the aforementioned projects experiment on the small scale ShapeNet. The significantly larger Objaverse-XL could pave the way to new levels of performance, and increase generalization to new domains in a zero-shot fashion. Over the past year, generative AI has made its foray into 3D. MCC [62] learns a generalizable representation with self-supervised learning for 3D reconstruction. DreamFusion [48] and later on Magic3D [33] demonstrated that 3D shapes could be generated from language prompts with the help of text-to-image models. Point-E [42] and Shape-E [28] also train for text-to-3D with the help of 3D models from an undisclosed source. Recently, Zero123 [36] introduced an image-conditioned diffusion model which generates novel object views and is trained on Objaverse 1.0. Stable Dreamfusion [56] replaces the text-to-image model in DreamFusion with the 3D-informed Zero123 and shows improved 3D generations. Recent findings in AI and scaling laws [29, 25] suggest that both generative and predictive models benefit.", "md": "The potential of a massive 3D dataset like Objaverse-XL promises exciting novel applications in computer vision, graphics, augmented reality and generative AI. Reconstructing 3D objects from images is a longstanding problem in computer vision and graphics. Here, several methods explore novel representations [12, 59, 38, 39], network architectures [22, 64] and differentiable rendering techniques [30, 10, 52, 34, 35] to predict the 3D shapes and textures of objects from images with or without 3D supervision. All of the aforementioned projects experiment on the small scale ShapeNet. The significantly larger Objaverse-XL could pave the way to new levels of performance, and increase generalization to new domains in a zero-shot fashion. Over the past year, generative AI has made its foray into 3D. MCC [62] learns a generalizable representation with self-supervised learning for 3D reconstruction. DreamFusion [48] and later on Magic3D [33] demonstrated that 3D shapes could be generated from language prompts with the help of text-to-image models. Point-E [42] and Shape-E [28] also train for text-to-3D with the help of 3D models from an undisclosed source. Recently, Zero123 [36] introduced an image-conditioned diffusion model which generates novel object views and is trained on Objaverse 1.0. Stable Dreamfusion [56] replaces the text-to-image model in DreamFusion with the 3D-informed Zero123 and shows improved 3D generations. Recent findings in AI and scaling laws [29, 25] suggest that both generative and predictive models benefit."}]}, {"page": 4, "text": "  GitHub\n  Thingiverse                              2 25\n  Polycam                                                                                          (@9\n                                                                                                     Clastke\n  Smithsonian Institution\n  Sketchfab\n Figure 3:     Examples of 3D objects from various sources of Objaverse-XL                                spanning GitHub,\nThingiverse, Polycam, the Smithsonian Institution, and Sketchfab. Objects from Thingiverse do not\n include color information, so each object\u2019s primary color is randomized during rendering.\n from larger models and larger pre-training datasets. For 3D, Objaverse-XL is by far the largest 3D\n dataset to date and has the potential to facilitate large-scale training for new applications in 3D.\n 3    Objaverse-XL\n Objaverse-XL is a web scale 3D object dataset composed of a highly diverse set of 3D data sources on\n the internet. In this section, we discuss the sources, metadata of the objects, and provide an analysis\n of the objects.\n 3.1    Composition\n Objaverse-XL is composed of 3D objects coming from several sources, including GitHub, Thingiverse,\n Sketchfab, Polycam, and the Smithsonian Institution. We detail each source below.\n GitHub     is a popular online platform for hosting code. We index 37M public files that contain\n common 3D object extensions; in particular,              .obj,   .glb,    .gltf,    .usdz,    .usd,    .usda,    .fbx,   .stl,\n .dae,   .ply,   .abc, and     .blend. These extensions were chosen as they are best supported in Blender,\nwhich we use to render 2D images of the 3D objects. We only index objects that come from \u201cbase\u201d\n GitHub repositories (i.e.      non-forked repos, excluding forks that had more stars than the original repo).\n In total, the files come from over 500K repositories.\nAcross all of Objaverse-XL, objects are deduplicated by file content hash, which removes approxi-\n mately 23 million files. Among the remaining files, we were able to import and successfully render\n                                                                4", "md": "# Objaverse-XL\n\n# Objaverse-XL\n\nObjaverse-XL is a web scale 3D object dataset composed of a highly diverse set of 3D data sources on the internet. In this section, we discuss the sources, metadata of the objects, and provide an analysis of the objects.\n\n## Composition\n\nObjaverse-XL is composed of 3D objects coming from several sources, including GitHub, Thingiverse, Sketchfab, Polycam, and the Smithsonian Institution. We detail each source below.\n\n### GitHub\n\nGitHub is a popular online platform for hosting code. We index 37M public files that contain common 3D object extensions; in particular, .obj, .glb, .gltf, .usdz, .usd, .usda, .fbx, .stl, .dae, .ply, .abc, and .blend. These extensions were chosen as they are best supported in Blender, which we use to render 2D images of the 3D objects. We only index objects that come from \u201cbase\u201d GitHub repositories (i.e. non-forked repos, excluding forks that had more stars than the original repo). In total, the files come from over 500K repositories.\n\nAcross all of Objaverse-XL, objects are deduplicated by file content hash, which removes approximately 23 million files. Among the remaining files, we were able to import and successfully render.", "images": [{"name": "img_p3_1", "height": 1021, "width": 1189}], "items": [{"type": "heading", "lvl": 1, "value": "Objaverse-XL", "md": "# Objaverse-XL"}, {"type": "heading", "lvl": 1, "value": "Objaverse-XL", "md": "# Objaverse-XL"}, {"type": "text", "value": "Objaverse-XL is a web scale 3D object dataset composed of a highly diverse set of 3D data sources on the internet. In this section, we discuss the sources, metadata of the objects, and provide an analysis of the objects.", "md": "Objaverse-XL is a web scale 3D object dataset composed of a highly diverse set of 3D data sources on the internet. In this section, we discuss the sources, metadata of the objects, and provide an analysis of the objects."}, {"type": "heading", "lvl": 2, "value": "Composition", "md": "## Composition"}, {"type": "text", "value": "Objaverse-XL is composed of 3D objects coming from several sources, including GitHub, Thingiverse, Sketchfab, Polycam, and the Smithsonian Institution. We detail each source below.", "md": "Objaverse-XL is composed of 3D objects coming from several sources, including GitHub, Thingiverse, Sketchfab, Polycam, and the Smithsonian Institution. We detail each source below."}, {"type": "heading", "lvl": 3, "value": "GitHub", "md": "### GitHub"}, {"type": "text", "value": "GitHub is a popular online platform for hosting code. We index 37M public files that contain common 3D object extensions; in particular, .obj, .glb, .gltf, .usdz, .usd, .usda, .fbx, .stl, .dae, .ply, .abc, and .blend. These extensions were chosen as they are best supported in Blender, which we use to render 2D images of the 3D objects. We only index objects that come from \u201cbase\u201d GitHub repositories (i.e. non-forked repos, excluding forks that had more stars than the original repo). In total, the files come from over 500K repositories.\n\nAcross all of Objaverse-XL, objects are deduplicated by file content hash, which removes approximately 23 million files. Among the remaining files, we were able to import and successfully render.", "md": "GitHub is a popular online platform for hosting code. We index 37M public files that contain common 3D object extensions; in particular, .obj, .glb, .gltf, .usdz, .usd, .usda, .fbx, .stl, .dae, .ply, .abc, and .blend. These extensions were chosen as they are best supported in Blender, which we use to render 2D images of the 3D objects. We only index objects that come from \u201cbase\u201d GitHub repositories (i.e. non-forked repos, excluding forks that had more stars than the original repo). In total, the files come from over 500K repositories.\n\nAcross all of Objaverse-XL, objects are deduplicated by file content hash, which removes approximately 23 million files. Among the remaining files, we were able to import and successfully render."}]}, {"page": 5, "text": "                                                                                                     0  1 2  3  4\n(a) Object location density in the United States             (b) Choropleth map of objects per country (log scale)\n                                                          Pose          zombietable sword doortaq\n                                                                  glasses                                           smalidbox\n                                                                                      Fai                  new  08       @\n                                                            leve]                             4                        tani\n                                                          left     chair  room   dead    ccorbo                 mod   #rreter\n                                                          sto  pon         lower   handz  menenv           obj with      U\n                                                                    cornem                      Lyeop                    61g\n                                   Duplicates (23M)                   pipe ( @U toprarinbow enemy        gun      8 4\n    GitHub (37M)                                              ceif tfemale          FU    grass) 8('loop 1  brokn       80\n                                                           9    16      2                 sphere3   bld       stand       Flod\n                                                              89            waod              characte           old      out\n                                                                  hatligharre] J   0        Todo                   male\n                                                          Fe                              Opart\n                                                              fish        barrel Kaieishest    cube      castle      \"QOhuman\n                        Rendered Successfully (10M)         teleath  body aim  Mesr1g     cask      @o::       Lamp  5\n                                                             rifle        2        sh                                   00 assel\n    Sketchfab (800K)                                      final   durag   60     headbush  car                greenV2  Ewindow\n    Polycam (72K)                                           lodi                     ball                        10d2  5 (0\n    Smithsonian (2.4K)                                                     char                         tanding coarse:\n    Thingiverse (3.5M)     Blender Incompatible (8M)      yellow  forward hairbluebridgerock basenonpbros              dancing\n (c) Sankey diagram of Objaverse-XL sources                         (d) Word cloud of file names for GitHub\n0.30       Polygon Count                   Vertex Count         0.30        Edge Count           Animation Count          459K\n0.25                            0.3                             0.25                               41K\n0.20                                                            0.20\n0.15                            0.2                             0.15                             Armature Count\n0.10                            0.1                             0.10                                                     438K\n0.05                                                            0.05                               34K\n0.00     1   2  3   4  5   6  7 0.0    1   2   3  4   5   6   7 0.00   1   2   3  4   5  6   7     Objaverse-XL   Objaverse 1.0\n     0 10  10  10 10  10 10  10    0  10 10  10  10  10 10  10     0  10 10  10  10 10  10  10\n                             (e) Statistics of the metadata extracted from the 3D objects.\nFigure 4:    Analysis of metadata from Objaverse-XL.                     Locations of geotagged objects in (a) the\nUnited States and (b) around the world. (c) Various sources and their contribution to Objaverse-XL.\n(d) Frequency of filenames of GitHub objects. (e) Further statistics of collected 3D objects.\n5.5 million of those files. Files that were not successfully rendered were either caused by import\ncompatibility issues (i.e.       FBX ASCII files are not natively importable to Blender), no meshes are\nin the files, or the file is not a valid 3D file (e.g.         an .obj    file may be a C compiler file instead of a\nWavefront Object file). Moving forward, we expect a solution for converting 3D file formats into a\nconsolidated representation may yield several million more unique 3D objects.\nThingiverse      is a platform for sharing objects most commonly used for 3D printing. We index and\ndownload around 3.5 million objects from the platform, which are predominantly released under\nCreative Commons licenses. The vast majority of the objects are STL files, which are often watertight\nmeshes that are untextured, and serve as useful data for learning a shape prior. During rendering, we\nrandomize the colors to broaden the distribution of the images.\nSketchfab     is an online platform where users can publish and share 3D models, encompassing a broad\nvariety of categories. The data sourced from Sketchfab for our project is specifically from Objaverse\n1.0, a dataset of 800K objects consisting of Creative Commons-licensed 3D models. Each model is\ndistributed as a standardized GLB file. The 3D models are freely usable and modifiable, covering an\narray of object types, from real-world 3D scans to intricate designs created in 3D software.\nPolycam      is a 3D scanning mobile application designed to facilitate the acquisition and sharing of\n3D data. One of its salient features is the          explore    functionality, which enables members of the user\ncommunity to contribute their 3D scans to a publicly accessible database. In the context of our dataset,\nwe focus specifically on the subset of objects within the explore page that are designated as savable.\nThese savable objects are governed by a Creative Commons Attribution 4.0 International License\n                                                               5", "md": "# OCR Text Analysis\n\n# OCR Text Analysis\n\n## (a) Object location density in the United States\n\n## (b) Choropleth map of objects per country (log scale)\n\n$$\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{Pose} & \\text{zombietable} & \\text{sword} & \\text{doortaq} \\\\\n\\hline\n\\text{glasses} & & & \\\\\n\\hline\n\\text{Fai} & & & \\\\\n\\hline\n\\text{leve]} & & & & \\\\\n\\hline\n\\text{left} & \\text{chair} & \\text{room} & \\text{dead} & \\text{ccorbo} & \\text{mod} & \\text{#rreter} \\\\\n\\hline\n\\text{sto} & \\text{pon} & & \\text{lower} & \\text{handz} & \\text{menenv} & \\text{obj with} & \\text{U} \\\\\n\\hline\n\\text{cornem} & & & & \\text{Lyeop} & & & \\text{61g} \\\\\n\\hline\n\\text{Duplicates (23M)} & \\text{pipe} & & \\text{toprarinbow} & \\text{enemy} & \\text{gun} & & \\text{8} \\\\\n\\hline\n\\text{GitHub (37M)} & \\text{ceif} & \\text{tfemale} & \\text{FU} & \\text{grass} & \\text{('loop} & \\text{brokn} & \\text{80} \\\\\n\\hline\n\\text{9} & & \\text{16} & & & \\text{sphere3} & \\text{bld} & \\text{stand} & \\text{Flod} \\\\\n\\hline\n\\text{89} & & & & & & \\text{characte} & & \\text{old} & \\text{out} \\\\\n\\hline\n\\text{hatligharre]} & \\text{J} & & & \\text{Todo} & & & \\text{male} \\\\\n\\hline\n\\text{Fe} & & & & & & \\text{Opart} & \\\\\n\\hline\n\\text{fish} & \\text{barrel} & \\text{Kaieishest} & \\text{cube} & \\text{castle} & & \\text{\"QOhuman\"} \\\\\n\\hline\n\\text{Rendered Successfully (10M)} & \\text{teleath} & \\text{body} & \\text{aim} & \\text{Mesr1g} & \\text{cask} & \\text{@o::} & \\text{Lamp} & \\text{5} \\\\\n\\hline\n\\text{rifle} & & \\text{2} & & & & & & \\text{00} & \\text{assel} \\\\\n\\hline\n\\text{Sketchfab (800K)} & \\text{final} & \\text{durag} & \\text{60} & \\text{headbush} & \\text{car} & \\text{greenV2} & \\text{Ewindow} \\\\\n\\hline\n\\text{Polycam (72K)} & & & & & & & \\text{10d2} & \\text{5 (0} \\\\\n\\hline\n\\text{Smithsonian (2.4K)} & & & & & & & & & \\\\\n\\hline\n\\text{Thingiverse (3.5M)} & \\text{Blender Incompatible (8M)} & \\text{yellow} & \\text{forward} & \\text{hairbluebridgerock} & \\text{basenonpbros} & & \\text{dancing} \\\\\n\\hline\n\\end{array}$$\n\n## (c) Sankey diagram of Objaverse-XL sources\n\n## (d) Word cloud of file names for GitHub\n\n$$\\begin{array}{|c|c|c|c|}\n\\hline\n0.30 & \\text{Polygon Count} & & \\text{Vertex Count} \\\\\n\\hline\n0.25 & & 0.3 & \\\\\n\\hline\n0.20 & & & \\\\\n\\hline\n0.15 & & 0.2 & \\\\\n\\hline\n0.10 & & 0.1 & & \\text{Armature Count} \\\\\n\\hline\n0.05 & & & 0.05 & \\\\\n\\hline\n0.00 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 0.0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 0.00 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & \\text{Objaverse-XL} & \\text{Objaverse 1.0} \\\\\n\\hline\n0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\\\\n\\hline\n\\end{array}$$\n\n## (e) Statistics of the metadata extracted from the 3D objects\n\nFigure 4: Analysis of metadata from Objaverse-XL. Locations of geotagged objects in (a) the United States and (b) around the world. (c) Various sources and their contribution to Objaverse-XL. (d) Frequency of filenames of GitHub objects. (e) Further statistics of collected 3D objects. 5.5 million of those files. Files that were not successfully rendered were either caused by import compatibility issues (i.e. FBX ASCII files are not natively importable to Blender), no meshes are in the files, or the file is not a valid 3D file (e.g. an .obj file may be a C compiler file instead of a Wavefront Object file). Moving forward, we expect a solution for converting 3D file formats into a consolidated representation may yield several million more unique 3D objects. Thingiverse is a platform for sharing objects most commonly used for 3D printing. We index and download around 3.5 million objects from the platform, which are predominantly released under Creative Commons licenses. The vast majority of the objects are STL files, which are often watertight meshes that are untextured, and serve as useful data for learning a shape prior. During rendering, we randomize the colors to broaden the distribution of the images. Sketchfab is an online platform where users can publish and share 3D models, encompassing a broad variety of categories. The data sourced from Sketchfab for our project is specifically from Objaverse 1.0, a dataset of 800K objects consisting of Creative Commons-licensed 3D models. Each model is distributed as a standardized GLB file. The 3D models are freely usable and modifiable, covering an array of object types, from real-world 3D scans to intricate designs created in 3D software. Polycam is a 3D scanning mobile application designed to facilitate the acquisition and sharing of 3D data. One of its salient features is the explore functionality, which enables members of the user community to contribute their 3D scans to a publicly accessible database. In the context of our dataset, we focus specifically on the subset of objects within the explore page that are designated as savable. These savable objects are governed by a Creative Commons Attribution 4.0 International License.", "images": [{"name": "img_p4_1", "height": 1, "width": 256}, {"name": "img_p4_2", "height": 1, "width": 256}, {"name": "img_p4_13", "height": 337, "width": 649}], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text Analysis", "md": "# OCR Text Analysis"}, {"type": "heading", "lvl": 1, "value": "OCR Text Analysis", "md": "# OCR Text Analysis"}, {"type": "heading", "lvl": 2, "value": "(a) Object location density in the United States", "md": "## (a) Object location density in the United States"}, {"type": "heading", "lvl": 2, "value": "(b) Choropleth map of objects per country (log scale)", "md": "## (b) Choropleth map of objects per country (log scale)"}, {"type": "text", "value": "$$\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{Pose} & \\text{zombietable} & \\text{sword} & \\text{doortaq} \\\\\n\\hline\n\\text{glasses} & & & \\\\\n\\hline\n\\text{Fai} & & & \\\\\n\\hline\n\\text{leve]} & & & & \\\\\n\\hline\n\\text{left} & \\text{chair} & \\text{room} & \\text{dead} & \\text{ccorbo} & \\text{mod} & \\text{#rreter} \\\\\n\\hline\n\\text{sto} & \\text{pon} & & \\text{lower} & \\text{handz} & \\text{menenv} & \\text{obj with} & \\text{U} \\\\\n\\hline\n\\text{cornem} & & & & \\text{Lyeop} & & & \\text{61g} \\\\\n\\hline\n\\text{Duplicates (23M)} & \\text{pipe} & & \\text{toprarinbow} & \\text{enemy} & \\text{gun} & & \\text{8} \\\\\n\\hline\n\\text{GitHub (37M)} & \\text{ceif} & \\text{tfemale} & \\text{FU} & \\text{grass} & \\text{('loop} & \\text{brokn} & \\text{80} \\\\\n\\hline\n\\text{9} & & \\text{16} & & & \\text{sphere3} & \\text{bld} & \\text{stand} & \\text{Flod} \\\\\n\\hline\n\\text{89} & & & & & & \\text{characte} & & \\text{old} & \\text{out} \\\\\n\\hline\n\\text{hatligharre]} & \\text{J} & & & \\text{Todo} & & & \\text{male} \\\\\n\\hline\n\\text{Fe} & & & & & & \\text{Opart} & \\\\\n\\hline\n\\text{fish} & \\text{barrel} & \\text{Kaieishest} & \\text{cube} & \\text{castle} & & \\text{\"QOhuman\"} \\\\\n\\hline\n\\text{Rendered Successfully (10M)} & \\text{teleath} & \\text{body} & \\text{aim} & \\text{Mesr1g} & \\text{cask} & \\text{@o::} & \\text{Lamp} & \\text{5} \\\\\n\\hline\n\\text{rifle} & & \\text{2} & & & & & & \\text{00} & \\text{assel} \\\\\n\\hline\n\\text{Sketchfab (800K)} & \\text{final} & \\text{durag} & \\text{60} & \\text{headbush} & \\text{car} & \\text{greenV2} & \\text{Ewindow} \\\\\n\\hline\n\\text{Polycam (72K)} & & & & & & & \\text{10d2} & \\text{5 (0} \\\\\n\\hline\n\\text{Smithsonian (2.4K)} & & & & & & & & & \\\\\n\\hline\n\\text{Thingiverse (3.5M)} & \\text{Blender Incompatible (8M)} & \\text{yellow} & \\text{forward} & \\text{hairbluebridgerock} & \\text{basenonpbros} & & \\text{dancing} \\\\\n\\hline\n\\end{array}$$", "md": "$$\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{Pose} & \\text{zombietable} & \\text{sword} & \\text{doortaq} \\\\\n\\hline\n\\text{glasses} & & & \\\\\n\\hline\n\\text{Fai} & & & \\\\\n\\hline\n\\text{leve]} & & & & \\\\\n\\hline\n\\text{left} & \\text{chair} & \\text{room} & \\text{dead} & \\text{ccorbo} & \\text{mod} & \\text{#rreter} \\\\\n\\hline\n\\text{sto} & \\text{pon} & & \\text{lower} & \\text{handz} & \\text{menenv} & \\text{obj with} & \\text{U} \\\\\n\\hline\n\\text{cornem} & & & & \\text{Lyeop} & & & \\text{61g} \\\\\n\\hline\n\\text{Duplicates (23M)} & \\text{pipe} & & \\text{toprarinbow} & \\text{enemy} & \\text{gun} & & \\text{8} \\\\\n\\hline\n\\text{GitHub (37M)} & \\text{ceif} & \\text{tfemale} & \\text{FU} & \\text{grass} & \\text{('loop} & \\text{brokn} & \\text{80} \\\\\n\\hline\n\\text{9} & & \\text{16} & & & \\text{sphere3} & \\text{bld} & \\text{stand} & \\text{Flod} \\\\\n\\hline\n\\text{89} & & & & & & \\text{characte} & & \\text{old} & \\text{out} \\\\\n\\hline\n\\text{hatligharre]} & \\text{J} & & & \\text{Todo} & & & \\text{male} \\\\\n\\hline\n\\text{Fe} & & & & & & \\text{Opart} & \\\\\n\\hline\n\\text{fish} & \\text{barrel} & \\text{Kaieishest} & \\text{cube} & \\text{castle} & & \\text{\"QOhuman\"} \\\\\n\\hline\n\\text{Rendered Successfully (10M)} & \\text{teleath} & \\text{body} & \\text{aim} & \\text{Mesr1g} & \\text{cask} & \\text{@o::} & \\text{Lamp} & \\text{5} \\\\\n\\hline\n\\text{rifle} & & \\text{2} & & & & & & \\text{00} & \\text{assel} \\\\\n\\hline\n\\text{Sketchfab (800K)} & \\text{final} & \\text{durag} & \\text{60} & \\text{headbush} & \\text{car} & \\text{greenV2} & \\text{Ewindow} \\\\\n\\hline\n\\text{Polycam (72K)} & & & & & & & \\text{10d2} & \\text{5 (0} \\\\\n\\hline\n\\text{Smithsonian (2.4K)} & & & & & & & & & \\\\\n\\hline\n\\text{Thingiverse (3.5M)} & \\text{Blender Incompatible (8M)} & \\text{yellow} & \\text{forward} & \\text{hairbluebridgerock} & \\text{basenonpbros} & & \\text{dancing} \\\\\n\\hline\n\\end{array}$$"}, {"type": "heading", "lvl": 2, "value": "(c) Sankey diagram of Objaverse-XL sources", "md": "## (c) Sankey diagram of Objaverse-XL sources"}, {"type": "heading", "lvl": 2, "value": "(d) Word cloud of file names for GitHub", "md": "## (d) Word cloud of file names for GitHub"}, {"type": "text", "value": "$$\\begin{array}{|c|c|c|c|}\n\\hline\n0.30 & \\text{Polygon Count} & & \\text{Vertex Count} \\\\\n\\hline\n0.25 & & 0.3 & \\\\\n\\hline\n0.20 & & & \\\\\n\\hline\n0.15 & & 0.2 & \\\\\n\\hline\n0.10 & & 0.1 & & \\text{Armature Count} \\\\\n\\hline\n0.05 & & & 0.05 & \\\\\n\\hline\n0.00 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 0.0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 0.00 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & \\text{Objaverse-XL} & \\text{Objaverse 1.0} \\\\\n\\hline\n0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\\\\n\\hline\n\\end{array}$$", "md": "$$\\begin{array}{|c|c|c|c|}\n\\hline\n0.30 & \\text{Polygon Count} & & \\text{Vertex Count} \\\\\n\\hline\n0.25 & & 0.3 & \\\\\n\\hline\n0.20 & & & \\\\\n\\hline\n0.15 & & 0.2 & \\\\\n\\hline\n0.10 & & 0.1 & & \\text{Armature Count} \\\\\n\\hline\n0.05 & & & 0.05 & \\\\\n\\hline\n0.00 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 0.0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 0.00 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & \\text{Objaverse-XL} & \\text{Objaverse 1.0} \\\\\n\\hline\n0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 0 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\\\\n\\hline\n\\end{array}$$"}, {"type": "heading", "lvl": 2, "value": "(e) Statistics of the metadata extracted from the 3D objects", "md": "## (e) Statistics of the metadata extracted from the 3D objects"}, {"type": "text", "value": "Figure 4: Analysis of metadata from Objaverse-XL. Locations of geotagged objects in (a) the United States and (b) around the world. (c) Various sources and their contribution to Objaverse-XL. (d) Frequency of filenames of GitHub objects. (e) Further statistics of collected 3D objects. 5.5 million of those files. Files that were not successfully rendered were either caused by import compatibility issues (i.e. FBX ASCII files are not natively importable to Blender), no meshes are in the files, or the file is not a valid 3D file (e.g. an .obj file may be a C compiler file instead of a Wavefront Object file). Moving forward, we expect a solution for converting 3D file formats into a consolidated representation may yield several million more unique 3D objects. Thingiverse is a platform for sharing objects most commonly used for 3D printing. We index and download around 3.5 million objects from the platform, which are predominantly released under Creative Commons licenses. The vast majority of the objects are STL files, which are often watertight meshes that are untextured, and serve as useful data for learning a shape prior. During rendering, we randomize the colors to broaden the distribution of the images. Sketchfab is an online platform where users can publish and share 3D models, encompassing a broad variety of categories. The data sourced from Sketchfab for our project is specifically from Objaverse 1.0, a dataset of 800K objects consisting of Creative Commons-licensed 3D models. Each model is distributed as a standardized GLB file. The 3D models are freely usable and modifiable, covering an array of object types, from real-world 3D scans to intricate designs created in 3D software. Polycam is a 3D scanning mobile application designed to facilitate the acquisition and sharing of 3D data. One of its salient features is the explore functionality, which enables members of the user community to contribute their 3D scans to a publicly accessible database. In the context of our dataset, we focus specifically on the subset of objects within the explore page that are designated as savable. These savable objects are governed by a Creative Commons Attribution 4.0 International License.", "md": "Figure 4: Analysis of metadata from Objaverse-XL. Locations of geotagged objects in (a) the United States and (b) around the world. (c) Various sources and their contribution to Objaverse-XL. (d) Frequency of filenames of GitHub objects. (e) Further statistics of collected 3D objects. 5.5 million of those files. Files that were not successfully rendered were either caused by import compatibility issues (i.e. FBX ASCII files are not natively importable to Blender), no meshes are in the files, or the file is not a valid 3D file (e.g. an .obj file may be a C compiler file instead of a Wavefront Object file). Moving forward, we expect a solution for converting 3D file formats into a consolidated representation may yield several million more unique 3D objects. Thingiverse is a platform for sharing objects most commonly used for 3D printing. We index and download around 3.5 million objects from the platform, which are predominantly released under Creative Commons licenses. The vast majority of the objects are STL files, which are often watertight meshes that are untextured, and serve as useful data for learning a shape prior. During rendering, we randomize the colors to broaden the distribution of the images. Sketchfab is an online platform where users can publish and share 3D models, encompassing a broad variety of categories. The data sourced from Sketchfab for our project is specifically from Objaverse 1.0, a dataset of 800K objects consisting of Creative Commons-licensed 3D models. Each model is distributed as a standardized GLB file. The 3D models are freely usable and modifiable, covering an array of object types, from real-world 3D scans to intricate designs created in 3D software. Polycam is a 3D scanning mobile application designed to facilitate the acquisition and sharing of 3D data. One of its salient features is the explore functionality, which enables members of the user community to contribute their 3D scans to a publicly accessible database. In the context of our dataset, we focus specifically on the subset of objects within the explore page that are designated as savable. These savable objects are governed by a Creative Commons Attribution 4.0 International License."}]}, {"page": 6, "text": "(CC-BY 4.0). We indexed 72K objects that were marked as savable and licensed under a CC-BY 4.0\nlicense. Following deduplication, we obtain 71K unique objects.\nSmithsonian 3D Digitization            is a project by the Smithsonian Institution dedicated to digitizing their\nvast collection of historical and cultural artifacts. The project has provided us with a set of 2.4K\nmodels, all licensed under a CC0 license, which signifies that these works are fully in the public\ndomain and free for use without any restrictions. The objects in this collection are primarily scans of\nreal-world artifacts. Each model is distributed in a standardized compressed GLB format.\n3.2    Metadata\nEach object comes with metadata from its source, and we also extract metadata from it in Blender\nand from its CLIP ViT-L/14 features. We describe the metadata acquisition process below.\nSource Metadata.           From the source, we often get metadata such as its popularity, license, and some\ntextual description. For example, on GitHub, the popularity is represented by the stars of the object\u2019s\nrepository and the file name serves as the object\u2019s textual pair.\nBlender Metadata.             For each object that we render, we obtain the following metadata for\nit:   sha256,      file-size,       polygon-count,           vertex-count,          edge-count,         material-count,\ntexture-count,          object-count,         animation-count,          linked-files,         scene-dimensions, and\nmissing-textures. During rendering, for objects that have a missing texture, we randomize the\ncolor of that texture. Figure 4 shows some charts extracted from the metadata, including density plots\nover the number of polygons, vertex counts, and edge counts.\nAnimated Objects.            From the Blender metadata, we find that the number of animated objects and\nthose with armature (a digital skeleton used to animate 3D models) significantly increases from\nObjaverse 1.0 to Objaverse-XL. Figure 4e (right) shows a bar chart of the increase, specifically from\n41K to 459K animated objects and from 34K to 438K objects with armature.\nModel Metadata.            For each object, we extract its CLIP ViT-L/14 [50] image embedding by averag-\ning the CLIP embedding from 12 different renders of the object at random camera positions inside of\na hollow sphere. We use the CLIP embeddings to predict different metadata properties, including\naesthetic scores, NSFW predictions, face detection, and for detecting holes in the photogrammetry\nrenderings. Section C.3 provides more details on the analysis.\n3.3    Analysis\nNSFW annotations.             Most data sources used for the creation of Objaverse-XL already have either a\nstrict NSFW policy or strong self-filtering. However, owing to the web scale of Objaverse-XL we\nperformed NSFW filtering using the rendered images of the objects. Each 3D object is rendered\nin 12 random views and each rendered image is passed through an NSFW classifier trained on the\nNSFW dataset introduced in LAION-5B [55] by Gadre et al.                          [20]  using the CLIP ViT-L/14 [50]\nfeatures. After careful analysis and manual inspection, we marked a rendered image as NSFW if\nit has an NSFW score above            0.9  and a 3D object is marked as NSFW if at least                3 rendered images\nare deemed to be NSFW. Overall, only              815   objects out of the 10M are filtered out as NSFW objects.\nNote that the high threshold and multi-view consistency are needed due to the distribution shift\nbetween LAION-5B and Objaverse-XL along with NSFW classification of certain viewpoint renders\nof harmless 3D objects.\nFace detection.        We analyze the presence of faces in Objaverse-XL using a detector trained by Gadre\net al. [20]. Like NSFW filtering, we count the objects where at least                      3  images contain a detected\nface. Out of 10M assets, we estimate             266K objects include faces. However, unlike most web-scale\ndatasets, the faces present in Objaverse-XL often come from the scans of dolls, historical sculptures,\nand anthropomorphic animations. Hence, there are less privacy concerns with most of these objects.\nPhotogrammetry hole detection.                 When scanning 3D objects, if the back or bottom of the object is\nnot scanned, rendering from various viewpoints may contain holes, leading to a \u201cbad\u201d render image.\nFor example, a non-trivial number of Polycam 3D objects lack the information from the \u201cback side\u201d.\n                                                               6", "md": "# Document\n\n(CC-BY 4.0). We indexed 72K objects that were marked as savable and licensed under a CC-BY 4.0\nlicense. Following deduplication, we obtain 71K unique objects.\n\nSmithsonian 3D Digitization is a project by the Smithsonian Institution dedicated to digitizing their\nvast collection of historical and cultural artifacts. The project has provided us with a set of 2.4K\nmodels, all licensed under a CC0 license, which signifies that these works are fully in the public\ndomain and free for use without any restrictions. The objects in this collection are primarily scans of\nreal-world artifacts. Each model is distributed in a standardized compressed GLB format.\n\n### 3.2 Metadata\n\nEach object comes with metadata from its source, and we also extract metadata from it in Blender\nand from its CLIP ViT-L/14 features. We describe the metadata acquisition process below.\n\nSource Metadata. From the source, we often get metadata such as its popularity, license, and some\ntextual description. For example, on GitHub, the popularity is represented by the stars of the object\u2019s\nrepository and the file name serves as the object\u2019s textual pair.\n\nBlender Metadata. For each object that we render, we obtain the following metadata for\nit: sha256, file-size, polygon-count, vertex-count, edge-count, material-count,\ntexture-count, object-count, animation-count, linked-files, scene-dimensions, and\nmissing-textures. During rendering, for objects that have a missing texture, we randomize the\ncolor of that texture. Figure 4 shows some charts extracted from the metadata, including density plots\nover the number of polygons, vertex counts, and edge counts.\n\nAnimated Objects. From the Blender metadata, we find that the number of animated objects and\nthose with armature (a digital skeleton used to animate 3D models) significantly increases from\nObjaverse 1.0 to Objaverse-XL. Figure 4e (right) shows a bar chart of the increase, specifically from\n41K to 459K animated objects and from 34K to 438K objects with armature.\n\nModel Metadata. For each object, we extract its CLIP ViT-L/14 [50] image embedding by averaging\nthe CLIP embedding from 12 different renders of the object at random camera positions inside of\na hollow sphere. We use the CLIP embeddings to predict different metadata properties, including\naesthetic scores, NSFW predictions, face detection, and for detecting holes in the photogrammetry\nrenderings. Section C.3 provides more details on the analysis.\n\n### 3.3 Analysis\n\nNSFW annotations. Most data sources used for the creation of Objaverse-XL already have either a\nstrict NSFW policy or strong self-filtering. However, owing to the web scale of Objaverse-XL we\nperformed NSFW filtering using the rendered images of the objects. Each 3D object is rendered\nin 12 random views and each rendered image is passed through an NSFW classifier trained on the\nNSFW dataset introduced in LAION-5B [55] by Gadre et al. [20] using the CLIP ViT-L/14 [50]\nfeatures. After careful analysis and manual inspection, we marked a rendered image as NSFW if\nit has an NSFW score above $$0.9$$ and a 3D object is marked as NSFW if at least $$3$$ rendered images\nare deemed to be NSFW. Overall, only $$815$$ objects out of the 10M are filtered out as NSFW objects.\nNote that the high threshold and multi-view consistency are needed due to the distribution shift\nbetween LAION-5B and Objaverse-XL along with NSFW classification of certain viewpoint renders\nof harmless 3D objects.\n\nFace detection. We analyze the presence of faces in Objaverse-XL using a detector trained by Gadre\net al. [20]. Like NSFW filtering, we count the objects where at least $$3$$ images contain a detected\nface. Out of 10M assets, we estimate $$266K$$ objects include faces. However, unlike most web-scale\ndatasets, the faces present in Objaverse-XL often come from the scans of dolls, historical sculptures,\nand anthropomorphic animations. Hence, there are less privacy concerns with most of these objects.\n\nPhotogrammetry hole detection. When scanning 3D objects, if the back or bottom of the object is\nnot scanned, rendering from various viewpoints may contain holes, leading to a \u201cbad\u201d render image.\nFor example, a non-trivial number of Polycam 3D objects lack the information from the \u201cback side\u201d.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "(CC-BY 4.0). We indexed 72K objects that were marked as savable and licensed under a CC-BY 4.0\nlicense. Following deduplication, we obtain 71K unique objects.\n\nSmithsonian 3D Digitization is a project by the Smithsonian Institution dedicated to digitizing their\nvast collection of historical and cultural artifacts. The project has provided us with a set of 2.4K\nmodels, all licensed under a CC0 license, which signifies that these works are fully in the public\ndomain and free for use without any restrictions. The objects in this collection are primarily scans of\nreal-world artifacts. Each model is distributed in a standardized compressed GLB format.", "md": "(CC-BY 4.0). We indexed 72K objects that were marked as savable and licensed under a CC-BY 4.0\nlicense. Following deduplication, we obtain 71K unique objects.\n\nSmithsonian 3D Digitization is a project by the Smithsonian Institution dedicated to digitizing their\nvast collection of historical and cultural artifacts. The project has provided us with a set of 2.4K\nmodels, all licensed under a CC0 license, which signifies that these works are fully in the public\ndomain and free for use without any restrictions. The objects in this collection are primarily scans of\nreal-world artifacts. Each model is distributed in a standardized compressed GLB format."}, {"type": "heading", "lvl": 3, "value": "3.2 Metadata", "md": "### 3.2 Metadata"}, {"type": "text", "value": "Each object comes with metadata from its source, and we also extract metadata from it in Blender\nand from its CLIP ViT-L/14 features. We describe the metadata acquisition process below.\n\nSource Metadata. From the source, we often get metadata such as its popularity, license, and some\ntextual description. For example, on GitHub, the popularity is represented by the stars of the object\u2019s\nrepository and the file name serves as the object\u2019s textual pair.\n\nBlender Metadata. For each object that we render, we obtain the following metadata for\nit: sha256, file-size, polygon-count, vertex-count, edge-count, material-count,\ntexture-count, object-count, animation-count, linked-files, scene-dimensions, and\nmissing-textures. During rendering, for objects that have a missing texture, we randomize the\ncolor of that texture. Figure 4 shows some charts extracted from the metadata, including density plots\nover the number of polygons, vertex counts, and edge counts.\n\nAnimated Objects. From the Blender metadata, we find that the number of animated objects and\nthose with armature (a digital skeleton used to animate 3D models) significantly increases from\nObjaverse 1.0 to Objaverse-XL. Figure 4e (right) shows a bar chart of the increase, specifically from\n41K to 459K animated objects and from 34K to 438K objects with armature.\n\nModel Metadata. For each object, we extract its CLIP ViT-L/14 [50] image embedding by averaging\nthe CLIP embedding from 12 different renders of the object at random camera positions inside of\na hollow sphere. We use the CLIP embeddings to predict different metadata properties, including\naesthetic scores, NSFW predictions, face detection, and for detecting holes in the photogrammetry\nrenderings. Section C.3 provides more details on the analysis.", "md": "Each object comes with metadata from its source, and we also extract metadata from it in Blender\nand from its CLIP ViT-L/14 features. We describe the metadata acquisition process below.\n\nSource Metadata. From the source, we often get metadata such as its popularity, license, and some\ntextual description. For example, on GitHub, the popularity is represented by the stars of the object\u2019s\nrepository and the file name serves as the object\u2019s textual pair.\n\nBlender Metadata. For each object that we render, we obtain the following metadata for\nit: sha256, file-size, polygon-count, vertex-count, edge-count, material-count,\ntexture-count, object-count, animation-count, linked-files, scene-dimensions, and\nmissing-textures. During rendering, for objects that have a missing texture, we randomize the\ncolor of that texture. Figure 4 shows some charts extracted from the metadata, including density plots\nover the number of polygons, vertex counts, and edge counts.\n\nAnimated Objects. From the Blender metadata, we find that the number of animated objects and\nthose with armature (a digital skeleton used to animate 3D models) significantly increases from\nObjaverse 1.0 to Objaverse-XL. Figure 4e (right) shows a bar chart of the increase, specifically from\n41K to 459K animated objects and from 34K to 438K objects with armature.\n\nModel Metadata. For each object, we extract its CLIP ViT-L/14 [50] image embedding by averaging\nthe CLIP embedding from 12 different renders of the object at random camera positions inside of\na hollow sphere. We use the CLIP embeddings to predict different metadata properties, including\naesthetic scores, NSFW predictions, face detection, and for detecting holes in the photogrammetry\nrenderings. Section C.3 provides more details on the analysis."}, {"type": "heading", "lvl": 3, "value": "3.3 Analysis", "md": "### 3.3 Analysis"}, {"type": "text", "value": "NSFW annotations. Most data sources used for the creation of Objaverse-XL already have either a\nstrict NSFW policy or strong self-filtering. However, owing to the web scale of Objaverse-XL we\nperformed NSFW filtering using the rendered images of the objects. Each 3D object is rendered\nin 12 random views and each rendered image is passed through an NSFW classifier trained on the\nNSFW dataset introduced in LAION-5B [55] by Gadre et al. [20] using the CLIP ViT-L/14 [50]\nfeatures. After careful analysis and manual inspection, we marked a rendered image as NSFW if\nit has an NSFW score above $$0.9$$ and a 3D object is marked as NSFW if at least $$3$$ rendered images\nare deemed to be NSFW. Overall, only $$815$$ objects out of the 10M are filtered out as NSFW objects.\nNote that the high threshold and multi-view consistency are needed due to the distribution shift\nbetween LAION-5B and Objaverse-XL along with NSFW classification of certain viewpoint renders\nof harmless 3D objects.\n\nFace detection. We analyze the presence of faces in Objaverse-XL using a detector trained by Gadre\net al. [20]. Like NSFW filtering, we count the objects where at least $$3$$ images contain a detected\nface. Out of 10M assets, we estimate $$266K$$ objects include faces. However, unlike most web-scale\ndatasets, the faces present in Objaverse-XL often come from the scans of dolls, historical sculptures,\nand anthropomorphic animations. Hence, there are less privacy concerns with most of these objects.\n\nPhotogrammetry hole detection. When scanning 3D objects, if the back or bottom of the object is\nnot scanned, rendering from various viewpoints may contain holes, leading to a \u201cbad\u201d render image.\nFor example, a non-trivial number of Polycam 3D objects lack the information from the \u201cback side\u201d.", "md": "NSFW annotations. Most data sources used for the creation of Objaverse-XL already have either a\nstrict NSFW policy or strong self-filtering. However, owing to the web scale of Objaverse-XL we\nperformed NSFW filtering using the rendered images of the objects. Each 3D object is rendered\nin 12 random views and each rendered image is passed through an NSFW classifier trained on the\nNSFW dataset introduced in LAION-5B [55] by Gadre et al. [20] using the CLIP ViT-L/14 [50]\nfeatures. After careful analysis and manual inspection, we marked a rendered image as NSFW if\nit has an NSFW score above $$0.9$$ and a 3D object is marked as NSFW if at least $$3$$ rendered images\nare deemed to be NSFW. Overall, only $$815$$ objects out of the 10M are filtered out as NSFW objects.\nNote that the high threshold and multi-view consistency are needed due to the distribution shift\nbetween LAION-5B and Objaverse-XL along with NSFW classification of certain viewpoint renders\nof harmless 3D objects.\n\nFace detection. We analyze the presence of faces in Objaverse-XL using a detector trained by Gadre\net al. [20]. Like NSFW filtering, we count the objects where at least $$3$$ images contain a detected\nface. Out of 10M assets, we estimate $$266K$$ objects include faces. However, unlike most web-scale\ndatasets, the faces present in Objaverse-XL often come from the scans of dolls, historical sculptures,\nand anthropomorphic animations. Hence, there are less privacy concerns with most of these objects.\n\nPhotogrammetry hole detection. When scanning 3D objects, if the back or bottom of the object is\nnot scanned, rendering from various viewpoints may contain holes, leading to a \u201cbad\u201d render image.\nFor example, a non-trivial number of Polycam 3D objects lack the information from the \u201cback side\u201d."}]}, {"page": 7, "text": "                  RR                     R                                                                         6\nCamera Pose        Input View       Zero123-XL           Zero123                Input View       Zero123-XL          Zero123        Camera Pose\n             Figure 5:   Novel view synthesis on in-the-wild images.              Comparison between Zero123-XL trained on\n             Objaverse-XL and Zero123 trained on Objaverse. Starting from the input view, the task is to generate\n             an image of the object under a specific camera pose transformation. The camera poses are shown\n             beside each example. Significant improvement can be found by training with more data, especially\n             for categories including people (1         strow), anime (2     nd row), cartoon (3     rd row), furniture (4     throw),\n             and sketches (5    th row). Additionally, viewpoint control is significantly improved (see                 2nd  row).\n             In most cases, images that are rendered from back-side viewpoints are noisy, low-fidelity, or contain\n             holes. To analyze \u201cbad rendering\u201d at scale, we manually annotated 1.2K Polycam renders as \u201cgood\u201d\n             (label  1) or \u201cbad\u201d (label     0). We trained a \u201cbad render\u201d classifier (2-layer MLP) on top of the CLIP\n             ViT-L/14 features of the rendered images; this classifier achieves a cross-validation accuracy of over\n             90%   with a \u201crender score\u201d threshold of          0.5. Overall, out of      71K Polycam objects with 12 renders\n             each, we found that      38.20%    renders are \u201cbad\u201d, with       58K objects having at least 2 bad renders.\n             4    Experiments\n             4.1    Novel View Synthesis with Zero123-XL\n             Generating 3D assets conditioned on in-the-wild 2D images has remained a challenging problem in\n             computer vision. A crucial lesson learned from large language models is that pretraining on simple\n             and easily scalable tasks, such as next word prediction, leads to consistently improved performance\n                                    Zero123-XL                 PSNR (\u2191)        SSIM (\u2191)        LPIPS (\u2193)       FID (\u2193)\n                                         Base                    18.225           0.877          0.088          0.070\n                            w/ Alignment Finetuning              19.876          0.888           0.075          0.056\n             Table 2:    Effect of high-quality data finetuning on Zero123-XL.                     When evaluated zero-shot on\n             Google Scanned Objects [17], a model finetuned on a high-quality alignment subset of Objaverse-XL\n             significantly outperforms the base model trained only on Objaverse-XL.\n                                                                          7", "md": "# Novel View Synthesis\n\n## Novel view synthesis on in-the-wild images\n\nComparison between Zero123-XL trained on Objaverse-XL and Zero123 trained on Objaverse. Starting from the input view, the task is to generate an image of the object under a specific camera pose transformation. The camera poses are shown beside each example. Significant improvement can be found by training with more data, especially for categories including people (1st row), anime (2nd row), cartoon (3rd row), furniture (4th row), and sketches (5th row). Additionally, viewpoint control is significantly improved (see 2nd row). In most cases, images that are rendered from back-side viewpoints are noisy, low-fidelity, or contain holes. To analyze \u201cbad rendering\u201d at scale, we manually annotated 1.2K Polycam renders as \u201cgood\u201d (label 1) or \u201cbad\u201d (label 0). We trained a \u201cbad render\u201d classifier (2-layer MLP) on top of the CLIP ViT-L/14 features of the rendered images; this classifier achieves a cross-validation accuracy of over 90% with a \u201crender score\u201d threshold of 0.5. Overall, out of 71K Polycam objects with 12 renders each, we found that 38.20% renders are \u201cbad\u201d, with 58K objects having at least 2 bad renders.\n\n### Experiments\n\n#### Novel View Synthesis with Zero123-XL\n\nGenerating 3D assets conditioned on in-the-wild 2D images has remained a challenging problem in computer vision. A crucial lesson learned from large language models is that pretraining on simple and easily scalable tasks, such as next word prediction, leads to consistently improved performance.\n\n| |Zero123-XL|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|FID (\u2193)|\n|---|---|---|---|---|---|\n| |Base|18.225|0.877|0.088|0.070|\n|w/ Alignment Finetuning| |19.876|0.888|0.075|0.056|\n\nWhen evaluated zero-shot on Google Scanned Objects [17], a model finetuned on a high-quality alignment subset of Objaverse-XL significantly outperforms the base model trained only on Objaverse-XL.", "images": [{"name": "img_p6_1", "height": 824, "width": 1456}], "items": [{"type": "heading", "lvl": 1, "value": "Novel View Synthesis", "md": "# Novel View Synthesis"}, {"type": "heading", "lvl": 2, "value": "Novel view synthesis on in-the-wild images", "md": "## Novel view synthesis on in-the-wild images"}, {"type": "text", "value": "Comparison between Zero123-XL trained on Objaverse-XL and Zero123 trained on Objaverse. Starting from the input view, the task is to generate an image of the object under a specific camera pose transformation. The camera poses are shown beside each example. Significant improvement can be found by training with more data, especially for categories including people (1st row), anime (2nd row), cartoon (3rd row), furniture (4th row), and sketches (5th row). Additionally, viewpoint control is significantly improved (see 2nd row). In most cases, images that are rendered from back-side viewpoints are noisy, low-fidelity, or contain holes. To analyze \u201cbad rendering\u201d at scale, we manually annotated 1.2K Polycam renders as \u201cgood\u201d (label 1) or \u201cbad\u201d (label 0). We trained a \u201cbad render\u201d classifier (2-layer MLP) on top of the CLIP ViT-L/14 features of the rendered images; this classifier achieves a cross-validation accuracy of over 90% with a \u201crender score\u201d threshold of 0.5. Overall, out of 71K Polycam objects with 12 renders each, we found that 38.20% renders are \u201cbad\u201d, with 58K objects having at least 2 bad renders.", "md": "Comparison between Zero123-XL trained on Objaverse-XL and Zero123 trained on Objaverse. Starting from the input view, the task is to generate an image of the object under a specific camera pose transformation. The camera poses are shown beside each example. Significant improvement can be found by training with more data, especially for categories including people (1st row), anime (2nd row), cartoon (3rd row), furniture (4th row), and sketches (5th row). Additionally, viewpoint control is significantly improved (see 2nd row). In most cases, images that are rendered from back-side viewpoints are noisy, low-fidelity, or contain holes. To analyze \u201cbad rendering\u201d at scale, we manually annotated 1.2K Polycam renders as \u201cgood\u201d (label 1) or \u201cbad\u201d (label 0). We trained a \u201cbad render\u201d classifier (2-layer MLP) on top of the CLIP ViT-L/14 features of the rendered images; this classifier achieves a cross-validation accuracy of over 90% with a \u201crender score\u201d threshold of 0.5. Overall, out of 71K Polycam objects with 12 renders each, we found that 38.20% renders are \u201cbad\u201d, with 58K objects having at least 2 bad renders."}, {"type": "heading", "lvl": 3, "value": "Experiments", "md": "### Experiments"}, {"type": "heading", "lvl": 4, "value": "Novel View Synthesis with Zero123-XL", "md": "#### Novel View Synthesis with Zero123-XL"}, {"type": "text", "value": "Generating 3D assets conditioned on in-the-wild 2D images has remained a challenging problem in computer vision. A crucial lesson learned from large language models is that pretraining on simple and easily scalable tasks, such as next word prediction, leads to consistently improved performance.", "md": "Generating 3D assets conditioned on in-the-wild 2D images has remained a challenging problem in computer vision. A crucial lesson learned from large language models is that pretraining on simple and easily scalable tasks, such as next word prediction, leads to consistently improved performance."}, {"type": "table", "rows": [["", "Zero123-XL", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)", "FID (\u2193)"], ["", "Base", "18.225", "0.877", "0.088", "0.070"], ["w/ Alignment Finetuning", "", "19.876", "0.888", "0.075", "0.056"]], "md": "| |Zero123-XL|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|FID (\u2193)|\n|---|---|---|---|---|---|\n| |Base|18.225|0.877|0.088|0.070|\n|w/ Alignment Finetuning| |19.876|0.888|0.075|0.056|", "isPerfectTable": true, "csv": "\"\",\"Zero123-XL\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\",\"FID (\u2193)\"\n\"\",\"Base\",\"18.225\",\"0.877\",\"0.088\",\"0.070\"\n\"w/ Alignment Finetuning\",\"\",\"19.876\",\"0.888\",\"0.075\",\"0.056\""}, {"type": "text", "value": "When evaluated zero-shot on Google Scanned Objects [17], a model finetuned on a high-quality alignment subset of Objaverse-XL significantly outperforms the base model trained only on Objaverse-XL.", "md": "When evaluated zero-shot on Google Scanned Objects [17], a model finetuned on a high-quality alignment subset of Objaverse-XL significantly outperforms the base model trained only on Objaverse-XL."}]}, {"page": 8, "text": "     6/6/23, 9:46 PM                       lineplot.html             6/7/23, 4:03AM                        lineplot.html\n       26                   PixelNeRF                                   4                     Zero123\n       24                                                               3\n       22                                                               2\n      PSNR                                                            LPIPS\n       20                                                               1\n       18                                                               0\n         1k         5k  10k       50k 100k       500k 1M   2M           800          8k          80k         800k         10M\n                           Dataset Size                                                    Dataset Size\nFigure 6:    Novel view synthesis at scale. Left: PixelNeRF [64] trained on varying scales of data\nand evaluated on a held-out subset of Objavserse-XL.                  Right: Zero123 [36] trained on varying scales\nof data and evaluated on a zero-shot dataset. Note that the 800K datapoint is Zero123 and the 10M\ndatapoint is Zero123-XL. The synthesis quality consistently improves with scale. LPIPS is scaled up\n10 times for visualization.\nand the emergence of zero-shot abilities. An analogous approach in 3D vision is to predict a novel\nview of an object from an input view. Zero123 [36] recently proposed a view-conditioned diffusion\nmodel to perform this task, where the weights of the diffusion model are initialized from Stable\nDiffusion to leverage its powerful zero-shot image generation abilities. Zero123 used objects in\nObjaverse 1.0 to render input and novel view pairs as the training dataset. We use this framework\nto create   Zero123-XL, which is the same approach except trained on the much larger Objaverse-XL\ninstead. As shown in [36], the pretrained view-conditioned diffusion model can also be plugged into\na score distillation framework such as DreamFusion [48] or SJC [58] to obtain a 3D assets.\nZero-shot Generalization.             We found that training Zero123 on Objaverse-XL achieves significantly\nbetter zero-shot generalization performance than using Objaverse 1.0. Figure 5 shows examples\nfrom categories of data commonly known to be challenging for baseline systems, including people,\ncartoons, paintings, and sketches. For example, in both of the examples shown in 2nd and 3rd\nrows of the first column, Zero123 interprets the input image as a 2D plane and performs a simple\ntransformation similar to a homography transformation. In comparison, Zero123-XL is able to\ngenerate novel views that are more consistent with the input view. Additionally, Zero123-XL is\nable to generate novel views from sketches of objects while keeping the original style as well as\nobject geometric details. These examples show the effectiveness of dataset scaling for zero-shot\ngeneralization in 3D.\nImprovement with Scale.              We further quantitatively evaluate the novel view synthesis performance\non Google Scanned Objects dataset [17]. As shown in Figure 6, the rvisual similarity score [65]\nbetween the predicted novel view and the ground truth view continues to improve as the dataset size\nincreases.\nAlignment Finetuning.             InstructGPT [44] shows that large-scale pretraining does not directly lead\nto a model aligned with human preferences. More recently, LIMA [66] shows that finetuning a\npretrained model on a curated subset with high-quality data can achieve impressive alignment results.\nWe adopted a similar approach here by selecting a high-quality subset of Objaverse-XL that contains\n1.3 million objects. Selection is done by defining proxy estimation of human preference based on\nheuristics including vertex count, face count, popularity on the source website, and source of data,\namong other metrics. After pretraining the base model on the entire Objaverse-XL, we finetune\nZero123-XL on the alignment subset with a reduced learning rate and performed an ablation study\nto evaluate the effect of alignment finetuning. Table 2 shows that alignment finetuning leads to\nsignificant improvement in zero-shot generalization performance. Please refer to Appendix A for\nmore implementation details regarding our model and experiments.\n4.2    Novel View Synthesis with PixelNeRF\nSynthesizing objects and scenes from novel views is a long-standing challenge. Notably, neural\nradiance fields [39] have shown impressive capabilities in rendering specific scenes from novel views.\n                                                                8", "md": "# Novel View Synthesis\n\n## Figure 6: Novel view synthesis at scale\n\nLeft: PixelNeRF [64] trained on varying scales of data and evaluated on a held-out subset of Objavserse-XL.\n\nRight: Zero123 [36] trained on varying scales of data and evaluated on a zero-shot dataset. Note that the 800K datapoint is Zero123 and the 10M datapoint is Zero123-XL. The synthesis quality consistently improves with scale. LPIPS is scaled up 10 times for visualization.\n\n## Zero-shot Generalization\n\nWe found that training Zero123 on Objaverse-XL achieves significantly better zero-shot generalization performance than using Objaverse 1.0. Figure 5 shows examples from categories of data commonly known to be challenging for baseline systems, including people, cartoons, paintings, and sketches.\n\n## Improvement with Scale\n\nWe further quantitatively evaluate the novel view synthesis performance on Google Scanned Objects dataset [17]. As shown in Figure 6, the visual similarity score [65] between the predicted novel view and the ground truth view continues to improve as the dataset size increases.\n\n## Alignment Finetuning\n\nInstructGPT [44] shows that large-scale pretraining does not directly lead to a model aligned with human preferences. More recently, LIMA [66] shows that finetuning a pretrained model on a curated subset with high-quality data can achieve impressive alignment results.\n\n## Novel View Synthesis with PixelNeRF\n\nSynthesizing objects and scenes from novel views is a long-standing challenge. Notably, neural radiance fields [39] have shown impressive capabilities in rendering specific scenes from novel views.\n\n### Table 2: Effect of alignment finetuning on zero-shot generalization performance\n\n|Model|Zero-shot Generalization Performance|\n|---|---|\n|Zero123|Baseline|\n|Zero123-XL|Improved|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Novel View Synthesis", "md": "# Novel View Synthesis"}, {"type": "heading", "lvl": 2, "value": "Figure 6: Novel view synthesis at scale", "md": "## Figure 6: Novel view synthesis at scale"}, {"type": "text", "value": "Left: PixelNeRF [64] trained on varying scales of data and evaluated on a held-out subset of Objavserse-XL.\n\nRight: Zero123 [36] trained on varying scales of data and evaluated on a zero-shot dataset. Note that the 800K datapoint is Zero123 and the 10M datapoint is Zero123-XL. The synthesis quality consistently improves with scale. LPIPS is scaled up 10 times for visualization.", "md": "Left: PixelNeRF [64] trained on varying scales of data and evaluated on a held-out subset of Objavserse-XL.\n\nRight: Zero123 [36] trained on varying scales of data and evaluated on a zero-shot dataset. Note that the 800K datapoint is Zero123 and the 10M datapoint is Zero123-XL. The synthesis quality consistently improves with scale. LPIPS is scaled up 10 times for visualization."}, {"type": "heading", "lvl": 2, "value": "Zero-shot Generalization", "md": "## Zero-shot Generalization"}, {"type": "text", "value": "We found that training Zero123 on Objaverse-XL achieves significantly better zero-shot generalization performance than using Objaverse 1.0. Figure 5 shows examples from categories of data commonly known to be challenging for baseline systems, including people, cartoons, paintings, and sketches.", "md": "We found that training Zero123 on Objaverse-XL achieves significantly better zero-shot generalization performance than using Objaverse 1.0. Figure 5 shows examples from categories of data commonly known to be challenging for baseline systems, including people, cartoons, paintings, and sketches."}, {"type": "heading", "lvl": 2, "value": "Improvement with Scale", "md": "## Improvement with Scale"}, {"type": "text", "value": "We further quantitatively evaluate the novel view synthesis performance on Google Scanned Objects dataset [17]. As shown in Figure 6, the visual similarity score [65] between the predicted novel view and the ground truth view continues to improve as the dataset size increases.", "md": "We further quantitatively evaluate the novel view synthesis performance on Google Scanned Objects dataset [17]. As shown in Figure 6, the visual similarity score [65] between the predicted novel view and the ground truth view continues to improve as the dataset size increases."}, {"type": "heading", "lvl": 2, "value": "Alignment Finetuning", "md": "## Alignment Finetuning"}, {"type": "text", "value": "InstructGPT [44] shows that large-scale pretraining does not directly lead to a model aligned with human preferences. More recently, LIMA [66] shows that finetuning a pretrained model on a curated subset with high-quality data can achieve impressive alignment results.", "md": "InstructGPT [44] shows that large-scale pretraining does not directly lead to a model aligned with human preferences. More recently, LIMA [66] shows that finetuning a pretrained model on a curated subset with high-quality data can achieve impressive alignment results."}, {"type": "heading", "lvl": 2, "value": "Novel View Synthesis with PixelNeRF", "md": "## Novel View Synthesis with PixelNeRF"}, {"type": "text", "value": "Synthesizing objects and scenes from novel views is a long-standing challenge. Notably, neural radiance fields [39] have shown impressive capabilities in rendering specific scenes from novel views.", "md": "Synthesizing objects and scenes from novel views is a long-standing challenge. Notably, neural radiance fields [39] have shown impressive capabilities in rendering specific scenes from novel views."}, {"type": "heading", "lvl": 3, "value": "Table 2: Effect of alignment finetuning on zero-shot generalization performance", "md": "### Table 2: Effect of alignment finetuning on zero-shot generalization performance"}, {"type": "table", "rows": [["Model", "Zero-shot Generalization Performance"], ["Zero123", "Baseline"], ["Zero123-XL", "Improved"]], "md": "|Model|Zero-shot Generalization Performance|\n|---|---|\n|Zero123|Baseline|\n|Zero123-XL|Improved|", "isPerfectTable": true, "csv": "\"Model\",\"Zero-shot Generalization Performance\"\n\"Zero123\",\"Baseline\"\n\"Zero123-XL\",\"Improved\""}]}, {"page": 9, "text": "However, these methods require dozens of views of an individual scene, and can only synthesize\nviews from the particular scene they were trained for. More recent methods [16,                     27,  60,64] have been\nproposed for constructing NeRF models that generalize across scenes with few input images. Due to\nthe challenging nature of obtaining the necessary camera parameters for training, such methods have\ntraditionally been trained on small scale data sets. With the Objaverse-XL data, we train a PixelNeRF\nmodel on over two million objects, magnitudes of more data than has previously been used. We find\nthat PixelNeRF generalizes to novel scenes and objects significantly better and performance improves\nconsistently with scale (Figure 6 and Table 3).\nImprovement with Scale.               We train PixelNeRF models conditioned on a single input image at\nvarying scales of data (Figure 6) and evaluate on a held out set of Objaverse-XL objects. We find that\nnovel view synthesis quality consistently improves with more objects even at the scale of 2 million\nobjects and 24 million rendered images.\nGeneralization         to   Downstream          Datasets.           PixelNeRF               DTU [2]          ShapeNet [9]\nSimilar to pretraining in 2D vision and language,\nwe observe that pretraining on Objaverse-XL                              Base                 15.32               22.71\nwith PixelNeRF improves performance when                         w/ Objaverse-XL          17.53   \u00b1.37        24.22   \u00b1.55\nfine-tuning to other datasets such as DTU [2]\nand ShapeNet [9] (Table 3). We pretrain and                    Table 3:      Comparison (PSNR (\u2191)) of PixelNeRF\nfine-tune the model conditioned on a single in-                 trained from scratch vs. fine-tuned from Objaverse-\nput view and report the peak signal-to-noise ra-               XL.   Performance significantly improves from pretrain-\ntio (PSNR).                                                     ing on the large-scale corpus.\n5     Limitations and Conclusion\nLimitations.        While Objaverse-XL is more than an order of magnitude larger than its predecessor,\nObjaverse 1.0, it is still orders of magnitude smaller than modern billion-scale image-text datasets.\nFuture work may consider how to continue scaling 3D datasets and make 3D content easier to capture\nand create. Additionally, it may not be the case that all samples in Objaverse-XL are necessary to\ntrain high performance models. Future work may also consider how to choose datapoints to train on.\nFinally, while we focus on generative tasks, future work may consider how Objaverse-XL can benefit\ndiscriminative tasks such as 3D segmentation and detection.\nConclusion.        We introduce Objaverse-XL, which is comprised of 10.2M 3D assets. In addition to\ndocumenting Objaverse-XL\u2019s unprecedented scale and sample diversity, we demonstrate the potential\nof Objaverse-XL for downstream applications. On the task of zero-shot novel view synthesis, we\nestablish empirically promising trends of scaling dataset size, while keeping the model architecture\nconstant. We hope Objaverse-XL will provide a foundation for future work in 3D.\n                                                               9", "md": "However, these methods require dozens of views of an individual scene, and can only synthesize views from the particular scene they were trained for. More recent methods [16, 27, 60, 64] have been proposed for constructing NeRF models that generalize across scenes with few input images. Due to the challenging nature of obtaining the necessary camera parameters for training, such methods have traditionally been trained on small scale data sets. With the Objaverse-XL data, we train a PixelNeRF model on over two million objects, magnitudes of more data than has previously been used. We find that PixelNeRF generalizes to novel scenes and objects significantly better and performance improves consistently with scale (Figure 6 and Table 3).\n\nImprovement with Scale. We train PixelNeRF models conditioned on a single input image at varying scales of data (Figure 6) and evaluate on a held out set of Objaverse-XL objects. We find that novel view synthesis quality consistently improves with more objects even at the scale of 2 million objects and 24 million rendered images.\n\nGeneralization to Downstream Datasets. Similar to pretraining in 2D vision and language, we observe that pretraining on Objaverse-XL with PixelNeRF improves performance when fine-tuning to other datasets such as DTU [2] and ShapeNet [9] (Table 3). We pretrain and fine-tune the model conditioned on a single input view and report the peak signal-to-noise ratio (PSNR).\n\n| |DTU [2]|ShapeNet [9]|\n|---|---|---|\n|Base|15.32|22.71|\n|w/ Objaverse-XL|17.53 \u00b1.37|24.22 \u00b1.55|\n\nLimitations and Conclusion\n\nLimitations. While Objaverse-XL is more than an order of magnitude larger than its predecessor, Objaverse 1.0, it is still orders of magnitude smaller than modern billion-scale image-text datasets. Future work may consider how to continue scaling 3D datasets and make 3D content easier to capture and create. Additionally, it may not be the case that all samples in Objaverse-XL are necessary to train high performance models. Future work may also consider how to choose datapoints to train on. Finally, while we focus on generative tasks, future work may consider how Objaverse-XL can benefit discriminative tasks such as 3D segmentation and detection.\n\nConclusion. We introduce Objaverse-XL, which is comprised of 10.2M 3D assets. In addition to documenting Objaverse-XL\u2019s unprecedented scale and sample diversity, we demonstrate the potential of Objaverse-XL for downstream applications. On the task of zero-shot novel view synthesis, we establish empirically promising trends of scaling dataset size, while keeping the model architecture constant. We hope Objaverse-XL will provide a foundation for future work in 3D.", "images": [], "items": [{"type": "text", "value": "However, these methods require dozens of views of an individual scene, and can only synthesize views from the particular scene they were trained for. More recent methods [16, 27, 60, 64] have been proposed for constructing NeRF models that generalize across scenes with few input images. Due to the challenging nature of obtaining the necessary camera parameters for training, such methods have traditionally been trained on small scale data sets. With the Objaverse-XL data, we train a PixelNeRF model on over two million objects, magnitudes of more data than has previously been used. We find that PixelNeRF generalizes to novel scenes and objects significantly better and performance improves consistently with scale (Figure 6 and Table 3).\n\nImprovement with Scale. We train PixelNeRF models conditioned on a single input image at varying scales of data (Figure 6) and evaluate on a held out set of Objaverse-XL objects. We find that novel view synthesis quality consistently improves with more objects even at the scale of 2 million objects and 24 million rendered images.\n\nGeneralization to Downstream Datasets. Similar to pretraining in 2D vision and language, we observe that pretraining on Objaverse-XL with PixelNeRF improves performance when fine-tuning to other datasets such as DTU [2] and ShapeNet [9] (Table 3). We pretrain and fine-tune the model conditioned on a single input view and report the peak signal-to-noise ratio (PSNR).", "md": "However, these methods require dozens of views of an individual scene, and can only synthesize views from the particular scene they were trained for. More recent methods [16, 27, 60, 64] have been proposed for constructing NeRF models that generalize across scenes with few input images. Due to the challenging nature of obtaining the necessary camera parameters for training, such methods have traditionally been trained on small scale data sets. With the Objaverse-XL data, we train a PixelNeRF model on over two million objects, magnitudes of more data than has previously been used. We find that PixelNeRF generalizes to novel scenes and objects significantly better and performance improves consistently with scale (Figure 6 and Table 3).\n\nImprovement with Scale. We train PixelNeRF models conditioned on a single input image at varying scales of data (Figure 6) and evaluate on a held out set of Objaverse-XL objects. We find that novel view synthesis quality consistently improves with more objects even at the scale of 2 million objects and 24 million rendered images.\n\nGeneralization to Downstream Datasets. Similar to pretraining in 2D vision and language, we observe that pretraining on Objaverse-XL with PixelNeRF improves performance when fine-tuning to other datasets such as DTU [2] and ShapeNet [9] (Table 3). We pretrain and fine-tune the model conditioned on a single input view and report the peak signal-to-noise ratio (PSNR)."}, {"type": "table", "rows": [["", "DTU [2]", "ShapeNet [9]"], ["Base", "15.32", "22.71"], ["w/ Objaverse-XL", "17.53 \u00b1.37", "24.22 \u00b1.55"]], "md": "| |DTU [2]|ShapeNet [9]|\n|---|---|---|\n|Base|15.32|22.71|\n|w/ Objaverse-XL|17.53 \u00b1.37|24.22 \u00b1.55|", "isPerfectTable": true, "csv": "\"\",\"DTU [2]\",\"ShapeNet [9]\"\n\"Base\",\"15.32\",\"22.71\"\n\"w/ Objaverse-XL\",\"17.53 \u00b1.37\",\"24.22 \u00b1.55\""}, {"type": "text", "value": "Limitations and Conclusion\n\nLimitations. While Objaverse-XL is more than an order of magnitude larger than its predecessor, Objaverse 1.0, it is still orders of magnitude smaller than modern billion-scale image-text datasets. Future work may consider how to continue scaling 3D datasets and make 3D content easier to capture and create. Additionally, it may not be the case that all samples in Objaverse-XL are necessary to train high performance models. Future work may also consider how to choose datapoints to train on. Finally, while we focus on generative tasks, future work may consider how Objaverse-XL can benefit discriminative tasks such as 3D segmentation and detection.\n\nConclusion. We introduce Objaverse-XL, which is comprised of 10.2M 3D assets. In addition to documenting Objaverse-XL\u2019s unprecedented scale and sample diversity, we demonstrate the potential of Objaverse-XL for downstream applications. On the task of zero-shot novel view synthesis, we establish empirically promising trends of scaling dataset size, while keeping the model architecture constant. We hope Objaverse-XL will provide a foundation for future work in 3D.", "md": "Limitations and Conclusion\n\nLimitations. While Objaverse-XL is more than an order of magnitude larger than its predecessor, Objaverse 1.0, it is still orders of magnitude smaller than modern billion-scale image-text datasets. Future work may consider how to continue scaling 3D datasets and make 3D content easier to capture and create. Additionally, it may not be the case that all samples in Objaverse-XL are necessary to train high performance models. Future work may also consider how to choose datapoints to train on. Finally, while we focus on generative tasks, future work may consider how Objaverse-XL can benefit discriminative tasks such as 3D segmentation and detection.\n\nConclusion. We introduce Objaverse-XL, which is comprised of 10.2M 3D assets. In addition to documenting Objaverse-XL\u2019s unprecedented scale and sample diversity, we demonstrate the potential of Objaverse-XL for downstream applications. On the task of zero-shot novel view synthesis, we establish empirically promising trends of scaling dataset size, while keeping the model architecture constant. We hope Objaverse-XL will provide a foundation for future work in 3D."}]}, {"page": 10, "text": " Acknowledgements\nWe would like to thank Stability AI for compute used to train the experiments and LAION for\n their support. We would also like to thank Luca Weihs, Mitchell Wortsman, Romain Beaumont,\n and Vaishaal Shankar, Rose Hendrix, Adam Letts, Sami Kama, Andreas Blattmann, Kunal Pratap\n Singh, and Kuo-Hao Zeng for their helpful guidance and conversations with the project. Finally,\nwe would like to thank the teams behind several open-source packages used throughout this project,\n including Blender [5], PyTorch [47], PyTorch Lightning [18], D3 [6], Matplotlib [26], NumPy [23],\n Pandas [45], Wandb [4], and Seaborn [61]. Following the NeurIPS guidelines, we would also like to\n acknowledge the use of LLMs for helping revise some text and general coding assistance. Finally, we\nwould also like to thank and acknowledge the content creators who contributed to the dataset.\n References\n  [1] URL     https://commoncrawl.org/the-data/. 3\n  [2]  H. Aan\u00e6s, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl. Large-scale data for multiple-view\n       stereopsis.    International Journal of Computer Vision, pages 1\u201316, 2016. 9\n  [3]  J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\n       M. Reynolds, et al. Flamingo: a visual language model for few-shot learning.                            Advances in\n       Neural Information Processing Systems, 35:23716\u201323736, 2022. 2\n  [4]  L. Biewald. Experiment tracking with weights and biases, 2020. URL                         https://www.wandb.\n       com/. Software available from wandb.com. 10\n  [5]  Blender Online Community. Blender - a 3d modelling and rendering package.                           https://www.\n       blender.org, 2023. 10\n  [6]  M. Bostock, V. Ogievetsky, and J. Heer. D3: Data-driven documents.                         IEEE Transactions on\n       Visualization and Computer Graphics, 2011. 10\n  [7]  T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n       G. Sastry, A. Askell, et al.           Language models are few-shot learners.                 Advances in neural\n       information processing systems, 33:1877\u20131901, 2020. 2\n  [8]  N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object\n       detection with transformers. In           Computer Vision\u2013ECCV 2020: 16th European Conference,\n       Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 213\u2013229. Springer, 2020. 2\n  [9]  A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva,\n       S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository.                            arXiv preprint\n       arXiv:1512.03012, 2015. 2, 3, 9\n [10]  W. Chen, H. Ling, J. Gao, E. Smith, J. Lehtinen, A. Jacobson, and S. Fidler. Learning to predict\n       3d objects with an interpolation-based differentiable renderer.                Advances in neural information\n       processing systems, 32, 2019. 3\n[11]   B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar.                          Masked-attention mask\n       transformer for universal image segmentation. 2022. 2\n [12]  C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-r2n2: A unified approach for single\n       and multi-view 3d object reconstruction. In                Computer Vision\u2013ECCV 2016: 14th European\n       Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14,\n       pages 628\u2013644. Springer, 2016. 3\n [13]  J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang, T. F. Y. Vicente,\n       T. Dideriksen, H. Arora, et al.           Abo: Dataset and benchmarks for real-world 3d object un-\n       derstanding. In      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n       Recognition, pages 21126\u201321136, 2022. 3\n [14]  M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani,\n       A. Kembhavi, and A. Farhadi. Objaverse: A universe of annotated 3d objects.                          arXiv preprint\n       arXiv:2212.08051, 2022. 2, 3\n [15]  J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\n       image database. In        2009 IEEE conference on computer vision and pattern recognition, pages\n       248\u2013255. Ieee, 2009. 2\n                                                              10", "md": "# Acknowledgements and References\n\n## Acknowledgements\n\nWe would like to thank Stability AI for compute used to train the experiments and LAION for their support. We would also like to thank Luca Weihs, Mitchell Wortsman, Romain Beaumont, and Vaishaal Shankar, Rose Hendrix, Adam Letts, Sami Kama, Andreas Blattmann, Kunal Pratap Singh, and Kuo-Hao Zeng for their helpful guidance and conversations with the project. Finally, we would like to thank the teams behind several open-source packages used throughout this project, including Blender, PyTorch, PyTorch Lightning, D3, Matplotlib, NumPy, Pandas, Wandb, and Seaborn. Following the NeurIPS guidelines, we would also like to acknowledge the use of LLMs for helping revise some text and general coding assistance. Finally, we would also like to thank and acknowledge the content creators who contributed to the dataset.\n\n## References\n\n1. [1] URL: https://commoncrawl.org/the-data/\n2. [2] H. Aan\u00e6s, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, pages 1\u201316, 2016.\n3. [3] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\n4. [4] L. Biewald. Experiment tracking with weights and biases, 2020. URL: https://www.wandb.com/. Software available from wandb.com.\n5. [5] Blender Online Community. Blender - a 3d modelling and rendering package. https://www.blender.org, 2023.\n6. [6] M. Bostock, V. Ogievetsky, and J. Heer. D3: Data-driven documents. IEEE Transactions on Visualization and Computer Graphics, 2011.\n7. [7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n8. [8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 213\u2013229. Springer, 2020.\n9. [9] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n10. [10] W. Chen, H. Ling, J. Gao, E. Smith, J. Lehtinen, A. Jacobson, and S. Fidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. Advances in neural information processing systems, 32, 2019.\n11. [11] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar. Masked-attention mask transformer for universal image segmentation. 2022.\n12. [12] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 628\u2013644. Springer, 2016.\n13. [13] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang, T. F. Y. Vicente, T. Dideriksen, H. Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21126\u201321136, 2022.\n14. [14] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi. Objaverse: A universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022.\n15. [15] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgements and References", "md": "# Acknowledgements and References"}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "We would like to thank Stability AI for compute used to train the experiments and LAION for their support. We would also like to thank Luca Weihs, Mitchell Wortsman, Romain Beaumont, and Vaishaal Shankar, Rose Hendrix, Adam Letts, Sami Kama, Andreas Blattmann, Kunal Pratap Singh, and Kuo-Hao Zeng for their helpful guidance and conversations with the project. Finally, we would like to thank the teams behind several open-source packages used throughout this project, including Blender, PyTorch, PyTorch Lightning, D3, Matplotlib, NumPy, Pandas, Wandb, and Seaborn. Following the NeurIPS guidelines, we would also like to acknowledge the use of LLMs for helping revise some text and general coding assistance. Finally, we would also like to thank and acknowledge the content creators who contributed to the dataset.", "md": "We would like to thank Stability AI for compute used to train the experiments and LAION for their support. We would also like to thank Luca Weihs, Mitchell Wortsman, Romain Beaumont, and Vaishaal Shankar, Rose Hendrix, Adam Letts, Sami Kama, Andreas Blattmann, Kunal Pratap Singh, and Kuo-Hao Zeng for their helpful guidance and conversations with the project. Finally, we would like to thank the teams behind several open-source packages used throughout this project, including Blender, PyTorch, PyTorch Lightning, D3, Matplotlib, NumPy, Pandas, Wandb, and Seaborn. Following the NeurIPS guidelines, we would also like to acknowledge the use of LLMs for helping revise some text and general coding assistance. Finally, we would also like to thank and acknowledge the content creators who contributed to the dataset."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. [1] URL: https://commoncrawl.org/the-data/\n2. [2] H. Aan\u00e6s, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, pages 1\u201316, 2016.\n3. [3] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\n4. [4] L. Biewald. Experiment tracking with weights and biases, 2020. URL: https://www.wandb.com/. Software available from wandb.com.\n5. [5] Blender Online Community. Blender - a 3d modelling and rendering package. https://www.blender.org, 2023.\n6. [6] M. Bostock, V. Ogievetsky, and J. Heer. D3: Data-driven documents. IEEE Transactions on Visualization and Computer Graphics, 2011.\n7. [7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n8. [8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 213\u2013229. Springer, 2020.\n9. [9] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n10. [10] W. Chen, H. Ling, J. Gao, E. Smith, J. Lehtinen, A. Jacobson, and S. Fidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. Advances in neural information processing systems, 32, 2019.\n11. [11] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar. Masked-attention mask transformer for universal image segmentation. 2022.\n12. [12] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 628\u2013644. Springer, 2016.\n13. [13] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang, T. F. Y. Vicente, T. Dideriksen, H. Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21126\u201321136, 2022.\n14. [14] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi. Objaverse: A universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022.\n15. [15] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.", "md": "1. [1] URL: https://commoncrawl.org/the-data/\n2. [2] H. Aan\u00e6s, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, pages 1\u201316, 2016.\n3. [3] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\n4. [4] L. Biewald. Experiment tracking with weights and biases, 2020. URL: https://www.wandb.com/. Software available from wandb.com.\n5. [5] Blender Online Community. Blender - a 3d modelling and rendering package. https://www.blender.org, 2023.\n6. [6] M. Bostock, V. Ogievetsky, and J. Heer. D3: Data-driven documents. IEEE Transactions on Visualization and Computer Graphics, 2011.\n7. [7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n8. [8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 213\u2013229. Springer, 2020.\n9. [9] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n10. [10] W. Chen, H. Ling, J. Gao, E. Smith, J. Lehtinen, A. Jacobson, and S. Fidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. Advances in neural information processing systems, 32, 2019.\n11. [11] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar. Masked-attention mask transformer for universal image segmentation. 2022.\n12. [12] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 628\u2013644. Springer, 2016.\n13. [13] J. Collins, S. Goel, K. Deng, A. Luthra, L. Xu, E. Gundogdu, X. Zhang, T. F. Y. Vicente, T. Dideriksen, H. Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21126\u201321136, 2022.\n14. [14] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi. Objaverse: A universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022.\n15. [15] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009."}]}, {"page": 11, "text": " [16]  K. Deng, A. Liu, J.-Y. Zhu, and D. Ramanan. Depth-supervised nerf: Fewer views and faster\n       training for free. In    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n       Recognition, pages 12882\u201312891, 2022. 9\n [17]  L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and\n       V. Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items.\n       In 2022 International Conference on Robotics and Automation (ICRA), pages 2553\u20132560. IEEE,\n       2022. 3, 7, 8\n [18]  W. Falcon and The PyTorch Lightning team. PyTorch Lightning, Mar. 2019. URL                                  https:\n       //github.com/Lightning-AI/lightning. 10\n[19]   H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao. 3d-future: 3d furniture\n       shape with texture.      International Journal of Computer Vision, 129:3313\u20133337, 2021. 3\n [20] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman,\n       D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets.\n       arXiv preprint arXiv:2304.14108, 2023. 6\n [21]  T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford.\n       Datasheets for datasets.       Communications of the ACM, 64(12):86\u201392, 2021. 27\n [22]  G. Gkioxari, J. Malik, and J. Johnson. Mesh r-cnn. In                Proceedings of the IEEE/CVF Interna-\n       tional Conference on Computer Vision, pages 9785\u20139795, 2019. 3\n [23]  C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau,\n       E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk,\n       M. Brett, A. Haldane, J. F. del R\u00edo, M. Wiebe, P. Peterson, P. G\u00e9rard-Marchant, K. Sheppard,\n       T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with\n       NumPy.      Nature, 585(7825):357\u2013362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL\n       https://doi.org/10.1038/s41586-020-2649-2. 10\n[24]   K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick.                 Mask r-cnn.       In  Proceedings of the IEEE\n       international conference on computer vision, pages 2961\u20132969, 2017. 2\n[25]   J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas,\n       L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models.\n       arXiv preprint arXiv:2203.15556, 2022. 2, 3\n [26]  J. D. Hunter. Matplotlib: A 2d graphics environment.                Computing in Science & Engineering, 9\n       (3):90\u201395, 2007. doi: 10.1109/MCSE.2007.55. 10\n [27]  A. Jain, M. Tancik, and P. Abbeel. Putting nerf on a diet: Semantically consistent few-shot\n       view synthesis. In     Proceedings of the IEEE/CVF International Conference on Computer Vision,\n       pages 5885\u20135894, 2021. 9\n [28]  H. Jun and A. Nichol. Shap-e: Generating conditional 3d implicit functions.                         arXiv preprint\n       arXiv:2305.02463, 2023. 3\n [29]  J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-\n       ford, J. Wu, and D. Amodei.               Scaling laws for neural language models.                 arXiv preprint\n       arXiv:2001.08361, 2020. 3\n [30]  H. Kato, Y. Ushiku, and T. Harada. Neural 3d mesh renderer. In                        Proceedings of the IEEE\n       conference on computer vision and pattern recognition, pages 3907\u20133916, 2018. 3\n [31]  A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C.\n       Berg, W.-Y. Lo, et al. Segment anything.            arXiv preprint arXiv:2304.02643, 2023. 2\n [32]  J. J. Lim, H. Pirsiavash, and A. Torralba.               Parsing ikea objects: Fine pose estimation.               In\n       Proceedings of the IEEE international conference on computer vision, pages 2992\u20132999, 2013.\n       3\n [33]  C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu,\n       and T.-Y. Lin. Magic3d: High-resolution text-to-3d content creation. In                       Proceedings of the\n       IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309, 2023. 3\n[34]   R. Liu and C. Vondrick. Humans as light bulbs: 3d human reconstruction from thermal reflection.\n       In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n       pages 12531\u201312542, 2023. 3\n                                                             11", "md": "- [16] K. Deng, A. Liu, J.-Y. Zhu, and D. Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12882\u201312891, 2022. 9\n- [17] L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and V. Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 2553\u20132560. IEEE, 2022. 3, 7, 8\n- [18] W. Falcon and The PyTorch Lightning team. PyTorch Lightning, Mar. 2019. URL https://github.com/Lightning-AI/lightning. 10\n- [19] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, 129:3313\u20133337, 2021. 3\n- [20] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 6\n- [21] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021. 27\n- [22] G. Gkioxari, J. Malik, and J. Johnson. Mesh r-cnn. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9785\u20139795, 2019. 3\n- [23] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del R\u00edo, M. Wiebe, P. Peterson, P. G\u00e9rard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2. 10\n- [24] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017. 2\n- [25] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 2, 3\n- [26] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9(3):90\u201395, 2007. doi: 10.1109/MCSE.2007.55. 10\n- [27] A. Jain, M. Tancik, and P. Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5885\u20135894, 2021. 9\n- [28] H. Jun and A. Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. 3\n- [29] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 3\n- [30] H. Kato, Y. Ushiku, and T. Harada. Neural 3d mesh renderer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3907\u20133916, 2018. 3\n- [31] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 2\n- [32] J. J. Lim, H. Pirsiavash, and A. Torralba. Parsing ikea objects: Fine pose estimation. In Proceedings of the IEEE international conference on computer vision, pages 2992\u20132999, 2013. 3\n- [33] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu, and T.-Y. Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309, 2023. 3\n- [34] R. Liu and C. Vondrick. Humans as light bulbs: 3d human reconstruction from thermal reflection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12531\u201312542, 2023. 3", "images": [], "items": [{"type": "text", "value": "- [16] K. Deng, A. Liu, J.-Y. Zhu, and D. Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12882\u201312891, 2022. 9\n- [17] L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and V. Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 2553\u20132560. IEEE, 2022. 3, 7, 8\n- [18] W. Falcon and The PyTorch Lightning team. PyTorch Lightning, Mar. 2019. URL https://github.com/Lightning-AI/lightning. 10\n- [19] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, 129:3313\u20133337, 2021. 3\n- [20] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 6\n- [21] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021. 27\n- [22] G. Gkioxari, J. Malik, and J. Johnson. Mesh r-cnn. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9785\u20139795, 2019. 3\n- [23] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del R\u00edo, M. Wiebe, P. Peterson, P. G\u00e9rard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2. 10\n- [24] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017. 2\n- [25] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 2, 3\n- [26] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9(3):90\u201395, 2007. doi: 10.1109/MCSE.2007.55. 10\n- [27] A. Jain, M. Tancik, and P. Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5885\u20135894, 2021. 9\n- [28] H. Jun and A. Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. 3\n- [29] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 3\n- [30] H. Kato, Y. Ushiku, and T. Harada. Neural 3d mesh renderer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3907\u20133916, 2018. 3\n- [31] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 2\n- [32] J. J. Lim, H. Pirsiavash, and A. Torralba. Parsing ikea objects: Fine pose estimation. In Proceedings of the IEEE international conference on computer vision, pages 2992\u20132999, 2013. 3\n- [33] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu, and T.-Y. Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309, 2023. 3\n- [34] R. Liu and C. Vondrick. Humans as light bulbs: 3d human reconstruction from thermal reflection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12531\u201312542, 2023. 3", "md": "- [16] K. Deng, A. Liu, J.-Y. Zhu, and D. Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12882\u201312891, 2022. 9\n- [17] L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and V. Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 2553\u20132560. IEEE, 2022. 3, 7, 8\n- [18] W. Falcon and The PyTorch Lightning team. PyTorch Lightning, Mar. 2019. URL https://github.com/Lightning-AI/lightning. 10\n- [19] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, 129:3313\u20133337, 2021. 3\n- [20] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 6\n- [21] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021. 27\n- [22] G. Gkioxari, J. Malik, and J. Johnson. Mesh r-cnn. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9785\u20139795, 2019. 3\n- [23] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del R\u00edo, M. Wiebe, P. Peterson, P. G\u00e9rard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2. 10\n- [24] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017. 2\n- [25] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 2, 3\n- [26] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9(3):90\u201395, 2007. doi: 10.1109/MCSE.2007.55. 10\n- [27] A. Jain, M. Tancik, and P. Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5885\u20135894, 2021. 9\n- [28] H. Jun and A. Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. 3\n- [29] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 3\n- [30] H. Kato, Y. Ushiku, and T. Harada. Neural 3d mesh renderer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3907\u20133916, 2018. 3\n- [31] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 2\n- [32] J. J. Lim, H. Pirsiavash, and A. Torralba. Parsing ikea objects: Fine pose estimation. In Proceedings of the IEEE international conference on computer vision, pages 2992\u20132999, 2013. 3\n- [33] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu, and T.-Y. Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309, 2023. 3\n- [34] R. Liu and C. Vondrick. Humans as light bulbs: 3d human reconstruction from thermal reflection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12531\u201312542, 2023. 3"}]}, {"page": 12, "text": "[35]   R. Liu, S. Menon, C. Mao, D. Park, S. Stent, and C. Vondrick. Shadows shed light on 3d objects.\n       arXiv preprint arXiv:2206.08990, 2022. 3\n[36]   R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick.                               Zero-1-to-3:\n       Zero-shot one image to 3d object, 2023. 2, 3, 8, 14\n[37]   J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi. Unified-io: A unified model for\n       vision, language, and multi-modal tasks.            ArXiv, abs/2206.08916, 2022. 2\n[38]   L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks:\n       Learning 3d reconstruction in function space. In              Proceedings of the IEEE/CVF conference on\n       computer vision and pattern recognition, pages 4460\u20134470, 2019. 3\n[39]   B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:\n       Representing scenes as neural radiance fields for view synthesis. In                  ECCV, 2020. 3, 8\n[40]   G. A. Miller. Wordnet: a lexical database for english.                Communications of the ACM, 38(11):\n       39\u201341, 1995. 3\n[41]   D. Morrison, P. Corke, and J. Leitner. Egad! an evolved grasping analysis dataset for diversity\n       and reproducibility in robotic manipulation.               IEEE Robotics and Automation Letters, 5(3):\n       4368\u20134375, 2020. 3\n[42]   A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen. Point-e: A system for generating 3d\n       point clouds from complex prompts.             arXiv preprint arXiv:2212.08751, 2022. 3\n[43] OpenAI. Gpt-4 technical report.            arXiv, 2023. 2, 3\n[44]   L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\n       K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\n      Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022. 8\n[45]   T. pandas development team. pandas-dev/pandas: Pandas, Feb. 2020. URL                               https://doi.\n       org/10.5281/zenodo.3509134. 10\n[46]   K. Park, K. Rematas, A. Farhadi, and S. M. Seitz. Photoshape: Photorealistic materials for\n       large-scale shape collections.        arXiv preprint arXiv:1809.09761, 2018. 3\n[47]   A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\n       N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning\n       library.  Advances in neural information processing systems, 32, 2019. 10\n[48]   B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\n       arXiv preprint arXiv:2209.14988, 2022. 3, 8\n[49]   A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\n       unsupervised multitask learners.         OpenAI blog, 1(8):9, 2019. 2\n[50]   A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n       P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\n       In International conference on machine learning, pages 8748\u20138763. PMLR, 2021. 2, 6\n[51]   A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image\n       generation with clip latents.       arXiv preprint arXiv:2204.06125, 2022. 2\n[52]   N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson, and G. Gkioxari.\n       Accelerating 3d deep learning with pytorch3d.               arXiv preprint arXiv:2007.08501, 2020. 3\n[53]   S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with\n       region proposal networks.         Advances in neural information processing systems, 28, 2015. 2\n[54]   R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\n       with latent diffusion models. In        Proceedings of the IEEE/CVF Conference on Computer Vision\n       and Pattern Recognition, pages 10684\u201310695, 2022. 2\n[55]   C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\n       A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next\n       generation image-text models.          arXiv preprint arXiv:2210.08402, 2022. 2, 6, 33\n[56]   J.    Tang.              Stable-dreamfusion:              Text-to-3d        with      stable-diffusion,         2022.\n       https://github.com/ashawkey/stable-dreamfusion. 3\n                                                             12", "md": "1. [35] R. Liu, S. Menon, C. Mao, D. Park, S. Stent, and C. Vondrick. Shadows shed light on 3D objects. arXiv preprint arXiv:2206.08990, 2022.\n2. [36] R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3: Zero-shot one image to 3D object, 2023.\n3. [37] J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. ArXiv, abs/2206.08916, 2022.\n4. [38] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3D reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4460\u20134470, 2019.\n5. [39] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n6. [40] G. A. Miller. Wordnet: a lexical database for English. Communications of the ACM, 38(11): 39\u201341, 1995.\n7. [41] D. Morrison, P. Corke, and J. Leitner. Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. IEEE Robotics and Automation Letters, 5(3): 4368\u20134375, 2020.\n8. [42] A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen. Point-e: A system for generating 3D point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\n9. [43] OpenAI. Gpt-4 technical report. arXiv, 2023.\n10. [44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730\u201327744, 2022.\n11. [45] T. pandas development team. pandas-dev/pandas: Pandas, Feb. 2020. URL https://doi.org/10.5281/zenodo.3509134.\n12. [46] K. Park, K. Rematas, A. Farhadi, and S. M. Seitz. Photoshape: Photorealistic materials for large-scale shape collections. arXiv preprint arXiv:1809.09761, 2018.\n13. [47] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n14. [48] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3D using 2D diffusion. arXiv preprint arXiv:2209.14988, 2022.\n15. [49] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9, 2019.\n16. [50] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n17. [51] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n18. [52] N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson, and G. Gkioxari. Accelerating 3D deep learning with pytorch3d. arXiv preprint arXiv:2007.08501, 2020.\n19. [53] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n20. [54] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n21. [55] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n22. [56] J. Tang. Stable-dreamfusion: Text-to-3D with stable-diffusion, 2022. URL: https://github.com/ashawkey/stable-dreamfusion", "images": [], "items": [{"type": "text", "value": "1. [35] R. Liu, S. Menon, C. Mao, D. Park, S. Stent, and C. Vondrick. Shadows shed light on 3D objects. arXiv preprint arXiv:2206.08990, 2022.\n2. [36] R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3: Zero-shot one image to 3D object, 2023.\n3. [37] J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. ArXiv, abs/2206.08916, 2022.\n4. [38] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3D reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4460\u20134470, 2019.\n5. [39] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n6. [40] G. A. Miller. Wordnet: a lexical database for English. Communications of the ACM, 38(11): 39\u201341, 1995.\n7. [41] D. Morrison, P. Corke, and J. Leitner. Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. IEEE Robotics and Automation Letters, 5(3): 4368\u20134375, 2020.\n8. [42] A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen. Point-e: A system for generating 3D point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\n9. [43] OpenAI. Gpt-4 technical report. arXiv, 2023.\n10. [44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730\u201327744, 2022.\n11. [45] T. pandas development team. pandas-dev/pandas: Pandas, Feb. 2020. URL https://doi.org/10.5281/zenodo.3509134.\n12. [46] K. Park, K. Rematas, A. Farhadi, and S. M. Seitz. Photoshape: Photorealistic materials for large-scale shape collections. arXiv preprint arXiv:1809.09761, 2018.\n13. [47] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n14. [48] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3D using 2D diffusion. arXiv preprint arXiv:2209.14988, 2022.\n15. [49] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9, 2019.\n16. [50] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n17. [51] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n18. [52] N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson, and G. Gkioxari. Accelerating 3D deep learning with pytorch3d. arXiv preprint arXiv:2007.08501, 2020.\n19. [53] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n20. [54] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n21. [55] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n22. [56] J. Tang. Stable-dreamfusion: Text-to-3D with stable-diffusion, 2022. URL: https://github.com/ashawkey/stable-dreamfusion", "md": "1. [35] R. Liu, S. Menon, C. Mao, D. Park, S. Stent, and C. Vondrick. Shadows shed light on 3D objects. arXiv preprint arXiv:2206.08990, 2022.\n2. [36] R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3: Zero-shot one image to 3D object, 2023.\n3. [37] J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. ArXiv, abs/2206.08916, 2022.\n4. [38] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3D reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4460\u20134470, 2019.\n5. [39] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n6. [40] G. A. Miller. Wordnet: a lexical database for English. Communications of the ACM, 38(11): 39\u201341, 1995.\n7. [41] D. Morrison, P. Corke, and J. Leitner. Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation. IEEE Robotics and Automation Letters, 5(3): 4368\u20134375, 2020.\n8. [42] A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen. Point-e: A system for generating 3D point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\n9. [43] OpenAI. Gpt-4 technical report. arXiv, 2023.\n10. [44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730\u201327744, 2022.\n11. [45] T. pandas development team. pandas-dev/pandas: Pandas, Feb. 2020. URL https://doi.org/10.5281/zenodo.3509134.\n12. [46] K. Park, K. Rematas, A. Farhadi, and S. M. Seitz. Photoshape: Photorealistic materials for large-scale shape collections. arXiv preprint arXiv:1809.09761, 2018.\n13. [47] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n14. [48] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3D using 2D diffusion. arXiv preprint arXiv:2209.14988, 2022.\n15. [49] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9, 2019.\n16. [50] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n17. [51] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n18. [52] N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson, and G. Gkioxari. Accelerating 3D deep learning with pytorch3d. arXiv preprint arXiv:2007.08501, 2020.\n19. [53] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n20. [54] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n21. [55] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n22. [56] J. Tang. Stable-dreamfusion: Text-to-3D with stable-diffusion, 2022. URL: https://github.com/ashawkey/stable-dreamfusion"}]}, {"page": 13, "text": " [57]  H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\n       E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models.                               arXiv\n       preprint arXiv:2302.13971, 2023. 2\n [58]  H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich. Score jacobian chaining: Lifting\n       pretrained 2d diffusion models for 3d generation. In               Proceedings of the IEEE/CVF Conference\n       on Computer Vision and Pattern Recognition, pages 12619\u201312629, 2023. 8\n [59]  N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: Generating 3d mesh\n       models from single rgb images. In           Proceedings of the European conference on computer vision\n       (ECCV), pages 52\u201367, 2018. 3\n [60]  Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla,\n       N. Snavely, and T. Funkhouser.               Ibrnet: Learning multi-view image-based rendering.                      In\n       Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n       4690\u20134699, 2021. 9\n[61]   M. L. Waskom. seaborn: statistical data visualization.               Journal of Open Source Software, 6(60):\n       3021, 2021. doi: 10.21105/joss.03021. URL               https://doi.org/10.21105/joss.03021. 10\n [62]  C.-Y. Wu, J. Johnson, J. Malik, C. Feichtenhofer, and G. Gkioxari. Multiview compressive\n       coding for 3d reconstruction.         arXiv preprint arXiv:2301.08247, 2023. 3\n [63]  T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian, D. Lin,\n       and Z. Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, recon-\n       struction and generation.        IEEE/CVF Conference on Computer Vision and Pattern Recognition\n       (CVPR), 2023. 3\n [64]  A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or\n       few images. In       Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n       Recognition, pages 4578\u20134587, 2021. 3, 8, 9\n [65]  R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of\n       deep features as a perceptual metric. In          Proceedings of the IEEE conference on computer vision\n       and pattern recognition, pages 586\u2013595, 2018. 8\n [66]  C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less\n       is more for alignment.        arXiv preprint arXiv:2305.11206, 2023. 8\n[67] Q. Zhou and A. Jacobson. Thingi10k: A dataset of 10,000 3d-printing models.                             arXiv preprint\n       arXiv:1605.04797, 2016. 3\n                                                              13", "md": "|[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.|2|\n|---|---|\n|[58] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12619\u201312629, 2023.|8|\n|[59] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 52\u201367, 2018.|3|\n|[60] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2021.|9|\n|[61] M. L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60): 3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.|10|\n|[62] C.-Y. Wu, J. Johnson, J. Malik, C. Feichtenhofer, and G. Gkioxari. Multiview compressive coding for 3d reconstruction. arXiv preprint arXiv:2301.08247, 2023.|3|\n|[63] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian, D. Lin, and Z. Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.|3|\n|[64] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578\u20134587, 2021.|3, 8, 9|\n|[65] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.|8|\n|[66] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.|8|\n|[67] Q. Zhou and A. Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016.|3|", "images": [], "items": [{"type": "table", "rows": [["[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.", "2"], ["[58] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12619\u201312629, 2023.", "8"], ["[59] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 52\u201367, 2018.", "3"], ["[60] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2021.", "9"], ["[61] M. L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60): 3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.", "10"], ["[62] C.-Y. Wu, J. Johnson, J. Malik, C. Feichtenhofer, and G. Gkioxari. Multiview compressive coding for 3d reconstruction. arXiv preprint arXiv:2301.08247, 2023.", "3"], ["[63] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian, D. Lin, and Z. Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.", "3"], ["[64] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578\u20134587, 2021.", "3, 8, 9"], ["[65] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.", "8"], ["[66] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.", "8"], ["[67] Q. Zhou and A. Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016.", "3"]], "md": "|[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.|2|\n|---|---|\n|[58] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12619\u201312629, 2023.|8|\n|[59] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 52\u201367, 2018.|3|\n|[60] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2021.|9|\n|[61] M. L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60): 3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.|10|\n|[62] C.-Y. Wu, J. Johnson, J. Malik, C. Feichtenhofer, and G. Gkioxari. Multiview compressive coding for 3d reconstruction. arXiv preprint arXiv:2301.08247, 2023.|3|\n|[63] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian, D. Lin, and Z. Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.|3|\n|[64] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578\u20134587, 2021.|3, 8, 9|\n|[65] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.|8|\n|[66] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.|8|\n|[67] Q. Zhou and A. Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016.|3|", "isPerfectTable": true, "csv": "\"[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\",\"2\"\n\"[58] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12619\u201312629, 2023.\",\"8\"\n\"[59] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 52\u201367, 2018.\",\"3\"\n\"[60] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2021.\",\"9\"\n\"[61] M. L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60): 3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.\",\"10\"\n\"[62] C.-Y. Wu, J. Johnson, J. Malik, C. Feichtenhofer, and G. Gkioxari. Multiview compressive coding for 3d reconstruction. arXiv preprint arXiv:2301.08247, 2023.\",\"3\"\n\"[63] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian, D. Lin, and Z. Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\",\"3\"\n\"[64] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578\u20134587, 2021.\",\"3, 8, 9\"\n\"[65] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.\",\"8\"\n\"[66] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\",\"8\"\n\"[67] Q. Zhou and A. Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016.\",\"3\""}]}, {"page": 14, "text": "A     Implementation Details\nA.1     Zero123-XL\nA batch size of 2048 is used during training with a learning rate of 1e-4. Different from the original\npaper [36], we performed a second-stage finetuning with a smaller learning rate of 5e-5 on a high-\nquality subset of Objaverse-XL selected with dataset metadata. The first stage was trained for 375K\niterations and the second stage is trained for 65K iterations. For dataset scaling experiment whose\nresults are shown in 6, datasets with size below 800K are randomly sampled subsets from Objaverse\n1.0. We keep the rest of the setting consistent with the original paper [36]. For calculating LPIPS\nmetric in Figure 6, we multiple the score by 10 for better visualization.\nB     Additional Zero123-XL Comparisons\nFigures 7-18 show additional comparisons between Zero123-XL and Zero123. Overall, Zero123-XL\nshows better generalization than Zero123 by both better following the camera transformation and\ngenerating more plausible outputs.                            14", "md": "```markdown\nA Implementation Details\n\nA.1 Zero123-XL\nA batch size of 2048 is used during training with a learning rate of 1e-4. Different from the original\npaper [36], we performed a second-stage finetuning with a smaller learning rate of 5e-5 on a high-\nquality subset of Objaverse-XL selected with dataset metadata. The first stage was trained for 375K\niterations and the second stage is trained for 65K iterations. For dataset scaling experiment whose\nresults are shown in 6, datasets with size below 800K are randomly sampled subsets from Objaverse\n1.0. We keep the rest of the setting consistent with the original paper [36]. For calculating LPIPS\nmetric in Figure 6, we multiple the score by 10 for better visualization.\n\nB Additional Zero123-XL Comparisons\nFigures 7-18 show additional comparisons between Zero123-XL and Zero123. Overall, Zero123-XL\nshows better generalization than Zero123 by both better following the camera transformation and\ngenerating more plausible outputs. 14\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nA Implementation Details\n\nA.1 Zero123-XL\nA batch size of 2048 is used during training with a learning rate of 1e-4. Different from the original\npaper [36], we performed a second-stage finetuning with a smaller learning rate of 5e-5 on a high-\nquality subset of Objaverse-XL selected with dataset metadata. The first stage was trained for 375K\niterations and the second stage is trained for 65K iterations. For dataset scaling experiment whose\nresults are shown in 6, datasets with size below 800K are randomly sampled subsets from Objaverse\n1.0. We keep the rest of the setting consistent with the original paper [36]. For calculating LPIPS\nmetric in Figure 6, we multiple the score by 10 for better visualization.\n\nB Additional Zero123-XL Comparisons\nFigures 7-18 show additional comparisons between Zero123-XL and Zero123. Overall, Zero123-XL\nshows better generalization than Zero123 by both better following the camera transformation and\ngenerating more plausible outputs. 14\n```", "md": "```markdown\nA Implementation Details\n\nA.1 Zero123-XL\nA batch size of 2048 is used during training with a learning rate of 1e-4. Different from the original\npaper [36], we performed a second-stage finetuning with a smaller learning rate of 5e-5 on a high-\nquality subset of Objaverse-XL selected with dataset metadata. The first stage was trained for 375K\niterations and the second stage is trained for 65K iterations. For dataset scaling experiment whose\nresults are shown in 6, datasets with size below 800K are randomly sampled subsets from Objaverse\n1.0. We keep the rest of the setting consistent with the original paper [36]. For calculating LPIPS\nmetric in Figure 6, we multiple the score by 10 for better visualization.\n\nB Additional Zero123-XL Comparisons\nFigures 7-18 show additional comparisons between Zero123-XL and Zero123. Overall, Zero123-XL\nshows better generalization than Zero123 by both better following the camera transformation and\ngenerating more plausible outputs. 14\n```"}]}, {"page": 15, "text": "                             Input View                                   Zero123-XL           Zero123\nFigure 7: Additional examples comparing the outputs of Zero123-XL and Zero123 under different\ncamera transformations.                                   15", "md": "Figure 7: Additional examples comparing the outputs of Zero123-XL and Zero123 under different camera transformations.", "images": [{"name": "img_p14_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "Figure 7: Additional examples comparing the outputs of Zero123-XL and Zero123 under different camera transformations.", "md": "Figure 7: Additional examples comparing the outputs of Zero123-XL and Zero123 under different camera transformations."}]}, {"page": 16, "text": "                  F                                              RA\n                                                                             R\n                    Input View                                  Zero123-XL       Zero123\nFigure 8: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                 16", "md": "Figure 8: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$16$$", "images": [{"name": "img_p15_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "Figure 8: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$16$$", "md": "Figure 8: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$16$$"}]}, {"page": 17, "text": "                     D                                                   IA\n                    Input View                                  Zero123-XL       Zero123\nFigure 9: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                 17", "md": "Zero123-XL       Zero123\n\nFigure 9: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n17", "images": [{"name": "img_p16_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "Zero123-XL       Zero123\n\nFigure 9: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n17", "md": "Zero123-XL       Zero123\n\nFigure 9: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n17"}]}, {"page": 18, "text": "                     Input View                                  Zero123-XL       Zero123\nFigure 10: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  18", "md": "Input View\n\nZero123-XL       Zero123\n\nFigure 10: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$18$$", "images": [{"name": "img_p17_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "Input View\n\nZero123-XL       Zero123\n\nFigure 10: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$18$$", "md": "Input View\n\nZero123-XL       Zero123\n\nFigure 10: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$18$$"}]}, {"page": 19, "text": "                     Input View                                  Zero123-XL       Zero123\nFigure 11: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  19", "md": "## Input View\n\nFigure 11: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n| |Zero123-XL|Zero123|\n|---|---|---|\n| |19| |", "images": [{"name": "img_p18_1", "height": 1844, "width": 1153}], "items": [{"type": "heading", "lvl": 2, "value": "Input View", "md": "## Input View"}, {"type": "text", "value": "Figure 11: Continuation of additional examples comparing Zero123-XL and Zero123.", "md": "Figure 11: Continuation of additional examples comparing Zero123-XL and Zero123."}, {"type": "table", "rows": [["", "Zero123-XL", "Zero123"], ["", "19", ""]], "md": "| |Zero123-XL|Zero123|\n|---|---|---|\n| |19| |", "isPerfectTable": true, "csv": "\"\",\"Zero123-XL\",\"Zero123\"\n\"\",\"19\",\"\""}]}, {"page": 20, "text": "                       L\n                     Input View                                  Zero123-XL       Zero123\nFigure 12: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  20", "md": "Figure 12: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n& Zero123-XL & Zero123 \\\\\n\\hline\n20 & & \\\\\n\\hline\n\\end{array}\n$$", "images": [{"name": "img_p19_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "Figure 12: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n& Zero123-XL & Zero123 \\\\\n\\hline\n20 & & \\\\\n\\hline\n\\end{array}\n$$", "md": "Figure 12: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n& Zero123-XL & Zero123 \\\\\n\\hline\n20 & & \\\\\n\\hline\n\\end{array}\n$$"}]}, {"page": 21, "text": "                                                                     R R\n                     Input View                                  Zero123-XL       Zero123\nFigure 13: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  21", "md": "## Input View\n\nFigure 13: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$21$$", "images": [{"name": "img_p20_1", "height": 1844, "width": 1153}], "items": [{"type": "heading", "lvl": 2, "value": "Input View", "md": "## Input View"}, {"type": "text", "value": "Figure 13: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$21$$", "md": "Figure 13: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$21$$"}]}, {"page": 22, "text": "                     Input View                                  Zero123-XL       Zero123\nFigure 14: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  22", "md": "Input View\n\nZero123-XL       Zero123\n\nFigure 14: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n22", "images": [{"name": "img_p21_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "Input View\n\nZero123-XL       Zero123\n\nFigure 14: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n22", "md": "Input View\n\nZero123-XL       Zero123\n\nFigure 14: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n22"}]}, {"page": 23, "text": "                    A                                             A A\n                     Input View                                  Zero123-XL       Zero123\nFigure 15: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  23", "md": "Input View\n\nZero123-XL\nZero123\nFigure 15: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$23$$", "images": [{"name": "img_p22_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "Input View\n\nZero123-XL\nZero123\nFigure 15: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$23$$", "md": "Input View\n\nZero123-XL\nZero123\nFigure 15: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n$$23$$"}]}, {"page": 24, "text": "                     7\n                     Input View                                  Zero123-XL       Zero123\nFigure 16: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  24", "md": "<h7>7</h7>\n\nInput View\nZero123-XL\nZero123\n\nFigure 16: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n24", "images": [{"name": "img_p23_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "<h7>7</h7>\n\nInput View\nZero123-XL\nZero123\n\nFigure 16: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n24", "md": "<h7>7</h7>\n\nInput View\nZero123-XL\nZero123\n\nFigure 16: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n24"}]}, {"page": 25, "text": "                     Input View                                  Zero123-XL       Zero123\nFigure 17: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  25", "md": "Input View\n\nZero123-XL       Zero123\n\nFigure 17: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n25", "images": [{"name": "img_p24_1", "height": 1844, "width": 1153}], "items": [{"type": "text", "value": "Input View\n\nZero123-XL       Zero123\n\nFigure 17: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n25", "md": "Input View\n\nZero123-XL       Zero123\n\nFigure 17: Continuation of additional examples comparing Zero123-XL and Zero123.\n\n25"}]}, {"page": 26, "text": "                                                                   5\n                     Input View                                  Zero123-XL       Zero123\nFigure 18: Continuation of additional examples comparing Zero123-XL and Zero123.\n                                                  26", "md": "##### Input View\n\nFigure 18: Continuation of additional examples comparing Zero123-XL and Zero123.", "images": [{"name": "img_p25_1", "height": 1844, "width": 1153}], "items": [{"type": "heading", "lvl": 5, "value": "Input View", "md": "##### Input View"}, {"type": "text", "value": "Figure 18: Continuation of additional examples comparing Zero123-XL and Zero123.", "md": "Figure 18: Continuation of additional examples comparing Zero123-XL and Zero123."}]}, {"page": 27, "text": "C      Datasheet\nThis section provides a datasheet [21] for Objaverse-XL.\nC.1     Motivation\n For what purpose was the dataset created?                       Was there a specific task in mind? Was there a\nspecific gap that needed to be filled? Please provide a description.\nThe Objaverse-XL dataset was created to address the lack of high-quality, large-scale datasets for\n3D vision tasks. This was due to challenges in acquiring such data and the associated complexities\nof 3D object generation and reconstruction, which were largely reliant on smaller, handcrafted\ndatasets. The dataset was designed with the aim of advancing the field of 3D vision, allowing for\nthe development and generalization improvement of models like Zero123 which work on tasks like\nnovel view synthesis. The creation of Objaverse-XL essentially fills the gap in data availability for\n3D vision tasks, particularly in light of increasing demand and interest in simulation, AR and VR\ntechnologies, and generative AI.\n Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)?\nThe dataset was created by researchers at the Allen Institute for AI and at the University of\nWashington, Seattle.\n What support was needed to make this dataset?                      (e.g., who funded the creation of the dataset? If\nthere is an associated grant, provide the name of the grantor and the grant name and number, or if it\nwas supported by a company or government agency, give those details.)\nStability AI provided compute support and guidance for the main experiments in the paper. The Allen\nInstitute for AI also provided compute support for collecting the dataset and performing rendering.\n Any other comments?\nNo\nC.2     Composition\n What do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)?        Are there multiple types of instances (e.g., movies, users, and ratings; people and\ninteractions between them; nodes and edges)? Please provide a description.\nInstances of the dataset comprise of 3D objects and their associated metadata.\n How many instances are there in total (of each type, if appropriate)?\nThere are approximately 10.2 million rendered 3D files.                       About 56% come from GitHub, 35%\ncome from Thingiverse, 8% come from Sketchfab, and less than 1% come from Polycam and the\nSmithsonian Institute.         We also release additional links to indexed GitHub files that are not in-\ncluded in the count due to being removed by deduplication or not being easily importable into Blender.\n Does the dataset contain all possible instances or is it a sample (not necessarily random)\nof instances from a larger set?                If the dataset is a sample, then what is the larger set? Is the\nsample representative of the larger set (e.g., geographic coverage)?                        If so, please describe how\nthis representativeness was validated/verified. If it is not representative of the larger set, please\ndescribe why not (e.g., to cover a more diverse range of instances, because instances were withheld\nor unavailable).\nThe dataset contains a sample of objects on GitHub, Sketchfab, Thingiverse, and Polycam, along\nwith all the objects from the Smithsonian Institute.\n What data does each instance consist of?                     \u201cRaw\u201d data (e.g., unprocessed text or images) or\nfeatures? In either case, please provide a description.\nInstances of the dataset vary based on source. For Polycam and Sketchfab objects, we release the full\n                                                              27", "md": "# Objaverse-XL Datasheet\n\n# Datasheet\n\nThis section provides a datasheet [21] for Objaverse-XL.\n\n## Motivation\n\nFor what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\nThe Objaverse-XL dataset was created to address the lack of high-quality, large-scale datasets for 3D vision tasks. This was due to challenges in acquiring such data and the associated complexities of 3D object generation and reconstruction, which were largely reliant on smaller, handcrafted datasets. The dataset was designed with the aim of advancing the field of 3D vision, allowing for the development and generalization improvement of models like Zero123 which work on tasks like novel view synthesis. The creation of Objaverse-XL essentially fills the gap in data availability for 3D vision tasks, particularly in light of increasing demand and interest in simulation, AR and VR technologies, and generative AI.\n\nWho created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\n\nThe dataset was created by researchers at the Allen Institute for AI and at the University of Washington, Seattle.\n\nWhat support was needed to make this dataset? (e.g., who funded the creation of the dataset? If there is an associated grant, provide the name of the grantor and the grant name and number, or if it was supported by a company or government agency, give those details.)\n\nStability AI provided compute support and guidance for the main experiments in the paper. The Allen Institute for AI also provided compute support for collecting the dataset and performing rendering.\n\nAny other comments?\n\nNo\n\n## Composition\n\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\nInstances of the dataset comprise of 3D objects and their associated metadata.\n\nHow many instances are there in total (of each type, if appropriate)?\n\nThere are approximately 10.2 million rendered 3D files. About 56% come from GitHub, 35% come from Thingiverse, 8% come from Sketchfab, and less than 1% come from Polycam and the Smithsonian Institute. We also release additional links to indexed GitHub files that are not included in the count due to being removed by deduplication or not being easily importable into Blender.\n\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\n\nThe dataset contains a sample of objects on GitHub, Sketchfab, Thingiverse, and Polycam, along with all the objects from the Smithsonian Institute.\n\nWhat data does each instance consist of? \"Raw\" data (e.g., unprocessed text or images) or features? In either case, please provide a description.\n\nInstances of the dataset vary based on source. For Polycam and Sketchfab objects, we release the full\n\n$$Raw data (e.g., unprocessed text or images) or features? In either case, please provide a description.$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Objaverse-XL Datasheet", "md": "# Objaverse-XL Datasheet"}, {"type": "heading", "lvl": 1, "value": "Datasheet", "md": "# Datasheet"}, {"type": "text", "value": "This section provides a datasheet [21] for Objaverse-XL.", "md": "This section provides a datasheet [21] for Objaverse-XL."}, {"type": "heading", "lvl": 2, "value": "Motivation", "md": "## Motivation"}, {"type": "text", "value": "For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\nThe Objaverse-XL dataset was created to address the lack of high-quality, large-scale datasets for 3D vision tasks. This was due to challenges in acquiring such data and the associated complexities of 3D object generation and reconstruction, which were largely reliant on smaller, handcrafted datasets. The dataset was designed with the aim of advancing the field of 3D vision, allowing for the development and generalization improvement of models like Zero123 which work on tasks like novel view synthesis. The creation of Objaverse-XL essentially fills the gap in data availability for 3D vision tasks, particularly in light of increasing demand and interest in simulation, AR and VR technologies, and generative AI.\n\nWho created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\n\nThe dataset was created by researchers at the Allen Institute for AI and at the University of Washington, Seattle.\n\nWhat support was needed to make this dataset? (e.g., who funded the creation of the dataset? If there is an associated grant, provide the name of the grantor and the grant name and number, or if it was supported by a company or government agency, give those details.)\n\nStability AI provided compute support and guidance for the main experiments in the paper. The Allen Institute for AI also provided compute support for collecting the dataset and performing rendering.\n\nAny other comments?\n\nNo", "md": "For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\nThe Objaverse-XL dataset was created to address the lack of high-quality, large-scale datasets for 3D vision tasks. This was due to challenges in acquiring such data and the associated complexities of 3D object generation and reconstruction, which were largely reliant on smaller, handcrafted datasets. The dataset was designed with the aim of advancing the field of 3D vision, allowing for the development and generalization improvement of models like Zero123 which work on tasks like novel view synthesis. The creation of Objaverse-XL essentially fills the gap in data availability for 3D vision tasks, particularly in light of increasing demand and interest in simulation, AR and VR technologies, and generative AI.\n\nWho created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\n\nThe dataset was created by researchers at the Allen Institute for AI and at the University of Washington, Seattle.\n\nWhat support was needed to make this dataset? (e.g., who funded the creation of the dataset? If there is an associated grant, provide the name of the grantor and the grant name and number, or if it was supported by a company or government agency, give those details.)\n\nStability AI provided compute support and guidance for the main experiments in the paper. The Allen Institute for AI also provided compute support for collecting the dataset and performing rendering.\n\nAny other comments?\n\nNo"}, {"type": "heading", "lvl": 2, "value": "Composition", "md": "## Composition"}, {"type": "text", "value": "What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\nInstances of the dataset comprise of 3D objects and their associated metadata.\n\nHow many instances are there in total (of each type, if appropriate)?\n\nThere are approximately 10.2 million rendered 3D files. About 56% come from GitHub, 35% come from Thingiverse, 8% come from Sketchfab, and less than 1% come from Polycam and the Smithsonian Institute. We also release additional links to indexed GitHub files that are not included in the count due to being removed by deduplication or not being easily importable into Blender.\n\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\n\nThe dataset contains a sample of objects on GitHub, Sketchfab, Thingiverse, and Polycam, along with all the objects from the Smithsonian Institute.\n\nWhat data does each instance consist of? \"Raw\" data (e.g., unprocessed text or images) or features? In either case, please provide a description.\n\nInstances of the dataset vary based on source. For Polycam and Sketchfab objects, we release the full\n\n$$Raw data (e.g., unprocessed text or images) or features? In either case, please provide a description.$$", "md": "What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\nInstances of the dataset comprise of 3D objects and their associated metadata.\n\nHow many instances are there in total (of each type, if appropriate)?\n\nThere are approximately 10.2 million rendered 3D files. About 56% come from GitHub, 35% come from Thingiverse, 8% come from Sketchfab, and less than 1% come from Polycam and the Smithsonian Institute. We also release additional links to indexed GitHub files that are not included in the count due to being removed by deduplication or not being easily importable into Blender.\n\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\n\nThe dataset contains a sample of objects on GitHub, Sketchfab, Thingiverse, and Polycam, along with all the objects from the Smithsonian Institute.\n\nWhat data does each instance consist of? \"Raw\" data (e.g., unprocessed text or images) or features? In either case, please provide a description.\n\nInstances of the dataset vary based on source. For Polycam and Sketchfab objects, we release the full\n\n$$Raw data (e.g., unprocessed text or images) or features? In either case, please provide a description.$$"}]}, {"page": 28, "text": " 3D objects along with associated metadata. For GitHub, Thingiverse, and Smithsonian objects, we\n release links to each of the files that can then be downloaded from the source, along with metadata\n such as license, poly count, vertex count, and other attributes discussed in Section 3.2.\n Is there a label or target associated with each instance?                       If so, please provide a description.\n No, just the 3D object and associated metadata. Labels and targets may be derived from the metadata,\n but vary based on the task.\n  Is any information missing from individual instances?                              If so, please provide a description,\n explaining why this information is missing (e.g., because it was unavailable). This does not include\n intentionally removed information, but might include, e.g., redacted text.\n No, information is not missing.\n Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings, social\n network links)?         If so, please describe how these relationships are made explicit.\n Individual instances are treated as independent.\n Are there recommended data splits (e.g., training, development/validation, testing)?                                         If so,\n please provide a description of these splits, explaining the rationale behind them.\nThere are no recommended data splits across the entire dataset as splits vary based on task.\n Are there any errors, sources of noise, or redundancies in the dataset?                               If so, please provide a\n description.\nWe deduplicated the objects by taking a sha256 of the file contents. There may still be near duplicates\n that exist, if the objects are slightly modified, which could potentially be filtered out, if desirable,\n using CLIP embeddings of the renders.\n Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)?                    If it links to or relies on external resources, a) are there\n guarantees that they will exist, and remain constant, over time; b) are there official archival versions\n of the complete dataset (i.e., including the external resources as they existed at the time the dataset\nwas created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external\n resources that might apply to a future user? Please provide descriptions of all external resources and\n any restrictions associated with them, as well as links or other access points, as appropriate.\nThere are no fees. The Smithsonian data is hosted on a governmental website, which we believe will\n be well supported over time. The Sketchfab and Polycam data will be available to download easily\n on our platform. For Thingiverse and GitHub, the platforms are relatively stable and we expect most\n of the content to remain in place. For Thingiverse, an API key must be provided to download the\n content from their API. For GitHub, the data can be easily cloned. Users must follow the license of\n the content with which the original files were distributed and the terms of service for each platform.\n Does the dataset contain data that might be considered confidential (e.g., data that is protected\n by legal privilege or by doctor-patient confidentiality, data that includes the content of\n individuals\u2019 non-public communications)?                     If so, please provide a description.\nWhile rare, it is possible that confidential data exists as part of the 3D objects on the platforms.\n Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\n or might otherwise cause anxiety?                 If so, please describe why.\nWhile rare, it is possible that data that is considered offensive, insulting, threatening, or might cause\n anxiety exists as part of the 3D objects on the platforms.\n Does the dataset relate to people?                If not, you may skip the remaining questions in this section.\n People may be present in the dataset, but only make up a small part of it. Section discusses the results\n and analysis of running a face detector on renders of the objects. Most often, faces appear from dolls,\n                                                                 28", "md": "# Dataset Information\n\n## Dataset Information\n\n3D objects along with associated metadata. For GitHub, Thingiverse, and Smithsonian objects, we release links to each of the files that can then be downloaded from the source, along with metadata such as license, poly count, vertex count, and other attributes discussed in Section 3.2.\n\nIs there a label or target associated with each instance?\n\nNo, just the 3D object and associated metadata. Labels and targets may be derived from the metadata, but vary based on the task.\n\nIs any information missing from individual instances?\n\nNo, information is not missing.\n\nAre relationships between individual instances made explicit?\n\nIndividual instances are treated as independent.\n\nAre there recommended data splits?\n\nThere are no recommended data splits across the entire dataset as splits vary based on task.\n\nAre there any errors, sources of noise, or redundancies in the dataset?\n\nWe deduplicated the objects by taking a sha256 of the file contents. There may still be near duplicates that exist, if the objects are slightly modified, which could potentially be filtered out, if desirable, using CLIP embeddings of the renders.\n\nIs the dataset self-contained, or does it rely on external resources?\n\nThere are no fees. The Smithsonian data is hosted on a governmental website, which we believe will be well supported over time. The Sketchfab and Polycam data will be available to download easily on our platform. For Thingiverse and GitHub, the platforms are relatively stable and we expect most of the content to remain in place. For Thingiverse, an API key must be provided to download the content from their API. For GitHub, the data can be easily cloned. Users must follow the license of the content with which the original files were distributed and the terms of service for each platform.\n\nDoes the dataset contain data that might be considered confidential?\n\nWhile rare, it is possible that confidential data exists as part of the 3D objects on the platforms.\n\nDoes the dataset contain data that might be offensive or threatening?\n\nWhile rare, it is possible that data that is considered offensive, insulting, threatening, or might cause anxiety exists as part of the 3D objects on the platforms.\n\nDoes the dataset relate to people?\n\nPeople may be present in the dataset, but only make up a small part of it. Section discusses the results and analysis of running a face detector on renders of the objects. Most often, faces appear from dolls.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Dataset Information", "md": "# Dataset Information"}, {"type": "heading", "lvl": 2, "value": "Dataset Information", "md": "## Dataset Information"}, {"type": "text", "value": "3D objects along with associated metadata. For GitHub, Thingiverse, and Smithsonian objects, we release links to each of the files that can then be downloaded from the source, along with metadata such as license, poly count, vertex count, and other attributes discussed in Section 3.2.\n\nIs there a label or target associated with each instance?\n\nNo, just the 3D object and associated metadata. Labels and targets may be derived from the metadata, but vary based on the task.\n\nIs any information missing from individual instances?\n\nNo, information is not missing.\n\nAre relationships between individual instances made explicit?\n\nIndividual instances are treated as independent.\n\nAre there recommended data splits?\n\nThere are no recommended data splits across the entire dataset as splits vary based on task.\n\nAre there any errors, sources of noise, or redundancies in the dataset?\n\nWe deduplicated the objects by taking a sha256 of the file contents. There may still be near duplicates that exist, if the objects are slightly modified, which could potentially be filtered out, if desirable, using CLIP embeddings of the renders.\n\nIs the dataset self-contained, or does it rely on external resources?\n\nThere are no fees. The Smithsonian data is hosted on a governmental website, which we believe will be well supported over time. The Sketchfab and Polycam data will be available to download easily on our platform. For Thingiverse and GitHub, the platforms are relatively stable and we expect most of the content to remain in place. For Thingiverse, an API key must be provided to download the content from their API. For GitHub, the data can be easily cloned. Users must follow the license of the content with which the original files were distributed and the terms of service for each platform.\n\nDoes the dataset contain data that might be considered confidential?\n\nWhile rare, it is possible that confidential data exists as part of the 3D objects on the platforms.\n\nDoes the dataset contain data that might be offensive or threatening?\n\nWhile rare, it is possible that data that is considered offensive, insulting, threatening, or might cause anxiety exists as part of the 3D objects on the platforms.\n\nDoes the dataset relate to people?\n\nPeople may be present in the dataset, but only make up a small part of it. Section discusses the results and analysis of running a face detector on renders of the objects. Most often, faces appear from dolls.", "md": "3D objects along with associated metadata. For GitHub, Thingiverse, and Smithsonian objects, we release links to each of the files that can then be downloaded from the source, along with metadata such as license, poly count, vertex count, and other attributes discussed in Section 3.2.\n\nIs there a label or target associated with each instance?\n\nNo, just the 3D object and associated metadata. Labels and targets may be derived from the metadata, but vary based on the task.\n\nIs any information missing from individual instances?\n\nNo, information is not missing.\n\nAre relationships between individual instances made explicit?\n\nIndividual instances are treated as independent.\n\nAre there recommended data splits?\n\nThere are no recommended data splits across the entire dataset as splits vary based on task.\n\nAre there any errors, sources of noise, or redundancies in the dataset?\n\nWe deduplicated the objects by taking a sha256 of the file contents. There may still be near duplicates that exist, if the objects are slightly modified, which could potentially be filtered out, if desirable, using CLIP embeddings of the renders.\n\nIs the dataset self-contained, or does it rely on external resources?\n\nThere are no fees. The Smithsonian data is hosted on a governmental website, which we believe will be well supported over time. The Sketchfab and Polycam data will be available to download easily on our platform. For Thingiverse and GitHub, the platforms are relatively stable and we expect most of the content to remain in place. For Thingiverse, an API key must be provided to download the content from their API. For GitHub, the data can be easily cloned. Users must follow the license of the content with which the original files were distributed and the terms of service for each platform.\n\nDoes the dataset contain data that might be considered confidential?\n\nWhile rare, it is possible that confidential data exists as part of the 3D objects on the platforms.\n\nDoes the dataset contain data that might be offensive or threatening?\n\nWhile rare, it is possible that data that is considered offensive, insulting, threatening, or might cause anxiety exists as part of the 3D objects on the platforms.\n\nDoes the dataset relate to people?\n\nPeople may be present in the dataset, but only make up a small part of it. Section discusses the results and analysis of running a face detector on renders of the objects. Most often, faces appear from dolls."}]}, {"page": 29, "text": "historic sculptures, and anthropomorphic animations. Moreover, even including such data, only about\n2.5% captured faces.\n Does the dataset identify any subpopulations (e.g., by age, gender)?                           If so, please describe how\nthese subpopulations are identified and provide a description of their respective distributions within\nthe dataset.\nWe do not identify people by subpopulation.\n Is it possible to identify individuals (i.e., one or more natural persons), either directly or\nindirectly (i.e., in combination with other data) from the dataset?                         If so, please describe how.\nIf a scanned person is included in the dataset, it may be possible to visually identify them or identify\nthem if their name is included as part of the metadata.\n Does the dataset contain data that might be considered sensitive in any way (e.g., data that\nreveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or\nunion memberships, or locations; financial or health data; biometric or genetic data; forms of\ngovernment identification, such as social security numbers; criminal history)?                                    If so, please\nprovide a description.\nWhile rare, it is possible that sensitive data may exist as part of the 3D objects on the platforms.\n Any other comments?\nNo.\nC.3     Collection\n How was the data associated with each instance acquired?                         Was the data directly observable (e.g.,\nraw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived\nfrom other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was\nreported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If\nso, please describe how.\nThe data was directly observable and hosted on several platforms, including GitHub, Thingiverse,\nSketchfab, Polycam, and the Smithsonian Institute.\n Over what timeframe was the data collected?                     Does this timeframe match the creation timeframe\nof the data associated with the instances (e.g., recent crawl of old news articles)? If not, please\ndescribe the timeframe in which the data associated with the instances was created. Finally, list when\nthe dataset was first published.\nSketchfab data was collected as part of Objaverse 1.0. The new data was collected in Q1 & Q2 of 2023.\n What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or\nsensor, manual human curation, software program, software API)?                              How were these mechanisms\nor procedures validated?\nPython scripts were used to collect the data.\n What was the resource cost of collecting the data?                      (e.g. what were the required computational\nresources, and the associated financial costs)\nThe cost of collecting the dataset was on the order of several thousand dollars, including costs to find,\nindex, and download it. Resources included AWS CPU instances.\n If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,\nprobabilistic with specific sampling probabilities)?\nThe dataset is filtered down based on licensing restrictions, duplicate content, and based on if the 3D\nobject can successfully be imported into Blender.              29", "md": "## Collection\n\nHow was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n\nThe data was directly observable and hosted on several platforms, including GitHub, Thingiverse, Sketchfab, Polycam, and the Smithsonian Institute.\n\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. Finally, list when the dataset was first published.\n\nSketchfab data was collected as part of Objaverse 1.0. The new data was collected in Q1 & Q2 of 2023.\n\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated?\n\nPython scripts were used to collect the data.\n\nWhat was the resource cost of collecting the data? (e.g. what were the required computational resources, and the associated financial costs)\n\nThe cost of collecting the dataset was on the order of several thousand dollars, including costs to find, index, and download it. Resources included AWS CPU instances.\n\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\n\nThe dataset is filtered down based on licensing restrictions, duplicate content, and based on if the 3D object can successfully be imported into Blender.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Collection", "md": "## Collection"}, {"type": "text", "value": "How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n\nThe data was directly observable and hosted on several platforms, including GitHub, Thingiverse, Sketchfab, Polycam, and the Smithsonian Institute.\n\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. Finally, list when the dataset was first published.\n\nSketchfab data was collected as part of Objaverse 1.0. The new data was collected in Q1 & Q2 of 2023.\n\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated?\n\nPython scripts were used to collect the data.\n\nWhat was the resource cost of collecting the data? (e.g. what were the required computational resources, and the associated financial costs)\n\nThe cost of collecting the dataset was on the order of several thousand dollars, including costs to find, index, and download it. Resources included AWS CPU instances.\n\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\n\nThe dataset is filtered down based on licensing restrictions, duplicate content, and based on if the 3D object can successfully be imported into Blender.", "md": "How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n\nThe data was directly observable and hosted on several platforms, including GitHub, Thingiverse, Sketchfab, Polycam, and the Smithsonian Institute.\n\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. Finally, list when the dataset was first published.\n\nSketchfab data was collected as part of Objaverse 1.0. The new data was collected in Q1 & Q2 of 2023.\n\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated?\n\nPython scripts were used to collect the data.\n\nWhat was the resource cost of collecting the data? (e.g. what were the required computational resources, and the associated financial costs)\n\nThe cost of collecting the dataset was on the order of several thousand dollars, including costs to find, index, and download it. Resources included AWS CPU instances.\n\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\n\nThe dataset is filtered down based on licensing restrictions, duplicate content, and based on if the 3D object can successfully be imported into Blender."}]}, {"page": 30, "text": " Who was involved in the data collection process (e.g., students, crowdworkers, contractors)\n and how were they compensated (e.g., how much were crowdworkers paid)?\nThe data collection process was primarily performed by employed researchers at the Allen Institute\n for AI.\n Were any ethical review processes conducted (e.g., by an institutional review board)?                                      If so,\n please provide a description of these review processes, including the outcomes, as well as a link or\n other access point to any supporting documentation.\n Institutional review boards were not involved in the collection of the dataset.\n Does the dataset relate to people?                 If not, you may skip the remainder of the questions in this\n section.\n People may be present in the dataset, but only make up a small part of it. Section discusses the results\n and analysis of running a face detector on renders of the objects. Most often, faces appear from dolls,\n historic sculptures, and anthropomorphic animations. Moreover, even including such data, only about\n 2.5% captured faces.\n Did you collect the data from the individuals in question directly, or obtain it via third parties\n or other sources (e.g., websites)?\n Data was collected from public facing platforms. On each of the platforms, users opted to make their\n data public.\n Were the individuals in question notified about the data collection?                             If so, please describe (or\n show with screenshots or other information) how notice was provided, and provide a link or other\n access point to, or otherwise reproduce, the exact language of the notification itself.\n Individuals were not notified about the collection of the dataset.\n Did the individuals in question consent to the collection and use of their data?                                 If so, please\n describe (or show with screenshots or other information) how consent was requested and provided,\n and provide a link or other access point to, or otherwise reproduce, the exact language to which the\n individuals consented.\n Individuals were not notified about the collection of the dataset.\n If consent was obtained, were the consenting individuals provided with a mechanism to revoke\n their consent in the future or for certain uses?                If so, please provide a description, as well as a link\n or other access point to the mechanism (if appropriate)\n Individuals were not notified about the collection of the dataset.\n Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data\n protection impact analysis) been conducted?                     If so, please provide a description of this analysis,\n including the outcomes, as well as a link or other access point to any supporting documentation.\nAn analysis has not been conducted.\n Any other comments?\n No.\n C.4     Processing / Cleaning / Labeling\n  Was any preprocessing/cleaning/labeling of the data done(e.g., discretization or bucketing,\n tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing\n of missing values)?         If so, please provide a description. If not, you may skip the remainder of the\n questions in this section.\n Preprocessing the data was performed by computing renders of the objects and performing\n                                                                30", "md": "Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The data collection process was primarily performed by employed researchers at the Allen Institute for AI.\n\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. Institutional review boards were not involved in the collection of the dataset.\n\nDoes the dataset relate to people? If not, you may skip the remainder of the questions in this section. People may be present in the dataset, but only make up a small part of it. Section discusses the results and analysis of running a face detector on renders of the objects. Most often, faces appear from dolls, historic sculptures, and anthropomorphic animations. Moreover, even including such data, only about 2.5% captured faces.\n\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? Data was collected from public facing platforms. On each of the platforms, users opted to make their data public.\n\nWere the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. Individuals were not notified about the collection of the dataset.\n\nDid the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. Individuals were not notified about the collection of the dataset.\n\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). Individuals were not notified about the collection of the dataset.\n\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. An analysis has not been conducted.\n\nAny other comments? No.\n\n## C.4 Processing / Cleaning / Labeling\n\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section. Preprocessing the data was performed by computing renders of the objects and performing\n\n$$30$$", "images": [], "items": [{"type": "text", "value": "Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The data collection process was primarily performed by employed researchers at the Allen Institute for AI.\n\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. Institutional review boards were not involved in the collection of the dataset.\n\nDoes the dataset relate to people? If not, you may skip the remainder of the questions in this section. People may be present in the dataset, but only make up a small part of it. Section discusses the results and analysis of running a face detector on renders of the objects. Most often, faces appear from dolls, historic sculptures, and anthropomorphic animations. Moreover, even including such data, only about 2.5% captured faces.\n\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? Data was collected from public facing platforms. On each of the platforms, users opted to make their data public.\n\nWere the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. Individuals were not notified about the collection of the dataset.\n\nDid the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. Individuals were not notified about the collection of the dataset.\n\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). Individuals were not notified about the collection of the dataset.\n\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. An analysis has not been conducted.\n\nAny other comments? No.", "md": "Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The data collection process was primarily performed by employed researchers at the Allen Institute for AI.\n\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. Institutional review boards were not involved in the collection of the dataset.\n\nDoes the dataset relate to people? If not, you may skip the remainder of the questions in this section. People may be present in the dataset, but only make up a small part of it. Section discusses the results and analysis of running a face detector on renders of the objects. Most often, faces appear from dolls, historic sculptures, and anthropomorphic animations. Moreover, even including such data, only about 2.5% captured faces.\n\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? Data was collected from public facing platforms. On each of the platforms, users opted to make their data public.\n\nWere the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. Individuals were not notified about the collection of the dataset.\n\nDid the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. Individuals were not notified about the collection of the dataset.\n\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). Individuals were not notified about the collection of the dataset.\n\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. An analysis has not been conducted.\n\nAny other comments? No."}, {"type": "heading", "lvl": 2, "value": "C.4 Processing / Cleaning / Labeling", "md": "## C.4 Processing / Cleaning / Labeling"}, {"type": "text", "value": "Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section. Preprocessing the data was performed by computing renders of the objects and performing\n\n$$30$$", "md": "Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section. Preprocessing the data was performed by computing renders of the objects and performing\n\n$$30$$"}]}, {"page": 31, "text": "deduplication.\n Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support\nunanticipated future uses)?             If so, please provide a link or other access point to the \u201craw\u201d data.\ns The data that is downloaded contains the raw data and does not modify the individual files.\n Is the software used to preprocess/clean/label the instances available?                             If so, please provide a\nlink or other access point.\nThe software for cleaning the dataset and rendering will be made available.\n Any other comments?\nNo.\nC.5     Uses\n Has the dataset been used for any tasks already?                     If so, please provide a description.\nYes, please see Section 4 of the paper.\n Is there a repository that links to any or all papers or systems that use the dataset?                                    If so,\nplease provide a link or other access point.\nWe recommend checking the Semantic Scholar page for the Objaverse-XL and Objaverse 1.0 papers\nto find up to date papers that use the dataset.\n What (other) tasks could the dataset be used for?\nThe dataset could be used for a large number of use cases. Examples include making 3D tools more\naccessible (e.g., 3D impainting, text to 3D, image to 3D), robotic simulation & embodied AI, training\nvideo models on animations, 2D vision tasks such as segmentation, and more.\n Is there anything about the composition of the dataset or the way it was collected and\npreprocessed/cleaned/labeled that might impact future uses?                          For example, is there anything that\na future user might need to know to avoid uses that could result in unfair treatment of individuals or\ngroups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms,\nlegal risks) If so, please provide a description. Is there anything a future user could do to mitigate\nthese undesirable harms?\nUsers should follow the license of the individual objects distributed as part of this dataset.\n Are there tasks for which the dataset should not be used?                        If so, please provide a description.\nNew tasks must make sure to follow the license of the dataset and the license of the individual objects\ndistributed as part of the dataset.\n Any other comments?\nNo.\nC.6     Distribution\n Will the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created?                         If so, please provide a description.\nYes. The dataset will be made public.\n How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?                                    Does the\ndataset have a digital object identifier (DOI)?\n                                                               31", "md": "# Dataset Information\n\n## Uses\n\nHas the dataset been used for any tasks already? If so, please provide a description.\n\nYes, please see Section 4 of the paper.\n\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\n\nWe recommend checking the Semantic Scholar page for the Objaverse-XL and Objaverse 1.0 papers to find up to date papers that use the dataset.\n\nWhat (other) tasks could the dataset be used for?\n\nThe dataset could be used for a large number of use cases. Examples include making 3D tools more accessible (e.g., 3D impainting, text to 3D, image to 3D), robotic simulation & embodied AI, training video models on animations, 2D vision tasks such as segmentation, and more.\n\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?\n\nUsers should follow the license of the individual objects distributed as part of this dataset.\n\nAre there tasks for which the dataset should not be used? If so, please provide a description.\n\nNew tasks must make sure to follow the license of the dataset and the license of the individual objects distributed as part of the dataset.\n\n## Distribution\n\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nYes. The dataset will be made public.\n\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\n31", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Dataset Information", "md": "# Dataset Information"}, {"type": "heading", "lvl": 2, "value": "Uses", "md": "## Uses"}, {"type": "text", "value": "Has the dataset been used for any tasks already? If so, please provide a description.\n\nYes, please see Section 4 of the paper.\n\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\n\nWe recommend checking the Semantic Scholar page for the Objaverse-XL and Objaverse 1.0 papers to find up to date papers that use the dataset.\n\nWhat (other) tasks could the dataset be used for?\n\nThe dataset could be used for a large number of use cases. Examples include making 3D tools more accessible (e.g., 3D impainting, text to 3D, image to 3D), robotic simulation & embodied AI, training video models on animations, 2D vision tasks such as segmentation, and more.\n\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?\n\nUsers should follow the license of the individual objects distributed as part of this dataset.\n\nAre there tasks for which the dataset should not be used? If so, please provide a description.\n\nNew tasks must make sure to follow the license of the dataset and the license of the individual objects distributed as part of the dataset.", "md": "Has the dataset been used for any tasks already? If so, please provide a description.\n\nYes, please see Section 4 of the paper.\n\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\n\nWe recommend checking the Semantic Scholar page for the Objaverse-XL and Objaverse 1.0 papers to find up to date papers that use the dataset.\n\nWhat (other) tasks could the dataset be used for?\n\nThe dataset could be used for a large number of use cases. Examples include making 3D tools more accessible (e.g., 3D impainting, text to 3D, image to 3D), robotic simulation & embodied AI, training video models on animations, 2D vision tasks such as segmentation, and more.\n\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?\n\nUsers should follow the license of the individual objects distributed as part of this dataset.\n\nAre there tasks for which the dataset should not be used? If so, please provide a description.\n\nNew tasks must make sure to follow the license of the dataset and the license of the individual objects distributed as part of the dataset."}, {"type": "heading", "lvl": 2, "value": "Distribution", "md": "## Distribution"}, {"type": "text", "value": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nYes. The dataset will be made public.\n\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\n31", "md": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nYes. The dataset will be made public.\n\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\n31"}]}, {"page": 32, "text": "The dataset will be distributed through a Python API.\n When will the dataset be distributed?\nThe dataset will be made publicly available towards the end of June, 2023.\n Will the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)?                    If so, please describe this license and/or ToU, and\nprovide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU,\nas well as any fees associated with these restrictions.\nThe dataset as a whole will be distributed under the ODC-By 1.0 license. The individual objects are\nsubject to the licenses that they are released under, and users need to assess license questions based\non downstream use.\n Have any third parties imposed IP-based or other restrictions on the data associated with\nthe instances?         If so, please describe these restrictions, and provide a link or other access point\nto, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these\nrestrictions.\nThe individual objects are subject to the licenses that they are released under, and users need to assess\nlicense questions based on downstream use.\n Do any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances?       If so, please describe these restrictions, and provide a link or other access point to, or\notherwise reproduce, any supporting documentation.\nNo.\n Any other comments?\nNo.\nC.7     Maintenance\n Who is supporting/hosting/maintaining the dataset?\nThe dataset will be hosted on Hugging Face.\n How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\nPlease contact     mattd@allenai.org.\n Is there an erratum?          If so, please provide a link or other access point.\nNo.\n  Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete\ninstances)?       If so, please describe how often, by whom, and how updates will be communicated to\nusers (e.g., mailing list, GitHub)?\nThe dataset is currently self contained without immediate plans for updates.\n  If the dataset relates to people, are there applicable limits on the retention of the data\nassociated with the instances (e.g., were individuals in question told that their data would be\nretained for a fixed period of time and then deleted)?                        If so, please describe these limits and\nexplain how they will be enforced.\nPeople may contact us to add specific samples to a blacklist.\n Will older versions of the dataset continue to be supported/hosted/maintained?                                 If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users.\n                                                               32", "md": "# Dataset Distribution Information\n\n## Dataset Distribution Information\n\nThe dataset will be distributed through a Python API.\n\n### When will the dataset be distributed?\n\nThe dataset will be made publicly available towards the end of June, 2023.\n\n### Copyright and Intellectual Property (IP) License\n\nThe dataset as a whole will be distributed under the ODC-By 1.0 license. The individual objects are subject to the licenses that they are released under, and users need to assess license questions based on downstream use.\n\n### Third-Party Restrictions\n\nThe individual objects are subject to the licenses that they are released under, and users need to assess license questions based on downstream use.\n\n### Export Controls and Regulatory Restrictions\n\nNo export controls or other regulatory restrictions apply to the dataset or to individual instances.\n\n### Maintenance\n\nHosted by: Hugging Face\n\nContact: mattd@allenai.org\n\n### Updates\n\nThe dataset is currently self-contained without immediate plans for updates.\n\n### Data Retention Limits\n\nPeople may contact us to add specific samples to a blacklist.\n\n### Older Versions\n\nThere are no plans to support/host/maintain older versions of the dataset.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Dataset Distribution Information", "md": "# Dataset Distribution Information"}, {"type": "heading", "lvl": 2, "value": "Dataset Distribution Information", "md": "## Dataset Distribution Information"}, {"type": "text", "value": "The dataset will be distributed through a Python API.", "md": "The dataset will be distributed through a Python API."}, {"type": "heading", "lvl": 3, "value": "When will the dataset be distributed?", "md": "### When will the dataset be distributed?"}, {"type": "text", "value": "The dataset will be made publicly available towards the end of June, 2023.", "md": "The dataset will be made publicly available towards the end of June, 2023."}, {"type": "heading", "lvl": 3, "value": "Copyright and Intellectual Property (IP) License", "md": "### Copyright and Intellectual Property (IP) License"}, {"type": "text", "value": "The dataset as a whole will be distributed under the ODC-By 1.0 license. The individual objects are subject to the licenses that they are released under, and users need to assess license questions based on downstream use.", "md": "The dataset as a whole will be distributed under the ODC-By 1.0 license. The individual objects are subject to the licenses that they are released under, and users need to assess license questions based on downstream use."}, {"type": "heading", "lvl": 3, "value": "Third-Party Restrictions", "md": "### Third-Party Restrictions"}, {"type": "text", "value": "The individual objects are subject to the licenses that they are released under, and users need to assess license questions based on downstream use.", "md": "The individual objects are subject to the licenses that they are released under, and users need to assess license questions based on downstream use."}, {"type": "heading", "lvl": 3, "value": "Export Controls and Regulatory Restrictions", "md": "### Export Controls and Regulatory Restrictions"}, {"type": "text", "value": "No export controls or other regulatory restrictions apply to the dataset or to individual instances.", "md": "No export controls or other regulatory restrictions apply to the dataset or to individual instances."}, {"type": "heading", "lvl": 3, "value": "Maintenance", "md": "### Maintenance"}, {"type": "text", "value": "Hosted by: Hugging Face\n\nContact: mattd@allenai.org", "md": "Hosted by: Hugging Face\n\nContact: mattd@allenai.org"}, {"type": "heading", "lvl": 3, "value": "Updates", "md": "### Updates"}, {"type": "text", "value": "The dataset is currently self-contained without immediate plans for updates.", "md": "The dataset is currently self-contained without immediate plans for updates."}, {"type": "heading", "lvl": 3, "value": "Data Retention Limits", "md": "### Data Retention Limits"}, {"type": "text", "value": "People may contact us to add specific samples to a blacklist.", "md": "People may contact us to add specific samples to a blacklist."}, {"type": "heading", "lvl": 3, "value": "Older Versions", "md": "### Older Versions"}, {"type": "text", "value": "There are no plans to support/host/maintain older versions of the dataset.", "md": "There are no plans to support/host/maintain older versions of the dataset."}]}, {"page": 33, "text": " Objaverse 1.0 will continue to be supported.\n If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\n them to do so?      If so, please provide a description. Will these contributions be validated/verified? If\n so, please describe how. If not, why not? Is there a process for communicating/distributing these\n contributions to other users? If so, please provide a description.\nWe encourage others to build and extend the dataset for different use cases and may highlight some\n of those use cases if applicable.\n Any other comments?\n No\n D     Aesthetic Annotations\nWe run LAION-Aesthetics V2 [55] on renders of the objects, which can be used for filtering a\n higher quality subset of the objects. We group the objects into 3 tiers, which are depicted in Table 4.\n Figure 19 shows examples of renders of objects placed on the different tiers.\n  Category                    Description                   Aesthetic Score Cutoff          Percentage of Dataset\n       T1         Highest aesthetic ranked objects               Greater than 4.5                      14.2%\n       T2         Medium aesthetic ranked objects               Between 4 and 4.5                      69.2%\n       T3           Low aesthetic ranked objects                    Less than 4                        16.6%\n          Table 4: LAION-Aesthetics V2 categorization for renders of Objaverse-XL objects.\n   TI Aesthetic Tier\n   T2 Aesthetic Tier\n   T3 Aesthetic Tier\n                                                                                                  WWAUEARER\n Figure 19: Random samples of renders showing LAION-Aesthetic V2 annotations across different\n tiers. Empirically, T1 tends to have the highest quality objects, followed by T2 and then T3.\n                                                          33", "md": "Objaverse 1.0 will continue to be supported.\n\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.\n\nWe encourage others to build and extend the dataset for different use cases and may highlight some of those use cases if applicable.\n\nAny other comments?\n\nNo\n\n## Aesthetic Annotations\n\nWe run LAION-Aesthetics V2 [55] on renders of the objects, which can be used for filtering a higher quality subset of the objects. We group the objects into 3 tiers, which are depicted in Table 4.\n\nFigure 19 shows examples of renders of objects placed on the different tiers.\n\n| Category | Description | Aesthetic Score Cutoff | Percentage of Dataset |\n| --- | --- | --- | --- |\n| T1 | Highest aesthetic ranked objects | Greater than 4.5 | 14.2% |\n| T2 | Medium aesthetic ranked objects | Between 4 and 4.5 | 69.2% |\n| T3 | Low aesthetic ranked objects | Less than 4 | 16.6% |\n\nTable 4: LAION-Aesthetics V2 categorization for renders of Objaverse-XL objects.\n\nTI Aesthetic Tier\n\nT2 Aesthetic Tier\n\nT3 Aesthetic Tier\n\nFigure 19: Random samples of renders showing LAION-Aesthetic V2 annotations across different tiers. Empirically, T1 tends to have the highest quality objects, followed by T2 and then T3.", "images": [{"name": "img_p32_1", "height": 791, "width": 1188}], "items": [{"type": "text", "value": "Objaverse 1.0 will continue to be supported.\n\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.\n\nWe encourage others to build and extend the dataset for different use cases and may highlight some of those use cases if applicable.\n\nAny other comments?\n\nNo", "md": "Objaverse 1.0 will continue to be supported.\n\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.\n\nWe encourage others to build and extend the dataset for different use cases and may highlight some of those use cases if applicable.\n\nAny other comments?\n\nNo"}, {"type": "heading", "lvl": 2, "value": "Aesthetic Annotations", "md": "## Aesthetic Annotations"}, {"type": "text", "value": "We run LAION-Aesthetics V2 [55] on renders of the objects, which can be used for filtering a higher quality subset of the objects. We group the objects into 3 tiers, which are depicted in Table 4.\n\nFigure 19 shows examples of renders of objects placed on the different tiers.", "md": "We run LAION-Aesthetics V2 [55] on renders of the objects, which can be used for filtering a higher quality subset of the objects. We group the objects into 3 tiers, which are depicted in Table 4.\n\nFigure 19 shows examples of renders of objects placed on the different tiers."}, {"type": "table", "rows": [["Category", "Description", "Aesthetic Score Cutoff", "Percentage of Dataset"], ["T1", "Highest aesthetic ranked objects", "Greater than 4.5", "14.2%"], ["T2", "Medium aesthetic ranked objects", "Between 4 and 4.5", "69.2%"], ["T3", "Low aesthetic ranked objects", "Less than 4", "16.6%"]], "md": "| Category | Description | Aesthetic Score Cutoff | Percentage of Dataset |\n| --- | --- | --- | --- |\n| T1 | Highest aesthetic ranked objects | Greater than 4.5 | 14.2% |\n| T2 | Medium aesthetic ranked objects | Between 4 and 4.5 | 69.2% |\n| T3 | Low aesthetic ranked objects | Less than 4 | 16.6% |", "isPerfectTable": true, "csv": "\"Category\",\"Description\",\"Aesthetic Score Cutoff\",\"Percentage of Dataset\"\n\"T1\",\"Highest aesthetic ranked objects\",\"Greater than 4.5\",\"14.2%\"\n\"T2\",\"Medium aesthetic ranked objects\",\"Between 4 and 4.5\",\"69.2%\"\n\"T3\",\"Low aesthetic ranked objects\",\"Less than 4\",\"16.6%\""}, {"type": "text", "value": "Table 4: LAION-Aesthetics V2 categorization for renders of Objaverse-XL objects.\n\nTI Aesthetic Tier\n\nT2 Aesthetic Tier\n\nT3 Aesthetic Tier\n\nFigure 19: Random samples of renders showing LAION-Aesthetic V2 annotations across different tiers. Empirically, T1 tends to have the highest quality objects, followed by T2 and then T3.", "md": "Table 4: LAION-Aesthetics V2 categorization for renders of Objaverse-XL objects.\n\nTI Aesthetic Tier\n\nT2 Aesthetic Tier\n\nT3 Aesthetic Tier\n\nFigure 19: Random samples of renders showing LAION-Aesthetic V2 annotations across different tiers. Empirically, T1 tends to have the highest quality objects, followed by T2 and then T3."}]}], "job_id": "17ab64f6-d6a7-46d7-ae4c-0074072d5d19", "file_path": "./corpus/2307.05663.pdf"}