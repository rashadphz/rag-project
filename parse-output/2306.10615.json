{"pages": [{"page": 1, "text": "            Agnostically Learning Single-Index Models using Omnipredictors\n                         Aravind Gollakota\u2217             Parikshit Gopalan\u2020            Adam R. Klivans\u2021\n                              UT Austin                        Apple                      UT Austin\narXiv:2306.10615v1  [cs.LG]  18 Jun 2023          Konstantinos Stavropoulos\u00a7\n                                                           UT Austin\n                                                          June 21, 2023\n                                                             Abstract\n                    We give the fi rst result for agnostically learning Single-Index Models (SIMs) with arbitrary\n                monotone and Lipschitz activations. All prior work either held only in the realizable setting or\n                required the activation to be known. Moreover, we only require the marginal to have bounded\n                second moments, whereas all prior work required stronger distributional assumptions (such as\n                anticoncentration or boundedness). Our algorithm is based on recent work by [GHK+23] on\n                omniprediction using predictors satisfying calibrated multiaccuracy. Our analysis is simple and\n                relies on the relationship between Bregman divergences (or matching losses) and \u2113p distances.\n                We also provide new guarantees for standard algorithms like GLMtron and logistic regression\n                in the agnostic setting.\n             \u2217\n              aravindg@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\n          Machine Learning (IFML).\n             \u2020\n              parikg@apple.com.\n             \u2021klivans@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\n          Machine Learning (IFML).\n             \u00a7kstavrop@cs.utexas.edu.    Supported by NSF award AF-1909204, the NSF AI Institute for Foundations of\n          Machine Learning (IFML), and by scholarships from Bodossaki Foundation and Leventis Foundation.\n                                                                 1", "md": "# Agnostically Learning Single-Index Models using Omnipredictors\n\n# Agnostically Learning Single-Index Models using Omnipredictors\n\nAravind Gollakota* UT Austin\n\nParikshit Gopalan\u2020 Apple\n\nAdam R. Klivans\u2021 UT Austin\n\nKonstantinos Stavropoulos\u00a7 UT Austin\n\nJune 21, 2023\n\n## Abstract\n\nWe give the first result for agnostically learning Single-Index Models (SIMs) with arbitrary\nmonotone and Lipschitz activations. All prior work either held only in the realizable setting or\nrequired the activation to be known. Moreover, we only require the marginal to have bounded\nsecond moments, whereas all prior work required stronger distributional assumptions (such as\nanticoncentration or boundedness). Our algorithm is based on recent work by [GHK+23] on\nomniprediction using predictors satisfying calibrated multiaccuracy. Our analysis is simple and\nrelies on the relationship between Bregman divergences (or matching losses) and \u2113p distances.\nWe also provide new guarantees for standard algorithms like GLMtron and logistic regression\nin the agnostic setting.\n\n* aravindg@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2020 parikg@apple.com.\n\n\u2021 klivans@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u00a7 kstavrop@cs.utexas.edu. Supported by NSF award AF-1909204, the NSF AI Institute for Foundations of\nMachine Learning (IFML), and by scholarships from Bodossaki Foundation and Leventis Foundation.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Agnostically Learning Single-Index Models using Omnipredictors", "md": "# Agnostically Learning Single-Index Models using Omnipredictors"}, {"type": "heading", "lvl": 1, "value": "Agnostically Learning Single-Index Models using Omnipredictors", "md": "# Agnostically Learning Single-Index Models using Omnipredictors"}, {"type": "text", "value": "Aravind Gollakota* UT Austin\n\nParikshit Gopalan\u2020 Apple\n\nAdam R. Klivans\u2021 UT Austin\n\nKonstantinos Stavropoulos\u00a7 UT Austin\n\nJune 21, 2023", "md": "Aravind Gollakota* UT Austin\n\nParikshit Gopalan\u2020 Apple\n\nAdam R. Klivans\u2021 UT Austin\n\nKonstantinos Stavropoulos\u00a7 UT Austin\n\nJune 21, 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "We give the first result for agnostically learning Single-Index Models (SIMs) with arbitrary\nmonotone and Lipschitz activations. All prior work either held only in the realizable setting or\nrequired the activation to be known. Moreover, we only require the marginal to have bounded\nsecond moments, whereas all prior work required stronger distributional assumptions (such as\nanticoncentration or boundedness). Our algorithm is based on recent work by [GHK+23] on\nomniprediction using predictors satisfying calibrated multiaccuracy. Our analysis is simple and\nrelies on the relationship between Bregman divergences (or matching losses) and \u2113p distances.\nWe also provide new guarantees for standard algorithms like GLMtron and logistic regression\nin the agnostic setting.\n\n* aravindg@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2020 parikg@apple.com.\n\n\u2021 klivans@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u00a7 kstavrop@cs.utexas.edu. Supported by NSF award AF-1909204, the NSF AI Institute for Foundations of\nMachine Learning (IFML), and by scholarships from Bodossaki Foundation and Leventis Foundation.", "md": "We give the first result for agnostically learning Single-Index Models (SIMs) with arbitrary\nmonotone and Lipschitz activations. All prior work either held only in the realizable setting or\nrequired the activation to be known. Moreover, we only require the marginal to have bounded\nsecond moments, whereas all prior work required stronger distributional assumptions (such as\nanticoncentration or boundedness). Our algorithm is based on recent work by [GHK+23] on\nomniprediction using predictors satisfying calibrated multiaccuracy. Our analysis is simple and\nrelies on the relationship between Bregman divergences (or matching losses) and \u2113p distances.\nWe also provide new guarantees for standard algorithms like GLMtron and logistic regression\nin the agnostic setting.\n\n* aravindg@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2020 parikg@apple.com.\n\n\u2021 klivans@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u00a7 kstavrop@cs.utexas.edu. Supported by NSF award AF-1909204, the NSF AI Institute for Foundations of\nMachine Learning (IFML), and by scholarships from Bodossaki Foundation and Leventis Foundation."}]}, {"page": 2, "text": "1    Introduction\nGeneralized Linear Models (GLMs) and Single-Index Models (SIMs) constitute fundamental frame-\nworks in supervised learning [McC84, Agr15], capturing and generalizing basic models such as lin-\near and logistic regression. In the GLM framework, labeled examples (x, y) are assumed to satisfy\nE[y|x] = u\u22121(w \u00b7 x), where u is a known monotone function (called the link function) and w is\nan unknown vector. Single-Index Models (SIMs) are defined similarly, but for the case when the\nmonotone link function u is also unknown.\n   In the realizable setting where the labels are indeed generated according to a GLM with a given\nLipschitz link function, the GLMtron algorithm of [KKSK11] is a simple and efficient learning\nalgorithm. When the ground truth is only assumed to be a SIM (and, hence, the link function is\nunknown), it can be learned efficiently by the Isotron algorithm [KS09, KKSK11].\n   In this work, we consider the significantly more challenging agnostic setting, where the labels\nare arbitrary and not necessarily realizable. Moreover, we do not assume that we know the optimal\nactivation; our goal is to output a predictor that has error comparable to that of the optimal SIM,\nwhatever its activation may be. That is, we must be competitive against not only all possible\nweights but also all possible monotone and Lipschitz link functions that might fit the distribution.\nConcretely, consider a distribution D over Rd \u00d7 [0, 1] and denote the squared error of a function\nh : Rd \u2192   R by err2(h) = E(x,y)\u223cD[(y \u2212     h(x))2]. Let opt(SIM) denote optimal value of err2(h)\nover all SIMs h with bounded weights and arbitrary 1-Lipschitz monotone activations (we call the\ninverse u\u22121 of a link function u the activation function). Given access to samples from D, our goal\nis to compute a predictor p : Rd \u2192   [0, 1] with error err2(p) comparable to the error of the optimal\nSIM:\n                                       err2(p) \u2264 opt(SIM) + \u01eb.\n   Our main result is the first efficient learning algorithm for this problem.\nTheorem 1.1 (Informal, see Theorem 3.1). Let SIMB denote the class of SIMs of the form x  \u2192\nu\u22121(w \u00b7 x) for some 1-Lipschitz function u\u22121 and \u2225w\u22252 \u2264          B.   Let D be any distribution over\nRd \u00d7[0, 1] whose marginal on Rd has bounded second moments. There is an effi       cient algorithm that\nagnostically learns SIMB over D up to error\n                                  err2(p) \u2264 O  B    opt(SIMB)    + \u01eb.\n   This result provides a guarantee comparable to that of the Isotron algorithm [KS09, KKSK11]\nbut for the challenging agnostic setting rather than the realizable setting (where opt(SIMB, D) = 0).\nMoreover, Isotron\u2019s guarantees require the distribution to be supported on the unit ball, whereas\nwe only require a mild second moment condition.\n   Minimizing the squared error in the agnostic setting is a standard benchmark in learning theory,\nbut it is often useful to simplify the problem by considering an alternative error function, tailored to\nthe specific problem at hand. In this sense, we can still find the GLM that is closest to the ground\ntruth even in the agnostic setting; the key is to define closeness using an appropriate Bregman\ndivergence, which depends on the link function through Fenchel-Legendre convex duality.\n   The problem of finding the closest model in Bregman divergence amounts to a convex pro-\ngram where we minimize a certain loss called the matching loss [AHW95]. In fact, recent work by\n[GKR+22, GHK+23] studying the notion of omniprediction has demonstrated that there exist effi-\ncient algorithms that minimize all of the matching losses (corresponding to monotone and bounded\nlinks) simultaneously. Their solution concept is called an omnipredictor, i.e., a single predictor\nthat is able to compete with the best-fitting classifier in a class C as measured by a large class\n                                                   2", "md": "# Document\n\n# Introduction\n\nGeneralized Linear Models (GLMs) and Single-Index Models (SIMs) constitute fundamental frameworks in supervised learning [McC84, Agr15], capturing and generalizing basic models such as linear and logistic regression. In the GLM framework, labeled examples (x, y) are assumed to satisfy $$E[y|x] = u^{-1}(w \\cdot x)$$, where u is a known monotone function (called the link function) and w is an unknown vector. Single-Index Models (SIMs) are defined similarly, but for the case when the monotone link function u is also unknown.\n\nIn the realizable setting where the labels are indeed generated according to a GLM with a given Lipschitz link function, the GLMtron algorithm of [KKSK11] is a simple and efficient learning algorithm. When the ground truth is only assumed to be a SIM (and, hence, the link function is unknown), it can be learned efficiently by the Isotron algorithm [KS09, KKSK11].\n\nIn this work, we consider the significantly more challenging agnostic setting, where the labels are arbitrary and not necessarily realizable. Moreover, we do not assume that we know the optimal activation; our goal is to output a predictor that has error comparable to that of the optimal SIM, whatever its activation may be. That is, we must be competitive against not only all possible weights but also all possible monotone and Lipschitz link functions that might fit the distribution.\n\nConcretely, consider a distribution D over $$\\mathbb{R}^d \\times [0, 1]$$ and denote the squared error of a function $$h : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ by $$err^2(h) = E(x,y) \\sim D[(y - h(x))^2]$$. Let opt(SIM) denote the optimal value of $$err^2(h)$$ over all SIMs h with bounded weights and arbitrary 1-Lipschitz monotone activations (we call the inverse $$u^{-1}$$ of a link function u the activation function). Given access to samples from D, our goal is to compute a predictor $$p : \\mathbb{R}^d \\rightarrow [0, 1]$$ with error $$err^2(p)$$ comparable to the error of the optimal SIM:\n\n$$err^2(p) \\leq opt(SIM) + \\epsilon$$.\n\nOur main result is the first efficient learning algorithm for this problem.\n\n## Theorem 1.1 (Informal, see Theorem 3.1)\n\nLet SIMB denote the class of SIMs of the form $$x \\rightarrow u^{-1}(w \\cdot x)$$ for some 1-Lipschitz function $$u^{-1}$$ and $$\\|w\\|_2 \\leq B$$. Let D be any distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has bounded second moments. There is an efficient algorithm that agnostically learns SIMB over D up to error\n\n$$err^2(p) \\leq O(B \\cdot opt(SIMB) + \\epsilon)$$.\n\nThis result provides a guarantee comparable to that of the Isotron algorithm [KS09, KKSK11] but for the challenging agnostic setting rather than the realizable setting (where opt(SIMB, D) = 0). Moreover, Isotron\u2019s guarantees require the distribution to be supported on the unit ball, whereas we only require a mild second moment condition.\n\nMinimizing the squared error in the agnostic setting is a standard benchmark in learning theory, but it is often useful to simplify the problem by considering an alternative error function, tailored to the specific problem at hand. In this sense, we can still find the GLM that is closest to the ground truth even in the agnostic setting; the key is to define closeness using an appropriate Bregman divergence, which depends on the link function through Fenchel-Legendre convex duality.\n\nThe problem of finding the closest model in Bregman divergence amounts to a convex program where we minimize a certain loss called the matching loss [AHW95]. In fact, recent work by [GKR+22, GHK+23] studying the notion of omniprediction has demonstrated that there exist efficient algorithms that minimize all of the matching losses (corresponding to monotone and bounded links) simultaneously. Their solution concept is called an omnipredictor, i.e., a single predictor that is able to compete with the best-fitting classifier in a class C as measured by a large class.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 1, "value": "Introduction", "md": "# Introduction"}, {"type": "text", "value": "Generalized Linear Models (GLMs) and Single-Index Models (SIMs) constitute fundamental frameworks in supervised learning [McC84, Agr15], capturing and generalizing basic models such as linear and logistic regression. In the GLM framework, labeled examples (x, y) are assumed to satisfy $$E[y|x] = u^{-1}(w \\cdot x)$$, where u is a known monotone function (called the link function) and w is an unknown vector. Single-Index Models (SIMs) are defined similarly, but for the case when the monotone link function u is also unknown.\n\nIn the realizable setting where the labels are indeed generated according to a GLM with a given Lipschitz link function, the GLMtron algorithm of [KKSK11] is a simple and efficient learning algorithm. When the ground truth is only assumed to be a SIM (and, hence, the link function is unknown), it can be learned efficiently by the Isotron algorithm [KS09, KKSK11].\n\nIn this work, we consider the significantly more challenging agnostic setting, where the labels are arbitrary and not necessarily realizable. Moreover, we do not assume that we know the optimal activation; our goal is to output a predictor that has error comparable to that of the optimal SIM, whatever its activation may be. That is, we must be competitive against not only all possible weights but also all possible monotone and Lipschitz link functions that might fit the distribution.\n\nConcretely, consider a distribution D over $$\\mathbb{R}^d \\times [0, 1]$$ and denote the squared error of a function $$h : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ by $$err^2(h) = E(x,y) \\sim D[(y - h(x))^2]$$. Let opt(SIM) denote the optimal value of $$err^2(h)$$ over all SIMs h with bounded weights and arbitrary 1-Lipschitz monotone activations (we call the inverse $$u^{-1}$$ of a link function u the activation function). Given access to samples from D, our goal is to compute a predictor $$p : \\mathbb{R}^d \\rightarrow [0, 1]$$ with error $$err^2(p)$$ comparable to the error of the optimal SIM:\n\n$$err^2(p) \\leq opt(SIM) + \\epsilon$$.\n\nOur main result is the first efficient learning algorithm for this problem.", "md": "Generalized Linear Models (GLMs) and Single-Index Models (SIMs) constitute fundamental frameworks in supervised learning [McC84, Agr15], capturing and generalizing basic models such as linear and logistic regression. In the GLM framework, labeled examples (x, y) are assumed to satisfy $$E[y|x] = u^{-1}(w \\cdot x)$$, where u is a known monotone function (called the link function) and w is an unknown vector. Single-Index Models (SIMs) are defined similarly, but for the case when the monotone link function u is also unknown.\n\nIn the realizable setting where the labels are indeed generated according to a GLM with a given Lipschitz link function, the GLMtron algorithm of [KKSK11] is a simple and efficient learning algorithm. When the ground truth is only assumed to be a SIM (and, hence, the link function is unknown), it can be learned efficiently by the Isotron algorithm [KS09, KKSK11].\n\nIn this work, we consider the significantly more challenging agnostic setting, where the labels are arbitrary and not necessarily realizable. Moreover, we do not assume that we know the optimal activation; our goal is to output a predictor that has error comparable to that of the optimal SIM, whatever its activation may be. That is, we must be competitive against not only all possible weights but also all possible monotone and Lipschitz link functions that might fit the distribution.\n\nConcretely, consider a distribution D over $$\\mathbb{R}^d \\times [0, 1]$$ and denote the squared error of a function $$h : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ by $$err^2(h) = E(x,y) \\sim D[(y - h(x))^2]$$. Let opt(SIM) denote the optimal value of $$err^2(h)$$ over all SIMs h with bounded weights and arbitrary 1-Lipschitz monotone activations (we call the inverse $$u^{-1}$$ of a link function u the activation function). Given access to samples from D, our goal is to compute a predictor $$p : \\mathbb{R}^d \\rightarrow [0, 1]$$ with error $$err^2(p)$$ comparable to the error of the optimal SIM:\n\n$$err^2(p) \\leq opt(SIM) + \\epsilon$$.\n\nOur main result is the first efficient learning algorithm for this problem."}, {"type": "heading", "lvl": 2, "value": "Theorem 1.1 (Informal, see Theorem 3.1)", "md": "## Theorem 1.1 (Informal, see Theorem 3.1)"}, {"type": "text", "value": "Let SIMB denote the class of SIMs of the form $$x \\rightarrow u^{-1}(w \\cdot x)$$ for some 1-Lipschitz function $$u^{-1}$$ and $$\\|w\\|_2 \\leq B$$. Let D be any distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has bounded second moments. There is an efficient algorithm that agnostically learns SIMB over D up to error\n\n$$err^2(p) \\leq O(B \\cdot opt(SIMB) + \\epsilon)$$.\n\nThis result provides a guarantee comparable to that of the Isotron algorithm [KS09, KKSK11] but for the challenging agnostic setting rather than the realizable setting (where opt(SIMB, D) = 0). Moreover, Isotron\u2019s guarantees require the distribution to be supported on the unit ball, whereas we only require a mild second moment condition.\n\nMinimizing the squared error in the agnostic setting is a standard benchmark in learning theory, but it is often useful to simplify the problem by considering an alternative error function, tailored to the specific problem at hand. In this sense, we can still find the GLM that is closest to the ground truth even in the agnostic setting; the key is to define closeness using an appropriate Bregman divergence, which depends on the link function through Fenchel-Legendre convex duality.\n\nThe problem of finding the closest model in Bregman divergence amounts to a convex program where we minimize a certain loss called the matching loss [AHW95]. In fact, recent work by [GKR+22, GHK+23] studying the notion of omniprediction has demonstrated that there exist efficient algorithms that minimize all of the matching losses (corresponding to monotone and bounded links) simultaneously. Their solution concept is called an omnipredictor, i.e., a single predictor that is able to compete with the best-fitting classifier in a class C as measured by a large class.", "md": "Let SIMB denote the class of SIMs of the form $$x \\rightarrow u^{-1}(w \\cdot x)$$ for some 1-Lipschitz function $$u^{-1}$$ and $$\\|w\\|_2 \\leq B$$. Let D be any distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has bounded second moments. There is an efficient algorithm that agnostically learns SIMB over D up to error\n\n$$err^2(p) \\leq O(B \\cdot opt(SIMB) + \\epsilon)$$.\n\nThis result provides a guarantee comparable to that of the Isotron algorithm [KS09, KKSK11] but for the challenging agnostic setting rather than the realizable setting (where opt(SIMB, D) = 0). Moreover, Isotron\u2019s guarantees require the distribution to be supported on the unit ball, whereas we only require a mild second moment condition.\n\nMinimizing the squared error in the agnostic setting is a standard benchmark in learning theory, but it is often useful to simplify the problem by considering an alternative error function, tailored to the specific problem at hand. In this sense, we can still find the GLM that is closest to the ground truth even in the agnostic setting; the key is to define closeness using an appropriate Bregman divergence, which depends on the link function through Fenchel-Legendre convex duality.\n\nThe problem of finding the closest model in Bregman divergence amounts to a convex program where we minimize a certain loss called the matching loss [AHW95]. In fact, recent work by [GKR+22, GHK+23] studying the notion of omniprediction has demonstrated that there exist efficient algorithms that minimize all of the matching losses (corresponding to monotone and bounded links) simultaneously. Their solution concept is called an omnipredictor, i.e., a single predictor that is able to compete with the best-fitting classifier in a class C as measured by a large class."}]}, {"page": 3, "text": "of losses (as opposed to just a single pre-specified loss, as is standard in machine learning). They\nobtain such predictors through calibrated multiaccuracy [GHK+23] or multicalibration [GKR+22].\nTheir results apply to the non-realizable setting and do not assume prior knowledge of the link\nfunction, but only provide guarantees for (simultaneous) matching loss minimization, rather than\nthe standard squared error minimization.\n      We propose a simple analytic approach to transforming matching loss guarantees over the\nclass of linear functions to squared error guarantees over the class of GLMs with link function\nthat corresponds to the matching loss at hand.                                  Our generic transformation, coupled with the\nomniprediction results from [GHK+23], yields our main result on agnostically learning SIMs. We\nthus obtain a best of all worlds statement: we do not need to know the link function, but we\ncan always compete with the best SIM model in terms of the squared loss. At the heart of our\napproach are distortion inequalities relating matching losses to \u2113p losses that we believe may be of\nindependent interest.\n      We first prove strong versions of such inequalities for matching losses arising from bi-Lipschitz\nlink functions, and obtain our results for general Lipschitz activations by carefully approximating\nthem using bi-Lipschitz activations. In particular, if we let opt(GLM                                    u\u22121  ,B) denote the optimal value\nof err2(h) over all GLMs of the form x  \u2192                           u\u22121(w \u00b7 x), where \u2225w\u22252 \u2264                   B, we obtain the following\nresult about bi-Lipschitz activations (including, for example, the Leaky ReLU activation).\nTheorem 1.2 (Informal, see Theorem 2.1). Let u : R \u2192                                    R be a bi-Lipschitz invertible link function.\nThen, any predictor p : Rd \u2192                  R that is an \u01eb-approximate minimizer of the population matching loss\nthat corresponds to u, with respect to a distribution D over Rd \u00d7 [0, 1] satisfi                                        es\n                                                err2(p) \u2264      O    opt(GLMu\u22121,B)              + O(\u01eb)\n      This guarantee holds under milder distributional assumptions than are required by comparable\nprior work on agnostically learning GLMs [FCG20, DKTZ22]. Moreover, when we focus on distor-\ntion bounds between the logistic loss and the squared loss, we obtain a near-optimal guarantee of\nO(opt(GLM          u\u22121  ,B)) for logistic regression, when u is the logit link function (i.e., when GLM                                     u\u22121  ,B\nis the class of sigmoid neurons).\n                                                                                               t\nTheorem 1.3 (Informal, see Theorem 4.1). Let u(t) = ln(                                       1\u2212t). Then, any predictor p : Rd \u2192                  R\nthat is an \u01eb-approximate minimizer of the population logistic loss, with respect to a distribution D\nover Rd \u00d7 [0, 1] whose marginal on Rd has subgaussian tails in every direction satisfi                                              es\n                                                err2(p) \u2264      O    opt(GLM        u\u22121  ,B)    + O(\u01eb)\n      While our error guarantee is weaker the one of [DKTZ22], we do not make the anti-concentration\nassumptions their results require. A natural question is to ask if our guarantees are near-optimal,\ne.g., whether we can obtain a guarantee of the form err2(p) \u2264                                         opt(SIM) + \u01eb. However, there is\nstrong evidence that such results cannot be obtained using efficient algorithms [GKK19, DKZ20,\nGGK20, DKPZ21]. Adapting a result due to [DKMR22], we show in Section 5 that one cannot\navoid a dependence on the norm bound B in our main result, Theorem 1.1.\n1.1       Background and Relation to Prior Work\nWe note that matching losses have been studied in various previous works either implicitly [KKSK11]\nor explicitly [AHW95, DGK+20, GHK+23] and capture various fundamental algorithms like logis-\ntic and linear regression [McC84, Agr15]. However, to the best of our knowledge, our generic and\ndirect approach for transforming matching loss guarantees to squared error bounds, has not been\n                                                                            3", "md": "of losses (as opposed to just a single pre-specified loss, as is standard in machine learning). They\nobtain such predictors through calibrated multiaccuracy [GHK+23] or multicalibration [GKR+22].\nTheir results apply to the non-realizable setting and do not assume prior knowledge of the link\nfunction, but only provide guarantees for (simultaneous) matching loss minimization, rather than\nthe standard squared error minimization.\n\nWe propose a simple analytic approach to transforming matching loss guarantees over the\nclass of linear functions to squared error guarantees over the class of GLMs with link function\nthat corresponds to the matching loss at hand. Our generic transformation, coupled with the\nomniprediction results from [GHK+23], yields our main result on agnostically learning SIMs. We\nthus obtain a best of all worlds statement: we do not need to know the link function, but we\ncan always compete with the best SIM model in terms of the squared loss. At the heart of our\napproach are distortion inequalities relating matching losses to \u2113p losses that we believe may be of\nindependent interest.\n\nWe first prove strong versions of such inequalities for matching losses arising from bi-Lipschitz\nlink functions, and obtain our results for general Lipschitz activations by carefully approximating\nthem using bi-Lipschitz activations. In particular, if we let $$\\text{opt(GLM}_{u^{-1},B})$$ denote the optimal value\nof $$\\text{err}^2(h)$$ over all GLMs of the form $$x \\rightarrow u^{-1}(w \\cdot x)$$, where $$\\|w\\|_2 \\leq B$$, we obtain the following\nresult about bi-Lipschitz activations (including, for example, the Leaky ReLU activation).\n\nTheorem 1.2 (Informal, see Theorem 2.1). Let $$u : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be a bi-Lipschitz invertible link function.\nThen, any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ that is an $$\\epsilon$$-approximate minimizer of the population matching loss\nthat corresponds to u, with respect to a distribution $$D$$ over $$\\mathbb{R}^d \\times [0, 1]$$ satisfies\n\n$$\\text{err}^2(p) \\leq O\\left(\\text{opt(GLM}_{u^{-1},B}\\right) + O(\\epsilon)$$\n\nThis guarantee holds under milder distributional assumptions than are required by comparable\nprior work on agnostically learning GLMs [FCG20, DKTZ22]. Moreover, when we focus on distortion bounds between the logistic loss and the squared loss, we obtain a near-optimal guarantee of\n$$O\\left(\\text{opt(GLM}_{u^{-1},B}\\right)$$ for logistic regression, when u is the logit link function (i.e., when $$\\text{GLM}_{u^{-1},B}$$\nis the class of sigmoid neurons).\n\nTheorem 1.3 (Informal, see Theorem 4.1). Let $$u(t) = \\ln(1-t)$$. Then, any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$\nthat is an $$\\epsilon$$-approximate minimizer of the population logistic loss, with respect to a distribution $$D$$\nover $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has subgaussian tails in every direction satisfies\n\n$$\\text{err}^2(p) \\leq O\\left(\\text{opt(GLM}_{u^{-1},B}\\right) + O(\\epsilon)$$\n\nWhile our error guarantee is weaker the one of [DKTZ22], we do not make the anti-concentration\nassumptions their results require. A natural question is to ask if our guarantees are near-optimal,\ne.g., whether we can obtain a guarantee of the form $$\\text{err}^2(p) \\leq \\text{opt(SIM)} + \\epsilon$$. However, there is\nstrong evidence that such results cannot be obtained using efficient algorithms [GKK19, DKZ20, GGK20, DKPZ21]. Adapting a result due to [DKMR22], we show in Section 5 that one cannot\navoid a dependence on the norm bound $$B$$ in our main result, Theorem 1.1.\n\n## Background and Relation to Prior Work\n\nWe note that matching losses have been studied in various previous works either implicitly [KKSK11]\nor explicitly [AHW95, DGK+20, GHK+23] and capture various fundamental algorithms like logistic and linear regression [McC84, Agr15]. However, to the best of our knowledge, our generic and\ndirect approach for transforming matching loss guarantees to squared error bounds, has not been", "images": [], "items": [{"type": "text", "value": "of losses (as opposed to just a single pre-specified loss, as is standard in machine learning). They\nobtain such predictors through calibrated multiaccuracy [GHK+23] or multicalibration [GKR+22].\nTheir results apply to the non-realizable setting and do not assume prior knowledge of the link\nfunction, but only provide guarantees for (simultaneous) matching loss minimization, rather than\nthe standard squared error minimization.\n\nWe propose a simple analytic approach to transforming matching loss guarantees over the\nclass of linear functions to squared error guarantees over the class of GLMs with link function\nthat corresponds to the matching loss at hand. Our generic transformation, coupled with the\nomniprediction results from [GHK+23], yields our main result on agnostically learning SIMs. We\nthus obtain a best of all worlds statement: we do not need to know the link function, but we\ncan always compete with the best SIM model in terms of the squared loss. At the heart of our\napproach are distortion inequalities relating matching losses to \u2113p losses that we believe may be of\nindependent interest.\n\nWe first prove strong versions of such inequalities for matching losses arising from bi-Lipschitz\nlink functions, and obtain our results for general Lipschitz activations by carefully approximating\nthem using bi-Lipschitz activations. In particular, if we let $$\\text{opt(GLM}_{u^{-1},B})$$ denote the optimal value\nof $$\\text{err}^2(h)$$ over all GLMs of the form $$x \\rightarrow u^{-1}(w \\cdot x)$$, where $$\\|w\\|_2 \\leq B$$, we obtain the following\nresult about bi-Lipschitz activations (including, for example, the Leaky ReLU activation).\n\nTheorem 1.2 (Informal, see Theorem 2.1). Let $$u : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be a bi-Lipschitz invertible link function.\nThen, any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ that is an $$\\epsilon$$-approximate minimizer of the population matching loss\nthat corresponds to u, with respect to a distribution $$D$$ over $$\\mathbb{R}^d \\times [0, 1]$$ satisfies\n\n$$\\text{err}^2(p) \\leq O\\left(\\text{opt(GLM}_{u^{-1},B}\\right) + O(\\epsilon)$$\n\nThis guarantee holds under milder distributional assumptions than are required by comparable\nprior work on agnostically learning GLMs [FCG20, DKTZ22]. Moreover, when we focus on distortion bounds between the logistic loss and the squared loss, we obtain a near-optimal guarantee of\n$$O\\left(\\text{opt(GLM}_{u^{-1},B}\\right)$$ for logistic regression, when u is the logit link function (i.e., when $$\\text{GLM}_{u^{-1},B}$$\nis the class of sigmoid neurons).\n\nTheorem 1.3 (Informal, see Theorem 4.1). Let $$u(t) = \\ln(1-t)$$. Then, any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$\nthat is an $$\\epsilon$$-approximate minimizer of the population logistic loss, with respect to a distribution $$D$$\nover $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has subgaussian tails in every direction satisfies\n\n$$\\text{err}^2(p) \\leq O\\left(\\text{opt(GLM}_{u^{-1},B}\\right) + O(\\epsilon)$$\n\nWhile our error guarantee is weaker the one of [DKTZ22], we do not make the anti-concentration\nassumptions their results require. A natural question is to ask if our guarantees are near-optimal,\ne.g., whether we can obtain a guarantee of the form $$\\text{err}^2(p) \\leq \\text{opt(SIM)} + \\epsilon$$. However, there is\nstrong evidence that such results cannot be obtained using efficient algorithms [GKK19, DKZ20, GGK20, DKPZ21]. Adapting a result due to [DKMR22], we show in Section 5 that one cannot\navoid a dependence on the norm bound $$B$$ in our main result, Theorem 1.1.", "md": "of losses (as opposed to just a single pre-specified loss, as is standard in machine learning). They\nobtain such predictors through calibrated multiaccuracy [GHK+23] or multicalibration [GKR+22].\nTheir results apply to the non-realizable setting and do not assume prior knowledge of the link\nfunction, but only provide guarantees for (simultaneous) matching loss minimization, rather than\nthe standard squared error minimization.\n\nWe propose a simple analytic approach to transforming matching loss guarantees over the\nclass of linear functions to squared error guarantees over the class of GLMs with link function\nthat corresponds to the matching loss at hand. Our generic transformation, coupled with the\nomniprediction results from [GHK+23], yields our main result on agnostically learning SIMs. We\nthus obtain a best of all worlds statement: we do not need to know the link function, but we\ncan always compete with the best SIM model in terms of the squared loss. At the heart of our\napproach are distortion inequalities relating matching losses to \u2113p losses that we believe may be of\nindependent interest.\n\nWe first prove strong versions of such inequalities for matching losses arising from bi-Lipschitz\nlink functions, and obtain our results for general Lipschitz activations by carefully approximating\nthem using bi-Lipschitz activations. In particular, if we let $$\\text{opt(GLM}_{u^{-1},B})$$ denote the optimal value\nof $$\\text{err}^2(h)$$ over all GLMs of the form $$x \\rightarrow u^{-1}(w \\cdot x)$$, where $$\\|w\\|_2 \\leq B$$, we obtain the following\nresult about bi-Lipschitz activations (including, for example, the Leaky ReLU activation).\n\nTheorem 1.2 (Informal, see Theorem 2.1). Let $$u : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be a bi-Lipschitz invertible link function.\nThen, any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ that is an $$\\epsilon$$-approximate minimizer of the population matching loss\nthat corresponds to u, with respect to a distribution $$D$$ over $$\\mathbb{R}^d \\times [0, 1]$$ satisfies\n\n$$\\text{err}^2(p) \\leq O\\left(\\text{opt(GLM}_{u^{-1},B}\\right) + O(\\epsilon)$$\n\nThis guarantee holds under milder distributional assumptions than are required by comparable\nprior work on agnostically learning GLMs [FCG20, DKTZ22]. Moreover, when we focus on distortion bounds between the logistic loss and the squared loss, we obtain a near-optimal guarantee of\n$$O\\left(\\text{opt(GLM}_{u^{-1},B}\\right)$$ for logistic regression, when u is the logit link function (i.e., when $$\\text{GLM}_{u^{-1},B}$$\nis the class of sigmoid neurons).\n\nTheorem 1.3 (Informal, see Theorem 4.1). Let $$u(t) = \\ln(1-t)$$. Then, any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$\nthat is an $$\\epsilon$$-approximate minimizer of the population logistic loss, with respect to a distribution $$D$$\nover $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has subgaussian tails in every direction satisfies\n\n$$\\text{err}^2(p) \\leq O\\left(\\text{opt(GLM}_{u^{-1},B}\\right) + O(\\epsilon)$$\n\nWhile our error guarantee is weaker the one of [DKTZ22], we do not make the anti-concentration\nassumptions their results require. A natural question is to ask if our guarantees are near-optimal,\ne.g., whether we can obtain a guarantee of the form $$\\text{err}^2(p) \\leq \\text{opt(SIM)} + \\epsilon$$. However, there is\nstrong evidence that such results cannot be obtained using efficient algorithms [GKK19, DKZ20, GGK20, DKPZ21]. Adapting a result due to [DKMR22], we show in Section 5 that one cannot\navoid a dependence on the norm bound $$B$$ in our main result, Theorem 1.1."}, {"type": "heading", "lvl": 2, "value": "Background and Relation to Prior Work", "md": "## Background and Relation to Prior Work"}, {"type": "text", "value": "We note that matching losses have been studied in various previous works either implicitly [KKSK11]\nor explicitly [AHW95, DGK+20, GHK+23] and capture various fundamental algorithms like logistic and linear regression [McC84, Agr15]. However, to the best of our knowledge, our generic and\ndirect approach for transforming matching loss guarantees to squared error bounds, has not been", "md": "We note that matching losses have been studied in various previous works either implicitly [KKSK11]\nor explicitly [AHW95, DGK+20, GHK+23] and capture various fundamental algorithms like logistic and linear regression [McC84, Agr15]. However, to the best of our knowledge, our generic and\ndirect approach for transforming matching loss guarantees to squared error bounds, has not been"}]}, {"page": 4, "text": "explored previously. Furthermore, our results do not depend on the specific implementation of an\nalgorithm, but only on the matching loss bounds achieved by its output. In this sense, we provide\nnew agnostic error guarantees for various existing algorithms of the literature. For example, our\nresults imply new guarantees for the GLMtron algorithm of [KKSK11] in the agnostic setting, since\nGLMtron can be viewed as performing gradient descent (with unit step size) on the matching loss\ncorresponding to a specified link function.\n    Matching losses over linear functions are also linked to the Chow parameters [OS08] through\ntheir gradient with respect to w, as observed by [DGK+20]. In fact, the norm of the matching loss\ngradient is also linked to multiaccuracy, a notion that originates to fairness literature [HJKRR18,\nKGZ19]. A stationary point w of a matching loss that corresponds to a GLM with link u turns\nout to be a multiaccurate predictor p(x) = u\u22121(w \u00b7 x), i.e., a predictor such that for all i \u2208      [d],\nE[xi(y\u2212p(x))] = 0. The work of [GKR+22, GHK+23] on omnipredictors presents a single predictor\nthat is better than any linear model w \u00b7 x for every matching loss. The results of [GKR+22] show\nthat a multicalibrated predictor (with respect to the features xi) is an omnipredictor for all convex\nlosses, whereas [GHK+23] shows that a simpler condition of calibrated multiaccuracy suffices for\nmatching losses that arise from GLMs. In view of the relationship between multiaccuracy and\nthe gradient of the matching loss, our results show that, while multiaccuracy implies bounds on\nagnostically learning GLMs, the additional guarantee of calibration is sufficient for agnostically\nlearning all SIMs.\n    The work of [SSSS11] showed strong agnostic learning guarantees in terms of the absolute error\n(rather than the squared error) of the form opt + \u01eb for a range of GLMs, but their work incurs an\nexponential dependence on the weight norm B. For the absolute loss, we obtain a bound of the\nform B opt log(1/opt) for logistic regression (see Theorem 4.3). In more recent years, the problem\nof agnostically learning GLMs has frequently also been phrased as the problem of agnostically\nlearning single neurons (with a known activation). For the ReLU activation, work by [GKKT17]\nshowed an algorithm achieving error opt + \u01eb in time poly(d) exp(1/\u01eb) over marginals on the unit\nsphere, and [DGK+20] showed an algorithm achieving error O(opt) + \u01eb in fully polynomial time\nover isotropic log-concave marginals. The work of [FCG20, DKTZ22] both show guarantees for\nlearning general neurons (with known activations) using the natural approach of running SGD\ndirectly on the squared loss (or a regularized variant thereof). [FCG20] achieves error O(opt) for\nstrictly increasing activations and O(\u221a  opt) for the ReLU activation over bounded marginals, while\n[DKTZ22] proved an O(opt) guarantee for a wide range of activations (including the ReLU) and\nover a large class of structured marginals.\n    In terms of lower bounds and hardness results for this problem, the work of [GKK19, DKZ20,\nGGK20, DKPZ21, DKMR22] has established superpolynomial hardness even for the setting of\nagnostically learning single ReLUs over Gaussian marginals.\nLimitations and directions for future work.            While we justify a certain dependence on the\nnorm bound B in our our main result on agnostically learning SIMs, we do not provide tight\nlower bounds corresponding to Theorem 1.1. An important direction for future work is to tightly\ncharacterize the optimal bounds achievable for this problem, as well as to show matching algorithms.\n1.2    Preliminaries\nFor the following, (x, y) is used to denote a labelled sample from a distribution D over Rd \u00d7 Y,\nwhere Y denotes the interval [0, 1] unless it is specified to be the set {0, 1}. We note that, although\nwe provide results for the setting where the labels lie within [0, 1], we may obtain similar results\nfor any bounded label space. We use PD (resp. ED) to denote the probability (resp. expectation)\n                                                   4", "md": "explored previously. Furthermore, our results do not depend on the specific implementation of an\nalgorithm, but only on the matching loss bounds achieved by its output. In this sense, we provide\nnew agnostic error guarantees for various existing algorithms of the literature. For example, our\nresults imply new guarantees for the GLMtron algorithm of [KKSK11] in the agnostic setting, since\nGLMtron can be viewed as performing gradient descent (with unit step size) on the matching loss\ncorresponding to a specified link function.\n\nMatching losses over linear functions are also linked to the Chow parameters [OS08] through\ntheir gradient with respect to w, as observed by [DGK+20]. In fact, the norm of the matching loss\ngradient is also linked to multiaccuracy, a notion that originates to fairness literature [HJKRR18,\nKGZ19]. A stationary point w of a matching loss that corresponds to a GLM with link u turns\nout to be a multiaccurate predictor $$p(x) = u^{-1}(w \\cdot x)$$, i.e., a predictor such that for all $$i \\in [d]$$,\n$$E[x_i(y - p(x))] = 0$$. The work of [GKR+22, GHK+23] on omnipredictors presents a single predictor\nthat is better than any linear model $$w \\cdot x$$ for every matching loss. The results of [GKR+22] show\nthat a multicalibrated predictor (with respect to the features $$x_i$$) is an omnipredictor for all convex\nlosses, whereas [GHK+23] shows that a simpler condition of calibrated multiaccuracy suffices for\nmatching losses that arise from GLMs. In view of the relationship between multiaccuracy and\nthe gradient of the matching loss, our results show that, while multiaccuracy implies bounds on\nagnostically learning GLMs, the additional guarantee of calibration is sufficient for agnostically\nlearning all SIMs.\n\nThe work of [SSSS11] showed strong agnostic learning guarantees in terms of the absolute error\n(rather than the squared error) of the form opt + $$\\epsilon$$ for a range of GLMs, but their work incurs an\nexponential dependence on the weight norm B. For the absolute loss, we obtain a bound of the\nform $$B \\cdot opt \\cdot \\log(1/opt)$$ for logistic regression (see Theorem 4.3). In more recent years, the problem\nof agnostically learning GLMs has frequently also been phrased as the problem of agnostically\nlearning single neurons (with a known activation). For the ReLU activation, work by [GKKT17]\nshowed an algorithm achieving error opt + $$\\epsilon$$ in time poly(d) exp(1/$$\\epsilon$$) over marginals on the unit\nsphere, and [DGK+20] showed an algorithm achieving error $$O(opt) + \\epsilon$$ in fully polynomial time\nover isotropic log-concave marginals. The work of [FCG20, DKTZ22] both show guarantees for\nlearning general neurons (with known activations) using the natural approach of running SGD\ndirectly on the squared loss (or a regularized variant thereof). [FCG20] achieves error $$O(opt)$$ for\nstrictly increasing activations and $$O(\\sqrt{opt})$$ for the ReLU activation over bounded marginals, while\n[DKTZ22] proved an $$O(opt)$$ guarantee for a wide range of activations (including the ReLU) and\nover a large class of structured marginals.\n\nIn terms of lower bounds and hardness results for this problem, the work of [GKK19, DKZ20,\nGGK20, DKPZ21, DKMR22] has established superpolynomial hardness even for the setting of\nagnostically learning single ReLUs over Gaussian marginals.\n\nLimitations and directions for future work. While we justify a certain dependence on the\nnorm bound B in our our main result on agnostically learning SIMs, we do not provide tight\nlower bounds corresponding to Theorem 1.1. An important direction for future work is to tightly\ncharacterize the optimal bounds achievable for this problem, as well as to show matching algorithms.\n\n## 1.2 Preliminaries\n\nFor the following, $$(x, y)$$ is used to denote a labelled sample from a distribution D over $$\\mathbb{R}^d \\times Y$$,\nwhere Y denotes the interval [0, 1] unless it is specified to be the set {0, 1}. We note that, although\nwe provide results for the setting where the labels lie within [0, 1], we may obtain similar results\nfor any bounded label space. We use $$P_D$$ (resp. $$E_D$$) to denote the probability (resp. expectation)", "images": [], "items": [{"type": "text", "value": "explored previously. Furthermore, our results do not depend on the specific implementation of an\nalgorithm, but only on the matching loss bounds achieved by its output. In this sense, we provide\nnew agnostic error guarantees for various existing algorithms of the literature. For example, our\nresults imply new guarantees for the GLMtron algorithm of [KKSK11] in the agnostic setting, since\nGLMtron can be viewed as performing gradient descent (with unit step size) on the matching loss\ncorresponding to a specified link function.\n\nMatching losses over linear functions are also linked to the Chow parameters [OS08] through\ntheir gradient with respect to w, as observed by [DGK+20]. In fact, the norm of the matching loss\ngradient is also linked to multiaccuracy, a notion that originates to fairness literature [HJKRR18,\nKGZ19]. A stationary point w of a matching loss that corresponds to a GLM with link u turns\nout to be a multiaccurate predictor $$p(x) = u^{-1}(w \\cdot x)$$, i.e., a predictor such that for all $$i \\in [d]$$,\n$$E[x_i(y - p(x))] = 0$$. The work of [GKR+22, GHK+23] on omnipredictors presents a single predictor\nthat is better than any linear model $$w \\cdot x$$ for every matching loss. The results of [GKR+22] show\nthat a multicalibrated predictor (with respect to the features $$x_i$$) is an omnipredictor for all convex\nlosses, whereas [GHK+23] shows that a simpler condition of calibrated multiaccuracy suffices for\nmatching losses that arise from GLMs. In view of the relationship between multiaccuracy and\nthe gradient of the matching loss, our results show that, while multiaccuracy implies bounds on\nagnostically learning GLMs, the additional guarantee of calibration is sufficient for agnostically\nlearning all SIMs.\n\nThe work of [SSSS11] showed strong agnostic learning guarantees in terms of the absolute error\n(rather than the squared error) of the form opt + $$\\epsilon$$ for a range of GLMs, but their work incurs an\nexponential dependence on the weight norm B. For the absolute loss, we obtain a bound of the\nform $$B \\cdot opt \\cdot \\log(1/opt)$$ for logistic regression (see Theorem 4.3). In more recent years, the problem\nof agnostically learning GLMs has frequently also been phrased as the problem of agnostically\nlearning single neurons (with a known activation). For the ReLU activation, work by [GKKT17]\nshowed an algorithm achieving error opt + $$\\epsilon$$ in time poly(d) exp(1/$$\\epsilon$$) over marginals on the unit\nsphere, and [DGK+20] showed an algorithm achieving error $$O(opt) + \\epsilon$$ in fully polynomial time\nover isotropic log-concave marginals. The work of [FCG20, DKTZ22] both show guarantees for\nlearning general neurons (with known activations) using the natural approach of running SGD\ndirectly on the squared loss (or a regularized variant thereof). [FCG20] achieves error $$O(opt)$$ for\nstrictly increasing activations and $$O(\\sqrt{opt})$$ for the ReLU activation over bounded marginals, while\n[DKTZ22] proved an $$O(opt)$$ guarantee for a wide range of activations (including the ReLU) and\nover a large class of structured marginals.\n\nIn terms of lower bounds and hardness results for this problem, the work of [GKK19, DKZ20,\nGGK20, DKPZ21, DKMR22] has established superpolynomial hardness even for the setting of\nagnostically learning single ReLUs over Gaussian marginals.\n\nLimitations and directions for future work. While we justify a certain dependence on the\nnorm bound B in our our main result on agnostically learning SIMs, we do not provide tight\nlower bounds corresponding to Theorem 1.1. An important direction for future work is to tightly\ncharacterize the optimal bounds achievable for this problem, as well as to show matching algorithms.", "md": "explored previously. Furthermore, our results do not depend on the specific implementation of an\nalgorithm, but only on the matching loss bounds achieved by its output. In this sense, we provide\nnew agnostic error guarantees for various existing algorithms of the literature. For example, our\nresults imply new guarantees for the GLMtron algorithm of [KKSK11] in the agnostic setting, since\nGLMtron can be viewed as performing gradient descent (with unit step size) on the matching loss\ncorresponding to a specified link function.\n\nMatching losses over linear functions are also linked to the Chow parameters [OS08] through\ntheir gradient with respect to w, as observed by [DGK+20]. In fact, the norm of the matching loss\ngradient is also linked to multiaccuracy, a notion that originates to fairness literature [HJKRR18,\nKGZ19]. A stationary point w of a matching loss that corresponds to a GLM with link u turns\nout to be a multiaccurate predictor $$p(x) = u^{-1}(w \\cdot x)$$, i.e., a predictor such that for all $$i \\in [d]$$,\n$$E[x_i(y - p(x))] = 0$$. The work of [GKR+22, GHK+23] on omnipredictors presents a single predictor\nthat is better than any linear model $$w \\cdot x$$ for every matching loss. The results of [GKR+22] show\nthat a multicalibrated predictor (with respect to the features $$x_i$$) is an omnipredictor for all convex\nlosses, whereas [GHK+23] shows that a simpler condition of calibrated multiaccuracy suffices for\nmatching losses that arise from GLMs. In view of the relationship between multiaccuracy and\nthe gradient of the matching loss, our results show that, while multiaccuracy implies bounds on\nagnostically learning GLMs, the additional guarantee of calibration is sufficient for agnostically\nlearning all SIMs.\n\nThe work of [SSSS11] showed strong agnostic learning guarantees in terms of the absolute error\n(rather than the squared error) of the form opt + $$\\epsilon$$ for a range of GLMs, but their work incurs an\nexponential dependence on the weight norm B. For the absolute loss, we obtain a bound of the\nform $$B \\cdot opt \\cdot \\log(1/opt)$$ for logistic regression (see Theorem 4.3). In more recent years, the problem\nof agnostically learning GLMs has frequently also been phrased as the problem of agnostically\nlearning single neurons (with a known activation). For the ReLU activation, work by [GKKT17]\nshowed an algorithm achieving error opt + $$\\epsilon$$ in time poly(d) exp(1/$$\\epsilon$$) over marginals on the unit\nsphere, and [DGK+20] showed an algorithm achieving error $$O(opt) + \\epsilon$$ in fully polynomial time\nover isotropic log-concave marginals. The work of [FCG20, DKTZ22] both show guarantees for\nlearning general neurons (with known activations) using the natural approach of running SGD\ndirectly on the squared loss (or a regularized variant thereof). [FCG20] achieves error $$O(opt)$$ for\nstrictly increasing activations and $$O(\\sqrt{opt})$$ for the ReLU activation over bounded marginals, while\n[DKTZ22] proved an $$O(opt)$$ guarantee for a wide range of activations (including the ReLU) and\nover a large class of structured marginals.\n\nIn terms of lower bounds and hardness results for this problem, the work of [GKK19, DKZ20,\nGGK20, DKPZ21, DKMR22] has established superpolynomial hardness even for the setting of\nagnostically learning single ReLUs over Gaussian marginals.\n\nLimitations and directions for future work. While we justify a certain dependence on the\nnorm bound B in our our main result on agnostically learning SIMs, we do not provide tight\nlower bounds corresponding to Theorem 1.1. An important direction for future work is to tightly\ncharacterize the optimal bounds achievable for this problem, as well as to show matching algorithms."}, {"type": "heading", "lvl": 2, "value": "1.2 Preliminaries", "md": "## 1.2 Preliminaries"}, {"type": "text", "value": "For the following, $$(x, y)$$ is used to denote a labelled sample from a distribution D over $$\\mathbb{R}^d \\times Y$$,\nwhere Y denotes the interval [0, 1] unless it is specified to be the set {0, 1}. We note that, although\nwe provide results for the setting where the labels lie within [0, 1], we may obtain similar results\nfor any bounded label space. We use $$P_D$$ (resp. $$E_D$$) to denote the probability (resp. expectation)", "md": "For the following, $$(x, y)$$ is used to denote a labelled sample from a distribution D over $$\\mathbb{R}^d \\times Y$$,\nwhere Y denotes the interval [0, 1] unless it is specified to be the set {0, 1}. We note that, although\nwe provide results for the setting where the labels lie within [0, 1], we may obtain similar results\nfor any bounded label space. We use $$P_D$$ (resp. $$E_D$$) to denote the probability (resp. expectation)"}]}, {"page": 5, "text": "over D and, similarly, PS (resp. ES) to denote the corresponding empirical quantity over a set S\nof labelled examples. Throughout the paper, we will use the term differentiable function to mean\na function that is differentiable except on finitely many points.\n     Our main results will assume the following about the marginal distribution on Rd.\nDefinition 1.4 (Bounded moments). For \u03bb > 0 and k \u2208                            N, we say that a distribution Dx over Rd\nhas \u03bb-bounded 2k-th moments if for any v \u2208                     Sd\u22121 we have Ex\u223cDx[(v \u00b7 x)2k] \u2264              \u03bb.\n     For a concept class C : Rd \u2192            R, we define opt(C, D) to be the minimum squared error achievable\nby a concept c : Rd \u2192          R in C with respect to the distribution D.\n     We will also provide results that are specific to the sigmoid activation and work under the\nassumption that the marginal distribution is sufficiently concentrated.\nDefinition 1.5 (Concentrated marginals). For \u03bb > 0 and \u03b3, we say that a distribution Dx over\nRd is (\u03bb, \u03b3)-concentrated if for any v \u2208               Sd\u22121 and r \u2265       0 we have Px\u223cDx[|v \u00b7 x| \u2265           r] \u2264   \u03bb \u00b7 exp(\u2212r\u03b3).\nDefinition 1.6 (Fenchel-Legendre pairs). We call a pair of functions (f, g) a Fenchel-Legendre pair\nif the following conditions hold.\n    1. g\u2032 : R \u2192     R is continuous, non-decreasing, differentiable and 1-Lipschitz with range ran(g\u2032) \u2287\n        (0, 1) and g(t) =        0tg\u2032(\u03c4) d\u03c4, for any t \u2208        R.\n    2. f : ran(g\u2032) \u2192         R is the convex conjugate (Fenchel-Legendre transform) of g, i.e., we have\n        f(r) = sup     t\u2208R r \u00b7 t \u2212   g(t) for any r \u2208      ran(g\u2032).\n     For such pairs of functions, the following are true for r \u2208                        ran(g\u2032) and t \u2208       ran(f \u2032) (note that\nran(f \u2032) is not necessarily R when g\u2032 is not invertible).\n                            g\u2032(f \u2032(r)) = r and f(r) = rf \u2032(r) \u2212            g(f \u2032(r)), for r \u2208      ran(g\u2032)                       (1.1)\n                            f \u2032(g\u2032(t)) = t and g(t) = tg\u2032(t) \u2212            f(g\u2032(t)), for t \u2208     ran(f \u2032)                         (1.2)\nNote that g\u2032 will be used as an activation function for single neurons and f \u2032 corresponds to the\nunknown link function of a SIM (or the known link function of a GLM). We say that g\u2032 is bi-\nLipschitz if for any t1 < t2 \u2208              R we have that (g\u2032(t2) \u2212            g\u2032(t1))/(t2 \u2212     t1) \u2208    [\u03b1, \u03b2].   If g\u2032 is [\u03b1, \u03b2]\nbi-Lipschitz, then f \u2032 is [ 1    \u03b2 , 1\n                                     \u03b1] bi-Lipschitz. However, the converse implication is not necessarily true\nwhen g\u2032 is not strictly increasing.\nDefinition 1.7 (Matching Losses). For a non-decreasing and Lipschitz activation g\u2032 : R \u2192                                       R, the\nmatching loss \u2113g : Y \u00d7 R \u2192             R is defined pointwise as follows:\n                                                  \u2113g(y, t) =      0tg\u2032(\u03c4) \u2212    y d\u03c4,\nwhere g(t) =        0tg\u2032. The function \u2113g is convex and smooth with respect to its second argument. The\ncorresponding population matching loss is\n                                              Lg(c ; D) =          E      \u2113g(y, c(x))                                            (1.3)\n                                                               (x,y)\u223cD\nIn Equation (1.3), c : R \u2192               R is some concept and D is some distribution over Rd \u00d7 [0, 1]. In\nthe specific case where c is a linear function, i.e., c(x) = w \u00b7 x, for some w \u2208                                Rd, then we may\nalternatively denote Lg(c ; D) with Lg(w ; D).                     5", "md": "# Math Equations and Definitions\n\nover D and, similarly, PS (resp. ES) to denote the corresponding empirical quantity over a set S\nof labelled examples. Throughout the paper, we will use the term differentiable function to mean\na function that is differentiable except on finitely many points.\n\nOur main results will assume the following about the marginal distribution on $$\\mathbb{R}^d$$.\n\nDefinition 1.4 (Bounded moments). For $$\\lambda > 0$$ and $$k \\in \\mathbb{N}$$, we say that a distribution $$D_x$$ over $$\\mathbb{R}^d$$\nhas $$\\lambda$$-bounded $$2k$$-th moments if for any $$v \\in S^{d-1}$$ we have $$\\mathbb{E}_{x \\sim D_x}[(v \\cdot x)^{2k}] \\leq \\lambda$$.\n\nFor a concept class $$C : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$, we define $$\\text{opt}(C, D)$$ to be the minimum squared error achievable\nby a concept $$c : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ in $$C$$ with respect to the distribution $$D$$.\n\nWe will also provide results that are specific to the sigmoid activation and work under the\nassumption that the marginal distribution is sufficiently concentrated.\n\nDefinition 1.5 (Concentrated marginals). For $$\\lambda > 0$$ and $$\\gamma$$, we say that a distribution $$D_x$$ over\n$$\\mathbb{R}^d$$ is $$(\\lambda, \\gamma)$$-concentrated if for any $$v \\in S^{d-1}$$ and $$r \\geq 0$$ we have $$\\mathbb{P}_{x \\sim D_x}[|v \\cdot x| \\geq r] \\leq \\lambda \\cdot \\exp(-r\\gamma)$$.\n\nDefinition 1.6 (Fenchel-Legendre pairs). We call a pair of functions $$(f, g)$$ a Fenchel-Legendre pair\nif the following conditions hold.\n\n1. $g' : \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous, non-decreasing, differentiable and 1-Lipschitz with range $\\text{ran}(g') \\supseteq (0, 1)$ and $g(t) = \\int_{0}^{t} g'(\\tau) d\\tau$, for any $t \\in \\mathbb{R}$.\n2. $f : \\text{ran}(g') \\rightarrow \\mathbb{R}$ is the convex conjugate (Fenchel-Legendre transform) of $g$, i.e., we have\n$f(r) = \\sup_{t \\in \\mathbb{R}} r \\cdot t - g(t)$ for any $r \\in \\text{ran}(g')$.\n\nFor such pairs of functions, the following are true for $$r \\in \\text{ran}(g')$$ and $$t \\in \\text{ran}(f')$$ (note that\n$$\\text{ran}(f')$$ is not necessarily $$\\mathbb{R}$$ when $$g'$$ is not invertible).\n\n$$g'(f'(r)) = r$$ and $$f(r) = r f'(r) - g(f'(r))$$, for $$r \\in \\text{ran}(g')$$ (1.1)\n\n$$f'(g'(t)) = t$$ and $$g(t) = t g'(t) - f(g'(t))$$, for $$t \\in \\text{ran}(f')$$ (1.2)\n\nNote that $$g'$$ will be used as an activation function for single neurons and $$f'$$ corresponds to the\nunknown link function of a SIM (or the known link function of a GLM). We say that $$g'$$ is bi-Lipschitz if for any $$t_1 < t_2 \\in \\mathbb{R}$$ we have that $$(g'(t_2) - g'(t_1))/(t_2 - t_1) \\in [\\alpha, \\beta]$$. If $$g'$$ is $$[\\alpha, \\beta]$$\nbi-Lipschitz, then $$f'$$ is $$[1/\\beta, 1/\\alpha]$$ bi-Lipschitz. However, the converse implication is not necessarily true\nwhen $$g'$$ is not strictly increasing.\n\nDefinition 1.7 (Matching Losses). For a non-decreasing and Lipschitz activation $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$, the\nmatching loss $$\\ell_g : Y \\times \\mathbb{R} \\rightarrow \\mathbb{R}$$ is defined pointwise as follows:\n\n$$\\ell_g(y, t) = \\int_{0}^{t} g'(\\tau) - y d\\tau$$,\n\nwhere $$g(t) = \\int_{0}^{t} g'$$ The function $$\\ell_g$$ is convex and smooth with respect to its second argument. The\ncorresponding population matching loss is\n\n$$L_g(c ; D) = \\mathbb{E}_{(x,y) \\sim D} \\ell_g(y, c(x))$$ (1.3)\n\nIn Equation (1.3), $$c : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is some concept and $$D$$ is some distribution over $$\\mathbb{R}^d \\times [0, 1]$$. In\nthe specific case where $$c$$ is a linear function, i.e., $$c(x) = w \\cdot x$$, for some $$w \\in \\mathbb{R}^d$$, then we may\nalternatively denote $$L_g(c ; D)$$ with $$L_g(w ; D)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Definitions", "md": "# Math Equations and Definitions"}, {"type": "text", "value": "over D and, similarly, PS (resp. ES) to denote the corresponding empirical quantity over a set S\nof labelled examples. Throughout the paper, we will use the term differentiable function to mean\na function that is differentiable except on finitely many points.\n\nOur main results will assume the following about the marginal distribution on $$\\mathbb{R}^d$$.\n\nDefinition 1.4 (Bounded moments). For $$\\lambda > 0$$ and $$k \\in \\mathbb{N}$$, we say that a distribution $$D_x$$ over $$\\mathbb{R}^d$$\nhas $$\\lambda$$-bounded $$2k$$-th moments if for any $$v \\in S^{d-1}$$ we have $$\\mathbb{E}_{x \\sim D_x}[(v \\cdot x)^{2k}] \\leq \\lambda$$.\n\nFor a concept class $$C : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$, we define $$\\text{opt}(C, D)$$ to be the minimum squared error achievable\nby a concept $$c : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ in $$C$$ with respect to the distribution $$D$$.\n\nWe will also provide results that are specific to the sigmoid activation and work under the\nassumption that the marginal distribution is sufficiently concentrated.\n\nDefinition 1.5 (Concentrated marginals). For $$\\lambda > 0$$ and $$\\gamma$$, we say that a distribution $$D_x$$ over\n$$\\mathbb{R}^d$$ is $$(\\lambda, \\gamma)$$-concentrated if for any $$v \\in S^{d-1}$$ and $$r \\geq 0$$ we have $$\\mathbb{P}_{x \\sim D_x}[|v \\cdot x| \\geq r] \\leq \\lambda \\cdot \\exp(-r\\gamma)$$.\n\nDefinition 1.6 (Fenchel-Legendre pairs). We call a pair of functions $$(f, g)$$ a Fenchel-Legendre pair\nif the following conditions hold.\n\n1. $g' : \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous, non-decreasing, differentiable and 1-Lipschitz with range $\\text{ran}(g') \\supseteq (0, 1)$ and $g(t) = \\int_{0}^{t} g'(\\tau) d\\tau$, for any $t \\in \\mathbb{R}$.\n2. $f : \\text{ran}(g') \\rightarrow \\mathbb{R}$ is the convex conjugate (Fenchel-Legendre transform) of $g$, i.e., we have\n$f(r) = \\sup_{t \\in \\mathbb{R}} r \\cdot t - g(t)$ for any $r \\in \\text{ran}(g')$.\n\nFor such pairs of functions, the following are true for $$r \\in \\text{ran}(g')$$ and $$t \\in \\text{ran}(f')$$ (note that\n$$\\text{ran}(f')$$ is not necessarily $$\\mathbb{R}$$ when $$g'$$ is not invertible).\n\n$$g'(f'(r)) = r$$ and $$f(r) = r f'(r) - g(f'(r))$$, for $$r \\in \\text{ran}(g')$$ (1.1)\n\n$$f'(g'(t)) = t$$ and $$g(t) = t g'(t) - f(g'(t))$$, for $$t \\in \\text{ran}(f')$$ (1.2)\n\nNote that $$g'$$ will be used as an activation function for single neurons and $$f'$$ corresponds to the\nunknown link function of a SIM (or the known link function of a GLM). We say that $$g'$$ is bi-Lipschitz if for any $$t_1 < t_2 \\in \\mathbb{R}$$ we have that $$(g'(t_2) - g'(t_1))/(t_2 - t_1) \\in [\\alpha, \\beta]$$. If $$g'$$ is $$[\\alpha, \\beta]$$\nbi-Lipschitz, then $$f'$$ is $$[1/\\beta, 1/\\alpha]$$ bi-Lipschitz. However, the converse implication is not necessarily true\nwhen $$g'$$ is not strictly increasing.\n\nDefinition 1.7 (Matching Losses). For a non-decreasing and Lipschitz activation $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$, the\nmatching loss $$\\ell_g : Y \\times \\mathbb{R} \\rightarrow \\mathbb{R}$$ is defined pointwise as follows:\n\n$$\\ell_g(y, t) = \\int_{0}^{t} g'(\\tau) - y d\\tau$$,\n\nwhere $$g(t) = \\int_{0}^{t} g'$$ The function $$\\ell_g$$ is convex and smooth with respect to its second argument. The\ncorresponding population matching loss is\n\n$$L_g(c ; D) = \\mathbb{E}_{(x,y) \\sim D} \\ell_g(y, c(x))$$ (1.3)\n\nIn Equation (1.3), $$c : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is some concept and $$D$$ is some distribution over $$\\mathbb{R}^d \\times [0, 1]$$. In\nthe specific case where $$c$$ is a linear function, i.e., $$c(x) = w \\cdot x$$, for some $$w \\in \\mathbb{R}^d$$, then we may\nalternatively denote $$L_g(c ; D)$$ with $$L_g(w ; D)$$.", "md": "over D and, similarly, PS (resp. ES) to denote the corresponding empirical quantity over a set S\nof labelled examples. Throughout the paper, we will use the term differentiable function to mean\na function that is differentiable except on finitely many points.\n\nOur main results will assume the following about the marginal distribution on $$\\mathbb{R}^d$$.\n\nDefinition 1.4 (Bounded moments). For $$\\lambda > 0$$ and $$k \\in \\mathbb{N}$$, we say that a distribution $$D_x$$ over $$\\mathbb{R}^d$$\nhas $$\\lambda$$-bounded $$2k$$-th moments if for any $$v \\in S^{d-1}$$ we have $$\\mathbb{E}_{x \\sim D_x}[(v \\cdot x)^{2k}] \\leq \\lambda$$.\n\nFor a concept class $$C : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$, we define $$\\text{opt}(C, D)$$ to be the minimum squared error achievable\nby a concept $$c : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ in $$C$$ with respect to the distribution $$D$$.\n\nWe will also provide results that are specific to the sigmoid activation and work under the\nassumption that the marginal distribution is sufficiently concentrated.\n\nDefinition 1.5 (Concentrated marginals). For $$\\lambda > 0$$ and $$\\gamma$$, we say that a distribution $$D_x$$ over\n$$\\mathbb{R}^d$$ is $$(\\lambda, \\gamma)$$-concentrated if for any $$v \\in S^{d-1}$$ and $$r \\geq 0$$ we have $$\\mathbb{P}_{x \\sim D_x}[|v \\cdot x| \\geq r] \\leq \\lambda \\cdot \\exp(-r\\gamma)$$.\n\nDefinition 1.6 (Fenchel-Legendre pairs). We call a pair of functions $$(f, g)$$ a Fenchel-Legendre pair\nif the following conditions hold.\n\n1. $g' : \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous, non-decreasing, differentiable and 1-Lipschitz with range $\\text{ran}(g') \\supseteq (0, 1)$ and $g(t) = \\int_{0}^{t} g'(\\tau) d\\tau$, for any $t \\in \\mathbb{R}$.\n2. $f : \\text{ran}(g') \\rightarrow \\mathbb{R}$ is the convex conjugate (Fenchel-Legendre transform) of $g$, i.e., we have\n$f(r) = \\sup_{t \\in \\mathbb{R}} r \\cdot t - g(t)$ for any $r \\in \\text{ran}(g')$.\n\nFor such pairs of functions, the following are true for $$r \\in \\text{ran}(g')$$ and $$t \\in \\text{ran}(f')$$ (note that\n$$\\text{ran}(f')$$ is not necessarily $$\\mathbb{R}$$ when $$g'$$ is not invertible).\n\n$$g'(f'(r)) = r$$ and $$f(r) = r f'(r) - g(f'(r))$$, for $$r \\in \\text{ran}(g')$$ (1.1)\n\n$$f'(g'(t)) = t$$ and $$g(t) = t g'(t) - f(g'(t))$$, for $$t \\in \\text{ran}(f')$$ (1.2)\n\nNote that $$g'$$ will be used as an activation function for single neurons and $$f'$$ corresponds to the\nunknown link function of a SIM (or the known link function of a GLM). We say that $$g'$$ is bi-Lipschitz if for any $$t_1 < t_2 \\in \\mathbb{R}$$ we have that $$(g'(t_2) - g'(t_1))/(t_2 - t_1) \\in [\\alpha, \\beta]$$. If $$g'$$ is $$[\\alpha, \\beta]$$\nbi-Lipschitz, then $$f'$$ is $$[1/\\beta, 1/\\alpha]$$ bi-Lipschitz. However, the converse implication is not necessarily true\nwhen $$g'$$ is not strictly increasing.\n\nDefinition 1.7 (Matching Losses). For a non-decreasing and Lipschitz activation $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$, the\nmatching loss $$\\ell_g : Y \\times \\mathbb{R} \\rightarrow \\mathbb{R}$$ is defined pointwise as follows:\n\n$$\\ell_g(y, t) = \\int_{0}^{t} g'(\\tau) - y d\\tau$$,\n\nwhere $$g(t) = \\int_{0}^{t} g'$$ The function $$\\ell_g$$ is convex and smooth with respect to its second argument. The\ncorresponding population matching loss is\n\n$$L_g(c ; D) = \\mathbb{E}_{(x,y) \\sim D} \\ell_g(y, c(x))$$ (1.3)\n\nIn Equation (1.3), $$c : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is some concept and $$D$$ is some distribution over $$\\mathbb{R}^d \\times [0, 1]$$. In\nthe specific case where $$c$$ is a linear function, i.e., $$c(x) = w \\cdot x$$, for some $$w \\in \\mathbb{R}^d$$, then we may\nalternatively denote $$L_g(c ; D)$$ with $$L_g(w ; D)$$."}]}, {"page": 6, "text": "    We also define the Bregman divergence associated with f to be Df(q, r) = f(q) \u2212             f(r) \u2212  (q \u2212\nr)f \u2032(r), for any q, r \u2208 ran(g\u2032). Note that Df(q, r) \u2265    0 with equality iff q = r.\nDefinition 1.8 (SIMs and GLMs as Concept Classes). For B > 0, we use SIMB to refer to the\nclass of all SIMs of the form x  \u2192       g\u2032(w \u00b7 x) where \u2225w\u22252 \u2264      B and g\u2032 : R \u2192       R is an arbitrary\n1-Lipschitz monotone activation that is differentiable (except possibly at finitely many points). We\ndefine GLMg\u2032,B similarly except for the case where g\u2032 is fixed and known.\n    We also define the notion of calibrated multiaccuracy that we need to obtain omnipredictors in\nour context.\nDefinition 1.9 (Calibrated Multiaccuracy). A predictor p : R \u2192            [0, 1] is called \u01eb-multiaccurate if\nfor all i \u2208 [d], | E[xi(y \u2212p(x))]| \u2264  \u01eb. It is called \u01eb-calibrated if | Ep(x) Ey|p(x)[y \u2212p(x)]| \u2264 \u01eb.\n2     Distortion Bounds for the Matching Loss\nIn this section, we propose a simple approach for bounding the squared error of a predictor that\nminimizes a (convex) matching loss, in the agnostic setting. We convert matching loss bounds to\nsquared loss bounds in a generic way, through appropriate pointwise distortion bounds between\nthe two losses. In particular, for a given matching loss Lg, we transform guarantees on Lg that are\ncompetitive with the optimum linear minimizer of Lg to guarantees on the squared error that are\ncompetitive with the optimum GLM whose activation (g\u2032) depends on the matching loss at hand.\n    We now provide the main result we establish in this section.\nTheorem 2.1 (Squared Error Minimization through Matching Loss Minimization). Let D be a\ndistribution over Rd \u00d7 [0, 1], let 0 < \u03b1 \u2264       \u03b2 and let (f, g) be a Fenchel-Legendre pair such that\ng\u2032 : R \u2192  R is [\u03b1, \u03b2] bi-Lipschitz. Suppose that for a predictor p : Rd \u2192      ran(g\u2032) we have\n                                  Lg(f \u2032 \u25e6 p ; D) \u2264   min                                               (2.1)\nThen we also have: err2(p) \u2264      \u03b2                 \u2225w\u22252\u2264B Lg(w ; D) + \u01eb\n                                  \u03b1 \u00b7 opt(GLMg\u2032,B) + 2\u03b2\u01eb.\n    The proof of Theorem 2.1 is based on the following pointwise distortion bound between matching\nlosses corresponding to bi-Lipschitz link functions and the squared distance.\nLemma 2.2 (Pointwise Distortion Bound for bi-Lipschitz link functions). Let 0 < \u03b1 \u2264                \u03b2 and let\n(f, g) be a Fenchel-Legendre pair such that f \u2032 : ran(g\u2032) \u2192       R is [ 1\n                                                                         \u03b2, 1\n                                                                            \u03b1] bi-Lipschitz. Then for any\ny, p \u2208 ran(g\u2032) we have\n                   \u2113g(y, f \u2032(p)) \u2212\u2113g(y, f \u2032(y)) = Df(y, p) \u2208    1\n                                                               2\u03b2 (y \u2212  p)2, 1\n                                                                             2\u03b1(y \u2212  p)2\n    In the case that f \u2032 is differentiable on (0, 1), the proof of Lemma 2.2 follows from an application\nof Taylor\u2019s approximation theorem of degree 2 on the function f, since the Bregman divergence\nD f(y, p) is exactly equal to the error of the second degree Taylor\u2019s approximation of f(y) around p\nand f \u2032\u2032(\u03be) \u2208 [ 1\n               \u03b2, 1\n                  \u03b1] for any \u03be \u2208   ran(g\u2032). The relationship between \u2113g and Df follows from property\n(1.2). Note that when g\u2032 is [\u03b1, \u03b2] bi-Lipschitz, then f \u2032 is [ 1\u03b2, 1\n                                                                   \u03b1] bi-Lipschitz.\n    Theorem 2.1 follows by applying Lemma 2.2 appropriately to bound the error of a predictor p\nby its matching loss Lg(f \u2032 \u25e6p) and bound the matching6loss of the linear function corresponding to", "md": "# Math Equations and Definitions\n\n## Definitions:\n\nBregman Divergence: We also define the Bregman divergence associated with \\( f \\) to be \\( D_f(q, r) = f(q) - f(r) - (q - r)f'(r) \\), for any \\( q, r \\in \\text{ran}(g') \\). Note that \\( D_f(q, r) \\geq 0 \\) with equality if and only if \\( q = r \\).\n\nSIMs and GLMs as Concept Classes: For \\( B > 0 \\), we use SIMB to refer to the class of all SIMs of the form \\( x \\rightarrow g'(w \\cdot x) \\) where \\( \\|w\\|_2 \\leq B \\) and \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) is an arbitrary 1-Lipschitz monotone activation that is differentiable (except possibly at finitely many points). We define GLM\\( g', B \\) similarly except for the case where \\( g' \\) is fixed and known.\n\nCalibrated Multiaccuracy: A predictor \\( p : \\mathbb{R} \\rightarrow [0, 1] \\) is called \\( \\epsilon \\)-multiaccurate if for all \\( i \\in [d] \\), \\( |E[x_i(y - p(x))] | \\leq \\epsilon \\). It is called \\( \\epsilon \\)-calibrated if \\( |E_{p(x)}E_y[p(x)[y - p(x)]] | \\leq \\epsilon \\).\n\n## Distortion Bounds for the Matching Loss:\n\nIn this section, we propose a simple approach for bounding the squared error of a predictor that minimizes a (convex) matching loss, in the agnostic setting. We convert matching loss bounds to squared loss bounds in a generic way, through appropriate pointwise distortion bounds between the two losses. In particular, for a given matching loss \\( L_g \\), we transform guarantees on \\( L_g \\) that are competitive with the optimum linear minimizer of \\( L_g \\) to guarantees on the squared error that are competitive with the optimum GLM whose activation (\\( g' \\)) depends on the matching loss at hand.\n\n### Main Result:\n\nTheorem 2.1 (Squared Error Minimization through Matching Loss Minimization): Let \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times [0, 1] \\), let \\( 0 < \\alpha \\leq \\beta \\) and let \\( (f, g) \\) be a Fenchel-Legendre pair such that \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) is \\( [\\alpha, \\beta] \\) bi-Lipschitz. Suppose that for a predictor \\( p : \\mathbb{R}^d \\rightarrow \\text{ran}(g') \\) we have \\( L_g(f' \\circ p ; D) \\leq \\min \\)\n\n$$ \\beta \\quad \\text{opt(GLM}_{g', B}) + 2\\beta\\epsilon. $$\n\n### Lemma:\n\nLemma 2.2 (Pointwise Distortion Bound for bi-Lipschitz link functions): Let \\( 0 < \\alpha \\leq \\beta \\) and let \\( (f, g) \\) be a Fenchel-Legendre pair such that \\( f' : \\text{ran}(g') \\rightarrow \\mathbb{R} \\) is \\( [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) bi-Lipschitz. Then for any \\( y, p \\in \\text{ran}(g') \\) we have\n\n$$ \\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = D_f(y, p) \\in \\frac{1}{2\\beta}(y - p)^2, \\frac{1}{2\\alpha}(y - p)^2. $$\n\nIn the case that \\( f' \\) is differentiable on \\( (0, 1) \\), the proof of Lemma 2.2 follows from an application of Taylor's approximation theorem of degree 2 on the function \\( f \\), since the Bregman divergence \\( D_f(y, p) \\) is exactly equal to the error of the second degree Taylor's approximation of \\( f(y) \\) around \\( p \\) and \\( f''(\\xi) \\in [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) for any \\( \\xi \\in \\text{ran}(g') \\). The relationship between \\( \\ell_g \\) and \\( D_f \\) follows from property (1.2). Note that when \\( g' \\) is \\( [\\alpha, \\beta] \\) bi-Lipschitz, then \\( f' \\) is \\( [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) bi-Lipschitz.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Definitions", "md": "# Math Equations and Definitions"}, {"type": "heading", "lvl": 2, "value": "Definitions:", "md": "## Definitions:"}, {"type": "text", "value": "Bregman Divergence: We also define the Bregman divergence associated with \\( f \\) to be \\( D_f(q, r) = f(q) - f(r) - (q - r)f'(r) \\), for any \\( q, r \\in \\text{ran}(g') \\). Note that \\( D_f(q, r) \\geq 0 \\) with equality if and only if \\( q = r \\).\n\nSIMs and GLMs as Concept Classes: For \\( B > 0 \\), we use SIMB to refer to the class of all SIMs of the form \\( x \\rightarrow g'(w \\cdot x) \\) where \\( \\|w\\|_2 \\leq B \\) and \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) is an arbitrary 1-Lipschitz monotone activation that is differentiable (except possibly at finitely many points). We define GLM\\( g', B \\) similarly except for the case where \\( g' \\) is fixed and known.\n\nCalibrated Multiaccuracy: A predictor \\( p : \\mathbb{R} \\rightarrow [0, 1] \\) is called \\( \\epsilon \\)-multiaccurate if for all \\( i \\in [d] \\), \\( |E[x_i(y - p(x))] | \\leq \\epsilon \\). It is called \\( \\epsilon \\)-calibrated if \\( |E_{p(x)}E_y[p(x)[y - p(x)]] | \\leq \\epsilon \\).", "md": "Bregman Divergence: We also define the Bregman divergence associated with \\( f \\) to be \\( D_f(q, r) = f(q) - f(r) - (q - r)f'(r) \\), for any \\( q, r \\in \\text{ran}(g') \\). Note that \\( D_f(q, r) \\geq 0 \\) with equality if and only if \\( q = r \\).\n\nSIMs and GLMs as Concept Classes: For \\( B > 0 \\), we use SIMB to refer to the class of all SIMs of the form \\( x \\rightarrow g'(w \\cdot x) \\) where \\( \\|w\\|_2 \\leq B \\) and \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) is an arbitrary 1-Lipschitz monotone activation that is differentiable (except possibly at finitely many points). We define GLM\\( g', B \\) similarly except for the case where \\( g' \\) is fixed and known.\n\nCalibrated Multiaccuracy: A predictor \\( p : \\mathbb{R} \\rightarrow [0, 1] \\) is called \\( \\epsilon \\)-multiaccurate if for all \\( i \\in [d] \\), \\( |E[x_i(y - p(x))] | \\leq \\epsilon \\). It is called \\( \\epsilon \\)-calibrated if \\( |E_{p(x)}E_y[p(x)[y - p(x)]] | \\leq \\epsilon \\)."}, {"type": "heading", "lvl": 2, "value": "Distortion Bounds for the Matching Loss:", "md": "## Distortion Bounds for the Matching Loss:"}, {"type": "text", "value": "In this section, we propose a simple approach for bounding the squared error of a predictor that minimizes a (convex) matching loss, in the agnostic setting. We convert matching loss bounds to squared loss bounds in a generic way, through appropriate pointwise distortion bounds between the two losses. In particular, for a given matching loss \\( L_g \\), we transform guarantees on \\( L_g \\) that are competitive with the optimum linear minimizer of \\( L_g \\) to guarantees on the squared error that are competitive with the optimum GLM whose activation (\\( g' \\)) depends on the matching loss at hand.", "md": "In this section, we propose a simple approach for bounding the squared error of a predictor that minimizes a (convex) matching loss, in the agnostic setting. We convert matching loss bounds to squared loss bounds in a generic way, through appropriate pointwise distortion bounds between the two losses. In particular, for a given matching loss \\( L_g \\), we transform guarantees on \\( L_g \\) that are competitive with the optimum linear minimizer of \\( L_g \\) to guarantees on the squared error that are competitive with the optimum GLM whose activation (\\( g' \\)) depends on the matching loss at hand."}, {"type": "heading", "lvl": 3, "value": "Main Result:", "md": "### Main Result:"}, {"type": "text", "value": "Theorem 2.1 (Squared Error Minimization through Matching Loss Minimization): Let \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times [0, 1] \\), let \\( 0 < \\alpha \\leq \\beta \\) and let \\( (f, g) \\) be a Fenchel-Legendre pair such that \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) is \\( [\\alpha, \\beta] \\) bi-Lipschitz. Suppose that for a predictor \\( p : \\mathbb{R}^d \\rightarrow \\text{ran}(g') \\) we have \\( L_g(f' \\circ p ; D) \\leq \\min \\)\n\n$$ \\beta \\quad \\text{opt(GLM}_{g', B}) + 2\\beta\\epsilon. $$", "md": "Theorem 2.1 (Squared Error Minimization through Matching Loss Minimization): Let \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times [0, 1] \\), let \\( 0 < \\alpha \\leq \\beta \\) and let \\( (f, g) \\) be a Fenchel-Legendre pair such that \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) is \\( [\\alpha, \\beta] \\) bi-Lipschitz. Suppose that for a predictor \\( p : \\mathbb{R}^d \\rightarrow \\text{ran}(g') \\) we have \\( L_g(f' \\circ p ; D) \\leq \\min \\)\n\n$$ \\beta \\quad \\text{opt(GLM}_{g', B}) + 2\\beta\\epsilon. $$"}, {"type": "heading", "lvl": 3, "value": "Lemma:", "md": "### Lemma:"}, {"type": "text", "value": "Lemma 2.2 (Pointwise Distortion Bound for bi-Lipschitz link functions): Let \\( 0 < \\alpha \\leq \\beta \\) and let \\( (f, g) \\) be a Fenchel-Legendre pair such that \\( f' : \\text{ran}(g') \\rightarrow \\mathbb{R} \\) is \\( [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) bi-Lipschitz. Then for any \\( y, p \\in \\text{ran}(g') \\) we have\n\n$$ \\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = D_f(y, p) \\in \\frac{1}{2\\beta}(y - p)^2, \\frac{1}{2\\alpha}(y - p)^2. $$\n\nIn the case that \\( f' \\) is differentiable on \\( (0, 1) \\), the proof of Lemma 2.2 follows from an application of Taylor's approximation theorem of degree 2 on the function \\( f \\), since the Bregman divergence \\( D_f(y, p) \\) is exactly equal to the error of the second degree Taylor's approximation of \\( f(y) \\) around \\( p \\) and \\( f''(\\xi) \\in [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) for any \\( \\xi \\in \\text{ran}(g') \\). The relationship between \\( \\ell_g \\) and \\( D_f \\) follows from property (1.2). Note that when \\( g' \\) is \\( [\\alpha, \\beta] \\) bi-Lipschitz, then \\( f' \\) is \\( [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) bi-Lipschitz.", "md": "Lemma 2.2 (Pointwise Distortion Bound for bi-Lipschitz link functions): Let \\( 0 < \\alpha \\leq \\beta \\) and let \\( (f, g) \\) be a Fenchel-Legendre pair such that \\( f' : \\text{ran}(g') \\rightarrow \\mathbb{R} \\) is \\( [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) bi-Lipschitz. Then for any \\( y, p \\in \\text{ran}(g') \\) we have\n\n$$ \\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = D_f(y, p) \\in \\frac{1}{2\\beta}(y - p)^2, \\frac{1}{2\\alpha}(y - p)^2. $$\n\nIn the case that \\( f' \\) is differentiable on \\( (0, 1) \\), the proof of Lemma 2.2 follows from an application of Taylor's approximation theorem of degree 2 on the function \\( f \\), since the Bregman divergence \\( D_f(y, p) \\) is exactly equal to the error of the second degree Taylor's approximation of \\( f(y) \\) around \\( p \\) and \\( f''(\\xi) \\in [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) for any \\( \\xi \\in \\text{ran}(g') \\). The relationship between \\( \\ell_g \\) and \\( D_f \\) follows from property (1.2). Note that when \\( g' \\) is \\( [\\alpha, \\beta] \\) bi-Lipschitz, then \\( f' \\) is \\( [\\frac{1}{\\beta}, \\frac{1}{\\alpha}] \\) bi-Lipschitz."}]}, {"page": 7, "text": "w\u2217    by the squared error of g\u2032(w\u2217                \u00b7 x), where g\u2032(w\u2217          \u00b7 x) is the element of GLMg\u2032,B with minimum\nsquared error.\n      Although Theorem 2.1 only applies to bi-Lipschitz activations g\u2032, it has the advantage that the\nassumption it makes on p corresponds to a convex optimization problem and, when the marginal dis-\ntribution has certain concentration properties (for generalization), can be solved efficiently through\ngradient descent on the empirical loss function. As a consequence, for bi-Lipschitz activations we\ncan obtain O(opt) efficiently under mild distributional assumptions in the agnostic setting.\n3       Agnostically Learning Single-Index Models\nIn this section, we provide our main result on agnostically learning SIMs. We combine the distortion\nbounds we established in Section 2 with results from [GHK+23] on omniprediction, which can be\nused to learn a predictor p that satisfies the assumption of Theorem 2.1 simultaneously for all\nbi-Lipschitz activations.               By doing so, we obtain a result for all Lipschitz and non-decreasing\nactivations simultaneously.\nTheorem 3.1 (Agnostically Learning SIMs). Let D be a distribution over Rd \u00d7 [0, 1] with second\nmoments bounded by \u03bb. There is an algorithm that agnostically learns the class SIMB over D up\nto \u21132 error O(B         \u221a  \u03bb    opt(SIMB, D)) + \u01eb using time and sample complexity poly(d, B, \u03bb, 1                                   \u01eb ).\n      In order to apply Theorem 2.1, we use the following theorem which is a combination of results\nin [GHK+23], where they show that the matching losses corresponding to a wide class of functions\ncan all be minimized simultaneously by an efficiently computable predictor.\nTheorem 3.2 (Omnipredictors for Matching Losses, combination of results in [GHK+23]). Let D\nbe a distribution over Rd \u00d7[0, 1] whose marginal on Rd has \u03bb-bounded second moments. There is an\nalgorithm that, given sample access to D, with high probability returns a predictor p : R \u2192                                          (0, 1) with\nthe following guarantee. For any Fenchel-Legendre pair (f, g) such that g\u2032 : R \u2192                                          R is L-Lipschitz,\nand f \u2032 satisfi     es some mild boundedness conditions (see Defi                        nition C.1), p satisfi         es\n                                              Lg(f \u2032 \u25e6   p ; D) \u2264       min\nThe algorithm requires time and sample complexity poly(\u03bb, B, L, 1     \u2225w\u22252\u2264B Lg(w ; D) + \u01eb.       \u01eb ).\n      We aim to apply Theorem 3.2 to the class of all Lipschitz activations (which is wider than the\nclass of bi-Lipschitz activations). This is enabled by the following lemma, whose proof is based\non Theorem 2.1 and the fact that the error of a predictor is bounded by the sum of the error of\nanother predictor and the squared expected distance between the two predictors.\nLemma 3.3. Let D be a distribution over Rd \u00d7 [0, 1]. Let g\u2032 : R \u2192                                    R be some fi      xed activation, and\nf \u2032 its dual. Consider the class GLMg\u2032,B, and let w\u2217                          be the weights achieving opt(GLMg\u2032,B, D). Let\n\u03c6\u2032 : R \u2192      R be an [\u03b1, \u03b2] bi-Lipschitz function (differentiable except possibly at fi                              nitely many points)\nthat we wish to approximate g\u2032 by. Any predictor p : Rd \u2192                                R that satisfi     es\n                                              L\u03c6(f \u2032 \u25e6    p ; D) \u2264       min\nalso satisfi    es the following \u21132 error guarantee:                  \u2225w\u22252\u2264B L\u03c6(w ; D) + \u01eb\n                     err2(p) \u2264      2\u03b2                                        g\u2032(w\u2217    \u00b7 x) \u2212    \u03c6\u2032(w\u2217    \u00b7 x)  2    + 2\u03b2\u01eb.\n                                     \u03b1 opt(GLMg\u2032,B) + 2\u03b2           \u03b1 E\n                                                                         7", "md": "$$w^*$$ by the squared error of $$g'(w^* \\cdot x)$$, where $$g'(w^* \\cdot x)$$ is the element of GLMg',B with minimum squared error.\n\nAlthough Theorem 2.1 only applies to bi-Lipschitz activations $$g'$$, it has the advantage that the assumption it makes on $$p$$ corresponds to a convex optimization problem and, when the marginal distribution has certain concentration properties (for generalization), can be solved efficiently through gradient descent on the empirical loss function. As a consequence, for bi-Lipschitz activations we can obtain $$O(opt)$$ efficiently under mild distributional assumptions in the agnostic setting.\n\n### Agnostically Learning Single-Index Models\n\nIn this section, we provide our main result on agnostically learning SIMs. We combine the distortion bounds we established in Section 2 with results from [GHK+23] on omniprediction, which can be used to learn a predictor $$p$$ that satisfies the assumption of Theorem 2.1 simultaneously for all bi-Lipschitz activations. By doing so, we obtain a result for all Lipschitz and non-decreasing activations simultaneously.\n\nTheorem 3.1 (Agnostically Learning SIMs). Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ with second moments bounded by $$\\lambda$$. There is an algorithm that agnostically learns the class SIMB over $$D$$ up to $$\\ell^2$$ error $$O(B \\sqrt{\\lambda} \\cdot opt(SIMB, D)) + \\epsilon$$ using time and sample complexity $$poly(d, B, \\lambda, \\frac{1}{\\epsilon})$$.\n\nIn order to apply Theorem 2.1, we use the following theorem which is a combination of results in [GHK+23], where they show that the matching losses corresponding to a wide class of functions can all be minimized simultaneously by an efficiently computable predictor.\n\nTheorem 3.2 (Omnipredictors for Matching Losses, combination of results in [GHK+23]). Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has $$\\lambda$$-bounded second moments. There is an algorithm that, given sample access to $$D$$, with high probability returns a predictor $$p : \\mathbb{R} \\rightarrow (0, 1)$$ with the following guarantee. For any Fenchel-Legendre pair $$(f, g)$$ such that $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is $$L$$-Lipschitz, and $$f'$$ satisfies some mild boundedness conditions, $$p$$ satisfies\n\nThe algorithm requires time and sample complexity $$poly(\\lambda, B, L, \\frac{1}{\\epsilon})$$.\n\nWe aim to apply Theorem 3.2 to the class of all Lipschitz activations (which is wider than the class of bi-Lipschitz activations). This is enabled by the following lemma, whose proof is based on Theorem 2.1 and the fact that the error of a predictor is bounded by the sum of the error of another predictor and the squared expected distance between the two predictors.\n\nLemma 3.3. Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$. Let $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be some fixed activation, and $$f'$$ its dual. Consider the class GLMg',B, and let $$w^*$$ be the weights achieving opt(GLMg',B, D). Let $$\\varphi' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be an [$$\\alpha, \\beta$$] bi-Lipschitz function that we wish to approximate $$g'$$ by. Any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ that satisfies\n\nalso satisfies the following $$\\ell^2$$ error guarantee: $$\\|w\\|_2 \\leq B L\\varphi(w ; D) + \\epsilon$$\n\n$$err^2(p) \\leq 2\\beta \\frac{g'(w^* \\cdot x) - \\varphi'(w^* \\cdot x)^2}{\\alpha \\cdot opt(GLMg',B)} + 2\\beta\\epsilon.$$", "images": [], "items": [{"type": "text", "value": "$$w^*$$ by the squared error of $$g'(w^* \\cdot x)$$, where $$g'(w^* \\cdot x)$$ is the element of GLMg',B with minimum squared error.\n\nAlthough Theorem 2.1 only applies to bi-Lipschitz activations $$g'$$, it has the advantage that the assumption it makes on $$p$$ corresponds to a convex optimization problem and, when the marginal distribution has certain concentration properties (for generalization), can be solved efficiently through gradient descent on the empirical loss function. As a consequence, for bi-Lipschitz activations we can obtain $$O(opt)$$ efficiently under mild distributional assumptions in the agnostic setting.", "md": "$$w^*$$ by the squared error of $$g'(w^* \\cdot x)$$, where $$g'(w^* \\cdot x)$$ is the element of GLMg',B with minimum squared error.\n\nAlthough Theorem 2.1 only applies to bi-Lipschitz activations $$g'$$, it has the advantage that the assumption it makes on $$p$$ corresponds to a convex optimization problem and, when the marginal distribution has certain concentration properties (for generalization), can be solved efficiently through gradient descent on the empirical loss function. As a consequence, for bi-Lipschitz activations we can obtain $$O(opt)$$ efficiently under mild distributional assumptions in the agnostic setting."}, {"type": "heading", "lvl": 3, "value": "Agnostically Learning Single-Index Models", "md": "### Agnostically Learning Single-Index Models"}, {"type": "text", "value": "In this section, we provide our main result on agnostically learning SIMs. We combine the distortion bounds we established in Section 2 with results from [GHK+23] on omniprediction, which can be used to learn a predictor $$p$$ that satisfies the assumption of Theorem 2.1 simultaneously for all bi-Lipschitz activations. By doing so, we obtain a result for all Lipschitz and non-decreasing activations simultaneously.\n\nTheorem 3.1 (Agnostically Learning SIMs). Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ with second moments bounded by $$\\lambda$$. There is an algorithm that agnostically learns the class SIMB over $$D$$ up to $$\\ell^2$$ error $$O(B \\sqrt{\\lambda} \\cdot opt(SIMB, D)) + \\epsilon$$ using time and sample complexity $$poly(d, B, \\lambda, \\frac{1}{\\epsilon})$$.\n\nIn order to apply Theorem 2.1, we use the following theorem which is a combination of results in [GHK+23], where they show that the matching losses corresponding to a wide class of functions can all be minimized simultaneously by an efficiently computable predictor.\n\nTheorem 3.2 (Omnipredictors for Matching Losses, combination of results in [GHK+23]). Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has $$\\lambda$$-bounded second moments. There is an algorithm that, given sample access to $$D$$, with high probability returns a predictor $$p : \\mathbb{R} \\rightarrow (0, 1)$$ with the following guarantee. For any Fenchel-Legendre pair $$(f, g)$$ such that $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is $$L$$-Lipschitz, and $$f'$$ satisfies some mild boundedness conditions, $$p$$ satisfies\n\nThe algorithm requires time and sample complexity $$poly(\\lambda, B, L, \\frac{1}{\\epsilon})$$.\n\nWe aim to apply Theorem 3.2 to the class of all Lipschitz activations (which is wider than the class of bi-Lipschitz activations). This is enabled by the following lemma, whose proof is based on Theorem 2.1 and the fact that the error of a predictor is bounded by the sum of the error of another predictor and the squared expected distance between the two predictors.\n\nLemma 3.3. Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$. Let $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be some fixed activation, and $$f'$$ its dual. Consider the class GLMg',B, and let $$w^*$$ be the weights achieving opt(GLMg',B, D). Let $$\\varphi' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be an [$$\\alpha, \\beta$$] bi-Lipschitz function that we wish to approximate $$g'$$ by. Any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ that satisfies\n\nalso satisfies the following $$\\ell^2$$ error guarantee: $$\\|w\\|_2 \\leq B L\\varphi(w ; D) + \\epsilon$$\n\n$$err^2(p) \\leq 2\\beta \\frac{g'(w^* \\cdot x) - \\varphi'(w^* \\cdot x)^2}{\\alpha \\cdot opt(GLMg',B)} + 2\\beta\\epsilon.$$", "md": "In this section, we provide our main result on agnostically learning SIMs. We combine the distortion bounds we established in Section 2 with results from [GHK+23] on omniprediction, which can be used to learn a predictor $$p$$ that satisfies the assumption of Theorem 2.1 simultaneously for all bi-Lipschitz activations. By doing so, we obtain a result for all Lipschitz and non-decreasing activations simultaneously.\n\nTheorem 3.1 (Agnostically Learning SIMs). Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ with second moments bounded by $$\\lambda$$. There is an algorithm that agnostically learns the class SIMB over $$D$$ up to $$\\ell^2$$ error $$O(B \\sqrt{\\lambda} \\cdot opt(SIMB, D)) + \\epsilon$$ using time and sample complexity $$poly(d, B, \\lambda, \\frac{1}{\\epsilon})$$.\n\nIn order to apply Theorem 2.1, we use the following theorem which is a combination of results in [GHK+23], where they show that the matching losses corresponding to a wide class of functions can all be minimized simultaneously by an efficiently computable predictor.\n\nTheorem 3.2 (Omnipredictors for Matching Losses, combination of results in [GHK+23]). Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has $$\\lambda$$-bounded second moments. There is an algorithm that, given sample access to $$D$$, with high probability returns a predictor $$p : \\mathbb{R} \\rightarrow (0, 1)$$ with the following guarantee. For any Fenchel-Legendre pair $$(f, g)$$ such that $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is $$L$$-Lipschitz, and $$f'$$ satisfies some mild boundedness conditions, $$p$$ satisfies\n\nThe algorithm requires time and sample complexity $$poly(\\lambda, B, L, \\frac{1}{\\epsilon})$$.\n\nWe aim to apply Theorem 3.2 to the class of all Lipschitz activations (which is wider than the class of bi-Lipschitz activations). This is enabled by the following lemma, whose proof is based on Theorem 2.1 and the fact that the error of a predictor is bounded by the sum of the error of another predictor and the squared expected distance between the two predictors.\n\nLemma 3.3. Let $$D$$ be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$. Let $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be some fixed activation, and $$f'$$ its dual. Consider the class GLMg',B, and let $$w^*$$ be the weights achieving opt(GLMg',B, D). Let $$\\varphi' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ be an [$$\\alpha, \\beta$$] bi-Lipschitz function that we wish to approximate $$g'$$ by. Any predictor $$p : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ that satisfies\n\nalso satisfies the following $$\\ell^2$$ error guarantee: $$\\|w\\|_2 \\leq B L\\varphi(w ; D) + \\epsilon$$\n\n$$err^2(p) \\leq 2\\beta \\frac{g'(w^* \\cdot x) - \\varphi'(w^* \\cdot x)^2}{\\alpha \\cdot opt(GLMg',B)} + 2\\beta\\epsilon.$$"}]}, {"page": 8, "text": "     By combining Theorem 3.2 with Lemma 3.3 (whose proofs can be found in Appendix C), we\nare now ready to prove our main theorem.\nProof of Theorem 3.1. We will combine Theorem 3.2, which states that there is an efficient algo-\nrithm that simultaneously minimizes the matching losses corresponding to bounded, non-decreasing\nand Lipschitz activations, with Lemma 3.3, which implies that minimizing the matching loss cor-\nresponding to the nearest bi-Lipschitz activation is sufficient to obtain small error. Note that we\nmay assume that \u01eb < 1/2, since otherwise the problem is trivial (output the zero function and pick\nC = 2).\n     As a first step, we show that link functions corresponding to bi-Lipschitz activations are bounded\n(according to Definition C.1, for \u03b3 = 0). In particular, let \u03c6\u2032 : R \u2192                                   R be an [\u03b1, \u03b2] bi-Lipschitz\nactivation for some \u03b2 \u2265             \u03b1 > 0 such that \u03c6\u2032(s) \u2208              [\u22121, 2] for some s \u2208          R and let \u03c8\u2032 be the inverse\nof \u03c6\u2032 (\u03c6\u2032 is invertible since it is strictly increasing). We will show that \u03c8\u2032(r) \u2208                                     [\u2212R, R] for any\nr \u2208  [0, 1], for some R = O(|s| + 1/\u03b1).                                                                1                         2\n     We pick r0 = 0, r1 = 1 and have that |\u03c8\u2032(\u03c6\u2032(s)) \u2212                                 \u03c8\u2032(r0)| \u2264       \u03b1|\u03c6\u2032(s) \u2212      r0| \u2264      \u03b1.    Hence\n\u03c8\u2032(r0) \u2265     \u03c8\u2032(\u03c6\u2032(s))\u2212      2\n                             \u03b1 = s\u2212      1\n[\u2212|s| \u2212    2                             \u03b1. Similarly, we have \u03c8\u2032(r1) \u2264              s+ 2 \u03b1. Therefore, \u03c8\u2032(r) \u2208           [\u03c8\u2032(0), \u03c8\u2032(1)] \u2286\n           \u03b1, |s| + 2  \u03b1], for any r \u2208       [0, 1], due to monotonicity of \u03c8\u2032.\n     For a given non-decreasing and 1-Lipschitz g\u2032, we will now show that there is a bounded bi-\nLipschitz activation \u03c6\u2032 such that if the assumption of Lemma 3.3 is satisfied for \u03c6\u2032 by a predictor\np, then the error of p is bounded by\n                                             err2(p) \u2264     O(B    \u221a  \u03bb optg) + O(\u03bbB2\u01eb)\nSuppose, first, that opt           g \u2264    \u01eb2. Then, we pick \u03c6\u2032(t) = g\u2032(t) + \u01ebt.                      Note that \u03c6\u2032 is [\u01eb, 1 + \u01eb] bi-\nLipschitz.       Moreover, since opt           g \u2264    \u01eb2, we must have some s \u2208                   R with |s| \u2264          2\u03bbB2 such that\ng\u2032(s) \u2208    [\u22121, 2]. Otherwise, optg \u2265              P[|w\u2217    \u00b7 x| \u2264    2\u03bbB2] = 1 \u2212         P[|w\u2217    \u00b7 x| > 2\u03bbB2] \u2265          1\nChebyshev\u2019s inequality, the fact that w\u2217                     \u2208  W and the bounded moments assumption. Therefore,           4 > \u01eb2, due to\n\u03c8\u2032 is (R = 2\u03bbB2 + 2\u01eb , \u03b3 = 0)-bounded and we have\n                              E      g\u2032(w\u2217    \u00b7 x) \u2212   \u03c6\u2032(w\u2217    \u00b7 x)  2    \u2264   \u01eb2 E[(w\u2217     \u00b7 x)2] \u2264    \u01eb2\u03bbB2\nAs a consequence, under the assumption of Lemma 3.3 for \u03c6\u2032, the error of the corresponding\npredictor p is err2(p) \u2264           2(1 + \u01eb)\u01eb + 2(1 + \u01eb)\u01eb\u03bbB2 + 2(1 + \u01eb)\u01eb = O(\u03bbB2\u01eb). \u221aoptg\n     In the case that opt          g > \u01eb2, we pick \u03c6\u2032(t) = g\u2032(t) + t                B \u221a  \u03bb . We may also assume that opt                  g \u2264\n1/2, since otherwise any predictor with range [0, 1] will have error at most 2opt                                         g.   Then, \u03c6\u2032 is\n[B1 optg/      \u221a  \u03bb, 1 + 1 B ] bi-Lipschitz which gives\n                             E      g\u2032(w\u2217    \u00b7 x) \u2212   \u03c6\u2032(w\u2217    \u00b7 x)  2    \u2264   optg\n                                                                              B2\u03bb E[(w\u2217         \u00b7 x)2] \u2264    optg\nAs a consequence, under the assumption of Lemma 3.3 for \u03c6\u2032, the error of the corresponding\npredictor p is err2(p) \u2264          4(1 + 1   B)B   \u221a  \u03bb optg + 2(1 + 1         B)\u01eb. Using a similar approach as for the case\nopt g \u2264   \u01eb, we can show that \u03c8\u2032 is polynomially bounded (as per Definition C.1), since optg \u2264                                           1\n                                                                                                                                         2.\n     To conclude the proof of Theorem 3.1, we apply Theorem 3.2 with appropriate (polynomial)\nchoice of parameters, to show that there is an efficient algorithm that outputs a predictor p : Rd \u2192\n(0, 1) for which the assumption of Lemma 3.3 holds simultaneously for all bi-Lipschitz activations\n(\u03c6\u2032) with sufficiently bounded inverses (\u03c8\u2032).\n                                                                       8", "md": "By combining Theorem 3.2 with Lemma 3.3 (whose proofs can be found in Appendix C), we are now ready to prove our main theorem.\n\nProof of Theorem 3.1. We will combine Theorem 3.2, which states that there is an efficient algorithm that simultaneously minimizes the matching losses corresponding to bounded, non-decreasing and Lipschitz activations, with Lemma 3.3, which implies that minimizing the matching loss corresponding to the nearest bi-Lipschitz activation is sufficient to obtain small error. Note that we may assume that \\( \\epsilon < \\frac{1}{2} \\), since otherwise the problem is trivial (output the zero function and pick \\( C = 2 \\)).\n\nAs a first step, we show that link functions corresponding to bi-Lipschitz activations are bounded (according to Definition C.1, for \\( \\gamma = 0 \\)). In particular, let \\( \\phi' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be an \\([ \\alpha, \\beta ]\\) bi-Lipschitz activation for some \\( \\beta \\geq \\alpha > 0 \\) such that \\( \\phi'(s) \\in [ -1, 2 ] \\) for some \\( s \\in \\mathbb{R} \\) and let \\( \\psi' \\) be the inverse of \\( \\phi' \\) (\\( \\phi' \\) is invertible since it is strictly increasing). We will show that \\( \\psi'(r) \\in [ -R, R ] \\) for any \\( r \\in [ 0, 1 ] \\), for some \\( R = O(|s| + \\frac{1}{\\alpha}) \\).\n\nWe pick \\( r_0 = 0 \\), \\( r_1 = 1 \\) and have that \\( | \\psi'(\\phi'(s)) - \\psi'(r_0) | \\leq \\alpha | \\phi'(s) - r_0 | \\leq \\alpha \\). Hence \\( \\psi'(r_0) \\geq \\psi'(\\phi'(s)) - \\frac{\\alpha}{2} = s - \\frac{1}{|s| + \\frac{1}{2\\alpha}} \\). Similarly, we have \\( \\psi'(r_1) \\leq s + \\frac{1}{2\\alpha} \\). Therefore, \\( \\psi'(r) \\in [ \\psi'(0), \\psi'(1) ] \\subseteq [ -|s| + \\frac{1}{2\\alpha}, |s| + \\frac{1}{2\\alpha} ] \\), for any \\( r \\in [ 0, 1 ] \\), due to monotonicity of \\( \\psi' \\).\n\nFor a given non-decreasing and 1-Lipschitz \\( g' \\), we will now show that there is a bounded bi-Lipschitz activation \\( \\phi' \\) such that if the assumption of Lemma 3.3 is satisfied for \\( \\phi' \\) by a predictor \\( p \\), then the error of \\( p \\) is bounded by\n\n\\[\n\\text{err}^2(p) \\leq O(B \\sqrt{\\lambda} \\text{opt} g) + O(\\lambda B^2 \\epsilon)\n\\]\n\nSuppose, first, that \\( \\text{opt} g \\leq \\epsilon^2 \\). Then, we pick \\( \\phi'(t) = g'(t) + \\epsilon t \\). Note that \\( \\phi' \\) is \\([ \\epsilon, 1 + \\epsilon ]\\) bi-Lipschitz. Moreover, since \\( \\text{opt} g \\leq \\epsilon^2 \\), we must have some \\( s \\in \\mathbb{R} \\) with \\( |s| \\leq 2\\lambda B^2 \\) such that \\( g'(s) \\in [ -1, 2 ] \\). Otherwise, \\( \\text{opt} g \\geq \\mathbb{P}[ |w^* \\cdot x| \\leq 2\\lambda B^2 ] = 1 - \\mathbb{P}[ |w^* \\cdot x| > 2\\lambda B^2 ] \\geq 1 - \\frac{1}{4} \\) by Chebyshev\u2019s inequality, the fact that \\( w^* \\in W \\) and the bounded moments assumption. Therefore, \\( 4 > \\epsilon^2 \\), due to \\( \\psi' \\) is (\\( R = 2\\lambda B^2 + 2\\epsilon \\), \\( \\gamma = 0 \\))-bounded and we have\n\n\\[\n\\begin{aligned}\nE [ g'(w^* \\cdot x) - \\phi'(w^* \\cdot x) ]^2 & \\leq \\epsilon^2 E[(w^* \\cdot x)^2] \\\\\n& \\leq \\epsilon^2 \\lambda B^2\n\\end{aligned}\n\\]\n\nAs a consequence, under the assumption of Lemma 3.3 for \\( \\phi' \\), the error of the corresponding predictor \\( p \\) is \\( \\text{err}^2(p) \\leq 2(1 + \\epsilon) \\epsilon + 2(1 + \\epsilon) \\epsilon \\lambda B^2 + 2(1 + \\epsilon) \\epsilon = O(\\lambda B^2 \\epsilon \\sqrt{\\text{opt} g}) \\).\n\nIn the case that \\( \\text{opt} g > \\epsilon^2 \\), we pick \\( \\phi'(t) = g'(t) + t \\frac{B \\sqrt{\\lambda}}{\\sqrt{}} \\). We may also assume that \\( \\text{opt} g \\leq \\frac{1}{2} \\), since otherwise any predictor with range [0, 1] will have error at most \\( 2 \\text{opt} g \\). Then, \\( \\phi' \\) is \\([ B \\sqrt{\\text{opt} g} / \\sqrt{\\lambda}, 1 + \\frac{1}{B} ]\\) bi-Lipschitz which gives\n\n\\[\nE [ g'(w^* \\cdot x) - \\phi'(w^* \\cdot x) ]^2 \\leq \\text{opt} g B^2 \\lambda E[(w^* \\cdot x)^2] \\leq \\text{opt} g\n\\]\n\nAs a consequence, under the assumption of Lemma 3.3 for \\( \\phi' \\), the error of the corresponding predictor \\( p \\) is\n\n\\[\n\\text{err}^2(p) \\leq 4(1 + \\frac{1}{B}) B \\sqrt{\\lambda \\text{opt} g} + 2(1 + \\frac{1}{B}) \\epsilon\n\\]\n\nUsing a similar approach as for the case \\( \\text{opt} g \\leq \\epsilon \\), we can show that \\( \\psi' \\) is polynomially bounded (as per Definition C.1), since \\( \\text{opt} g \\leq \\frac{1}{2} \\).\n\nTo conclude the proof of Theorem 3.1, we apply Theorem 3.2 with appropriate (polynomial) choice of parameters, to show that there is an efficient algorithm that outputs a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) for which the assumption of Lemma 3.3 holds simultaneously for all bi-Lipschitz activations (\\( \\phi' \\)) with sufficiently bounded inverses (\\( \\psi' \\)).", "images": [], "items": [{"type": "text", "value": "By combining Theorem 3.2 with Lemma 3.3 (whose proofs can be found in Appendix C), we are now ready to prove our main theorem.\n\nProof of Theorem 3.1. We will combine Theorem 3.2, which states that there is an efficient algorithm that simultaneously minimizes the matching losses corresponding to bounded, non-decreasing and Lipschitz activations, with Lemma 3.3, which implies that minimizing the matching loss corresponding to the nearest bi-Lipschitz activation is sufficient to obtain small error. Note that we may assume that \\( \\epsilon < \\frac{1}{2} \\), since otherwise the problem is trivial (output the zero function and pick \\( C = 2 \\)).\n\nAs a first step, we show that link functions corresponding to bi-Lipschitz activations are bounded (according to Definition C.1, for \\( \\gamma = 0 \\)). In particular, let \\( \\phi' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be an \\([ \\alpha, \\beta ]\\) bi-Lipschitz activation for some \\( \\beta \\geq \\alpha > 0 \\) such that \\( \\phi'(s) \\in [ -1, 2 ] \\) for some \\( s \\in \\mathbb{R} \\) and let \\( \\psi' \\) be the inverse of \\( \\phi' \\) (\\( \\phi' \\) is invertible since it is strictly increasing). We will show that \\( \\psi'(r) \\in [ -R, R ] \\) for any \\( r \\in [ 0, 1 ] \\), for some \\( R = O(|s| + \\frac{1}{\\alpha}) \\).\n\nWe pick \\( r_0 = 0 \\), \\( r_1 = 1 \\) and have that \\( | \\psi'(\\phi'(s)) - \\psi'(r_0) | \\leq \\alpha | \\phi'(s) - r_0 | \\leq \\alpha \\). Hence \\( \\psi'(r_0) \\geq \\psi'(\\phi'(s)) - \\frac{\\alpha}{2} = s - \\frac{1}{|s| + \\frac{1}{2\\alpha}} \\). Similarly, we have \\( \\psi'(r_1) \\leq s + \\frac{1}{2\\alpha} \\). Therefore, \\( \\psi'(r) \\in [ \\psi'(0), \\psi'(1) ] \\subseteq [ -|s| + \\frac{1}{2\\alpha}, |s| + \\frac{1}{2\\alpha} ] \\), for any \\( r \\in [ 0, 1 ] \\), due to monotonicity of \\( \\psi' \\).\n\nFor a given non-decreasing and 1-Lipschitz \\( g' \\), we will now show that there is a bounded bi-Lipschitz activation \\( \\phi' \\) such that if the assumption of Lemma 3.3 is satisfied for \\( \\phi' \\) by a predictor \\( p \\), then the error of \\( p \\) is bounded by\n\n\\[\n\\text{err}^2(p) \\leq O(B \\sqrt{\\lambda} \\text{opt} g) + O(\\lambda B^2 \\epsilon)\n\\]\n\nSuppose, first, that \\( \\text{opt} g \\leq \\epsilon^2 \\). Then, we pick \\( \\phi'(t) = g'(t) + \\epsilon t \\). Note that \\( \\phi' \\) is \\([ \\epsilon, 1 + \\epsilon ]\\) bi-Lipschitz. Moreover, since \\( \\text{opt} g \\leq \\epsilon^2 \\), we must have some \\( s \\in \\mathbb{R} \\) with \\( |s| \\leq 2\\lambda B^2 \\) such that \\( g'(s) \\in [ -1, 2 ] \\). Otherwise, \\( \\text{opt} g \\geq \\mathbb{P}[ |w^* \\cdot x| \\leq 2\\lambda B^2 ] = 1 - \\mathbb{P}[ |w^* \\cdot x| > 2\\lambda B^2 ] \\geq 1 - \\frac{1}{4} \\) by Chebyshev\u2019s inequality, the fact that \\( w^* \\in W \\) and the bounded moments assumption. Therefore, \\( 4 > \\epsilon^2 \\), due to \\( \\psi' \\) is (\\( R = 2\\lambda B^2 + 2\\epsilon \\), \\( \\gamma = 0 \\))-bounded and we have\n\n\\[\n\\begin{aligned}\nE [ g'(w^* \\cdot x) - \\phi'(w^* \\cdot x) ]^2 & \\leq \\epsilon^2 E[(w^* \\cdot x)^2] \\\\\n& \\leq \\epsilon^2 \\lambda B^2\n\\end{aligned}\n\\]\n\nAs a consequence, under the assumption of Lemma 3.3 for \\( \\phi' \\), the error of the corresponding predictor \\( p \\) is \\( \\text{err}^2(p) \\leq 2(1 + \\epsilon) \\epsilon + 2(1 + \\epsilon) \\epsilon \\lambda B^2 + 2(1 + \\epsilon) \\epsilon = O(\\lambda B^2 \\epsilon \\sqrt{\\text{opt} g}) \\).\n\nIn the case that \\( \\text{opt} g > \\epsilon^2 \\), we pick \\( \\phi'(t) = g'(t) + t \\frac{B \\sqrt{\\lambda}}{\\sqrt{}} \\). We may also assume that \\( \\text{opt} g \\leq \\frac{1}{2} \\), since otherwise any predictor with range [0, 1] will have error at most \\( 2 \\text{opt} g \\). Then, \\( \\phi' \\) is \\([ B \\sqrt{\\text{opt} g} / \\sqrt{\\lambda}, 1 + \\frac{1}{B} ]\\) bi-Lipschitz which gives\n\n\\[\nE [ g'(w^* \\cdot x) - \\phi'(w^* \\cdot x) ]^2 \\leq \\text{opt} g B^2 \\lambda E[(w^* \\cdot x)^2] \\leq \\text{opt} g\n\\]\n\nAs a consequence, under the assumption of Lemma 3.3 for \\( \\phi' \\), the error of the corresponding predictor \\( p \\) is\n\n\\[\n\\text{err}^2(p) \\leq 4(1 + \\frac{1}{B}) B \\sqrt{\\lambda \\text{opt} g} + 2(1 + \\frac{1}{B}) \\epsilon\n\\]\n\nUsing a similar approach as for the case \\( \\text{opt} g \\leq \\epsilon \\), we can show that \\( \\psi' \\) is polynomially bounded (as per Definition C.1), since \\( \\text{opt} g \\leq \\frac{1}{2} \\).\n\nTo conclude the proof of Theorem 3.1, we apply Theorem 3.2 with appropriate (polynomial) choice of parameters, to show that there is an efficient algorithm that outputs a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) for which the assumption of Lemma 3.3 holds simultaneously for all bi-Lipschitz activations (\\( \\phi' \\)) with sufficiently bounded inverses (\\( \\psi' \\)).", "md": "By combining Theorem 3.2 with Lemma 3.3 (whose proofs can be found in Appendix C), we are now ready to prove our main theorem.\n\nProof of Theorem 3.1. We will combine Theorem 3.2, which states that there is an efficient algorithm that simultaneously minimizes the matching losses corresponding to bounded, non-decreasing and Lipschitz activations, with Lemma 3.3, which implies that minimizing the matching loss corresponding to the nearest bi-Lipschitz activation is sufficient to obtain small error. Note that we may assume that \\( \\epsilon < \\frac{1}{2} \\), since otherwise the problem is trivial (output the zero function and pick \\( C = 2 \\)).\n\nAs a first step, we show that link functions corresponding to bi-Lipschitz activations are bounded (according to Definition C.1, for \\( \\gamma = 0 \\)). In particular, let \\( \\phi' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be an \\([ \\alpha, \\beta ]\\) bi-Lipschitz activation for some \\( \\beta \\geq \\alpha > 0 \\) such that \\( \\phi'(s) \\in [ -1, 2 ] \\) for some \\( s \\in \\mathbb{R} \\) and let \\( \\psi' \\) be the inverse of \\( \\phi' \\) (\\( \\phi' \\) is invertible since it is strictly increasing). We will show that \\( \\psi'(r) \\in [ -R, R ] \\) for any \\( r \\in [ 0, 1 ] \\), for some \\( R = O(|s| + \\frac{1}{\\alpha}) \\).\n\nWe pick \\( r_0 = 0 \\), \\( r_1 = 1 \\) and have that \\( | \\psi'(\\phi'(s)) - \\psi'(r_0) | \\leq \\alpha | \\phi'(s) - r_0 | \\leq \\alpha \\). Hence \\( \\psi'(r_0) \\geq \\psi'(\\phi'(s)) - \\frac{\\alpha}{2} = s - \\frac{1}{|s| + \\frac{1}{2\\alpha}} \\). Similarly, we have \\( \\psi'(r_1) \\leq s + \\frac{1}{2\\alpha} \\). Therefore, \\( \\psi'(r) \\in [ \\psi'(0), \\psi'(1) ] \\subseteq [ -|s| + \\frac{1}{2\\alpha}, |s| + \\frac{1}{2\\alpha} ] \\), for any \\( r \\in [ 0, 1 ] \\), due to monotonicity of \\( \\psi' \\).\n\nFor a given non-decreasing and 1-Lipschitz \\( g' \\), we will now show that there is a bounded bi-Lipschitz activation \\( \\phi' \\) such that if the assumption of Lemma 3.3 is satisfied for \\( \\phi' \\) by a predictor \\( p \\), then the error of \\( p \\) is bounded by\n\n\\[\n\\text{err}^2(p) \\leq O(B \\sqrt{\\lambda} \\text{opt} g) + O(\\lambda B^2 \\epsilon)\n\\]\n\nSuppose, first, that \\( \\text{opt} g \\leq \\epsilon^2 \\). Then, we pick \\( \\phi'(t) = g'(t) + \\epsilon t \\). Note that \\( \\phi' \\) is \\([ \\epsilon, 1 + \\epsilon ]\\) bi-Lipschitz. Moreover, since \\( \\text{opt} g \\leq \\epsilon^2 \\), we must have some \\( s \\in \\mathbb{R} \\) with \\( |s| \\leq 2\\lambda B^2 \\) such that \\( g'(s) \\in [ -1, 2 ] \\). Otherwise, \\( \\text{opt} g \\geq \\mathbb{P}[ |w^* \\cdot x| \\leq 2\\lambda B^2 ] = 1 - \\mathbb{P}[ |w^* \\cdot x| > 2\\lambda B^2 ] \\geq 1 - \\frac{1}{4} \\) by Chebyshev\u2019s inequality, the fact that \\( w^* \\in W \\) and the bounded moments assumption. Therefore, \\( 4 > \\epsilon^2 \\), due to \\( \\psi' \\) is (\\( R = 2\\lambda B^2 + 2\\epsilon \\), \\( \\gamma = 0 \\))-bounded and we have\n\n\\[\n\\begin{aligned}\nE [ g'(w^* \\cdot x) - \\phi'(w^* \\cdot x) ]^2 & \\leq \\epsilon^2 E[(w^* \\cdot x)^2] \\\\\n& \\leq \\epsilon^2 \\lambda B^2\n\\end{aligned}\n\\]\n\nAs a consequence, under the assumption of Lemma 3.3 for \\( \\phi' \\), the error of the corresponding predictor \\( p \\) is \\( \\text{err}^2(p) \\leq 2(1 + \\epsilon) \\epsilon + 2(1 + \\epsilon) \\epsilon \\lambda B^2 + 2(1 + \\epsilon) \\epsilon = O(\\lambda B^2 \\epsilon \\sqrt{\\text{opt} g}) \\).\n\nIn the case that \\( \\text{opt} g > \\epsilon^2 \\), we pick \\( \\phi'(t) = g'(t) + t \\frac{B \\sqrt{\\lambda}}{\\sqrt{}} \\). We may also assume that \\( \\text{opt} g \\leq \\frac{1}{2} \\), since otherwise any predictor with range [0, 1] will have error at most \\( 2 \\text{opt} g \\). Then, \\( \\phi' \\) is \\([ B \\sqrt{\\text{opt} g} / \\sqrt{\\lambda}, 1 + \\frac{1}{B} ]\\) bi-Lipschitz which gives\n\n\\[\nE [ g'(w^* \\cdot x) - \\phi'(w^* \\cdot x) ]^2 \\leq \\text{opt} g B^2 \\lambda E[(w^* \\cdot x)^2] \\leq \\text{opt} g\n\\]\n\nAs a consequence, under the assumption of Lemma 3.3 for \\( \\phi' \\), the error of the corresponding predictor \\( p \\) is\n\n\\[\n\\text{err}^2(p) \\leq 4(1 + \\frac{1}{B}) B \\sqrt{\\lambda \\text{opt} g} + 2(1 + \\frac{1}{B}) \\epsilon\n\\]\n\nUsing a similar approach as for the case \\( \\text{opt} g \\leq \\epsilon \\), we can show that \\( \\psi' \\) is polynomially bounded (as per Definition C.1), since \\( \\text{opt} g \\leq \\frac{1}{2} \\).\n\nTo conclude the proof of Theorem 3.1, we apply Theorem 3.2 with appropriate (polynomial) choice of parameters, to show that there is an efficient algorithm that outputs a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) for which the assumption of Lemma 3.3 holds simultaneously for all bi-Lipschitz activations (\\( \\phi' \\)) with sufficiently bounded inverses (\\( \\psi' \\))."}]}, {"page": 9, "text": "4      Stronger Guarantees for Logistic Regression\nIn this section, we follow the same recipe we used in Section 2 to obtain distortion bounds similar\nto Theorem 2.1 for the sigmoid activation (or, equivalently, the logistic model) under the assump-\ntion that the marginal distribution is sufficiently concentrated (see Definition C.1). In particular,\nTheorem 2.1 does not hold, since the sigmoid is not bi-Lipschitz and our main Theorem 3.1 only\nprovides a guarantee of O(\u221aopt) for squared error. We use appropriate pointwise distortion bounds\nfor the matching loss corresponding to the sigmoid activation and provide guarantees of                       O(opt) for\nlogistic regression with respect to both squared and absolute error, under appropriate assumptions\nabout the concentration of the marginal distribution. The proofs of this section are provided in\nAppendix D.\n     For the logistic model, the link function f \u2032 is defined as f \u2032(r) = ln(1\u2212r), for r \u2208 r              (0, 1) and the\n                                                                   1\ncorresponding activation g\u2032 is the sigmoid g\u2032(t) =               1+e\u2212t for t \u2208    R. The corresponding matching\nloss is the logistic loss.\nSquared error.          We first provide a result for squared loss minimization. In comparison to Theo-\nrem 2.1, the qualitative interpretation of the following theorem is that, while the sigmoid activation\nis not bi-Lipschitz, it is effectively bi-Lipschitz under sufficiently concentrated marginals.\nTheorem 4.1 (Squared Loss Minimization through Logistic Loss Minimization). Let D be a dis-\ntribution over Rd \u00d7[0, 1] whose marginal on Rd is (1, 2)-concentrated. Let g\u2032 : R \u2192                    R be the sigmoid\nactivation, i.e., g\u2032(t) = (1 + e\u2212t)\u22121 for t \u2208         R. Assume that for some B > 0, \u01eb > 0 and a predictor\np : Rd \u2192    (0, 1) we have           Lg(f \u2032 \u25e6  p ; D) \u2264      min                                                     (4.1)\n                                                         w:\u2225w\u22252\u2264B Lg(w ; D) + \u01eb\nIf we let opt   g = min\u2225w\u22252\u2264B err2(g\u2032      w), then for the predictor p and some universal constant C > 0\nwe also have                   err2(p) \u2264   C optg exp      B2 +      B2 log     1     + 2\u01eb.\n                                                                              opt g\n     In particular, the squared error of p is upper bounded by                   O(opt   g), since the function t  \u2192\nexp(log1/2 t) is asymptotically smaller than any polynomial function t  \u2192                   t\u03b3 with \u03b3 > 0.\n     Once more, the proof of our result is based on an appropriate pointwise distortion bound which\nwe provide below. It follows by the fact that the Bregman divergence corresponding to the sigmoid is\nthe Kullback-Leibler divergence and by combining Pinsker\u2019s inequality (lower bound) with Lemma\n4.1 of [GSS19] (upper bound).\nLemma 4.2 (Pointwise Distortion Bound for Sigmoid). Let g\u2032 be the sigmoid activation. Then,\nfor any y, p \u2208    (0, 1) we have that Df(y, p) = DKL(y\u2225p) = y ln(y/p) + (1 \u2212                y) ln(1\u2212y\n                                                                                                   1\u2212p). Moreover\n              \u2113g(y, f \u2032(p)) \u2212  \u2113g(y, f \u2032(y)) = DKL(y\u2225p) \u2208       12(y \u2212   p)2,  min{p, 12\u2212    p} \u00b7 (y \u2212  p)2\n     We translate Lemma 4.2 to a bound on the squared error of a matching loss minimizer (in this\ncase, the logistic loss) using an approach similar to the one for Theorem 2.1. In order to use the\nupper bound on the surrogate loss provided by Lemma 4.2, we apply it to1p \u2190                         g\u2032(w \u00b7 x), where g\u2032\nis the sigmoid function, and observe that the quantity              p(1\u2212p) is exponential in |w \u00b7 x|. Hence, when\n                                                                                          1\nthe marginal is (\u03bb, 2)-concentrated (subgaussian concentration), then                  p(1\u2212p) is effectively bounded.\n                                                             9", "md": "# Stronger Guarantees for Logistic Regression\n\n## Stronger Guarantees for Logistic Regression\n\nIn this section, we follow the same recipe we used in Section 2 to obtain distortion bounds similar to Theorem 2.1 for the sigmoid activation (or, equivalently, the logistic model) under the assumption that the marginal distribution is sufficiently concentrated (see Definition C.1). In particular, Theorem 2.1 does not hold, since the sigmoid is not bi-Lipschitz and our main Theorem 3.1 only provides a guarantee of O(\u221aopt) for squared error. We use appropriate pointwise distortion bounds for the matching loss corresponding to the sigmoid activation and provide guarantees of O(opt) for logistic regression with respect to both squared and absolute error, under appropriate assumptions about the concentration of the marginal distribution. The proofs of this section are provided in Appendix D.\n\nFor the logistic model, the link function \\( f' \\) is defined as \\( f'(r) = \\ln(1-r) \\), for \\( r \\in (0, 1) \\) and the corresponding activation \\( g' \\) is the sigmoid \\( g'(t) = \\frac{1}{1+e^{-t}} \\) for \\( t \\in \\mathbb{R} \\). The corresponding matching loss is the logistic loss.\n\n### Squared error\n\nWe first provide a result for squared loss minimization. In comparison to Theorem 2.1, the qualitative interpretation of the following theorem is that, while the sigmoid activation is not bi-Lipschitz, it is effectively bi-Lipschitz under sufficiently concentrated marginals.\n\nTheorem 4.1 (Squared Loss Minimization through Logistic Loss Minimization). Let \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times [0, 1] \\) whose marginal on \\( \\mathbb{R}^d \\) is (1, 2)-concentrated. Let \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be the sigmoid activation, i.e., \\( g'(t) = (1 + e^{-t})^{-1} \\) for \\( t \\in \\mathbb{R} \\). Assume that for some \\( B > 0 \\), \\( \\epsilon > 0 \\) and a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) we have\n\n$$\nL_g(f' \\circ p ; D) \\leq \\min_{w:\\|w\\|_2 \\leq B} L_g(w ; D) + \\epsilon \\tag{4.1}\n$$\nIf we let \\( \\text{opt}_g = \\min_{\\|w\\|_2 \\leq B} \\text{err}_2(g' \\circ w) \\), then for the predictor \\( p \\) and some universal constant \\( C > 0 \\) we also have\n\n$$\n\\text{err}_2(p) \\leq C \\text{opt}_g \\exp(B^2 + B^2 \\log(1 + 2\\epsilon) \\tag{4.2}\n$$\nIn particular, the squared error of \\( p \\) is upper bounded by \\( O(\\text{opt}_g) \\), since the function \\( t \\rightarrow \\exp(\\sqrt{\\log t}) \\) is asymptotically smaller than any polynomial function \\( t \\rightarrow t^\\gamma \\) with \\( \\gamma > 0 \\).\n\nOnce more, the proof of our result is based on an appropriate pointwise distortion bound which we provide below. It follows by the fact that the Bregman divergence corresponding to the sigmoid is the Kullback-Leibler divergence and by combining Pinsker\u2019s inequality (lower bound) with Lemma 4.1 of [GSS19] (upper bound).\n\nLemma 4.2 (Pointwise Distortion Bound for Sigmoid). Let \\( g' \\) be the sigmoid activation. Then, for any \\( y, p \\in (0, 1) \\) we have that \\( D_f(y, p) = D_{KL}(y\\|p) = y \\ln(y/p) + (1 - y) \\ln(1-y) - p \\). Moreover\n\n$$\n\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = D_{KL}(y\\|p) \\in \\frac{1}{2}(y - p)^2, \\min\\{p, \\frac{1}{2} - p\\} \\cdot (y - p)^2\n$$\nWe translate Lemma 4.2 to a bound on the squared error of a matching loss minimizer (in this case, the logistic loss) using an approach similar to the one for Theorem 2.1. In order to use the upper bound on the surrogate loss provided by Lemma 4.2, we apply it to \\( p \\leftarrow g'(w \\cdot x) \\), where \\( g' \\) is the sigmoid function, and observe that the quantity \\( p(1-p) \\) is exponential in \\( |w \\cdot x| \\). Hence, when the marginal is (\u03bb, 2)-concentrated (subgaussian concentration), then \\( p(1-p) \\) is effectively bounded.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Stronger Guarantees for Logistic Regression", "md": "# Stronger Guarantees for Logistic Regression"}, {"type": "heading", "lvl": 2, "value": "Stronger Guarantees for Logistic Regression", "md": "## Stronger Guarantees for Logistic Regression"}, {"type": "text", "value": "In this section, we follow the same recipe we used in Section 2 to obtain distortion bounds similar to Theorem 2.1 for the sigmoid activation (or, equivalently, the logistic model) under the assumption that the marginal distribution is sufficiently concentrated (see Definition C.1). In particular, Theorem 2.1 does not hold, since the sigmoid is not bi-Lipschitz and our main Theorem 3.1 only provides a guarantee of O(\u221aopt) for squared error. We use appropriate pointwise distortion bounds for the matching loss corresponding to the sigmoid activation and provide guarantees of O(opt) for logistic regression with respect to both squared and absolute error, under appropriate assumptions about the concentration of the marginal distribution. The proofs of this section are provided in Appendix D.\n\nFor the logistic model, the link function \\( f' \\) is defined as \\( f'(r) = \\ln(1-r) \\), for \\( r \\in (0, 1) \\) and the corresponding activation \\( g' \\) is the sigmoid \\( g'(t) = \\frac{1}{1+e^{-t}} \\) for \\( t \\in \\mathbb{R} \\). The corresponding matching loss is the logistic loss.", "md": "In this section, we follow the same recipe we used in Section 2 to obtain distortion bounds similar to Theorem 2.1 for the sigmoid activation (or, equivalently, the logistic model) under the assumption that the marginal distribution is sufficiently concentrated (see Definition C.1). In particular, Theorem 2.1 does not hold, since the sigmoid is not bi-Lipschitz and our main Theorem 3.1 only provides a guarantee of O(\u221aopt) for squared error. We use appropriate pointwise distortion bounds for the matching loss corresponding to the sigmoid activation and provide guarantees of O(opt) for logistic regression with respect to both squared and absolute error, under appropriate assumptions about the concentration of the marginal distribution. The proofs of this section are provided in Appendix D.\n\nFor the logistic model, the link function \\( f' \\) is defined as \\( f'(r) = \\ln(1-r) \\), for \\( r \\in (0, 1) \\) and the corresponding activation \\( g' \\) is the sigmoid \\( g'(t) = \\frac{1}{1+e^{-t}} \\) for \\( t \\in \\mathbb{R} \\). The corresponding matching loss is the logistic loss."}, {"type": "heading", "lvl": 3, "value": "Squared error", "md": "### Squared error"}, {"type": "text", "value": "We first provide a result for squared loss minimization. In comparison to Theorem 2.1, the qualitative interpretation of the following theorem is that, while the sigmoid activation is not bi-Lipschitz, it is effectively bi-Lipschitz under sufficiently concentrated marginals.\n\nTheorem 4.1 (Squared Loss Minimization through Logistic Loss Minimization). Let \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times [0, 1] \\) whose marginal on \\( \\mathbb{R}^d \\) is (1, 2)-concentrated. Let \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be the sigmoid activation, i.e., \\( g'(t) = (1 + e^{-t})^{-1} \\) for \\( t \\in \\mathbb{R} \\). Assume that for some \\( B > 0 \\), \\( \\epsilon > 0 \\) and a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) we have\n\n$$\nL_g(f' \\circ p ; D) \\leq \\min_{w:\\|w\\|_2 \\leq B} L_g(w ; D) + \\epsilon \\tag{4.1}\n$$\nIf we let \\( \\text{opt}_g = \\min_{\\|w\\|_2 \\leq B} \\text{err}_2(g' \\circ w) \\), then for the predictor \\( p \\) and some universal constant \\( C > 0 \\) we also have\n\n$$\n\\text{err}_2(p) \\leq C \\text{opt}_g \\exp(B^2 + B^2 \\log(1 + 2\\epsilon) \\tag{4.2}\n$$\nIn particular, the squared error of \\( p \\) is upper bounded by \\( O(\\text{opt}_g) \\), since the function \\( t \\rightarrow \\exp(\\sqrt{\\log t}) \\) is asymptotically smaller than any polynomial function \\( t \\rightarrow t^\\gamma \\) with \\( \\gamma > 0 \\).\n\nOnce more, the proof of our result is based on an appropriate pointwise distortion bound which we provide below. It follows by the fact that the Bregman divergence corresponding to the sigmoid is the Kullback-Leibler divergence and by combining Pinsker\u2019s inequality (lower bound) with Lemma 4.1 of [GSS19] (upper bound).\n\nLemma 4.2 (Pointwise Distortion Bound for Sigmoid). Let \\( g' \\) be the sigmoid activation. Then, for any \\( y, p \\in (0, 1) \\) we have that \\( D_f(y, p) = D_{KL}(y\\|p) = y \\ln(y/p) + (1 - y) \\ln(1-y) - p \\). Moreover\n\n$$\n\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = D_{KL}(y\\|p) \\in \\frac{1}{2}(y - p)^2, \\min\\{p, \\frac{1}{2} - p\\} \\cdot (y - p)^2\n$$\nWe translate Lemma 4.2 to a bound on the squared error of a matching loss minimizer (in this case, the logistic loss) using an approach similar to the one for Theorem 2.1. In order to use the upper bound on the surrogate loss provided by Lemma 4.2, we apply it to \\( p \\leftarrow g'(w \\cdot x) \\), where \\( g' \\) is the sigmoid function, and observe that the quantity \\( p(1-p) \\) is exponential in \\( |w \\cdot x| \\). Hence, when the marginal is (\u03bb, 2)-concentrated (subgaussian concentration), then \\( p(1-p) \\) is effectively bounded.", "md": "We first provide a result for squared loss minimization. In comparison to Theorem 2.1, the qualitative interpretation of the following theorem is that, while the sigmoid activation is not bi-Lipschitz, it is effectively bi-Lipschitz under sufficiently concentrated marginals.\n\nTheorem 4.1 (Squared Loss Minimization through Logistic Loss Minimization). Let \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times [0, 1] \\) whose marginal on \\( \\mathbb{R}^d \\) is (1, 2)-concentrated. Let \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be the sigmoid activation, i.e., \\( g'(t) = (1 + e^{-t})^{-1} \\) for \\( t \\in \\mathbb{R} \\). Assume that for some \\( B > 0 \\), \\( \\epsilon > 0 \\) and a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) we have\n\n$$\nL_g(f' \\circ p ; D) \\leq \\min_{w:\\|w\\|_2 \\leq B} L_g(w ; D) + \\epsilon \\tag{4.1}\n$$\nIf we let \\( \\text{opt}_g = \\min_{\\|w\\|_2 \\leq B} \\text{err}_2(g' \\circ w) \\), then for the predictor \\( p \\) and some universal constant \\( C > 0 \\) we also have\n\n$$\n\\text{err}_2(p) \\leq C \\text{opt}_g \\exp(B^2 + B^2 \\log(1 + 2\\epsilon) \\tag{4.2}\n$$\nIn particular, the squared error of \\( p \\) is upper bounded by \\( O(\\text{opt}_g) \\), since the function \\( t \\rightarrow \\exp(\\sqrt{\\log t}) \\) is asymptotically smaller than any polynomial function \\( t \\rightarrow t^\\gamma \\) with \\( \\gamma > 0 \\).\n\nOnce more, the proof of our result is based on an appropriate pointwise distortion bound which we provide below. It follows by the fact that the Bregman divergence corresponding to the sigmoid is the Kullback-Leibler divergence and by combining Pinsker\u2019s inequality (lower bound) with Lemma 4.1 of [GSS19] (upper bound).\n\nLemma 4.2 (Pointwise Distortion Bound for Sigmoid). Let \\( g' \\) be the sigmoid activation. Then, for any \\( y, p \\in (0, 1) \\) we have that \\( D_f(y, p) = D_{KL}(y\\|p) = y \\ln(y/p) + (1 - y) \\ln(1-y) - p \\). Moreover\n\n$$\n\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = D_{KL}(y\\|p) \\in \\frac{1}{2}(y - p)^2, \\min\\{p, \\frac{1}{2} - p\\} \\cdot (y - p)^2\n$$\nWe translate Lemma 4.2 to a bound on the squared error of a matching loss minimizer (in this case, the logistic loss) using an approach similar to the one for Theorem 2.1. In order to use the upper bound on the surrogate loss provided by Lemma 4.2, we apply it to \\( p \\leftarrow g'(w \\cdot x) \\), where \\( g' \\) is the sigmoid function, and observe that the quantity \\( p(1-p) \\) is exponential in \\( |w \\cdot x| \\). Hence, when the marginal is (\u03bb, 2)-concentrated (subgaussian concentration), then \\( p(1-p) \\) is effectively bounded."}]}, {"page": 10, "text": "Absolute error.         All of the results we have provided so far have focused on squared error min-\nimization. We now show that our approach yields results even for the absolute error, which can\nalso be viewed as learning in the p-concept model [KS94]. In particular, for a distribution D over\nRd \u00d7 [0, 1], we define the absolute error of a predictor p : Rd \u2192          [0, 1] as follows.\n                                          err1(p) =      E\n                                                      (x,y)\u223cD[|y \u2212  p(x)|]\nIn the specific case when the labels are binary, i.e., y \u2208        {0, 1}, we have\n                 err1(p) =      E                         P      [y \u0338= yp]                 (see Proposition A.2)\n                             (x,y)\u223cD[|y \u2212  p(x)|] =  (x,y,yp)\u223cDp\nwhere the distribution Dp is over Rd \u00d7 {0, 1}\u00d7 {0, 1} and is formed by drawing samples (x, y) from\nD and, given x, forming yp by drawing a conditionally independent Bernoulli random variable with\nparameter p(x). We provide the following result.\nTheorem 4.3 (Absolute Loss Minimization through Logistic Loss Minimization). Let D be a\ndistribution over Rd \u00d7 {0, 1} whose marginal on Rd is (1, 1)-concentrated. Let g\u2032 : R \u2192                   R be the\nsigmoid activation, i.e., g\u2032(t) = (1 + e\u2212t)\u22121 for t \u2208         R. Assume that for some B > 0, \u01eb > 0 and a\npredictor p : Rd \u2192     (0, 1) we have\n                                   Lg(f \u2032 \u25e6 p ; D) \u2264     min                                                   (4.2)\n                                                      w:\u2225w\u22252\u2264B Lg(w ; D) + \u01eb\nIf we let opt  g = min\u2225w\u22252\u2264B err1(g\u2032    w), then for the predictor p and some universal constant C > 0\nwe also have                                                         1\n                                        err1(p) \u2264  C B optg log    opt g + \u01eb\n    The corresponding distortion bound in this case is between the absolute and logistic losses and\nworks when the labels are binary.\nLemma 4.4 (Pointwise Distortion between Absolute and Logistic Loss). Let g\u2032 be the sigmoid\nactivation. Then, there is a constant c \u2208        R such that for any y \u2208      {0, 1} and p \u2208   (0, 1), we have\n                           \u2113g(y, f \u2032(p)) \u2212 c \u2208   |y \u2212 p| , 2 \u00b7 ln      1       \u00b7 |y \u2212 p|\n                                                                   p(1 \u2212  p)\n    The bound of Theorem 4.3 implies an algorithm for learning an unknown sigmoid neuron in the\np-concept model, by minimizing a convex loss. While there are algorithms achieving stronger guar-\nantees [DKTZ22] for agnostically learning sigmoid neurons, such algorithms typically make strong\ndistributional assumptions including concentration, anti-concentration and anti-anti-concentration\nor boundedness.\n    Moreover, it is useful to compare the bound we provide in Theorem 4.3 to a lower bound by\n[DKTZ20, Theorem 4.1], which concerns the problem of agnostically learning halfspaces by min-\nimizing convex surrogates. In particular, they show that even under log-concave marginals, no\nconvex surrogate loss can achieve a guarantee better than O(opt log(1/opt)), where opt is mea-\nsured with respect to the \u21131 error (which is equal to the probability of error). The result is not\ndirectly comparable to our upper bound, since we examine the sigmoid activation. Their setting\ncan be viewed as a limit case of ours by letting the norm of the weight vector grow indefinitely (the\nsigmoid tends to the step function), but the main complication is that our upper bound is of the\nform O(Bopt log(1/opt)), which scales with B. However, their lower bound concerns marginal dis-\ntributions that are not only concentrated, but are also anti-concentrated and anti-anticoncentrated,\nwhile our results only make concentration assumptions.\n                                                         10", "md": "# Absolute Error\n\n## Absolute error\n\nAll of the results we have provided so far have focused on squared error minimization. We now show that our approach yields results even for the absolute error, which can also be viewed as learning in the p-concept model [KS94]. In particular, for a distribution D over \\( \\mathbb{R}^d \\times [0, 1] \\), we define the absolute error of a predictor \\( p : \\mathbb{R}^d \\rightarrow [0, 1] \\) as follows:\n\n$$\nerr_1(p) = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|]\n$$\nIn the specific case when the labels are binary, i.e., \\( y \\in \\{0, 1\\} \\), we have:\n\n$$\nerr_1(p) = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|] = \\mathbb{E}_{(x,y,yp) \\sim Dp}[y \\neq yp] \\quad \\text{(see Proposition A.2)}\n$$\nwhere the distribution \\( Dp \\) is over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\times \\{0, 1\\} \\) and is formed by drawing samples \\( (x, y) \\) from \\( D \\) and, given \\( x \\), forming \\( yp \\) by drawing a conditionally independent Bernoulli random variable with parameter \\( p(x) \\).\n\n### Theorem 4.3 (Absolute Loss Minimization through Logistic Loss Minimization)\n\nLet \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\) whose marginal on \\( \\mathbb{R}^d \\) is (1, 1)-concentrated. Let \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be the sigmoid activation, i.e., \\( g'(t) = (1 + e^{-t})^{-1} \\) for \\( t \\in \\mathbb{R} \\). Assume that for some \\( B > 0 \\), \\( \\epsilon > 0 \\) and a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) we have:\n\n$$\nL_g(f' \\circ p ; D) \\leq \\min_{w: \\|w\\|_2 \\leq B} L_g(w ; D) + \\epsilon \\quad \\text{(4.2)}\n$$\nIf we let \\( \\text{opt}_g = \\min_{\\|w\\|_2 \\leq B} err_1(g' \\circ w) \\), then for the predictor \\( p \\) and some universal constant \\( C > 0 \\) we also have:\n\n$$\nerr_1(p) \\leq C B \\text{opt}_g \\log(\\text{opt}_g) + \\epsilon\n$$\nThe corresponding distortion bound in this case is between the absolute and logistic losses and works when the labels are binary.\n\n### Lemma 4.4 (Pointwise Distortion between Absolute and Logistic Loss)\n\nLet \\( g' \\) be the sigmoid activation. Then, there is a constant \\( c \\in \\mathbb{R} \\) such that for any \\( y \\in \\{0, 1\\} \\) and \\( p \\in (0, 1) \\), we have:\n\n$$\n\\ell_g(y, f'(p)) - c \\in |y - p| , 2 \\cdot \\ln \\left( \\frac{1}{p(1 - p)} \\right) \\cdot |y - p|\n$$\nThe bound of Theorem 4.3 implies an algorithm for learning an unknown sigmoid neuron in the p-concept model, by minimizing a convex loss. While there are algorithms achieving stronger guarantees [DKTZ22] for agnostically learning sigmoid neurons, such algorithms typically make strong distributional assumptions including concentration, anti-concentration, and anti-anti-concentration or boundedness.\n\nMoreover, it is useful to compare the bound we provide in Theorem 4.3 to a lower bound by [DKTZ20, Theorem 4.1], which concerns the problem of agnostically learning halfspaces by minimizing convex surrogates. In particular, they show that even under log-concave marginals, no convex surrogate loss can achieve a guarantee better than \\( O(\\text{opt} \\log(1/\\text{opt})) \\), where opt is measured with respect to the \\( \\ell_1 \\) error (which is equal to the probability of error). The result is not directly comparable to our upper bound, since we examine the sigmoid activation. Their setting can be viewed as a limit case of ours by letting the norm of the weight vector grow indefinitely (the sigmoid tends to the step function), but the main complication is that our upper bound is of the form \\( O(B \\text{opt} \\log(1/\\text{opt})) \\), which scales with \\( B \\). However, their lower bound concerns marginal distributions that are not only concentrated but are also anti-concentrated and anti-anticoncentrated, while our results only make concentration assumptions.\n\n10", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Absolute Error", "md": "# Absolute Error"}, {"type": "heading", "lvl": 2, "value": "Absolute error", "md": "## Absolute error"}, {"type": "text", "value": "All of the results we have provided so far have focused on squared error minimization. We now show that our approach yields results even for the absolute error, which can also be viewed as learning in the p-concept model [KS94]. In particular, for a distribution D over \\( \\mathbb{R}^d \\times [0, 1] \\), we define the absolute error of a predictor \\( p : \\mathbb{R}^d \\rightarrow [0, 1] \\) as follows:\n\n$$\nerr_1(p) = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|]\n$$\nIn the specific case when the labels are binary, i.e., \\( y \\in \\{0, 1\\} \\), we have:\n\n$$\nerr_1(p) = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|] = \\mathbb{E}_{(x,y,yp) \\sim Dp}[y \\neq yp] \\quad \\text{(see Proposition A.2)}\n$$\nwhere the distribution \\( Dp \\) is over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\times \\{0, 1\\} \\) and is formed by drawing samples \\( (x, y) \\) from \\( D \\) and, given \\( x \\), forming \\( yp \\) by drawing a conditionally independent Bernoulli random variable with parameter \\( p(x) \\).", "md": "All of the results we have provided so far have focused on squared error minimization. We now show that our approach yields results even for the absolute error, which can also be viewed as learning in the p-concept model [KS94]. In particular, for a distribution D over \\( \\mathbb{R}^d \\times [0, 1] \\), we define the absolute error of a predictor \\( p : \\mathbb{R}^d \\rightarrow [0, 1] \\) as follows:\n\n$$\nerr_1(p) = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|]\n$$\nIn the specific case when the labels are binary, i.e., \\( y \\in \\{0, 1\\} \\), we have:\n\n$$\nerr_1(p) = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|] = \\mathbb{E}_{(x,y,yp) \\sim Dp}[y \\neq yp] \\quad \\text{(see Proposition A.2)}\n$$\nwhere the distribution \\( Dp \\) is over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\times \\{0, 1\\} \\) and is formed by drawing samples \\( (x, y) \\) from \\( D \\) and, given \\( x \\), forming \\( yp \\) by drawing a conditionally independent Bernoulli random variable with parameter \\( p(x) \\)."}, {"type": "heading", "lvl": 3, "value": "Theorem 4.3 (Absolute Loss Minimization through Logistic Loss Minimization)", "md": "### Theorem 4.3 (Absolute Loss Minimization through Logistic Loss Minimization)"}, {"type": "text", "value": "Let \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\) whose marginal on \\( \\mathbb{R}^d \\) is (1, 1)-concentrated. Let \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be the sigmoid activation, i.e., \\( g'(t) = (1 + e^{-t})^{-1} \\) for \\( t \\in \\mathbb{R} \\). Assume that for some \\( B > 0 \\), \\( \\epsilon > 0 \\) and a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) we have:\n\n$$\nL_g(f' \\circ p ; D) \\leq \\min_{w: \\|w\\|_2 \\leq B} L_g(w ; D) + \\epsilon \\quad \\text{(4.2)}\n$$\nIf we let \\( \\text{opt}_g = \\min_{\\|w\\|_2 \\leq B} err_1(g' \\circ w) \\), then for the predictor \\( p \\) and some universal constant \\( C > 0 \\) we also have:\n\n$$\nerr_1(p) \\leq C B \\text{opt}_g \\log(\\text{opt}_g) + \\epsilon\n$$\nThe corresponding distortion bound in this case is between the absolute and logistic losses and works when the labels are binary.", "md": "Let \\( D \\) be a distribution over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\) whose marginal on \\( \\mathbb{R}^d \\) is (1, 1)-concentrated. Let \\( g' : \\mathbb{R} \\rightarrow \\mathbb{R} \\) be the sigmoid activation, i.e., \\( g'(t) = (1 + e^{-t})^{-1} \\) for \\( t \\in \\mathbb{R} \\). Assume that for some \\( B > 0 \\), \\( \\epsilon > 0 \\) and a predictor \\( p : \\mathbb{R}^d \\rightarrow (0, 1) \\) we have:\n\n$$\nL_g(f' \\circ p ; D) \\leq \\min_{w: \\|w\\|_2 \\leq B} L_g(w ; D) + \\epsilon \\quad \\text{(4.2)}\n$$\nIf we let \\( \\text{opt}_g = \\min_{\\|w\\|_2 \\leq B} err_1(g' \\circ w) \\), then for the predictor \\( p \\) and some universal constant \\( C > 0 \\) we also have:\n\n$$\nerr_1(p) \\leq C B \\text{opt}_g \\log(\\text{opt}_g) + \\epsilon\n$$\nThe corresponding distortion bound in this case is between the absolute and logistic losses and works when the labels are binary."}, {"type": "heading", "lvl": 3, "value": "Lemma 4.4 (Pointwise Distortion between Absolute and Logistic Loss)", "md": "### Lemma 4.4 (Pointwise Distortion between Absolute and Logistic Loss)"}, {"type": "text", "value": "Let \\( g' \\) be the sigmoid activation. Then, there is a constant \\( c \\in \\mathbb{R} \\) such that for any \\( y \\in \\{0, 1\\} \\) and \\( p \\in (0, 1) \\), we have:\n\n$$\n\\ell_g(y, f'(p)) - c \\in |y - p| , 2 \\cdot \\ln \\left( \\frac{1}{p(1 - p)} \\right) \\cdot |y - p|\n$$\nThe bound of Theorem 4.3 implies an algorithm for learning an unknown sigmoid neuron in the p-concept model, by minimizing a convex loss. While there are algorithms achieving stronger guarantees [DKTZ22] for agnostically learning sigmoid neurons, such algorithms typically make strong distributional assumptions including concentration, anti-concentration, and anti-anti-concentration or boundedness.\n\nMoreover, it is useful to compare the bound we provide in Theorem 4.3 to a lower bound by [DKTZ20, Theorem 4.1], which concerns the problem of agnostically learning halfspaces by minimizing convex surrogates. In particular, they show that even under log-concave marginals, no convex surrogate loss can achieve a guarantee better than \\( O(\\text{opt} \\log(1/\\text{opt})) \\), where opt is measured with respect to the \\( \\ell_1 \\) error (which is equal to the probability of error). The result is not directly comparable to our upper bound, since we examine the sigmoid activation. Their setting can be viewed as a limit case of ours by letting the norm of the weight vector grow indefinitely (the sigmoid tends to the step function), but the main complication is that our upper bound is of the form \\( O(B \\text{opt} \\log(1/\\text{opt})) \\), which scales with \\( B \\). However, their lower bound concerns marginal distributions that are not only concentrated but are also anti-concentrated and anti-anticoncentrated, while our results only make concentration assumptions.\n\n10", "md": "Let \\( g' \\) be the sigmoid activation. Then, there is a constant \\( c \\in \\mathbb{R} \\) such that for any \\( y \\in \\{0, 1\\} \\) and \\( p \\in (0, 1) \\), we have:\n\n$$\n\\ell_g(y, f'(p)) - c \\in |y - p| , 2 \\cdot \\ln \\left( \\frac{1}{p(1 - p)} \\right) \\cdot |y - p|\n$$\nThe bound of Theorem 4.3 implies an algorithm for learning an unknown sigmoid neuron in the p-concept model, by minimizing a convex loss. While there are algorithms achieving stronger guarantees [DKTZ22] for agnostically learning sigmoid neurons, such algorithms typically make strong distributional assumptions including concentration, anti-concentration, and anti-anti-concentration or boundedness.\n\nMoreover, it is useful to compare the bound we provide in Theorem 4.3 to a lower bound by [DKTZ20, Theorem 4.1], which concerns the problem of agnostically learning halfspaces by minimizing convex surrogates. In particular, they show that even under log-concave marginals, no convex surrogate loss can achieve a guarantee better than \\( O(\\text{opt} \\log(1/\\text{opt})) \\), where opt is measured with respect to the \\( \\ell_1 \\) error (which is equal to the probability of error). The result is not directly comparable to our upper bound, since we examine the sigmoid activation. Their setting can be viewed as a limit case of ours by letting the norm of the weight vector grow indefinitely (the sigmoid tends to the step function), but the main complication is that our upper bound is of the form \\( O(B \\text{opt} \\log(1/\\text{opt})) \\), which scales with \\( B \\). However, their lower bound concerns marginal distributions that are not only concentrated but are also anti-concentrated and anti-anticoncentrated, while our results only make concentration assumptions.\n\n10"}]}, {"page": 11, "text": "5     Necessity of Norm Dependence\nIn this final section, we use a lower bound due to [DKMR22] on agnostic learning of GLMs using\nSQ algorithms and compare it with our main result (Theorem 3.1). For simplicity, we specialize\nto the case of the standard sigmoid or logistic function. A modification of their proof ensures that\nthe bound holds under isotropic marginals.1\nTheorem 5.1 (SQ Lower Bound for Agnostically Learning GLMs, variant of [DKMR22, Thm\nC.3]). Let g\u2032 : Rd \u2192      R be the standard logistic function. Any SQ algorithm either requires d\u03c9(1)\nqueries or d\u2212\u03c9(1) tolerance to distinguish between the following two labeled distributions:\n    \u2022 (Labels have signal.) Dsignal on Rd\u00d7R is such that opt(GLMg\u2032,B, Dsignal) \u2264              exp(\u2212\u2126(log1/4 d)) =\n       o(1) for some B = poly(d).\n    \u2022 (Labels are random.) Drandom on Rd \u00d7 R is such that the labels y are drawn i.i.d. from {a, b}\n       for certain universal constants a, b \u2208      [0, 1]. In particular, opt(GLMg\u2032,B, Drandom) = \u2126(1) for\n       any B.\nBoth Dsignal and Drandom have the same marginal on Rd, with 1-bounded second moments.\n     Let us consider applying our main theorem (Theorem 3.1) to this setting, with D being either\nDsignal or Drandom, and with the same B = poly(d) as is required to achieve small error in the\n\u201clabels have signal\u201d case. We would obtain a predictor with \u21132 error at most B                    opt(GLMg\u2032,B)\n(or indeed with SIMB in place of GLMg\u2032,B). Since this is \u03c9(1), this guarantee is insufficient to\ndistinguish the two cases above, which is as it should be since our main algorithm indeed fits into\nthe SQ framework.\n     Theorem 5.1 does, however, justify a dependence on the norm B in our main result. In par-\nticular, it is clear that a guarantee of the form opt(GLMg\u2032,B)c for any universal constant c > 0\n(independent of B) would be too strong, as it would let us distinguish the two cases above. In fact,\nthis lower bound rules out a large space of potential error guarantees stated as functions of B and\nopt(GLMg\u2032,B). For instance, for sufficiently large d, it rules out any error guarantee of the form\nexp(O(log1/5 B)) \u00b7 opt(GLMg\u2032,B)c\u2032 for any universal constant c\u2032 > 0.\nReferences\n[Agr15]        Alan Agresti. Foundations of linear and generalized linear models. John Wiley & Sons,\n               2015. 1, 1.1\n[AHW95]        Peter Auer, Mark Herbster, and Manfred K. K Warmuth. Exponentially many local\n               minima for single neurons. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors,\n               Advances in Neural Information Processing Systems, volume 8. MIT Press, 1995. 1,\n               1.1\n[DGK+20]       Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R Klivans, and Mahdi\n               Soltanolkotabi. Approximation schemes for relu regression. In Conference on Learning\n               Theory, pages 1452\u20131485. PMLR, 2020. 1.1\n   1Specifically, our features correspond to all multilinear monomials (or parities) of degree at most k over {\u00b11}n,\nwhereas they use all monomials (not necessarily multilinear) of degree at most k. These yield equivalent representa-\ntions since the hard distributions are obtained from the uniform distribution on {\u00b11}n.\n                                                         11", "md": "# Necessity of Norm Dependence\n\n## Necessity of Norm Dependence\n\nIn this final section, we use a lower bound due to [DKMR22] on agnostic learning of GLMs using SQ algorithms and compare it with our main result (Theorem 3.1). For simplicity, we specialize to the case of the standard sigmoid or logistic function. A modification of their proof ensures that the bound holds under isotropic marginals.\n\n### Theorem 5.1 (SQ Lower Bound for Agnostically Learning GLMs, variant of [DKMR22, Thm C.3])\n\nLet \\( g' : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) be the standard logistic function. Any SQ algorithm either requires \\( d^{\\omega(1)} \\) queries or \\( d^{-\\omega(1)} \\) tolerance to distinguish between the following two labeled distributions:\n\n- (Labels have signal.) \\( D_{\\text{signal}} \\) on \\( \\mathbb{R}^d \\times \\mathbb{R} \\) is such that \\( \\text{opt}(\\text{GLM}g',B, D_{\\text{signal}}) \\leq \\exp(-\\Omega(\\log^{1/4} d)) = o(1) \\) for some \\( B = \\text{poly}(d) \\).\n- (Labels are random.) \\( D_{\\text{random}} \\) on \\( \\mathbb{R}^d \\times \\mathbb{R} \\) is such that the labels \\( y \\) are drawn i.i.d. from {a, b} for certain universal constants \\( a, b \\in [0, 1] \\). In particular, \\( \\text{opt}(\\text{GLM}g',B, D_{\\text{random}}) = \\Omega(1) \\) for any \\( B \\).\n\nBoth \\( D_{\\text{signal}} \\) and \\( D_{\\text{random}} \\) have the same marginal on \\( \\mathbb{R}^d \\), with 1-bounded second moments.\n\nLet us consider applying our main theorem (Theorem 3.1) to this setting, with \\( D \\) being either \\( D_{\\text{signal}} \\) or \\( D_{\\text{random}} \\), and with the same \\( B = \\text{poly}(d) \\) as is required to achieve small error in the \"labels have signal\" case. We would obtain a predictor with \\( \\ell^2 \\) error at most \\( B \\cdot \\text{opt}(\\text{GLM}g',B) \\) (or indeed with SIMB in place of GLMg',B). Since this is \\( \\omega(1) \\), this guarantee is insufficient to distinguish the two cases above, which is as it should be since our main algorithm indeed fits into the SQ framework.\n\nTheorem 5.1 does, however, justify a dependence on the norm \\( B \\) in our main result. In particular, it is clear that a guarantee of the form \\( \\text{opt}(\\text{GLM}g',B)^c \\) for any universal constant \\( c > 0 \\) (independent of \\( B \\)) would be too strong, as it would let us distinguish the two cases above. In fact, this lower bound rules out a large space of potential error guarantees stated as functions of \\( B \\) and \\( \\text{opt}(\\text{GLM}g',B) \\). For instance, for sufficiently large \\( d \\), it rules out any error guarantee of the form \\( \\exp(O(\\log^{1/5} B)) \\cdot \\text{opt}(\\text{GLM}g',B)^{c'} \\) for any universal constant \\( c' > 0 \\).\n\n### References\n\n[Agr15] Alan Agresti. Foundations of linear and generalized linear models. John Wiley & Sons, 2015. 1, 1.1\n\n[AHW95] Peter Auer, Mark Herbster, and Manfred K. K Warmuth. Exponentially many local minima for single neurons. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8. MIT Press, 1995. 1, 1.1\n\n[DGK+20] Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R Klivans, and Mahdi Soltanolkotabi. Approximation schemes for relu regression. In Conference on Learning Theory, pages 1452\u20131485. PMLR, 2020. 1.1\n\n1Specifically, our features correspond to all multilinear monomials (or parities) of degree at most \\( k \\) over {\u00b11}^n, whereas they use all monomials (not necessarily multilinear) of degree at most \\( k \\). These yield equivalent representations since the hard distributions are obtained from the uniform distribution on {\u00b11}^n.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Necessity of Norm Dependence", "md": "# Necessity of Norm Dependence"}, {"type": "heading", "lvl": 2, "value": "Necessity of Norm Dependence", "md": "## Necessity of Norm Dependence"}, {"type": "text", "value": "In this final section, we use a lower bound due to [DKMR22] on agnostic learning of GLMs using SQ algorithms and compare it with our main result (Theorem 3.1). For simplicity, we specialize to the case of the standard sigmoid or logistic function. A modification of their proof ensures that the bound holds under isotropic marginals.", "md": "In this final section, we use a lower bound due to [DKMR22] on agnostic learning of GLMs using SQ algorithms and compare it with our main result (Theorem 3.1). For simplicity, we specialize to the case of the standard sigmoid or logistic function. A modification of their proof ensures that the bound holds under isotropic marginals."}, {"type": "heading", "lvl": 3, "value": "Theorem 5.1 (SQ Lower Bound for Agnostically Learning GLMs, variant of [DKMR22, Thm C.3])", "md": "### Theorem 5.1 (SQ Lower Bound for Agnostically Learning GLMs, variant of [DKMR22, Thm C.3])"}, {"type": "text", "value": "Let \\( g' : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) be the standard logistic function. Any SQ algorithm either requires \\( d^{\\omega(1)} \\) queries or \\( d^{-\\omega(1)} \\) tolerance to distinguish between the following two labeled distributions:\n\n- (Labels have signal.) \\( D_{\\text{signal}} \\) on \\( \\mathbb{R}^d \\times \\mathbb{R} \\) is such that \\( \\text{opt}(\\text{GLM}g',B, D_{\\text{signal}}) \\leq \\exp(-\\Omega(\\log^{1/4} d)) = o(1) \\) for some \\( B = \\text{poly}(d) \\).\n- (Labels are random.) \\( D_{\\text{random}} \\) on \\( \\mathbb{R}^d \\times \\mathbb{R} \\) is such that the labels \\( y \\) are drawn i.i.d. from {a, b} for certain universal constants \\( a, b \\in [0, 1] \\). In particular, \\( \\text{opt}(\\text{GLM}g',B, D_{\\text{random}}) = \\Omega(1) \\) for any \\( B \\).\n\nBoth \\( D_{\\text{signal}} \\) and \\( D_{\\text{random}} \\) have the same marginal on \\( \\mathbb{R}^d \\), with 1-bounded second moments.\n\nLet us consider applying our main theorem (Theorem 3.1) to this setting, with \\( D \\) being either \\( D_{\\text{signal}} \\) or \\( D_{\\text{random}} \\), and with the same \\( B = \\text{poly}(d) \\) as is required to achieve small error in the \"labels have signal\" case. We would obtain a predictor with \\( \\ell^2 \\) error at most \\( B \\cdot \\text{opt}(\\text{GLM}g',B) \\) (or indeed with SIMB in place of GLMg',B). Since this is \\( \\omega(1) \\), this guarantee is insufficient to distinguish the two cases above, which is as it should be since our main algorithm indeed fits into the SQ framework.\n\nTheorem 5.1 does, however, justify a dependence on the norm \\( B \\) in our main result. In particular, it is clear that a guarantee of the form \\( \\text{opt}(\\text{GLM}g',B)^c \\) for any universal constant \\( c > 0 \\) (independent of \\( B \\)) would be too strong, as it would let us distinguish the two cases above. In fact, this lower bound rules out a large space of potential error guarantees stated as functions of \\( B \\) and \\( \\text{opt}(\\text{GLM}g',B) \\). For instance, for sufficiently large \\( d \\), it rules out any error guarantee of the form \\( \\exp(O(\\log^{1/5} B)) \\cdot \\text{opt}(\\text{GLM}g',B)^{c'} \\) for any universal constant \\( c' > 0 \\).", "md": "Let \\( g' : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) be the standard logistic function. Any SQ algorithm either requires \\( d^{\\omega(1)} \\) queries or \\( d^{-\\omega(1)} \\) tolerance to distinguish between the following two labeled distributions:\n\n- (Labels have signal.) \\( D_{\\text{signal}} \\) on \\( \\mathbb{R}^d \\times \\mathbb{R} \\) is such that \\( \\text{opt}(\\text{GLM}g',B, D_{\\text{signal}}) \\leq \\exp(-\\Omega(\\log^{1/4} d)) = o(1) \\) for some \\( B = \\text{poly}(d) \\).\n- (Labels are random.) \\( D_{\\text{random}} \\) on \\( \\mathbb{R}^d \\times \\mathbb{R} \\) is such that the labels \\( y \\) are drawn i.i.d. from {a, b} for certain universal constants \\( a, b \\in [0, 1] \\). In particular, \\( \\text{opt}(\\text{GLM}g',B, D_{\\text{random}}) = \\Omega(1) \\) for any \\( B \\).\n\nBoth \\( D_{\\text{signal}} \\) and \\( D_{\\text{random}} \\) have the same marginal on \\( \\mathbb{R}^d \\), with 1-bounded second moments.\n\nLet us consider applying our main theorem (Theorem 3.1) to this setting, with \\( D \\) being either \\( D_{\\text{signal}} \\) or \\( D_{\\text{random}} \\), and with the same \\( B = \\text{poly}(d) \\) as is required to achieve small error in the \"labels have signal\" case. We would obtain a predictor with \\( \\ell^2 \\) error at most \\( B \\cdot \\text{opt}(\\text{GLM}g',B) \\) (or indeed with SIMB in place of GLMg',B). Since this is \\( \\omega(1) \\), this guarantee is insufficient to distinguish the two cases above, which is as it should be since our main algorithm indeed fits into the SQ framework.\n\nTheorem 5.1 does, however, justify a dependence on the norm \\( B \\) in our main result. In particular, it is clear that a guarantee of the form \\( \\text{opt}(\\text{GLM}g',B)^c \\) for any universal constant \\( c > 0 \\) (independent of \\( B \\)) would be too strong, as it would let us distinguish the two cases above. In fact, this lower bound rules out a large space of potential error guarantees stated as functions of \\( B \\) and \\( \\text{opt}(\\text{GLM}g',B) \\). For instance, for sufficiently large \\( d \\), it rules out any error guarantee of the form \\( \\exp(O(\\log^{1/5} B)) \\cdot \\text{opt}(\\text{GLM}g',B)^{c'} \\) for any universal constant \\( c' > 0 \\)."}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "text", "value": "[Agr15] Alan Agresti. Foundations of linear and generalized linear models. John Wiley & Sons, 2015. 1, 1.1\n\n[AHW95] Peter Auer, Mark Herbster, and Manfred K. K Warmuth. Exponentially many local minima for single neurons. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8. MIT Press, 1995. 1, 1.1\n\n[DGK+20] Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R Klivans, and Mahdi Soltanolkotabi. Approximation schemes for relu regression. In Conference on Learning Theory, pages 1452\u20131485. PMLR, 2020. 1.1\n\n1Specifically, our features correspond to all multilinear monomials (or parities) of degree at most \\( k \\) over {\u00b11}^n, whereas they use all monomials (not necessarily multilinear) of degree at most \\( k \\). These yield equivalent representations since the hard distributions are obtained from the uniform distribution on {\u00b11}^n.", "md": "[Agr15] Alan Agresti. Foundations of linear and generalized linear models. John Wiley & Sons, 2015. 1, 1.1\n\n[AHW95] Peter Auer, Mark Herbster, and Manfred K. K Warmuth. Exponentially many local minima for single neurons. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8. MIT Press, 1995. 1, 1.1\n\n[DGK+20] Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R Klivans, and Mahdi Soltanolkotabi. Approximation schemes for relu regression. In Conference on Learning Theory, pages 1452\u20131485. PMLR, 2020. 1.1\n\n1Specifically, our features correspond to all multilinear monomials (or parities) of degree at most \\( k \\) over {\u00b11}^n, whereas they use all monomials (not necessarily multilinear) of degree at most \\( k \\). These yield equivalent representations since the hard distributions are obtained from the uniform distribution on {\u00b11}^n."}]}, {"page": 12, "text": "[DKMR22]  Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, and Lisheng Ren. Hardness of\n          learning a single neuron with adversarial label noise. In Gustau Camps-Valls, Fran-\n          cisco J. R. Ruiz, and Isabel Valera, editors, Proceedings of The 25th International\n          Conference on Artifi  cial Intelligence and Statistics, volume 151 of Proceedings of Ma-\n          chine Learning Research, pages 8199\u20138213. PMLR, 28\u201330 Mar 2022. 1, 1.1, 5, 5.1\n[DKPZ21]  Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, and Nikos Zarifis. The optimal-\n          ity of polynomial regression for agnostic learning under gaussian marginals in the sq\n          model. In Mikhail Belkin and Samory Kpotufe, editors, Proceedings of Thirty Fourth\n          Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Re-\n          search, pages 1552\u20131584. PMLR, 15\u201319 Aug 2021. 1, 1.1\n[DKTZ20]  Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex\n          sgd learns halfspaces with adversarial label noise.     In H. Larochelle, M. Ranzato,\n          R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Pro-\n          cessing Systems, volume 33, pages 18540\u201318549. Curran Associates, Inc., 2020. 4\n[DKTZ22]  Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning a\n          single neuron with adversarial label noise via gradient descent. In Po-Ling Loh and\n          Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory,\n          volume 178 of Proceedings of Machine Learning Research, pages 4313\u20134361. PMLR,\n          02\u201305 Jul 2022. 1, 1, 1.1, 4\n[DKZ20]   Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis.      Near-optimal sq lower bounds\n          for agnostically learning halfspaces and relus under gaussian marginals. Advances in\n          Neural Information Processing Systems, 33:13586\u201313596, 2020. 1, 1.1\n[FCG20]   Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with\n          gradient descent. Advances in Neural Information Processing Systems, 33:5417\u20135428,\n          2020. 1, 1.1\n[GGK20]   Surbhi Goel, Aravind Gollakota, and Adam Klivans. Statistical-query lower bounds\n          via functional gradients. Advances in Neural Information Processing Systems, 33:2147\u2013\n          2158, 2020. 1, 1.1\n[GHK+23]  Parikshit Gopalan, Lunjia Hu, Michael P. Kim, Omer Reingold, and Udi Wieder.\n          Loss minimization through the lens of outcome indistinguishability. In Yael Tauman\n          Kalai, editor, 14th Innovations in Theoretical Computer Science Conference, ITCS\n          2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA, volume 251 of\n          LIPIcs, pages 60:1\u201360:20. Schloss Dagstuhl - Leibniz-Zentrum f\u00a8     ur Informatik, 2023.\n          (document), 1, 1.1, 3, 3, 3.2, C.1, C.2, C.1\n[GKK19]   Surbhi Goel, Sushrut Karmalkar, and Adam Klivans. Time/accuracy tradeoffs for\n          learning a relu with respect to gaussian marginals. Advances in neural information\n          processing systems, 32, 2019. 1, 1.1\n[GKKT17]  Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the\n          relu in polynomial time. In Conference on Learning Theory, pages 1004\u20131042. PMLR,\n          2017. 1.1\n                                               12", "md": "# List of References\n\n# List of References\n\n|Reference|Authors|Title|Publication Details|Sections|\n|---|---|---|---|---|\n|[DKMR22]|Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, Lisheng Ren|Hardness of learning a single neuron with adversarial label noise|In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 8199\u20138213. PMLR, 28\u201330 Mar 2022.|1, 1.1, 5, 5.1|\n|[DKPZ21]|Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis|The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model|In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 1552\u20131584. PMLR, 15\u201319 Aug 2021.|1, 1.1|\n|[DKTZ20]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis|Non-convex sgd learns halfspaces with adversarial label noise|In Advances in Neural Information Processing Systems, volume 33, pages 18540\u201318549. Curran Associates, Inc., 2020.|4|\n|[DKTZ22]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis|Learning a single neuron with adversarial label noise via gradient descent|In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 4313\u20134361. PMLR, 02\u201305 Jul 2022.|1, 1, 1.1, 4|\n|[DKZ20]|Ilias Diakonikolas, Daniel Kane, Nikos Zarifis|Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals|In Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.|1, 1.1|\n|[FCG20]|Spencer Frei, Yuan Cao, Quanquan Gu|Agnostic learning of a single neuron with gradient descent|In Advances in Neural Information Processing Systems, 33:5417\u20135428, 2020.|1, 1.1|\n|[GGK20]|Surbhi Goel, Aravind Gollakota, Adam Klivans|Statistical-query lower bounds via functional gradients|In Advances in Neural Information Processing Systems, 33:2147\u20132158, 2020.|1, 1.1|\n|[GHK+23]|Parikshit Gopalan, Lunjia Hu, Michael P. Kim, Omer Reingold, Udi Wieder|Loss minimization through the lens of outcome indistinguishability|In 14th Innovations in Theoretical Computer Science Conference, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA, volume 251 of LIPIcs, pages 60:1\u201360:20. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2023.|1, 1.1, 3, 3, 3.2, C.1, C.2, C.1|\n|[GKK19]|Surbhi Goel, Sushrut Karmalkar, Adam Klivans|Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals|In Advances in neural information processing systems, 32, 2019.|1, 1.1|\n|[GKKT17]|Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler|Reliably learning the relu in polynomial time|In Conference on Learning Theory, pages 1004\u20131042. PMLR, 2017.|1.1|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["Reference", "Authors", "Title", "Publication Details", "Sections"], ["[DKMR22]", "Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, Lisheng Ren", "Hardness of learning a single neuron with adversarial label noise", "In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 8199\u20138213. PMLR, 28\u201330 Mar 2022.", "1, 1.1, 5, 5.1"], ["[DKPZ21]", "Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis", "The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model", "In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 1552\u20131584. PMLR, 15\u201319 Aug 2021.", "1, 1.1"], ["[DKTZ20]", "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis", "Non-convex sgd learns halfspaces with adversarial label noise", "In Advances in Neural Information Processing Systems, volume 33, pages 18540\u201318549. Curran Associates, Inc., 2020.", "4"], ["[DKTZ22]", "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis", "Learning a single neuron with adversarial label noise via gradient descent", "In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 4313\u20134361. PMLR, 02\u201305 Jul 2022.", "1, 1, 1.1, 4"], ["[DKZ20]", "Ilias Diakonikolas, Daniel Kane, Nikos Zarifis", "Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals", "In Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.", "1, 1.1"], ["[FCG20]", "Spencer Frei, Yuan Cao, Quanquan Gu", "Agnostic learning of a single neuron with gradient descent", "In Advances in Neural Information Processing Systems, 33:5417\u20135428, 2020.", "1, 1.1"], ["[GGK20]", "Surbhi Goel, Aravind Gollakota, Adam Klivans", "Statistical-query lower bounds via functional gradients", "In Advances in Neural Information Processing Systems, 33:2147\u20132158, 2020.", "1, 1.1"], ["[GHK+23]", "Parikshit Gopalan, Lunjia Hu, Michael P. Kim, Omer Reingold, Udi Wieder", "Loss minimization through the lens of outcome indistinguishability", "In 14th Innovations in Theoretical Computer Science Conference, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA, volume 251 of LIPIcs, pages 60:1\u201360:20. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2023.", "1, 1.1, 3, 3, 3.2, C.1, C.2, C.1"], ["[GKK19]", "Surbhi Goel, Sushrut Karmalkar, Adam Klivans", "Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals", "In Advances in neural information processing systems, 32, 2019.", "1, 1.1"], ["[GKKT17]", "Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler", "Reliably learning the relu in polynomial time", "In Conference on Learning Theory, pages 1004\u20131042. PMLR, 2017.", "1.1"]], "md": "|Reference|Authors|Title|Publication Details|Sections|\n|---|---|---|---|---|\n|[DKMR22]|Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, Lisheng Ren|Hardness of learning a single neuron with adversarial label noise|In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 8199\u20138213. PMLR, 28\u201330 Mar 2022.|1, 1.1, 5, 5.1|\n|[DKPZ21]|Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis|The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model|In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 1552\u20131584. PMLR, 15\u201319 Aug 2021.|1, 1.1|\n|[DKTZ20]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis|Non-convex sgd learns halfspaces with adversarial label noise|In Advances in Neural Information Processing Systems, volume 33, pages 18540\u201318549. Curran Associates, Inc., 2020.|4|\n|[DKTZ22]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis|Learning a single neuron with adversarial label noise via gradient descent|In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 4313\u20134361. PMLR, 02\u201305 Jul 2022.|1, 1, 1.1, 4|\n|[DKZ20]|Ilias Diakonikolas, Daniel Kane, Nikos Zarifis|Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals|In Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.|1, 1.1|\n|[FCG20]|Spencer Frei, Yuan Cao, Quanquan Gu|Agnostic learning of a single neuron with gradient descent|In Advances in Neural Information Processing Systems, 33:5417\u20135428, 2020.|1, 1.1|\n|[GGK20]|Surbhi Goel, Aravind Gollakota, Adam Klivans|Statistical-query lower bounds via functional gradients|In Advances in Neural Information Processing Systems, 33:2147\u20132158, 2020.|1, 1.1|\n|[GHK+23]|Parikshit Gopalan, Lunjia Hu, Michael P. Kim, Omer Reingold, Udi Wieder|Loss minimization through the lens of outcome indistinguishability|In 14th Innovations in Theoretical Computer Science Conference, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA, volume 251 of LIPIcs, pages 60:1\u201360:20. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2023.|1, 1.1, 3, 3, 3.2, C.1, C.2, C.1|\n|[GKK19]|Surbhi Goel, Sushrut Karmalkar, Adam Klivans|Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals|In Advances in neural information processing systems, 32, 2019.|1, 1.1|\n|[GKKT17]|Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler|Reliably learning the relu in polynomial time|In Conference on Learning Theory, pages 1004\u20131042. PMLR, 2017.|1.1|", "isPerfectTable": true, "csv": "\"Reference\",\"Authors\",\"Title\",\"Publication Details\",\"Sections\"\n\"[DKMR22]\",\"Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, Lisheng Ren\",\"Hardness of learning a single neuron with adversarial label noise\",\"In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 8199\u20138213. PMLR, 28\u201330 Mar 2022.\",\"1, 1.1, 5, 5.1\"\n\"[DKPZ21]\",\"Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis\",\"The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model\",\"In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 1552\u20131584. PMLR, 15\u201319 Aug 2021.\",\"1, 1.1\"\n\"[DKTZ20]\",\"Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis\",\"Non-convex sgd learns halfspaces with adversarial label noise\",\"In Advances in Neural Information Processing Systems, volume 33, pages 18540\u201318549. Curran Associates, Inc., 2020.\",\"4\"\n\"[DKTZ22]\",\"Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis\",\"Learning a single neuron with adversarial label noise via gradient descent\",\"In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 4313\u20134361. PMLR, 02\u201305 Jul 2022.\",\"1, 1, 1.1, 4\"\n\"[DKZ20]\",\"Ilias Diakonikolas, Daniel Kane, Nikos Zarifis\",\"Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals\",\"In Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.\",\"1, 1.1\"\n\"[FCG20]\",\"Spencer Frei, Yuan Cao, Quanquan Gu\",\"Agnostic learning of a single neuron with gradient descent\",\"In Advances in Neural Information Processing Systems, 33:5417\u20135428, 2020.\",\"1, 1.1\"\n\"[GGK20]\",\"Surbhi Goel, Aravind Gollakota, Adam Klivans\",\"Statistical-query lower bounds via functional gradients\",\"In Advances in Neural Information Processing Systems, 33:2147\u20132158, 2020.\",\"1, 1.1\"\n\"[GHK+23]\",\"Parikshit Gopalan, Lunjia Hu, Michael P. Kim, Omer Reingold, Udi Wieder\",\"Loss minimization through the lens of outcome indistinguishability\",\"In 14th Innovations in Theoretical Computer Science Conference, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA, volume 251 of LIPIcs, pages 60:1\u201360:20. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2023.\",\"1, 1.1, 3, 3, 3.2, C.1, C.2, C.1\"\n\"[GKK19]\",\"Surbhi Goel, Sushrut Karmalkar, Adam Klivans\",\"Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals\",\"In Advances in neural information processing systems, 32, 2019.\",\"1, 1.1\"\n\"[GKKT17]\",\"Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler\",\"Reliably learning the relu in polynomial time\",\"In Conference on Learning Theory, pages 1004\u20131042. PMLR, 2017.\",\"1.1\""}]}, {"page": 13, "text": "[GKR+22]     Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi\n             Wieder. Omnipredictors. In 13th Innovations in Theoretical Computer Science Con-\n             ference (ITCS 2022). Schloss Dagstuhl-Leibniz-Zentrum f\u00a8    ur Informatik, 2022. 1, 1.1\n[GSS19]      Friedrich G\u00a8otze, Holger Sambale, and Arthur Sinulis. Higher order concentration for\n             functions of weakly dependent random variables. Electronic Journal of Probability,\n             24:1\u201319, 2019. 4\n[HJKRR18] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicali-\n             bration: Calibration for the (Computationally-identifiable) masses. In Jennifer Dy and\n             Andreas Krause, editors, Proceedings of the 35th International Conference on Machine\n             Learning, volume 80 of Proceedings of Machine Learning Research, pages 1939\u20131948.\n             PMLR, 10\u201315 Jul 2018. 1.1\n[KGZ19]      Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-\n             processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Con-\n             ference on AI, Ethics, and Society, pages 247\u2013254, 2019. 1.1\n[KKSK11]     Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning\n             of generalized linear and single index models with isotonic regression. Advances in\n             Neural Information Processing Systems, 24, 2011. 1, 1, 1.1\n[KS94]       Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of prob-\n             abilistic concepts. Journal of Computer and System Sciences, 48(3):464\u2013497, 1994.\n             4\n[KS09]       Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional iso-\n             tonic regression. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal,\n             Quebec, Canada, June 18-21, 2009, 2009. 1, 1\n[McC84]      Peter McCullagh. Generalized linear models. European Journal of Operational Re-\n             search, 16(3):285\u2013292, 1984. 1, 1.1\n[OS08]       Ryan O\u2019Donnell and Rocco A Servedio. The chow parameters problem. In Proceedings\n             of the fortieth annual ACM symposium on Theory of computing, pages 517\u2013526, 2008.\n             1.1\n[SSSS11]     Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based\n             halfspaces with the 0-1 loss. SIAM Journal on Computing, 40(6):1623\u20131646, 2011. 1.1\n                                                  13", "md": "# References\n\n# References\n\n[GKR+22] Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi Wieder. Omnipredictors. In 13th Innovations in Theoretical Computer Science Conference (ITCS 2022). Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2022. 1, 1.1\n\n[GSS19] Friedrich G\u00f6tz, Holger Sambale, and Arthur Sinulis. Higher order concentration for functions of weakly dependent random variables. Electronic Journal of Probability, 24:1\u201319, 2019. 4\n\n[HJKRR18] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (Computationally-identifiable) masses. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1939\u20131948. PMLR, 10\u201315 Jul 2018. 1.1\n\n[KGZ19] Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 247\u2013254, 2019. 1.1\n\n[KKSK11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. Advances in Neural Information Processing Systems, 24, 2011. 1, 1, 1.1\n\n[KS94] Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of probabilistic concepts. Journal of Computer and System Sciences, 48(3):464\u2013497, 1994. 4\n\n[KS09] Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009, 2009. 1, 1\n\n[McC84] Peter McCullagh. Generalized linear models. European Journal of Operational Research, 16(3):285\u2013292, 1984. 1, 1.1\n\n[OS08] Ryan O\u2019Donnell and Rocco A Servedio. The chow parameters problem. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 517\u2013526, 2008. 1.1\n\n[SSSS11] Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM Journal on Computing, 40(6):1623\u20131646, 2011. 1.1", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "[GKR+22] Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi Wieder. Omnipredictors. In 13th Innovations in Theoretical Computer Science Conference (ITCS 2022). Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2022. 1, 1.1\n\n[GSS19] Friedrich G\u00f6tz, Holger Sambale, and Arthur Sinulis. Higher order concentration for functions of weakly dependent random variables. Electronic Journal of Probability, 24:1\u201319, 2019. 4\n\n[HJKRR18] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (Computationally-identifiable) masses. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1939\u20131948. PMLR, 10\u201315 Jul 2018. 1.1\n\n[KGZ19] Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 247\u2013254, 2019. 1.1\n\n[KKSK11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. Advances in Neural Information Processing Systems, 24, 2011. 1, 1, 1.1\n\n[KS94] Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of probabilistic concepts. Journal of Computer and System Sciences, 48(3):464\u2013497, 1994. 4\n\n[KS09] Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009, 2009. 1, 1\n\n[McC84] Peter McCullagh. Generalized linear models. European Journal of Operational Research, 16(3):285\u2013292, 1984. 1, 1.1\n\n[OS08] Ryan O\u2019Donnell and Rocco A Servedio. The chow parameters problem. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 517\u2013526, 2008. 1.1\n\n[SSSS11] Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM Journal on Computing, 40(6):1623\u20131646, 2011. 1.1", "md": "[GKR+22] Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi Wieder. Omnipredictors. In 13th Innovations in Theoretical Computer Science Conference (ITCS 2022). Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2022. 1, 1.1\n\n[GSS19] Friedrich G\u00f6tz, Holger Sambale, and Arthur Sinulis. Higher order concentration for functions of weakly dependent random variables. Electronic Journal of Probability, 24:1\u201319, 2019. 4\n\n[HJKRR18] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (Computationally-identifiable) masses. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1939\u20131948. PMLR, 10\u201315 Jul 2018. 1.1\n\n[KGZ19] Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 247\u2013254, 2019. 1.1\n\n[KKSK11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. Advances in Neural Information Processing Systems, 24, 2011. 1, 1, 1.1\n\n[KS94] Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of probabilistic concepts. Journal of Computer and System Sciences, 48(3):464\u2013497, 1994. 4\n\n[KS09] Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009, 2009. 1, 1\n\n[McC84] Peter McCullagh. Generalized linear models. European Journal of Operational Research, 16(3):285\u2013292, 1984. 1, 1.1\n\n[OS08] Ryan O\u2019Donnell and Rocco A Servedio. The chow parameters problem. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 517\u2013526, 2008. 1.1\n\n[SSSS11] Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM Journal on Computing, 40(6):1623\u20131646, 2011. 1.1"}]}, {"page": 14, "text": "A       Technical Lemmas\nIn this section, we provide some technical Lemmas that we use in our proofs.\nProposition A.1 (Weak Learner for Linear Functions). Let D be a distribution over Rd \u00d7 [\u22121, 1]\nwhose marginal on Rd has \u03bb-bounded second moments and B > 0. For any \u01eb > 0 and \u03b4 \u2208                                              (0, 1),\nthere is a universal constant C > 0 and an algorithm that given a set S of i.i.d. samples from D of\nsize at least C \u00b7 d2\u03bbB2  \u01eb2   log 1\u03b4 , runs in time poly(d, |S|) and satisfi             es the following specifi       cations with\nprobability at least 1 \u2212       \u03b4\n   1. If E   (x,z)\u223cD[z(w \u00b7 x)] \u2265          \u01eb for some w \u2208           Rd with \u2225w\u22252 \u2264            B, then the algorithm accepts.\n        Otherwise, it may or may not reject and return a special symbol.\n   2. If the algorithm accepts then it returns w \u2208                 Rd with \u2225w\u22252 \u2264         B such that we have E(x,z)\u223cD[z(w\u00b7\n        x)] \u2265   \u01eb/4.\nProof. We will prove the proposition for \u03b4 = 1/6. We may boost the probability of success with\nrepetition.\n     The algorithm computes the vector v = ES[z x]. If \u2225v\u22252 \u2264                            3\u01eb\n                                                                                    B   4B , then the algorithm rejects and\noutputs a special symbol. Otherwise, it outputs the vector                         \u2225v\u22252 v.\n     Suppose, first, that E       (x,z)\u223cD[z(w\u00b7x)] \u2265        \u01eb for some w with \u2225w\u22252 \u2264             B. Then, due to Chebyshev\u2019s\ninequality we have for any i \u2208            [d]\n       P    E                       >      \u01eb\u221a      \u2264   64 d B2         i ] \u2264  64 d B2 \u03bb    \u2264    1         (for large enough |S|)\n            S[z xi] \u2212    E\n                         D[z xi]        8 B    d         |S| \u01eb2 E[x2            |S| \u01eb2         6 d  \u01eb\nHence, with probability at least 5/6, we have \u2225                    ES[zx] \u2212      ED[zx]\u22252 \u2264        8B , due to a union bound.\n                                              \u01eb                        \u01eb\nTherefore, \u2225v\u22252 \u2265         \u2225 ED[zx]\u22252 \u2212       8B \u2265    ED[z(w\u00b7x)]   \u2212\n                                                          B           8B \u2265    7B\n                                                                               8 and the algorithm accepts.\n     Suppose, now, that the algorithm accepts. Then, we have \u2225v\u22252 > 3\u01eb                            4B and (with probability at\nleast 5/6) we have                      E      B    z(v \u00b7 x)     =     B    v \u00b7 E[zx] \u2265     \u01eb/4\n                                        D\u01eb   \u2225v\u22252                    \u2225v\u22252\nsince \u2225  ES[zx] \u2212      ED[zx]\u22252 \u2264       8B . This concludes the proof.\nProposition A.2. Let D be a distribution over Rd \u00d7 {0, 1} and p : Rd \u2192                                       [0, 1]. Consider the\ndistribution Dp over Rd\u00d7{0, 1}\u00d7{0, 1}, which is formed by drawing samples (x, y) from D and, given\nx, forming yp by drawing a conditionally independent Bernoulli random variable with parameter\np(x). Then we have\n                                  err1(p) =         E                              P       [y \u0338= yp]\n                                                (x,y)\u223cD[|y \u2212     p(x)|] =    (x,y,yp)\u223cDp\nProof. Since over Dp, y and yp are conditionally independendent, we have\n                err1(p) = E[|y \u2212       p(x)|] = E[(1 \u2212       p(x))  1{y = 1} + p(x)1{y = 0}]\n                                               = E x   P[yp = 0|x] P[y = 1|x] + P[yp = 1|x] P[y = 0|x]\n                                               = E x[P[y \u0338= yp|x]] = P[y \u0338= yp]\n                                                                  14", "md": "## Technical Lemmas\n\nIn this section, we provide some technical Lemmas that we use in our proofs.\n\n### Proposition A.1 (Weak Learner for Linear Functions)\n\nLet D be a distribution over \\( \\mathbb{R}^d \\times [-1, 1] \\) whose marginal on \\( \\mathbb{R}^d \\) has \u03bb-bounded second moments and \\( B > 0 \\). For any \\( \\epsilon > 0 \\) and \\( \\delta \\in (0, 1) \\), there is a universal constant \\( C > 0 \\) and an algorithm that given a set S of i.i.d. samples from D of size at least \\( C \\cdot d^2\\lambda B^2 \\epsilon^2 \\log \\frac{1}{\\delta} \\), runs in time poly(d, |S|) and satisfies the following specifications with probability at least \\( 1 - \\delta \\):\n\n1. If \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon \\) for some \\( w \\in \\mathbb{R}^d \\) with \\( \\|w\\|_2 \\leq B \\), then the algorithm accepts. Otherwise, it may or may not reject and return a special symbol.\n2. If the algorithm accepts then it returns \\( w \\in \\mathbb{R}^d \\) with \\( \\|w\\|_2 \\leq B \\) such that we have \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon/4 \\).\n\nProof: We will prove the proposition for \\( \\delta = 1/6 \\). We may boost the probability of success with repetition.\n\nThe algorithm computes the vector \\( v = \\mathbb{E}_S[z x] \\). If \\( \\|v\\|_2 \\leq \\frac{3\\epsilon}{4B} \\), then the algorithm rejects and outputs a special symbol. Otherwise, it outputs the vector \\( \\frac{\\|v\\|_2}{B} v \\).\n\nSuppose, first, that \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon \\) for some \\( w \\) with \\( \\|w\\|_2 \\leq B \\). Then, due to Chebyshev\u2019s inequality we have for any \\( i \\in [d] \\):\n\n$$\n\\begin{align*}\nP\\left( \\left| \\mathbb{E}_S[z x_i] - \\mathbb{E}_D[z x_i] \\right| > \\frac{\\epsilon}{\\sqrt{8}} \\right) & \\leq \\frac{64 d B^2}{i} \\leq \\frac{64 d B^2 \\lambda}{i} \\leq 1 \\quad \\text{(for large enough } |S| \\text{)}\n\\end{align*}\n$$\nHence, with probability at least \\( 5/6 \\), we have \\( \\| \\mathbb{E}_S[zx] - \\mathbb{E}_D[zx] \\|_2 \\leq \\frac{8B}{\\epsilon} \\), due to a union bound. Therefore, \\( \\|v\\|_2 \\geq \\epsilon - \\frac{8B}{\\epsilon} \\geq \\frac{7B}{8} \\) and the algorithm accepts.\n\nSuppose, now, that the algorithm accepts. Then, we have \\( \\|v\\|_2 > \\frac{3\\epsilon}{4B} \\) and (with probability at least \\( 5/6 \\)) we have \\( \\mathbb{E}_{D_\\epsilon}[\\|v\\|_2 z(v \\cdot x)] = \\frac{\\epsilon}{\\|v\\|_2} \\) since \\( \\| \\mathbb{E}_S[zx] - \\mathbb{E}_D[zx] \\|_2 \\leq \\frac{8B}{\\epsilon} \\). This concludes the proof.\n\n### Proposition A.2\n\nLet D be a distribution over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\) and \\( p : \\mathbb{R}^d \\to [0, 1] \\). Consider the distribution \\( D_p \\) over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\times \\{0, 1\\} \\), which is formed by drawing samples \\( (x, y) \\) from D and, given \\( x \\), forming \\( y_p \\) by drawing a conditionally independent Bernoulli random variable with parameter \\( p(x) \\). Then we have:\n\n$$\n\\begin{align*}\n\\text{err}_1(p) & = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|] = \\mathbb{E}_{(x,y,y_p) \\sim D_p}[y \\neq y_p]\n\\end{align*}\n$$\n\nProof: Since over \\( D_p \\), \\( y \\) and \\( y_p \\) are conditionally independent, we have:\n\n$$\n\\begin{align*}\n\\text{err}_1(p) & = \\mathbb{E}[|y - p(x)|] = \\mathbb{E}[(1 - p(x)) \\mathbb{1}_{\\{y = 1\\}} + p(x) \\mathbb{1}_{\\{y = 0\\}}] \\\\\n& = \\mathbb{E}_x [P[y_p = 0 | x] P[y = 1 | x] + P[y_p = 1 | x] P[y = 0 | x]] \\\\\n& = \\mathbb{E}_x [P[y \\neq y_p | x]] = P[y \\neq y_p]\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Technical Lemmas", "md": "## Technical Lemmas"}, {"type": "text", "value": "In this section, we provide some technical Lemmas that we use in our proofs.", "md": "In this section, we provide some technical Lemmas that we use in our proofs."}, {"type": "heading", "lvl": 3, "value": "Proposition A.1 (Weak Learner for Linear Functions)", "md": "### Proposition A.1 (Weak Learner for Linear Functions)"}, {"type": "text", "value": "Let D be a distribution over \\( \\mathbb{R}^d \\times [-1, 1] \\) whose marginal on \\( \\mathbb{R}^d \\) has \u03bb-bounded second moments and \\( B > 0 \\). For any \\( \\epsilon > 0 \\) and \\( \\delta \\in (0, 1) \\), there is a universal constant \\( C > 0 \\) and an algorithm that given a set S of i.i.d. samples from D of size at least \\( C \\cdot d^2\\lambda B^2 \\epsilon^2 \\log \\frac{1}{\\delta} \\), runs in time poly(d, |S|) and satisfies the following specifications with probability at least \\( 1 - \\delta \\):\n\n1. If \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon \\) for some \\( w \\in \\mathbb{R}^d \\) with \\( \\|w\\|_2 \\leq B \\), then the algorithm accepts. Otherwise, it may or may not reject and return a special symbol.\n2. If the algorithm accepts then it returns \\( w \\in \\mathbb{R}^d \\) with \\( \\|w\\|_2 \\leq B \\) such that we have \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon/4 \\).\n\nProof: We will prove the proposition for \\( \\delta = 1/6 \\). We may boost the probability of success with repetition.\n\nThe algorithm computes the vector \\( v = \\mathbb{E}_S[z x] \\). If \\( \\|v\\|_2 \\leq \\frac{3\\epsilon}{4B} \\), then the algorithm rejects and outputs a special symbol. Otherwise, it outputs the vector \\( \\frac{\\|v\\|_2}{B} v \\).\n\nSuppose, first, that \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon \\) for some \\( w \\) with \\( \\|w\\|_2 \\leq B \\). Then, due to Chebyshev\u2019s inequality we have for any \\( i \\in [d] \\):\n\n$$\n\\begin{align*}\nP\\left( \\left| \\mathbb{E}_S[z x_i] - \\mathbb{E}_D[z x_i] \\right| > \\frac{\\epsilon}{\\sqrt{8}} \\right) & \\leq \\frac{64 d B^2}{i} \\leq \\frac{64 d B^2 \\lambda}{i} \\leq 1 \\quad \\text{(for large enough } |S| \\text{)}\n\\end{align*}\n$$\nHence, with probability at least \\( 5/6 \\), we have \\( \\| \\mathbb{E}_S[zx] - \\mathbb{E}_D[zx] \\|_2 \\leq \\frac{8B}{\\epsilon} \\), due to a union bound. Therefore, \\( \\|v\\|_2 \\geq \\epsilon - \\frac{8B}{\\epsilon} \\geq \\frac{7B}{8} \\) and the algorithm accepts.\n\nSuppose, now, that the algorithm accepts. Then, we have \\( \\|v\\|_2 > \\frac{3\\epsilon}{4B} \\) and (with probability at least \\( 5/6 \\)) we have \\( \\mathbb{E}_{D_\\epsilon}[\\|v\\|_2 z(v \\cdot x)] = \\frac{\\epsilon}{\\|v\\|_2} \\) since \\( \\| \\mathbb{E}_S[zx] - \\mathbb{E}_D[zx] \\|_2 \\leq \\frac{8B}{\\epsilon} \\). This concludes the proof.", "md": "Let D be a distribution over \\( \\mathbb{R}^d \\times [-1, 1] \\) whose marginal on \\( \\mathbb{R}^d \\) has \u03bb-bounded second moments and \\( B > 0 \\). For any \\( \\epsilon > 0 \\) and \\( \\delta \\in (0, 1) \\), there is a universal constant \\( C > 0 \\) and an algorithm that given a set S of i.i.d. samples from D of size at least \\( C \\cdot d^2\\lambda B^2 \\epsilon^2 \\log \\frac{1}{\\delta} \\), runs in time poly(d, |S|) and satisfies the following specifications with probability at least \\( 1 - \\delta \\):\n\n1. If \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon \\) for some \\( w \\in \\mathbb{R}^d \\) with \\( \\|w\\|_2 \\leq B \\), then the algorithm accepts. Otherwise, it may or may not reject and return a special symbol.\n2. If the algorithm accepts then it returns \\( w \\in \\mathbb{R}^d \\) with \\( \\|w\\|_2 \\leq B \\) such that we have \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon/4 \\).\n\nProof: We will prove the proposition for \\( \\delta = 1/6 \\). We may boost the probability of success with repetition.\n\nThe algorithm computes the vector \\( v = \\mathbb{E}_S[z x] \\). If \\( \\|v\\|_2 \\leq \\frac{3\\epsilon}{4B} \\), then the algorithm rejects and outputs a special symbol. Otherwise, it outputs the vector \\( \\frac{\\|v\\|_2}{B} v \\).\n\nSuppose, first, that \\( \\mathbb{E}_{(x,z) \\sim D}[z(w \\cdot x)] \\geq \\epsilon \\) for some \\( w \\) with \\( \\|w\\|_2 \\leq B \\). Then, due to Chebyshev\u2019s inequality we have for any \\( i \\in [d] \\):\n\n$$\n\\begin{align*}\nP\\left( \\left| \\mathbb{E}_S[z x_i] - \\mathbb{E}_D[z x_i] \\right| > \\frac{\\epsilon}{\\sqrt{8}} \\right) & \\leq \\frac{64 d B^2}{i} \\leq \\frac{64 d B^2 \\lambda}{i} \\leq 1 \\quad \\text{(for large enough } |S| \\text{)}\n\\end{align*}\n$$\nHence, with probability at least \\( 5/6 \\), we have \\( \\| \\mathbb{E}_S[zx] - \\mathbb{E}_D[zx] \\|_2 \\leq \\frac{8B}{\\epsilon} \\), due to a union bound. Therefore, \\( \\|v\\|_2 \\geq \\epsilon - \\frac{8B}{\\epsilon} \\geq \\frac{7B}{8} \\) and the algorithm accepts.\n\nSuppose, now, that the algorithm accepts. Then, we have \\( \\|v\\|_2 > \\frac{3\\epsilon}{4B} \\) and (with probability at least \\( 5/6 \\)) we have \\( \\mathbb{E}_{D_\\epsilon}[\\|v\\|_2 z(v \\cdot x)] = \\frac{\\epsilon}{\\|v\\|_2} \\) since \\( \\| \\mathbb{E}_S[zx] - \\mathbb{E}_D[zx] \\|_2 \\leq \\frac{8B}{\\epsilon} \\). This concludes the proof."}, {"type": "heading", "lvl": 3, "value": "Proposition A.2", "md": "### Proposition A.2"}, {"type": "text", "value": "Let D be a distribution over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\) and \\( p : \\mathbb{R}^d \\to [0, 1] \\). Consider the distribution \\( D_p \\) over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\times \\{0, 1\\} \\), which is formed by drawing samples \\( (x, y) \\) from D and, given \\( x \\), forming \\( y_p \\) by drawing a conditionally independent Bernoulli random variable with parameter \\( p(x) \\). Then we have:\n\n$$\n\\begin{align*}\n\\text{err}_1(p) & = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|] = \\mathbb{E}_{(x,y,y_p) \\sim D_p}[y \\neq y_p]\n\\end{align*}\n$$\n\nProof: Since over \\( D_p \\), \\( y \\) and \\( y_p \\) are conditionally independent, we have:\n\n$$\n\\begin{align*}\n\\text{err}_1(p) & = \\mathbb{E}[|y - p(x)|] = \\mathbb{E}[(1 - p(x)) \\mathbb{1}_{\\{y = 1\\}} + p(x) \\mathbb{1}_{\\{y = 0\\}}] \\\\\n& = \\mathbb{E}_x [P[y_p = 0 | x] P[y = 1 | x] + P[y_p = 1 | x] P[y = 0 | x]] \\\\\n& = \\mathbb{E}_x [P[y \\neq y_p | x]] = P[y \\neq y_p]\n\\end{align*}\n$$", "md": "Let D be a distribution over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\) and \\( p : \\mathbb{R}^d \\to [0, 1] \\). Consider the distribution \\( D_p \\) over \\( \\mathbb{R}^d \\times \\{0, 1\\} \\times \\{0, 1\\} \\), which is formed by drawing samples \\( (x, y) \\) from D and, given \\( x \\), forming \\( y_p \\) by drawing a conditionally independent Bernoulli random variable with parameter \\( p(x) \\). Then we have:\n\n$$\n\\begin{align*}\n\\text{err}_1(p) & = \\mathbb{E}_{(x,y) \\sim D}[|y - p(x)|] = \\mathbb{E}_{(x,y,y_p) \\sim D_p}[y \\neq y_p]\n\\end{align*}\n$$\n\nProof: Since over \\( D_p \\), \\( y \\) and \\( y_p \\) are conditionally independent, we have:\n\n$$\n\\begin{align*}\n\\text{err}_1(p) & = \\mathbb{E}[|y - p(x)|] = \\mathbb{E}[(1 - p(x)) \\mathbb{1}_{\\{y = 1\\}} + p(x) \\mathbb{1}_{\\{y = 0\\}}] \\\\\n& = \\mathbb{E}_x [P[y_p = 0 | x] P[y = 1 | x] + P[y_p = 1 | x] P[y = 0 | x]] \\\\\n& = \\mathbb{E}_x [P[y \\neq y_p | x]] = P[y \\neq y_p]\n\\end{align*}\n$$"}]}, {"page": 15, "text": "B     Proofs from Section 2\nB.1    Proof of Theorem 2.1\nTo prove Theorem 2.1, we first prove the following more general theorem. Theorem 2.1 may then\nbe easily recovered from this by setting H = GLMg\u2032,B and observing that f \u2032(g\u2032(w \u00b7 x)) = w \u00b7 x,\nsince g\u2032 is invertible.\nTheorem B.1 (Squared Error Minimization through Distorted Matching Loss Minimization). Let\nD be a distribution over Rd \u00d7 [0, 1], let 0 < \u03b1 \u2264         \u03b2 and let (f, g) be a pair of Fenchel-Legendre\ndual functions such that g\u2032 : R \u2192      R is continuous, non-decreasing and f \u2032 : ran(g\u2032) \u2192        R is [ 1\n                                                                                                         \u03b2, 1\n                                                                                                            \u03b1]\nbi-Lipschitz. Let \u01eb > 0 and H \u2286      {Rd \u2192   ran(g\u2032)}. Assume that for a predictor p : Rd \u2192        ran(g\u2032) we\nhave\n                                  Lg(f \u2032 \u25e6 p ; D) \u2264 min                                                  (B.1)\n                                                    h\u2208H Lg(f \u2032 \u25e6 h ; D) + \u01eb\nThen, for the predictor p, we also have: err2(p) \u2264       \u03b2\n                                                         \u03b1 \u00b7 minh\u2208H err2(h) + 2\u03b2\u01eb.\nProof. We apply Lemma 2.2 with y \u2190         y and p \u2190    p(x) and take expectations over D on both sides.\nWe have that\n                           err2(p) \u2264  2\u03b2 \u00b7 E \u2113g(y, f \u2032(p(x))) \u2212 2\u03b2 \u00b7 E \u2113g(y, f \u2032(y))\n    Therefore, we can bound the squared error of p as follows.\n                err2(p) \u2264  2\u03b2 \u00b7 Lg(f \u2032 \u25e6 p ; D) \u2212 2\u03b2 \u00b7 Q\u2217\n                         \u2264 2\u03b2 \u00b7 Lg(f \u2032 \u25e6 h ; D) \u2212 2\u03b2 \u00b7 Q\u2217                  (by assumption, for any h \u2208     H)\nwhere Q\u2217   = E \u2113g(y, f \u2032(y)).\n    We now apply Lemma 2.2 again with y \u2190           y and p \u2190    h(x) and we similarly have\n                                   E \u2113g(y, f \u2032 \u25e6h(x)) \u2212  Q\u2217  \u2264   1\n                                                                2\u03b1err2(h)\nTherefore, for any h \u2208   H, we have, in total: err2(p) \u2264     \u03b2\n                                                             \u03b1err2(h) + 2\u03b2\u01eb.\n    We first prove Lemma 2.2, which we restate here for convenience.\nLemma B.2. Assume f \u2032 is [1/\u03b2, 1/\u03b1] bi-Lipschitz and differentiable on all except from a fi               nite\nnumber of points on any bounded interval. Then for any y, p \u2208          ran(g\u2032) we have\n                   \u2113g(y, f \u2032(p)) \u2212\u2113g(y, f \u2032(y)) = Df(y, p) \u2208     1\n                                                                2\u03b2 (y \u2212  p)2, 1\n                                                                              2\u03b1(y \u2212  p)2\nProof. We first show that \u2113g(y, f \u2032(p)) \u2212    \u2113g(y, f \u2032(y)) = Df(y, p). In particular, we have\n                       g(f \u2032(p)) = f \u2032(p)g\u2032(f \u2032(p)) \u2212 f(g\u2032(f \u2032(p))) = pf \u2032(p) \u2212 f(p) ,\nsince f \u2032(p) \u2208  ran(f \u2032) and we know that g(t) = tg\u2032(t) \u2212         f(g\u2032(t)) for any t \u2208    ran(f \u2032) as well as\ng\u2032(f \u2032(p)) = p for any p \u2208  ran(g\u2032). Therefore, we have\n           \u2113g(y, f \u2032(p)) \u2212 \u2113g(y, f \u2032(y)) = g(f \u2032(p)) \u2212 yf \u2032(p) \u2212 g(f \u2032(y)) + yf \u2032(y)\n                                        = pf \u2032(p) \u2212  f(p) \u2212  yf \u2032(p) \u2212 yf \u2032(y) + f(y) + yf \u2032(y) =\n                                        = f(y) \u2212   f(p) \u2212  (y \u2212 p)f \u2032(p) = Df(y, p) .\n                                                      15", "md": "# Proofs from Section 2\n\n## Proof of Theorem 2.1\n\nTo prove Theorem 2.1, we first prove the following more general theorem. Theorem 2.1 may then be easily recovered from this by setting $$H = GLMg',B$$ and observing that $$f'(g'(w \\cdot x)) = w \\cdot x$$, since $$g'$$ is invertible.\n\nTheorem B.1 (Squared Error Minimization through Distorted Matching Loss Minimization). Let D be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$, let $$0 < \\alpha \\leq \\beta$$ and let $$(f, g)$$ be a pair of Fenchel-Legendre dual functions such that $$g': \\mathbb{R} \\rightarrow \\mathbb{R}$$ is continuous, non-decreasing and $$f': \\text{ran}(g') \\rightarrow \\mathbb{R}$$ is $$\\left[\\frac{1}{\\beta}, \\frac{1}{\\alpha}\\right]$$ bi-Lipschitz. Let $$\\epsilon > 0$$ and $$H \\subseteq \\{ \\mathbb{R}^d \\rightarrow \\text{ran}(g') \\}$$. Assume that for a predictor $$p: \\mathbb{R}^d \\rightarrow \\text{ran}(g')$$ we have\n\n$$Lg(f' \\circ p ; D) \\leq \\min_{h \\in H} Lg(f' \\circ h ; D) + \\epsilon$$\n\nThen, for the predictor $$p$$, we also have: $$\\text{err}^2(p) \\leq \\frac{\\beta}{\\alpha} \\cdot \\min_{h \\in H} \\text{err}^2(h) + 2\\beta\\epsilon$$.\n\n### Proof:\n\nWe apply Lemma 2.2 with $$y \\leftarrow y$$ and $$p \\leftarrow p(x)$$ and take expectations over $$D$$ on both sides. We have that\n\n$$\\text{err}^2(p) \\leq 2\\beta \\cdot E \\ell_g(y, f'(p(x))) - 2\\beta \\cdot E \\ell_g(y, f'(y))$$\n\nTherefore, we can bound the squared error of $$p$$ as follows.\n\n$$\\text{err}^2(p) \\leq 2\\beta \\cdot Lg(f' \\circ p ; D) - 2\\beta \\cdot Q^*$$\n\n$$\\leq 2\\beta \\cdot Lg(f' \\circ h ; D) - 2\\beta \\cdot Q^*$$ (by assumption, for any $$h \\in H$$)\n\nwhere $$Q^* = E \\ell_g(y, f'(y))$$.\n\nWe now apply Lemma 2.2 again with $$y \\leftarrow y$$ and $$p \\leftarrow h(x)$$ and we similarly have\n\n$$E \\ell_g(y, f' \\circ h(x)) - Q^* \\leq \\frac{1}{2\\alpha} \\text{err}^2(h)$$\n\nTherefore, for any $$h \\in H$$, we have, in total: $$\\text{err}^2(p) \\leq \\frac{\\beta}{\\alpha} \\text{err}^2(h) + 2\\beta\\epsilon$$.\n\nWe first prove Lemma 2.2, which we restate here for convenience.\n\nLemma B.2. Assume $$f'$$ is $$\\left[\\frac{1}{\\beta}, \\frac{1}{\\alpha}\\right]$$ bi-Lipschitz and differentiable on all except from a finite number of points on any bounded interval. Then for any $$y, p \\in \\text{ran}(g')$$ we have\n\n$$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = Df(y, p) \\in \\frac{1}{2\\beta} (y - p)^2, \\frac{1}{2\\alpha} (y - p)^2$$\n\n### Proof:\n\nWe first show that $$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = Df(y, p)$$. In particular, we have\n\n$$g(f'(p)) = f'(p)g'(f'(p)) - f(g'(f'(p))) = pf'(p) - f(p)$$\n\nsince $$f'(p) \\in \\text{ran}(f')$$ and we know that $$g(t) = tg'(t) - f(g'(t))$$ for any $$t \\in \\text{ran}(f')$$ as well as $$g'(f'(p)) = p$$ for any $$p \\in \\text{ran}(g')$$. Therefore, we have\n\n$$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = g(f'(p)) - yf'(p) - g(f'(y)) + yf'(y)$$\n\n$$= pf'(p) - f(p) - yf'(p) - yf'(y) + f(y) + yf'(y)$$\n\n$$= f(y) - f(p) - (y - p)f'(p) = Df(y, p)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Proofs from Section 2", "md": "# Proofs from Section 2"}, {"type": "heading", "lvl": 2, "value": "Proof of Theorem 2.1", "md": "## Proof of Theorem 2.1"}, {"type": "text", "value": "To prove Theorem 2.1, we first prove the following more general theorem. Theorem 2.1 may then be easily recovered from this by setting $$H = GLMg',B$$ and observing that $$f'(g'(w \\cdot x)) = w \\cdot x$$, since $$g'$$ is invertible.\n\nTheorem B.1 (Squared Error Minimization through Distorted Matching Loss Minimization). Let D be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$, let $$0 < \\alpha \\leq \\beta$$ and let $$(f, g)$$ be a pair of Fenchel-Legendre dual functions such that $$g': \\mathbb{R} \\rightarrow \\mathbb{R}$$ is continuous, non-decreasing and $$f': \\text{ran}(g') \\rightarrow \\mathbb{R}$$ is $$\\left[\\frac{1}{\\beta}, \\frac{1}{\\alpha}\\right]$$ bi-Lipschitz. Let $$\\epsilon > 0$$ and $$H \\subseteq \\{ \\mathbb{R}^d \\rightarrow \\text{ran}(g') \\}$$. Assume that for a predictor $$p: \\mathbb{R}^d \\rightarrow \\text{ran}(g')$$ we have\n\n$$Lg(f' \\circ p ; D) \\leq \\min_{h \\in H} Lg(f' \\circ h ; D) + \\epsilon$$\n\nThen, for the predictor $$p$$, we also have: $$\\text{err}^2(p) \\leq \\frac{\\beta}{\\alpha} \\cdot \\min_{h \\in H} \\text{err}^2(h) + 2\\beta\\epsilon$$.", "md": "To prove Theorem 2.1, we first prove the following more general theorem. Theorem 2.1 may then be easily recovered from this by setting $$H = GLMg',B$$ and observing that $$f'(g'(w \\cdot x)) = w \\cdot x$$, since $$g'$$ is invertible.\n\nTheorem B.1 (Squared Error Minimization through Distorted Matching Loss Minimization). Let D be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$, let $$0 < \\alpha \\leq \\beta$$ and let $$(f, g)$$ be a pair of Fenchel-Legendre dual functions such that $$g': \\mathbb{R} \\rightarrow \\mathbb{R}$$ is continuous, non-decreasing and $$f': \\text{ran}(g') \\rightarrow \\mathbb{R}$$ is $$\\left[\\frac{1}{\\beta}, \\frac{1}{\\alpha}\\right]$$ bi-Lipschitz. Let $$\\epsilon > 0$$ and $$H \\subseteq \\{ \\mathbb{R}^d \\rightarrow \\text{ran}(g') \\}$$. Assume that for a predictor $$p: \\mathbb{R}^d \\rightarrow \\text{ran}(g')$$ we have\n\n$$Lg(f' \\circ p ; D) \\leq \\min_{h \\in H} Lg(f' \\circ h ; D) + \\epsilon$$\n\nThen, for the predictor $$p$$, we also have: $$\\text{err}^2(p) \\leq \\frac{\\beta}{\\alpha} \\cdot \\min_{h \\in H} \\text{err}^2(h) + 2\\beta\\epsilon$$."}, {"type": "heading", "lvl": 3, "value": "Proof:", "md": "### Proof:"}, {"type": "text", "value": "We apply Lemma 2.2 with $$y \\leftarrow y$$ and $$p \\leftarrow p(x)$$ and take expectations over $$D$$ on both sides. We have that\n\n$$\\text{err}^2(p) \\leq 2\\beta \\cdot E \\ell_g(y, f'(p(x))) - 2\\beta \\cdot E \\ell_g(y, f'(y))$$\n\nTherefore, we can bound the squared error of $$p$$ as follows.\n\n$$\\text{err}^2(p) \\leq 2\\beta \\cdot Lg(f' \\circ p ; D) - 2\\beta \\cdot Q^*$$\n\n$$\\leq 2\\beta \\cdot Lg(f' \\circ h ; D) - 2\\beta \\cdot Q^*$$ (by assumption, for any $$h \\in H$$)\n\nwhere $$Q^* = E \\ell_g(y, f'(y))$$.\n\nWe now apply Lemma 2.2 again with $$y \\leftarrow y$$ and $$p \\leftarrow h(x)$$ and we similarly have\n\n$$E \\ell_g(y, f' \\circ h(x)) - Q^* \\leq \\frac{1}{2\\alpha} \\text{err}^2(h)$$\n\nTherefore, for any $$h \\in H$$, we have, in total: $$\\text{err}^2(p) \\leq \\frac{\\beta}{\\alpha} \\text{err}^2(h) + 2\\beta\\epsilon$$.\n\nWe first prove Lemma 2.2, which we restate here for convenience.\n\nLemma B.2. Assume $$f'$$ is $$\\left[\\frac{1}{\\beta}, \\frac{1}{\\alpha}\\right]$$ bi-Lipschitz and differentiable on all except from a finite number of points on any bounded interval. Then for any $$y, p \\in \\text{ran}(g')$$ we have\n\n$$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = Df(y, p) \\in \\frac{1}{2\\beta} (y - p)^2, \\frac{1}{2\\alpha} (y - p)^2$$", "md": "We apply Lemma 2.2 with $$y \\leftarrow y$$ and $$p \\leftarrow p(x)$$ and take expectations over $$D$$ on both sides. We have that\n\n$$\\text{err}^2(p) \\leq 2\\beta \\cdot E \\ell_g(y, f'(p(x))) - 2\\beta \\cdot E \\ell_g(y, f'(y))$$\n\nTherefore, we can bound the squared error of $$p$$ as follows.\n\n$$\\text{err}^2(p) \\leq 2\\beta \\cdot Lg(f' \\circ p ; D) - 2\\beta \\cdot Q^*$$\n\n$$\\leq 2\\beta \\cdot Lg(f' \\circ h ; D) - 2\\beta \\cdot Q^*$$ (by assumption, for any $$h \\in H$$)\n\nwhere $$Q^* = E \\ell_g(y, f'(y))$$.\n\nWe now apply Lemma 2.2 again with $$y \\leftarrow y$$ and $$p \\leftarrow h(x)$$ and we similarly have\n\n$$E \\ell_g(y, f' \\circ h(x)) - Q^* \\leq \\frac{1}{2\\alpha} \\text{err}^2(h)$$\n\nTherefore, for any $$h \\in H$$, we have, in total: $$\\text{err}^2(p) \\leq \\frac{\\beta}{\\alpha} \\text{err}^2(h) + 2\\beta\\epsilon$$.\n\nWe first prove Lemma 2.2, which we restate here for convenience.\n\nLemma B.2. Assume $$f'$$ is $$\\left[\\frac{1}{\\beta}, \\frac{1}{\\alpha}\\right]$$ bi-Lipschitz and differentiable on all except from a finite number of points on any bounded interval. Then for any $$y, p \\in \\text{ran}(g')$$ we have\n\n$$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = Df(y, p) \\in \\frac{1}{2\\beta} (y - p)^2, \\frac{1}{2\\alpha} (y - p)^2$$"}, {"type": "heading", "lvl": 3, "value": "Proof:", "md": "### Proof:"}, {"type": "text", "value": "We first show that $$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = Df(y, p)$$. In particular, we have\n\n$$g(f'(p)) = f'(p)g'(f'(p)) - f(g'(f'(p))) = pf'(p) - f(p)$$\n\nsince $$f'(p) \\in \\text{ran}(f')$$ and we know that $$g(t) = tg'(t) - f(g'(t))$$ for any $$t \\in \\text{ran}(f')$$ as well as $$g'(f'(p)) = p$$ for any $$p \\in \\text{ran}(g')$$. Therefore, we have\n\n$$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = g(f'(p)) - yf'(p) - g(f'(y)) + yf'(y)$$\n\n$$= pf'(p) - f(p) - yf'(p) - yf'(y) + f(y) + yf'(y)$$\n\n$$= f(y) - f(p) - (y - p)f'(p) = Df(y, p)$$.", "md": "We first show that $$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = Df(y, p)$$. In particular, we have\n\n$$g(f'(p)) = f'(p)g'(f'(p)) - f(g'(f'(p))) = pf'(p) - f(p)$$\n\nsince $$f'(p) \\in \\text{ran}(f')$$ and we know that $$g(t) = tg'(t) - f(g'(t))$$ for any $$t \\in \\text{ran}(f')$$ as well as $$g'(f'(p)) = p$$ for any $$p \\in \\text{ran}(g')$$. Therefore, we have\n\n$$\\ell_g(y, f'(p)) - \\ell_g(y, f'(y)) = g(f'(p)) - yf'(p) - g(f'(y)) + yf'(y)$$\n\n$$= pf'(p) - f(p) - yf'(p) - yf'(y) + f(y) + yf'(y)$$\n\n$$= f(y) - f(p) - (y - p)f'(p) = Df(y, p)$$."}]}, {"page": 16, "text": "      Let \u03c8 : ran(g\u2032) \u2192             R be such that \u03c8\u2032(p) = f \u2032(p) and \u03c8\u2032 is differentiable on the open interval\nbetween y and p, with \u03c8\u2032\u2032(\u03be) \u2208                    [1/\u03b2, 1/\u03b1] for any \u03be between y and p. Let \u03b3y := f(y) \u2212                                  \u03c8(y), \u03b3p :=\nf(p) \u2212      \u03c8(p) and \u03b3\u03c8 := 2 max{|\u03b3y|, |\u03b3p|}. Then we have that\n               Df(y, p) = \u03c8(y) \u2212            \u03c8(p) \u2212      (y \u2212    p)\u03c8\u2032(p) + (\u03b3y \u2212          \u03b3p) = 1   2\u03c8\u2032\u2032(\u03be)(y \u2212       p)2 + (\u03b3y \u2212        \u03b3p)\n                                      Df(y, p) \u2208          1                                                          ,\n                                                         2\u03b2 (y \u2212      p)2 \u2212    2\u03b3\u03c8, 1 2\u03b1(y \u2212       p)2 + 2\u03b3\u03c8\nfor any \u03c8 as defined above (say \u03c8 \u2208                       \u03a8). In particular, we have\n                                               Df(y, p) \u2264          1                     \u03c8\u2208\u03a8 \u03b3\u03c8 and\n                                                                  2\u03b1(y \u2212      p)2 + 2 inf\n                                                    Df(y, p) \u2265         1                     \u03c8\u2208\u03a8 \u03b3\u03c8\n                                                                      2\u03b2 (y \u2212     p)2 \u2212    2 inf\nSince, we have only a finite number of points where the derivative is not well defined, a simple\nsmoothening technique may give us \u03a8 such that inf\u03c8\u2208\u03a8 \u03b3\u03c8 = 0.\nC         Proofs from Section 3\nC.1        Proof of Theorem 3.2\nWe first define a boundedness property which we use in order to apply the results from [GHK+23].\nThe property states that the activation function (the partial inverse of the link function) must\neither have a range that covers all possible labels, or has a range whose closure covers all possible\nlabels and the rate with which the labels are covered as we tend to the limits of the domain is at\nleast polynomial. For example, the sigmoid activation tends to 1 (resp. 0) exponentially fast as its\nargument increases (resp. decreases).\nDefinition C.1 (Bounded Functions). Let u : (0, 1) \u2192                                     R be a non-decreasing function defined on\nthe interval (0, 1). For R, \u03b3 \u2265                 0, we say that u is (R, \u03b3)-bounded on [0, 1] if for any \u01eb > 0, there are\nr0 \u2264    r1 \u2208    [0, 1] such that if we let u(ri) = limr\u2192ri u(r), i = 0, 1 then\n                                                 max{\u2212u(r0), u(r1)} \u2264                R   1 \u01eb  \u03b3\n                                              (1 \u2212   r1)(u(r) \u2212       u(r1)) \u2264       \u01eb for r \u2265     r1 and\n                                                       r0(u(r0) \u2212       u(r)) \u2264      \u01eb for r \u2264     r0\n      We restate a slightly more quantitative version of Theorem 3.2 here for convenience.\nTheorem C.2 (Omnipredictors for Matching Losses, combination of results in [GHK+23]). Let D\nbe a distribution over Rd \u00d7[0, 1] whose marginal on Rd has \u03bb-bounded second moments. There is an\nalgorithm that, given sample access to D, with high probability returns a predictor p : R \u2192                                                 (0, 1) with\nthe following guarantee. For any pair of Fenchel-Legendre dual functions (f, g) such that g\u2032 : R \u2192                                                       R\nis continuous, non-decreasing and L-Lipschitz, and f \u2032 is (R, \u03b3)-bounded (see Defi                                                  nition C.1), p\nsatisfi  es\n                                                Lg(f \u2032 \u25e6    p ; D) \u2264        min\nThe algorithm requires time and sample complexity poly(\u03bb, B, L, R, 1      \u2225w\u22252\u2264B Lg(w ; D) + \u01eb.             \u01eb, 1\n                                                                                                               \u01eb\u03b3 ).\n                                                                            16", "md": "# Math Equations and Text\n\nLet $$\\psi : \\text{ran}(g') \\rightarrow \\mathbb{R}$$ be such that $$\\psi'(p) = f'(p)$$ and $$\\psi'$$ is differentiable on the open interval between y and p, with $$\\psi''(\\xi) \\in [\\frac{1}{\\beta}, \\frac{1}{\\alpha}]$$ for any $$\\xi$$ between y and p. Let $$\\gamma_y := f(y) - \\psi(y)$$, $$\\gamma_p := f(p) - \\psi(p)$$, and $$\\gamma_{\\psi} := 2 \\max\\{|\\gamma_y|, |\\gamma_p|\\}$$. Then we have that\n\n$$\nDf(y, p) = \\psi(y) - \\psi(p) - (y - p)\\psi'(p) + (\\gamma_y - \\gamma_p) = \\frac{1}{2}\\psi''(\\xi)(y - p)^2 + (\\gamma_y - \\gamma_p)\n$$\n$$Df(y, p) \\in [\\frac{1}{2\\beta}(y - p)^2 - 2\\gamma_{\\psi}, \\frac{1}{2\\alpha}(y - p)^2 + 2\\gamma_{\\psi}]$$ for any $$\\psi$$ as defined above (say $$\\psi \\in \\Psi$$). In particular, we have\n\n$$\nDf(y, p) \\leq \\frac{1}{2\\alpha}(y - p)^2 + 2 \\inf_{\\psi \\in \\Psi} \\gamma_{\\psi}\n$$\n$$\nDf(y, p) \\geq \\frac{1}{2\\beta}(y - p)^2 - 2 \\inf_{\\psi \\in \\Psi} \\gamma_{\\psi}\n$$\nSince we have only a finite number of points where the derivative is not well defined, a simple smoothening technique may give us $$\\Psi$$ such that $$\\inf_{\\psi \\in \\Psi} \\gamma_{\\psi} = 0$$.\n\n### Proofs from Section 3\n\n#### Proof of Theorem 3.2\n\nWe first define a boundedness property which we use in order to apply the results from [GHK+23]. The property states that the activation function (the partial inverse of the link function) must either have a range that covers all possible labels, or has a range whose closure covers all possible labels and the rate with which the labels are covered as we tend to the limits of the domain is at least polynomial. For example, the sigmoid activation tends to 1 (resp. 0) exponentially fast as its argument increases (resp. decreases).\n\nDefinition C.1 (Bounded Functions). Let $$u : (0, 1) \\rightarrow \\mathbb{R}$$ be a non-decreasing function defined on the interval (0, 1). For $$R, \\gamma \\geq 0$$, we say that u is $$(R, \\gamma)$$-bounded on [0, 1] if for any $$\\epsilon > 0$$, there are $$r_0 \\leq r_1 \\in [0, 1]$$ such that if we let $$u(r_i) = \\lim_{r \\rightarrow r_i} u(r)$$, $$i = 0, 1$$ then\n\n$$\n\\max\\{-u(r_0), u(r_1)\\} \\leq R \\cdot \\frac{1}{\\epsilon} \\cdot \\gamma\n$$\n$$\n(1 - r_1)(u(r) - u(r_1)) \\leq \\epsilon \\text{ for } r \\geq r_1 \\text{ and}\n$$\n$$\nr_0(u(r_0) - u(r)) \\leq \\epsilon \\text{ for } r \\leq r_0\n$$\nWe restate a slightly more quantitative version of Theorem 3.2 here for convenience.\n\nTheorem C.2 (Omnipredictors for Matching Losses, combination of results in [GHK+23]). Let D be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has $$\\lambda$$-bounded second moments. There is an algorithm that, given sample access to D, with high probability returns a predictor $$p : \\mathbb{R} \\rightarrow (0, 1)$$ with the following guarantee. For any pair of Fenchel-Legendre dual functions $$(f, g)$$ such that $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is continuous, non-decreasing and L-Lipschitz, and $$f'$$ is $$(R, \\gamma)$$-bounded (see Definition C.1), p satisfies\n\n$$\nLg(f' \\circ p ; D) \\leq \\min\n$$\nThe algorithm requires time and sample complexity $$\\text{poly}(\\lambda, B, L, R, \\frac{1}{\\|w\\|^2} \\leq B Lg(w ; D) + \\epsilon, \\epsilon, \\epsilon \\gamma)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Let $$\\psi : \\text{ran}(g') \\rightarrow \\mathbb{R}$$ be such that $$\\psi'(p) = f'(p)$$ and $$\\psi'$$ is differentiable on the open interval between y and p, with $$\\psi''(\\xi) \\in [\\frac{1}{\\beta}, \\frac{1}{\\alpha}]$$ for any $$\\xi$$ between y and p. Let $$\\gamma_y := f(y) - \\psi(y)$$, $$\\gamma_p := f(p) - \\psi(p)$$, and $$\\gamma_{\\psi} := 2 \\max\\{|\\gamma_y|, |\\gamma_p|\\}$$. Then we have that\n\n$$\nDf(y, p) = \\psi(y) - \\psi(p) - (y - p)\\psi'(p) + (\\gamma_y - \\gamma_p) = \\frac{1}{2}\\psi''(\\xi)(y - p)^2 + (\\gamma_y - \\gamma_p)\n$$\n$$Df(y, p) \\in [\\frac{1}{2\\beta}(y - p)^2 - 2\\gamma_{\\psi}, \\frac{1}{2\\alpha}(y - p)^2 + 2\\gamma_{\\psi}]$$ for any $$\\psi$$ as defined above (say $$\\psi \\in \\Psi$$). In particular, we have\n\n$$\nDf(y, p) \\leq \\frac{1}{2\\alpha}(y - p)^2 + 2 \\inf_{\\psi \\in \\Psi} \\gamma_{\\psi}\n$$\n$$\nDf(y, p) \\geq \\frac{1}{2\\beta}(y - p)^2 - 2 \\inf_{\\psi \\in \\Psi} \\gamma_{\\psi}\n$$\nSince we have only a finite number of points where the derivative is not well defined, a simple smoothening technique may give us $$\\Psi$$ such that $$\\inf_{\\psi \\in \\Psi} \\gamma_{\\psi} = 0$$.", "md": "Let $$\\psi : \\text{ran}(g') \\rightarrow \\mathbb{R}$$ be such that $$\\psi'(p) = f'(p)$$ and $$\\psi'$$ is differentiable on the open interval between y and p, with $$\\psi''(\\xi) \\in [\\frac{1}{\\beta}, \\frac{1}{\\alpha}]$$ for any $$\\xi$$ between y and p. Let $$\\gamma_y := f(y) - \\psi(y)$$, $$\\gamma_p := f(p) - \\psi(p)$$, and $$\\gamma_{\\psi} := 2 \\max\\{|\\gamma_y|, |\\gamma_p|\\}$$. Then we have that\n\n$$\nDf(y, p) = \\psi(y) - \\psi(p) - (y - p)\\psi'(p) + (\\gamma_y - \\gamma_p) = \\frac{1}{2}\\psi''(\\xi)(y - p)^2 + (\\gamma_y - \\gamma_p)\n$$\n$$Df(y, p) \\in [\\frac{1}{2\\beta}(y - p)^2 - 2\\gamma_{\\psi}, \\frac{1}{2\\alpha}(y - p)^2 + 2\\gamma_{\\psi}]$$ for any $$\\psi$$ as defined above (say $$\\psi \\in \\Psi$$). In particular, we have\n\n$$\nDf(y, p) \\leq \\frac{1}{2\\alpha}(y - p)^2 + 2 \\inf_{\\psi \\in \\Psi} \\gamma_{\\psi}\n$$\n$$\nDf(y, p) \\geq \\frac{1}{2\\beta}(y - p)^2 - 2 \\inf_{\\psi \\in \\Psi} \\gamma_{\\psi}\n$$\nSince we have only a finite number of points where the derivative is not well defined, a simple smoothening technique may give us $$\\Psi$$ such that $$\\inf_{\\psi \\in \\Psi} \\gamma_{\\psi} = 0$$."}, {"type": "heading", "lvl": 3, "value": "Proofs from Section 3", "md": "### Proofs from Section 3"}, {"type": "heading", "lvl": 4, "value": "Proof of Theorem 3.2", "md": "#### Proof of Theorem 3.2"}, {"type": "text", "value": "We first define a boundedness property which we use in order to apply the results from [GHK+23]. The property states that the activation function (the partial inverse of the link function) must either have a range that covers all possible labels, or has a range whose closure covers all possible labels and the rate with which the labels are covered as we tend to the limits of the domain is at least polynomial. For example, the sigmoid activation tends to 1 (resp. 0) exponentially fast as its argument increases (resp. decreases).\n\nDefinition C.1 (Bounded Functions). Let $$u : (0, 1) \\rightarrow \\mathbb{R}$$ be a non-decreasing function defined on the interval (0, 1). For $$R, \\gamma \\geq 0$$, we say that u is $$(R, \\gamma)$$-bounded on [0, 1] if for any $$\\epsilon > 0$$, there are $$r_0 \\leq r_1 \\in [0, 1]$$ such that if we let $$u(r_i) = \\lim_{r \\rightarrow r_i} u(r)$$, $$i = 0, 1$$ then\n\n$$\n\\max\\{-u(r_0), u(r_1)\\} \\leq R \\cdot \\frac{1}{\\epsilon} \\cdot \\gamma\n$$\n$$\n(1 - r_1)(u(r) - u(r_1)) \\leq \\epsilon \\text{ for } r \\geq r_1 \\text{ and}\n$$\n$$\nr_0(u(r_0) - u(r)) \\leq \\epsilon \\text{ for } r \\leq r_0\n$$\nWe restate a slightly more quantitative version of Theorem 3.2 here for convenience.\n\nTheorem C.2 (Omnipredictors for Matching Losses, combination of results in [GHK+23]). Let D be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has $$\\lambda$$-bounded second moments. There is an algorithm that, given sample access to D, with high probability returns a predictor $$p : \\mathbb{R} \\rightarrow (0, 1)$$ with the following guarantee. For any pair of Fenchel-Legendre dual functions $$(f, g)$$ such that $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is continuous, non-decreasing and L-Lipschitz, and $$f'$$ is $$(R, \\gamma)$$-bounded (see Definition C.1), p satisfies\n\n$$\nLg(f' \\circ p ; D) \\leq \\min\n$$\nThe algorithm requires time and sample complexity $$\\text{poly}(\\lambda, B, L, R, \\frac{1}{\\|w\\|^2} \\leq B Lg(w ; D) + \\epsilon, \\epsilon, \\epsilon \\gamma)$$.", "md": "We first define a boundedness property which we use in order to apply the results from [GHK+23]. The property states that the activation function (the partial inverse of the link function) must either have a range that covers all possible labels, or has a range whose closure covers all possible labels and the rate with which the labels are covered as we tend to the limits of the domain is at least polynomial. For example, the sigmoid activation tends to 1 (resp. 0) exponentially fast as its argument increases (resp. decreases).\n\nDefinition C.1 (Bounded Functions). Let $$u : (0, 1) \\rightarrow \\mathbb{R}$$ be a non-decreasing function defined on the interval (0, 1). For $$R, \\gamma \\geq 0$$, we say that u is $$(R, \\gamma)$$-bounded on [0, 1] if for any $$\\epsilon > 0$$, there are $$r_0 \\leq r_1 \\in [0, 1]$$ such that if we let $$u(r_i) = \\lim_{r \\rightarrow r_i} u(r)$$, $$i = 0, 1$$ then\n\n$$\n\\max\\{-u(r_0), u(r_1)\\} \\leq R \\cdot \\frac{1}{\\epsilon} \\cdot \\gamma\n$$\n$$\n(1 - r_1)(u(r) - u(r_1)) \\leq \\epsilon \\text{ for } r \\geq r_1 \\text{ and}\n$$\n$$\nr_0(u(r_0) - u(r)) \\leq \\epsilon \\text{ for } r \\leq r_0\n$$\nWe restate a slightly more quantitative version of Theorem 3.2 here for convenience.\n\nTheorem C.2 (Omnipredictors for Matching Losses, combination of results in [GHK+23]). Let D be a distribution over $$\\mathbb{R}^d \\times [0, 1]$$ whose marginal on $$\\mathbb{R}^d$$ has $$\\lambda$$-bounded second moments. There is an algorithm that, given sample access to D, with high probability returns a predictor $$p : \\mathbb{R} \\rightarrow (0, 1)$$ with the following guarantee. For any pair of Fenchel-Legendre dual functions $$(f, g)$$ such that $$g' : \\mathbb{R} \\rightarrow \\mathbb{R}$$ is continuous, non-decreasing and L-Lipschitz, and $$f'$$ is $$(R, \\gamma)$$-bounded (see Definition C.1), p satisfies\n\n$$\nLg(f' \\circ p ; D) \\leq \\min\n$$\nThe algorithm requires time and sample complexity $$\\text{poly}(\\lambda, B, L, R, \\frac{1}{\\|w\\|^2} \\leq B Lg(w ; D) + \\epsilon, \\epsilon, \\epsilon \\gamma)$$."}]}, {"page": 17, "text": "Proof of Theorem 3.2. Suppose first that the marginal of D on Rd is supported on the unit ball\nB d and that the labels are binary.             Then, the result would follow from Theorems 7.7 and A.4\nof [GHK+23].        In particular, Theorem 7.7 states that given access to a weak learner with the\nspecifications of Proposition A.1, there is an efficient algorithm that computes an \u01eb1-calibrated\nand (C, \u01eb1)-multiaccurate predictor p, where the notions of calibration and multiaccuracy originate\nto the literature of fairness and are defined, e.g., in Definitions 3.1 and 3.2 of [GHK+23] and\nC = {x \u2192       w \u00b7 x | \u2225w\u22252 \u2264        B} \u222a   {x \u2192     1} (the class C is bounded as long as \u2225x\u22252 \u2264                 1 almost\nsurely). Theorem A.4 states that for \u01eb2 > 0, any \u01eb1-calibrated and (C, \u01eb1)-multiaccurate predictor\np minimizes simultaneously the matching loss corresponding to any pair (f, g) \u2208                            F (where f is\n(R, \u03b3)-bounded) up to error\n                                                 R(1/\u01eb2)\u03b3\u01eb    1 + \u01eb1 + \u01eb2\nThe expression above is formed by proving that any pair (f, g) \u2208                      F has the property that f \u2032 is\n(\u01eb2, R(1/\u01eb2)\u03b3)-approximately optimal (as per the Definition A.1 of [GHK+23]), for any \u01eb2 > 0. In\nparticular, we would like to show that for any \u01eb2 > 0 there exists                 f \u2032 such that the following is true\nfor any p \u2208    [0, 1]                       \u2113g(r, f \u2032(r)) \u2264  \u2113g(r, f \u2032(r)) + \u01eb2\n                                                  |\n                                                  f \u2032(r)| \u2264  R \u00b7 (1/\u01eb2)\u03b3\nWe may pick      f \u2032 as follows (for r0 \u2264     r1 as given by Definition C.1).\n                                                    \uf8f1\n                                                    \uf8f4f \u2032(r), if r \u2208    [r0, r1]\n                                          f \u2032(r) =  \uf8f2  f \u2032(r0), if r < r0\n                                                    \uf8f4\n                                                    \uf8f3f \u2032(r1), if r > r1\nThe desired result follows from using the expression for \u2113g, the convexity of g (since g\u2032 is non\ndecreasing) and the guarantees of Definition C.1.\n    However, we only assume that the marginal distribution is not bounded and we, therefore, need\nto make certain modifications to the proof of their Theorem 7.7. In particular, the boundedness\nassumption is used in the proofs of Lemma 7.2, Lemma 7.6 and Theorem 7.7 in [GHK+23]. For\nLemma 7.2, the guarantee for p2 changes to (C, \u03b1 + B                    \u221a \u03bb\u03b4)-multiaccuracy, by using a Cauchy-\nSchwarz inequality and bounding E[(w \u00b7 x)2] by B2 \u00b7 \u03bb.                     Note that \u03b4 here is a parameter they\nuse within their algorithm and it can be picked quadratically smaller (resulting into a polynomial\nincrease of the time and sample complexity of their algorithm).                       For Lemma 7.6, one needs to\npick a step size that is polynomially smaller than the guarantee that the weak learner provides.\nIn particular, within the proof of their Lemma 7.6 in their appendix, if the weak learner of our\nProposition A.1 is run with \u01eb \u2190           \u01eb3, then one gets (within their proof)\n                                    l2(p\u2217, pt)2 \u2212  l2(p\u2217, pt+1)2 \u2265     \u03c3 \u00b7 \u01eb3 \u2212  B2\u03bb\u03c32\n                                                                         2\nIf \u03c3 (the step size of their Algorithm 1 for multiaccuracy) is picked small enough with respect to\n\u01eb3 and parameters B2, \u03bb, then the quantity of interest l2(p\u2217, pt)2 \u2212                 l2(p\u2217, pt+1)2 has a good enough\nlower bound. Note that in their original algorithm, \u03c3 was picked equal to \u01eb3 and this is why \u01eb3\n(or another corresponding parameter) does not appear in their proofs. Once more, the updated\nchoice of \u03c3 generates a polynomial overhead in time and sample complexity. The aforementioned\nmodifications are sufficient for the modified version of Theorem 7.7 and its proof.\n    The final technical complication we need to address is that their algorithm is guaranteed to\nwork only given binary labels. We can, however, form binary labels as follows. Let (x, y) be drawn\n                                                             17", "md": "Proof of Theorem 3.2. Suppose first that the marginal of D on $$\\mathbb{R}^d$$ is supported on the unit ball $$B_d$$ and that the labels are binary. Then, the result would follow from Theorems 7.7 and A.4 of [GHK+23]. In particular, Theorem 7.7 states that given access to a weak learner with the specifications of Proposition A.1, there is an efficient algorithm that computes an $$\\epsilon_1$$-calibrated and $$(C, \\epsilon_1)$$-multiaccurate predictor $$p$$, where the notions of calibration and multiaccuracy originate to the literature of fairness and are defined, e.g., in Definitions 3.1 and 3.2 of [GHK+23] and $$C = \\{x \\to w \\cdot x | ||w||_2 \\leq B\\} \\cup \\{x \\to 1\\}$$ (the class $$C$$ is bounded as long as $$||x||_2 \\leq 1$$ almost surely). Theorem A.4 states that for $$\\epsilon_2 > 0$$, any $$\\epsilon_1$$-calibrated and $$(C, \\epsilon_1)$$-multiaccurate predictor $$p$$ minimizes simultaneously the matching loss corresponding to any pair $$(f, g) \\in F$$ (where $$f$$ is $$(R, \\gamma)$$-bounded) up to error\n\n$$\nR(1/\\epsilon_2)\\gamma\\epsilon_1 + \\epsilon_1 + \\epsilon_2\n$$\n\nThe expression above is formed by proving that any pair $$(f, g) \\in F$$ has the property that $$f'$$ is $$(\\epsilon_2, R(1/\\epsilon_2)\\gamma)$$-approximately optimal (as per the Definition A.1 of [GHK+23]), for any $$\\epsilon_2 > 0$$. In particular, we would like to show that for any $$\\epsilon_2 > 0$$ there exists $$f'$$ such that the following is true for any $$p \\in [0, 1]$$\n\n$$\n\\ell_g(r, f'(r)) \\leq \\ell_g(r, f'(r)) + \\epsilon_2\n$$\n\n$$\n|f'(r)| \\leq R \\cdot (1/\\epsilon_2)^\\gamma\n$$\n\nWe may pick $$f'$$ as follows (for $$r_0 \\leq r \\leq r_1$$ as given by Definition C.1).\n\n$$\nf'(r) = \\begin{cases} f'(r), & \\text{if } r \\in [r_0, r_1] \\\\ f'(r_0), & \\text{if } r < r_0 \\\\ f'(r_1), & \\text{if } r > r_1 \\end{cases}\n$$\n\nThe desired result follows from using the expression for $$\\ell_g$$, the convexity of $$g$$ (since $$g'$$ is non-decreasing) and the guarantees of Definition C.1.\n\nHowever, we only assume that the marginal distribution is not bounded and we, therefore, need to make certain modifications to the proof of their Theorem 7.7. In particular, the boundedness assumption is used in the proofs of Lemma 7.2, Lemma 7.6 and Theorem 7.7 in [GHK+23]. For Lemma 7.2, the guarantee for $$p_2$$ changes to $$(C, \\alpha + B\\sqrt{\\lambda}\\delta)$$-multiaccuracy, by using a Cauchy-Schwarz inequality and bounding $$E[(w \\cdot x)^2]$$ by $$B^2 \\cdot \\lambda$$. Note that $$\\delta$$ here is a parameter they use within their algorithm and it can be picked quadratically smaller (resulting in a polynomial increase of the time and sample complexity of their algorithm). For Lemma 7.6, one needs to pick a step size that is polynomially smaller than the guarantee that the weak learner provides. In particular, within the proof of their Lemma 7.6 in their appendix, if the weak learner of our Proposition A.1 is run with $$\\epsilon \\leftarrow \\epsilon_3$$, then one gets (within their proof)\n\n$$\n\\ell_2(p^*, p_t)^2 - \\ell_2(p^*, p_{t+1})^2 \\geq \\sigma \\cdot \\epsilon_3 - B^2\\lambda\\sigma^2\n$$\n\nIf $$\\sigma$$ (the step size of their Algorithm 1 for multiaccuracy) is picked small enough with respect to $$\\epsilon_3$$ and parameters $$B^2$$, $$\\lambda$$, then the quantity of interest $$\\ell_2(p^*, p_t)^2 - \\ell_2(p^*, p_{t+1})^2$$ has a good enough lower bound. Note that in their original algorithm, $$\\sigma$$ was picked equal to $$\\epsilon_3$$ and this is why $$\\epsilon_3$$ (or another corresponding parameter) does not appear in their proofs. Once more, the updated choice of $$\\sigma$$ generates a polynomial overhead in time and sample complexity. The aforementioned modifications are sufficient for the modified version of Theorem 7.7 and its proof.\n\nThe final technical complication we need to address is that their algorithm is guaranteed to work only given binary labels. We can, however, form binary labels as follows. Let $$(x, y)$$ be drawn", "images": [], "items": [{"type": "text", "value": "Proof of Theorem 3.2. Suppose first that the marginal of D on $$\\mathbb{R}^d$$ is supported on the unit ball $$B_d$$ and that the labels are binary. Then, the result would follow from Theorems 7.7 and A.4 of [GHK+23]. In particular, Theorem 7.7 states that given access to a weak learner with the specifications of Proposition A.1, there is an efficient algorithm that computes an $$\\epsilon_1$$-calibrated and $$(C, \\epsilon_1)$$-multiaccurate predictor $$p$$, where the notions of calibration and multiaccuracy originate to the literature of fairness and are defined, e.g., in Definitions 3.1 and 3.2 of [GHK+23] and $$C = \\{x \\to w \\cdot x | ||w||_2 \\leq B\\} \\cup \\{x \\to 1\\}$$ (the class $$C$$ is bounded as long as $$||x||_2 \\leq 1$$ almost surely). Theorem A.4 states that for $$\\epsilon_2 > 0$$, any $$\\epsilon_1$$-calibrated and $$(C, \\epsilon_1)$$-multiaccurate predictor $$p$$ minimizes simultaneously the matching loss corresponding to any pair $$(f, g) \\in F$$ (where $$f$$ is $$(R, \\gamma)$$-bounded) up to error\n\n$$\nR(1/\\epsilon_2)\\gamma\\epsilon_1 + \\epsilon_1 + \\epsilon_2\n$$\n\nThe expression above is formed by proving that any pair $$(f, g) \\in F$$ has the property that $$f'$$ is $$(\\epsilon_2, R(1/\\epsilon_2)\\gamma)$$-approximately optimal (as per the Definition A.1 of [GHK+23]), for any $$\\epsilon_2 > 0$$. In particular, we would like to show that for any $$\\epsilon_2 > 0$$ there exists $$f'$$ such that the following is true for any $$p \\in [0, 1]$$\n\n$$\n\\ell_g(r, f'(r)) \\leq \\ell_g(r, f'(r)) + \\epsilon_2\n$$\n\n$$", "md": "Proof of Theorem 3.2. Suppose first that the marginal of D on $$\\mathbb{R}^d$$ is supported on the unit ball $$B_d$$ and that the labels are binary. Then, the result would follow from Theorems 7.7 and A.4 of [GHK+23]. In particular, Theorem 7.7 states that given access to a weak learner with the specifications of Proposition A.1, there is an efficient algorithm that computes an $$\\epsilon_1$$-calibrated and $$(C, \\epsilon_1)$$-multiaccurate predictor $$p$$, where the notions of calibration and multiaccuracy originate to the literature of fairness and are defined, e.g., in Definitions 3.1 and 3.2 of [GHK+23] and $$C = \\{x \\to w \\cdot x | ||w||_2 \\leq B\\} \\cup \\{x \\to 1\\}$$ (the class $$C$$ is bounded as long as $$||x||_2 \\leq 1$$ almost surely). Theorem A.4 states that for $$\\epsilon_2 > 0$$, any $$\\epsilon_1$$-calibrated and $$(C, \\epsilon_1)$$-multiaccurate predictor $$p$$ minimizes simultaneously the matching loss corresponding to any pair $$(f, g) \\in F$$ (where $$f$$ is $$(R, \\gamma)$$-bounded) up to error\n\n$$\nR(1/\\epsilon_2)\\gamma\\epsilon_1 + \\epsilon_1 + \\epsilon_2\n$$\n\nThe expression above is formed by proving that any pair $$(f, g) \\in F$$ has the property that $$f'$$ is $$(\\epsilon_2, R(1/\\epsilon_2)\\gamma)$$-approximately optimal (as per the Definition A.1 of [GHK+23]), for any $$\\epsilon_2 > 0$$. In particular, we would like to show that for any $$\\epsilon_2 > 0$$ there exists $$f'$$ such that the following is true for any $$p \\in [0, 1]$$\n\n$$\n\\ell_g(r, f'(r)) \\leq \\ell_g(r, f'(r)) + \\epsilon_2\n$$\n\n$$"}, {"type": "table", "rows": [["f'(r)"]], "md": "|f'(r)| \\leq R \\cdot (1/\\epsilon_2)^\\gamma", "isPerfectTable": true, "csv": "\"f'(r)\""}, {"type": "text", "value": "$$\n\nWe may pick $$f'$$ as follows (for $$r_0 \\leq r \\leq r_1$$ as given by Definition C.1).\n\n$$\nf'(r) = \\begin{cases} f'(r), & \\text{if } r \\in [r_0, r_1] \\\\ f'(r_0), & \\text{if } r < r_0 \\\\ f'(r_1), & \\text{if } r > r_1 \\end{cases}\n$$\n\nThe desired result follows from using the expression for $$\\ell_g$$, the convexity of $$g$$ (since $$g'$$ is non-decreasing) and the guarantees of Definition C.1.\n\nHowever, we only assume that the marginal distribution is not bounded and we, therefore, need to make certain modifications to the proof of their Theorem 7.7. In particular, the boundedness assumption is used in the proofs of Lemma 7.2, Lemma 7.6 and Theorem 7.7 in [GHK+23]. For Lemma 7.2, the guarantee for $$p_2$$ changes to $$(C, \\alpha + B\\sqrt{\\lambda}\\delta)$$-multiaccuracy, by using a Cauchy-Schwarz inequality and bounding $$E[(w \\cdot x)^2]$$ by $$B^2 \\cdot \\lambda$$. Note that $$\\delta$$ here is a parameter they use within their algorithm and it can be picked quadratically smaller (resulting in a polynomial increase of the time and sample complexity of their algorithm). For Lemma 7.6, one needs to pick a step size that is polynomially smaller than the guarantee that the weak learner provides. In particular, within the proof of their Lemma 7.6 in their appendix, if the weak learner of our Proposition A.1 is run with $$\\epsilon \\leftarrow \\epsilon_3$$, then one gets (within their proof)\n\n$$\n\\ell_2(p^*, p_t)^2 - \\ell_2(p^*, p_{t+1})^2 \\geq \\sigma \\cdot \\epsilon_3 - B^2\\lambda\\sigma^2\n$$\n\nIf $$\\sigma$$ (the step size of their Algorithm 1 for multiaccuracy) is picked small enough with respect to $$\\epsilon_3$$ and parameters $$B^2$$, $$\\lambda$$, then the quantity of interest $$\\ell_2(p^*, p_t)^2 - \\ell_2(p^*, p_{t+1})^2$$ has a good enough lower bound. Note that in their original algorithm, $$\\sigma$$ was picked equal to $$\\epsilon_3$$ and this is why $$\\epsilon_3$$ (or another corresponding parameter) does not appear in their proofs. Once more, the updated choice of $$\\sigma$$ generates a polynomial overhead in time and sample complexity. The aforementioned modifications are sufficient for the modified version of Theorem 7.7 and its proof.\n\nThe final technical complication we need to address is that their algorithm is guaranteed to work only given binary labels. We can, however, form binary labels as follows. Let $$(x, y)$$ be drawn", "md": "$$\n\nWe may pick $$f'$$ as follows (for $$r_0 \\leq r \\leq r_1$$ as given by Definition C.1).\n\n$$\nf'(r) = \\begin{cases} f'(r), & \\text{if } r \\in [r_0, r_1] \\\\ f'(r_0), & \\text{if } r < r_0 \\\\ f'(r_1), & \\text{if } r > r_1 \\end{cases}\n$$\n\nThe desired result follows from using the expression for $$\\ell_g$$, the convexity of $$g$$ (since $$g'$$ is non-decreasing) and the guarantees of Definition C.1.\n\nHowever, we only assume that the marginal distribution is not bounded and we, therefore, need to make certain modifications to the proof of their Theorem 7.7. In particular, the boundedness assumption is used in the proofs of Lemma 7.2, Lemma 7.6 and Theorem 7.7 in [GHK+23]. For Lemma 7.2, the guarantee for $$p_2$$ changes to $$(C, \\alpha + B\\sqrt{\\lambda}\\delta)$$-multiaccuracy, by using a Cauchy-Schwarz inequality and bounding $$E[(w \\cdot x)^2]$$ by $$B^2 \\cdot \\lambda$$. Note that $$\\delta$$ here is a parameter they use within their algorithm and it can be picked quadratically smaller (resulting in a polynomial increase of the time and sample complexity of their algorithm). For Lemma 7.6, one needs to pick a step size that is polynomially smaller than the guarantee that the weak learner provides. In particular, within the proof of their Lemma 7.6 in their appendix, if the weak learner of our Proposition A.1 is run with $$\\epsilon \\leftarrow \\epsilon_3$$, then one gets (within their proof)\n\n$$\n\\ell_2(p^*, p_t)^2 - \\ell_2(p^*, p_{t+1})^2 \\geq \\sigma \\cdot \\epsilon_3 - B^2\\lambda\\sigma^2\n$$\n\nIf $$\\sigma$$ (the step size of their Algorithm 1 for multiaccuracy) is picked small enough with respect to $$\\epsilon_3$$ and parameters $$B^2$$, $$\\lambda$$, then the quantity of interest $$\\ell_2(p^*, p_t)^2 - \\ell_2(p^*, p_{t+1})^2$$ has a good enough lower bound. Note that in their original algorithm, $$\\sigma$$ was picked equal to $$\\epsilon_3$$ and this is why $$\\epsilon_3$$ (or another corresponding parameter) does not appear in their proofs. Once more, the updated choice of $$\\sigma$$ generates a polynomial overhead in time and sample complexity. The aforementioned modifications are sufficient for the modified version of Theorem 7.7 and its proof.\n\nThe final technical complication we need to address is that their algorithm is guaranteed to work only given binary labels. We can, however, form binary labels as follows. Let $$(x, y)$$ be drawn"}]}, {"page": 18, "text": "from D. We have that y \u2208                 [0, 1]. We draw a conditionally (on y) independent Bernoulli random\nvariable y\u2032 with probability of success y, forming the distribution D\u2032 over Rd \u00d7 {0, 1}. We run the\nalgorithm of [GHK+23] on D\u2032 and obtain a predictor p such that\n                                          Lg(f \u2032 \u25e6   p ; D\u2032) \u2264       min\n                                                                   \u2225w\u22252\u2264B Lg(w ; D\u2032) + \u01eb ,\nfor any (f, g) as described in the theorem statement. We, additionally, have that for any c : Rd \u2192                                         R\n             Lg(c ; D\u2032) = E    x,y\u2032[g(c(x)) \u2212     y\u2032c(x)]\n                           = E x   g(c(x)) \u2212     E    y\u2032  x   c(x)\n                                                 y\u2032\n                           = E     g(c(x)) \u2212     E    E   y\u2032  y, x     x   c(x)\n                               x                  y   y\u2032\n                           = E     g(c(x)) \u2212     E    E   y\u2032  y     x   c(x)                (y\u2032 is independent from x given y)\n                               x                  y   y\u2032\n                           = E x   g(c(x)) \u2212     Ey   y  x   c(x)                                                  (y\u2032 is Bernoulli(y))\n                           = E x,y[g(c(x)) \u2212     yc(x)] = Lg(c ; D)\nThis concludes the proof of Theorem 3.2.\nC.2       Proof of Lemma 3.3\nWe first apply Theorem 2.1 with g\u2032 \u2190                    \u03c6\u2032 to obtain that for \u03c6\u2032         w(x) = \u03c6\u2032(w \u00b7 x), we have\n                          err2(p) \u2264     \u03b2           w\u2217) + 2\u03b2\u01eb                               (since inequality holds for w \u2208              W)\nMoreover, we have                       \u03b1 err2(\u03c6\u2032\n                     err2(\u03c6\u2032 w\u2217) = E         y \u2212   \u03c6\u2032(w\u2217    \u00b7 x)  2\n                                   = E       y \u2212   g\u2032(w\u2217    \u00b7 x) + g\u2032(w\u2217     \u00b7 x) \u2212   \u03c6\u2032(w\u2217    \u00b7 x)  2\n                                   \u2264   2 E     y \u2212   g\u2032(w\u2217    \u00b7 x)  2    + 2 E      g\u2032(w\u2217    \u00b7 x) \u2212   \u03c6\u2032(w\u2217    \u00b7 x)  2\n                                   = 2 opt    g + 2 E       g\u2032(w\u2217   \u00b7 x) \u2212   \u03c6\u2032(w\u2217    \u00b7 x)  2\nThis concludes the proof of Lemma 3.3.\nD        Proofs from Section 4\nD.1        Proof of Theorem 4.1\nIn the case we consider here, g\u2032 is the sigmoid activation, i.e., g\u2032(t) = (1 + e\u2212t)\u22121 for any t \u2208                                     R. In\nparticular, the pointwise surrogate loss we consider satisfies\n                                       \u2113g(y, f \u2032(p)) = y ln 1    p + (1 \u2212     y) ln  1 \u22121  p \u2212   ln 2 ,\n                                                                     18", "md": "# Math Equations\n\nfrom D. We have that y \u2208 [0, 1]. We draw a conditionally (on y) independent Bernoulli random variable y\u2032 with probability of success y, forming the distribution D\u2032 over Rd \u00d7 {0, 1}. We run the algorithm of [GHK+23] on D\u2032 and obtain a predictor p such that\n\n$$\nL_g(f' \\circ p ; D') \\leq \\min_{\\|w\\|^2 \\leq B} L_g(w ; D') + \\epsilon ,\n$$\n\nfor any (f, g) as described in the theorem statement. We, additionally, have that for any c : Rd \u2192 R\n\n$$\n\\begin{align*}\nL_g(c ; D') & = E_{x,y'}[g(c(x)) - y'c(x)] \\\\\n& = E_x g(c(x)) - E_{y'} E_{x,y} c(x) \\\\\n& = E_x g(c(x)) - E_{y'} E_{y,x} c(x) \\quad (y' \\text{ is independent from x given y}) \\\\\n& = E_x g(c(x)) - E_y yx c(x) \\quad (y' \\text{ is Bernoulli}(y)) \\\\\n& = E_{x,y}[g(c(x)) - yc(x)] = L_g(c ; D)\n\\end{align*}\n$$\n\nThis concludes the proof of Theorem 3.2.\n\n## Proof of Lemma 3.3\n\nWe first apply Theorem 2.1 with g' \u2190 \u03c6' to obtain that for \u03c6' w(x) = \u03c6'(w \\cdot x), we have\n\n$$\n\\text{err}_2(p) \\leq \\beta \\|\\text{w}^* \\| + 2\\beta\\epsilon \\quad (\\text{since inequality holds for } w \\in W)\n$$\n\nMoreover, we have\n\n$$\n\\begin{align*}\n\\text{err}_2(\\phi' w^*) & = E_y (y - \\phi'(w^* \\cdot x))^2 \\\\\n& = E_y (y - g'(w^* \\cdot x) + g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2 \\\\\n& \\leq 2 E_y (y - g'(w^* \\cdot x))^2 + 2 E_y (g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2 \\\\\n& = 2 \\text{opt}_g + 2 E_y (g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2\n\\end{align*}\n$$\n\nThis concludes the proof of Lemma 3.3.\n\n## Proofs from Section 4\n\n### Proof of Theorem 4.1\n\nIn the case we consider here, g' is the sigmoid activation, i.e., g'(t) = (1 + e^{-t})^{-1} for any t \u2208 R. In particular, the pointwise surrogate loss we consider satisfies\n\n$$\n\\ell_g(y, f'(p)) = y \\ln(1 + p) + (1 - y) \\ln\\left(1 - \\frac{1}{1 + p}\\right) - \\ln 2 ,\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "from D. We have that y \u2208 [0, 1]. We draw a conditionally (on y) independent Bernoulli random variable y\u2032 with probability of success y, forming the distribution D\u2032 over Rd \u00d7 {0, 1}. We run the algorithm of [GHK+23] on D\u2032 and obtain a predictor p such that\n\n$$\nL_g(f' \\circ p ; D') \\leq \\min_{\\|w\\|^2 \\leq B} L_g(w ; D') + \\epsilon ,\n$$\n\nfor any (f, g) as described in the theorem statement. We, additionally, have that for any c : Rd \u2192 R\n\n$$\n\\begin{align*}\nL_g(c ; D') & = E_{x,y'}[g(c(x)) - y'c(x)] \\\\\n& = E_x g(c(x)) - E_{y'} E_{x,y} c(x) \\\\\n& = E_x g(c(x)) - E_{y'} E_{y,x} c(x) \\quad (y' \\text{ is independent from x given y}) \\\\\n& = E_x g(c(x)) - E_y yx c(x) \\quad (y' \\text{ is Bernoulli}(y)) \\\\\n& = E_{x,y}[g(c(x)) - yc(x)] = L_g(c ; D)\n\\end{align*}\n$$\n\nThis concludes the proof of Theorem 3.2.", "md": "from D. We have that y \u2208 [0, 1]. We draw a conditionally (on y) independent Bernoulli random variable y\u2032 with probability of success y, forming the distribution D\u2032 over Rd \u00d7 {0, 1}. We run the algorithm of [GHK+23] on D\u2032 and obtain a predictor p such that\n\n$$\nL_g(f' \\circ p ; D') \\leq \\min_{\\|w\\|^2 \\leq B} L_g(w ; D') + \\epsilon ,\n$$\n\nfor any (f, g) as described in the theorem statement. We, additionally, have that for any c : Rd \u2192 R\n\n$$\n\\begin{align*}\nL_g(c ; D') & = E_{x,y'}[g(c(x)) - y'c(x)] \\\\\n& = E_x g(c(x)) - E_{y'} E_{x,y} c(x) \\\\\n& = E_x g(c(x)) - E_{y'} E_{y,x} c(x) \\quad (y' \\text{ is independent from x given y}) \\\\\n& = E_x g(c(x)) - E_y yx c(x) \\quad (y' \\text{ is Bernoulli}(y)) \\\\\n& = E_{x,y}[g(c(x)) - yc(x)] = L_g(c ; D)\n\\end{align*}\n$$\n\nThis concludes the proof of Theorem 3.2."}, {"type": "heading", "lvl": 2, "value": "Proof of Lemma 3.3", "md": "## Proof of Lemma 3.3"}, {"type": "text", "value": "We first apply Theorem 2.1 with g' \u2190 \u03c6' to obtain that for \u03c6' w(x) = \u03c6'(w \\cdot x), we have\n\n$$\n\\text{err}_2(p) \\leq \\beta \\|\\text{w}^* \\| + 2\\beta\\epsilon \\quad (\\text{since inequality holds for } w \\in W)\n$$\n\nMoreover, we have\n\n$$\n\\begin{align*}\n\\text{err}_2(\\phi' w^*) & = E_y (y - \\phi'(w^* \\cdot x))^2 \\\\\n& = E_y (y - g'(w^* \\cdot x) + g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2 \\\\\n& \\leq 2 E_y (y - g'(w^* \\cdot x))^2 + 2 E_y (g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2 \\\\\n& = 2 \\text{opt}_g + 2 E_y (g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2\n\\end{align*}\n$$\n\nThis concludes the proof of Lemma 3.3.", "md": "We first apply Theorem 2.1 with g' \u2190 \u03c6' to obtain that for \u03c6' w(x) = \u03c6'(w \\cdot x), we have\n\n$$\n\\text{err}_2(p) \\leq \\beta \\|\\text{w}^* \\| + 2\\beta\\epsilon \\quad (\\text{since inequality holds for } w \\in W)\n$$\n\nMoreover, we have\n\n$$\n\\begin{align*}\n\\text{err}_2(\\phi' w^*) & = E_y (y - \\phi'(w^* \\cdot x))^2 \\\\\n& = E_y (y - g'(w^* \\cdot x) + g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2 \\\\\n& \\leq 2 E_y (y - g'(w^* \\cdot x))^2 + 2 E_y (g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2 \\\\\n& = 2 \\text{opt}_g + 2 E_y (g'(w^* \\cdot x) - \\phi'(w^* \\cdot x))^2\n\\end{align*}\n$$\n\nThis concludes the proof of Lemma 3.3."}, {"type": "heading", "lvl": 2, "value": "Proofs from Section 4", "md": "## Proofs from Section 4"}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 4.1", "md": "### Proof of Theorem 4.1"}, {"type": "text", "value": "In the case we consider here, g' is the sigmoid activation, i.e., g'(t) = (1 + e^{-t})^{-1} for any t \u2208 R. In particular, the pointwise surrogate loss we consider satisfies\n\n$$\n\\ell_g(y, f'(p)) = y \\ln(1 + p) + (1 - y) \\ln\\left(1 - \\frac{1}{1 + p}\\right) - \\ln 2 ,\n$$", "md": "In the case we consider here, g' is the sigmoid activation, i.e., g'(t) = (1 + e^{-t})^{-1} for any t \u2208 R. In particular, the pointwise surrogate loss we consider satisfies\n\n$$\n\\ell_g(y, f'(p)) = y \\ln(1 + p) + (1 - y) \\ln\\left(1 - \\frac{1}{1 + p}\\right) - \\ln 2 ,\n$$"}]}, {"page": 19, "text": "for any y \u2208     [0, 1] and p \u2208     (0, 1).  We may extend Lemma 4.2 to also capture y \u2208                    {0, 1}, by\ndefining \u2113g(0, f \u2032(0)) = \u2113g(1, f \u2032(1)) = \u2212ln 2 (the inequality would hold under this definition). Hence,\nfollowing a similar procedure as the one used for proving Theorem B.1, we obtain the following by\napplying Lemma 4.2\n                                 err2(p) \u2264   2  Lg(f \u2032 \u25e6 p) \u2212  E[\u2113g(y, f \u2032(y))]                                 (D.1)\n                                                                 2\n             Lg(w\u2217) \u2212    E[\u2113g(y, f \u2032(y))] \u2264  E   g\u2032(w\u2217   \u00b7 x) \u2228 (1 \u2212  g\u2032(w\u2217  \u00b7 x)) \u00b7 (y \u2212 g\u2032(w\u2217   \u00b7 x))2        (D.2)\n                              Lg(f \u2032 \u25e6 p) \u2264  Lg(w\u2217) + \u01eb                                                         (D.3)\n    Therefore, in order to prove Theorem 4.1, it is sufficient to provide a strong enough upper bound\nfor the quantity of the right hand side of Equation (D.2) in terms of opt              g. We observe that\n                                      2\n                      g\u2032(w\u2217  \u00b7 x) \u2228  (1 \u2212  g\u2032(w\u2217  \u00b7 x)) \u2264  4 exp(|w\u2217   \u00b7 x|) , for any x \u2208   Rd\n    It remains to bound the quantity E[e|w\u2217\u00b7x|(y \u2212            g\u2032\nrandom variable). Then for any r \u2265          0 we have          w(x))2]. Set Q = e|w\u2217\u00b7x|(y \u2212        g\u2032w(x))2 (Q is a\n                  E[Q] = E[Q \u00b7    1{|w\u2217   \u00b7 x| \u2264 r}] + E[Q \u00b7   1{|w\u2217   \u00b7 x| > r}]\n                        \u2264  er E[(y \u2212   g\u2032        1{|w\u2217   \u00b7 x| \u2264 r}] + E[e|w\u2217\u00b7x|1{|w\u2217     \u00b7 x| > r}]\n                                        w\u2217(x))2\n                        \u2264  er \u00b7 opt + E[e|w\u2217\u00b7x|1{|w\u2217     \u00b7 x| > r}]\nTo bound the quantity E[e|w\u2217\u00b7x|1{|w\u2217           \u00b7 x| > r}], consider F(s) := Pr[e|w\u2217\u00b7x|1{|w\u2217          \u00b7 x| > r} \u2265    s].\nWe have that\n                      F(s) =   1{s = 0} Pr[|w\u2217     \u00b7 x| \u2264 r] + Pr[|w\u2217    \u00b7 x| \u2265 max{ln s, r}] .\n                                          \u221e                 \u221e\nSince E[e|w\u2217\u00b7x|1{|w\u2217     \u00b7 x| > r}] =    s=0 F(s) ds =     s=0 Pr[|w\u2217   \u00b7 x| \u2265  max{ln s, r}] ds, we obtain\n         E[Q] =      \u221e  Pr [|w\u2217  \u00b7 x| \u2265  max{ln s, r}] ds\n                    s=0\n                \u2264    er Pr   |w\u2217  \u00b7 x| \u2265  r    ds +    \u221e        |w\u2217  \u00b7 x| \u2265  ln s   ds          (since \u2225w\u2217\u22252 \u2264     B)\n                    s=0                   B           s=er Pr                 B\n                \u2264 er \u00b7 e\u2212(r/B)2 +      \u221e                                                              (see Def. 1.5)\n                                     s=er e\u2212(ln s/B)2 ds\n                                           \u221e         B )2 du\n                = er \u00b7 e\u2212(r/B)2 + er \u00b7    u=0 eu\u2212( u+r    B2      \u221e                           (define u = ln s \u2212    r)\n                \u2264 er \u00b7 e\u2212(r/B)2 + er \u00b7 e\u2212(r/B)2 \u00b7 B \u00b7 e    2 \u00b7  u=0  e\u2212(u\u2212B 2 )2 du\n                \u2264 CeB2ere\u2212(r/B)2\nTherefore, in total, we have that err2(p) \u2264            8eroptg + 8CeB2ere\u2212(r/B)2 + 2\u01eb and we may obtain\n                                          1\nTheorem 4.1 by picking r = B(ln          opt)1/2.\nD.2     Proof of Theorem 4.3\nWe first prove Lemma 4.4. We have that for any y \u2208               {0, 1} and p \u2208   (0, 1)\n                     \u2113g(y, f \u2032(p)) = y ln 1p + (1 \u2212   y) ln 1 \u22121 p \u2212  ln 2 = CE(y, p) \u2212    ln 2 ,\n                                                          19", "md": "# Math Equations\n\nFor any \\( y \\in [0, 1] \\) and \\( p \\in (0, 1) \\). We may extend Lemma 4.2 to also capture \\( y \\in \\{0, 1\\} \\), by defining \\( \\ell_g(0, f'(0)) = \\ell_g(1, f'(1)) = -\\ln 2 \\) (the inequality would hold under this definition). Hence, following a similar procedure as the one used for proving Theorem B.1, we obtain the following by applying Lemma 4.2\n\n$$\n\\text{err}_2(p) \\leq \\frac{2}{L_g(f' \\circ p) - E[\\ell_g(y, f'(y))]} \\quad \\text{(D.1)}\n$$\n\n$$\nL_g(w^*) - E[\\ell_g(y, f'(y))] \\leq E[g'(w^* \\cdot x) \\vee (1 - g'(w^* \\cdot x)) \\cdot (y - g'(w^* \\cdot x))^2] \\quad \\text{(D.2)}\n$$\n\n$$\nL_g(f' \\circ p) \\leq L_g(w^*) + \\epsilon \\quad \\text{(D.3)}\n$$\n\nTherefore, in order to prove Theorem 4.1, it is sufficient to provide a strong enough upper bound for the quantity of the right hand side of Equation (D.2) in terms of \\( \\text{opt}_g \\). We observe that\n\n$$\ng'(w^* \\cdot x) \\vee (1 - g'(w^* \\cdot x)) \\leq 4 \\exp(|w^* \\cdot x|), \\text{ for any } x \\in \\mathbb{R}^d\n$$\n\nIt remains to bound the quantity \\( E[e|w^* \\cdot x|(y - g'(w(x)))^2] \\). Set \\( Q = e|w^* \\cdot x|(y - g'(w(x)))^2 \\) (\\( Q \\) is a random variable). Then for any \\( r \\geq 0 \\) we have\n\n$$\n\\begin{align*}\nE[Q] & = E[Q \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[Q \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq er E[(y - g'(w^* \\cdot x))^2 \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}]\n\\end{align*}\n$$\n\nTo bound the quantity \\( E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] \\), consider \\( F(s) := \\text{Pr}[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\} \\geq s] \\). We have that\n\n$$\nF(s) = 1\\{s = 0\\} \\text{Pr}[|w^* \\cdot x| \\leq r] + \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] .\n$$\n\nSince \\( E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] = \\int_{s=0} F(s) ds = \\int_{s=0} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] ds \\), we obtain\n\n$$\nE[Q] = \\int_{s=0}^{\\infty} \\text{Pr} [|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] ds \\\\\n\\leq er \\text{Pr} [|w^* \\cdot x| \\geq r] ds + \\int_{s=er}^{\\infty} |w^* \\cdot x| \\geq \\ln s ds \\quad \\text{(since } \\|w^*\\|_2 \\leq B) \\\\\n\\leq er \\cdot e^{-(r/B)^2} + \\int_{s=er}^{\\infty} e^{-(\\ln s/B)^2} ds \\quad \\text{(see Def. 1.5)} \\\\\n= er \\cdot e^{-(r/B)^2} + er \\cdot \\int_{u=0} e^u e^{-(u+r/B)^2} du \\quad \\text{(define } u = \\ln s - r) \\\\\n\\leq CeB^2 e^{r/B} e^{-(r/B)^2}\n$$\n\nTherefore, in total, we have that \\( \\text{err}_2(p) \\leq 8er\\text{opt}_g + 8CeB^2 e^{r/B} e^{-(r/B)^2} + 2\\epsilon \\) and we may obtain\n\n$$\n\\text{Theorem 4.1 by picking } r = B(\\ln \\text{opt}_g)^{1/2}.\n$$\n\nD.2 Proof of Theorem 4.3\n\nWe first prove Lemma 4.4. We have that for any \\( y \\in \\{0, 1\\} \\) and \\( p \\in (0, 1) \\)\n\n$$\n\\ell_g(y, f'(p)) = y \\ln \\frac{1}{p} + (1 - y) \\ln \\frac{1}{1 - p} - \\ln 2 = CE(y, p) - \\ln 2 ,\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "For any \\( y \\in [0, 1] \\) and \\( p \\in (0, 1) \\). We may extend Lemma 4.2 to also capture \\( y \\in \\{0, 1\\} \\), by defining \\( \\ell_g(0, f'(0)) = \\ell_g(1, f'(1)) = -\\ln 2 \\) (the inequality would hold under this definition). Hence, following a similar procedure as the one used for proving Theorem B.1, we obtain the following by applying Lemma 4.2\n\n$$\n\\text{err}_2(p) \\leq \\frac{2}{L_g(f' \\circ p) - E[\\ell_g(y, f'(y))]} \\quad \\text{(D.1)}\n$$\n\n$$\nL_g(w^*) - E[\\ell_g(y, f'(y))] \\leq E[g'(w^* \\cdot x) \\vee (1 - g'(w^* \\cdot x)) \\cdot (y - g'(w^* \\cdot x))^2] \\quad \\text{(D.2)}\n$$\n\n$$\nL_g(f' \\circ p) \\leq L_g(w^*) + \\epsilon \\quad \\text{(D.3)}\n$$\n\nTherefore, in order to prove Theorem 4.1, it is sufficient to provide a strong enough upper bound for the quantity of the right hand side of Equation (D.2) in terms of \\( \\text{opt}_g \\). We observe that\n\n$$\ng'(w^* \\cdot x) \\vee (1 - g'(w^* \\cdot x)) \\leq 4 \\exp(|w^* \\cdot x|), \\text{ for any } x \\in \\mathbb{R}^d\n$$\n\nIt remains to bound the quantity \\( E[e|w^* \\cdot x|(y - g'(w(x)))^2] \\). Set \\( Q = e|w^* \\cdot x|(y - g'(w(x)))^2 \\) (\\( Q \\) is a random variable). Then for any \\( r \\geq 0 \\) we have\n\n$$\n\\begin{align*}\nE[Q] & = E[Q \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[Q \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq er E[(y - g'(w^* \\cdot x))^2 \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}]\n\\end{align*}\n$$\n\nTo bound the quantity \\( E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] \\), consider \\( F(s) := \\text{Pr}[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\} \\geq s] \\). We have that\n\n$$\nF(s) = 1\\{s = 0\\} \\text{Pr}[|w^* \\cdot x| \\leq r] + \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] .\n$$\n\nSince \\( E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] = \\int_{s=0} F(s) ds = \\int_{s=0} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] ds \\), we obtain\n\n$$\nE[Q] = \\int_{s=0}^{\\infty} \\text{Pr} [|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] ds \\\\\n\\leq er \\text{Pr} [|w^* \\cdot x| \\geq r] ds + \\int_{s=er}^{\\infty} |w^* \\cdot x| \\geq \\ln s ds \\quad \\text{(since } \\|w^*\\|_2 \\leq B) \\\\\n\\leq er \\cdot e^{-(r/B)^2} + \\int_{s=er}^{\\infty} e^{-(\\ln s/B)^2} ds \\quad \\text{(see Def. 1.5)} \\\\\n= er \\cdot e^{-(r/B)^2} + er \\cdot \\int_{u=0} e^u e^{-(u+r/B)^2} du \\quad \\text{(define } u = \\ln s - r) \\\\\n\\leq CeB^2 e^{r/B} e^{-(r/B)^2}\n$$\n\nTherefore, in total, we have that \\( \\text{err}_2(p) \\leq 8er\\text{opt}_g + 8CeB^2 e^{r/B} e^{-(r/B)^2} + 2\\epsilon \\) and we may obtain\n\n$$\n\\text{Theorem 4.1 by picking } r = B(\\ln \\text{opt}_g)^{1/2}.\n$$\n\nD.2 Proof of Theorem 4.3\n\nWe first prove Lemma 4.4. We have that for any \\( y \\in \\{0, 1\\} \\) and \\( p \\in (0, 1) \\)\n\n$$\n\\ell_g(y, f'(p)) = y \\ln \\frac{1}{p} + (1 - y) \\ln \\frac{1}{1 - p} - \\ln 2 = CE(y, p) - \\ln 2 ,\n$$", "md": "For any \\( y \\in [0, 1] \\) and \\( p \\in (0, 1) \\). We may extend Lemma 4.2 to also capture \\( y \\in \\{0, 1\\} \\), by defining \\( \\ell_g(0, f'(0)) = \\ell_g(1, f'(1)) = -\\ln 2 \\) (the inequality would hold under this definition). Hence, following a similar procedure as the one used for proving Theorem B.1, we obtain the following by applying Lemma 4.2\n\n$$\n\\text{err}_2(p) \\leq \\frac{2}{L_g(f' \\circ p) - E[\\ell_g(y, f'(y))]} \\quad \\text{(D.1)}\n$$\n\n$$\nL_g(w^*) - E[\\ell_g(y, f'(y))] \\leq E[g'(w^* \\cdot x) \\vee (1 - g'(w^* \\cdot x)) \\cdot (y - g'(w^* \\cdot x))^2] \\quad \\text{(D.2)}\n$$\n\n$$\nL_g(f' \\circ p) \\leq L_g(w^*) + \\epsilon \\quad \\text{(D.3)}\n$$\n\nTherefore, in order to prove Theorem 4.1, it is sufficient to provide a strong enough upper bound for the quantity of the right hand side of Equation (D.2) in terms of \\( \\text{opt}_g \\). We observe that\n\n$$\ng'(w^* \\cdot x) \\vee (1 - g'(w^* \\cdot x)) \\leq 4 \\exp(|w^* \\cdot x|), \\text{ for any } x \\in \\mathbb{R}^d\n$$\n\nIt remains to bound the quantity \\( E[e|w^* \\cdot x|(y - g'(w(x)))^2] \\). Set \\( Q = e|w^* \\cdot x|(y - g'(w(x)))^2 \\) (\\( Q \\) is a random variable). Then for any \\( r \\geq 0 \\) we have\n\n$$\n\\begin{align*}\nE[Q] & = E[Q \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[Q \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq er E[(y - g'(w^* \\cdot x))^2 \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}]\n\\end{align*}\n$$\n\nTo bound the quantity \\( E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] \\), consider \\( F(s) := \\text{Pr}[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\} \\geq s] \\). We have that\n\n$$\nF(s) = 1\\{s = 0\\} \\text{Pr}[|w^* \\cdot x| \\leq r] + \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] .\n$$\n\nSince \\( E[e|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] = \\int_{s=0} F(s) ds = \\int_{s=0} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] ds \\), we obtain\n\n$$\nE[Q] = \\int_{s=0}^{\\infty} \\text{Pr} [|w^* \\cdot x| \\geq \\max\\{\\ln s, r\\}] ds \\\\\n\\leq er \\text{Pr} [|w^* \\cdot x| \\geq r] ds + \\int_{s=er}^{\\infty} |w^* \\cdot x| \\geq \\ln s ds \\quad \\text{(since } \\|w^*\\|_2 \\leq B) \\\\\n\\leq er \\cdot e^{-(r/B)^2} + \\int_{s=er}^{\\infty} e^{-(\\ln s/B)^2} ds \\quad \\text{(see Def. 1.5)} \\\\\n= er \\cdot e^{-(r/B)^2} + er \\cdot \\int_{u=0} e^u e^{-(u+r/B)^2} du \\quad \\text{(define } u = \\ln s - r) \\\\\n\\leq CeB^2 e^{r/B} e^{-(r/B)^2}\n$$\n\nTherefore, in total, we have that \\( \\text{err}_2(p) \\leq 8er\\text{opt}_g + 8CeB^2 e^{r/B} e^{-(r/B)^2} + 2\\epsilon \\) and we may obtain\n\n$$\n\\text{Theorem 4.1 by picking } r = B(\\ln \\text{opt}_g)^{1/2}.\n$$\n\nD.2 Proof of Theorem 4.3\n\nWe first prove Lemma 4.4. We have that for any \\( y \\in \\{0, 1\\} \\) and \\( p \\in (0, 1) \\)\n\n$$\n\\ell_g(y, f'(p)) = y \\ln \\frac{1}{p} + (1 - y) \\ln \\frac{1}{1 - p} - \\ln 2 = CE(y, p) - \\ln 2 ,\n$$"}]}, {"page": 20, "text": "where CE(y, p) is the cross entropy function. It is sufficient to show that for y \u2208                                         {0, 1} and p \u2208    [0, 1],\n      Observe that                         |y \u2212   p| \u2264    CE(y, p) \u2264        2|y \u2212    p| log      p(1 \u22121   p)                                  (D.4)\n                                                    CE(0, p) = log               1        =    \u221e    pi                                        (D.5)\nwhere the series on the right converges for all p < 1. We can also write      1 \u2212   p         i=1    i\n                                                   CE(1, p) = log          1 p     =    \u221e    (1 \u2212 i p)i                                       (D.6)\n                                                                                       i=1\nwith the series converging for p > 0.\n      For the lower bound, we observe the following inequalities hold for all p \u2208                                           [0, 1]\n                                                         CE(0, p) \u2265        p = |0 \u2212      p|\n                                                         CE(1, p) \u2265        1 \u2212   p = |1 \u2212      p|.\nFor the upper bound, we first prove the claim for y = 0, where it states that\n                                          CE(0, p) = log            1 \u22121  p     \u2264   2p log      p(1 \u2212 1   p)     .                            (D.7)\n      When p \u2264         1/2 we can use Eq. (D.5) to bound\n                                              CE(0, p) \u2264         \u221e    pi       \u221e    pi =       p                                              (D.8)\n                                                                i=1    i \u2264    i=1           1 \u2212   p \u2264    2p.\nThe bounds holds by observing that since p(1 \u2212                               p) \u2265    1/4,\n      When p \u2265         1/2, we note that               log      p(1 \u22121   p)     \u2265   log(4) \u2265      1\n                                             log     p(1 \u22121   p)      \u2265   log     1 \u22121  p     = CE(0, p)\nand 2p \u2265        1. Hence the bound holds in this case too.\n      In the case where y = 1, the bound states that\n                                        CE(1, p) = log           1 p    \u2264   2(1 \u2212     p) log      p(1 \u22121   p)     .\nThis is implied by our bound for y = 0 by taking q = 1 \u2212                                      p. This concludes the proof of Lemma\n4.4 and we are ready to prove Theorem 4.3. The following are true\n                          err1(p) \u2264       Lg(f \u2032 \u25e6   p) \u2212    ln 2                                                                             (D.9)\n               Lg(w\u2217) \u2212        ln 2 \u2264     2 \u00b7 E    ln                        1                         \u00b7 |y \u2212   g\u2032(w\u2217     \u00b7 x)|             (D.10)\n                     Lg(f \u2032 \u25e6    p) \u2264     Lg(w\u2217) + \u01eb     g\u2032(w\u2217     \u00b7 x) \u00b7 (1 \u2212     g\u2032(w\u2217    \u00b7 x))                                           (D.11)\n                                                                            20", "md": "where $$CE(y, p)$$ is the cross entropy function. It is sufficient to show that for $$y \\in \\{0, 1\\}$$ and $$p \\in [0, 1]$$,\n\nObserve that $$|y - p| \\leq CE(y, p) \\leq 2|y - p| \\log p(1 - p)$$ (D.4)\n\n$$CE(0, p) = \\log 1 = \\infty$$ (D.5)\n\nwhere the series on the right converges for all $$p < 1$$. We can also write $$CE(1, p) = \\log \\frac{1}{p} = \\sum_{i=1}^{\\infty} (1 - i p)^i$$ (D.6)\n\nwith the series converging for $$p > 0$$.\n\nFor the lower bound, we observe the following inequalities hold for all $$p \\in [0, 1]$$\n\n$$CE(0, p) \\geq p = |0 - p|$$\n\n$$CE(1, p) \\geq 1 - p = |1 - p|$$.\n\nFor the upper bound, we first prove the claim for $$y = 0$$, where it states that\n\n$$CE(0, p) = \\log \\frac{1}{p} \\leq 2p \\log p(1 - p)$$ (D.7)\n\nWhen $$p \\leq 1/2$$ we can use Eq. (D.5) to bound\n\n$$CE(0, p) \\leq \\sum_{i=1}^{\\infty} pi \\leq \\sum_{i=1}^{1/p} i \\leq 2p$$.\n\nThe bounds hold by observing that since $$p(1 - p) \\geq 1/4$$,\n\nWhen $$p \\geq 1/2$$, we note that $$\\log p(1 - p) \\geq \\log(4) \\geq 1$$\n\n$$\\log p(1 - p) \\geq \\log \\frac{1}{p} = CE(0, p)$$\n\nand $$2p \\geq 1$$. Hence the bound holds in this case too.\n\nIn the case where $$y = 1$$, the bound states that\n\n$$CE(1, p) = \\log \\frac{1}{p} \\leq 2(1 - p) \\log p(1 - p)$$.\n\nThis is implied by our bound for $$y = 0$$ by taking $$q = 1 - p$$. This concludes the proof of Lemma 4.4 and we are ready to prove Theorem 4.3. The following are true\n\n$$err1(p) \\leq Lg(f' \\circ p) - \\ln 2$$ (D.9)\n\n$$Lg(w^*) - \\ln 2 \\leq 2 \\cdot E \\ln \\frac{1}{1} \\cdot |y - g'(w^* \\cdot x)|$$ (D.10)\n\n$$Lg(f' \\circ p) \\leq Lg(w^*) + \\epsilon g'(w^* \\cdot x) \\cdot (1 - g'(w^* \\cdot x))$$ (D.11)\n\n$$20$$", "images": [], "items": [{"type": "text", "value": "where $$CE(y, p)$$ is the cross entropy function. It is sufficient to show that for $$y \\in \\{0, 1\\}$$ and $$p \\in [0, 1]$$,\n\nObserve that $$|y - p| \\leq CE(y, p) \\leq 2|y - p| \\log p(1 - p)$$ (D.4)\n\n$$CE(0, p) = \\log 1 = \\infty$$ (D.5)\n\nwhere the series on the right converges for all $$p < 1$$. We can also write $$CE(1, p) = \\log \\frac{1}{p} = \\sum_{i=1}^{\\infty} (1 - i p)^i$$ (D.6)\n\nwith the series converging for $$p > 0$$.\n\nFor the lower bound, we observe the following inequalities hold for all $$p \\in [0, 1]$$\n\n$$CE(0, p) \\geq p = |0 - p|$$\n\n$$CE(1, p) \\geq 1 - p = |1 - p|$$.\n\nFor the upper bound, we first prove the claim for $$y = 0$$, where it states that\n\n$$CE(0, p) = \\log \\frac{1}{p} \\leq 2p \\log p(1 - p)$$ (D.7)\n\nWhen $$p \\leq 1/2$$ we can use Eq. (D.5) to bound\n\n$$CE(0, p) \\leq \\sum_{i=1}^{\\infty} pi \\leq \\sum_{i=1}^{1/p} i \\leq 2p$$.\n\nThe bounds hold by observing that since $$p(1 - p) \\geq 1/4$$,\n\nWhen $$p \\geq 1/2$$, we note that $$\\log p(1 - p) \\geq \\log(4) \\geq 1$$\n\n$$\\log p(1 - p) \\geq \\log \\frac{1}{p} = CE(0, p)$$\n\nand $$2p \\geq 1$$. Hence the bound holds in this case too.\n\nIn the case where $$y = 1$$, the bound states that\n\n$$CE(1, p) = \\log \\frac{1}{p} \\leq 2(1 - p) \\log p(1 - p)$$.\n\nThis is implied by our bound for $$y = 0$$ by taking $$q = 1 - p$$. This concludes the proof of Lemma 4.4 and we are ready to prove Theorem 4.3. The following are true\n\n$$err1(p) \\leq Lg(f' \\circ p) - \\ln 2$$ (D.9)\n\n$$Lg(w^*) - \\ln 2 \\leq 2 \\cdot E \\ln \\frac{1}{1} \\cdot |y - g'(w^* \\cdot x)|$$ (D.10)\n\n$$Lg(f' \\circ p) \\leq Lg(w^*) + \\epsilon g'(w^* \\cdot x) \\cdot (1 - g'(w^* \\cdot x))$$ (D.11)\n\n$$20$$", "md": "where $$CE(y, p)$$ is the cross entropy function. It is sufficient to show that for $$y \\in \\{0, 1\\}$$ and $$p \\in [0, 1]$$,\n\nObserve that $$|y - p| \\leq CE(y, p) \\leq 2|y - p| \\log p(1 - p)$$ (D.4)\n\n$$CE(0, p) = \\log 1 = \\infty$$ (D.5)\n\nwhere the series on the right converges for all $$p < 1$$. We can also write $$CE(1, p) = \\log \\frac{1}{p} = \\sum_{i=1}^{\\infty} (1 - i p)^i$$ (D.6)\n\nwith the series converging for $$p > 0$$.\n\nFor the lower bound, we observe the following inequalities hold for all $$p \\in [0, 1]$$\n\n$$CE(0, p) \\geq p = |0 - p|$$\n\n$$CE(1, p) \\geq 1 - p = |1 - p|$$.\n\nFor the upper bound, we first prove the claim for $$y = 0$$, where it states that\n\n$$CE(0, p) = \\log \\frac{1}{p} \\leq 2p \\log p(1 - p)$$ (D.7)\n\nWhen $$p \\leq 1/2$$ we can use Eq. (D.5) to bound\n\n$$CE(0, p) \\leq \\sum_{i=1}^{\\infty} pi \\leq \\sum_{i=1}^{1/p} i \\leq 2p$$.\n\nThe bounds hold by observing that since $$p(1 - p) \\geq 1/4$$,\n\nWhen $$p \\geq 1/2$$, we note that $$\\log p(1 - p) \\geq \\log(4) \\geq 1$$\n\n$$\\log p(1 - p) \\geq \\log \\frac{1}{p} = CE(0, p)$$\n\nand $$2p \\geq 1$$. Hence the bound holds in this case too.\n\nIn the case where $$y = 1$$, the bound states that\n\n$$CE(1, p) = \\log \\frac{1}{p} \\leq 2(1 - p) \\log p(1 - p)$$.\n\nThis is implied by our bound for $$y = 0$$ by taking $$q = 1 - p$$. This concludes the proof of Lemma 4.4 and we are ready to prove Theorem 4.3. The following are true\n\n$$err1(p) \\leq Lg(f' \\circ p) - \\ln 2$$ (D.9)\n\n$$Lg(w^*) - \\ln 2 \\leq 2 \\cdot E \\ln \\frac{1}{1} \\cdot |y - g'(w^* \\cdot x)|$$ (D.10)\n\n$$Lg(f' \\circ p) \\leq Lg(w^*) + \\epsilon g'(w^* \\cdot x) \\cdot (1 - g'(w^* \\cdot x))$$ (D.11)\n\n$$20$$"}]}, {"page": 21, "text": "    Similarly to the proof of Theorem 4.1, we observe that\n                             ln                 1                   \u2264 ln 4 + |w\u2217  \u00b7 x|\n                                  g\u2032(w\u2217  \u00b7 x) \u00b7 (1 \u2212g\u2032(w\u2217  \u00b7 x))\n    It remains to bound the quantity E[|w\u2217         \u00b7 x| \u00b7 |y \u2212g\u2032\n                                                               w(x)|]. Set Q = |w\u2217     \u00b7 x| \u00b7 |y \u2212g\u2032\nrandom variable). Then for any r \u2265         0 we have                                               w(x)| (Q is a\n                E[Q] = E[Q \u00b7    1{|w\u2217   \u00b7 x| \u2264 r}] + E[Q \u00b7  1{|w\u2217   \u00b7 x| > r}]\n                       \u2264 r E[|y \u2212  g\u2032         1{|w\u2217  \u00b7 x| \u2264  r}] + E[|w\u2217  \u00b7 x| \u00b71{|w\u2217   \u00b7 x| > r}]\n                                    w\u2217(x)| \u00b7\n                       \u2264 r \u00b7 opt + E[|w\u2217   \u00b7 x| \u00b71{|w\u2217  \u00b7 x| > r}]\nTo bound the quantity E[|w\u2217       \u00b7 x|\u00b71{|w\u2217\u00b7x| > r}], consider F(s) := Pr[|w\u2217         \u00b7 x|\u00b71{|w\u2217\u00b7x| > r} \u2265     s].\nWe have that\n                       F(s) =   1{s = 0} Pr[|w\u2217    \u00b7 x| \u2264 r] + Pr[|w\u2217   \u00b7 x| \u2265 max{s, r}] .\n                                            \u221e                \u221e\nSince E[|w\u2217   \u00b7 x| \u00b71{|w\u2217   \u00b7 x| > r}] =   s=0 F(s) ds =     s=0 Pr[|w\u2217  \u00b7 x| \u2265 max{s, r}] ds, we obtain\n           E[Q] =     \u221e  Pr [|w\u2217  \u00b7 x| \u2265 max{s, r}] ds\n                     s=0\n                 \u2264    r  Pr   |w\u2217  \u00b7 x| \u2265 Br   ds +    \u221e  Pr   |w\u2217  \u00b7 x| \u2265 Bs   ds           (since \u2225w\u2217\u22252 \u2264    B)\n                     s=0                              s=r\n                 \u2264  r \u00b7 e\u2212r/B +    \u221e  e\u2212s/B ds                                                     (see Def. 1.5)\n                                  s=r\n                 = r \u00b7 e\u2212r/B + B \u00b7 e\u2212r/B\nTherefore, in total, we have that err1(p) \u2264       2 ln 4 \u00b7 optg + 2(r + B)e\u2212r/B + 2r \u00b7 optg + \u01eb and we may\n                                                  1\nobtain Theorem 4.3 by picking r = B \u00b7 ln         opt.\n                                                        21", "md": "# Math Equations\n\nSimilarly to the proof of Theorem 4.1, we observe that\n\n$$\n\\frac{\\ln(1)}{g'(w^* \\cdot x) \\cdot (1 - g'(w^* \\cdot x))} \\leq \\ln(4) + |w^* \\cdot x|\n$$\nIt remains to bound the quantity E[|w^* \\cdot x| \u00b7 |y - g'(w(x))|]. Set Q = |w^* \\cdot x| \u00b7 |y - g'(w(x))| (Q is a random variable). Then for any r \u2265 0 we have\n\n$$\n\\begin{align*}\nE[Q] & = E[Q \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[Q \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq r E[|y - g'(w(x))| \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq r \\cdot \\text{opt} + E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}]\n\\end{align*}\n$$\nTo bound the quantity E[|w^* \\cdot x| \u00b7 1\\{|w^* \\cdot x| > r\\}], consider F(s) := Pr[|w^* \\cdot x| \u00b7 1\\{|w^* \\cdot x| > r\\} \u2265 s]. We have that\n\n$$\nF(s) = 1\\{s = 0\\} \\cdot \\text{Pr}[|w^* \\cdot x| \\leq r] + \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}]\n$$\nSince E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] = $\\int_{s=0}^{\\infty} F(s) ds = \\int_{s=0}^{\\infty} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}] ds$, we obtain\n\n$$\nE[Q] = \\int_{s=0}^{\\infty} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}] ds \\\\\n\\leq \\int_{s=0}^{r} \\text{Pr} \\{|w^* \\cdot x| \\geq Bs\\} ds + \\int_{s=r}^{\\infty} \\text{Pr} \\{|w^* \\cdot x| \\geq Bs\\} ds \\text{ (since } \\|w^*\\|_2 \\leq B) \\\\\n\\leq r \\cdot e^{-r/B} + \\int_{s=r}^{\\infty} e^{-s/B} ds \\text{ (see Def. 1.5)} \\\\\n= r \\cdot e^{-r/B} + B \\cdot e^{-r/B}\n$$\nTherefore, in total, we have that err1(p) \u2264 2 ln(4) \u00b7 optg + 2(r + B)e^{-r/B} + 2r \u00b7 optg + \u01eb and we may obtain Theorem 4.3 by picking r = B \u00b7 ln(1/21).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Similarly to the proof of Theorem 4.1, we observe that\n\n$$\n\\frac{\\ln(1)}{g'(w^* \\cdot x) \\cdot (1 - g'(w^* \\cdot x))} \\leq \\ln(4) + |w^* \\cdot x|\n$$\nIt remains to bound the quantity E[|w^* \\cdot x| \u00b7 |y - g'(w(x))|]. Set Q = |w^* \\cdot x| \u00b7 |y - g'(w(x))| (Q is a random variable). Then for any r \u2265 0 we have\n\n$$\n\\begin{align*}\nE[Q] & = E[Q \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[Q \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq r E[|y - g'(w(x))| \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq r \\cdot \\text{opt} + E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}]\n\\end{align*}\n$$\nTo bound the quantity E[|w^* \\cdot x| \u00b7 1\\{|w^* \\cdot x| > r\\}], consider F(s) := Pr[|w^* \\cdot x| \u00b7 1\\{|w^* \\cdot x| > r\\} \u2265 s]. We have that\n\n$$\nF(s) = 1\\{s = 0\\} \\cdot \\text{Pr}[|w^* \\cdot x| \\leq r] + \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}]\n$$\nSince E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] = $\\int_{s=0}^{\\infty} F(s) ds = \\int_{s=0}^{\\infty} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}] ds$, we obtain\n\n$$\nE[Q] = \\int_{s=0}^{\\infty} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}] ds \\\\\n\\leq \\int_{s=0}^{r} \\text{Pr} \\{|w^* \\cdot x| \\geq Bs\\} ds + \\int_{s=r}^{\\infty} \\text{Pr} \\{|w^* \\cdot x| \\geq Bs\\} ds \\text{ (since } \\|w^*\\|_2 \\leq B) \\\\\n\\leq r \\cdot e^{-r/B} + \\int_{s=r}^{\\infty} e^{-s/B} ds \\text{ (see Def. 1.5)} \\\\\n= r \\cdot e^{-r/B} + B \\cdot e^{-r/B}\n$$\nTherefore, in total, we have that err1(p) \u2264 2 ln(4) \u00b7 optg + 2(r + B)e^{-r/B} + 2r \u00b7 optg + \u01eb and we may obtain Theorem 4.3 by picking r = B \u00b7 ln(1/21).", "md": "Similarly to the proof of Theorem 4.1, we observe that\n\n$$\n\\frac{\\ln(1)}{g'(w^* \\cdot x) \\cdot (1 - g'(w^* \\cdot x))} \\leq \\ln(4) + |w^* \\cdot x|\n$$\nIt remains to bound the quantity E[|w^* \\cdot x| \u00b7 |y - g'(w(x))|]. Set Q = |w^* \\cdot x| \u00b7 |y - g'(w(x))| (Q is a random variable). Then for any r \u2265 0 we have\n\n$$\n\\begin{align*}\nE[Q] & = E[Q \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[Q \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq r E[|y - g'(w(x))| \\cdot 1\\{|w^* \\cdot x| \\leq r\\}] + E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] \\\\\n& \\leq r \\cdot \\text{opt} + E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}]\n\\end{align*}\n$$\nTo bound the quantity E[|w^* \\cdot x| \u00b7 1\\{|w^* \\cdot x| > r\\}], consider F(s) := Pr[|w^* \\cdot x| \u00b7 1\\{|w^* \\cdot x| > r\\} \u2265 s]. We have that\n\n$$\nF(s) = 1\\{s = 0\\} \\cdot \\text{Pr}[|w^* \\cdot x| \\leq r] + \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}]\n$$\nSince E[|w^* \\cdot x| \\cdot 1\\{|w^* \\cdot x| > r\\}] = $\\int_{s=0}^{\\infty} F(s) ds = \\int_{s=0}^{\\infty} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}] ds$, we obtain\n\n$$\nE[Q] = \\int_{s=0}^{\\infty} \\text{Pr}[|w^* \\cdot x| \\geq \\max\\{s, r\\}] ds \\\\\n\\leq \\int_{s=0}^{r} \\text{Pr} \\{|w^* \\cdot x| \\geq Bs\\} ds + \\int_{s=r}^{\\infty} \\text{Pr} \\{|w^* \\cdot x| \\geq Bs\\} ds \\text{ (since } \\|w^*\\|_2 \\leq B) \\\\\n\\leq r \\cdot e^{-r/B} + \\int_{s=r}^{\\infty} e^{-s/B} ds \\text{ (see Def. 1.5)} \\\\\n= r \\cdot e^{-r/B} + B \\cdot e^{-r/B}\n$$\nTherefore, in total, we have that err1(p) \u2264 2 ln(4) \u00b7 optg + 2(r + B)e^{-r/B} + 2r \u00b7 optg + \u01eb and we may obtain Theorem 4.3 by picking r = B \u00b7 ln(1/21)."}]}], "job_id": "8bb03f8a-3928-454c-9c91-a8ae9640fc83", "file_path": "./corpus/2306.10615.pdf"}