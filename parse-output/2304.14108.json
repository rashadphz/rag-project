{"pages": [{"page": 1, "text": "                                                             DATACOMP:\n                    In search of the next generation of multimodal datasets\n                              Samir Yitzhak Gadre*2, Gabriel Ilharco*1, Alex Fang*1, Jonathan Hayase1,\n                               Georgios Smyrnis5, Thao Nguyen1, Ryan Marten7,9, Mitchell Wortsman1,\n                           Dhruba Ghosh1, Jieyu Zhang1, Eyal Orgad3, Rahim Entezari10, Giannis Daras5,\n                                Sarah Pratt1, Vivek Ramanujan1, Yonatan Bitton11, Kalyani Marathe1,\narXiv:2304.14108v5  [cs.CV]  20 Oct 2023\n                               Stephen Mussmann1, Richard Vencu6, Mehdi Cherti6,8, Ranjay Krishna1,\n                                Pang Wei Koh1,12, Olga Saukh10, Alexander Ratner1,13, Shuran Song2,\n                                       Hannaneh Hajishirzi1,7, Ali Farhadi1, Romain Beaumont6,\n                                               Sewoong Oh1, Alex Dimakis5, Jenia Jitsev6,8,\n                                         Yair Carmon3, Vaishaal Shankar4, Ludwig Schmidt1,6,7\n                                                                   Abstract\n                              Multimodal datasets are a critical component in recent breakthroughs such as CLIP,\n                              Stable Diffusion and GPT-4, yet their design does not receive the same research\n                              attention as model architectures or training algorithms. To address this shortcoming\n                              in the machine learning ecosystem, we introduce DATACOMP, a testbed for dataset\n                              experiments centered around a new candidate pool of 12.8 billion image-text\n                              pairs from Common Crawl. Participants in our benchmark design new filtering\n                              techniques or curate new data sources and then evaluate their new dataset by\n                              running our standardized CLIP training code and testing the resulting model on 38\n                              downstream test sets. Our benchmark consists of multiple compute scales spanning\n                              four orders of magnitude, which enables the study of scaling trends and makes\n                              the benchmark accessible to researchers with varying resources. Our baseline\n                              experiments show that the DATACOMP workflow leads to better training sets. Our\n                              best baseline, DATACOMP-1B, enables training a CLIP ViT-L/14 from scratch to\n                              79.2% zero-shot accuracy on ImageNet, outperforming OpenAI\u2019s CLIP ViT-L/14\n                              by 3.7 percentage points while using the same training procedure and compute. We\n                              release DATACOMP and all accompanying code at www.datacomp.ai.\n                    1    Introduction\n                    Recent advances in multimodal learning such as CLIP [111], DALL-E [115, 116], Stable\n                    Diffusion [123], Flamingo [8], and GPT-4 [103] offer unprecedented generalization capabilities\n                    in zero-shot classification, image generation, and in-context learning. While these advances use\n                    different algorithmic techniques, e.g., contrastive learning, diffusion, or auto-regressive modeling,\n                    they all rest on a common foundation: large datasets containing paired image-text examples. For\n                    instance, CLIP\u2019s training set contains 400 million image-text pairs, and Stable Diffusion was trained\n                    on the two billion examples from LAION-2B [129]. This new generation of image-text datasets is\n                    1,000 times larger than previous datasets such as ImageNet, which contains 1.2M images [37, 126].\n                    Despite the central role of image-text datasets, little is known about them. Many state-of-the-art\n                    datasets are proprietary, and even for public datasets such as LAION-2B [129], it is unclear how\n                    design choices such as the data source or filtering techniques affect the resulting models. While there\n                    are thousands of ablation studies for algorithmic design choices (loss function, model architecture,\n                    etc.), datasets are often treated as monolithic artifacts without detailed investigation. Moreover,\n                       \u2217Equal contribution, randomly ordered.      Correspondence to contact@datacomp.ai.         1University\n                    of Washington 2Columbia University 3Tel Aviv University 4Apple 5UT Austin 6LAION 7AI2 8Juelich\n                    Supercomputing Center, Research Center Juelich 9University of Illinois Urbana-Champaign 10Graz University\n                    of Technology 11Hebrew University 12Google Research 13Snorkel AI\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "md": "# DATACOMP: In search of the next generation of multimodal datasets\n\n# DATACOMP: In search of the next generation of multimodal datasets\n\n*Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt*\n\narXiv:2304.14108v5 [cs.CV] 20 Oct 2023\n\n## Abstract\n\nMultimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DATACOMP, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DATACOMP workflow leads to better training sets. Our best baseline, DATACOMP-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI\u2019s CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DATACOMP and all accompanying code at www.datacomp.ai.\n\n## Introduction\n\nRecent advances in multimodal learning such as CLIP, DALL-E, Stable Diffusion, Flamingo, and GPT-4 offer unprecedented generalization capabilities in zero-shot classification, image generation, and in-context learning. While these advances use different algorithmic techniques, e.g., contrastive learning, diffusion, or auto-regressive modeling, they all rest on a common foundation: large datasets containing paired image-text examples. For instance, CLIP\u2019s training set contains 400 million image-text pairs, and Stable Diffusion was trained on the two billion examples from LAION-2B. This new generation of image-text datasets is 1,000 times larger than previous datasets such as ImageNet, which contains 1.2M images. Despite the central role of image-text datasets, little is known about them. Many state-of-the-art datasets are proprietary, and even for public datasets such as LAION-2B, it is unclear how design choices such as the data source or filtering techniques affect the resulting models. While there are thousands of ablation studies for algorithmic design choices (loss function, model architecture, etc.), datasets are often treated as monolithic artifacts without detailed investigation.\n\n*Equal contribution, randomly ordered. Correspondence to contact@datacomp.ai.\n\n1University of Washington 2Columbia University 3Tel Aviv University 4Apple 5UT Austin 6LAION 7AI2 8Juelich Supercomputing Center, Research Center Juelich 9University of Illinois Urbana-Champaign 10Graz University of Technology 11Hebrew University 12Google Research 13Snorkel AI\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "DATACOMP: In search of the next generation of multimodal datasets", "md": "# DATACOMP: In search of the next generation of multimodal datasets"}, {"type": "heading", "lvl": 1, "value": "DATACOMP: In search of the next generation of multimodal datasets", "md": "# DATACOMP: In search of the next generation of multimodal datasets"}, {"type": "text", "value": "*Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt*\n\narXiv:2304.14108v5 [cs.CV] 20 Oct 2023", "md": "*Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt*\n\narXiv:2304.14108v5 [cs.CV] 20 Oct 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Multimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DATACOMP, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DATACOMP workflow leads to better training sets. Our best baseline, DATACOMP-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI\u2019s CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DATACOMP and all accompanying code at www.datacomp.ai.", "md": "Multimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DATACOMP, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DATACOMP workflow leads to better training sets. Our best baseline, DATACOMP-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI\u2019s CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DATACOMP and all accompanying code at www.datacomp.ai."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Recent advances in multimodal learning such as CLIP, DALL-E, Stable Diffusion, Flamingo, and GPT-4 offer unprecedented generalization capabilities in zero-shot classification, image generation, and in-context learning. While these advances use different algorithmic techniques, e.g., contrastive learning, diffusion, or auto-regressive modeling, they all rest on a common foundation: large datasets containing paired image-text examples. For instance, CLIP\u2019s training set contains 400 million image-text pairs, and Stable Diffusion was trained on the two billion examples from LAION-2B. This new generation of image-text datasets is 1,000 times larger than previous datasets such as ImageNet, which contains 1.2M images. Despite the central role of image-text datasets, little is known about them. Many state-of-the-art datasets are proprietary, and even for public datasets such as LAION-2B, it is unclear how design choices such as the data source or filtering techniques affect the resulting models. While there are thousands of ablation studies for algorithmic design choices (loss function, model architecture, etc.), datasets are often treated as monolithic artifacts without detailed investigation.\n\n*Equal contribution, randomly ordered. Correspondence to contact@datacomp.ai.\n\n1University of Washington 2Columbia University 3Tel Aviv University 4Apple 5UT Austin 6LAION 7AI2 8Juelich Supercomputing Center, Research Center Juelich 9University of Illinois Urbana-Champaign 10Graz University of Technology 11Hebrew University 12Google Research 13Snorkel AI\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "md": "Recent advances in multimodal learning such as CLIP, DALL-E, Stable Diffusion, Flamingo, and GPT-4 offer unprecedented generalization capabilities in zero-shot classification, image generation, and in-context learning. While these advances use different algorithmic techniques, e.g., contrastive learning, diffusion, or auto-regressive modeling, they all rest on a common foundation: large datasets containing paired image-text examples. For instance, CLIP\u2019s training set contains 400 million image-text pairs, and Stable Diffusion was trained on the two billion examples from LAION-2B. This new generation of image-text datasets is 1,000 times larger than previous datasets such as ImageNet, which contains 1.2M images. Despite the central role of image-text datasets, little is known about them. Many state-of-the-art datasets are proprietary, and even for public datasets such as LAION-2B, it is unclear how design choices such as the data source or filtering techniques affect the resulting models. While there are thousands of ablation studies for algorithmic design choices (loss function, model architecture, etc.), datasets are often treated as monolithic artifacts without detailed investigation.\n\n*Equal contribution, randomly ordered. Correspondence to contact@datacomp.ai.\n\n1University of Washington 2Columbia University 3Tel Aviv University 4Apple 5UT Austin 6LAION 7AI2 8Juelich Supercomputing Center, Research Center Juelich 9University of Illinois Urbana-Champaign 10Graz University of Technology 11Hebrew University 12Google Research 13Snorkel AI\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks."}]}, {"page": 2, "text": "Table 1: Zero-shot performance of CLIP models trained on different datasets. DATACOMP-1B,\nassembled with a simple filtering procedure on image-text pairs from Common Crawl, leads to a\nmodel with higher accuracy than previous results while using the same number of multiply-accumulate\noperations (MACs) or less during training. See Section 3.5 for details on the evaluation datasets.\n   Dataset                     Dataset size   # samples    Architecture    Train compute     ImageNet\n                                                 seen                         (MACs)         accuracy\n   OpenAI\u2019s WIT [111]              0.4B          13B         ViT-L/14        1.1 \u00d7 1021         75.5\n   LAION-400M [128, 28]            0.4B          13B         ViT-L/14        1.1 \u00d7 1021         72.8\n   LAION-2B [129, 28]              2.3B          13B         ViT-L/14        1.1 \u00d7 1021         73.1\n   LAION-2B [129, 28]              2.3B          34B         ViT-H/14        6.5 \u00d7 1021         78.0\n   LAION-2B [129, 28]              2.3B          34B         ViT-g/14        9.9 \u00d7 1021         78.5\n   DATACOMP-1B (ours)              1.4B          13B         ViT-L/14        1.1 \u00d7 1021         79.2\ndatasets currently lack the benchmark-driven development process that has enabled a steady stream\nof improvements on the model side and isolates data enhancements from changes to the model. These\nissues impede further progress in multimodal learning, as evidenced by recent work showing that\npublic datasets currently do not match the scaling behavior of proprietary alternatives [28].\nIn this paper, we take a step towards a more rigorous dataset development process. Our first and central\ncontribution is DATACOMP, a new benchmark for multimodal dataset design. DATACOMP flips\nthe traditional benchmarking paradigm in machine learning where the dataset is fixed and researchers\npropose new training algorithms. Instead, we hold the entire training code and computational budget\nconstant so that participants innovate by proposing new training sets. To evaluate the quality of a\ntraining set, we score the resulting model with a testbed of 38 classification and retrieval tasks such as\nImageNet [37], ImageNetV2 [121], DTD [30], EuroSAT [63], SUN-397 [146], and MSCOCO [26].\nDATACOMP focuses on two key challenges that arise when assembling large training datasets: what\ndata sources to train on, and how to filter a given data source. Each challenge corresponds to one\ntrack in our benchmark. To facilitate the filtering track, our second contribution is COMMONPOOL,\na dataset of 12.8B image-text pairs collected from Common Crawl and currently the largest public\nimage-text dataset. We release CommonPool as an index of image url-text pairs under a CC-BY-4.0\nlicense, and apply content checks in its construction to remove unsafe or unwanted content. In the\nfiltering track, the goal of participants is to find the best subset of COMMONPOOL to train on. In the\nsecond track, Bring Your Own Data (BYOD), participants may leverage any data source, as long as\nit does not overlap with our evaluation testbed.\nOur third contribution is an investigation of scaling trends for dataset design. In particular,\nDATACOMP contains four scales, where we vary the training budget and the candidate pool size from\n12.8M to 12.8B samples (see Table 2). Expressed in GPU hours, the cost of a single training run\nranges from 4 to 40,000 GPU hours on the A100 cluster we used for development. The different\nscales enable researchers with different resources to participate in our benchmark. Moreover, our\nresults show that the ranking of filtering approaches is largely consistent across scale.\nOur fourth contribution is over three hundred baseline experiments, including techniques such\nas querying captions for relevant keywords, filtering based on image embeddings, and applying\na threshold on CLIP scores. A key result from our baselines experiments is that smaller, more\nstringently filtered datasets can lead to models that generalize better than larger datasets coming from\nthe same pool. At the 12.8B scale, our best filtering baseline increases ImageNet zero-shot accuracy\nby 6.9 percentage points (pp) relative to the unfiltered pool (see Table 3). For the BYOD track, our\ninitial experiments show that 109M additional data points (less than 1% of the 12.8B pool) improve\nthe CLIP-filtered subsets of COMMONPOOL by up to 1.2 pp ImageNet accuracy (see Table 18).\nFinally, our fifth contribution is DATACOMP-1B, a new state-of-the-art multimodal dataset. We\nobtain DATACOMP-1B by combining our two most promising filtering baselines. DATACOMP-1B\nenables training a CLIP ViT-L/14 model to an ImageNet zero-shot accuracy of 79.2% (see Table 1),\ncorresponding to a 9\u00d7 computational cost reduction when compared to a larger CLIP ViT-g/14 model\ntrained on LAION-2B for about 3\u00d7 longer. Moreover, our model outperforms OpenAI\u2019s original\nCLIP ViT-L/14 by 3.7 percentage points, while using the same compute budget.\nTo make DATACOMP a shared environment for controlled dataset experiments, we publicly release\nour candidate pool url index, our tooling for assembling these pools, our filtering baselines, and our\n                                                     2", "md": "|DATASET|Dataset size|# samples seen|Architecture|Train compute (MACs)|ImageNet accuracy|\n|---|---|---|---|---|---|\n|OpenAI\u2019s WIT [111]|0.4B|13B|ViT-L/14|1.1 \u00d7 1021|75.5|\n|LAION-400M [128, 28]|0.4B|13B|ViT-L/14|1.1 \u00d7 1021|72.8|\n|LAION-2B [129, 28]|2.3B|13B|ViT-L/14|1.1 \u00d7 1021|73.1|\n|LAION-2B [129, 28]|2.3B|34B|ViT-H/14|6.5 \u00d7 1021|78.0|\n|LAION-2B [129, 28]|2.3B|34B|ViT-g/14|9.9 \u00d7 1021|78.5|\n|DATACOMP-1B (ours)|1.4B|13B|ViT-L/14|1.1 \u00d7 1021|79.2|\n\ndatasets currently lack the benchmark-driven development process that has enabled a steady stream of improvements on the model side and isolates data enhancements from changes to the model. These issues impede further progress in multimodal learning, as evidenced by recent work showing that public datasets currently do not match the scaling behavior of proprietary alternatives [28].\n\nIn this paper, we take a step towards a more rigorous dataset development process. Our first and central contribution is DATACOMP, a new benchmark for multimodal dataset design. DATACOMP flips the traditional benchmarking paradigm in machine learning where the dataset is fixed and researchers propose new training algorithms. Instead, we hold the entire training code and computational budget constant so that participants innovate by proposing new training sets. To evaluate the quality of a training set, we score the resulting model with a testbed of 38 classification and retrieval tasks such as ImageNet [37], ImageNetV2 [121], DTD [30], EuroSAT [63], SUN-397 [146], and MSCOCO [26].\n\nDATACOMP focuses on two key challenges that arise when assembling large training datasets: what data sources to train on, and how to filter a given data source. Each challenge corresponds to one track in our benchmark. To facilitate the filtering track, our second contribution is COMMONPOOL, a dataset of 12.8B image-text pairs collected from Common Crawl and currently the largest public image-text dataset. We release CommonPool as an index of image url-text pairs under a CC-BY-4.0 license, and apply content checks in its construction to remove unsafe or unwanted content. In the filtering track, the goal of participants is to find the best subset of COMMONPOOL to train on. In the second track, Bring Your Own Data (BYOD), participants may leverage any data source, as long as it does not overlap with our evaluation testbed.\n\nOur third contribution is an investigation of scaling trends for dataset design. In particular, DATACOMP contains four scales, where we vary the training budget and the candidate pool size from 12.8M to 12.8B samples (see Table 2). Expressed in GPU hours, the cost of a single training run ranges from 4 to 40,000 GPU hours on the A100 cluster we used for development. The different scales enable researchers with different resources to participate in our benchmark. Moreover, our results show that the ranking of filtering approaches is largely consistent across scale.\n\nOur fourth contribution is over three hundred baseline experiments, including techniques such as querying captions for relevant keywords, filtering based on image embeddings, and applying a threshold on CLIP scores. A key result from our baselines experiments is that smaller, more stringently filtered datasets can lead to models that generalize better than larger datasets coming from the same pool. At the 12.8B scale, our best filtering baseline increases ImageNet zero-shot accuracy by 6.9 percentage points (pp) relative to the unfiltered pool (see Table 3). For the BYOD track, our initial experiments show that 109M additional data points (less than 1% of the 12.8B pool) improve the CLIP-filtered subsets of COMMONPOOL by up to 1.2 pp ImageNet accuracy (see Table 18).\n\nFinally, our fifth contribution is DATACOMP-1B, a new state-of-the-art multimodal dataset. We obtain DATACOMP-1B by combining our two most promising filtering baselines. DATACOMP-1B enables training a CLIP ViT-L/14 model to an ImageNet zero-shot accuracy of 79.2% (see Table 1), corresponding to a 9\u00d7 computational cost reduction when compared to a larger CLIP ViT-g/14 model trained on LAION-2B for about 3\u00d7 longer. Moreover, our model outperforms OpenAI\u2019s original CLIP ViT-L/14 by 3.7 percentage points, while using the same compute budget.\n\nTo make DATACOMP a shared environment for controlled dataset experiments, we publicly release our candidate pool url index, our tooling for assembling these pools, our filtering baselines, and our", "images": [], "items": [{"type": "table", "rows": [["DATASET", "Dataset size", "# samples seen", "Architecture", "Train compute (MACs)", "ImageNet accuracy"], ["OpenAI\u2019s WIT [111]", "0.4B", "13B", "ViT-L/14", "1.1 \u00d7 1021", "75.5"], ["LAION-400M [128, 28]", "0.4B", "13B", "ViT-L/14", "1.1 \u00d7 1021", "72.8"], ["LAION-2B [129, 28]", "2.3B", "13B", "ViT-L/14", "1.1 \u00d7 1021", "73.1"], ["LAION-2B [129, 28]", "2.3B", "34B", "ViT-H/14", "6.5 \u00d7 1021", "78.0"], ["LAION-2B [129, 28]", "2.3B", "34B", "ViT-g/14", "9.9 \u00d7 1021", "78.5"], ["DATACOMP-1B (ours)", "1.4B", "13B", "ViT-L/14", "1.1 \u00d7 1021", "79.2"]], "md": "|DATASET|Dataset size|# samples seen|Architecture|Train compute (MACs)|ImageNet accuracy|\n|---|---|---|---|---|---|\n|OpenAI\u2019s WIT [111]|0.4B|13B|ViT-L/14|1.1 \u00d7 1021|75.5|\n|LAION-400M [128, 28]|0.4B|13B|ViT-L/14|1.1 \u00d7 1021|72.8|\n|LAION-2B [129, 28]|2.3B|13B|ViT-L/14|1.1 \u00d7 1021|73.1|\n|LAION-2B [129, 28]|2.3B|34B|ViT-H/14|6.5 \u00d7 1021|78.0|\n|LAION-2B [129, 28]|2.3B|34B|ViT-g/14|9.9 \u00d7 1021|78.5|\n|DATACOMP-1B (ours)|1.4B|13B|ViT-L/14|1.1 \u00d7 1021|79.2|", "isPerfectTable": true, "csv": "\"DATASET\",\"Dataset size\",\"# samples seen\",\"Architecture\",\"Train compute (MACs)\",\"ImageNet accuracy\"\n\"OpenAI\u2019s WIT [111]\",\"0.4B\",\"13B\",\"ViT-L/14\",\"1.1 \u00d7 1021\",\"75.5\"\n\"LAION-400M [128, 28]\",\"0.4B\",\"13B\",\"ViT-L/14\",\"1.1 \u00d7 1021\",\"72.8\"\n\"LAION-2B [129, 28]\",\"2.3B\",\"13B\",\"ViT-L/14\",\"1.1 \u00d7 1021\",\"73.1\"\n\"LAION-2B [129, 28]\",\"2.3B\",\"34B\",\"ViT-H/14\",\"6.5 \u00d7 1021\",\"78.0\"\n\"LAION-2B [129, 28]\",\"2.3B\",\"34B\",\"ViT-g/14\",\"9.9 \u00d7 1021\",\"78.5\"\n\"DATACOMP-1B (ours)\",\"1.4B\",\"13B\",\"ViT-L/14\",\"1.1 \u00d7 1021\",\"79.2\""}, {"type": "text", "value": "datasets currently lack the benchmark-driven development process that has enabled a steady stream of improvements on the model side and isolates data enhancements from changes to the model. These issues impede further progress in multimodal learning, as evidenced by recent work showing that public datasets currently do not match the scaling behavior of proprietary alternatives [28].\n\nIn this paper, we take a step towards a more rigorous dataset development process. Our first and central contribution is DATACOMP, a new benchmark for multimodal dataset design. DATACOMP flips the traditional benchmarking paradigm in machine learning where the dataset is fixed and researchers propose new training algorithms. Instead, we hold the entire training code and computational budget constant so that participants innovate by proposing new training sets. To evaluate the quality of a training set, we score the resulting model with a testbed of 38 classification and retrieval tasks such as ImageNet [37], ImageNetV2 [121], DTD [30], EuroSAT [63], SUN-397 [146], and MSCOCO [26].\n\nDATACOMP focuses on two key challenges that arise when assembling large training datasets: what data sources to train on, and how to filter a given data source. Each challenge corresponds to one track in our benchmark. To facilitate the filtering track, our second contribution is COMMONPOOL, a dataset of 12.8B image-text pairs collected from Common Crawl and currently the largest public image-text dataset. We release CommonPool as an index of image url-text pairs under a CC-BY-4.0 license, and apply content checks in its construction to remove unsafe or unwanted content. In the filtering track, the goal of participants is to find the best subset of COMMONPOOL to train on. In the second track, Bring Your Own Data (BYOD), participants may leverage any data source, as long as it does not overlap with our evaluation testbed.\n\nOur third contribution is an investigation of scaling trends for dataset design. In particular, DATACOMP contains four scales, where we vary the training budget and the candidate pool size from 12.8M to 12.8B samples (see Table 2). Expressed in GPU hours, the cost of a single training run ranges from 4 to 40,000 GPU hours on the A100 cluster we used for development. The different scales enable researchers with different resources to participate in our benchmark. Moreover, our results show that the ranking of filtering approaches is largely consistent across scale.\n\nOur fourth contribution is over three hundred baseline experiments, including techniques such as querying captions for relevant keywords, filtering based on image embeddings, and applying a threshold on CLIP scores. A key result from our baselines experiments is that smaller, more stringently filtered datasets can lead to models that generalize better than larger datasets coming from the same pool. At the 12.8B scale, our best filtering baseline increases ImageNet zero-shot accuracy by 6.9 percentage points (pp) relative to the unfiltered pool (see Table 3). For the BYOD track, our initial experiments show that 109M additional data points (less than 1% of the 12.8B pool) improve the CLIP-filtered subsets of COMMONPOOL by up to 1.2 pp ImageNet accuracy (see Table 18).\n\nFinally, our fifth contribution is DATACOMP-1B, a new state-of-the-art multimodal dataset. We obtain DATACOMP-1B by combining our two most promising filtering baselines. DATACOMP-1B enables training a CLIP ViT-L/14 model to an ImageNet zero-shot accuracy of 79.2% (see Table 1), corresponding to a 9\u00d7 computational cost reduction when compared to a larger CLIP ViT-g/14 model trained on LAION-2B for about 3\u00d7 longer. Moreover, our model outperforms OpenAI\u2019s original CLIP ViT-L/14 by 3.7 percentage points, while using the same compute budget.\n\nTo make DATACOMP a shared environment for controlled dataset experiments, we publicly release our candidate pool url index, our tooling for assembling these pools, our filtering baselines, and our", "md": "datasets currently lack the benchmark-driven development process that has enabled a steady stream of improvements on the model side and isolates data enhancements from changes to the model. These issues impede further progress in multimodal learning, as evidenced by recent work showing that public datasets currently do not match the scaling behavior of proprietary alternatives [28].\n\nIn this paper, we take a step towards a more rigorous dataset development process. Our first and central contribution is DATACOMP, a new benchmark for multimodal dataset design. DATACOMP flips the traditional benchmarking paradigm in machine learning where the dataset is fixed and researchers propose new training algorithms. Instead, we hold the entire training code and computational budget constant so that participants innovate by proposing new training sets. To evaluate the quality of a training set, we score the resulting model with a testbed of 38 classification and retrieval tasks such as ImageNet [37], ImageNetV2 [121], DTD [30], EuroSAT [63], SUN-397 [146], and MSCOCO [26].\n\nDATACOMP focuses on two key challenges that arise when assembling large training datasets: what data sources to train on, and how to filter a given data source. Each challenge corresponds to one track in our benchmark. To facilitate the filtering track, our second contribution is COMMONPOOL, a dataset of 12.8B image-text pairs collected from Common Crawl and currently the largest public image-text dataset. We release CommonPool as an index of image url-text pairs under a CC-BY-4.0 license, and apply content checks in its construction to remove unsafe or unwanted content. In the filtering track, the goal of participants is to find the best subset of COMMONPOOL to train on. In the second track, Bring Your Own Data (BYOD), participants may leverage any data source, as long as it does not overlap with our evaluation testbed.\n\nOur third contribution is an investigation of scaling trends for dataset design. In particular, DATACOMP contains four scales, where we vary the training budget and the candidate pool size from 12.8M to 12.8B samples (see Table 2). Expressed in GPU hours, the cost of a single training run ranges from 4 to 40,000 GPU hours on the A100 cluster we used for development. The different scales enable researchers with different resources to participate in our benchmark. Moreover, our results show that the ranking of filtering approaches is largely consistent across scale.\n\nOur fourth contribution is over three hundred baseline experiments, including techniques such as querying captions for relevant keywords, filtering based on image embeddings, and applying a threshold on CLIP scores. A key result from our baselines experiments is that smaller, more stringently filtered datasets can lead to models that generalize better than larger datasets coming from the same pool. At the 12.8B scale, our best filtering baseline increases ImageNet zero-shot accuracy by 6.9 percentage points (pp) relative to the unfiltered pool (see Table 3). For the BYOD track, our initial experiments show that 109M additional data points (less than 1% of the 12.8B pool) improve the CLIP-filtered subsets of COMMONPOOL by up to 1.2 pp ImageNet accuracy (see Table 18).\n\nFinally, our fifth contribution is DATACOMP-1B, a new state-of-the-art multimodal dataset. We obtain DATACOMP-1B by combining our two most promising filtering baselines. DATACOMP-1B enables training a CLIP ViT-L/14 model to an ImageNet zero-shot accuracy of 79.2% (see Table 1), corresponding to a 9\u00d7 computational cost reduction when compared to a larger CLIP ViT-g/14 model trained on LAION-2B for about 3\u00d7 longer. Moreover, our model outperforms OpenAI\u2019s original CLIP ViT-L/14 by 3.7 percentage points, while using the same compute budget.\n\nTo make DATACOMP a shared environment for controlled dataset experiments, we publicly release our candidate pool url index, our tooling for assembling these pools, our filtering baselines, and our"}]}, {"page": 3, "text": "Figure 1: DATACOMP participant workflow. A) Choose a scale based on resource constraints. B)\nDesign a dataset, in either the filtering or BYOD track. C) Train a CLIP model on the designed\ndataset using a fixed architecture and hyperparameters (Section 3.4). D) Evaluate the trained model\non a suite of diverse downstream tasks (Section 3.5).\ncode for training and evaluating models at www.datacomp.ai. We believe that our infrastructure\nwill help put research on dataset design on rigorous empirical foundations, draw attention to this\nunderstudied research area, and lead to the next generation of multimodal datasets.\n2    Related Work\nWe review the most closely related work and include additional related work in Appendix C.\nThe effects of data curation. Classical work considers dataset cleaning and outlier removal [74, 152,\n124, 125] to discard samples that may lead to undesirable model bias. A related line of work develops\ncoreset selection algorithms [61, 7, 46, 11, 94, 145, 32], which aim to select data subsets that lead to\nthe same performance as training on the entire dataset. These techniques appear to scale poorly to\nlarger data regimes [51, 6]. More recent efforts in subset selection often operate on already curated\ndatasets [98, 141, 130, 16, 33, 106] (e.g., CIFAR-10, ImageNet) or on smaller data regimes (e.g.,\nYFCC-15M [111, 140]). These settings often do not reflect newer training paradigms that involve (1)\nnoisy image-text pairs instead of category labeled images and (2) large scale datasets (e.g., billions\nof samples). While data-centric investigations have led to community competitions like DCBENCH\n[43] and DATAPERF [97], existing benchmarks have likewise operated at small data scales [100]\ncompared to datasets like LAION-2B [129], which contains over two billion images. DATACOMP\nbridges this gap by aligning data-centric investigation with large scale image-text training.\nThere has also been renewed interest in dataset pruning and deduplication. Sorscher et al. [135] show\nthat data pruning can improve traditional scaling trends on ImageNet, but do not consider image-text\ntraining or larger datasets. Raffel et al. [113] remove sentence redundancies when creating the C4\ncorpus. Subsequent work further demonstrated the benefits of deduplication for better language\nmodeling [90]. Radenovic et al. [110] introduce CAT filtering for image-text datasets\u2014a rule-based\nsystem to retain high quality samples. Abbas et al. [6] propose SemDeDup, which starts with the\nCAT-filtered LAION-440M subset, further employing clustering to remove semantic duplicates.\nDATACOMP facilitates data-centric investigation at an even larger scale (i.e., 12.8B sample scale) and\nprovides a common experimental setting for fair comparison amongst dataset creation algorithms.\nLarge-scale multimodal datasets. Datasets have been instrumental to building multimodal models\nlike CLIP [111], Flamingo [8], Stable Diffusion [123], DALL-E [115, 116] and GPT-4 [103]. These\nmethods succeeded by training on large, heterogeneous datasets rather than solely through advanced\nmodelling techniques. For example, OpenAI\u2019s CLIP trains on 400M image-text pairs from the web,\nroughly 300\u00d7 the size of ImageNet [37]. Prior work on scaling image-text datasets also provides\npromising trends with respect to zero-shot model performance [73, 107]. Additional large scale\ndatasets like FILIP-300M [149], FLD-900M [153], and PaLI-10B [25] were constructed to train\nmultimodal models. However, many datasets used to train such models (including the dataset for\nOpenAI\u2019s CLIP) are proprietary, making it hard to conduct data-centric investigations.\nEven for public image-text datasets like SBU [104], Flickr30k [151], MS-COCO [26], TaiSu [92],\nConceptual Captions [131], CC12M [24], RedCaps [38], WIT [136], Shutterstock [101], YFCC-\n                                                   3", "md": "# DATACOMP Participant Workflow\n\n## Figure 1: DATACOMP participant workflow\n\nA) Choose a scale based on resource constraints.\n\nB) Design a dataset, in either the filtering or BYOD track.\n\nC) Train a CLIP model on the designed dataset using a fixed architecture and hyperparameters (Section 3.4).\n\nD) Evaluate the trained model on a suite of diverse downstream tasks (Section 3.5).\n\nCode for training and evaluating models at www.datacomp.ai.\n\nWe believe that our infrastructure will help put research on dataset design on rigorous empirical foundations, draw attention to this understudied research area, and lead to the next generation of multimodal datasets.\n\n## Related Work\n\nWe review the most closely related work and include additional related work in Appendix C.\n\nThe effects of data curation. Classical work considers dataset cleaning and outlier removal [74, 152, 124, 125] to discard samples that may lead to undesirable model bias. A related line of work develops coreset selection algorithms [61, 7, 46, 11, 94, 145, 32], which aim to select data subsets that lead to the same performance as training on the entire dataset. These techniques appear to scale poorly to larger data regimes [51, 6]. More recent efforts in subset selection often operate on already curated datasets [98, 141, 130, 16, 33, 106] (e.g., CIFAR-10, ImageNet) or on smaller data regimes (e.g., YFCC-15M [111, 140]). These settings often do not reflect newer training paradigms that involve (1) noisy image-text pairs instead of category labeled images and (2) large scale datasets (e.g., billions of samples). While data-centric investigations have led to community competitions like DCBENCH [43] and DATAPERF [97], existing benchmarks have likewise operated at small data scales [100] compared to datasets like LAION-2B [129], which contains over two billion images. DATACOMP bridges this gap by aligning data-centric investigation with large scale image-text training.\n\nThere has also been renewed interest in dataset pruning and deduplication. Sorscher et al. [135] show that data pruning can improve traditional scaling trends on ImageNet, but do not consider image-text training or larger datasets. Raffel et al. [113] remove sentence redundancies when creating the C4 corpus. Subsequent work further demonstrated the benefits of deduplication for better language modeling [90]. Radenovic et al. [110] introduce CAT filtering for image-text datasets\u2014a rule-based system to retain high quality samples. Abbas et al. [6] propose SemDeDup, which starts with the CAT-filtered LAION-440M subset, further employing clustering to remove semantic duplicates. DATACOMP facilitates data-centric investigation at an even larger scale (i.e., 12.8B sample scale) and provides a common experimental setting for fair comparison amongst dataset creation algorithms.\n\nLarge-scale multimodal datasets. Datasets have been instrumental to building multimodal models like CLIP [111], Flamingo [8], Stable Diffusion [123], DALL-E [115, 116] and GPT-4 [103]. These methods succeeded by training on large, heterogeneous datasets rather than solely through advanced modelling techniques. For example, OpenAI\u2019s CLIP trains on 400M image-text pairs from the web, roughly 300\u00d7 the size of ImageNet [37]. Prior work on scaling image-text datasets also provides promising trends with respect to zero-shot model performance [73, 107]. Additional large scale datasets like FILIP-300M [149], FLD-900M [153], and PaLI-10B [25] were constructed to train multimodal models. However, many datasets used to train such models (including the dataset for OpenAI\u2019s CLIP) are proprietary, making it hard to conduct data-centric investigations.\n\nEven for public image-text datasets like SBU [104], Flickr30k [151], MS-COCO [26], TaiSu [92], Conceptual Captions [131], CC12M [24], RedCaps [38], WIT [136], Shutterstock [101], YFCC-3", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "DATACOMP Participant Workflow", "md": "# DATACOMP Participant Workflow"}, {"type": "heading", "lvl": 2, "value": "Figure 1: DATACOMP participant workflow", "md": "## Figure 1: DATACOMP participant workflow"}, {"type": "text", "value": "A) Choose a scale based on resource constraints.\n\nB) Design a dataset, in either the filtering or BYOD track.\n\nC) Train a CLIP model on the designed dataset using a fixed architecture and hyperparameters (Section 3.4).\n\nD) Evaluate the trained model on a suite of diverse downstream tasks (Section 3.5).\n\nCode for training and evaluating models at www.datacomp.ai.\n\nWe believe that our infrastructure will help put research on dataset design on rigorous empirical foundations, draw attention to this understudied research area, and lead to the next generation of multimodal datasets.", "md": "A) Choose a scale based on resource constraints.\n\nB) Design a dataset, in either the filtering or BYOD track.\n\nC) Train a CLIP model on the designed dataset using a fixed architecture and hyperparameters (Section 3.4).\n\nD) Evaluate the trained model on a suite of diverse downstream tasks (Section 3.5).\n\nCode for training and evaluating models at www.datacomp.ai.\n\nWe believe that our infrastructure will help put research on dataset design on rigorous empirical foundations, draw attention to this understudied research area, and lead to the next generation of multimodal datasets."}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "We review the most closely related work and include additional related work in Appendix C.\n\nThe effects of data curation. Classical work considers dataset cleaning and outlier removal [74, 152, 124, 125] to discard samples that may lead to undesirable model bias. A related line of work develops coreset selection algorithms [61, 7, 46, 11, 94, 145, 32], which aim to select data subsets that lead to the same performance as training on the entire dataset. These techniques appear to scale poorly to larger data regimes [51, 6]. More recent efforts in subset selection often operate on already curated datasets [98, 141, 130, 16, 33, 106] (e.g., CIFAR-10, ImageNet) or on smaller data regimes (e.g., YFCC-15M [111, 140]). These settings often do not reflect newer training paradigms that involve (1) noisy image-text pairs instead of category labeled images and (2) large scale datasets (e.g., billions of samples). While data-centric investigations have led to community competitions like DCBENCH [43] and DATAPERF [97], existing benchmarks have likewise operated at small data scales [100] compared to datasets like LAION-2B [129], which contains over two billion images. DATACOMP bridges this gap by aligning data-centric investigation with large scale image-text training.\n\nThere has also been renewed interest in dataset pruning and deduplication. Sorscher et al. [135] show that data pruning can improve traditional scaling trends on ImageNet, but do not consider image-text training or larger datasets. Raffel et al. [113] remove sentence redundancies when creating the C4 corpus. Subsequent work further demonstrated the benefits of deduplication for better language modeling [90]. Radenovic et al. [110] introduce CAT filtering for image-text datasets\u2014a rule-based system to retain high quality samples. Abbas et al. [6] propose SemDeDup, which starts with the CAT-filtered LAION-440M subset, further employing clustering to remove semantic duplicates. DATACOMP facilitates data-centric investigation at an even larger scale (i.e., 12.8B sample scale) and provides a common experimental setting for fair comparison amongst dataset creation algorithms.\n\nLarge-scale multimodal datasets. Datasets have been instrumental to building multimodal models like CLIP [111], Flamingo [8], Stable Diffusion [123], DALL-E [115, 116] and GPT-4 [103]. These methods succeeded by training on large, heterogeneous datasets rather than solely through advanced modelling techniques. For example, OpenAI\u2019s CLIP trains on 400M image-text pairs from the web, roughly 300\u00d7 the size of ImageNet [37]. Prior work on scaling image-text datasets also provides promising trends with respect to zero-shot model performance [73, 107]. Additional large scale datasets like FILIP-300M [149], FLD-900M [153], and PaLI-10B [25] were constructed to train multimodal models. However, many datasets used to train such models (including the dataset for OpenAI\u2019s CLIP) are proprietary, making it hard to conduct data-centric investigations.\n\nEven for public image-text datasets like SBU [104], Flickr30k [151], MS-COCO [26], TaiSu [92], Conceptual Captions [131], CC12M [24], RedCaps [38], WIT [136], Shutterstock [101], YFCC-3", "md": "We review the most closely related work and include additional related work in Appendix C.\n\nThe effects of data curation. Classical work considers dataset cleaning and outlier removal [74, 152, 124, 125] to discard samples that may lead to undesirable model bias. A related line of work develops coreset selection algorithms [61, 7, 46, 11, 94, 145, 32], which aim to select data subsets that lead to the same performance as training on the entire dataset. These techniques appear to scale poorly to larger data regimes [51, 6]. More recent efforts in subset selection often operate on already curated datasets [98, 141, 130, 16, 33, 106] (e.g., CIFAR-10, ImageNet) or on smaller data regimes (e.g., YFCC-15M [111, 140]). These settings often do not reflect newer training paradigms that involve (1) noisy image-text pairs instead of category labeled images and (2) large scale datasets (e.g., billions of samples). While data-centric investigations have led to community competitions like DCBENCH [43] and DATAPERF [97], existing benchmarks have likewise operated at small data scales [100] compared to datasets like LAION-2B [129], which contains over two billion images. DATACOMP bridges this gap by aligning data-centric investigation with large scale image-text training.\n\nThere has also been renewed interest in dataset pruning and deduplication. Sorscher et al. [135] show that data pruning can improve traditional scaling trends on ImageNet, but do not consider image-text training or larger datasets. Raffel et al. [113] remove sentence redundancies when creating the C4 corpus. Subsequent work further demonstrated the benefits of deduplication for better language modeling [90]. Radenovic et al. [110] introduce CAT filtering for image-text datasets\u2014a rule-based system to retain high quality samples. Abbas et al. [6] propose SemDeDup, which starts with the CAT-filtered LAION-440M subset, further employing clustering to remove semantic duplicates. DATACOMP facilitates data-centric investigation at an even larger scale (i.e., 12.8B sample scale) and provides a common experimental setting for fair comparison amongst dataset creation algorithms.\n\nLarge-scale multimodal datasets. Datasets have been instrumental to building multimodal models like CLIP [111], Flamingo [8], Stable Diffusion [123], DALL-E [115, 116] and GPT-4 [103]. These methods succeeded by training on large, heterogeneous datasets rather than solely through advanced modelling techniques. For example, OpenAI\u2019s CLIP trains on 400M image-text pairs from the web, roughly 300\u00d7 the size of ImageNet [37]. Prior work on scaling image-text datasets also provides promising trends with respect to zero-shot model performance [73, 107]. Additional large scale datasets like FILIP-300M [149], FLD-900M [153], and PaLI-10B [25] were constructed to train multimodal models. However, many datasets used to train such models (including the dataset for OpenAI\u2019s CLIP) are proprietary, making it hard to conduct data-centric investigations.\n\nEven for public image-text datasets like SBU [104], Flickr30k [151], MS-COCO [26], TaiSu [92], Conceptual Captions [131], CC12M [24], RedCaps [38], WIT [136], Shutterstock [101], YFCC-3"}]}, {"page": 4, "text": "  Table 2: Experimental configurations, with compute in multiply-accumulate operations (MACs).\n             Scale       Model         Train compute (MACs)       Pool size and # samples seen\n             small       ViT-B/32            9.5 \u00d7 1016                      12.8M\n             medium      ViT-B/32            9.5 \u00d7 1017                      128M\n             large       ViT-B/16            2.6 \u00d7 1019                      1.28B\n             xlarge      ViT-L/14            1.1 \u00d7 1021                      12.8B\n100M [140], COYO-700M [20], LAION-400M [128], or LAION-2B [129] little is known about what\n constitutes a good image-text dataset. Preliminary analysis suggests that different image-text data\n sources lead to CLIP models with different properties [101]. However, previous work is limited to\n smaller scale data (10-15M examples). Birhane et al. [15] examine LAION-400M and find NSFW\n imagery and racial slurs, centering the dangers in web-scale multimodal datasets. To combat toxicity,\n we preprocess our pool to remove NSFW content and blur human faces detected in images. For more\n details on our safety preprocessing see Section 3.2, Appendices E and G.\n 3   The DATACOMP benchmark\n DATACOMP is meant to facilitate data-centric experimentation. While traditional benchmarks\n emphasize model design, DATACOMP is centered around dataset development, where the resulting\n datasets can be used to train high accuracy models. We focus on large image-text datasets and\n quantify a dataset submission by training a CLIP model on it from scratch [111] and evaluating on 38\n downstream image classification and retrieval tasks. We additionally have three secret test sets, which\n will be released after a year, to guard against overfitting. To facilitate such investigations, we provide\n a candidate pool of uncurated image-text pairs sourced from the public internet. Our benchmark offers\n two tracks: one where participants must filter samples from the pools we provide, and another where\n participants can use external data. Moreover, DATACOMP is structured to accommodate participants\n with diverse levels of computational resources: each track is broken down into four scales with\n varying compute requirements. We now discuss high-level design decisions, construction of a 12.8B\n image-text data pool to facilitate the competition, benchmark tracks, model training, and evaluation.\n 3.1   Competition design\n Overview. In many areas of machine learning, larger datasets lead to better performing models [87,\n 79, 73, 107, 66, 28, 19, 111, 112]. Hence comparing only datasets with the same size is a natural\n starting point. However, this approach is flawed as controlling the dataset size ignores critical curation\n constraints: candidate pool size (i.e., number of image-text pairs to harvest) and training compute. For\n instance, assembling a dataset like LAION-2B consists of identifying data sources (e.g., Common\n Crawl or Reddit) and filtering the data source. Notably, the final dataset size is a design choice and is\n only upper-bounded by the data sources. Hence, the true data constraint is the size of the reservoir of\n samples: candidate pool to be filtered. To make DATACOMP a realistic benchmark, we therefore fix\n the candidate pool in the filtering track, but give participants control over the training set size.\n Compute cost is another relevant constraint. To put datasets of different size on equal footing, we\n specify the total number of training samples seen. Consider the 12.8B compute scale and filtered\n datasets A and B, with 6.4B and 3.2B image-text pairs respectively. At this scale, we train by making\n two passes over A, while making four passes over B. A key result from our experiments is that\n smaller, more stringently filtered datasets can lead to models that generalize better.\n Competition tracks. Two key procedures in assembling a training dataset are filtering a data source\n [128, 129, 20] and aggregating data sources [36, 37]. To reflect this structure, DATACOMP has two\n tracks: filtering, where participants select a subset of the samples from COMMONPOOL, and Bring\nYour Own Data (BYOD), where participants can use any source of data. Key decisions for each tracks\n are described in Sections 3.2 and 3.3, respectively. For full competition track rules see Appendix A.\n Competition compute scales. To facilitate study of scaling trends and accommodate participants\n with various computational resources, we structure DATACOMP using four scales of compute: small,\n medium, large and xlarge. Each new scale increases the number of samples seen during training by\n                                                     4", "md": "|Scale|Model|Train compute (MACs)|Pool size and # samples seen|\n|---|---|---|---|\n|small|ViT-B/32|9.5 \u00d7 1016|12.8M|\n|medium|ViT-B/32|9.5 \u00d7 1017|128M|\n|large|ViT-B/16|2.6 \u00d7 1019|1.28B|\n|xlarge|ViT-L/14|1.1 \u00d7 1021|12.8B|\n\n100M [140], COYO-700M [20], LAION-400M [128], or LAION-2B [129] little is known about what constitutes a good image-text dataset. Preliminary analysis suggests that different image-text data sources lead to CLIP models with different properties [101]. However, previous work is limited to smaller scale data (10-15M examples). Birhane et al. [15] examine LAION-400M and find NSFW imagery and racial slurs, centering the dangers in web-scale multimodal datasets. To combat toxicity, we preprocess our pool to remove NSFW content and blur human faces detected in images. For more details on our safety preprocessing see Section 3.2, Appendices E and G.\n\n### The DATACOMP benchmark\n\nDATACOMP is meant to facilitate data-centric experimentation. While traditional benchmarks emphasize model design, DATACOMP is centered around dataset development, where the resulting datasets can be used to train high accuracy models. We focus on large image-text datasets and quantify a dataset submission by training a CLIP model on it from scratch [111] and evaluating on 38 downstream image classification and retrieval tasks. We additionally have three secret test sets, which will be released after a year, to guard against overfitting. To facilitate such investigations, we provide a candidate pool of uncurated image-text pairs sourced from the public internet. Our benchmark offers two tracks: one where participants must filter samples from the pools we provide, and another where participants can use external data. Moreover, DATACOMP is structured to accommodate participants with diverse levels of computational resources: each track is broken down into four scales with varying compute requirements. We now discuss high-level design decisions, construction of a 12.8B image-text data pool to facilitate the competition, benchmark tracks, model training, and evaluation.\n\n### Competition design\n\nOverview. In many areas of machine learning, larger datasets lead to better performing models [87, 79, 73, 107, 66, 28, 19, 111, 112]. Hence comparing only datasets with the same size is a natural starting point. However, this approach is flawed as controlling the dataset size ignores critical curation constraints: candidate pool size (i.e., number of image-text pairs to harvest) and training compute. For instance, assembling a dataset like LAION-2B consists of identifying data sources (e.g., Common Crawl or Reddit) and filtering the data source. Notably, the final dataset size is a design choice and is only upper-bounded by the data sources. Hence, the true data constraint is the size of the reservoir of samples: candidate pool to be filtered. To make DATACOMP a realistic benchmark, we therefore fix the candidate pool in the filtering track, but give participants control over the training set size. Compute cost is another relevant constraint. To put datasets of different size on equal footing, we specify the total number of training samples seen. Consider the 12.8B compute scale and filtered datasets A and B, with 6.4B and 3.2B image-text pairs respectively. At this scale, we train by making two passes over A, while making four passes over B. A key result from our experiments is that smaller, more stringently filtered datasets can lead to models that generalize better.\n\nCompetition tracks. Two key procedures in assembling a training dataset are filtering a data source [128, 129, 20] and aggregating data sources [36, 37]. To reflect this structure, DATACOMP has two tracks: filtering, where participants select a subset of the samples from COMMONPOOL, and Bring Your Own Data (BYOD), where participants can use any source of data. Key decisions for each track are described in Sections 3.2 and 3.3, respectively. For full competition track rules see Appendix A.\n\nCompetition compute scales. To facilitate study of scaling trends and accommodate participants with various computational resources, we structure DATACOMP using four scales of compute: small, medium, large and xlarge. Each new scale increases the number of samples seen during training by 4.", "images": [], "items": [{"type": "table", "rows": [["Scale", "Model", "Train compute (MACs)", "Pool size and # samples seen"], ["small", "ViT-B/32", "9.5 \u00d7 1016", "12.8M"], ["medium", "ViT-B/32", "9.5 \u00d7 1017", "128M"], ["large", "ViT-B/16", "2.6 \u00d7 1019", "1.28B"], ["xlarge", "ViT-L/14", "1.1 \u00d7 1021", "12.8B"]], "md": "|Scale|Model|Train compute (MACs)|Pool size and # samples seen|\n|---|---|---|---|\n|small|ViT-B/32|9.5 \u00d7 1016|12.8M|\n|medium|ViT-B/32|9.5 \u00d7 1017|128M|\n|large|ViT-B/16|2.6 \u00d7 1019|1.28B|\n|xlarge|ViT-L/14|1.1 \u00d7 1021|12.8B|", "isPerfectTable": true, "csv": "\"Scale\",\"Model\",\"Train compute (MACs)\",\"Pool size and # samples seen\"\n\"small\",\"ViT-B/32\",\"9.5 \u00d7 1016\",\"12.8M\"\n\"medium\",\"ViT-B/32\",\"9.5 \u00d7 1017\",\"128M\"\n\"large\",\"ViT-B/16\",\"2.6 \u00d7 1019\",\"1.28B\"\n\"xlarge\",\"ViT-L/14\",\"1.1 \u00d7 1021\",\"12.8B\""}, {"type": "text", "value": "100M [140], COYO-700M [20], LAION-400M [128], or LAION-2B [129] little is known about what constitutes a good image-text dataset. Preliminary analysis suggests that different image-text data sources lead to CLIP models with different properties [101]. However, previous work is limited to smaller scale data (10-15M examples). Birhane et al. [15] examine LAION-400M and find NSFW imagery and racial slurs, centering the dangers in web-scale multimodal datasets. To combat toxicity, we preprocess our pool to remove NSFW content and blur human faces detected in images. For more details on our safety preprocessing see Section 3.2, Appendices E and G.", "md": "100M [140], COYO-700M [20], LAION-400M [128], or LAION-2B [129] little is known about what constitutes a good image-text dataset. Preliminary analysis suggests that different image-text data sources lead to CLIP models with different properties [101]. However, previous work is limited to smaller scale data (10-15M examples). Birhane et al. [15] examine LAION-400M and find NSFW imagery and racial slurs, centering the dangers in web-scale multimodal datasets. To combat toxicity, we preprocess our pool to remove NSFW content and blur human faces detected in images. For more details on our safety preprocessing see Section 3.2, Appendices E and G."}, {"type": "heading", "lvl": 3, "value": "The DATACOMP benchmark", "md": "### The DATACOMP benchmark"}, {"type": "text", "value": "DATACOMP is meant to facilitate data-centric experimentation. While traditional benchmarks emphasize model design, DATACOMP is centered around dataset development, where the resulting datasets can be used to train high accuracy models. We focus on large image-text datasets and quantify a dataset submission by training a CLIP model on it from scratch [111] and evaluating on 38 downstream image classification and retrieval tasks. We additionally have three secret test sets, which will be released after a year, to guard against overfitting. To facilitate such investigations, we provide a candidate pool of uncurated image-text pairs sourced from the public internet. Our benchmark offers two tracks: one where participants must filter samples from the pools we provide, and another where participants can use external data. Moreover, DATACOMP is structured to accommodate participants with diverse levels of computational resources: each track is broken down into four scales with varying compute requirements. We now discuss high-level design decisions, construction of a 12.8B image-text data pool to facilitate the competition, benchmark tracks, model training, and evaluation.", "md": "DATACOMP is meant to facilitate data-centric experimentation. While traditional benchmarks emphasize model design, DATACOMP is centered around dataset development, where the resulting datasets can be used to train high accuracy models. We focus on large image-text datasets and quantify a dataset submission by training a CLIP model on it from scratch [111] and evaluating on 38 downstream image classification and retrieval tasks. We additionally have three secret test sets, which will be released after a year, to guard against overfitting. To facilitate such investigations, we provide a candidate pool of uncurated image-text pairs sourced from the public internet. Our benchmark offers two tracks: one where participants must filter samples from the pools we provide, and another where participants can use external data. Moreover, DATACOMP is structured to accommodate participants with diverse levels of computational resources: each track is broken down into four scales with varying compute requirements. We now discuss high-level design decisions, construction of a 12.8B image-text data pool to facilitate the competition, benchmark tracks, model training, and evaluation."}, {"type": "heading", "lvl": 3, "value": "Competition design", "md": "### Competition design"}, {"type": "text", "value": "Overview. In many areas of machine learning, larger datasets lead to better performing models [87, 79, 73, 107, 66, 28, 19, 111, 112]. Hence comparing only datasets with the same size is a natural starting point. However, this approach is flawed as controlling the dataset size ignores critical curation constraints: candidate pool size (i.e., number of image-text pairs to harvest) and training compute. For instance, assembling a dataset like LAION-2B consists of identifying data sources (e.g., Common Crawl or Reddit) and filtering the data source. Notably, the final dataset size is a design choice and is only upper-bounded by the data sources. Hence, the true data constraint is the size of the reservoir of samples: candidate pool to be filtered. To make DATACOMP a realistic benchmark, we therefore fix the candidate pool in the filtering track, but give participants control over the training set size. Compute cost is another relevant constraint. To put datasets of different size on equal footing, we specify the total number of training samples seen. Consider the 12.8B compute scale and filtered datasets A and B, with 6.4B and 3.2B image-text pairs respectively. At this scale, we train by making two passes over A, while making four passes over B. A key result from our experiments is that smaller, more stringently filtered datasets can lead to models that generalize better.\n\nCompetition tracks. Two key procedures in assembling a training dataset are filtering a data source [128, 129, 20] and aggregating data sources [36, 37]. To reflect this structure, DATACOMP has two tracks: filtering, where participants select a subset of the samples from COMMONPOOL, and Bring Your Own Data (BYOD), where participants can use any source of data. Key decisions for each track are described in Sections 3.2 and 3.3, respectively. For full competition track rules see Appendix A.\n\nCompetition compute scales. To facilitate study of scaling trends and accommodate participants with various computational resources, we structure DATACOMP using four scales of compute: small, medium, large and xlarge. Each new scale increases the number of samples seen during training by 4.", "md": "Overview. In many areas of machine learning, larger datasets lead to better performing models [87, 79, 73, 107, 66, 28, 19, 111, 112]. Hence comparing only datasets with the same size is a natural starting point. However, this approach is flawed as controlling the dataset size ignores critical curation constraints: candidate pool size (i.e., number of image-text pairs to harvest) and training compute. For instance, assembling a dataset like LAION-2B consists of identifying data sources (e.g., Common Crawl or Reddit) and filtering the data source. Notably, the final dataset size is a design choice and is only upper-bounded by the data sources. Hence, the true data constraint is the size of the reservoir of samples: candidate pool to be filtered. To make DATACOMP a realistic benchmark, we therefore fix the candidate pool in the filtering track, but give participants control over the training set size. Compute cost is another relevant constraint. To put datasets of different size on equal footing, we specify the total number of training samples seen. Consider the 12.8B compute scale and filtered datasets A and B, with 6.4B and 3.2B image-text pairs respectively. At this scale, we train by making two passes over A, while making four passes over B. A key result from our experiments is that smaller, more stringently filtered datasets can lead to models that generalize better.\n\nCompetition tracks. Two key procedures in assembling a training dataset are filtering a data source [128, 129, 20] and aggregating data sources [36, 37]. To reflect this structure, DATACOMP has two tracks: filtering, where participants select a subset of the samples from COMMONPOOL, and Bring Your Own Data (BYOD), where participants can use any source of data. Key decisions for each track are described in Sections 3.2 and 3.3, respectively. For full competition track rules see Appendix A.\n\nCompetition compute scales. To facilitate study of scaling trends and accommodate participants with various computational resources, we structure DATACOMP using four scales of compute: small, medium, large and xlarge. Each new scale increases the number of samples seen during training by 4."}]}, {"page": 5, "text": "10\u00d7 (from 12.8M to 12.8B samples seen), and the pool we provide by the same factor (from 12.8M\nsamples to 12.8B samples). Table 2 gives the experimental configuration used for each scale. For the\nsmall scale, our runs took 4 hours on an A100 GPU, and for the xlarge scale 81 hours on 512 GPUs.\n3.2   COMMONPOOL generation, for the filtering track\nWe construct a large-scale pool of image-text pairs, COMMONPOOL, from Common Crawl [3].\nCommonPool is distributed as an image url-text pair index under a CC-BY-4.0 license. Our pool\nconstruction pipeline has four steps: url extraction and data download, NSFW detection, evaluation\nset deduplication, and face blurring. We additionally provide per sample metadata (e.g., CLIP\nfeatures). Starting from the xlarge COMMONPOOL, we take successive random subsets to create\nlarge, medium, and small COMMONPOOL (e.g., medium is a subset of large).\nExtracting urls and dowloading data. We first use cc2dataset [1], which utilizes Apache\nSpark [155], to extract pairs of image urls and nonempty alt-text from all Common Crawl snapshots\nfrom 2014 to 2022. We then deduplicate the url-text pairs and randomly shuffle. This step results in\n\u223c88B possible samples. Not all samples are downloadable; other samples are not suitable due to\nNSFW content or overlap with our evaluation sets. We attempt to download \u223c40B samples using\nimg2dataset [5] resulting in \u223c16.8B image-text pairs. For more details, see Appendix D.\nSafety preprocessing. Since Common Crawl is a snapshot of the internet, we require strict\npreprocessing to remove unsafe content. We use Detoxify [60] to prune samples that contain\nunsafe text (e.g., obscene, sexually explicit, or threatening language). We also discard samples with\nexplicit visual content. To do so, we train a classifier on CLIP ViT-L/14 [111] features, using the\nNSFW dataset used in LAION-5B [129]. We validate our classifier against the Google commercial\nimage safety API. See Appendix E for details. Around 19% of image-text pairs are considered NSFW,\ntaking the pool of \u223c16.8B downloads to \u223c13.6B samples.\nEvaluation set deduplication. To prevent accidental overfitting to certain test sets in our evaluation\nsuite, we perform a thorough near-duplicate removal between the candidate pool and our evaluation\nsets, using a state-of-the-art image deduplication model [150]. Appendix F contains additional details.\nThe model flags \u223c3% of the 16.8B images as near-duplicates, reducing the \u223c13.6B pool to \u223c13.1B\nsamples. From here we select a random subset to get the xlarge pool of 12.8B samples.\nFace detection & blurring. To protect the privacy of individuals, we detect and blur faces from\nimages in our pool using a face detector [53]. As observed by Yang et al. [148], obfuscating faces\nhas little impact on model performance, as we also observe in our experiments (Appendix G).\nPool metadata. To bootstrap participants we distribute metadata for each sample in COMMONPOOL\n(e.g., image url, alt-text, original image resolution, CLIP features, and CLIP similarity scores).\nFollowing Carlini et al. [22], we release SHA256 hashes for each image to guard against data\npoisoning in subsequent COMMONPOOL downloads. For additional details see Appendix H. We\nopen-source our metadata processing pipeline as dataset2metadata [4].\n3.3   The bring your own data (BYOD) track\nWhile COMMONPOOL can be used to study different filtering techniques, state-of-the-art models\noften train on data from different sources. For instance, the Flamingo model [8] uses both multimodal\nmassive web (M3W) and ALIGN datasets [73]. To facilitate non-proprietary research on curating\ndata from many sources, we instantiate a separate DATACOMP track to allow participants to combine\nmultiple data streams. For example, participants could construct a training set from CC12M [24],\nYFCC100M [140], and data sources they label themselves. In Section 4.2 and Appendix P.2 we\ndescribe our exploration using existing public, image-text datasets. These datasets are acquired from\ntheir respective sources and are not re-release as part of DATACOMP.\n3.4   Training\nWe create a common experimental setting that enables comparable experiments by fixing the training\nprocedure. We closely follow the CLIP training recipe proposed by Radford et al. [111]: training\n                                                   5", "md": "## COMMONPOOL generation, for the filtering track\n\nWe construct a large-scale pool of image-text pairs, COMMONPOOL, from Common Crawl [3]. COMMONPOOL is distributed as an image url-text pair index under a CC-BY-4.0 license. Our pool construction pipeline has four steps: url extraction and data download, NSFW detection, evaluation set deduplication, and face blurring. We additionally provide per sample metadata (e.g., CLIP features). Starting from the xlarge COMMONPOOL, we take successive random subsets to create large, medium, and small COMMONPOOL (e.g., medium is a subset of large).\n\nExtracting urls and downloading data. We first use cc2dataset [1], which utilizes Apache Spark [155], to extract pairs of image urls and nonempty alt-text from all Common Crawl snapshots from 2014 to 2022. We then deduplicate the url-text pairs and randomly shuffle. This step results in approximately 88B possible samples. Not all samples are downloadable; other samples are not suitable due to NSFW content or overlap with our evaluation sets. We attempt to download approximately 40B samples using img2dataset [5] resulting in approximately 16.8B image-text pairs. For more details, see Appendix D.\n\nSafety preprocessing. Since Common Crawl is a snapshot of the internet, we require strict preprocessing to remove unsafe content. We use Detoxify [60] to prune samples that contain unsafe text (e.g., obscene, sexually explicit, or threatening language). We also discard samples with explicit visual content. To do so, we train a classifier on CLIP ViT-L/14 [111] features, using the NSFW dataset used in LAION-5B [129]. We validate our classifier against the Google commercial image safety API. See Appendix E for details. Around 19% of image-text pairs are considered NSFW, taking the pool of approximately 16.8B downloads to approximately 13.6B samples.\n\nEvaluation set deduplication. To prevent accidental overfitting to certain test sets in our evaluation suite, we perform a thorough near-duplicate removal between the candidate pool and our evaluation sets, using a state-of-the-art image deduplication model [150]. Appendix F contains additional details. The model flags approximately 3% of the 16.8B images as near-duplicates, reducing the approximately 13.6B pool to approximately 13.1B samples. From here we select a random subset to get the xlarge pool of 12.8B samples.\n\nFace detection & blurring. To protect the privacy of individuals, we detect and blur faces from images in our pool using a face detector [53]. As observed by Yang et al. [148], obfuscating faces has little impact on model performance, as we also observe in our experiments (Appendix G).\n\nPool metadata. To bootstrap participants we distribute metadata for each sample in COMMONPOOL (e.g., image url, alt-text, original image resolution, CLIP features, and CLIP similarity scores). Following Carlini et al. [22], we release SHA256 hashes for each image to guard against data poisoning in subsequent COMMONPOOL downloads. For additional details see Appendix H. We open-source our metadata processing pipeline as dataset2metadata [4].", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "COMMONPOOL generation, for the filtering track", "md": "## COMMONPOOL generation, for the filtering track"}, {"type": "text", "value": "We construct a large-scale pool of image-text pairs, COMMONPOOL, from Common Crawl [3]. COMMONPOOL is distributed as an image url-text pair index under a CC-BY-4.0 license. Our pool construction pipeline has four steps: url extraction and data download, NSFW detection, evaluation set deduplication, and face blurring. We additionally provide per sample metadata (e.g., CLIP features). Starting from the xlarge COMMONPOOL, we take successive random subsets to create large, medium, and small COMMONPOOL (e.g., medium is a subset of large).\n\nExtracting urls and downloading data. We first use cc2dataset [1], which utilizes Apache Spark [155], to extract pairs of image urls and nonempty alt-text from all Common Crawl snapshots from 2014 to 2022. We then deduplicate the url-text pairs and randomly shuffle. This step results in approximately 88B possible samples. Not all samples are downloadable; other samples are not suitable due to NSFW content or overlap with our evaluation sets. We attempt to download approximately 40B samples using img2dataset [5] resulting in approximately 16.8B image-text pairs. For more details, see Appendix D.\n\nSafety preprocessing. Since Common Crawl is a snapshot of the internet, we require strict preprocessing to remove unsafe content. We use Detoxify [60] to prune samples that contain unsafe text (e.g., obscene, sexually explicit, or threatening language). We also discard samples with explicit visual content. To do so, we train a classifier on CLIP ViT-L/14 [111] features, using the NSFW dataset used in LAION-5B [129]. We validate our classifier against the Google commercial image safety API. See Appendix E for details. Around 19% of image-text pairs are considered NSFW, taking the pool of approximately 16.8B downloads to approximately 13.6B samples.\n\nEvaluation set deduplication. To prevent accidental overfitting to certain test sets in our evaluation suite, we perform a thorough near-duplicate removal between the candidate pool and our evaluation sets, using a state-of-the-art image deduplication model [150]. Appendix F contains additional details. The model flags approximately 3% of the 16.8B images as near-duplicates, reducing the approximately 13.6B pool to approximately 13.1B samples. From here we select a random subset to get the xlarge pool of 12.8B samples.\n\nFace detection & blurring. To protect the privacy of individuals, we detect and blur faces from images in our pool using a face detector [53]. As observed by Yang et al. [148], obfuscating faces has little impact on model performance, as we also observe in our experiments (Appendix G).\n\nPool metadata. To bootstrap participants we distribute metadata for each sample in COMMONPOOL (e.g., image url, alt-text, original image resolution, CLIP features, and CLIP similarity scores). Following Carlini et al. [22], we release SHA256 hashes for each image to guard against data poisoning in subsequent COMMONPOOL downloads. For additional details see Appendix H. We open-source our metadata processing pipeline as dataset2metadata [4].", "md": "We construct a large-scale pool of image-text pairs, COMMONPOOL, from Common Crawl [3]. COMMONPOOL is distributed as an image url-text pair index under a CC-BY-4.0 license. Our pool construction pipeline has four steps: url extraction and data download, NSFW detection, evaluation set deduplication, and face blurring. We additionally provide per sample metadata (e.g., CLIP features). Starting from the xlarge COMMONPOOL, we take successive random subsets to create large, medium, and small COMMONPOOL (e.g., medium is a subset of large).\n\nExtracting urls and downloading data. We first use cc2dataset [1], which utilizes Apache Spark [155], to extract pairs of image urls and nonempty alt-text from all Common Crawl snapshots from 2014 to 2022. We then deduplicate the url-text pairs and randomly shuffle. This step results in approximately 88B possible samples. Not all samples are downloadable; other samples are not suitable due to NSFW content or overlap with our evaluation sets. We attempt to download approximately 40B samples using img2dataset [5] resulting in approximately 16.8B image-text pairs. For more details, see Appendix D.\n\nSafety preprocessing. Since Common Crawl is a snapshot of the internet, we require strict preprocessing to remove unsafe content. We use Detoxify [60] to prune samples that contain unsafe text (e.g., obscene, sexually explicit, or threatening language). We also discard samples with explicit visual content. To do so, we train a classifier on CLIP ViT-L/14 [111] features, using the NSFW dataset used in LAION-5B [129]. We validate our classifier against the Google commercial image safety API. See Appendix E for details. Around 19% of image-text pairs are considered NSFW, taking the pool of approximately 16.8B downloads to approximately 13.6B samples.\n\nEvaluation set deduplication. To prevent accidental overfitting to certain test sets in our evaluation suite, we perform a thorough near-duplicate removal between the candidate pool and our evaluation sets, using a state-of-the-art image deduplication model [150]. Appendix F contains additional details. The model flags approximately 3% of the 16.8B images as near-duplicates, reducing the approximately 13.6B pool to approximately 13.1B samples. From here we select a random subset to get the xlarge pool of 12.8B samples.\n\nFace detection & blurring. To protect the privacy of individuals, we detect and blur faces from images in our pool using a face detector [53]. As observed by Yang et al. [148], obfuscating faces has little impact on model performance, as we also observe in our experiments (Appendix G).\n\nPool metadata. To bootstrap participants we distribute metadata for each sample in COMMONPOOL (e.g., image url, alt-text, original image resolution, CLIP features, and CLIP similarity scores). Following Carlini et al. [22], we release SHA256 hashes for each image to guard against data poisoning in subsequent COMMONPOOL downloads. For additional details see Appendix H. We open-source our metadata processing pipeline as dataset2metadata [4]."}]}, {"page": 6, "text": "models from scratch with a contrastive objective over images and captions. Given a set of image-\ncaption pairs, we train an image encoder and a text encoder such that the similarity between the\nrepresentations of images and their corresponding text is maximized relative to unaligned pairs.1\nFor each scale, we fix the model architecture and hyperparameters (see Table 2). We pick Vision\nTransformers (ViTs) [39] as the image encoder, considering the better scaling trends observed by\nRadford et al. [111] compared to ResNets [62]. Models are trained for a fixed number of steps\ndetermined by the scale (Table 2), using the OpenCLIP repository [69]. See Appendix N for details.\n3.5    Evaluation\nWe evaluate on a suite of 38 image classification and retrieval tasks. We also study two additional\nfairness tasks, detailed in Section 5 and Appendix Q. As discussed in Section 3.2, we remove test set\nimages from DATACOMP to avoid contamination. Image classification datasets range from satellite\nimagery recognition to classifying metastatic tissues. In total we have (with some overlap): 22 of the\ndatasets evaluated in Radford et al. [111], 6 ImageNet distribution shifts (i.e., ImageNet-Sketch [143],\nImageNet-V2 [121], ImageNet-A [65], ImageNet-O [65], ImageNet-R [64], and ObjectNet [13]),\n13 datasets from VTAB [156], and 3 datasets from WILDS [83, 127]. Retrieval datasets include\nFlickr30k [151], MSCOCO [26], and the WinoGAViL commonsense association task [17]. To\naggregate results over all evaluation tasks, we average the preferred metric for each task.\nDATACOMP adopts a zero-shot evaluation protocol: models are tested without training on the\nevaluation tasks. This approach is computationally efficient and measures a model\u2019s ability to perform\nwell without any additional training. We find a strong rank correlation (>0.99) between performance\nin linear probe zero-shot settings (Appendix Figure 16). Additional details are in Appendix O.\n4     Baselines\n4.1    Filtering baselines\nWe study six simple filtering methods for the filtering track; see Appendix P.1 for further details.\nNo filtering. We simply use the entire pool as the subset, without any filtering. Since each pool size\nis equal to the sample budget, training consists of one pass over the data.\nRandom subsets. To isolate the effects of increasing the compute budget from increasing the dataset\nsize, we form subsets consisting of 1%, 10%, 25%, 50% and 75% of the pool chosen at random.\nBasic fi  ltering. We consider many simple filtering operations inspired by Schuhmann et al. [128]\nand Byeon et al. [20]: filtering by language (English captions, using either fasttext [77] or cld3 [2]);\nfiltering by caption length (over two words and fi           ve characters); and filtering by image size (smaller\ndimension above 200 pixels and aspect ratio below three). We also experiment with combining\nlanguage and caption length filtering and combining language, caption length, image size fitering.\nUnless otherwise specified, \u201cbasic\u201d refers fasttext English, caption length, and image size filtering.\nCLIP score and LAION filtering. We experiment with CLIP score filtering (also employed by\nLAION), where we take only examples having cosine similarity scores between CLIP image and text\nembeddings that exceed a pre-defined threshold. We investigate a range of thresholds and two OpenAI\nCLIP models for computing the scores: the ViT-B/32 model (as in LAION) and the larger ViT-L/14.\nWe also combine CLIP score thresholds and cld3 English filtering to reproduce the LAION-2B\nfiltering scheme. Table 16 in Appendix P.1 summarizes the different CLIP score configurations.\nText-based filtering. We select examples that contain text overlapping with ImageNet class names,\nwhich serve as a proxy for relevance to downstream tasks. Specifically, we select English captions\n(according to fasttext) that contain words from ImageNet-21K or ImageNet-1K [37] class synsets.\n    1More precisely, given a batch of data {(x1, y1), ..., (xB, yB)} with images x and captions y, we train\nthe image encoder g and text encoder v with the loss \u2113             =   1  B i=1   B \u03c3ii           B i=1   B \u03c3ii\n                                                                       2           j=1 \u03c3ij + 1 2           j=1 \u03c3ji , where\n\u03c3ij = exp \u27e8g(xi), h(yj)\u27e9. We also use a learnable temperature parameter as in Radford et al. [111].\n                                                             6", "md": "Models from scratch with a contrastive objective over images and captions. Given a set of image-caption pairs, we train an image encoder and a text encoder such that the similarity between the representations of images and their corresponding text is maximized relative to unaligned pairs.1\n\nFor each scale, we fix the model architecture and hyperparameters (see Table 2). We pick Vision Transformers (ViTs) [39] as the image encoder, considering the better scaling trends observed by Radford et al. [111] compared to ResNets [62]. Models are trained for a fixed number of steps determined by the scale (Table 2), using the OpenCLIP repository [69]. See Appendix N for details.\n\n### Evaluation\n\nWe evaluate on a suite of 38 image classification and retrieval tasks. We also study two additional fairness tasks, detailed in Section 5 and Appendix Q. As discussed in Section 3.2, we remove test set images from DATACOMP to avoid contamination. Image classification datasets range from satellite imagery recognition to classifying metastatic tissues. In total we have (with some overlap): 22 of the datasets evaluated in Radford et al. [111], 6 ImageNet distribution shifts (i.e., ImageNet-Sketch [143], ImageNet-V2 [121], ImageNet-A [65], ImageNet-O [65], ImageNet-R [64], and ObjectNet [13]), 13 datasets from VTAB [156], and 3 datasets from WILDS [83, 127]. Retrieval datasets include Flickr30k [151], MSCOCO [26], and the WinoGAViL commonsense association task [17]. To aggregate results over all evaluation tasks, we average the preferred metric for each task.\n\nDATACOMP adopts a zero-shot evaluation protocol: models are tested without training on the evaluation tasks. This approach is computationally efficient and measures a model\u2019s ability to perform well without any additional training. We find a strong rank correlation (>0.99) between performance in linear probe zero-shot settings (Appendix Figure 16). Additional details are in Appendix O.\n\n### Baselines\n\n#### Filtering baselines\n\nWe study six simple filtering methods for the filtering track; see Appendix P.1 for further details.\n\n1. No filtering. We simply use the entire pool as the subset, without any filtering. Since each pool size is equal to the sample budget, training consists of one pass over the data.\n2. Random subsets. To isolate the effects of increasing the compute budget from increasing the dataset size, we form subsets consisting of 1%, 10%, 25%, 50%, and 75% of the pool chosen at random.\n3. Basic filtering. We consider many simple filtering operations inspired by Schuhmann et al. [128] and Byeon et al. [20]: filtering by language (English captions, using either fasttext [77] or cld3 [2]); filtering by caption length (over two words and five characters); and filtering by image size (smaller dimension above 200 pixels and aspect ratio below three). We also experiment with combining language and caption length filtering and combining language, caption length, image size filtering. Unless otherwise specified, \u201cbasic\u201d refers fasttext English, caption length, and image size filtering.\n4. CLIP score and LAION filtering. We experiment with CLIP score filtering (also employed by LAION), where we take only examples having cosine similarity scores between CLIP image and text embeddings that exceed a pre-defined threshold. We investigate a range of thresholds and two OpenAI CLIP models for computing the scores: the ViT-B/32 model (as in LAION) and the larger ViT-L/14. We also combine CLIP score thresholds and cld3 English filtering to reproduce the LAION-2B filtering scheme. Table 16 in Appendix P.1 summarizes the different CLIP score configurations.\n5. Text-based filtering. We select examples that contain text overlapping with ImageNet class names, which serve as a proxy for relevance to downstream tasks. Specifically, we select English captions (according to fasttext) that contain words from ImageNet-21K or ImageNet-1K [37] class synsets.\n\n1 More precisely, given a batch of data {($$x_1$$, $$y_1$$), ..., ($$x_B$$, $$y_B$$)} with images $$x$$ and captions $$y$$, we train the image encoder $$g$$ and text encoder $$v$$ with the loss $$\\ell = \\frac{1}{B} \\sum_{i=1}^{B} \\sum_{j=1}^{B} \\sigma_{ij} + \\frac{1}{2} \\sum_{j=1}^{B} \\sigma_{ji}$$, where $$\\sigma_{ij} = \\exp \\langle g(x_i), h(y_j) \\rangle$$. We also use a learnable temperature parameter as in Radford et al. [111].", "images": [], "items": [{"type": "text", "value": "Models from scratch with a contrastive objective over images and captions. Given a set of image-caption pairs, we train an image encoder and a text encoder such that the similarity between the representations of images and their corresponding text is maximized relative to unaligned pairs.1\n\nFor each scale, we fix the model architecture and hyperparameters (see Table 2). We pick Vision Transformers (ViTs) [39] as the image encoder, considering the better scaling trends observed by Radford et al. [111] compared to ResNets [62]. Models are trained for a fixed number of steps determined by the scale (Table 2), using the OpenCLIP repository [69]. See Appendix N for details.", "md": "Models from scratch with a contrastive objective over images and captions. Given a set of image-caption pairs, we train an image encoder and a text encoder such that the similarity between the representations of images and their corresponding text is maximized relative to unaligned pairs.1\n\nFor each scale, we fix the model architecture and hyperparameters (see Table 2). We pick Vision Transformers (ViTs) [39] as the image encoder, considering the better scaling trends observed by Radford et al. [111] compared to ResNets [62]. Models are trained for a fixed number of steps determined by the scale (Table 2), using the OpenCLIP repository [69]. See Appendix N for details."}, {"type": "heading", "lvl": 3, "value": "Evaluation", "md": "### Evaluation"}, {"type": "text", "value": "We evaluate on a suite of 38 image classification and retrieval tasks. We also study two additional fairness tasks, detailed in Section 5 and Appendix Q. As discussed in Section 3.2, we remove test set images from DATACOMP to avoid contamination. Image classification datasets range from satellite imagery recognition to classifying metastatic tissues. In total we have (with some overlap): 22 of the datasets evaluated in Radford et al. [111], 6 ImageNet distribution shifts (i.e., ImageNet-Sketch [143], ImageNet-V2 [121], ImageNet-A [65], ImageNet-O [65], ImageNet-R [64], and ObjectNet [13]), 13 datasets from VTAB [156], and 3 datasets from WILDS [83, 127]. Retrieval datasets include Flickr30k [151], MSCOCO [26], and the WinoGAViL commonsense association task [17]. To aggregate results over all evaluation tasks, we average the preferred metric for each task.\n\nDATACOMP adopts a zero-shot evaluation protocol: models are tested without training on the evaluation tasks. This approach is computationally efficient and measures a model\u2019s ability to perform well without any additional training. We find a strong rank correlation (>0.99) between performance in linear probe zero-shot settings (Appendix Figure 16). Additional details are in Appendix O.", "md": "We evaluate on a suite of 38 image classification and retrieval tasks. We also study two additional fairness tasks, detailed in Section 5 and Appendix Q. As discussed in Section 3.2, we remove test set images from DATACOMP to avoid contamination. Image classification datasets range from satellite imagery recognition to classifying metastatic tissues. In total we have (with some overlap): 22 of the datasets evaluated in Radford et al. [111], 6 ImageNet distribution shifts (i.e., ImageNet-Sketch [143], ImageNet-V2 [121], ImageNet-A [65], ImageNet-O [65], ImageNet-R [64], and ObjectNet [13]), 13 datasets from VTAB [156], and 3 datasets from WILDS [83, 127]. Retrieval datasets include Flickr30k [151], MSCOCO [26], and the WinoGAViL commonsense association task [17]. To aggregate results over all evaluation tasks, we average the preferred metric for each task.\n\nDATACOMP adopts a zero-shot evaluation protocol: models are tested without training on the evaluation tasks. This approach is computationally efficient and measures a model\u2019s ability to perform well without any additional training. We find a strong rank correlation (>0.99) between performance in linear probe zero-shot settings (Appendix Figure 16). Additional details are in Appendix O."}, {"type": "heading", "lvl": 3, "value": "Baselines", "md": "### Baselines"}, {"type": "heading", "lvl": 4, "value": "Filtering baselines", "md": "#### Filtering baselines"}, {"type": "text", "value": "We study six simple filtering methods for the filtering track; see Appendix P.1 for further details.\n\n1. No filtering. We simply use the entire pool as the subset, without any filtering. Since each pool size is equal to the sample budget, training consists of one pass over the data.\n2. Random subsets. To isolate the effects of increasing the compute budget from increasing the dataset size, we form subsets consisting of 1%, 10%, 25%, 50%, and 75% of the pool chosen at random.\n3. Basic filtering. We consider many simple filtering operations inspired by Schuhmann et al. [128] and Byeon et al. [20]: filtering by language (English captions, using either fasttext [77] or cld3 [2]); filtering by caption length (over two words and five characters); and filtering by image size (smaller dimension above 200 pixels and aspect ratio below three). We also experiment with combining language and caption length filtering and combining language, caption length, image size filtering. Unless otherwise specified, \u201cbasic\u201d refers fasttext English, caption length, and image size filtering.\n4. CLIP score and LAION filtering. We experiment with CLIP score filtering (also employed by LAION), where we take only examples having cosine similarity scores between CLIP image and text embeddings that exceed a pre-defined threshold. We investigate a range of thresholds and two OpenAI CLIP models for computing the scores: the ViT-B/32 model (as in LAION) and the larger ViT-L/14. We also combine CLIP score thresholds and cld3 English filtering to reproduce the LAION-2B filtering scheme. Table 16 in Appendix P.1 summarizes the different CLIP score configurations.\n5. Text-based filtering. We select examples that contain text overlapping with ImageNet class names, which serve as a proxy for relevance to downstream tasks. Specifically, we select English captions (according to fasttext) that contain words from ImageNet-21K or ImageNet-1K [37] class synsets.\n\n1 More precisely, given a batch of data {($$x_1$$, $$y_1$$), ..., ($$x_B$$, $$y_B$$)} with images $$x$$ and captions $$y$$, we train the image encoder $$g$$ and text encoder $$v$$ with the loss $$\\ell = \\frac{1}{B} \\sum_{i=1}^{B} \\sum_{j=1}^{B} \\sigma_{ij} + \\frac{1}{2} \\sum_{j=1}^{B} \\sigma_{ji}$$, where $$\\sigma_{ij} = \\exp \\langle g(x_i), h(y_j) \\rangle$$. We also use a learnable temperature parameter as in Radford et al. [111].", "md": "We study six simple filtering methods for the filtering track; see Appendix P.1 for further details.\n\n1. No filtering. We simply use the entire pool as the subset, without any filtering. Since each pool size is equal to the sample budget, training consists of one pass over the data.\n2. Random subsets. To isolate the effects of increasing the compute budget from increasing the dataset size, we form subsets consisting of 1%, 10%, 25%, 50%, and 75% of the pool chosen at random.\n3. Basic filtering. We consider many simple filtering operations inspired by Schuhmann et al. [128] and Byeon et al. [20]: filtering by language (English captions, using either fasttext [77] or cld3 [2]); filtering by caption length (over two words and five characters); and filtering by image size (smaller dimension above 200 pixels and aspect ratio below three). We also experiment with combining language and caption length filtering and combining language, caption length, image size filtering. Unless otherwise specified, \u201cbasic\u201d refers fasttext English, caption length, and image size filtering.\n4. CLIP score and LAION filtering. We experiment with CLIP score filtering (also employed by LAION), where we take only examples having cosine similarity scores between CLIP image and text embeddings that exceed a pre-defined threshold. We investigate a range of thresholds and two OpenAI CLIP models for computing the scores: the ViT-B/32 model (as in LAION) and the larger ViT-L/14. We also combine CLIP score thresholds and cld3 English filtering to reproduce the LAION-2B filtering scheme. Table 16 in Appendix P.1 summarizes the different CLIP score configurations.\n5. Text-based filtering. We select examples that contain text overlapping with ImageNet class names, which serve as a proxy for relevance to downstream tasks. Specifically, we select English captions (according to fasttext) that contain words from ImageNet-21K or ImageNet-1K [37] class synsets.\n\n1 More precisely, given a batch of data {($$x_1$$, $$y_1$$), ..., ($$x_B$$, $$y_B$$)} with images $$x$$ and captions $$y$$, we train the image encoder $$g$$ and text encoder $$v$$ with the loss $$\\ell = \\frac{1}{B} \\sum_{i=1}^{B} \\sum_{j=1}^{B} \\sigma_{ij} + \\frac{1}{2} \\sum_{j=1}^{B} \\sigma_{ji}$$, where $$\\sigma_{ij} = \\exp \\langle g(x_i), h(y_j) \\rangle$$. We also use a learnable temperature parameter as in Radford et al. [111]."}]}, {"page": 7, "text": "Table 3: Zero-shot performance for select baselines in the filtering track. On all scales, filtering\nstrategies lead to better performance than using the entire, unfiltered pool. The intersection between\nimaged-based and CLIP score strategies performs well on most tasks and scales. For all metrics,\nhigher is better (see Appendix O for details). \u2229                             denotes the intersection of filtering strategies.\n    Scale      Filtering strategy                               Dataset    Samples      ImageNet       ImageNet       VTAB      Retrieval  Average over\n                                                                  size        seen                     dist. shifts                          38 datasets\n               No filtering                                     12.8M        12.8M         0.025          0.033        0.145      0.114         0.132\n               Basic filtering                                    3M         12.8M         0.038          0.043        0.150      0.118         0.142\n               Text-based                                        3.2M        12.8M         0.046          0.052        0.169      0.125         0.157\n   small       Image-based                                        3M         12.8M         0.043          0.047        0.178      0.121         0.159\n               LAION-2B filtering                                1.3M        12.8M         0.031          0.040        0.136      0.092         0.133\n               CLIP score (L/14 30%)                             3.8M        12.8M         0.051          0.055        0.190      0.119         0.173\n               Image-based \u2229     CLIP score (L/14 30%)           1.4M        12.8M         0.039          0.045        0.162      0.094         0.144\n               No filtering                                      128M        128M          0.176          0.152        0.259      0.219         0.258\n               Basic filtering                                   30M         128M          0.226          0.193        0.284      0.251         0.285\n               Text-based                                        31M         128M          0.255          0.215        0.328      0.249         0.307\n   medium      Image-based                                       29M         128M          0.268          0.213        0.319      0.256         0.312\n               LAION-2B filtering                                13M         128M          0.230          0.198        0.307      0.233         0.292\n               CLIP score (L/14 30%)                             38M         128M          0.273          0.230        0.338      0.251         0.328\n               Image-based \u2229     CLIP score (L/14 30%)           14M         128M          0.297          0.239        0.346      0.231         0.328\n               No filtering                                     1.28B        1.28B         0.459          0.378        0.426      0.419         0.437\n               Basic filtering                                   298M        1.28B         0.516          0.423        0.446      0.480         0.458\n               Text-based                                        317M        1.28B         0.561          0.465        0.465      0.352         0.466\n    large      Image-based                                       293M        1.28B         0.572          0.454        0.483      0.479         0.476\n               LAION-2B filtering                                130M        1.28B         0.553          0.453        0.510      0.495         0.501\n               CLIP score (L/14 30%)                             384M        1.28B         0.578          0.474        0.538      0.466         0.529\n               Image-based \u2229     CLIP score (L/14 30%)           140M        1.28B         0.631          0.508        0.546      0.498         0.537\n               No filtering                                     12.8B        12.8B         0.723          0.612        0.611      0.569         0.621\n   xlarge      LAION-2B filtering                                1.3B        12.8B         0.755          0.637        0.624      0.620         0.636\n               CLIP score (L/14 30%)                             3.8B        12.8B         0.764          0.655        0.643      0.588         0.650\n               Image-based \u2229     CLIP score (L/14 30%)           1.4B        12.8B         0.792          0.679        0.652      0.608         0.663\nImage-based filtering. We select a subset of examples whose visual content overlaps with ImageNet\nclasses. After applying English language (fasttext) and caption length filtering, we cluster the\nimage embeddings extracted by the OpenAI ViT-L/14 model for each image into 100K groups using\nFaiss [75]. We then find the nearest neighbor group for every ImageNet training example, and keep\nexamples belonging to these groups. We apply this procedure using either ImageNet-21K (14M\nimages) or ImageNet-1K (1.2M images), forming two subsets.\n4.2      BYOD baselines\nWe experiment with multiple external data sources, including four moderately sized datasets (10 to\n58M samples) studied by Nguyen et al. [101]\u2014CC12M [24], YFCC15M [140, 111], RedCaps [38]\nand Shutterstock [101]\u2014and the larger LAION-2B [129]. Additional experiments, along with more\ndetails about the data sources are provided in Appendix P.2. We consider these data sources as they\nare and do not perform additional preprocessing. We also present experiments combining some of\nthe data sources (using only the external datasets, or in addition to data from our pool).\n5      Results and discussion\n5.1      Building better datasets\nMain results. Our key results are in Table 3. Most notably, the intersection between image-based\nfiltering and CLIP score filtering excels on most tasks. The exception is at the small scale and\nfor retrieval datasets.2 Furthermore, other filtering strategies like basic, CLIP score, image-based,\ntext-based filtering show better downstream performance when compared to no filtering. A much\nlarger suite of experiment results can be found in Appendix R.\nDATACOMP leads to better image-text datasets. We hope DATACOMP catalyzes the search for\nthe next generation of multimodal datasets. We contribute DATACOMP-1B, which is the output of\nthe Image-based \u2229               CLIP score (L/14 30%) baseline filter at the xlarge scale of the filtering track.\n     2Cherti et al. [28] also observe that models rank differently on classification and retrieval tasks.\n                                                                               7", "md": "|Scale|Filtering strategy|Dataset size|Samples seen|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|---|---|\n|small|No filtering|12.8M|12.8M|0.025|0.033|0.145|0.114|0.132|\n|Basic filtering| | |3M|12.8M|0.038|0.043|0.150|0.118|0.142|\n|Text-based| | |3.2M|12.8M|0.046|0.052|0.169|0.125|0.157|\n|Image-based| | |3M|12.8M|0.043|0.047|0.178|0.121|0.159|\n|LAION-2B filtering| | |1.3M|12.8M|0.031|0.040|0.136|0.092|0.133|\n|CLIP score (L/14 30%)| | |3.8M|12.8M|0.051|0.055|0.190|0.119|0.173|\n|Image-based \u2229 CLIP score (L/14 30%)| | |1.4M|12.8M|0.039|0.045|0.162|0.094|0.144|\n|medium|No filtering|128M|128M|0.176|0.152|0.259|0.219|0.258|\n|Basic filtering| | |30M|128M|0.226|0.193|0.284|0.251|0.285|\n|Text-based| | |31M|128M|0.255|0.215|0.328|0.249|0.307|\n|Image-based| | |29M|128M|0.268|0.213|0.319|0.256|0.312|\n|LAION-2B filtering| | |13M|128M|0.230|0.198|0.307|0.233|0.292|\n|CLIP score (L/14 30%)| | |38M|128M|0.273|0.230|0.338|0.251|0.328|\n|Image-based \u2229 CLIP score (L/14 30%)| | |14M|128M|0.297|0.239|0.346|0.231|0.328|\n|large|No filtering|1.28B|1.28B|0.459|0.378|0.426|0.419|0.437|\n|Basic filtering| | |298M|1.28B|0.516|0.423|0.446|0.480|0.458|\n|Text-based| | |317M|1.28B|0.561|0.465|0.465|0.352|0.466|\n|Image-based| | |293M|1.28B|0.572|0.454|0.483|0.479|0.476|\n|LAION-2B filtering| | |130M|1.28B|0.553|0.453|0.510|0.495|0.501|\n|CLIP score (L/14 30%)| | |384M|1.28B|0.578|0.474|0.538|0.466|0.529|\n|Image-based \u2229 CLIP score (L/14 30%)| | |140M|1.28B|0.631|0.508|0.546|0.498|0.537|\n|xlarge|LAION-2B filtering|1.3B|12.8B|0.755|0.637|0.624|0.620|0.636|\n|CLIP score (L/14 30%)| | |3.8B|12.8B|0.764|0.655|0.643|0.588|0.650|\n|Image-based \u2229 CLIP score (L/14 30%)| | |1.4B|12.8B|0.792|0.679|0.652|0.608|0.663|\n\nImage-based filtering. We select a subset of examples whose visual content overlaps with ImageNet classes. After applying English language (fasttext) and caption length filtering, we cluster the image embeddings extracted by the OpenAI ViT-L/14 model for each image into 100K groups using Faiss [75]. We then find the nearest neighbor group for every ImageNet training example, and keep examples belonging to these groups. We apply this procedure using either ImageNet-21K (14M images) or ImageNet-1K (1.2M images), forming two subsets.\n\n#### 4.2 BYOD baselines\n\nWe experiment with multiple external data sources, including four moderately sized datasets (10 to 58M samples) studied by Nguyen et al. [101]\u2014CC12M [24], YFCC15M [140, 111], RedCaps [38] and Shutterstock [101]\u2014and the larger LAION-2B [129]. Additional experiments, along with more details about the data sources are provided in Appendix P.2. We consider these data sources as they are and do not perform additional preprocessing. We also present experiments combining some of the data sources (using only the external datasets, or in addition to data from our pool).\n\n### 5 Results and discussion\n\n#### 5.1 Building better datasets\n\nMain results. Our key results are in Table 3. Most notably, the intersection between image-based filtering and CLIP score filtering excels on most tasks. The exception is at the small scale and for retrieval datasets.2 Furthermore, other filtering strategies like basic, CLIP score, image-based, text-based filtering show better downstream performance when compared to no filtering. A much larger suite of experiment results can be found in Appendix R.\n\nDATACOMP leads to better image-text datasets. We hope DATACOMP catalyzes the search for the next generation of multimodal datasets. We contribute DATACOMP-1B, which is the output of the Image-based \u2229 CLIP score (L/14 30%) baseline filter at the xlarge scale of the filtering track.\n\n2Cherti et al. [28] also observe that models rank differently on classification and retrieval tasks.", "images": [], "items": [{"type": "table", "rows": [["Scale", "Filtering strategy", "Dataset size", "Samples seen", "ImageNet", "ImageNet dist. shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["small", "No filtering", "12.8M", "12.8M", "0.025", "0.033", "0.145", "0.114", "0.132"], ["Basic filtering", "", "", "3M", "12.8M", "0.038", "0.043", "0.150", "0.118", "0.142"], ["Text-based", "", "", "3.2M", "12.8M", "0.046", "0.052", "0.169", "0.125", "0.157"], ["Image-based", "", "", "3M", "12.8M", "0.043", "0.047", "0.178", "0.121", "0.159"], ["LAION-2B filtering", "", "", "1.3M", "12.8M", "0.031", "0.040", "0.136", "0.092", "0.133"], ["CLIP score (L/14 30%)", "", "", "3.8M", "12.8M", "0.051", "0.055", "0.190", "0.119", "0.173"], ["Image-based \u2229 CLIP score (L/14 30%)", "", "", "1.4M", "12.8M", "0.039", "0.045", "0.162", "0.094", "0.144"], ["medium", "No filtering", "128M", "128M", "0.176", "0.152", "0.259", "0.219", "0.258"], ["Basic filtering", "", "", "30M", "128M", "0.226", "0.193", "0.284", "0.251", "0.285"], ["Text-based", "", "", "31M", "128M", "0.255", "0.215", "0.328", "0.249", "0.307"], ["Image-based", "", "", "29M", "128M", "0.268", "0.213", "0.319", "0.256", "0.312"], ["LAION-2B filtering", "", "", "13M", "128M", "0.230", "0.198", "0.307", "0.233", "0.292"], ["CLIP score (L/14 30%)", "", "", "38M", "128M", "0.273", "0.230", "0.338", "0.251", "0.328"], ["Image-based \u2229 CLIP score (L/14 30%)", "", "", "14M", "128M", "0.297", "0.239", "0.346", "0.231", "0.328"], ["large", "No filtering", "1.28B", "1.28B", "0.459", "0.378", "0.426", "0.419", "0.437"], ["Basic filtering", "", "", "298M", "1.28B", "0.516", "0.423", "0.446", "0.480", "0.458"], ["Text-based", "", "", "317M", "1.28B", "0.561", "0.465", "0.465", "0.352", "0.466"], ["Image-based", "", "", "293M", "1.28B", "0.572", "0.454", "0.483", "0.479", "0.476"], ["LAION-2B filtering", "", "", "130M", "1.28B", "0.553", "0.453", "0.510", "0.495", "0.501"], ["CLIP score (L/14 30%)", "", "", "384M", "1.28B", "0.578", "0.474", "0.538", "0.466", "0.529"], ["Image-based \u2229 CLIP score (L/14 30%)", "", "", "140M", "1.28B", "0.631", "0.508", "0.546", "0.498", "0.537"], ["xlarge", "LAION-2B filtering", "1.3B", "12.8B", "0.755", "0.637", "0.624", "0.620", "0.636"], ["CLIP score (L/14 30%)", "", "", "3.8B", "12.8B", "0.764", "0.655", "0.643", "0.588", "0.650"], ["Image-based \u2229 CLIP score (L/14 30%)", "", "", "1.4B", "12.8B", "0.792", "0.679", "0.652", "0.608", "0.663"]], "md": "|Scale|Filtering strategy|Dataset size|Samples seen|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|---|---|\n|small|No filtering|12.8M|12.8M|0.025|0.033|0.145|0.114|0.132|\n|Basic filtering| | |3M|12.8M|0.038|0.043|0.150|0.118|0.142|\n|Text-based| | |3.2M|12.8M|0.046|0.052|0.169|0.125|0.157|\n|Image-based| | |3M|12.8M|0.043|0.047|0.178|0.121|0.159|\n|LAION-2B filtering| | |1.3M|12.8M|0.031|0.040|0.136|0.092|0.133|\n|CLIP score (L/14 30%)| | |3.8M|12.8M|0.051|0.055|0.190|0.119|0.173|\n|Image-based \u2229 CLIP score (L/14 30%)| | |1.4M|12.8M|0.039|0.045|0.162|0.094|0.144|\n|medium|No filtering|128M|128M|0.176|0.152|0.259|0.219|0.258|\n|Basic filtering| | |30M|128M|0.226|0.193|0.284|0.251|0.285|\n|Text-based| | |31M|128M|0.255|0.215|0.328|0.249|0.307|\n|Image-based| | |29M|128M|0.268|0.213|0.319|0.256|0.312|\n|LAION-2B filtering| | |13M|128M|0.230|0.198|0.307|0.233|0.292|\n|CLIP score (L/14 30%)| | |38M|128M|0.273|0.230|0.338|0.251|0.328|\n|Image-based \u2229 CLIP score (L/14 30%)| | |14M|128M|0.297|0.239|0.346|0.231|0.328|\n|large|No filtering|1.28B|1.28B|0.459|0.378|0.426|0.419|0.437|\n|Basic filtering| | |298M|1.28B|0.516|0.423|0.446|0.480|0.458|\n|Text-based| | |317M|1.28B|0.561|0.465|0.465|0.352|0.466|\n|Image-based| | |293M|1.28B|0.572|0.454|0.483|0.479|0.476|\n|LAION-2B filtering| | |130M|1.28B|0.553|0.453|0.510|0.495|0.501|\n|CLIP score (L/14 30%)| | |384M|1.28B|0.578|0.474|0.538|0.466|0.529|\n|Image-based \u2229 CLIP score (L/14 30%)| | |140M|1.28B|0.631|0.508|0.546|0.498|0.537|\n|xlarge|LAION-2B filtering|1.3B|12.8B|0.755|0.637|0.624|0.620|0.636|\n|CLIP score (L/14 30%)| | |3.8B|12.8B|0.764|0.655|0.643|0.588|0.650|\n|Image-based \u2229 CLIP score (L/14 30%)| | |1.4B|12.8B|0.792|0.679|0.652|0.608|0.663|", "isPerfectTable": false, "csv": "\"Scale\",\"Filtering strategy\",\"Dataset size\",\"Samples seen\",\"ImageNet\",\"ImageNet dist. shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"small\",\"No filtering\",\"12.8M\",\"12.8M\",\"0.025\",\"0.033\",\"0.145\",\"0.114\",\"0.132\"\n\"Basic filtering\",\"\",\"\",\"3M\",\"12.8M\",\"0.038\",\"0.043\",\"0.150\",\"0.118\",\"0.142\"\n\"Text-based\",\"\",\"\",\"3.2M\",\"12.8M\",\"0.046\",\"0.052\",\"0.169\",\"0.125\",\"0.157\"\n\"Image-based\",\"\",\"\",\"3M\",\"12.8M\",\"0.043\",\"0.047\",\"0.178\",\"0.121\",\"0.159\"\n\"LAION-2B filtering\",\"\",\"\",\"1.3M\",\"12.8M\",\"0.031\",\"0.040\",\"0.136\",\"0.092\",\"0.133\"\n\"CLIP score (L/14 30%)\",\"\",\"\",\"3.8M\",\"12.8M\",\"0.051\",\"0.055\",\"0.190\",\"0.119\",\"0.173\"\n\"Image-based \u2229 CLIP score (L/14 30%)\",\"\",\"\",\"1.4M\",\"12.8M\",\"0.039\",\"0.045\",\"0.162\",\"0.094\",\"0.144\"\n\"medium\",\"No filtering\",\"128M\",\"128M\",\"0.176\",\"0.152\",\"0.259\",\"0.219\",\"0.258\"\n\"Basic filtering\",\"\",\"\",\"30M\",\"128M\",\"0.226\",\"0.193\",\"0.284\",\"0.251\",\"0.285\"\n\"Text-based\",\"\",\"\",\"31M\",\"128M\",\"0.255\",\"0.215\",\"0.328\",\"0.249\",\"0.307\"\n\"Image-based\",\"\",\"\",\"29M\",\"128M\",\"0.268\",\"0.213\",\"0.319\",\"0.256\",\"0.312\"\n\"LAION-2B filtering\",\"\",\"\",\"13M\",\"128M\",\"0.230\",\"0.198\",\"0.307\",\"0.233\",\"0.292\"\n\"CLIP score (L/14 30%)\",\"\",\"\",\"38M\",\"128M\",\"0.273\",\"0.230\",\"0.338\",\"0.251\",\"0.328\"\n\"Image-based \u2229 CLIP score (L/14 30%)\",\"\",\"\",\"14M\",\"128M\",\"0.297\",\"0.239\",\"0.346\",\"0.231\",\"0.328\"\n\"large\",\"No filtering\",\"1.28B\",\"1.28B\",\"0.459\",\"0.378\",\"0.426\",\"0.419\",\"0.437\"\n\"Basic filtering\",\"\",\"\",\"298M\",\"1.28B\",\"0.516\",\"0.423\",\"0.446\",\"0.480\",\"0.458\"\n\"Text-based\",\"\",\"\",\"317M\",\"1.28B\",\"0.561\",\"0.465\",\"0.465\",\"0.352\",\"0.466\"\n\"Image-based\",\"\",\"\",\"293M\",\"1.28B\",\"0.572\",\"0.454\",\"0.483\",\"0.479\",\"0.476\"\n\"LAION-2B filtering\",\"\",\"\",\"130M\",\"1.28B\",\"0.553\",\"0.453\",\"0.510\",\"0.495\",\"0.501\"\n\"CLIP score (L/14 30%)\",\"\",\"\",\"384M\",\"1.28B\",\"0.578\",\"0.474\",\"0.538\",\"0.466\",\"0.529\"\n\"Image-based \u2229 CLIP score (L/14 30%)\",\"\",\"\",\"140M\",\"1.28B\",\"0.631\",\"0.508\",\"0.546\",\"0.498\",\"0.537\"\n\"xlarge\",\"LAION-2B filtering\",\"1.3B\",\"12.8B\",\"0.755\",\"0.637\",\"0.624\",\"0.620\",\"0.636\"\n\"CLIP score (L/14 30%)\",\"\",\"\",\"3.8B\",\"12.8B\",\"0.764\",\"0.655\",\"0.643\",\"0.588\",\"0.650\"\n\"Image-based \u2229 CLIP score (L/14 30%)\",\"\",\"\",\"1.4B\",\"12.8B\",\"0.792\",\"0.679\",\"0.652\",\"0.608\",\"0.663\""}, {"type": "text", "value": "Image-based filtering. We select a subset of examples whose visual content overlaps with ImageNet classes. After applying English language (fasttext) and caption length filtering, we cluster the image embeddings extracted by the OpenAI ViT-L/14 model for each image into 100K groups using Faiss [75]. We then find the nearest neighbor group for every ImageNet training example, and keep examples belonging to these groups. We apply this procedure using either ImageNet-21K (14M images) or ImageNet-1K (1.2M images), forming two subsets.", "md": "Image-based filtering. We select a subset of examples whose visual content overlaps with ImageNet classes. After applying English language (fasttext) and caption length filtering, we cluster the image embeddings extracted by the OpenAI ViT-L/14 model for each image into 100K groups using Faiss [75]. We then find the nearest neighbor group for every ImageNet training example, and keep examples belonging to these groups. We apply this procedure using either ImageNet-21K (14M images) or ImageNet-1K (1.2M images), forming two subsets."}, {"type": "heading", "lvl": 4, "value": "4.2 BYOD baselines", "md": "#### 4.2 BYOD baselines"}, {"type": "text", "value": "We experiment with multiple external data sources, including four moderately sized datasets (10 to 58M samples) studied by Nguyen et al. [101]\u2014CC12M [24], YFCC15M [140, 111], RedCaps [38] and Shutterstock [101]\u2014and the larger LAION-2B [129]. Additional experiments, along with more details about the data sources are provided in Appendix P.2. We consider these data sources as they are and do not perform additional preprocessing. We also present experiments combining some of the data sources (using only the external datasets, or in addition to data from our pool).", "md": "We experiment with multiple external data sources, including four moderately sized datasets (10 to 58M samples) studied by Nguyen et al. [101]\u2014CC12M [24], YFCC15M [140, 111], RedCaps [38] and Shutterstock [101]\u2014and the larger LAION-2B [129]. Additional experiments, along with more details about the data sources are provided in Appendix P.2. We consider these data sources as they are and do not perform additional preprocessing. We also present experiments combining some of the data sources (using only the external datasets, or in addition to data from our pool)."}, {"type": "heading", "lvl": 3, "value": "5 Results and discussion", "md": "### 5 Results and discussion"}, {"type": "heading", "lvl": 4, "value": "5.1 Building better datasets", "md": "#### 5.1 Building better datasets"}, {"type": "text", "value": "Main results. Our key results are in Table 3. Most notably, the intersection between image-based filtering and CLIP score filtering excels on most tasks. The exception is at the small scale and for retrieval datasets.2 Furthermore, other filtering strategies like basic, CLIP score, image-based, text-based filtering show better downstream performance when compared to no filtering. A much larger suite of experiment results can be found in Appendix R.\n\nDATACOMP leads to better image-text datasets. We hope DATACOMP catalyzes the search for the next generation of multimodal datasets. We contribute DATACOMP-1B, which is the output of the Image-based \u2229 CLIP score (L/14 30%) baseline filter at the xlarge scale of the filtering track.\n\n2Cherti et al. [28] also observe that models rank differently on classification and retrieval tasks.", "md": "Main results. Our key results are in Table 3. Most notably, the intersection between image-based filtering and CLIP score filtering excels on most tasks. The exception is at the small scale and for retrieval datasets.2 Furthermore, other filtering strategies like basic, CLIP score, image-based, text-based filtering show better downstream performance when compared to no filtering. A much larger suite of experiment results can be found in Appendix R.\n\nDATACOMP leads to better image-text datasets. We hope DATACOMP catalyzes the search for the next generation of multimodal datasets. We contribute DATACOMP-1B, which is the output of the Image-based \u2229 CLIP score (L/14 30%) baseline filter at the xlarge scale of the filtering track.\n\n2Cherti et al. [28] also observe that models rank differently on classification and retrieval tasks."}]}, {"page": 8, "text": "                  ImageNet                     Average over 38 datasets\n    0.6                                    Average performance\n   ImageNet accuracy                        0.5                                         small scale\n    0.4                                     0.4                                         medium scale\n                                                                                        large scale\n                                            0.3                                         CLIP score (L/14)\n    0.2                                     0.2                                         CLIP score (B/32)\n                                                                                        Rand. subset\n    0.0                                     0.1\n        0.0           0.5          1.0          0.0           0.5          1.0\n              Fraction of the pool                   Fraction of the pool\n               used for training                       used for training\nFigure 2: Performance of random subsets (dotted line) and CLIP score filtering (solid line) when\nvarying the subset size. When taking random subsets, larger subsets are always better. For CLIP\nscore filtering, subsets with intermediate size perform best.\nOur dataset is comprised of 1.4B samples, which not only is smaller than the LAION-2B dataset\nwith 2.3B samples, but also comes from a smaller pool. Nevertheless, a CLIP L/14 trained on\nDATACOMP-1B outperforms the LAION-2B competitor by 6.1 percentage points on ImageNet (see\nTable 1). Moreover, training on DATACOMP-1B improves ImageNet accuracy by 3.7 percentage\npoints over OpenAI\u2019s ViT-L/14 trained with the same compute budget. Additionally, even if we\nrestrict ourselves to 400M samples, we can still find a subset of DATACOMP-1B that outperforms\nOpenAI\u2019s ViT-L/14, as seen in Table 24. These results demonstrate the impact that DATACOMP can\nmake and provide a foundation upon which participants can build.\nExternal data sources can improve performance. Appendix P.2 Table 18 shows results for several\nbaselines in the BYOD track. We find several instances where adding external data sources improves\nperformance over using just data from COMMONPOOL. For example, at the large scale, combining\nCLIP-filtered data from COMMONPOOL with external data from CC12M [24], YFCC15M [140, 111],\nRedCaps [38] and Shutterstock [101] boosts ImageNet accuracy by 4.3 percentage points. See\nAppendix P.2 for more experiments and details.\nTrade-off between data diversity and repetition. In Figure 2, we see that randomly selecting\nsubsets of the pool has little effect and degrades performance substantially when only small fractions\nare used. When filtering with CLIP scores, the optimal training set comes from selecting \u223c30% of\nthe pool with the highest scores. The difference in performance trends between random subsets and\nCLIP score filtering highlights the importance of filtering strategies for selecting samples.\n5.2   DATACOMP design analyses\nCOMMONPOOL and LAION are comparable with the same filtering. To validate our pool\nconstruction, we show that we can build datasets comparable to LAION-2B by employing their\nfiltering technique on our pool. LAION-2B selects all samples where the caption is in English and the\ncosine similarity score from a trained ViT-B/32 CLIP model is above 0.28. We compare this filtering\napproach on our pool using the same number samples, 130M samples at the large scale. We find that\nthe different data sources perform comparably: 55.3% vs 55.7% accuracy on ImageNet, and 0.501 vs\n0.489 average performance over our evaluation sets using our pool and LAION-2B, respectively.\nConsistency across scales. We find that the ranking between filtering strategies is typically consistent\nacross different scales. This is illustrated in Figure 3, which shows that the baselines at small\nand medium scales are positively correlated. Moreover, as shown in Appendix Table 22, the rank\ncorrelations of performance is high, between 0.71 and 0.90 for different scale pairs.\nConsistency across training changes. DATACOMP fixes the training procedure, so a natural question\nis whether better datasets from DATACOMP are better outside of DATACOMP. While DATACOMP-1B\nis trained at the xlarge scale, we show in Appendix Table 23 that even when substituting the ViT-L/14\n                                                     8", "md": "# DATACOMP Analysis\n\n## ImageNet Average over 38 datasets\n\n$$\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Average performance} & \\\\\n\\hline\n\\text{ImageNet accuracy} & 0.5 & \\text{small scale} \\\\\n0.4 & 0.4 & \\text{medium scale} \\\\\n& & \\text{large scale} \\\\\n0.3 & \\text{CLIP score (L/14)} & \\\\\n0.2 & 0.2 & \\text{CLIP score (B/32)} \\\\\n& & \\text{Rand. subset} \\\\\n0.0 & 0.1 & \\\\\n\\hline\n\\end{array}$$\n\n### Figure 2: Performance of random subsets and CLIP score filtering\n\nWhen varying the subset size, larger subsets are always better for random subsets. For CLIP score filtering, subsets with intermediate size perform best.\n\nOur dataset is comprised of 1.4B samples, which is smaller than the LAION-2B dataset with 2.3B samples. However, a CLIP L/14 trained on DATACOMP-1B outperforms the LAION-2B competitor by 6.1 percentage points on ImageNet. Training on DATACOMP-1B also improves ImageNet accuracy by 3.7 percentage points over OpenAI\u2019s ViT-L/14 trained with the same compute budget. Even with 400M samples, a subset of DATACOMP-1B outperforms OpenAI\u2019s ViT-L/14.\n\n### External data sources impact\n\nAdding external data sources improves performance. Combining CLIP-filtered data from COMMONPOOL with external data from CC12M, YFCC15M, RedCaps, and Shutterstock boosts ImageNet accuracy by 4.3 percentage points.\n\n### Trade-off between data diversity and repetition\n\nRandomly selecting subsets has little effect and degrades performance with small fractions. Filtering with CLIP scores shows that selecting around 30% of the pool with the highest scores is optimal.\n\n### DATACOMP design analyses\n\nCOMMONPOOL and LAION are comparable with the same filtering technique. The filtering approach on our pool yields comparable results to LAION-2B. Consistency across scales and training changes is also observed.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "DATACOMP Analysis", "md": "# DATACOMP Analysis"}, {"type": "heading", "lvl": 2, "value": "ImageNet Average over 38 datasets", "md": "## ImageNet Average over 38 datasets"}, {"type": "text", "value": "$$\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Average performance} & \\\\\n\\hline\n\\text{ImageNet accuracy} & 0.5 & \\text{small scale} \\\\\n0.4 & 0.4 & \\text{medium scale} \\\\\n& & \\text{large scale} \\\\\n0.3 & \\text{CLIP score (L/14)} & \\\\\n0.2 & 0.2 & \\text{CLIP score (B/32)} \\\\\n& & \\text{Rand. subset} \\\\\n0.0 & 0.1 & \\\\\n\\hline\n\\end{array}$$", "md": "$$\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Average performance} & \\\\\n\\hline\n\\text{ImageNet accuracy} & 0.5 & \\text{small scale} \\\\\n0.4 & 0.4 & \\text{medium scale} \\\\\n& & \\text{large scale} \\\\\n0.3 & \\text{CLIP score (L/14)} & \\\\\n0.2 & 0.2 & \\text{CLIP score (B/32)} \\\\\n& & \\text{Rand. subset} \\\\\n0.0 & 0.1 & \\\\\n\\hline\n\\end{array}$$"}, {"type": "heading", "lvl": 3, "value": "Figure 2: Performance of random subsets and CLIP score filtering", "md": "### Figure 2: Performance of random subsets and CLIP score filtering"}, {"type": "text", "value": "When varying the subset size, larger subsets are always better for random subsets. For CLIP score filtering, subsets with intermediate size perform best.\n\nOur dataset is comprised of 1.4B samples, which is smaller than the LAION-2B dataset with 2.3B samples. However, a CLIP L/14 trained on DATACOMP-1B outperforms the LAION-2B competitor by 6.1 percentage points on ImageNet. Training on DATACOMP-1B also improves ImageNet accuracy by 3.7 percentage points over OpenAI\u2019s ViT-L/14 trained with the same compute budget. Even with 400M samples, a subset of DATACOMP-1B outperforms OpenAI\u2019s ViT-L/14.", "md": "When varying the subset size, larger subsets are always better for random subsets. For CLIP score filtering, subsets with intermediate size perform best.\n\nOur dataset is comprised of 1.4B samples, which is smaller than the LAION-2B dataset with 2.3B samples. However, a CLIP L/14 trained on DATACOMP-1B outperforms the LAION-2B competitor by 6.1 percentage points on ImageNet. Training on DATACOMP-1B also improves ImageNet accuracy by 3.7 percentage points over OpenAI\u2019s ViT-L/14 trained with the same compute budget. Even with 400M samples, a subset of DATACOMP-1B outperforms OpenAI\u2019s ViT-L/14."}, {"type": "heading", "lvl": 3, "value": "External data sources impact", "md": "### External data sources impact"}, {"type": "text", "value": "Adding external data sources improves performance. Combining CLIP-filtered data from COMMONPOOL with external data from CC12M, YFCC15M, RedCaps, and Shutterstock boosts ImageNet accuracy by 4.3 percentage points.", "md": "Adding external data sources improves performance. Combining CLIP-filtered data from COMMONPOOL with external data from CC12M, YFCC15M, RedCaps, and Shutterstock boosts ImageNet accuracy by 4.3 percentage points."}, {"type": "heading", "lvl": 3, "value": "Trade-off between data diversity and repetition", "md": "### Trade-off between data diversity and repetition"}, {"type": "text", "value": "Randomly selecting subsets has little effect and degrades performance with small fractions. Filtering with CLIP scores shows that selecting around 30% of the pool with the highest scores is optimal.", "md": "Randomly selecting subsets has little effect and degrades performance with small fractions. Filtering with CLIP scores shows that selecting around 30% of the pool with the highest scores is optimal."}, {"type": "heading", "lvl": 3, "value": "DATACOMP design analyses", "md": "### DATACOMP design analyses"}, {"type": "text", "value": "COMMONPOOL and LAION are comparable with the same filtering technique. The filtering approach on our pool yields comparable results to LAION-2B. Consistency across scales and training changes is also observed.", "md": "COMMONPOOL and LAION are comparable with the same filtering technique. The filtering approach on our pool yields comparable results to LAION-2B. Consistency across scales and training changes is also observed."}]}, {"page": 9, "text": "                    ImageNet                            Average over 38 datasets\n                                                Avg. performance (medium)\n     0.3                                          0.35\n    ImageNet acc. (medium)\n                                                  0.30                                                Basic\n     0.2                                          0.25                                                CLIP score\n                                                                                                      Image-based\n                                                  0.20                                                No filtering\n     0.1                                                                                              Rand. subset\n                                                  0.15                                                Text-based\n     0.0                                          0.10\n        0.00       0.02       0.04                   0.075 0.100 0.125 0.150 0.175\n               ImageNet acc. (small)                       Avg. performance (small)\n Figure 3: Correlation between small and medium scale baselines. Smaller scales can serve as useful\n guides for larger scales. Results for additional scales are shown in Appendix Figure 22.\n for a ViT-B/16 or ViT-B/32, training on DATACOMP-1B outperforms training on OpenAI\u2019s WIT\n and LAION-2B. Additionally, we found that modifying hyperparameters such as training steps and\n batch size minimally affects the relative ordering of different data curation methods on downstream\n performance. Details on hyperparameter ablations are in Appendix L.\n 5.3   Evaluation trends\n ImageNet accuracy is indicative, but not the complete picture. Similarly to Kornblith et al. [84],\n in Appendix Figure 25 we find that ImageNet performance is highly correlated with the average\n performance across all datasets we study, with an overall correlation of 0.99. 3 However, ImageNet\n performance is not representative of all evaluation tasks, as the correlation between ImageNet\n accuracy and accuracy on other individual datasets varies substantially, in some cases even exhibiting\n a negative correlation, as discussed in Appendix R.\n Robustness and fairness. While typical models trained on a target task suffer large performance\n drops under data distribution shift, zero-shot CLIP models are known to exhibit strong performance\n across many distributions [111]. In Appendix Figure 26, we show that CLIP models trained with\n data from our pool are more robust to distribution shift than ImageNet-trained models from Taori\n et al. [139]\u2019s testbed. Examining geographic diversity, we find that our models are better than\n ImageNet-trained models, but fall short of models fine-tuned on diverse curated datasets (see\n Appendix Figure 21). We also perform a face classification analysis and identify demographic biases\n in our models: notably, the BYOD datasets we consider can increase the risk of misclassification.\n See Appendix Q for more fairness and diversity analyses.\n 6    Limitations and conclusion\n In terms of societal risks, creating an index of image-text pairs from the public internet can be\n problematic. The internet contains unsafe, toxic, and sensitive content, which ideally should not\n percolate into machine learning datasets. Though we take steps to remove NSFW content and blur\n human faces to protect privacy, we hope future work will further explore the biases and risks from\n COMMONPOOL and DATACOMP-1B. We see several additional directions for future work, including\n1) Curating more data sources. 2) Improved data filtering algorithms. 3) Further supervision signals\n (e.g., image captions coming from captioning models). 4) Additional input modalities (e.g., video,\n 3D objects). 5) Broader evaluations for vision-and-language and robotics tasks.\n Overall, we see DATACOMP as a first step towards improving training datasets, and hope our new\n benchmark will foster further research. By providing a controlled experimental setting, DATACOMP\n enables researchers to iterate on dataset design on rigorous empirical foundations. We open-source\n all of our code, data, and infrastructure, and hope these resources will help the community build the\n next generation of multimodal datasets.\n    3Note that unlike Kornblith et al. [84] we evaluate zero-shot performance rather than transfer learning.\n                                                           9", "md": "# Evaluation Trends\n\n## ImageNet vs Average over 38 datasets\n\n| |ImageNet|Average over 38 datasets|\n|---|---|---|\n|0.3| |0.35|\n|ImageNet acc. (medium)|0.30| |\n|0.2| |0.25|\n| | |Basic|CLIP score|\n|0.1| | |\n|0.0| |0.10|\n| |0.00|0.02|0.04|0.075|0.100|0.125|0.150|0.175|\n|ImageNet acc. (small)| |Average performance (small)|\n\nFigure 3: Correlation between small and medium scale baselines. Smaller scales can serve as useful guides for larger scales. Results for additional scales are shown in Appendix Figure 22.\n\nFor a ViT-B/16 or ViT-B/32, training on DATACOMP-1B outperforms training on OpenAI\u2019s WIT and LAION-2B. Additionally, we found that modifying hyperparameters such as training steps and batch size minimally affects the relative ordering of different data curation methods on downstream performance. Details on hyperparameter ablations are in Appendix L.\n\n### Evaluation Trends\n\nImageNet accuracy is indicative, but not the complete picture. Similarly to Kornblith et al. [84], in Appendix Figure 25 we find that ImageNet performance is highly correlated with the average performance across all datasets we study, with an overall correlation of 0.99. However, ImageNet performance is not representative of all evaluation tasks, as the correlation between ImageNet accuracy and accuracy on other individual datasets varies substantially, in some cases even exhibiting a negative correlation, as discussed in Appendix R.\n\nRobustness and fairness. While typical models trained on a target task suffer large performance drops under data distribution shift, zero-shot CLIP models are known to exhibit strong performance across many distributions. In Appendix Figure 26, we show that CLIP models trained with data from our pool are more robust to distribution shift than ImageNet-trained models from Taori et al.\u2019s testbed. Examining geographic diversity, we find that our models are better than ImageNet-trained models, but fall short of models fine-tuned on diverse curated datasets (see Appendix Figure 21). We also perform a face classification analysis and identify demographic biases in our models: notably, the BYOD datasets we consider can increase the risk of misclassification. See Appendix Q for more fairness and diversity analyses.\n\n### Limitations and Conclusion\n\nIn terms of societal risks, creating an index of image-text pairs from the public internet can be problematic. The internet contains unsafe, toxic, and sensitive content, which ideally should not percolate into machine learning datasets. Though we take steps to remove NSFW content and blur human faces to protect privacy, we hope future work will further explore the biases and risks from COMMONPOOL and DATACOMP-1B. We see several additional directions for future work, including 1) Curating more data sources. 2) Improved data filtering algorithms. 3) Further supervision signals (e.g., image captions coming from captioning models). 4) Additional input modalities (e.g., video, 3D objects). 5) Broader evaluations for vision-and-language and robotics tasks.\n\nOverall, we see DATACOMP as a first step towards improving training datasets, and hope our new benchmark will foster further research. By providing a controlled experimental setting, DATACOMP enables researchers to iterate on dataset design on rigorous empirical foundations. We open-source all of our code, data, and infrastructure, and hope these resources will help the community build the next generation of multimodal datasets.\n\nNote that unlike Kornblith et al. [84] we evaluate zero-shot performance rather than transfer learning.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Evaluation Trends", "md": "# Evaluation Trends"}, {"type": "heading", "lvl": 2, "value": "ImageNet vs Average over 38 datasets", "md": "## ImageNet vs Average over 38 datasets"}, {"type": "table", "rows": [["", "ImageNet", "Average over 38 datasets"], ["0.3", "", "0.35"], ["ImageNet acc. (medium)", "0.30", ""], ["0.2", "", "0.25"], ["", "", "Basic", "CLIP score"], ["0.1", "", ""], ["0.0", "", "0.10"], ["", "0.00", "0.02", "0.04", "0.075", "0.100", "0.125", "0.150", "0.175"], ["ImageNet acc. (small)", "", "Average performance (small)"]], "md": "| |ImageNet|Average over 38 datasets|\n|---|---|---|\n|0.3| |0.35|\n|ImageNet acc. (medium)|0.30| |\n|0.2| |0.25|\n| | |Basic|CLIP score|\n|0.1| | |\n|0.0| |0.10|\n| |0.00|0.02|0.04|0.075|0.100|0.125|0.150|0.175|\n|ImageNet acc. (small)| |Average performance (small)|", "isPerfectTable": false, "csv": "\"\",\"ImageNet\",\"Average over 38 datasets\"\n\"0.3\",\"\",\"0.35\"\n\"ImageNet acc. (medium)\",\"0.30\",\"\"\n\"0.2\",\"\",\"0.25\"\n\"\",\"\",\"Basic\",\"CLIP score\"\n\"0.1\",\"\",\"\"\n\"0.0\",\"\",\"0.10\"\n\"\",\"0.00\",\"0.02\",\"0.04\",\"0.075\",\"0.100\",\"0.125\",\"0.150\",\"0.175\"\n\"ImageNet acc. (small)\",\"\",\"Average performance (small)\""}, {"type": "text", "value": "Figure 3: Correlation between small and medium scale baselines. Smaller scales can serve as useful guides for larger scales. Results for additional scales are shown in Appendix Figure 22.\n\nFor a ViT-B/16 or ViT-B/32, training on DATACOMP-1B outperforms training on OpenAI\u2019s WIT and LAION-2B. Additionally, we found that modifying hyperparameters such as training steps and batch size minimally affects the relative ordering of different data curation methods on downstream performance. Details on hyperparameter ablations are in Appendix L.", "md": "Figure 3: Correlation between small and medium scale baselines. Smaller scales can serve as useful guides for larger scales. Results for additional scales are shown in Appendix Figure 22.\n\nFor a ViT-B/16 or ViT-B/32, training on DATACOMP-1B outperforms training on OpenAI\u2019s WIT and LAION-2B. Additionally, we found that modifying hyperparameters such as training steps and batch size minimally affects the relative ordering of different data curation methods on downstream performance. Details on hyperparameter ablations are in Appendix L."}, {"type": "heading", "lvl": 3, "value": "Evaluation Trends", "md": "### Evaluation Trends"}, {"type": "text", "value": "ImageNet accuracy is indicative, but not the complete picture. Similarly to Kornblith et al. [84], in Appendix Figure 25 we find that ImageNet performance is highly correlated with the average performance across all datasets we study, with an overall correlation of 0.99. However, ImageNet performance is not representative of all evaluation tasks, as the correlation between ImageNet accuracy and accuracy on other individual datasets varies substantially, in some cases even exhibiting a negative correlation, as discussed in Appendix R.\n\nRobustness and fairness. While typical models trained on a target task suffer large performance drops under data distribution shift, zero-shot CLIP models are known to exhibit strong performance across many distributions. In Appendix Figure 26, we show that CLIP models trained with data from our pool are more robust to distribution shift than ImageNet-trained models from Taori et al.\u2019s testbed. Examining geographic diversity, we find that our models are better than ImageNet-trained models, but fall short of models fine-tuned on diverse curated datasets (see Appendix Figure 21). We also perform a face classification analysis and identify demographic biases in our models: notably, the BYOD datasets we consider can increase the risk of misclassification. See Appendix Q for more fairness and diversity analyses.", "md": "ImageNet accuracy is indicative, but not the complete picture. Similarly to Kornblith et al. [84], in Appendix Figure 25 we find that ImageNet performance is highly correlated with the average performance across all datasets we study, with an overall correlation of 0.99. However, ImageNet performance is not representative of all evaluation tasks, as the correlation between ImageNet accuracy and accuracy on other individual datasets varies substantially, in some cases even exhibiting a negative correlation, as discussed in Appendix R.\n\nRobustness and fairness. While typical models trained on a target task suffer large performance drops under data distribution shift, zero-shot CLIP models are known to exhibit strong performance across many distributions. In Appendix Figure 26, we show that CLIP models trained with data from our pool are more robust to distribution shift than ImageNet-trained models from Taori et al.\u2019s testbed. Examining geographic diversity, we find that our models are better than ImageNet-trained models, but fall short of models fine-tuned on diverse curated datasets (see Appendix Figure 21). We also perform a face classification analysis and identify demographic biases in our models: notably, the BYOD datasets we consider can increase the risk of misclassification. See Appendix Q for more fairness and diversity analyses."}, {"type": "heading", "lvl": 3, "value": "Limitations and Conclusion", "md": "### Limitations and Conclusion"}, {"type": "text", "value": "In terms of societal risks, creating an index of image-text pairs from the public internet can be problematic. The internet contains unsafe, toxic, and sensitive content, which ideally should not percolate into machine learning datasets. Though we take steps to remove NSFW content and blur human faces to protect privacy, we hope future work will further explore the biases and risks from COMMONPOOL and DATACOMP-1B. We see several additional directions for future work, including 1) Curating more data sources. 2) Improved data filtering algorithms. 3) Further supervision signals (e.g., image captions coming from captioning models). 4) Additional input modalities (e.g., video, 3D objects). 5) Broader evaluations for vision-and-language and robotics tasks.\n\nOverall, we see DATACOMP as a first step towards improving training datasets, and hope our new benchmark will foster further research. By providing a controlled experimental setting, DATACOMP enables researchers to iterate on dataset design on rigorous empirical foundations. We open-source all of our code, data, and infrastructure, and hope these resources will help the community build the next generation of multimodal datasets.\n\nNote that unlike Kornblith et al. [84] we evaluate zero-shot performance rather than transfer learning.", "md": "In terms of societal risks, creating an index of image-text pairs from the public internet can be problematic. The internet contains unsafe, toxic, and sensitive content, which ideally should not percolate into machine learning datasets. Though we take steps to remove NSFW content and blur human faces to protect privacy, we hope future work will further explore the biases and risks from COMMONPOOL and DATACOMP-1B. We see several additional directions for future work, including 1) Curating more data sources. 2) Improved data filtering algorithms. 3) Further supervision signals (e.g., image captions coming from captioning models). 4) Additional input modalities (e.g., video, 3D objects). 5) Broader evaluations for vision-and-language and robotics tasks.\n\nOverall, we see DATACOMP as a first step towards improving training datasets, and hope our new benchmark will foster further research. By providing a controlled experimental setting, DATACOMP enables researchers to iterate on dataset design on rigorous empirical foundations. We open-source all of our code, data, and infrastructure, and hope these resources will help the community build the next generation of multimodal datasets.\n\nNote that unlike Kornblith et al. [84] we evaluate zero-shot performance rather than transfer learning."}]}, {"page": 10, "text": "Acknowledgements\nSYG and JH are supported by NSF Graduate Research Fellowships. GS is supported by the Onassis\nFoundation - Scholarship ID: F ZS 056-1/2022-2023. GD has been supported by the Onassis\nFellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis\nFellowship. This research has been supported by NSF Grants AF 1901292, CNS 2148141, DMS\n2134012, TRIPODS II-DMS 2023166, Tripods CCF 1934932, IFML CCF 2019844 and research\ngifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco, the Len\nBlavatnik and the Blavatnik Family Foundation, the Stanly P. Finch Centennial Professorship in\nEngineering, Open Philanthropy, Google, Microsoft, and the Allen Institute for AI.\nWe would like to thank Amro Abbas, Danny Bickson, Alper Canberk, Jessie Chapman, Brian\nCheung, Tim Dettmers, Joshua Gardner, Nancy Garland, Sachin Goyal, Huy Ha, Zaid Harchaoui,\nAri Holtzman, Andrew Hundt, Andy Jones, Adam Klivans, Ronak Mehta, Sachit Menon, Ari\nMorcos, Raviteja Mullapudi, Jonathon Shlens, Brandon McKinzie, Alexander Toshev, David Grangier,\nNavdeep Jaitly, Kentrell Owens, Marco Tulio Ribeiro, Shiori Sagawa, Christoph Schuhmann, Matthew\nWallingford, and Ross Wightman for helpful feedback at various stages of the project. We are\nparticularly grateful to Daniel Levy and Alec Radford for early encouragement to pursue this project\nand feedback on the experimental design.\nWe thank Stability AI and the Gauss Centre for Supercomputing e.V.4 for providing us with compute\nresources to train models. We are thankful for the compute time provided through the John von\nNeumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster [78] at J\u00fclich\nSupercomputing Centre (JSC), and for storage resources on JUST [50] granted and operated by JSC,\nas well as computing and storage resources from the Helmholtz Data Federation (HDF).\n   4https://gauss-centre.eu\n                                                 10", "md": "# Acknowledgements\n\n## Acknowledgements\n\nSYG and JH are supported by NSF Graduate Research Fellowships. GS is supported by the Onassis Foundation - Scholarship ID: F ZS 056-1/2022-2023. GD has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. This research has been supported by NSF Grants AF 1901292, CNS 2148141, DMS 2134012, TRIPODS II-DMS 2023166, Tripods CCF 1934932, IFML CCF 2019844 and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco, the Len Blavatnik and the Blavatnik Family Foundation, the Stanly P. Finch Centennial Professorship in Engineering, Open Philanthropy, Google, Microsoft, and the Allen Institute for AI.\n\nWe would like to thank Amro Abbas, Danny Bickson, Alper Canberk, Jessie Chapman, Brian Cheung, Tim Dettmers, Joshua Gardner, Nancy Garland, Sachin Goyal, Huy Ha, Zaid Harchaoui, Ari Holtzman, Andrew Hundt, Andy Jones, Adam Klivans, Ronak Mehta, Sachit Menon, Ari Morcos, Raviteja Mullapudi, Jonathon Shlens, Brandon McKinzie, Alexander Toshev, David Grangier, Navdeep Jaitly, Kentrell Owens, Marco Tulio Ribeiro, Shiori Sagawa, Christoph Schuhmann, Matthew Wallingford, and Ross Wightman for helpful feedback at various stages of the project. We are particularly grateful to Daniel Levy and Alec Radford for early encouragement to pursue this project and feedback on the experimental design.\n\nWe thank Stability AI and the Gauss Centre for Supercomputing e.V. for providing us with compute resources to train models. We are thankful for the compute time provided through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\u00fclich Supercomputing Centre (JSC), and for storage resources on JUST granted and operated by JSC, as well as computing and storage resources from the Helmholtz Data Federation (HDF).\n\nGauss Centre for Supercomputing e.V.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgements", "md": "# Acknowledgements"}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "SYG and JH are supported by NSF Graduate Research Fellowships. GS is supported by the Onassis Foundation - Scholarship ID: F ZS 056-1/2022-2023. GD has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. This research has been supported by NSF Grants AF 1901292, CNS 2148141, DMS 2134012, TRIPODS II-DMS 2023166, Tripods CCF 1934932, IFML CCF 2019844 and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco, the Len Blavatnik and the Blavatnik Family Foundation, the Stanly P. Finch Centennial Professorship in Engineering, Open Philanthropy, Google, Microsoft, and the Allen Institute for AI.\n\nWe would like to thank Amro Abbas, Danny Bickson, Alper Canberk, Jessie Chapman, Brian Cheung, Tim Dettmers, Joshua Gardner, Nancy Garland, Sachin Goyal, Huy Ha, Zaid Harchaoui, Ari Holtzman, Andrew Hundt, Andy Jones, Adam Klivans, Ronak Mehta, Sachit Menon, Ari Morcos, Raviteja Mullapudi, Jonathon Shlens, Brandon McKinzie, Alexander Toshev, David Grangier, Navdeep Jaitly, Kentrell Owens, Marco Tulio Ribeiro, Shiori Sagawa, Christoph Schuhmann, Matthew Wallingford, and Ross Wightman for helpful feedback at various stages of the project. We are particularly grateful to Daniel Levy and Alec Radford for early encouragement to pursue this project and feedback on the experimental design.\n\nWe thank Stability AI and the Gauss Centre for Supercomputing e.V. for providing us with compute resources to train models. We are thankful for the compute time provided through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\u00fclich Supercomputing Centre (JSC), and for storage resources on JUST granted and operated by JSC, as well as computing and storage resources from the Helmholtz Data Federation (HDF).\n\nGauss Centre for Supercomputing e.V.", "md": "SYG and JH are supported by NSF Graduate Research Fellowships. GS is supported by the Onassis Foundation - Scholarship ID: F ZS 056-1/2022-2023. GD has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. This research has been supported by NSF Grants AF 1901292, CNS 2148141, DMS 2134012, TRIPODS II-DMS 2023166, Tripods CCF 1934932, IFML CCF 2019844 and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco, the Len Blavatnik and the Blavatnik Family Foundation, the Stanly P. Finch Centennial Professorship in Engineering, Open Philanthropy, Google, Microsoft, and the Allen Institute for AI.\n\nWe would like to thank Amro Abbas, Danny Bickson, Alper Canberk, Jessie Chapman, Brian Cheung, Tim Dettmers, Joshua Gardner, Nancy Garland, Sachin Goyal, Huy Ha, Zaid Harchaoui, Ari Holtzman, Andrew Hundt, Andy Jones, Adam Klivans, Ronak Mehta, Sachit Menon, Ari Morcos, Raviteja Mullapudi, Jonathon Shlens, Brandon McKinzie, Alexander Toshev, David Grangier, Navdeep Jaitly, Kentrell Owens, Marco Tulio Ribeiro, Shiori Sagawa, Christoph Schuhmann, Matthew Wallingford, and Ross Wightman for helpful feedback at various stages of the project. We are particularly grateful to Daniel Levy and Alec Radford for early encouragement to pursue this project and feedback on the experimental design.\n\nWe thank Stability AI and the Gauss Centre for Supercomputing e.V. for providing us with compute resources to train models. We are thankful for the compute time provided through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\u00fclich Supercomputing Centre (JSC), and for storage resources on JUST granted and operated by JSC, as well as computing and storage resources from the Helmholtz Data Federation (HDF).\n\nGauss Centre for Supercomputing e.V."}]}, {"page": 11, "text": "References\n   [1] cc2dataset. https://github.com/rom1504/cc2dataset.\n   [2] CLD3. https://github.com/google/cld3.\n   [3] Common Crawl. https://commoncrawl.org.\n   [4] dataset2metadata. https://github.com/mlfoundations/dataset2metadata.\n   [5] img2dataset. https://github.com/rom1504/img2dataset.\n   [6] Amro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup:\n       Data-efficient learning at web-scale through semantic deduplication, 2023. https://arxiv.\n       org/abs/2303.09540.\n   [7] Pankaj K. Agarwal, Sariel Har-Peled, and Kasturi R. Varadarajan. Approximating extent\n       measures of points. Journal of the ACM (JACM), 2004. https://doi.org/10.1145/\n       1008731.1008736.\n   [8] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\n       Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\n       language model for few-shot learning. In Advances in Neural Information Processing Systems\n       (NeurIPS), 2022. https://openreview.net/forum?id=EbMuimAbPbs.\n   [9] Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. Learning from rules\n       generalizing labeled exemplars. In International Conference on Learning Representations\n       (ICLR), 2020. https://openreview.net/forum?id=SkeuexBtDr.\n  [10] Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia,\n       Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher\n       R\u00e9, and Rob Malkin.      Snorkel drybell: A case study in deploying weak supervision at\n       industrial scale. In Special Interest Group on Management of Data (SIGMOD), 2019. https:\n       //arxiv.org/abs/1812.00417.\n  [11] Olivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation\n       - the case of dp-means. In International Conference on Machine Learning (ICML), 2015.\n       https://proceedings.mlr.press/v37/bachem15.html.\n  [12] Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol,\n       Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao\n       Zhong, et al. From detection of individual metastases to classification of lymph node status at\n       the patient level: the camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018.\n       https://pubmed.ncbi.nlm.nih.gov/30716025/.\n  [13] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund,\n       Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing\n       the limits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer,\n       F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing\n       Systems (NeurIPS), volume 32. Curran Associates, Inc., 2019. https://proceedings.\n       neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf.\n  [14] Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset, 2020.\n       https://arxiv.org/abs/2004.10340.\n  [15] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe.                 Multimodal datasets:\n       misogyny, pornography, and malignant stereotypes, 2021. https://arxiv.org/abs/2110.\n       01963.\n  [16] Vighnesh Birodkar, Hossein Mobahi, and Samy Bengio. Semantic redundancies in image-\n       classification datasets: The 10% you don\u2019t need. arXiv preprint arXiv:1901.11409, 2019.\n       https://arxiv.org/abs/1901.11409.\n  [17] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel\n       Stanovsky, and Roy Schwartz. WinoGAViL: Gamified association benchmark to challenge\n       vision-and-language models, 2022. https://arxiv.org/abs/2207.12576.\n  [18] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative\n       components with random forests. In European Conference on Computer Vision (ECCV), 2014.\n       https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29.\n                                                  11", "md": "# References\n\n## References\n\n|[1]|cc2dataset. https://github.com/rom1504/cc2dataset|\n|---|---|\n|[2]|CLD3. https://github.com/google/cld3|\n|[3]|Common Crawl. https://commoncrawl.org|\n|[4]|dataset2metadata. https://github.com/mlfoundations/dataset2metadata|\n|[5]|img2dataset. https://github.com/rom1504/img2dataset|\n|[6]|Amro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023. https://arxiv.org/abs/2303.09540|\n|[7]|Pankaj K. Agarwal, Sariel Har-Peled, and Kasturi R. Varadarajan. Approximating extent measures of points. Journal of the ACM (JACM), 2004. https://doi.org/10.1145/1008731.1008736|\n|[8]|Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. https://openreview.net/forum?id=EbMuimAbPbs|\n|[9]|Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. Learning from rules generalizing labeled exemplars. In International Conference on Learning Representations (ICLR), 2020. https://openreview.net/forum?id=SkeuexBtDr|\n|[10]|Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher R\u00e9, and Rob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In Special Interest Group on Management of Data (SIGMOD), 2019. https://arxiv.org/abs/1812.00417|\n|[11]|Olivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation - the case of dp-means. In International Conference on Machine Learning (ICML), 2015. https://proceedings.mlr.press/v37/bachem15.html|\n|[12]|Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018. https://pubmed.ncbi.nlm.nih.gov/30716025/|\n|[13]|Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems (NeurIPS), volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf|\n|[14]|Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset, 2020. https://arxiv.org/abs/2004.10340|\n|[15]|Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes, 2021. https://arxiv.org/abs/2110.01963|\n|[16]|Vighnesh Birodkar, Hossein Mobahi, and Samy Bengio. Semantic redundancies in image-classification datasets: The 10% you don\u2019t need. arXiv preprint arXiv:1901.11409, 2019. https://arxiv.org/abs/1901.11409|\n|[17]|Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. WinoGAViL: Gamified association benchmark to challenge vision-and-language models, 2022. https://arxiv.org/abs/2207.12576|\n|[18]|Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In European Conference on Computer Vision (ECCV), 2014. https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "table", "rows": [["[1]", "cc2dataset. https://github.com/rom1504/cc2dataset"], ["[2]", "CLD3. https://github.com/google/cld3"], ["[3]", "Common Crawl. https://commoncrawl.org"], ["[4]", "dataset2metadata. https://github.com/mlfoundations/dataset2metadata"], ["[5]", "img2dataset. https://github.com/rom1504/img2dataset"], ["[6]", "Amro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023. https://arxiv.org/abs/2303.09540"], ["[7]", "Pankaj K. Agarwal, Sariel Har-Peled, and Kasturi R. Varadarajan. Approximating extent measures of points. Journal of the ACM (JACM), 2004. https://doi.org/10.1145/1008731.1008736"], ["[8]", "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. https://openreview.net/forum?id=EbMuimAbPbs"], ["[9]", "Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. Learning from rules generalizing labeled exemplars. In International Conference on Learning Representations (ICLR), 2020. https://openreview.net/forum?id=SkeuexBtDr"], ["[10]", "Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher R\u00e9, and Rob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In Special Interest Group on Management of Data (SIGMOD), 2019. https://arxiv.org/abs/1812.00417"], ["[11]", "Olivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation - the case of dp-means. In International Conference on Machine Learning (ICML), 2015. https://proceedings.mlr.press/v37/bachem15.html"], ["[12]", "Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018. https://pubmed.ncbi.nlm.nih.gov/30716025/"], ["[13]", "Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems (NeurIPS), volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf"], ["[14]", "Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset, 2020. https://arxiv.org/abs/2004.10340"], ["[15]", "Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes, 2021. https://arxiv.org/abs/2110.01963"], ["[16]", "Vighnesh Birodkar, Hossein Mobahi, and Samy Bengio. Semantic redundancies in image-classification datasets: The 10% you don\u2019t need. arXiv preprint arXiv:1901.11409, 2019. https://arxiv.org/abs/1901.11409"], ["[17]", "Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. WinoGAViL: Gamified association benchmark to challenge vision-and-language models, 2022. https://arxiv.org/abs/2207.12576"], ["[18]", "Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In European Conference on Computer Vision (ECCV), 2014. https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29"]], "md": "|[1]|cc2dataset. https://github.com/rom1504/cc2dataset|\n|---|---|\n|[2]|CLD3. https://github.com/google/cld3|\n|[3]|Common Crawl. https://commoncrawl.org|\n|[4]|dataset2metadata. https://github.com/mlfoundations/dataset2metadata|\n|[5]|img2dataset. https://github.com/rom1504/img2dataset|\n|[6]|Amro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023. https://arxiv.org/abs/2303.09540|\n|[7]|Pankaj K. Agarwal, Sariel Har-Peled, and Kasturi R. Varadarajan. Approximating extent measures of points. Journal of the ACM (JACM), 2004. https://doi.org/10.1145/1008731.1008736|\n|[8]|Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. https://openreview.net/forum?id=EbMuimAbPbs|\n|[9]|Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. Learning from rules generalizing labeled exemplars. In International Conference on Learning Representations (ICLR), 2020. https://openreview.net/forum?id=SkeuexBtDr|\n|[10]|Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher R\u00e9, and Rob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In Special Interest Group on Management of Data (SIGMOD), 2019. https://arxiv.org/abs/1812.00417|\n|[11]|Olivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation - the case of dp-means. In International Conference on Machine Learning (ICML), 2015. https://proceedings.mlr.press/v37/bachem15.html|\n|[12]|Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018. https://pubmed.ncbi.nlm.nih.gov/30716025/|\n|[13]|Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems (NeurIPS), volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf|\n|[14]|Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset, 2020. https://arxiv.org/abs/2004.10340|\n|[15]|Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes, 2021. https://arxiv.org/abs/2110.01963|\n|[16]|Vighnesh Birodkar, Hossein Mobahi, and Samy Bengio. Semantic redundancies in image-classification datasets: The 10% you don\u2019t need. arXiv preprint arXiv:1901.11409, 2019. https://arxiv.org/abs/1901.11409|\n|[17]|Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. WinoGAViL: Gamified association benchmark to challenge vision-and-language models, 2022. https://arxiv.org/abs/2207.12576|\n|[18]|Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In European Conference on Computer Vision (ECCV), 2014. https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29|", "isPerfectTable": true, "csv": "\"[1]\",\"cc2dataset. https://github.com/rom1504/cc2dataset\"\n\"[2]\",\"CLD3. https://github.com/google/cld3\"\n\"[3]\",\"Common Crawl. https://commoncrawl.org\"\n\"[4]\",\"dataset2metadata. https://github.com/mlfoundations/dataset2metadata\"\n\"[5]\",\"img2dataset. https://github.com/rom1504/img2dataset\"\n\"[6]\",\"Amro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023. https://arxiv.org/abs/2303.09540\"\n\"[7]\",\"Pankaj K. Agarwal, Sariel Har-Peled, and Kasturi R. Varadarajan. Approximating extent measures of points. Journal of the ACM (JACM), 2004. https://doi.org/10.1145/1008731.1008736\"\n\"[8]\",\"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. https://openreview.net/forum?id=EbMuimAbPbs\"\n\"[9]\",\"Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. Learning from rules generalizing labeled exemplars. In International Conference on Learning Representations (ICLR), 2020. https://openreview.net/forum?id=SkeuexBtDr\"\n\"[10]\",\"Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher R\u00e9, and Rob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In Special Interest Group on Management of Data (SIGMOD), 2019. https://arxiv.org/abs/1812.00417\"\n\"[11]\",\"Olivier Bachem, Mario Lucic, and Andreas Krause. Coresets for nonparametric estimation - the case of dp-means. In International Conference on Machine Learning (ICML), 2015. https://proceedings.mlr.press/v37/bachem15.html\"\n\"[12]\",\"Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018. https://pubmed.ncbi.nlm.nih.gov/30716025/\"\n\"[13]\",\"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems (NeurIPS), volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf\"\n\"[14]\",\"Sara Beery, Elijah Cole, and Arvi Gjoka. The iwildcam 2020 competition dataset, 2020. https://arxiv.org/abs/2004.10340\"\n\"[15]\",\"Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes, 2021. https://arxiv.org/abs/2110.01963\"\n\"[16]\",\"Vighnesh Birodkar, Hossein Mobahi, and Samy Bengio. Semantic redundancies in image-classification datasets: The 10% you don\u2019t need. arXiv preprint arXiv:1901.11409, 2019. https://arxiv.org/abs/1901.11409\"\n\"[17]\",\"Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. WinoGAViL: Gamified association benchmark to challenge vision-and-language models, 2022. https://arxiv.org/abs/2207.12576\"\n\"[18]\",\"Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In European Conference on Computer Vision (ECCV), 2014. https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29\""}]}, {"page": 12, "text": "[19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\n     Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\n     Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\n     Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\n     Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n     Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In\n     H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural\n     Information Processing Systems (NeurIPS), 2020. https://proceedings.neurips.cc/\n     paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n[20] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and\n     Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/\n     coyo-dataset, 2022.\n[21] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws.\n     International Conference on Learning Representations (ICLR), 2023. https://arxiv.org/\n     abs/2210.14891.\n[22] Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will\n     Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. Poisoning\n     web-scale training datasets is practical, 2023. https://arxiv.org/abs/2302.10149.\n[23] Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh,\n     Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive\n     emergent in-context learning in transformers. In Advances in Neural Information Processing\n     Systems (NeurIPS), 2022. https://arxiv.org/abs/2205.05055.\n[24] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\n     web-scale image-text pre-training to recognize long-tail visual concepts. In Conference on\n     Computer Vision and Pattern Recognition (CVPR), 2021. https://arxiv.org/abs/2102.\n     08981.\n[25] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz,\n     Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan\n     Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish\n     Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol\n     Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\n     Radu Soricut. Pali: A jointly-scaled multilingual language-image model. In International\n     Conference on Learning Representations (ICLR), 2022. https://arxiv.org/abs/2209.\n     06794.\n[26] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r,\n     and C Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server,\n     2015. https://arxiv.org/abs/1504.00325.\n[27] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification:\n     Benchmark and state of the art.         Proceedings of the Institute of Electrical and\n     Electronics Engineers (IEEE), 2017.        https://ieeexplore.ieee.org/abstract/\n     document/7891544.\n[28] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade\n     Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws\n     for contrastive language-image learning, 2022. https://arxiv.org/abs/2212.07143.\n[29] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the\n     world. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018. https:\n     //arxiv.org/abs/1711.07846.\n[30] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea\n     Vedaldi. Describing textures in the wild. In Conference on Computer Vision and Pattern\n     Recognition (CVPR), 2014. https://openaccess.thecvf.com/content_cvpr_2014/\n     html/Cimpoi_Describing_Textures_in_2014_CVPR_paper.html.\n[31] Adam Coates, Andrew Ng, and Honglak Lee.          An analysis of single-layer networks in\n     unsupervised feature learning. In International Conference on Artificial Intelligence and\n     Statistics (AISTATS), 2011. https://proceedings.mlr.press/v15/coates11a.html.\n                                              12", "md": "# References\n\n# References\n\n1. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems (NeurIPS)*, 2020. Link to paper.\n2. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. Link to dataset, 2022.\n3. Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. *International Conference on Learning Representations (ICLR)*, 2023. Link to paper.\n4. Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. Poisoning web-scale training datasets is practical, 2023. Link to paper.\n5. Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2022. Link to paper.\n6. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021. Link to paper.\n7. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model. In *International Conference on Learning Representations (ICLR)*, 2022. Link to paper.\n8. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server, 2015. Link to paper.\n9. Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. *Proceedings of the Institute of Electrical and Electronics Engineers (IEEE)*, 2017. Link to paper.\n10. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022. Link to paper.\n11. Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018. Link to paper.\n12. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2014. Link to paper.\n13. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In *International Conference on Artificial Intelligence and Statistics (AISTATS)*, 2011. Link to paper.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "1. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems (NeurIPS)*, 2020. Link to paper.\n2. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. Link to dataset, 2022.\n3. Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. *International Conference on Learning Representations (ICLR)*, 2023. Link to paper.\n4. Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. Poisoning web-scale training datasets is practical, 2023. Link to paper.\n5. Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2022. Link to paper.\n6. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021. Link to paper.\n7. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model. In *International Conference on Learning Representations (ICLR)*, 2022. Link to paper.\n8. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server, 2015. Link to paper.\n9. Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. *Proceedings of the Institute of Electrical and Electronics Engineers (IEEE)*, 2017. Link to paper.\n10. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022. Link to paper.\n11. Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018. Link to paper.\n12. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2014. Link to paper.\n13. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In *International Conference on Artificial Intelligence and Statistics (AISTATS)*, 2011. Link to paper.", "md": "1. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems (NeurIPS)*, 2020. Link to paper.\n2. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. Link to dataset, 2022.\n3. Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. *International Conference on Learning Representations (ICLR)*, 2023. Link to paper.\n4. Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. Poisoning web-scale training datasets is practical, 2023. Link to paper.\n5. Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2022. Link to paper.\n6. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021. Link to paper.\n7. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model. In *International Conference on Learning Representations (ICLR)*, 2022. Link to paper.\n8. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server, 2015. Link to paper.\n9. Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. *Proceedings of the Institute of Electrical and Electronics Engineers (IEEE)*, 2017. Link to paper.\n10. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022. Link to paper.\n11. Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018. Link to paper.\n12. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2014. Link to paper.\n13. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In *International Conference on Artificial Intelligence and Statistics (AISTATS)*, 2011. Link to paper."}]}, {"page": 13, "text": "[32] Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank\n      approximation via ridge leverage score sampling. In ACM-SIAM Symposium on Discrete\n     Algorithms, 2017. https://dl.acm.org/doi/10.5555/3039686.3039801.\n[33] C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis, P Liang, J Leskovec, and\n      M Zaharia. Selection via proxy: Effi cient data selection for deep learning. In International\n     Conference on Learning Representations (ICLR), 2020. https://arxiv.org/abs/1906.\n      11829.\n[34] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\n      Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.\n      Unsupervised cross-lingual representation learning at scale.      In Annual Meeting of the\n     Association for Computational Linguistics (ACL), 2019. https://arxiv.org/abs/1911.\n      02116.\n[35] R Dennis Cook. Detection of influential observation in linear. Technometrics, 19(1):15\u201318,\n     1977.\n[36] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. Tao:\n     A large-scale benchmark for tracking any object. In European Conference on Computer Vision\n     (ECCV), 2020. https://arxiv.org/abs/2005.10356.\n[37] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n      hierarchical image database. In Conference on Computer Vision and Pattern Recognition\n     (CVPR), 2009. https://ieeexplore.ieee.org/abstract/document/5206848.\n[38] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-\n      text data created by the people, for the people, 2021. https://arxiv.org/abs/2111.\n      11431.\n[39] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\n     Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\n      Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\n      recognition at scale. In International Conference on Learning Representations (ICLR), 2021.\n      https://openreview.net/forum?id=YicbFdNTTy.\n[40] Matthijs Douze, Giorgos Tolias, Ed Pizzi, Zo\u00eb Papakipos, Lowik Chanussot, Filip Radenovic,\n     Tomas Jenicek, Maxim Maximov, Laura Leal-Taix\u00e9, Ismail Elezi, Ondrej Chum, and Cristian\n      Canton-Ferrer. The 2021 image similarity dataset and challenge, 2021. https://arxiv.\n      org/abs/2106.09672.\n[41] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset diffi        culty\n     with v-usable information. In International Conference on Machine Learning (ICML), 2022.\n      https://arxiv.org/abs/2110.08420.\n[42] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.                 The\n      PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results, 2007. http://www.\n      pascal-network.org/challenges/VOC/voc2007/workshop/index.html.\n[43] Sabri Eyuboglu, Bojan Karla\u0161, Christopher R\u00e9, Ce Zhang, and James Zou. dcbench: a\n      benchmark for data-centric ai systems.     In Proceedings of the Sixth Workshop on Data\n     Management for End-To-End Machine Learning, 2022. https://dl.acm.org/doi/abs/\n      10.1145/3533028.3533310.\n[44] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave,\n      and Ludwig Schmidt. Data determines distributional robustness in contrastive language\n      image pre-training (clip). In International Conference on Machine Learning (ICML), 2022.\n      https://arxiv.org/abs/2205.01397.\n[45] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\n      examples: An incremental Bayesian approach tested on 101 object categories. Conference on\n     Computer Vision and Pattern Recognition (CVPR) Workshop, 2004. https://ieeexplore.\n      ieee.org/document/1384978.\n[46] Dan Feldman,       Matthew Faulkner,      and Andreas Krause.          Scalable training of\n      mixture models via coresets.     In Advances in Neural Information Processing Systems\n     (NeuIPS), 2011.        https://proceedings.neurips.cc/paper_files/paper/2011/\n      file/2b6d65b9a9445c4271ab9076ead5605a-Paper.pdf.\n                                                13", "md": "- **[32]** Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In ACM-SIAM Symposium on Discrete Algorithms, 2017. Link.\n- **[33]** C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis, P Liang, J Leskovec, and M Zaharia. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations (ICLR), 2020. Link.\n- **[34]** Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Annual Meeting of the Association for Computational Linguistics (ACL), 2019. Link.\n- **[35]** R Dennis Cook. Detection of influential observation in linear. Technometrics, 19(1):15\u201318, 1977.\n- **[36]** Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. Tao: A large-scale benchmark for tracking any object. In European Conference on Computer Vision (ECCV), 2020. Link.\n- **[37]** Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), 2009. Link.\n- **[38]** Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people, 2021. Link.\n- **[39]** Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. Link.\n- **[40]** Matthijs Douze, Giorgos Tolias, Ed Pizzi, Zo\u00eb Papakipos, Lowik Chanussot, Filip Radenovic, Tomas Jenicek, Maxim Maximov, Laura Leal-Taix\u00e9, Ismail Elezi, Ondrej Chum, and Cristian Canton-Ferrer. The 2021 image similarity dataset and challenge, 2021. Link.\n- **[41]** Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with v-usable information. In International Conference on Machine Learning (ICML), 2022. Link.\n- **[42]** M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results, 2007. Link.\n- **[43]** Sabri Eyuboglu, Bojan Karla\u0161, Christopher R\u00e9, Ce Zhang, and James Zou. dcbench: a benchmark for data-centric AI systems. In Proceedings of the Sixth Workshop on Data Management for End-To-End Machine Learning, 2022. Link.\n- **[44]** Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning (ICML), 2022. Link.\n- **[45]** Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2004. Link.\n- **[46]** Dan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via coresets. In Advances in Neural Information Processing Systems (NeuIPS), 2011. Link.", "images": [], "items": [{"type": "text", "value": "- **[32]** Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In ACM-SIAM Symposium on Discrete Algorithms, 2017. Link.\n- **[33]** C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis, P Liang, J Leskovec, and M Zaharia. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations (ICLR), 2020. Link.\n- **[34]** Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Annual Meeting of the Association for Computational Linguistics (ACL), 2019. Link.\n- **[35]** R Dennis Cook. Detection of influential observation in linear. Technometrics, 19(1):15\u201318, 1977.\n- **[36]** Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. Tao: A large-scale benchmark for tracking any object. In European Conference on Computer Vision (ECCV), 2020. Link.\n- **[37]** Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), 2009. Link.\n- **[38]** Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people, 2021. Link.\n- **[39]** Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. Link.\n- **[40]** Matthijs Douze, Giorgos Tolias, Ed Pizzi, Zo\u00eb Papakipos, Lowik Chanussot, Filip Radenovic, Tomas Jenicek, Maxim Maximov, Laura Leal-Taix\u00e9, Ismail Elezi, Ondrej Chum, and Cristian Canton-Ferrer. The 2021 image similarity dataset and challenge, 2021. Link.\n- **[41]** Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with v-usable information. In International Conference on Machine Learning (ICML), 2022. Link.\n- **[42]** M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results, 2007. Link.\n- **[43]** Sabri Eyuboglu, Bojan Karla\u0161, Christopher R\u00e9, Ce Zhang, and James Zou. dcbench: a benchmark for data-centric AI systems. In Proceedings of the Sixth Workshop on Data Management for End-To-End Machine Learning, 2022. Link.\n- **[44]** Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning (ICML), 2022. Link.\n- **[45]** Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2004. Link.\n- **[46]** Dan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via coresets. In Advances in Neural Information Processing Systems (NeuIPS), 2011. Link.", "md": "- **[32]** Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In ACM-SIAM Symposium on Discrete Algorithms, 2017. Link.\n- **[33]** C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis, P Liang, J Leskovec, and M Zaharia. Selection via proxy: Efficient data selection for deep learning. In International Conference on Learning Representations (ICLR), 2020. Link.\n- **[34]** Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Annual Meeting of the Association for Computational Linguistics (ACL), 2019. Link.\n- **[35]** R Dennis Cook. Detection of influential observation in linear. Technometrics, 19(1):15\u201318, 1977.\n- **[36]** Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. Tao: A large-scale benchmark for tracking any object. In European Conference on Computer Vision (ECCV), 2020. Link.\n- **[37]** Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), 2009. Link.\n- **[38]** Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people, 2021. Link.\n- **[39]** Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. Link.\n- **[40]** Matthijs Douze, Giorgos Tolias, Ed Pizzi, Zo\u00eb Papakipos, Lowik Chanussot, Filip Radenovic, Tomas Jenicek, Maxim Maximov, Laura Leal-Taix\u00e9, Ismail Elezi, Ondrej Chum, and Cristian Canton-Ferrer. The 2021 image similarity dataset and challenge, 2021. Link.\n- **[41]** Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with v-usable information. In International Conference on Machine Learning (ICML), 2022. Link.\n- **[42]** M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results, 2007. Link.\n- **[43]** Sabri Eyuboglu, Bojan Karla\u0161, Christopher R\u00e9, Ce Zhang, and James Zou. dcbench: a benchmark for data-centric AI systems. In Proceedings of the Sixth Workshop on Data Management for End-To-End Machine Learning, 2022. Link.\n- **[44]** Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning (ICML), 2022. Link.\n- **[45]** Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2004. Link.\n- **[46]** Dan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via coresets. In Advances in Neural Information Processing Systems (NeuIPS), 2011. Link."}]}, {"page": 14, "text": "[47] Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and\n     Christopher R\u00e9. Fast and three-rious: Speeding up weak supervision with triplet methods. In\n     International Conference on Machine Learning (ICML), 2020. https://arxiv.org/abs/\n     2002.11955.\n[48] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving?\n     the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition\n     (CVPR), 2012. https://ieeexplore.ieee.org/abstract/document/6248074.\n[49] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine\n     learning. In International Conference on Machine Learning, pp. 2242\u20132251. PMLR, 2019.\n[50] Stephan Graf and Olaf Mextorf. Just: Large-scale multi-tier storage infrastructure at the\n     j\u00fclich supercomputing centre. Journal of large-scale research facilities JLSRF, 2021. https:\n     //jlsrf.org/index.php/lsf/article/view/180.\n[51] Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A comprehensive library for coreset\n     selection in deep learning, 2022. https://arxiv.org/abs/2204.08499.\n[52] Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. Fastif:\n     Scalable influence functions for efficient model interpretation and debugging, 2020. https:\n     //arxiv.org/abs/2012.15781.\n[53] Jia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation\n     redistribution for effi cient face detection.     In International Conference on Learning\n     Representations (ICLR), 2021. https://arxiv.org/abs/2105.04714.\n[54] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance\n     segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n[55] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and\n     Noah A. Smith. Annotation artifacts in natural language inference data. In Conference of the\n     North American Chapter of the Association for Computational Linguistics (NAACL), 2018.\n     https://aclanthology.org/N18-2017.\n[56] Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi.\n     Simfluence: Modeling the influence of individual training examples by simulating training\n     runs, 2023. https://arxiv.org/abs/2303.08114.\n[57] Frank R Hampel. The influence curve and its role in robust estimation. Journal of the american\n     statistical association, 1974. https://www.jstor.org/stable/2285666.\n[58] Xiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and\n     unveiling data artifacts through influence functions, 2020. https://arxiv.org/abs/2005.\n     06676.\n[59] A. Hanna, Emily L. Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical\n     race methodology in algorithmic fairness. In Conference on Fairness, Accountability, and\n     Transparency (FAccT), 2020. https://arxiv.org/abs/1912.03593.\n[60] Laura Hanu and Unitary team.         Detoxify, 2020.     https://github.com/unitaryai/\n     detoxify.\n[61] Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In\n     Symposium on Theory of Computing (STOC), 2004. https://doi.org/10.1145/1007352.\n     1007400.\n[62] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n     recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n     https://arxiv.org/abs/1512.03385.\n[63] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel\n     dataset and deep learning benchmark for land use and land cover classification. Journal of\n     Selected Topics in Applied Earth Observations and Remote Sensing, 2019. https://arxiv.\n     org/abs/1709.00029.\n[64] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,\n     Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin\n     Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization.\n     ICCV, 2021. https://arxiv.org/abs/2006.16241.\n                                                14", "md": "- [47] Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher R\u00e9. Fast and three-rious: Speeding up weak supervision with triplet methods. In International Conference on Machine Learning (ICML), 2020. Link\n- [48] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. Link\n- [49] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In International Conference on Machine Learning, pp. 2242\u20132251. PMLR, 2019.\n- [50] Stephan Graf and Olaf Mextorf. Just: Large-scale multi-tier storage infrastructure at the j\u00fclich supercomputing centre. Journal of large-scale research facilities JLSRF, 2021. Link\n- [51] Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A comprehensive library for coreset selection in deep learning, 2022. Link\n- [52] Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. Fastif: Scalable influence functions for efficient model interpretation and debugging, 2020. Link\n- [53] Jia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation redistribution for efficient face detection. In International Conference on Learning Representations (ICLR), 2021. Link\n- [54] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [55] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2018. Link\n- [56] Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence: Modeling the influence of individual training examples by simulating training runs, 2023. Link\n- [57] Frank R Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 1974. Link\n- [58] Xiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling data artifacts through influence functions, 2020. Link\n- [59] A. Hanna, Emily L. Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In Conference on Fairness, Accountability, and Transparency (FAccT), 2020. Link\n- [60] Laura Hanu and Unitary team. Detoxify, 2020. Link\n- [61] Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In Symposium on Theory of Computing (STOC), 2004. Link\n- [62] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Link\n- [63] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. Link\n- [64] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Link", "images": [], "items": [{"type": "text", "value": "- [47] Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher R\u00e9. Fast and three-rious: Speeding up weak supervision with triplet methods. In International Conference on Machine Learning (ICML), 2020. Link\n- [48] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. Link\n- [49] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In International Conference on Machine Learning, pp. 2242\u20132251. PMLR, 2019.\n- [50] Stephan Graf and Olaf Mextorf. Just: Large-scale multi-tier storage infrastructure at the j\u00fclich supercomputing centre. Journal of large-scale research facilities JLSRF, 2021. Link\n- [51] Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A comprehensive library for coreset selection in deep learning, 2022. Link\n- [52] Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. Fastif: Scalable influence functions for efficient model interpretation and debugging, 2020. Link\n- [53] Jia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation redistribution for efficient face detection. In International Conference on Learning Representations (ICLR), 2021. Link\n- [54] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [55] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2018. Link\n- [56] Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence: Modeling the influence of individual training examples by simulating training runs, 2023. Link\n- [57] Frank R Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 1974. Link\n- [58] Xiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling data artifacts through influence functions, 2020. Link\n- [59] A. Hanna, Emily L. Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In Conference on Fairness, Accountability, and Transparency (FAccT), 2020. Link\n- [60] Laura Hanu and Unitary team. Detoxify, 2020. Link\n- [61] Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In Symposium on Theory of Computing (STOC), 2004. Link\n- [62] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Link\n- [63] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. Link\n- [64] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Link", "md": "- [47] Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher R\u00e9. Fast and three-rious: Speeding up weak supervision with triplet methods. In International Conference on Machine Learning (ICML), 2020. Link\n- [48] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. Link\n- [49] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In International Conference on Machine Learning, pp. 2242\u20132251. PMLR, 2019.\n- [50] Stephan Graf and Olaf Mextorf. Just: Large-scale multi-tier storage infrastructure at the j\u00fclich supercomputing centre. Journal of large-scale research facilities JLSRF, 2021. Link\n- [51] Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A comprehensive library for coreset selection in deep learning, 2022. Link\n- [52] Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. Fastif: Scalable influence functions for efficient model interpretation and debugging, 2020. Link\n- [53] Jia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation redistribution for efficient face detection. In International Conference on Learning Representations (ICLR), 2021. Link\n- [54] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [55] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2018. Link\n- [56] Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence: Modeling the influence of individual training examples by simulating training runs, 2023. Link\n- [57] Frank R Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 1974. Link\n- [58] Xiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling data artifacts through influence functions, 2020. Link\n- [59] A. Hanna, Emily L. Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In Conference on Fairness, Accountability, and Transparency (FAccT), 2020. Link\n- [60] Laura Hanu and Unitary team. Detoxify, 2020. Link\n- [61] Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In Symposium on Theory of Computing (STOC), 2004. Link\n- [62] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Link\n- [63] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. Link\n- [64] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Link"}]}, {"page": 15, "text": "[65] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural\n     adversarial examples. In Conference on Computer Vision and Pattern Recognition (CVPR),\n     2021. https://arxiv.org/abs/1907.07174.\n[66] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\n     Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\n     Training compute-optimal large language models, 2022. https://arxiv.org/abs/2203.\n     15556.\n[67] Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld.\n     Knowledge-based weak supervision for information extraction of overlapping relations. In\n     Annual Meeting of the Association for Computational Linguistics (ACL), 2011.        https:\n     //aclanthology.org/P11-1055.\n[68] Andrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, and Matthew Gombolay.\n     Robots enact malignant stereotypes.      In Conference on Fairness, Accountability, and\n     Transparency (FAccT), 2022. https://arxiv.org/abs/2207.11569.\n[69] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan\n     Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi,\n     Ali Farhadi, and Ludwig Schmidt. OpenCLIP, July 2021. https://doi.org/10.5281/\n     zenodo.5143773.\n[70] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi,\n     Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by\n     interpolating weights. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n     https://arXiv.org/abs/2208.05592.\n[71] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry.\n     Datamodels: Predicting predictions from training data, 2022. https://arxiv.org/abs/\n     2202.00622.\n[72] Tanuj Jain, Christopher Lennan, Zubin John, and Dat Tran. Imagededup, 2019. https:\n     //github.com/idealo/imagededup.\n[73] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V\n     Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language\n     representation learning with noisy text supervision. In International Conference on Machine\n     Learning (ICML), 2021. https://arxiv.org/abs/2102.05918.\n[74] Mon-Fong Jiang, Shian-Shyong Tseng, and Chih-Ming Su. Two-phase clustering process for\n     outliers detection. Pattern recognition letters, 2001. https://www.sciencedirect.com/\n     science/article/abs/pii/S0167865500001318.\n[75] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs.\n     IEEE Transactions on Big Data, 2019. https://arxiv.org/abs/1702.08734.\n[76] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick,\n     and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary\n     visual reasoning. Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n     https://arxiv.org/abs/1612.06890.\n[77] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for\n     effi\n        cient text classification. In Conference of the European Chapter of the Association for\n     Computational Linguistics (EACL), 2017. https://arxiv.org/abs/1607.01759.\n[78] Juelich   Supercomputing     Center.        JUWELS      Booster    Supercomputer,     2020.\n     https://apps.fz-juelich.de/jsc/hps/juwels/configuration.html#\n     hardware-configuration-of-the-system-name-booster-module.\n[79] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\n     Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\n     language models, 2020. https://arxiv.org/abs/2001.08361.\n[80] Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race,\n     gender, and age for bias measurement and mitigation. In IEEE/CVF Winter Conference on\n     Applications of Computer Vision, 2021. https://arxiv.org/abs/1908.04913.\n                                              15", "md": "# References\n\n# References\n\n|[65]|Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Link|\n|---|---|\n|[66]|Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models, 2022. Link|\n|[67]|Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. Knowledge-based weak supervision for information extraction of overlapping relations. In Annual Meeting of the Association for Computational Linguistics (ACL), 2011. Link|\n|[68]|Andrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, and Matthew Gombolay. Robots enact malignant stereotypes. In Conference on Fairness, Accountability, and Transparency (FAccT), 2022. Link|\n|[69]|Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP, July 2021. Link|\n|[70]|Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Link|\n|[71]|Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data, 2022. Link|\n|[72]|Tanuj Jain, Christopher Lennan, Zubin John, and Dat Tran. Imagededup, 2019. Link|\n|[73]|Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021. Link|\n|[74]|Mon-Fong Jiang, Shian-Shyong Tseng, and Chih-Ming Su. Two-phase clustering process for outliers detection. Pattern recognition letters, 2001. Link|\n|[75]|Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 2019. Link|\n|[76]|Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Link|\n|[77]|Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2017. Link|\n|[78]|Juelich Supercomputing Center. JUWELS Booster Supercomputer, 2020. Link|\n|[79]|Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Link|\n|[80]|Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In IEEE/CVF Winter Conference on Applications of Computer Vision, 2021. Link|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "table", "rows": [["[65]", "Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Link"], ["[66]", "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models, 2022. Link"], ["[67]", "Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. Knowledge-based weak supervision for information extraction of overlapping relations. In Annual Meeting of the Association for Computational Linguistics (ACL), 2011. Link"], ["[68]", "Andrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, and Matthew Gombolay. Robots enact malignant stereotypes. In Conference on Fairness, Accountability, and Transparency (FAccT), 2022. Link"], ["[69]", "Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP, July 2021. Link"], ["[70]", "Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Link"], ["[71]", "Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data, 2022. Link"], ["[72]", "Tanuj Jain, Christopher Lennan, Zubin John, and Dat Tran. Imagededup, 2019. Link"], ["[73]", "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021. Link"], ["[74]", "Mon-Fong Jiang, Shian-Shyong Tseng, and Chih-Ming Su. Two-phase clustering process for outliers detection. Pattern recognition letters, 2001. Link"], ["[75]", "Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 2019. Link"], ["[76]", "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Link"], ["[77]", "Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2017. Link"], ["[78]", "Juelich Supercomputing Center. JUWELS Booster Supercomputer, 2020. Link"], ["[79]", "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Link"], ["[80]", "Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In IEEE/CVF Winter Conference on Applications of Computer Vision, 2021. Link"]], "md": "|[65]|Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Link|\n|---|---|\n|[66]|Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models, 2022. Link|\n|[67]|Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. Knowledge-based weak supervision for information extraction of overlapping relations. In Annual Meeting of the Association for Computational Linguistics (ACL), 2011. Link|\n|[68]|Andrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, and Matthew Gombolay. Robots enact malignant stereotypes. In Conference on Fairness, Accountability, and Transparency (FAccT), 2022. Link|\n|[69]|Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP, July 2021. Link|\n|[70]|Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Link|\n|[71]|Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data, 2022. Link|\n|[72]|Tanuj Jain, Christopher Lennan, Zubin John, and Dat Tran. Imagededup, 2019. Link|\n|[73]|Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021. Link|\n|[74]|Mon-Fong Jiang, Shian-Shyong Tseng, and Chih-Ming Su. Two-phase clustering process for outliers detection. Pattern recognition letters, 2001. Link|\n|[75]|Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 2019. Link|\n|[76]|Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Link|\n|[77]|Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2017. Link|\n|[78]|Juelich Supercomputing Center. JUWELS Booster Supercomputer, 2020. Link|\n|[79]|Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Link|\n|[80]|Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In IEEE/CVF Winter Conference on Applications of Computer Vision, 2021. Link|", "isPerfectTable": true, "csv": "\"[65]\",\"Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Link\"\n\"[66]\",\"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models, 2022. Link\"\n\"[67]\",\"Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. Knowledge-based weak supervision for information extraction of overlapping relations. In Annual Meeting of the Association for Computational Linguistics (ACL), 2011. Link\"\n\"[68]\",\"Andrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, and Matthew Gombolay. Robots enact malignant stereotypes. In Conference on Fairness, Accountability, and Transparency (FAccT), 2022. Link\"\n\"[69]\",\"Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP, July 2021. Link\"\n\"[70]\",\"Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Link\"\n\"[71]\",\"Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data, 2022. Link\"\n\"[72]\",\"Tanuj Jain, Christopher Lennan, Zubin John, and Dat Tran. Imagededup, 2019. Link\"\n\"[73]\",\"Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021. Link\"\n\"[74]\",\"Mon-Fong Jiang, Shian-Shyong Tseng, and Chih-Ming Su. Two-phase clustering process for outliers detection. Pattern recognition letters, 2001. Link\"\n\"[75]\",\"Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 2019. Link\"\n\"[76]\",\"Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Link\"\n\"[77]\",\"Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2017. Link\"\n\"[78]\",\"Juelich Supercomputing Center. JUWELS Booster Supercomputer, 2020. Link\"\n\"[79]\",\"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Link\"\n\"[80]\",\"Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In IEEE/CVF Winter Conference on Applications of Computer Vision, 2021. Link\""}]}, {"page": 16, "text": "[81] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions.\n      In International Conference on Machine Learning (ICML), 2017. https://arxiv.org/\n      abs/1703.04730.\n[82] Pang Wei Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang. On the accuracy of influence\n      functions for measuring group effects. Advances in Neural Information Processing Systems\n      (NeurIPS), 2019. https://arxiv.org/abs/1905.13289.\n[83] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay\n      Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,\n      Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery,\n      Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy\n      Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference\n      on Machine Learning (ICML), 2021. https://arxiv.org/abs/2012.07421.\n[84] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer\n      better? In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. https:\n      //arxiv.org/abs/1805.08974.\n[85] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-\n      grained categorization. In International Conference on Computer Vision Workshops (ICML),\n      2013.    https://www.cv-foundation.org/openaccess/content_iccv_workshops_\n      2013/W19/html/Krause_3D_Object_Representations_2013_ICCV_paper.html.\n[86] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images,\n      2009. https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.\n[87] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.          Imagenet classification with\n      deep convolutional neural networks.       In Advances in Neural Information Processing\n      Systems (NeurIPS), 2012. https://proceedings.neurips.cc/paper_files/paper/\n      2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\n[88] Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters,\n      Ashish Sabharwal, and Yejin Choi. Adversarial filters of dataset biases. In International\n      Conference on Machine Learning (ICML), 2020. https://arxiv.org/abs/2002.04108.\n[89] Yann LeCun. The MNIST database of handwritten digits, 1998. http://yann.lecun.com/\n      exdb/mnist/.\n[90] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\n      Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\n      better. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021.\n      https://arxiv.org/abs/2107.06499.\n[91] Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In\n      Conference on Computer Vision and Pattern Recognition (CVPR), 2019. https://arxiv.\n      org/abs/1904.07911.\n[92] Yulong Liu, Guibo Zhu, Bin Zhu, Qi Song, Guojing Ge, Haoran Chen, GuanHui Qiao,\n      Ru Peng, Lingxiang Wu, and Jinqiao Wang. Taisu: A 166m large-scale high-quality dataset for\n      chinese vision-language pre-training. In Advances in Neural Information Processing Systems\n      (NeurIPS), 2022.      https://proceedings.neurips.cc/paper_files/paper/2022/\n      file/6a386d703b50f1cf1f61ab02a15967bb-Paper-Datasets_and_Benchmarks.\n      pdf.\n[93] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\n      Xie. A convnet for the 2020s. Conference on Computer Vision and Pattern Recognition\n      (CVPR), 2022. https://arxiv.org/abs/2201.03545.\n[94] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian\n      mixture models at scale via coresets. Journal of Machine Learning Research (JMLR), 2018.\n      http://jmlr.org/papers/v18/15-506.html.\n[95] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification\n      of aircraft, 2013. https://arxiv.org/abs/1306.5151.\n[96] Gideon S Mann and Andrew McCallum. Generalized expectation criteria for semi-supervised\n      learning with weakly labeled data. Journal of Machine Learning Research (JMLR), 2010.\n      https://www.jmlr.org/papers/v11/mann10a.html.\n                                                16", "md": "- [81] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning (ICML), 2017.\n- [82] Pang Wei Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang. On the accuracy of influence functions for measuring group effects. Advances in Neural Information Processing Systems (NeurIPS), 2019.\n- [83] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML), 2021.\n- [84] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [85] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In International Conference on Computer Vision Workshops (ICML), 2013.\n- [86] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.\n- [87] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012.\n- [88] Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, and Yejin Choi. Adversarial filters of dataset biases. In International Conference on Machine Learning (ICML), 2020.\n- [89] Yann LeCun. The MNIST database of handwritten digits, 1998.\n- [90] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021.\n- [91] Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [92] Yulong Liu, Guibo Zhu, Bin Zhu, Qi Song, Guojing Ge, Haoran Chen, GuanHui Qiao, Ru Peng, Lingxiang Wu, and Jinqiao Wang. Taisu: A 166m large-scale high-quality dataset for Chinese vision-language pre-training. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n- [93] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n- [94] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture models at scale via coresets. Journal of Machine Learning Research (JMLR), 2018.\n- [95] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft, 2013.\n- [96] Gideon S Mann and Andrew McCallum. Generalized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Machine Learning Research (JMLR), 2010.", "images": [], "items": [{"type": "text", "value": "- [81] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning (ICML), 2017.\n- [82] Pang Wei Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang. On the accuracy of influence functions for measuring group effects. Advances in Neural Information Processing Systems (NeurIPS), 2019.\n- [83] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML), 2021.\n- [84] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [85] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In International Conference on Computer Vision Workshops (ICML), 2013.\n- [86] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.\n- [87] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012.\n- [88] Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, and Yejin Choi. Adversarial filters of dataset biases. In International Conference on Machine Learning (ICML), 2020.\n- [89] Yann LeCun. The MNIST database of handwritten digits, 1998.\n- [90] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021.\n- [91] Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [92] Yulong Liu, Guibo Zhu, Bin Zhu, Qi Song, Guojing Ge, Haoran Chen, GuanHui Qiao, Ru Peng, Lingxiang Wu, and Jinqiao Wang. Taisu: A 166m large-scale high-quality dataset for Chinese vision-language pre-training. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n- [93] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n- [94] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture models at scale via coresets. Journal of Machine Learning Research (JMLR), 2018.\n- [95] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft, 2013.\n- [96] Gideon S Mann and Andrew McCallum. Generalized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Machine Learning Research (JMLR), 2010.", "md": "- [81] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning (ICML), 2017.\n- [82] Pang Wei Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang. On the accuracy of influence functions for measuring group effects. Advances in Neural Information Processing Systems (NeurIPS), 2019.\n- [83] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML), 2021.\n- [84] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [85] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In International Conference on Computer Vision Workshops (ICML), 2013.\n- [86] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.\n- [87] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012.\n- [88] Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, and Yejin Choi. Adversarial filters of dataset biases. In International Conference on Machine Learning (ICML), 2020.\n- [89] Yann LeCun. The MNIST database of handwritten digits, 1998.\n- [90] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021.\n- [91] Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n- [92] Yulong Liu, Guibo Zhu, Bin Zhu, Qi Song, Guojing Ge, Haoran Chen, GuanHui Qiao, Ru Peng, Lingxiang Wu, and Jinqiao Wang. Taisu: A 166m large-scale high-quality dataset for Chinese vision-language pre-training. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n- [93] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n- [94] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture models at scale via coresets. Journal of Machine Learning Research (JMLR), 2018.\n- [95] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft, 2013.\n- [96] Gideon S Mann and Andrew McCallum. Generalized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Machine Learning Research (JMLR), 2010."}]}, {"page": 17, "text": " [97] Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karla\u0161, William Gaviria Rojas, Sudnya\n      Diamos, Greg Diamos, Lynn He, Douwe Kiela, David Jurado, David Kanter, Rafael Mosquera,\n      Juan Ciro, Lora Aroyo, Bilge Acun, Sabri Eyuboglu, Amirata Ghorbani, Emmett Goodman,\n      Tariq Kane, Christine R. Kirkpatrick, Tzu-Sheng Kuo, Jonas Mueller, Tristan Thrush, Joaquin\n      Vanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen\n      Paritosh, Ce Zhang, James Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson,\n      and Vijay Janapa Reddi.     Dataperf: Benchmarks for data-centric ai development, 2022.\n      https://arxiv.org/abs/2207.10062.\n [98] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training\n      of machine learning models. In International Conference on Machine Learning (ICML), 2020.\n      https://arxiv.org/abs/1906.01827.\n [99] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y\n      Ng. Reading digits in natural images with unsupervised feature learning. In Advances\n      in Neural Information Processing Systems (NeurIPS) Workshops, 2011. https://storage.\n      googleapis.com/pub-tools-public-publication-data/pdf/37648.pdf.\n[100] Andrew Ng, Dillon Laird, and Lynn He. Data-centric ai competition, 2021. https://\n      https-deeplearning-ai.github.io/data-centric-comp/.\n[101] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality\n      not quantity: On the interaction between dataset design and robustness of clip. In Advances\n      in Neural Information Processing Systems (NeurIPS), 2022. https://openreview.net/\n      forum?id=LTCBavFWp5C.\n[102] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large\n      number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing,\n      2008. https://ieeexplore.ieee.org/document/4756141.\n[103] OpenAI. Gpt-4 technical report, 2023. https://arxiv.org/abs/2303.08774.\n[104] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.          Im2text: Describing images\n      using 1 million captioned photographs.      In Advances in Neural Information Processing\n      Systems (NeurIPS), 2011. https://papers.nips.cc/paper_files/paper/2011/file/\n      5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf.\n[105] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar.               Cats and\n      dogs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. https:\n      //ieeexplore.ieee.org/document/6248092.\n[106] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet:\n      Finding important examples early in training. In Advances in Neural Information Processing\n      Systems (NeurIPS), 2021. https://arxiv.org/abs/2107.07075.\n[107] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong,\n      Mingxing Tan, and Quoc V. Le. Combined scaling for zero-shot transfer learning, 2021.\n      https://arxiv.org/abs/2111.10050.\n[108] Vinay Uday Prabhu and Abeba Birhane. Large image datasets: A pyrrhic win for computer\n      vision? In Winter Conference on Applications of Computer Vision (WACV), 2020. https:\n      //arxiv.org/abs/2006.16923.\n[109] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training\n      data influence by tracing gradient descent. Advances in Neural Information Processing Systems\n      (NeurIPS), 2020. https://arxiv.org/abs/2002.08484.\n[110] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende,\n      Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distillation, and hard\n      negatives for vision-language pre-training. In Conference on Computer Vision and Pattern\n      Recognition (CVPR), 2023. https://arxiv.org/abs/2301.02280.\n[111] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\n      Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and\n      Ilya Sutskever. Learning transferable visual models from natural language supervision. In\n      International Conference on Machine Learning (ICML), 2021. https://arxiv.org/abs/\n      2103.00020.\n                                                17", "md": "# References\n\n## References\n\n[97] Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karla\u0161, William Gaviria Rojas, Sudnya\nDiamos, Greg Diamos, Lynn He, Douwe Kiela, David Jurado, David Kanter, Rafael Mosquera,\nJuan Ciro, Lora Aroyo, Bilge Acun, Sabri Eyuboglu, Amirata Ghorbani, Emmett Goodman,\nTariq Kane, Christine R. Kirkpatrick, Tzu-Sheng Kuo, Jonas Mueller, Tristan Thrush, Joaquin\nVanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen\nParitosh, Ce Zhang, James Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson,\nand Vijay Janapa Reddi. Dataperf: Benchmarks for data-centric ai development, 2022.\nLink\n\n[98] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training\nof machine learning models. In International Conference on Machine Learning (ICML), 2020.\nLink\n\n[99] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y\nNg. Reading digits in natural images with unsupervised feature learning. In Advances\nin Neural Information Processing Systems (NeurIPS) Workshops, 2011. Link\n\n[100] Andrew Ng, Dillon Laird, and Lynn He. Data-centric ai competition, 2021. Link\n\n[101] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality\nnot quantity: On the interaction between dataset design and robustness of clip. In Advances\nin Neural Information Processing Systems (NeurIPS), 2022. Link\n\n[102] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large\nnumber of classes. In Indian Conference on Computer Vision, Graphics and Image Processing,\n2008. Link\n\n[103] OpenAI. Gpt-4 technical report, 2023. Link\n\n[104] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images\nusing 1 million captioned photographs. In Advances in Neural Information Processing\nSystems (NeurIPS), 2011. Link\n\n[105] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and\ndogs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. Link\n\n[106] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet:\nFinding important examples early in training. In Advances in Neural Information Processing\nSystems (NeurIPS), 2021. Link\n\n[107] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong,\nMingxing Tan, and Quoc V. Le. Combined scaling for zero-shot transfer learning, 2021.\nLink\n\n[108] Vinay Uday Prabhu and Abeba Birhane. Large image datasets: A pyrrhic win for computer\nvision? In Winter Conference on Applications of Computer Vision (WACV), 2020. Link\n\n[109] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training\ndata influence by tracing gradient descent. Advances in Neural Information Processing Systems\n(NeurIPS), 2020. Link\n\n[110] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende,\nYash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distillation, and hard\nnegatives for vision-language pre-training. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. Link\n\n[111] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and\nIlya Sutskever. Learning transferable visual models from natural language supervision. In\nInternational Conference on Machine Learning (ICML), 2021. Link", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "[97] Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karla\u0161, William Gaviria Rojas, Sudnya\nDiamos, Greg Diamos, Lynn He, Douwe Kiela, David Jurado, David Kanter, Rafael Mosquera,\nJuan Ciro, Lora Aroyo, Bilge Acun, Sabri Eyuboglu, Amirata Ghorbani, Emmett Goodman,\nTariq Kane, Christine R. Kirkpatrick, Tzu-Sheng Kuo, Jonas Mueller, Tristan Thrush, Joaquin\nVanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen\nParitosh, Ce Zhang, James Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson,\nand Vijay Janapa Reddi. Dataperf: Benchmarks for data-centric ai development, 2022.\nLink\n\n[98] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training\nof machine learning models. In International Conference on Machine Learning (ICML), 2020.\nLink\n\n[99] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y\nNg. Reading digits in natural images with unsupervised feature learning. In Advances\nin Neural Information Processing Systems (NeurIPS) Workshops, 2011. Link\n\n[100] Andrew Ng, Dillon Laird, and Lynn He. Data-centric ai competition, 2021. Link\n\n[101] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality\nnot quantity: On the interaction between dataset design and robustness of clip. In Advances\nin Neural Information Processing Systems (NeurIPS), 2022. Link\n\n[102] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large\nnumber of classes. In Indian Conference on Computer Vision, Graphics and Image Processing,\n2008. Link\n\n[103] OpenAI. Gpt-4 technical report, 2023. Link\n\n[104] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images\nusing 1 million captioned photographs. In Advances in Neural Information Processing\nSystems (NeurIPS), 2011. Link\n\n[105] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and\ndogs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. Link\n\n[106] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet:\nFinding important examples early in training. In Advances in Neural Information Processing\nSystems (NeurIPS), 2021. Link\n\n[107] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong,\nMingxing Tan, and Quoc V. Le. Combined scaling for zero-shot transfer learning, 2021.\nLink\n\n[108] Vinay Uday Prabhu and Abeba Birhane. Large image datasets: A pyrrhic win for computer\nvision? In Winter Conference on Applications of Computer Vision (WACV), 2020. Link\n\n[109] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training\ndata influence by tracing gradient descent. Advances in Neural Information Processing Systems\n(NeurIPS), 2020. Link\n\n[110] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende,\nYash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distillation, and hard\nnegatives for vision-language pre-training. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. Link\n\n[111] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and\nIlya Sutskever. Learning transferable visual models from natural language supervision. In\nInternational Conference on Machine Learning (ICML), 2021. Link", "md": "[97] Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karla\u0161, William Gaviria Rojas, Sudnya\nDiamos, Greg Diamos, Lynn He, Douwe Kiela, David Jurado, David Kanter, Rafael Mosquera,\nJuan Ciro, Lora Aroyo, Bilge Acun, Sabri Eyuboglu, Amirata Ghorbani, Emmett Goodman,\nTariq Kane, Christine R. Kirkpatrick, Tzu-Sheng Kuo, Jonas Mueller, Tristan Thrush, Joaquin\nVanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, Praveen\nParitosh, Ce Zhang, James Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson,\nand Vijay Janapa Reddi. Dataperf: Benchmarks for data-centric ai development, 2022.\nLink\n\n[98] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training\nof machine learning models. In International Conference on Machine Learning (ICML), 2020.\nLink\n\n[99] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y\nNg. Reading digits in natural images with unsupervised feature learning. In Advances\nin Neural Information Processing Systems (NeurIPS) Workshops, 2011. Link\n\n[100] Andrew Ng, Dillon Laird, and Lynn He. Data-centric ai competition, 2021. Link\n\n[101] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality\nnot quantity: On the interaction between dataset design and robustness of clip. In Advances\nin Neural Information Processing Systems (NeurIPS), 2022. Link\n\n[102] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large\nnumber of classes. In Indian Conference on Computer Vision, Graphics and Image Processing,\n2008. Link\n\n[103] OpenAI. Gpt-4 technical report, 2023. Link\n\n[104] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images\nusing 1 million captioned photographs. In Advances in Neural Information Processing\nSystems (NeurIPS), 2011. Link\n\n[105] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and\ndogs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. Link\n\n[106] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet:\nFinding important examples early in training. In Advances in Neural Information Processing\nSystems (NeurIPS), 2021. Link\n\n[107] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong,\nMingxing Tan, and Quoc V. Le. Combined scaling for zero-shot transfer learning, 2021.\nLink\n\n[108] Vinay Uday Prabhu and Abeba Birhane. Large image datasets: A pyrrhic win for computer\nvision? In Winter Conference on Applications of Computer Vision (WACV), 2020. Link\n\n[109] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training\ndata influence by tracing gradient descent. Advances in Neural Information Processing Systems\n(NeurIPS), 2020. Link\n\n[110] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende,\nYash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distillation, and hard\nnegatives for vision-language pre-training. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. Link\n\n[111] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and\nIlya Sutskever. Learning transferable visual models from natural language supervision. In\nInternational Conference on Machine Learning (ICML), 2021. Link"}]}, {"page": 18, "text": "[112] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya\n      Sutskever. Robust speech recognition via large-scale weak supervision, 2022. https://\n      arxiv.org/abs/2212.04356.\n[113] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\n      Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\n      text-to-text transformer. The Journal of Machine Learning Research (JMLR), 2020. https:\n      //arxiv.org/abs/1910.10683.\n[114] Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B. Adcock, Laurens van der Maaten,\n      Deepti Ghadiyaram, and Olga Russakovsky.         Beyond web-scraping: Crowd-sourcing a\n      geodiverse datase, 2023. https://arxiv.org/abs/2301.02560.\n[115] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\n      Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\n      Machine Learning (ICML), 2021. https://arxiv.org/abs/2102.12092.\n[116] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\n      text-conditional image generation with clip latents, 2022. https://arxiv.org/abs/2204.\n      06125.\n[117] A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. R\u00e9. Training complex\n      models with multi-task weak supervision. In Association for the Advancement of Artificial\n      Intelligence (AAAI), 2019. https://arxiv.org/abs/1810.02840.\n[118] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R\u00e9.\n      Data programming: Creating large training sets, quickly. In Advances in Neural Information\n      Processing Systems (NeurIPS), 2016. https://arxiv.org/abs/1605.07723.\n[119] Alexander J Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher\n      R\u00e9. Snorkel: Rapid training data creation with weak supervision. In Very Large Data Bases\n      Conference (VLDB), 2017. https://arxiv.org/abs/1711.10160.\n[120] Christopher R\u00e9. Overton: A data system for monitoring and improving machine-learned\n      products. In 10th Conference on Innovative Data Systems Research, CIDR 2020, Amsterdam,\n      The Netherlands, January 12-15, 2020, Online Proceedings. www.cidrdb.org, 2020. URL\n      http://cidrdb.org/cidr2020/papers/p33-re-cidr20.pdf.\n[121] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet\n      classifiers generalize to ImageNet? In International Conference on Machine Learning (ICML),\n      2019. http://proceedings.mlr.press/v97/recht19a.html.\n[122] William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa\n      Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and\n      socioeconomic diversity of the world. In Advances in Neural Information Processing Systems\n      (NeurIPS) Datasets and Benchmarks Track, 2022. https://openreview.net/forum?id=\n      qnfYsave0U4.\n[123] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n      High-resolution image synthesis with latent diffusion models. In Conference on Computer\n      Vision and Pattern Recognition (CVPR), 2022. https://arxiv.org/abs/2112.10752.\n[124] Peter J Rousseeuw and Mia Hubert.          Robust statistics for outlier detection.     Wiley\n      interdisciplinary reviews: Data mining and knowledge discovery, 2011.         http://i2pc.\n      es/coss/Docencia/SignalProcessingReviews/Rousseeuw2011.pdf.\n[125] Peter J Rousseeuw and Mia Hubert.          Anomaly detection by robust statistics.      Wiley\n      interdisciplinary reviews: Data mining and knowledge discovery, 2018. https://wires.\n      onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1236.\n[126] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\n      Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-\n      Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer\n      Vision (IJCV), 2015. https://arxiv.org/abs/1409.0575.\n[127] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen,\n      Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne\n      David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey\n      Levine, Chelsea Finn, and Percy Liang. Extending the wilds benchmark for unsupervised\n                                                18", "md": "- Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. Link\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research (JMLR), 2020. Link\n- Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B. Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Beyond web-scraping: Crowd-sourcing a geodiverse datase, 2023. Link\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), 2021. Link\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. Link\n- A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. R\u00e9. Training complex models with multi-task weak supervision. In Association for the Advancement of Artificial Intelligence (AAAI), 2019. Link\n- Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R\u00e9. Data programming: Creating large training sets, quickly. In Advances in Neural Information Processing Systems (NeurIPS), 2016. Link\n- Alexander J Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R\u00e9. Snorkel: Rapid training data creation with weak supervision. In Very Large Data Bases Conference (VLDB), 2017. Link\n- Christopher R\u00e9. Overton: A data system for monitoring and improving machine-learned products. In 10th Conference on Innovative Data Systems Research, CIDR 2020, Amsterdam, The Netherlands, January 12-15, 2020, Online Proceedings. Link\n- Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning (ICML), 2019. Link\n- William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2022. Link\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Link\n- Peter J Rousseeuw and Mia Hubert. Robust statistics for outlier detection. Wiley interdisciplinary reviews: Data mining and knowledge discovery, 2011. Link\n- Peter J Rousseeuw and Mia Hubert. Anomaly detection by robust statistics. Wiley interdisciplinary reviews: Data mining and knowledge discovery, 2018. Link\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. Link\n- Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang. Extending the wilds benchmark for unsupervised", "images": [], "items": [{"type": "text", "value": "- Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. Link\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research (JMLR), 2020. Link\n- Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B. Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Beyond web-scraping: Crowd-sourcing a geodiverse datase, 2023. Link\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), 2021. Link\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. Link\n- A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. R\u00e9. Training complex models with multi-task weak supervision. In Association for the Advancement of Artificial Intelligence (AAAI), 2019. Link\n- Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R\u00e9. Data programming: Creating large training sets, quickly. In Advances in Neural Information Processing Systems (NeurIPS), 2016. Link\n- Alexander J Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R\u00e9. Snorkel: Rapid training data creation with weak supervision. In Very Large Data Bases Conference (VLDB), 2017. Link\n- Christopher R\u00e9. Overton: A data system for monitoring and improving machine-learned products. In 10th Conference on Innovative Data Systems Research, CIDR 2020, Amsterdam, The Netherlands, January 12-15, 2020, Online Proceedings. Link\n- Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning (ICML), 2019. Link\n- William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2022. Link\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Link\n- Peter J Rousseeuw and Mia Hubert. Robust statistics for outlier detection. Wiley interdisciplinary reviews: Data mining and knowledge discovery, 2011. Link\n- Peter J Rousseeuw and Mia Hubert. Anomaly detection by robust statistics. Wiley interdisciplinary reviews: Data mining and knowledge discovery, 2018. Link\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. Link\n- Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang. Extending the wilds benchmark for unsupervised", "md": "- Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. Link\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research (JMLR), 2020. Link\n- Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B. Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Beyond web-scraping: Crowd-sourcing a geodiverse datase, 2023. Link\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), 2021. Link\n- Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. Link\n- A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. R\u00e9. Training complex models with multi-task weak supervision. In Association for the Advancement of Artificial Intelligence (AAAI), 2019. Link\n- Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R\u00e9. Data programming: Creating large training sets, quickly. In Advances in Neural Information Processing Systems (NeurIPS), 2016. Link\n- Alexander J Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R\u00e9. Snorkel: Rapid training data creation with weak supervision. In Very Large Data Bases Conference (VLDB), 2017. Link\n- Christopher R\u00e9. Overton: A data system for monitoring and improving machine-learned products. In 10th Conference on Innovative Data Systems Research, CIDR 2020, Amsterdam, The Netherlands, January 12-15, 2020, Online Proceedings. Link\n- Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning (ICML), 2019. Link\n- William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2022. Link\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Link\n- Peter J Rousseeuw and Mia Hubert. Robust statistics for outlier detection. Wiley interdisciplinary reviews: Data mining and knowledge discovery, 2011. Link\n- Peter J Rousseeuw and Mia Hubert. Anomaly detection by robust statistics. Wiley interdisciplinary reviews: Data mining and knowledge discovery, 2018. Link\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. Link\n- Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang. Extending the wilds benchmark for unsupervised"}]}, {"page": 19, "text": "      adaptation. In International Conference on Learning Representations (ICLR), 2022. https:\n      //arxiv.org/abs/2112.05090.\n[128] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\n      Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M:\n      Open dataset of clip-filtered 400 million image-text pairs, 2021. https://arxiv.org/abs/\n      2111.02114.\n[129] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross\n      Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\n      Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert\n      Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next\n      generation image-text models. In Thirty-sixth Conference on Neural Information Processing\n      Systems (NeurIPS), Datasets and Benchmarks Track, 2022. https://openreview.net/\n      forum?id=M3Y74vmsMcY.\n[130] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A\n      core-set approach. In International Conference on Learning Representations (ICLR), 2018.\n      https://openreview.net/forum?id=H1aIuk-RW.\n[131] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\n      cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting\n      of the Association for Computational Linguistics (ACL), 2018. https://aclanthology.\n      org/P18-1238/.\n[132] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang,\n      Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks?, 2021.\n      https://arxiv.org/abs/2107.06383.\n[133] Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, and Frederic Sala.\n      Universalizing weak supervision. In International Conference on Learning Representations\n      (ICLR), 2022. https://openreview.net/forum?id=YpPiNigTzMT.\n[134] Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. CLIP models are few-\n      shot learners: Empirical studies on VQA and visual entailment. In Annual Meeting of the\n      Association for Computational Linguistics (ACL), 2022. https://aclanthology.org/\n      2022.acl-long.421.\n[135] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond\n      neural scaling laws: beating power law scaling via data pruning. In Advances in Neural\n      Information Processing Systems (NeurIPS), 2022. https://openreview.net/forum?id=\n      UmvSlP-PyV.\n[136] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit:\n      Wikipedia-based image text dataset for multimodal multilingual machine learning. In 44th\n      International ACM SIGIR Conference on Research and Development in Information Retrieval,\n      2021. https://arxiv.org/abs/2103.01913.\n[137] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffi      c\n      sign recognition benchmark: a multi-class classification competition.        In International\n      Joint Conference on Neural Networks (IJCNN), 2011. https://ieeexplore.ieee.org/\n      document/6033395.\n[138] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi,\n      Noah A. Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with\n      training dynamics. In Conference on Empirical Methods in Natural Language Processing\n      (EMNLP), 2020. https://aclanthology.org/2020.emnlp-main.746.\n[139] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig\n      Schmidt. Measuring robustness to natural distribution shifts in image classification. In\n      Advances in Neural Information Processing Systems (NeurIPS), 2020. https://dl.acm.\n      org/doi/abs/10.5555/3495724.3497285.\n[140] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas\n      Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research.\n      Communications of the ACM, 2016. https://arxiv.org/abs/1503.01817.\n                                                19", "md": "# References\n\n# References\n\n[128] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open dataset of clip-filtered 400 million image-text pairs, 2021. Link\n\n[129] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022. Link\n\n[130] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations (ICLR), 2018. Link\n\n[131] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics (ACL), 2018. Link\n\n[132] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks?, 2021. Link\n\n[133] Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, and Frederic Sala. Universalizing weak supervision. In International Conference on Learning Representations (ICLR), 2022. Link\n\n[134] Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. CLIP models are few-shot learners: Empirical studies on VQA and visual entailment. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022. Link\n\n[135] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Link\n\n[136] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021. Link\n\n[137] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In International Joint Conference on Neural Networks (IJCNN), 2011. Link\n\n[138] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. Link\n\n[139] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Link\n\n[140] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. Communications of the ACM, 2016. Link", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "[128] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open dataset of clip-filtered 400 million image-text pairs, 2021. Link\n\n[129] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022. Link\n\n[130] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations (ICLR), 2018. Link\n\n[131] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics (ACL), 2018. Link\n\n[132] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks?, 2021. Link\n\n[133] Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, and Frederic Sala. Universalizing weak supervision. In International Conference on Learning Representations (ICLR), 2022. Link\n\n[134] Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. CLIP models are few-shot learners: Empirical studies on VQA and visual entailment. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022. Link\n\n[135] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Link\n\n[136] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021. Link\n\n[137] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In International Joint Conference on Neural Networks (IJCNN), 2011. Link\n\n[138] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. Link\n\n[139] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Link\n\n[140] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. Communications of the ACM, 2016. Link", "md": "[128] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open dataset of clip-filtered 400 million image-text pairs, 2021. Link\n\n[129] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022. Link\n\n[130] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations (ICLR), 2018. Link\n\n[131] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational Linguistics (ACL), 2018. Link\n\n[132] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks?, 2021. Link\n\n[133] Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, and Frederic Sala. Universalizing weak supervision. In International Conference on Learning Representations (ICLR), 2022. Link\n\n[134] Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. CLIP models are few-shot learners: Empirical studies on VQA and visual entailment. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022. Link\n\n[135] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Link\n\n[136] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021. Link\n\n[137] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In International Joint Conference on Neural Networks (IJCNN), 2011. Link\n\n[138] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. Link\n\n[139] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Link\n\n[140] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. Communications of the ACM, 2016. Link"}]}, {"page": 20, "text": "[141] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua\n       Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural\n       network learning. In International Conference on Learning Representations (ICLR), 2018.\n       https://arxiv.org/abs/1812.05159.\n[142] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation\n       equivariant CNNs for digital pathology, 2018. https://arxiv.org/abs/1806.03962.\n[143] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global\n       representations by penalizing local predictive power. In Advances in Neural Information\n       Processing Systems (NeurIPS), 2019. https://arxiv.org/abs/1905.13549.\n[144] Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b,\n       2023. https://arxiv.org/abs/2303.12733.\n[145] Kai Wei, Rishabh Iyer, and Jeff Bilmes.            Submodularity in data subset selection and\n       active learning. In International Conference on Machine Learning (ICML), 2015. https:\n       //proceedings.mlr.press/v37/wei15.html.\n[146] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva.\n       Sun database: Exploring a large collection of scene categories.            International Journal\n       of Computer Vision (IJCV), 2016.         https://link.springer.com/article/10.1007/\n       s11263-014-0748-y.\n[147] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer\n       datasets: filtering and balancing the distribution of the people subtree in the imagenet hierarchy.\n       In Conference on Fairness, Accountability, and Transparency (FAccT), 2020. https://\n       arxiv.org/abs/1912.07726.\n[148] Kaiyu Yang, Jacqueline H Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face\n       obfuscation in ImageNet. In International Conference on Machine Learning (ICML), 2022.\n       https://arxiv.org/abs/2103.06191.\n[149] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang,\n       Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image\n       pre-training. In International Conference on Learning Representations (ICLR), 2022. https:\n       //arxiv.org/abs/2111.07783.\n[150] Shuhei Yokoo.       Contrastive learning with large memory bank and negative embedding\n       subtraction for accurate copy detection, 2021. https://arxiv.org/abs/2112.04323.\n[151] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions\n       to visual denotations: New similarity metrics for semantic inference over event descriptions.\n       Transactions of the Association for Computational Linguistics, 2014.                    https://\n       aclanthology.org/Q14-1006/.\n[152] Dantong Yu, Gholamhosein Sheikholeslami, and Aidong Zhang. Findout: Finding outliers in\n       very large datasets. Knowledge and information Systems, 2002. https://link.springer.\n       com/article/10.1007/s101150200013.\n[153] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong\n       Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for\n       computer vision, 2021. https://arxiv.org/abs/2111.11432.\n[154] Man-Ching Yuen, Irwin King, and Kwong-Sak Leung. A survey of crowdsourcing systems.\n       In SocialCom. IEEE, 2011. https://ieeexplore.ieee.org/document/6113213.\n[155] Matei Zaharia, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur\n       Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J Franklin, et al. Apache\n       spark: a unified engine for big data processing. Communications of the ACM, 2016. https:\n       //dl.acm.org/doi/10.1145/2934664.\n[156] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme,\n       Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy,\n       Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet,\n       Sylvain Gelly, and Neil Houlsby.        The visual task adaptation benchmark, 2019.          http:\n       //arxiv.org/abs/1910.04867.\n                                                    20", "md": "- Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. In International Conference on Learning Representations (ICLR), 2018. Link\n- Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology, 2018. Link\n- Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Link\n- Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b, 2023. Link\n- Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In International Conference on Machine Learning (ICML), 2015. Link\n- Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision (IJCV), 2016. Link\n- Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: filtering and balancing the distribution of the people subtree in the imagenet hierarchy. In Conference on Fairness, Accountability, and Transparency (FAccT), 2020. Link\n- Kaiyu Yang, Jacqueline H Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in ImageNet. In International Conference on Machine Learning (ICML), 2022. Link\n- Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. In International Conference on Learning Representations (ICLR), 2022. Link\n- Shuhei Yokoo. Contrastive learning with large memory bank and negative embedding subtraction for accurate copy detection, 2021. Link\n- Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2014. Link\n- Dantong Yu, Gholamhosein Sheikholeslami, and Aidong Zhang. Findout: Finding outliers in very large datasets. Knowledge and information Systems, 2002. Link\n- Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision, 2021. Link\n- Man-Ching Yuen, Irwin King, and Kwong-Sak Leung. A survey of crowdsourcing systems. In SocialCom. IEEE, 2011. Link\n- Matei Zaharia, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J Franklin, et al. Apache spark: a unified engine for big data processing. Communications of the ACM, 2016. Link\n- Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. The visual task adaptation benchmark, 2019. Link", "images": [], "items": [{"type": "text", "value": "- Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. In International Conference on Learning Representations (ICLR), 2018. Link\n- Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology, 2018. Link\n- Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Link\n- Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b, 2023. Link\n- Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In International Conference on Machine Learning (ICML), 2015. Link\n- Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision (IJCV), 2016. Link\n- Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: filtering and balancing the distribution of the people subtree in the imagenet hierarchy. In Conference on Fairness, Accountability, and Transparency (FAccT), 2020. Link\n- Kaiyu Yang, Jacqueline H Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in ImageNet. In International Conference on Machine Learning (ICML), 2022. Link\n- Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. In International Conference on Learning Representations (ICLR), 2022. Link\n- Shuhei Yokoo. Contrastive learning with large memory bank and negative embedding subtraction for accurate copy detection, 2021. Link\n- Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2014. Link\n- Dantong Yu, Gholamhosein Sheikholeslami, and Aidong Zhang. Findout: Finding outliers in very large datasets. Knowledge and information Systems, 2002. Link\n- Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision, 2021. Link\n- Man-Ching Yuen, Irwin King, and Kwong-Sak Leung. A survey of crowdsourcing systems. In SocialCom. IEEE, 2011. Link\n- Matei Zaharia, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J Franklin, et al. Apache spark: a unified engine for big data processing. Communications of the ACM, 2016. Link\n- Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. The visual task adaptation benchmark, 2019. Link", "md": "- Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. In International Conference on Learning Representations (ICLR), 2018. Link\n- Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology, 2018. Link\n- Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Link\n- Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b, 2023. Link\n- Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In International Conference on Machine Learning (ICML), 2015. Link\n- Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision (IJCV), 2016. Link\n- Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: filtering and balancing the distribution of the people subtree in the imagenet hierarchy. In Conference on Fairness, Accountability, and Transparency (FAccT), 2020. Link\n- Kaiyu Yang, Jacqueline H Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in ImageNet. In International Conference on Machine Learning (ICML), 2022. Link\n- Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. In International Conference on Learning Representations (ICLR), 2022. Link\n- Shuhei Yokoo. Contrastive learning with large memory bank and negative embedding subtraction for accurate copy detection, 2021. Link\n- Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2014. Link\n- Dantong Yu, Gholamhosein Sheikholeslami, and Aidong Zhang. Findout: Finding outliers in very large datasets. Knowledge and information Systems, 2002. Link\n- Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision, 2021. Link\n- Man-Ching Yuen, Irwin King, and Kwong-Sak Leung. A survey of crowdsourcing systems. In SocialCom. IEEE, 2011. Link\n- Matei Zaharia, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J Franklin, et al. Apache spark: a unified engine for big data processing. Communications of the ACM, 2016. Link\n- Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. The visual task adaptation benchmark, 2019. Link"}]}, {"page": 21, "text": "[157] Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander\n      Ratner. WRENCH: A comprehensive benchmark for weak supervision. In NeurIPS, 2021.\n      URL https://openreview.net/forum?id=Q9SKS5k8io.\n[158] Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. A survey on\n      programmatic weak supervision, 2022. https://arxiv.org/abs/2202.05433.\n[159] Zhifei Zhang, Yang Song, and Hairong Qi.      Age progression/regression by conditional\n      adversarial autoencoder. In Conference on Computer Vision and Pattern Recognition (CVPR),\n      2017. https://arxiv.org/abs/1702.08423.\n[160] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, and Ishan Misra. Detecting\n      twenty-thousand classes using image-level supervision. In European Conference on Computer\n      Vision (ECCV), 2022. https://arxiv.org/abs/2201.02605.\n                                              21", "md": "# References\n\n## References\n\n1. Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. WRENCH: A comprehensive benchmark for weak supervision. In NeurIPS, 2021. URL https://openreview.net/forum?id=Q9SKS5k8io.\n2. Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. A survey on programmatic weak supervision, 2022. https://arxiv.org/abs/2202.05433.\n3. Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. https://arxiv.org/abs/1702.08423.\n4. Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In European Conference on Computer Vision (ECCV), 2022. https://arxiv.org/abs/2201.02605.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. WRENCH: A comprehensive benchmark for weak supervision. In NeurIPS, 2021. URL https://openreview.net/forum?id=Q9SKS5k8io.\n2. Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. A survey on programmatic weak supervision, 2022. https://arxiv.org/abs/2202.05433.\n3. Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. https://arxiv.org/abs/1702.08423.\n4. Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In European Conference on Computer Vision (ECCV), 2022. https://arxiv.org/abs/2201.02605.", "md": "1. Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. WRENCH: A comprehensive benchmark for weak supervision. In NeurIPS, 2021. URL https://openreview.net/forum?id=Q9SKS5k8io.\n2. Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. A survey on programmatic weak supervision, 2022. https://arxiv.org/abs/2202.05433.\n3. Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. https://arxiv.org/abs/1702.08423.\n4. Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In European Conference on Computer Vision (ECCV), 2022. https://arxiv.org/abs/2201.02605."}]}, {"page": 22, "text": "Appendix\n Contents\n 1   Introduction                                                                                       1\n 2   Related Work                                                                                       3\n 3  The DATACOMP benchmark                                                                              4\n     3.1   Competition design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       4\n     3.2   COMMONPOOL generation, for the filtering track . . . . . . . . . . . . . . . . . .           5\n     3.3   The bring your own data (BYOD) track . . . . . . . . . . . . . . . . . . . . . . .           5\n     3.4   Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     5\n     3.5   Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     6\n 4   Baselines                                                                                          6\n     4.1   Filtering baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    6\n     4.2   BYOD baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         7\n 5   Results and discussion                                                                             7\n     5.1   Building better datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     7\n     5.2   DATACOMP design analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           8\n     5.3   Evaluation trends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      9\n 6   Limitations and conclusion                                                                         9\n A Benchmark rules                                                                                    24\n     A.1   Filtering track rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  24\n     A.2   Bring your own data track: amendments . . . . . . . . . . . . . . . . . . . . . . .        24\n B   Contributions                                                                                    25\n     B.1   Candidate pool    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\n     B.2   Participant tooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\n     B.3   Baselines   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\n     B.4   Leadership and Advising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      25\n C Additional related work                                                                            26\n D Parsing Common Crawl                                                                               26\n E   Not safe for work (NSFW) filtering                                                               27\n F   Deduplication against evaluation sets                                                            27\n                                                    22", "md": "# Appendix\n\n## Contents\n\n1. Introduction - 1\n2. Related Work - 3\n3. The DATACOMP benchmark - 4\n- Competition design - 4\n- COMMONPOOL generation, for the filtering track - 5\n- The bring your own data (BYOD) track - 5\n- Training - 5\n- Evaluation - 6\n4. Baselines - 6\n- Filtering baselines - 6\n- BYOD baselines - 7\n5. Results and discussion - 7\n- Building better datasets - 7\n- DATACOMP design analyses - 8\n- Evaluation trends - 9\n6. Limitations and conclusion - 9\n\n### A Benchmark rules - 24\n\n- Filtering track rules - 24\n- Bring your own data track: amendments - 24\n\n### B Contributions - 25\n\n- Candidate pool - 25\n- Participant tooling - 25\n- Baselines - 25\n- Leadership and Advising - 25\n\n### C Additional related work - 26\n\n### D Parsing Common Crawl - 26\n\n### E Not safe for work (NSFW) filtering - 27\n\n### F Deduplication against evaluation sets - 27\n\nPage number: 22", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Appendix", "md": "# Appendix"}, {"type": "heading", "lvl": 2, "value": "Contents", "md": "## Contents"}, {"type": "text", "value": "1. Introduction - 1\n2. Related Work - 3\n3. The DATACOMP benchmark - 4\n- Competition design - 4\n- COMMONPOOL generation, for the filtering track - 5\n- The bring your own data (BYOD) track - 5\n- Training - 5\n- Evaluation - 6\n4. Baselines - 6\n- Filtering baselines - 6\n- BYOD baselines - 7\n5. Results and discussion - 7\n- Building better datasets - 7\n- DATACOMP design analyses - 8\n- Evaluation trends - 9\n6. Limitations and conclusion - 9", "md": "1. Introduction - 1\n2. Related Work - 3\n3. The DATACOMP benchmark - 4\n- Competition design - 4\n- COMMONPOOL generation, for the filtering track - 5\n- The bring your own data (BYOD) track - 5\n- Training - 5\n- Evaluation - 6\n4. Baselines - 6\n- Filtering baselines - 6\n- BYOD baselines - 7\n5. Results and discussion - 7\n- Building better datasets - 7\n- DATACOMP design analyses - 8\n- Evaluation trends - 9\n6. Limitations and conclusion - 9"}, {"type": "heading", "lvl": 3, "value": "A Benchmark rules - 24", "md": "### A Benchmark rules - 24"}, {"type": "text", "value": "- Filtering track rules - 24\n- Bring your own data track: amendments - 24", "md": "- Filtering track rules - 24\n- Bring your own data track: amendments - 24"}, {"type": "heading", "lvl": 3, "value": "B Contributions - 25", "md": "### B Contributions - 25"}, {"type": "text", "value": "- Candidate pool - 25\n- Participant tooling - 25\n- Baselines - 25\n- Leadership and Advising - 25", "md": "- Candidate pool - 25\n- Participant tooling - 25\n- Baselines - 25\n- Leadership and Advising - 25"}, {"type": "heading", "lvl": 3, "value": "C Additional related work - 26", "md": "### C Additional related work - 26"}, {"type": "heading", "lvl": 3, "value": "D Parsing Common Crawl - 26", "md": "### D Parsing Common Crawl - 26"}, {"type": "heading", "lvl": 3, "value": "E Not safe for work (NSFW) filtering - 27", "md": "### E Not safe for work (NSFW) filtering - 27"}, {"type": "heading", "lvl": 3, "value": "F Deduplication against evaluation sets - 27", "md": "### F Deduplication against evaluation sets - 27"}, {"type": "text", "value": "Page number: 22", "md": "Page number: 22"}]}, {"page": 23, "text": "G Face blurring                                                                                       29\nH DATACOMP COMMONPOOL creation pipeline                                                               31\nI   COMMONPOOL statistics                                                                             32\nJ   Efficient training on data subsets                                                                35\nK Effect of duplicates in the training data                                                           35\nL   Hyperparameter ablations                                                                          36\n    L.1   Batch size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    36\n    L.2   Model architecture     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  36\n    L.3   Number of training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      36\nM Detector-based baselines                                                                            39\nN Training details                                                                                    40\nO Evaluation details                                                                                  40\n    O.1 Visual Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         42\nP   Baseline details                                                                                  43\n    P.1   Filtering track . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   47\n    P.2   BYOD track       . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  48\n          P.2.1   Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    49\nQ Fairness and biases                                                                                 49\n    Q.1   Diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   49\n    Q.2   Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    50\nR Extra figures and tables                                                                            53\nS   Datasheet                                                                                         60\n    S.1   Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    60\n    S.2   Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     60\n    S.3   Collection Process     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  62\n    S.4   Preprocessing, Cleaning, and/or Labeling       . . . . . . . . . . . . . . . . . . . . . .  63\n    S.5   Uses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    64\n    S.6   Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    65\n    S.7   Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     65\n                                                    23", "md": "# Document\n\n## Face blurring\n\n29\n\n## DATACOMP COMMONPOOL creation pipeline\n\n31\n\n## COMMONPOOL statistics\n\n32\n\n## Efficient training on data subsets\n\n35\n\n## Effect of duplicates in the training data\n\n35\n\n## Hyperparameter ablations\n\n|L.1 Batch size|36|\n|---|---|\n|L.2 Model architecture|36|\n|L.3 Number of training steps|36|\n\n## Detector-based baselines\n\n39\n\n## Training details\n\n40\n\n## Evaluation details\n\n40\n\n## Visual Question Answering\n\n42\n\n## Baseline details\n\n|P.1 Filtering track|47|\n|---|---|\n|P.2 BYOD track|48|\n|P.2.1 Additional results|49|\n\n## Fairness and biases\n\n|Q.1 Diversity|49|\n|---|---|\n|Q.2 Fairness|50|\n\n## Extra figures and tables\n\n53\n\n## Datasheet\n\n|S.1 Motivation|60|\n|---|---|\n|S.2 Composition|60|\n|S.3 Collection Process|62|\n|S.4 Preprocessing, Cleaning, and/or Labeling|63|\n|S.5 Uses|64|\n|S.6 Distribution|65|\n|S.7 Maintenance|65|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Face blurring", "md": "## Face blurring"}, {"type": "text", "value": "29", "md": "29"}, {"type": "heading", "lvl": 2, "value": "DATACOMP COMMONPOOL creation pipeline", "md": "## DATACOMP COMMONPOOL creation pipeline"}, {"type": "text", "value": "31", "md": "31"}, {"type": "heading", "lvl": 2, "value": "COMMONPOOL statistics", "md": "## COMMONPOOL statistics"}, {"type": "text", "value": "32", "md": "32"}, {"type": "heading", "lvl": 2, "value": "Efficient training on data subsets", "md": "## Efficient training on data subsets"}, {"type": "text", "value": "35", "md": "35"}, {"type": "heading", "lvl": 2, "value": "Effect of duplicates in the training data", "md": "## Effect of duplicates in the training data"}, {"type": "text", "value": "35", "md": "35"}, {"type": "heading", "lvl": 2, "value": "Hyperparameter ablations", "md": "## Hyperparameter ablations"}, {"type": "table", "rows": [["L.1 Batch size", "36"], ["L.2 Model architecture", "36"], ["L.3 Number of training steps", "36"]], "md": "|L.1 Batch size|36|\n|---|---|\n|L.2 Model architecture|36|\n|L.3 Number of training steps|36|", "isPerfectTable": true, "csv": "\"L.1 Batch size\",\"36\"\n\"L.2 Model architecture\",\"36\"\n\"L.3 Number of training steps\",\"36\""}, {"type": "heading", "lvl": 2, "value": "Detector-based baselines", "md": "## Detector-based baselines"}, {"type": "text", "value": "39", "md": "39"}, {"type": "heading", "lvl": 2, "value": "Training details", "md": "## Training details"}, {"type": "text", "value": "40", "md": "40"}, {"type": "heading", "lvl": 2, "value": "Evaluation details", "md": "## Evaluation details"}, {"type": "text", "value": "40", "md": "40"}, {"type": "heading", "lvl": 2, "value": "Visual Question Answering", "md": "## Visual Question Answering"}, {"type": "text", "value": "42", "md": "42"}, {"type": "heading", "lvl": 2, "value": "Baseline details", "md": "## Baseline details"}, {"type": "table", "rows": [["P.1 Filtering track", "47"], ["P.2 BYOD track", "48"], ["P.2.1 Additional results", "49"]], "md": "|P.1 Filtering track|47|\n|---|---|\n|P.2 BYOD track|48|\n|P.2.1 Additional results|49|", "isPerfectTable": true, "csv": "\"P.1 Filtering track\",\"47\"\n\"P.2 BYOD track\",\"48\"\n\"P.2.1 Additional results\",\"49\""}, {"type": "heading", "lvl": 2, "value": "Fairness and biases", "md": "## Fairness and biases"}, {"type": "table", "rows": [["Q.1 Diversity", "49"], ["Q.2 Fairness", "50"]], "md": "|Q.1 Diversity|49|\n|---|---|\n|Q.2 Fairness|50|", "isPerfectTable": true, "csv": "\"Q.1 Diversity\",\"49\"\n\"Q.2 Fairness\",\"50\""}, {"type": "heading", "lvl": 2, "value": "Extra figures and tables", "md": "## Extra figures and tables"}, {"type": "text", "value": "53", "md": "53"}, {"type": "heading", "lvl": 2, "value": "Datasheet", "md": "## Datasheet"}, {"type": "table", "rows": [["S.1 Motivation", "60"], ["S.2 Composition", "60"], ["S.3 Collection Process", "62"], ["S.4 Preprocessing, Cleaning, and/or Labeling", "63"], ["S.5 Uses", "64"], ["S.6 Distribution", "65"], ["S.7 Maintenance", "65"]], "md": "|S.1 Motivation|60|\n|---|---|\n|S.2 Composition|60|\n|S.3 Collection Process|62|\n|S.4 Preprocessing, Cleaning, and/or Labeling|63|\n|S.5 Uses|64|\n|S.6 Distribution|65|\n|S.7 Maintenance|65|", "isPerfectTable": true, "csv": "\"S.1 Motivation\",\"60\"\n\"S.2 Composition\",\"60\"\n\"S.3 Collection Process\",\"62\"\n\"S.4 Preprocessing, Cleaning, and/or Labeling\",\"63\"\n\"S.5 Uses\",\"64\"\n\"S.6 Distribution\",\"65\"\n\"S.7 Maintenance\",\"65\""}]}, {"page": 24, "text": "A     Benchmark rules\nWe provide concrete rules below for the two competition tracks that comprise DATACOMP: filtering\nand BYOD. Additionally, we provide a checklist, which encourages participants to specify design\ndecisions, which allows for more granular comparison between submissions.\nA.1    Filtering track rules\n        \u2022 Participants can enter submissions for one or many different scales: small, medium, large\n          or xlarge, which represent the raw number of image-text pairs in CommonPool that should\n          be filtered.\n        \u2022 After choosing a scale, participants generate a list of uids, where each uid refers to a\n          COMMONPOOL sample. The list of uids is used to recover image-text pairs from the pool,\n          which is used for downstream CLIP training.\n        \u2022 Duplicate uids are allowed.\n        \u2022 Participants are not allowed to modify the training procedure.                   Hence, changing\n          hyperparameters, model architecture, optimizer, compute budget, or number of training\n          steps is not allowed. Changing any other training details is also not allowed.\n        \u2022 Participants are strongly encouraged to submit and open-source both the list of uids and the\n          code used to generate this list; however, this is not required.\n        \u2022 To avoid overfitting, we do not permit running any code or algorithmic dependence on the\n          test images of the evaluation tasks. However, use of other images associated with these tasks\n          (e.g., supervised training sets) is permitted.\n        \u2022 Participants can use templates or class labels from the downstream tasks in their filtering\n          algorithms.\nFor clarity, we include some examples of permitted and forbidden uses:\n      \u2713   We permit using the ImageNet class label \u201ctriceratops\u201d in a filtering algorithm.\n      \u00d7 We forbid examining individual or aggregate predictions on the test sets of the evaluation\n          tasks.\nA.2    Bring your own data track: amendments\nTo facilitate more open-ended exploration, we provide amendments to the Track 1 competition to\nallow for more diverse submissions in Track 2.\n        \u2022 Participants are allowed to augment COMMONPOOL data with existing datasets, so long as\n          these data sources do not contain test images from the evaluation tasks. Participants can use\n          data from any COMMONPOOL; however, they are not required to do so.\n        \u2022 Assembling one\u2019s own dataset is allowed; however, test images from the evaluation tasks can\n          neither be contained nor otherwise used to construct said dataset. We encourage releasing the\n          image urls or the images themselves in addition to the text for each image. We also encourage\n          rigorous documentation of face-blurring and other data safety checks (see Section 3.2 for\n          more details). We reserve the right to run our own safety code on participant provided data\n          and disqualify entries that do not meet adequate safety standards.\nChecklist. The following checklist provides the basis for more fine-grained comparison between\nsubmissions.\n      \u25a1   Images from the evaluation tasks are included in my submission. If yes, please specify\n          which datasets.\n      \u25a1   I used an existing datasets (e.g., YFCC100M [140]) in my submission. If yes, please specify\n          which datasets. (Note: applies to BYOD only)\n      \u25a1   I curated my own data. If yes, please provide (1) image data or urls, (2) text for each image,\n          (3) list of safety steps taken including but not limited to face blurring, explicit content image\n          and text filtering. (Note: applies to BYOD only)\n                                                      24", "md": "# DATACOMP Benchmark Rules\n\n## A Benchmark rules\n\nWe provide concrete rules below for the two competition tracks that comprise DATACOMP: filtering and BYOD. Additionally, we provide a checklist, which encourages participants to specify design decisions, which allows for more granular comparison between submissions.\n\n### A.1 Filtering track rules\n\n- Participants can enter submissions for one or many different scales: small, medium, large or xlarge, which represent the raw number of image-text pairs in CommonPool that should be filtered.\n- After choosing a scale, participants generate a list of uids, where each uid refers to a COMMONPOOL sample. The list of uids is used to recover image-text pairs from the pool, which is used for downstream CLIP training.\n- Duplicate uids are allowed.\n- Participants are not allowed to modify the training procedure. Hence, changing hyperparameters, model architecture, optimizer, compute budget, or number of training steps is not allowed. Changing any other training details is also not allowed.\n- Participants are strongly encouraged to submit and open-source both the list of uids and the code used to generate this list; however, this is not required.\n- To avoid overfitting, we do not permit running any code or algorithmic dependence on the test images of the evaluation tasks. However, use of other images associated with these tasks (e.g., supervised training sets) is permitted.\n- Participants can use templates or class labels from the downstream tasks in their filtering algorithms.\n\nFor clarity, we include some examples of permitted and forbidden uses:\n\n- \u2713 We permit using the ImageNet class label \"triceratops\" in a filtering algorithm.\n- \u00d7 We forbid examining individual or aggregate predictions on the test sets of the evaluation tasks.\n\n### A.2 Bring your own data track: amendments\n\nTo facilitate more open-ended exploration, we provide amendments to the Track 1 competition to allow for more diverse submissions in Track 2.\n\n- Participants are allowed to augment COMMONPOOL data with existing datasets, so long as these data sources do not contain test images from the evaluation tasks. Participants can use data from any COMMONPOOL; however, they are not required to do so.\n- Assembling one\u2019s own dataset is allowed; however, test images from the evaluation tasks can neither be contained nor otherwise used to construct said dataset. We encourage releasing the image urls or the images themselves in addition to the text for each image. We also encourage rigorous documentation of face-blurring and other data safety checks (see Section 3.2 for more details). We reserve the right to run our own safety code on participant provided data and disqualify entries that do not meet adequate safety standards.\n\n### Checklist\n\nThe following checklist provides the basis for more fine-grained comparison between submissions.\n\n- \u2610 Images from the evaluation tasks are included in my submission. If yes, please specify which datasets.\n- \u2610 I used an existing datasets (e.g., YFCC100M [140]) in my submission. If yes, please specify which datasets. (Note: applies to BYOD only)\n- \u2610 I curated my own data. If yes, please provide (1) image data or urls, (2) text for each image, (3) list of safety steps taken including but not limited to face blurring, explicit content image and text filtering. (Note: applies to BYOD only)", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "DATACOMP Benchmark Rules", "md": "# DATACOMP Benchmark Rules"}, {"type": "heading", "lvl": 2, "value": "A Benchmark rules", "md": "## A Benchmark rules"}, {"type": "text", "value": "We provide concrete rules below for the two competition tracks that comprise DATACOMP: filtering and BYOD. Additionally, we provide a checklist, which encourages participants to specify design decisions, which allows for more granular comparison between submissions.", "md": "We provide concrete rules below for the two competition tracks that comprise DATACOMP: filtering and BYOD. Additionally, we provide a checklist, which encourages participants to specify design decisions, which allows for more granular comparison between submissions."}, {"type": "heading", "lvl": 3, "value": "A.1 Filtering track rules", "md": "### A.1 Filtering track rules"}, {"type": "text", "value": "- Participants can enter submissions for one or many different scales: small, medium, large or xlarge, which represent the raw number of image-text pairs in CommonPool that should be filtered.\n- After choosing a scale, participants generate a list of uids, where each uid refers to a COMMONPOOL sample. The list of uids is used to recover image-text pairs from the pool, which is used for downstream CLIP training.\n- Duplicate uids are allowed.\n- Participants are not allowed to modify the training procedure. Hence, changing hyperparameters, model architecture, optimizer, compute budget, or number of training steps is not allowed. Changing any other training details is also not allowed.\n- Participants are strongly encouraged to submit and open-source both the list of uids and the code used to generate this list; however, this is not required.\n- To avoid overfitting, we do not permit running any code or algorithmic dependence on the test images of the evaluation tasks. However, use of other images associated with these tasks (e.g., supervised training sets) is permitted.\n- Participants can use templates or class labels from the downstream tasks in their filtering algorithms.\n\nFor clarity, we include some examples of permitted and forbidden uses:\n\n- \u2713 We permit using the ImageNet class label \"triceratops\" in a filtering algorithm.\n- \u00d7 We forbid examining individual or aggregate predictions on the test sets of the evaluation tasks.", "md": "- Participants can enter submissions for one or many different scales: small, medium, large or xlarge, which represent the raw number of image-text pairs in CommonPool that should be filtered.\n- After choosing a scale, participants generate a list of uids, where each uid refers to a COMMONPOOL sample. The list of uids is used to recover image-text pairs from the pool, which is used for downstream CLIP training.\n- Duplicate uids are allowed.\n- Participants are not allowed to modify the training procedure. Hence, changing hyperparameters, model architecture, optimizer, compute budget, or number of training steps is not allowed. Changing any other training details is also not allowed.\n- Participants are strongly encouraged to submit and open-source both the list of uids and the code used to generate this list; however, this is not required.\n- To avoid overfitting, we do not permit running any code or algorithmic dependence on the test images of the evaluation tasks. However, use of other images associated with these tasks (e.g., supervised training sets) is permitted.\n- Participants can use templates or class labels from the downstream tasks in their filtering algorithms.\n\nFor clarity, we include some examples of permitted and forbidden uses:\n\n- \u2713 We permit using the ImageNet class label \"triceratops\" in a filtering algorithm.\n- \u00d7 We forbid examining individual or aggregate predictions on the test sets of the evaluation tasks."}, {"type": "heading", "lvl": 3, "value": "A.2 Bring your own data track: amendments", "md": "### A.2 Bring your own data track: amendments"}, {"type": "text", "value": "To facilitate more open-ended exploration, we provide amendments to the Track 1 competition to allow for more diverse submissions in Track 2.\n\n- Participants are allowed to augment COMMONPOOL data with existing datasets, so long as these data sources do not contain test images from the evaluation tasks. Participants can use data from any COMMONPOOL; however, they are not required to do so.\n- Assembling one\u2019s own dataset is allowed; however, test images from the evaluation tasks can neither be contained nor otherwise used to construct said dataset. We encourage releasing the image urls or the images themselves in addition to the text for each image. We also encourage rigorous documentation of face-blurring and other data safety checks (see Section 3.2 for more details). We reserve the right to run our own safety code on participant provided data and disqualify entries that do not meet adequate safety standards.", "md": "To facilitate more open-ended exploration, we provide amendments to the Track 1 competition to allow for more diverse submissions in Track 2.\n\n- Participants are allowed to augment COMMONPOOL data with existing datasets, so long as these data sources do not contain test images from the evaluation tasks. Participants can use data from any COMMONPOOL; however, they are not required to do so.\n- Assembling one\u2019s own dataset is allowed; however, test images from the evaluation tasks can neither be contained nor otherwise used to construct said dataset. We encourage releasing the image urls or the images themselves in addition to the text for each image. We also encourage rigorous documentation of face-blurring and other data safety checks (see Section 3.2 for more details). We reserve the right to run our own safety code on participant provided data and disqualify entries that do not meet adequate safety standards."}, {"type": "heading", "lvl": 3, "value": "Checklist", "md": "### Checklist"}, {"type": "text", "value": "The following checklist provides the basis for more fine-grained comparison between submissions.\n\n- \u2610 Images from the evaluation tasks are included in my submission. If yes, please specify which datasets.\n- \u2610 I used an existing datasets (e.g., YFCC100M [140]) in my submission. If yes, please specify which datasets. (Note: applies to BYOD only)\n- \u2610 I curated my own data. If yes, please provide (1) image data or urls, (2) text for each image, (3) list of safety steps taken including but not limited to face blurring, explicit content image and text filtering. (Note: applies to BYOD only)", "md": "The following checklist provides the basis for more fine-grained comparison between submissions.\n\n- \u2610 Images from the evaluation tasks are included in my submission. If yes, please specify which datasets.\n- \u2610 I used an existing datasets (e.g., YFCC100M [140]) in my submission. If yes, please specify which datasets. (Note: applies to BYOD only)\n- \u2610 I curated my own data. If yes, please provide (1) image data or urls, (2) text for each image, (3) list of safety steps taken including but not limited to face blurring, explicit content image and text filtering. (Note: applies to BYOD only)"}]}, {"page": 25, "text": "B    Contributions\nFor this section, contributors are ordered alphabetically.\nB.1   Candidate pool\nCandidate pool lead. Vaishaal Shankar\nData collection. Romain Beaumont, Vaishaal Shankar\nPre-processing and metadata. Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak\nGadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar,\nGeorge Smyrnis (face blurring lead)\nB.2   Participant tooling\nParticipant tooling lead. Gabriel Ilharco\nResharder. Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco,\nVivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis\nTraining. Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell\nWortsman (lead)\nEvaluation. Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco\nAdditional infrastructure. Stephen Mussmann, Sarah Pratt\nB.3   Baselines\nBaselines lead. Yair Carmon\nFiltering track. Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco,\nKalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu\nZhang (co-lead)\nBYOD track. Gabriel Ilharco, Thao Nguyen\nExperiment babysitting. Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre\nB.4   Leadership and Advising\nAdvising. Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh\nHajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander\nRatner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\nLeadership. Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt,\nVaishaal Shankar\nOverall project lead. Ludwig Schmidt\n                                                  25", "md": "## Contributions\n\nFor this section, contributors are ordered alphabetically.\n\n### Candidate pool\n\nCandidate pool lead: Vaishaal Shankar\n\nData collection: Romain Beaumont, Vaishaal Shankar\n\nPre-processing and metadata: Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak Gadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar, George Smyrnis (face blurring lead)\n\n### Participant tooling\n\nParticipant tooling lead: Gabriel Ilharco\n\nResharder: Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco, Vivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis\n\nTraining: Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell Wortsman (lead)\n\nEvaluation: Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco\n\nAdditional infrastructure: Stephen Mussmann, Sarah Pratt\n\n### Baselines\n\nBaselines lead: Yair Carmon\n\nFiltering track: Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco, Kalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu Zhang (co-lead)\n\nBYOD track: Gabriel Ilharco, Thao Nguyen\n\nExperiment babysitting: Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre\n\n### Leadership and Advising\n\nAdvising: Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\n\nLeadership: Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar\n\nOverall project lead: Ludwig Schmidt", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Contributions", "md": "## Contributions"}, {"type": "text", "value": "For this section, contributors are ordered alphabetically.", "md": "For this section, contributors are ordered alphabetically."}, {"type": "heading", "lvl": 3, "value": "Candidate pool", "md": "### Candidate pool"}, {"type": "text", "value": "Candidate pool lead: Vaishaal Shankar\n\nData collection: Romain Beaumont, Vaishaal Shankar\n\nPre-processing and metadata: Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak Gadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar, George Smyrnis (face blurring lead)", "md": "Candidate pool lead: Vaishaal Shankar\n\nData collection: Romain Beaumont, Vaishaal Shankar\n\nPre-processing and metadata: Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak Gadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar, George Smyrnis (face blurring lead)"}, {"type": "heading", "lvl": 3, "value": "Participant tooling", "md": "### Participant tooling"}, {"type": "text", "value": "Participant tooling lead: Gabriel Ilharco\n\nResharder: Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco, Vivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis\n\nTraining: Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell Wortsman (lead)\n\nEvaluation: Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco\n\nAdditional infrastructure: Stephen Mussmann, Sarah Pratt", "md": "Participant tooling lead: Gabriel Ilharco\n\nResharder: Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco, Vivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis\n\nTraining: Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell Wortsman (lead)\n\nEvaluation: Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco\n\nAdditional infrastructure: Stephen Mussmann, Sarah Pratt"}, {"type": "heading", "lvl": 3, "value": "Baselines", "md": "### Baselines"}, {"type": "text", "value": "Baselines lead: Yair Carmon\n\nFiltering track: Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco, Kalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu Zhang (co-lead)\n\nBYOD track: Gabriel Ilharco, Thao Nguyen\n\nExperiment babysitting: Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre", "md": "Baselines lead: Yair Carmon\n\nFiltering track: Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco, Kalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu Zhang (co-lead)\n\nBYOD track: Gabriel Ilharco, Thao Nguyen\n\nExperiment babysitting: Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre"}, {"type": "heading", "lvl": 3, "value": "Leadership and Advising", "md": "### Leadership and Advising"}, {"type": "text", "value": "Advising: Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\n\nLeadership: Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar\n\nOverall project lead: Ludwig Schmidt", "md": "Advising: Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\n\nLeadership: Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar\n\nOverall project lead: Ludwig Schmidt"}]}, {"page": 26, "text": "C    Additional related work\nHere we expand on the related work described in Section 2.\nImage dataset safety is an active area of research, especially in the context of large-scale dataset\nconstruction. In addition to Birhane et al. [15], who study problematic content in LAION-400M,\nYang et al. [147] study the ImageNet dataset and reveal limitations associated with the ImageNet\ncuration strategy\u2014with negative implications for downstream model fairness. Prabhu & Birhane\n[108] also study the ImageNet dataset and find pornographic content. Both Birhane et al. [15] and\nPrabhu & Birhane [108] survey ethical conundrums and harms that are borne out of improper dataset\ncuration. In an effort to combat dataset toxicity, we conduct NSFW preprocessing (Section 3.2,\nAppendix E) and blur detected faces (Section 3.2, Appendix G) during pool construction. We also\nconduct preliminary fairness evaluations (Section 5.3, Appendix Q) for models trained on our data.\nWe hope COMMONPOOL will serve as a research artifact for future work examining dataset safety.\nBeyond data selection, Chan et al. [23] investigate the effects of dataset distribution on emergent\nproperties of transformers, while Fang et al. [44] look at the relationship between data and model\nrobustness to distribution shifts. We hope our extensive evaluation suite comprised of 38 diverse tasks\nwill facilitate similar studies when training multimodal models at large scale.\nOthers study how to reduce the burdens of training data annotation in the curation process. Classic\napproaches include distant supervision [67], crowd-sourced labels [154], heuristic rules [9] and\nfeature annotation [96], among others. A recent line of work known as data programming or\nprogrammatic weak supervision [118, 119, 157, 158] attempts to reduce annotation cost and is found\nin many industry applications [10, 120]. In data programming, developers write programmatic\nlabeling functions to automatically label a large amount of unlabeled data. The labeling functions\ncould produce noisy and confl   icting labels, so researchers have developed methods to aggregate noisy\nvotes to produce the final training labels [117, 47, 133].\nPrevious literature also studies methods for training data attribution, which seek to link a model\u2019s\nbehavior (e.g., its accuracy on a particular task or subset of data) to particular subsets of its training\ndata. Such methods include influence functions, a classic technique from robust statistics [57, 35]\nthat uses a second-order Taylor expansion to approximate the effect of removing a training point\non the learned model parameters [81, 82, 58, 52], as well as methods that fit attribution functions\ndirectly to the dynamics of repeated training runs [49, 109, 71, 56]. Training data attribution methods\nassume that we have already trained a model, though they can be subsequently used to refine the\ntraining data (e.g., by identifying potentially mislabeled training points [81]). Our focus in this paper\nis instead on data curation methods\u2014that is, methods for selecting a subset of the training data to\ntrain a model in the first place.\nIn the context of natural language processing, Swayamdipta et al. [138] proposes a tool for\ncharacterizing samples in a dataset based on training dynamics, labelling instances as ambiguous,\neasy to learn or hard to learn. Previous literature such as work by Le Bras et al. [88], Li & Vasconcelos\n[91], Gururangan et al. [55] advocate for removing easy instances from the training data. Ethayarajh\net al. [41] propose a measure of how difficult a dataset is to learn, V-usable information. Such\ntechniques could be promising directions of further exploration in the context of our benchmark.\nFinally, another related line of work is studying scaling trends. In addition to Sorscher et al. [135],\nresearchers have investigated how model performance changes as a function of compute budget,\nmodel size, and number of training samples [79, 66, 21, 28]. However, this line of work does not\nconsider how dataset design may affects scaling trends. Beyond dataset size, we measure the effects of\ndifferent dataset sources and filtering strategies. While scaling trends are central to our investigations,\nthe purpose of our benchmark is to search for the next generation of large multimodal datasets to\nfacilitate more accurate and reliable models.\nD    Parsing Common Crawl\nCommon Crawl releases metadata files for the websites that they index (i.e., WAT files). They\nrelease these files approximately once a month. We consider all files available from 2014 through\nNovember of 2022. We first parse these files, utilizing Apache Spark [155] to extract image urls\nand corresponding alt-text. We map each url, text pair to a uid hash and remove duplicates. This\n                                                    26", "md": "# Additional Related Work\n\n## Additional related work\n\nHere we expand on the related work described in Section 2.\n\nImage dataset safety is an active area of research, especially in the context of large-scale dataset construction. In addition to Birhane et al. [15], who study problematic content in LAION-400M, Yang et al. [147] study the ImageNet dataset and reveal limitations associated with the ImageNet curation strategy\u2014with negative implications for downstream model fairness. Prabhu & Birhane [108] also study the ImageNet dataset and find pornographic content. Both Birhane et al. [15] and Prabhu & Birhane [108] survey ethical conundrums and harms that are borne out of improper dataset curation. In an effort to combat dataset toxicity, we conduct NSFW preprocessing (Section 3.2, Appendix E) and blur detected faces (Section 3.2, Appendix G) during pool construction. We also conduct preliminary fairness evaluations (Section 5.3, Appendix Q) for models trained on our data. We hope COMMONPOOL will serve as a research artifact for future work examining dataset safety.\n\nBeyond data selection, Chan et al. [23] investigate the effects of dataset distribution on emergent properties of transformers, while Fang et al. [44] look at the relationship between data and model robustness to distribution shifts. We hope our extensive evaluation suite comprised of 38 diverse tasks will facilitate similar studies when training multimodal models at large scale.\n\nOthers study how to reduce the burdens of training data annotation in the curation process. Classic approaches include distant supervision [67], crowd-sourced labels [154], heuristic rules [9] and feature annotation [96], among others. A recent line of work known as data programming or programmatic weak supervision [118, 119, 157, 158] attempts to reduce annotation cost and is found in many industry applications [10, 120]. In data programming, developers write programmatic labeling functions to automatically label a large amount of unlabeled data. The labeling functions could produce noisy and conflicting labels, so researchers have developed methods to aggregate noisy votes to produce the final training labels [117, 47, 133].\n\nPrevious literature also studies methods for training data attribution, which seek to link a model\u2019s behavior (e.g., its accuracy on a particular task or subset of data) to particular subsets of its training data. Such methods include influence functions, a classic technique from robust statistics [57, 35] that uses a second-order Taylor expansion to approximate the effect of removing a training point on the learned model parameters [81, 82, 58, 52], as well as methods that fit attribution functions directly to the dynamics of repeated training runs [49, 109, 71, 56]. Training data attribution methods assume that we have already trained a model, though they can be subsequently used to refine the training data (e.g., by identifying potentially mislabeled training points [81]). Our focus in this paper is instead on data curation methods\u2014that is, methods for selecting a subset of the training data to train a model in the first place.\n\nIn the context of natural language processing, Swayamdipta et al. [138] proposes a tool for characterizing samples in a dataset based on training dynamics, labelling instances as ambiguous, easy to learn or hard to learn. Previous literature such as work by Le Bras et al. [88], Li & Vasconcelos [91], Gururangan et al. [55] advocate for removing easy instances from the training data. Ethayarajh et al. [41] propose a measure of how difficult a dataset is to learn, V-usable information. Such techniques could be promising directions of further exploration in the context of our benchmark.\n\nFinally, another related line of work is studying scaling trends. In addition to Sorscher et al. [135], researchers have investigated how model performance changes as a function of compute budget, model size, and number of training samples [79, 66, 21, 28]. However, this line of work does not consider how dataset design may affect scaling trends. Beyond dataset size, we measure the effects of different dataset sources and filtering strategies. While scaling trends are central to our investigations, the purpose of our benchmark is to search for the next generation of large multimodal datasets to facilitate more accurate and reliable models.\n\n## Parsing Common Crawl\n\nCommon Crawl releases metadata files for the websites that they index (i.e., WAT files). They release these files approximately once a month. We consider all files available from 2014 through November of 2022. We first parse these files, utilizing Apache Spark [155] to extract image urls and corresponding alt-text. We map each url, text pair to a uid hash and remove duplicates.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Related Work", "md": "# Additional Related Work"}, {"type": "heading", "lvl": 2, "value": "Additional related work", "md": "## Additional related work"}, {"type": "text", "value": "Here we expand on the related work described in Section 2.\n\nImage dataset safety is an active area of research, especially in the context of large-scale dataset construction. In addition to Birhane et al. [15], who study problematic content in LAION-400M, Yang et al. [147] study the ImageNet dataset and reveal limitations associated with the ImageNet curation strategy\u2014with negative implications for downstream model fairness. Prabhu & Birhane [108] also study the ImageNet dataset and find pornographic content. Both Birhane et al. [15] and Prabhu & Birhane [108] survey ethical conundrums and harms that are borne out of improper dataset curation. In an effort to combat dataset toxicity, we conduct NSFW preprocessing (Section 3.2, Appendix E) and blur detected faces (Section 3.2, Appendix G) during pool construction. We also conduct preliminary fairness evaluations (Section 5.3, Appendix Q) for models trained on our data. We hope COMMONPOOL will serve as a research artifact for future work examining dataset safety.\n\nBeyond data selection, Chan et al. [23] investigate the effects of dataset distribution on emergent properties of transformers, while Fang et al. [44] look at the relationship between data and model robustness to distribution shifts. We hope our extensive evaluation suite comprised of 38 diverse tasks will facilitate similar studies when training multimodal models at large scale.\n\nOthers study how to reduce the burdens of training data annotation in the curation process. Classic approaches include distant supervision [67], crowd-sourced labels [154], heuristic rules [9] and feature annotation [96], among others. A recent line of work known as data programming or programmatic weak supervision [118, 119, 157, 158] attempts to reduce annotation cost and is found in many industry applications [10, 120]. In data programming, developers write programmatic labeling functions to automatically label a large amount of unlabeled data. The labeling functions could produce noisy and conflicting labels, so researchers have developed methods to aggregate noisy votes to produce the final training labels [117, 47, 133].\n\nPrevious literature also studies methods for training data attribution, which seek to link a model\u2019s behavior (e.g., its accuracy on a particular task or subset of data) to particular subsets of its training data. Such methods include influence functions, a classic technique from robust statistics [57, 35] that uses a second-order Taylor expansion to approximate the effect of removing a training point on the learned model parameters [81, 82, 58, 52], as well as methods that fit attribution functions directly to the dynamics of repeated training runs [49, 109, 71, 56]. Training data attribution methods assume that we have already trained a model, though they can be subsequently used to refine the training data (e.g., by identifying potentially mislabeled training points [81]). Our focus in this paper is instead on data curation methods\u2014that is, methods for selecting a subset of the training data to train a model in the first place.\n\nIn the context of natural language processing, Swayamdipta et al. [138] proposes a tool for characterizing samples in a dataset based on training dynamics, labelling instances as ambiguous, easy to learn or hard to learn. Previous literature such as work by Le Bras et al. [88], Li & Vasconcelos [91], Gururangan et al. [55] advocate for removing easy instances from the training data. Ethayarajh et al. [41] propose a measure of how difficult a dataset is to learn, V-usable information. Such techniques could be promising directions of further exploration in the context of our benchmark.\n\nFinally, another related line of work is studying scaling trends. In addition to Sorscher et al. [135], researchers have investigated how model performance changes as a function of compute budget, model size, and number of training samples [79, 66, 21, 28]. However, this line of work does not consider how dataset design may affect scaling trends. Beyond dataset size, we measure the effects of different dataset sources and filtering strategies. While scaling trends are central to our investigations, the purpose of our benchmark is to search for the next generation of large multimodal datasets to facilitate more accurate and reliable models.", "md": "Here we expand on the related work described in Section 2.\n\nImage dataset safety is an active area of research, especially in the context of large-scale dataset construction. In addition to Birhane et al. [15], who study problematic content in LAION-400M, Yang et al. [147] study the ImageNet dataset and reveal limitations associated with the ImageNet curation strategy\u2014with negative implications for downstream model fairness. Prabhu & Birhane [108] also study the ImageNet dataset and find pornographic content. Both Birhane et al. [15] and Prabhu & Birhane [108] survey ethical conundrums and harms that are borne out of improper dataset curation. In an effort to combat dataset toxicity, we conduct NSFW preprocessing (Section 3.2, Appendix E) and blur detected faces (Section 3.2, Appendix G) during pool construction. We also conduct preliminary fairness evaluations (Section 5.3, Appendix Q) for models trained on our data. We hope COMMONPOOL will serve as a research artifact for future work examining dataset safety.\n\nBeyond data selection, Chan et al. [23] investigate the effects of dataset distribution on emergent properties of transformers, while Fang et al. [44] look at the relationship between data and model robustness to distribution shifts. We hope our extensive evaluation suite comprised of 38 diverse tasks will facilitate similar studies when training multimodal models at large scale.\n\nOthers study how to reduce the burdens of training data annotation in the curation process. Classic approaches include distant supervision [67], crowd-sourced labels [154], heuristic rules [9] and feature annotation [96], among others. A recent line of work known as data programming or programmatic weak supervision [118, 119, 157, 158] attempts to reduce annotation cost and is found in many industry applications [10, 120]. In data programming, developers write programmatic labeling functions to automatically label a large amount of unlabeled data. The labeling functions could produce noisy and conflicting labels, so researchers have developed methods to aggregate noisy votes to produce the final training labels [117, 47, 133].\n\nPrevious literature also studies methods for training data attribution, which seek to link a model\u2019s behavior (e.g., its accuracy on a particular task or subset of data) to particular subsets of its training data. Such methods include influence functions, a classic technique from robust statistics [57, 35] that uses a second-order Taylor expansion to approximate the effect of removing a training point on the learned model parameters [81, 82, 58, 52], as well as methods that fit attribution functions directly to the dynamics of repeated training runs [49, 109, 71, 56]. Training data attribution methods assume that we have already trained a model, though they can be subsequently used to refine the training data (e.g., by identifying potentially mislabeled training points [81]). Our focus in this paper is instead on data curation methods\u2014that is, methods for selecting a subset of the training data to train a model in the first place.\n\nIn the context of natural language processing, Swayamdipta et al. [138] proposes a tool for characterizing samples in a dataset based on training dynamics, labelling instances as ambiguous, easy to learn or hard to learn. Previous literature such as work by Le Bras et al. [88], Li & Vasconcelos [91], Gururangan et al. [55] advocate for removing easy instances from the training data. Ethayarajh et al. [41] propose a measure of how difficult a dataset is to learn, V-usable information. Such techniques could be promising directions of further exploration in the context of our benchmark.\n\nFinally, another related line of work is studying scaling trends. In addition to Sorscher et al. [135], researchers have investigated how model performance changes as a function of compute budget, model size, and number of training samples [79, 66, 21, 28]. However, this line of work does not consider how dataset design may affect scaling trends. Beyond dataset size, we measure the effects of different dataset sources and filtering strategies. While scaling trends are central to our investigations, the purpose of our benchmark is to search for the next generation of large multimodal datasets to facilitate more accurate and reliable models."}, {"type": "heading", "lvl": 2, "value": "Parsing Common Crawl", "md": "## Parsing Common Crawl"}, {"type": "text", "value": "Common Crawl releases metadata files for the websites that they index (i.e., WAT files). They release these files approximately once a month. We consider all files available from 2014 through November of 2022. We first parse these files, utilizing Apache Spark [155] to extract image urls and corresponding alt-text. We map each url, text pair to a uid hash and remove duplicates.", "md": "Common Crawl releases metadata files for the websites that they index (i.e., WAT files). They release these files approximately once a month. We consider all files available from 2014 through November of 2022. We first parse these files, utilizing Apache Spark [155] to extract image urls and corresponding alt-text. We map each url, text pair to a uid hash and remove duplicates."}]}, {"page": 27, "text": "    Table 4: Detoxify positive rates by threshold on 1 million caption subset of Common Crawl.\n   Threshold    Toxicity    Severe Toxicity     Obscene     Identity Attack    Insult    Threat    Sexual Explicit\n      0.01        9.5%            1.0%           33.4%           1.8%          35.0%      1.3%          2.0%\n      0.1         3.6%            0.1%            0.8%           0.3%           1.4%      0.1%          1.0%\nTable 5: Comparing LAION-2B CLIP based NSFW filtering model to Google Vision API Safe\nSearch adult category on a 40,000 random subset of Common Crawl.\n   Threshold     False Positive Rate       True Positives      Model Positive Rate      Google API Positive Rate\n                (Relative to Google)     (Manual Review)\n      0.1                3.6%                     2                   14.4%                        3.5%\n      0.2                0.6%                     2                    9.1%                        3.5%\n      0.3                0.3%                     3                    7.2%                        3.5%\nresults in 88 billion url, text pairs, which are randomized via a distributed shuffl           e. Note, we do not\nconsider image content when running uid deduplication at this step. Hence, two identical images\nwith different urls and the same caption would both be retained.\nE     Not safe for work (NSFW) filtering\nOur data is sourced from Common Crawl, which contains snapshots of the web. Therefore, we\napply multiple layers of NSFW content filtering to remove problematic images and captions from\nCOMMONPOOL.\nFirst, we filter our captions with Detoxify [60], a language model for toxic comment classification.\nSpecifically, we use the multilingual XLM-RoBERTa [34] variant. The model outputs scores between\nzero and one for the following categories: toxicity, severe toxicity, obscene, identity attack, insult,\nthreat, and sexually explicit. As we had no ground truth for our data, we manually spot check a 1\nmillion random subset of COMMONPOOL at varying thresholds. We found that a threshold of 0.1\nprovided good coverage of filtering out NSFW text. If any of the detoxify category scores exceeds the\nthreshold, the sample is discarded. Qualitatively, we found that the model struggled with multilingual\ncontent, acronyms, and innuendo. Even at 0.1, we noticed there are some captions that are NSFW.\nHowever, lowering the threshold further heavily affected false positives. We therefore use a 0.1\nthreshold for all NSFW categories, which on a random subset of one million captions achieves\npositive rates shown in Table 4.\nSecond, on the vision side, we use a modified version of LAION-5B\u2019s [129] CLIP-based binary\nclassification NSFW model, which takes CLIP ViT-L/14 visual embeddings as input. We remove\nthe initial multi-category encoder from the model, and retrain on the same data with an initial\nnormalization layer followed by a 4-layer multilayer perceptron. Our retrained model matches the\nperformance of the original model on their manually annotated testset. Specifically, we achieve 97.4%\nclassification accuracy on a held out test set compared to 96.1% for the original LAION NSFW\nimage filtering model. Additional details about the training data can be found in Appendix C.5 of\nthe LAION-5B paper. In brief, the training data contains 682K images that is roughly balanced with\nimages from safe for work and NSFW categories.\nTo evaluate our model and determine a threshold, we used Google Vision API\u2019s SafeSearch explicit\ncontent detector to generate labels for an 40,000 random subset of our candidate pool. Specifically,\nan image is NSFW if SafeSearch classifies it as likely or very likely adult (i.e., sexually explicit). As\nshown in Table 5, we found that by thresholding at 0.1 we achieve high recall relative to SafeSearch\nand very few true positives after manual review. We also manually reviewed images classified by\nSafeSearch as likely or very likely racy and found that the images were either benign, subjectively\nsuggestive but not explicit, or already found in the set of images labeled as adult.\nF     Deduplication against evaluation sets\nTo prevent data leakage, we filter COMMONPOOL by removing duplicate and near-duplicate matches\nof evaluation set images. See Figure 4 for example query images from Common Crawl and\ncorresponding near-duplicates in our evaluations sets. We consider images as duplicates when\n                                                         27", "md": "# NSFW Filtering and Deduplication\n\n## Table 4: Detoxify positive rates by threshold on 1 million caption subset of Common Crawl\n\n|Threshold|Toxicity|Severe Toxicity|Obscene|Identity Attack|Insult|Threat|Sexual Explicit|\n|---|---|---|---|---|---|---|---|\n|0.01|9.5%|1.0%|33.4%|1.8%|35.0%|1.3%|2.0%|\n|0.1|3.6%|0.1%|0.8%|0.3%|1.4%|0.1%|1.0%|\n\n## Table 5: Comparing LAION-2B CLIP based NSFW filtering model to Google Vision API Safe Search adult category on a 40,000 random subset of Common Crawl\n\n|Threshold|False Positive Rate (Relative to Google)|True Positives (Manual Review)|Model Positive Rate|Google API Positive Rate|\n|---|---|---|---|---|\n|0.1|3.6%|2|14.4%|3.5%|\n|0.2|0.6%|2|9.1%|3.5%|\n|0.3|0.3%|3|7.2%|3.5%|\n\nResults in 88 billion url, text pairs, which are randomized via a distributed shuffle. Note, we do not consider image content when running uid deduplication at this step. Hence, two identical images with different urls and the same caption would both be retained.\n\n## Not safe for work (NSFW) filtering\n\nOur data is sourced from Common Crawl, which contains snapshots of the web. Therefore, we apply multiple layers of NSFW content filtering to remove problematic images and captions from COMMONPOOL.\n\nFirst, we filter our captions with Detoxify, a language model for toxic comment classification. Specifically, we use the multilingual XLM-RoBERTa variant. The model outputs scores between zero and one for the following categories: toxicity, severe toxicity, obscene, identity attack, insult, threat, and sexually explicit. As we had no ground truth for our data, we manually spot check a 1 million random subset of COMMONPOOL at varying thresholds. We found that a threshold of 0.1 provided good coverage of filtering out NSFW text. If any of the detoxify category scores exceeds the threshold, the sample is discarded. Qualitatively, we found that the model struggled with multilingual content, acronyms, and innuendo. Even at 0.1, we noticed there are some captions that are NSFW. However, lowering the threshold further heavily affected false positives. We therefore use a 0.1 threshold for all NSFW categories, which on a random subset of one million captions achieves positive rates shown in Table 4.\n\nSecond, on the vision side, we use a modified version of LAION-5B\u2019s CLIP-based binary classification NSFW model, which takes CLIP ViT-L/14 visual embeddings as input. We remove the initial multi-category encoder from the model, and retrain on the same data with an initial normalization layer followed by a 4-layer multilayer perceptron. Our retrained model matches the performance of the original model on their manually annotated testset. Specifically, we achieve 97.4% classification accuracy on a held out test set compared to 96.1% for the original LAION NSFW image filtering model. Additional details about the training data can be found in Appendix C.5 of the LAION-5B paper. In brief, the training data contains 682K images that is roughly balanced with images from safe for work and NSFW categories.\n\nTo evaluate our model and determine a threshold, we used Google Vision API\u2019s SafeSearch explicit content detector to generate labels for an 40,000 random subset of our candidate pool. Specifically, an image is NSFW if SafeSearch classifies it as likely or very likely adult (i.e., sexually explicit). As shown in Table 5, we found that by thresholding at 0.1 we achieve high recall relative to SafeSearch and very few true positives after manual review. We also manually reviewed images classified by SafeSearch as likely or very likely racy and found that the images were either benign, subjectively suggestive but not explicit, or already found in the set of images labeled as adult.\n\n## Deduplication against evaluation sets\n\nTo prevent data leakage, we filter COMMONPOOL by removing duplicate and near-duplicate matches of evaluation set images. See Figure 4 for example query images from Common Crawl and corresponding near-duplicates in our evaluations sets. We consider images as duplicates when", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "NSFW Filtering and Deduplication", "md": "# NSFW Filtering and Deduplication"}, {"type": "heading", "lvl": 2, "value": "Table 4: Detoxify positive rates by threshold on 1 million caption subset of Common Crawl", "md": "## Table 4: Detoxify positive rates by threshold on 1 million caption subset of Common Crawl"}, {"type": "table", "rows": [["Threshold", "Toxicity", "Severe Toxicity", "Obscene", "Identity Attack", "Insult", "Threat", "Sexual Explicit"], ["0.01", "9.5%", "1.0%", "33.4%", "1.8%", "35.0%", "1.3%", "2.0%"], ["0.1", "3.6%", "0.1%", "0.8%", "0.3%", "1.4%", "0.1%", "1.0%"]], "md": "|Threshold|Toxicity|Severe Toxicity|Obscene|Identity Attack|Insult|Threat|Sexual Explicit|\n|---|---|---|---|---|---|---|---|\n|0.01|9.5%|1.0%|33.4%|1.8%|35.0%|1.3%|2.0%|\n|0.1|3.6%|0.1%|0.8%|0.3%|1.4%|0.1%|1.0%|", "isPerfectTable": true, "csv": "\"Threshold\",\"Toxicity\",\"Severe Toxicity\",\"Obscene\",\"Identity Attack\",\"Insult\",\"Threat\",\"Sexual Explicit\"\n\"0.01\",\"9.5%\",\"1.0%\",\"33.4%\",\"1.8%\",\"35.0%\",\"1.3%\",\"2.0%\"\n\"0.1\",\"3.6%\",\"0.1%\",\"0.8%\",\"0.3%\",\"1.4%\",\"0.1%\",\"1.0%\""}, {"type": "heading", "lvl": 2, "value": "Table 5: Comparing LAION-2B CLIP based NSFW filtering model to Google Vision API Safe Search adult category on a 40,000 random subset of Common Crawl", "md": "## Table 5: Comparing LAION-2B CLIP based NSFW filtering model to Google Vision API Safe Search adult category on a 40,000 random subset of Common Crawl"}, {"type": "table", "rows": [["Threshold", "False Positive Rate (Relative to Google)", "True Positives (Manual Review)", "Model Positive Rate", "Google API Positive Rate"], ["0.1", "3.6%", "2", "14.4%", "3.5%"], ["0.2", "0.6%", "2", "9.1%", "3.5%"], ["0.3", "0.3%", "3", "7.2%", "3.5%"]], "md": "|Threshold|False Positive Rate (Relative to Google)|True Positives (Manual Review)|Model Positive Rate|Google API Positive Rate|\n|---|---|---|---|---|\n|0.1|3.6%|2|14.4%|3.5%|\n|0.2|0.6%|2|9.1%|3.5%|\n|0.3|0.3%|3|7.2%|3.5%|", "isPerfectTable": true, "csv": "\"Threshold\",\"False Positive Rate (Relative to Google)\",\"True Positives (Manual Review)\",\"Model Positive Rate\",\"Google API Positive Rate\"\n\"0.1\",\"3.6%\",\"2\",\"14.4%\",\"3.5%\"\n\"0.2\",\"0.6%\",\"2\",\"9.1%\",\"3.5%\"\n\"0.3\",\"0.3%\",\"3\",\"7.2%\",\"3.5%\""}, {"type": "text", "value": "Results in 88 billion url, text pairs, which are randomized via a distributed shuffle. Note, we do not consider image content when running uid deduplication at this step. Hence, two identical images with different urls and the same caption would both be retained.", "md": "Results in 88 billion url, text pairs, which are randomized via a distributed shuffle. Note, we do not consider image content when running uid deduplication at this step. Hence, two identical images with different urls and the same caption would both be retained."}, {"type": "heading", "lvl": 2, "value": "Not safe for work (NSFW) filtering", "md": "## Not safe for work (NSFW) filtering"}, {"type": "text", "value": "Our data is sourced from Common Crawl, which contains snapshots of the web. Therefore, we apply multiple layers of NSFW content filtering to remove problematic images and captions from COMMONPOOL.\n\nFirst, we filter our captions with Detoxify, a language model for toxic comment classification. Specifically, we use the multilingual XLM-RoBERTa variant. The model outputs scores between zero and one for the following categories: toxicity, severe toxicity, obscene, identity attack, insult, threat, and sexually explicit. As we had no ground truth for our data, we manually spot check a 1 million random subset of COMMONPOOL at varying thresholds. We found that a threshold of 0.1 provided good coverage of filtering out NSFW text. If any of the detoxify category scores exceeds the threshold, the sample is discarded. Qualitatively, we found that the model struggled with multilingual content, acronyms, and innuendo. Even at 0.1, we noticed there are some captions that are NSFW. However, lowering the threshold further heavily affected false positives. We therefore use a 0.1 threshold for all NSFW categories, which on a random subset of one million captions achieves positive rates shown in Table 4.\n\nSecond, on the vision side, we use a modified version of LAION-5B\u2019s CLIP-based binary classification NSFW model, which takes CLIP ViT-L/14 visual embeddings as input. We remove the initial multi-category encoder from the model, and retrain on the same data with an initial normalization layer followed by a 4-layer multilayer perceptron. Our retrained model matches the performance of the original model on their manually annotated testset. Specifically, we achieve 97.4% classification accuracy on a held out test set compared to 96.1% for the original LAION NSFW image filtering model. Additional details about the training data can be found in Appendix C.5 of the LAION-5B paper. In brief, the training data contains 682K images that is roughly balanced with images from safe for work and NSFW categories.\n\nTo evaluate our model and determine a threshold, we used Google Vision API\u2019s SafeSearch explicit content detector to generate labels for an 40,000 random subset of our candidate pool. Specifically, an image is NSFW if SafeSearch classifies it as likely or very likely adult (i.e., sexually explicit). As shown in Table 5, we found that by thresholding at 0.1 we achieve high recall relative to SafeSearch and very few true positives after manual review. We also manually reviewed images classified by SafeSearch as likely or very likely racy and found that the images were either benign, subjectively suggestive but not explicit, or already found in the set of images labeled as adult.", "md": "Our data is sourced from Common Crawl, which contains snapshots of the web. Therefore, we apply multiple layers of NSFW content filtering to remove problematic images and captions from COMMONPOOL.\n\nFirst, we filter our captions with Detoxify, a language model for toxic comment classification. Specifically, we use the multilingual XLM-RoBERTa variant. The model outputs scores between zero and one for the following categories: toxicity, severe toxicity, obscene, identity attack, insult, threat, and sexually explicit. As we had no ground truth for our data, we manually spot check a 1 million random subset of COMMONPOOL at varying thresholds. We found that a threshold of 0.1 provided good coverage of filtering out NSFW text. If any of the detoxify category scores exceeds the threshold, the sample is discarded. Qualitatively, we found that the model struggled with multilingual content, acronyms, and innuendo. Even at 0.1, we noticed there are some captions that are NSFW. However, lowering the threshold further heavily affected false positives. We therefore use a 0.1 threshold for all NSFW categories, which on a random subset of one million captions achieves positive rates shown in Table 4.\n\nSecond, on the vision side, we use a modified version of LAION-5B\u2019s CLIP-based binary classification NSFW model, which takes CLIP ViT-L/14 visual embeddings as input. We remove the initial multi-category encoder from the model, and retrain on the same data with an initial normalization layer followed by a 4-layer multilayer perceptron. Our retrained model matches the performance of the original model on their manually annotated testset. Specifically, we achieve 97.4% classification accuracy on a held out test set compared to 96.1% for the original LAION NSFW image filtering model. Additional details about the training data can be found in Appendix C.5 of the LAION-5B paper. In brief, the training data contains 682K images that is roughly balanced with images from safe for work and NSFW categories.\n\nTo evaluate our model and determine a threshold, we used Google Vision API\u2019s SafeSearch explicit content detector to generate labels for an 40,000 random subset of our candidate pool. Specifically, an image is NSFW if SafeSearch classifies it as likely or very likely adult (i.e., sexually explicit). As shown in Table 5, we found that by thresholding at 0.1 we achieve high recall relative to SafeSearch and very few true positives after manual review. We also manually reviewed images classified by SafeSearch as likely or very likely racy and found that the images were either benign, subjectively suggestive but not explicit, or already found in the set of images labeled as adult."}, {"type": "heading", "lvl": 2, "value": "Deduplication against evaluation sets", "md": "## Deduplication against evaluation sets"}, {"type": "text", "value": "To prevent data leakage, we filter COMMONPOOL by removing duplicate and near-duplicate matches of evaluation set images. See Figure 4 for example query images from Common Crawl and corresponding near-duplicates in our evaluations sets. We consider images as duplicates when", "md": "To prevent data leakage, we filter COMMONPOOL by removing duplicate and near-duplicate matches of evaluation set images. See Figure 4 for example query images from Common Crawl and corresponding near-duplicates in our evaluations sets. We consider images as duplicates when"}]}, {"page": 28, "text": "       Query Image\n       Eval Image\n                     sun397_test/s0011797                    cars_test/s0005478                    country211_test/s0002908              vtab-caltech101_test/s0004353\nFigure 4: Candidate images (top) that are detected as duplicates against images in the evaluation\nsets (bottom) are removed from the pool. In addition to exact duplicate images, near-duplicates with\nvariable aspect ratios, JPEG compression, overlays, color adjustment, and artistic rendering are also\ndetected.\n         100                       Yokoo et al. 2022                                         100                          CLIP ViT-L/14\n           99                                                                                  90\n       Precision (%)                                                                       Precision (%)\n                                                                                               80\n           98                Aspect Ratio                                                      70               Aspect Ratio\n                             Encoding                                                                           Encoding\n           97                Flips                                                             60               Flips\n                             Grayscale                                                         50               Grayscale\n           96                Rotations                                                         40               Rotations\n                    40                  60                  80                 100                      40                  60                 80                100\n                                         Recall (%)                                                                          Recall (%)\nFigure 5: Analysis of different de-duplication strategies across a variety of image transformations.\nWe see that the model introduced by Yokoo [150] is better in almost every transformation, with the\nexception of very aggressive aspect ratio modification.\nthe cosine similarity between a query (Common Crawl image) feature and a reference (evaluation\nimage) feature is higher than a fixed threshold. We employ the deduplication model proposed by\nYokoo [150], which earned 1st place in the Facebook AI Image Similarity Challenge (ISC) [40]. We\nchoose a cosine similarity threshold of 0.604169 to maximize the true duplicates detected, without\nremoving too many false duplicates from the pool. We compare against OpenAI\u2019s CLIP ViT-B/32 as\na baseline on ISC. We find that for our threshold, the ISC model achieves precision 0.9 and recall 0.8.\nAt a threshold of 0.96, CLIP achieves the same precision 0.9, but a significantly worse recall of 0.02.\nApproximately 2.8% of downloaded samples are flagged as evaluation set near-duplicates.\nTo verify the performance of our de-duplication models with greater granularity, we modify the\nevaluation procedure in Douze et al. [40] to include transformations which are representative of\nnaturally-occurring duplications on the Internet. Specifically, we study: 1) jpeg compression\n(encoding), 2) image flips, 3) image rotations, 4) aspect ratio modifications, and 5) grayscaling.\nTo do this, we sample 20% of the images from each of our evaluation datasets uniformly at random\nto serve as a reference set of about 140,000 images. Next we sample 560,000 images uniformly at\nrandom from LAION-2B to serve as distractors, for a 4-to-1 distractor to reference ratio. Finally, we\napply each of the augmentations above and use threshold filtering to determine duplicates. Figure 5\nshows the results from the deduplication model [150] compared with OpenAI\u2019s CLIP ViT-L/14. At\nhigh recall values, we see that CLIP filtering results in removing over 2\u00d7 the data as that of the\ndeduplication model from Yokoo [150].\n                                                                                    28", "md": "# De-duplication Analysis\n\n## Query Image\n\n## Eval Image\n\nFigure 4: Candidate images (top) that are detected as duplicates against images in the evaluation\nsets (bottom) are removed from the pool. In addition to exact duplicate images, near-duplicates with\nvariable aspect ratios, JPEG compression, overlays, color adjustment, and artistic rendering are also\ndetected.\n\n| |sun397_test/s0011797|cars_test/s0005478|country211_test/s0002908|vtab-caltech101_test/s0004353|\n|---|---|---|---|---|\n|100|Yokoo et al. 2022|100| |CLIP ViT-L/14|\n|99| | |90| |\n|Precision (%)| | | | |\n|98|Aspect Ratio|Encoding|Aspect Ratio|Encoding|\n|97|Flips|Grayscale|Flips|Grayscale|\n|96|Rotations| |Rotations| |\n| |40|60|80|100|40|60|80|100|\n| |Recall (%)| | | |Recall (%)| | | |\n\nFigure 5: Analysis of different de-duplication strategies across a variety of image transformations.\nWe see that the model introduced by Yokoo [150] is better in almost every transformation, with the\nexception of very aggressive aspect ratio modification.\n\nThe cosine similarity between a query (Common Crawl image) feature and a reference (evaluation\nimage) feature is higher than a fixed threshold. We employ the deduplication model proposed by\nYokoo [150], which earned 1st place in the Facebook AI Image Similarity Challenge (ISC). We\nchoose a cosine similarity threshold of 0.604169 to maximize the true duplicates detected, without\nremoving too many false duplicates from the pool. We compare against OpenAI\u2019s CLIP ViT-B/32 as\na baseline on ISC. We find that for our threshold, the ISC model achieves precision 0.9 and recall 0.8.\nAt a threshold of 0.96, CLIP achieves the same precision 0.9, but a significantly worse recall of 0.02.\nApproximately 2.8% of downloaded samples are flagged as evaluation set near-duplicates.\n\nTo verify the performance of our de-duplication models with greater granularity, we modify the\nevaluation procedure in Douze et al. to include transformations which are representative of\nnaturally-occurring duplications on the Internet. Specifically, we study: 1) jpeg compression\n(encoding), 2) image flips, 3) image rotations, 4) aspect ratio modifications, and 5) grayscaling.\nTo do this, we sample 20% of the images from each of our evaluation datasets uniformly at random\nto serve as a reference set of about 140,000 images. Next we sample 560,000 images uniformly at\nrandom from LAION-2B to serve as distractors, for a 4-to-1 distractor to reference ratio. Finally, we\napply each of the augmentations above and use threshold filtering to determine duplicates. Figure 5\nshows the results from the deduplication model compared with OpenAI\u2019s CLIP ViT-L/14. At\nhigh recall values, we see that CLIP filtering results in removing over 2\u00d7 the data as that of the\ndeduplication model from Yokoo.\n\n28", "images": [{"name": "page-28-6.jpg", "height": 59, "width": 81, "x": 416, "y": 80}, {"name": "page-28-2.jpg", "height": 61, "width": 81, "x": 228, "y": 78}, {"name": "page-28-5.jpg", "height": 45, "width": 88, "x": 319, "y": 163}, {"name": "page-28-4.jpg", "height": 61, "width": 91, "x": 316, "y": 78}, {"name": "page-28-3.jpg", "height": 35, "width": 88, "x": 225, "y": 173}, {"name": "page-28-0.jpg", "height": 61, "width": 88, "x": 134, "y": 78}, {"name": "page-28-7.jpg", "height": 54, "width": 81, "x": 417, "y": 153}, {"name": "page-28-1.jpg", "height": 61, "width": 81, "x": 137, "y": 147}], "items": [{"type": "heading", "lvl": 1, "value": "De-duplication Analysis", "md": "# De-duplication Analysis"}, {"type": "heading", "lvl": 2, "value": "Query Image", "md": "## Query Image"}, {"type": "heading", "lvl": 2, "value": "Eval Image", "md": "## Eval Image"}, {"type": "text", "value": "Figure 4: Candidate images (top) that are detected as duplicates against images in the evaluation\nsets (bottom) are removed from the pool. In addition to exact duplicate images, near-duplicates with\nvariable aspect ratios, JPEG compression, overlays, color adjustment, and artistic rendering are also\ndetected.", "md": "Figure 4: Candidate images (top) that are detected as duplicates against images in the evaluation\nsets (bottom) are removed from the pool. In addition to exact duplicate images, near-duplicates with\nvariable aspect ratios, JPEG compression, overlays, color adjustment, and artistic rendering are also\ndetected."}, {"type": "table", "rows": [["", "sun397_test/s0011797", "cars_test/s0005478", "country211_test/s0002908", "vtab-caltech101_test/s0004353"], ["100", "Yokoo et al. 2022", "100", "", "CLIP ViT-L/14"], ["99", "", "", "90", ""], ["Precision (%)", "", "", "", ""], ["98", "Aspect Ratio", "Encoding", "Aspect Ratio", "Encoding"], ["97", "Flips", "Grayscale", "Flips", "Grayscale"], ["96", "Rotations", "", "Rotations", ""], ["", "40", "60", "80", "100", "40", "60", "80", "100"], ["", "Recall (%)", "", "", "", "Recall (%)", "", "", ""]], "md": "| |sun397_test/s0011797|cars_test/s0005478|country211_test/s0002908|vtab-caltech101_test/s0004353|\n|---|---|---|---|---|\n|100|Yokoo et al. 2022|100| |CLIP ViT-L/14|\n|99| | |90| |\n|Precision (%)| | | | |\n|98|Aspect Ratio|Encoding|Aspect Ratio|Encoding|\n|97|Flips|Grayscale|Flips|Grayscale|\n|96|Rotations| |Rotations| |\n| |40|60|80|100|40|60|80|100|\n| |Recall (%)| | | |Recall (%)| | | |", "isPerfectTable": false, "csv": "\"\",\"sun397_test/s0011797\",\"cars_test/s0005478\",\"country211_test/s0002908\",\"vtab-caltech101_test/s0004353\"\n\"100\",\"Yokoo et al. 2022\",\"100\",\"\",\"CLIP ViT-L/14\"\n\"99\",\"\",\"\",\"90\",\"\"\n\"Precision (%)\",\"\",\"\",\"\",\"\"\n\"98\",\"Aspect Ratio\",\"Encoding\",\"Aspect Ratio\",\"Encoding\"\n\"97\",\"Flips\",\"Grayscale\",\"Flips\",\"Grayscale\"\n\"96\",\"Rotations\",\"\",\"Rotations\",\"\"\n\"\",\"40\",\"60\",\"80\",\"100\",\"40\",\"60\",\"80\",\"100\"\n\"\",\"Recall (%)\",\"\",\"\",\"\",\"Recall (%)\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 5: Analysis of different de-duplication strategies across a variety of image transformations.\nWe see that the model introduced by Yokoo [150] is better in almost every transformation, with the\nexception of very aggressive aspect ratio modification.\n\nThe cosine similarity between a query (Common Crawl image) feature and a reference (evaluation\nimage) feature is higher than a fixed threshold. We employ the deduplication model proposed by\nYokoo [150], which earned 1st place in the Facebook AI Image Similarity Challenge (ISC). We\nchoose a cosine similarity threshold of 0.604169 to maximize the true duplicates detected, without\nremoving too many false duplicates from the pool. We compare against OpenAI\u2019s CLIP ViT-B/32 as\na baseline on ISC. We find that for our threshold, the ISC model achieves precision 0.9 and recall 0.8.\nAt a threshold of 0.96, CLIP achieves the same precision 0.9, but a significantly worse recall of 0.02.\nApproximately 2.8% of downloaded samples are flagged as evaluation set near-duplicates.\n\nTo verify the performance of our de-duplication models with greater granularity, we modify the\nevaluation procedure in Douze et al. to include transformations which are representative of\nnaturally-occurring duplications on the Internet. Specifically, we study: 1) jpeg compression\n(encoding), 2) image flips, 3) image rotations, 4) aspect ratio modifications, and 5) grayscaling.\nTo do this, we sample 20% of the images from each of our evaluation datasets uniformly at random\nto serve as a reference set of about 140,000 images. Next we sample 560,000 images uniformly at\nrandom from LAION-2B to serve as distractors, for a 4-to-1 distractor to reference ratio. Finally, we\napply each of the augmentations above and use threshold filtering to determine duplicates. Figure 5\nshows the results from the deduplication model compared with OpenAI\u2019s CLIP ViT-L/14. At\nhigh recall values, we see that CLIP filtering results in removing over 2\u00d7 the data as that of the\ndeduplication model from Yokoo.\n\n28", "md": "Figure 5: Analysis of different de-duplication strategies across a variety of image transformations.\nWe see that the model introduced by Yokoo [150] is better in almost every transformation, with the\nexception of very aggressive aspect ratio modification.\n\nThe cosine similarity between a query (Common Crawl image) feature and a reference (evaluation\nimage) feature is higher than a fixed threshold. We employ the deduplication model proposed by\nYokoo [150], which earned 1st place in the Facebook AI Image Similarity Challenge (ISC). We\nchoose a cosine similarity threshold of 0.604169 to maximize the true duplicates detected, without\nremoving too many false duplicates from the pool. We compare against OpenAI\u2019s CLIP ViT-B/32 as\na baseline on ISC. We find that for our threshold, the ISC model achieves precision 0.9 and recall 0.8.\nAt a threshold of 0.96, CLIP achieves the same precision 0.9, but a significantly worse recall of 0.02.\nApproximately 2.8% of downloaded samples are flagged as evaluation set near-duplicates.\n\nTo verify the performance of our de-duplication models with greater granularity, we modify the\nevaluation procedure in Douze et al. to include transformations which are representative of\nnaturally-occurring duplications on the Internet. Specifically, we study: 1) jpeg compression\n(encoding), 2) image flips, 3) image rotations, 4) aspect ratio modifications, and 5) grayscaling.\nTo do this, we sample 20% of the images from each of our evaluation datasets uniformly at random\nto serve as a reference set of about 140,000 images. Next we sample 560,000 images uniformly at\nrandom from LAION-2B to serve as distractors, for a 4-to-1 distractor to reference ratio. Finally, we\napply each of the augmentations above and use threshold filtering to determine duplicates. Figure 5\nshows the results from the deduplication model compared with OpenAI\u2019s CLIP ViT-L/14. At\nhigh recall values, we see that CLIP filtering results in removing over 2\u00d7 the data as that of the\ndeduplication model from Yokoo.\n\n28"}]}, {"page": 29, "text": "    Table 6: Face detection performance on a set of 3293 random images from COMMONPOOL.\n                                       SCRFD-10G        Amazon Rekognition\n                          Accuracy             93.87                    96.57\n                          Precision            75.87                    86.09\n                          Recall               90.53                    93.75\n                        1.01e7\n                        0.8\n                       Frequency\n                        0.6\n                        0.4\n                        0.2\n                        0.0     0         1        2         3         4         5\n                                          Number of predicted faces\n                                 mean: 0.57, median: 0.00, min: 0, max: 228\n          Figure 6: Frequency of predicted number of faces in the small COMMONPOOL.\nG    Face blurring\nAs an extra step to safeguard against issues of privacy that may arise from the use of data scraped\nfrom the web, we include face blurring as part of our pool creation. To create face metadata, we\nuse the SCRFD face detector [53] to extract bounding boxes for the faces in our images. These\nbounding boxes are included as part of the image metadata in our pool. We make use of the pretrained\nSCRFD-10G model. We use the same preprocessing as the one described in the offi          cial repository\nof the paper, with the exception of providing 224 \u00d7 224 input images (by padding each image to\nsquare and then resizing) to limit computation costs. Invoking this model provides us with bounding\nboxes along with an associated score, which we then compare against a threshold of 0.3 to keep or\ndiscard this bounding box. This threshold is the default one used in the repository of SCRFD for the\nvisualization of bounding boxes, and we found it to perform well on our data as discussed next.\nIn Table 6 we can see the result of face detection on a set of 3293 images from COMMONPOOL. We\nevaluate the detection on whether the image has visible faces or not (where images such as cartoon\ndrawings of non-real human faces are not considered as positives), and whether the detector has\ndetected these visible faces. We considered an image as a true positive if all the clearly visible faces\nin the image were detected, based on the above thresholding process. We did not do extensive box\nlabeling. True positives are instead determined by human inspection. We compare the quality of these\ndetections with the Amazon Rekognition system, which is the one upon which the face detections on\nImageNet were based [148]. Note that in this scenario, the recall of the detectors is more important\nthan precision (as detecting a few more bounding boxes across our pool does not affect privacy).\nTo utilize these bounding boxes on our data, we apply a standard blurring pipeline, as proposed by\nYang et al. [148]. The result of this process is an image where the faces is blurred and there is a\nsmooth transition from blurred to clean parts of the image. In Figure 6 we see the distribution of\nfaces for the small COMMONPOOL. Note that the majority of images do not contain faces.\nAs part of our competition pipeline, images are by default blurred during the download process. In\nTable 7 we can see the results of training on a set of images with the size of our medium scale after\nfiltering with each method, with and without the application of face blurring as provided by our\ndetector. We can see that the difference in performance is small, which suggests that the application\nof face blurring does not significantly affect the performance on our downstream tasks. However,\nwe note that this design decision may be more detrimental in generative settings, especially when a\ngenerative model needs to output faces. Our competition is primarily focused on discriminative tasks,\nand as such when designing our dataset, we wished to prioritize the safety and privacy of individuals\nthrough blurring faces in our download tooling by default.\n                                                   29", "md": "**Table 6: Face detection performance on a set of 3293 random images from COMMONPOOL.**\n| |SCRFD-10G|Amazon Rekognition|\n|---|---|---|\n|Accuracy|93.87|96.57|\n|Precision|75.87|86.09|\n|Recall|90.53|93.75|\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\text{Frequency} & 0 & 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n\\text{mean:} & 0.57 & \\text{median:} & 0.00 & \\text{min:} & 0 & \\text{max:} & 228 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 6: Frequency of predicted number of faces in the small COMMONPOOL.\n\nFace blurring\n\nAs an extra step to safeguard against issues of privacy that may arise from the use of data scraped from the web, we include face blurring as part of our pool creation. To create face metadata, we use the SCRFD face detector [53] to extract bounding boxes for the faces in our images. These bounding boxes are included as part of the image metadata in our pool. We make use of the pretrained SCRFD-10G model. We use the same preprocessing as the one described in the official repository of the paper, with the exception of providing 224 \u00d7 224 input images (by padding each image to square and then resizing) to limit computation costs. Invoking this model provides us with bounding boxes along with an associated score, which we then compare against a threshold of 0.3 to keep or discard this bounding box. This threshold is the default one used in the repository of SCRFD for the visualization of bounding boxes, and we found it to perform well on our data as discussed next.\n\nIn Table 6 we can see the result of face detection on a set of 3293 images from COMMONPOOL. We evaluate the detection on whether the image has visible faces or not (where images such as cartoon drawings of non-real human faces are not considered as positives), and whether the detector has detected these visible faces. We considered an image as a true positive if all the clearly visible faces in the image were detected, based on the above thresholding process. We did not do extensive box labeling. True positives are instead determined by human inspection. We compare the quality of these detections with the Amazon Rekognition system, which is the one upon which the face detections on ImageNet were based [148]. Note that in this scenario, the recall of the detectors is more important than precision (as detecting a few more bounding boxes across our pool does not affect privacy).\n\nTo utilize these bounding boxes on our data, we apply a standard blurring pipeline, as proposed by Yang et al. [148]. The result of this process is an image where the faces are blurred and there is a smooth transition from blurred to clean parts of the image. In Figure 6 we see the distribution of faces for the small COMMONPOOL. Note that the majority of images do not contain faces.\n\nAs part of our competition pipeline, images are by default blurred during the download process. In Table 7 we can see the results of training on a set of images with the size of our medium scale after filtering with each method, with and without the application of face blurring as provided by our detector. We can see that the difference in performance is small, which suggests that the application of face blurring does not significantly affect the performance on our downstream tasks. However, we note that this design decision may be more detrimental in generative settings, especially when a generative model needs to output faces. Our competition is primarily focused on discriminative tasks, and as such when designing our dataset, we wished to prioritize the safety and privacy of individuals through blurring faces in our download tooling by default.", "images": [], "items": [{"type": "text", "value": "**Table 6: Face detection performance on a set of 3293 random images from COMMONPOOL.**", "md": "**Table 6: Face detection performance on a set of 3293 random images from COMMONPOOL.**"}, {"type": "table", "rows": [["", "SCRFD-10G", "Amazon Rekognition"], ["Accuracy", "93.87", "96.57"], ["Precision", "75.87", "86.09"], ["Recall", "90.53", "93.75"]], "md": "| |SCRFD-10G|Amazon Rekognition|\n|---|---|---|\n|Accuracy|93.87|96.57|\n|Precision|75.87|86.09|\n|Recall|90.53|93.75|", "isPerfectTable": true, "csv": "\"\",\"SCRFD-10G\",\"Amazon Rekognition\"\n\"Accuracy\",\"93.87\",\"96.57\"\n\"Precision\",\"75.87\",\"86.09\"\n\"Recall\",\"90.53\",\"93.75\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\text{Frequency} & 0 & 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n\\text{mean:} & 0.57 & \\text{median:} & 0.00 & \\text{min:} & 0 & \\text{max:} & 228 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 6: Frequency of predicted number of faces in the small COMMONPOOL.\n\nFace blurring\n\nAs an extra step to safeguard against issues of privacy that may arise from the use of data scraped from the web, we include face blurring as part of our pool creation. To create face metadata, we use the SCRFD face detector [53] to extract bounding boxes for the faces in our images. These bounding boxes are included as part of the image metadata in our pool. We make use of the pretrained SCRFD-10G model. We use the same preprocessing as the one described in the official repository of the paper, with the exception of providing 224 \u00d7 224 input images (by padding each image to square and then resizing) to limit computation costs. Invoking this model provides us with bounding boxes along with an associated score, which we then compare against a threshold of 0.3 to keep or discard this bounding box. This threshold is the default one used in the repository of SCRFD for the visualization of bounding boxes, and we found it to perform well on our data as discussed next.\n\nIn Table 6 we can see the result of face detection on a set of 3293 images from COMMONPOOL. We evaluate the detection on whether the image has visible faces or not (where images such as cartoon drawings of non-real human faces are not considered as positives), and whether the detector has detected these visible faces. We considered an image as a true positive if all the clearly visible faces in the image were detected, based on the above thresholding process. We did not do extensive box labeling. True positives are instead determined by human inspection. We compare the quality of these detections with the Amazon Rekognition system, which is the one upon which the face detections on ImageNet were based [148]. Note that in this scenario, the recall of the detectors is more important than precision (as detecting a few more bounding boxes across our pool does not affect privacy).\n\nTo utilize these bounding boxes on our data, we apply a standard blurring pipeline, as proposed by Yang et al. [148]. The result of this process is an image where the faces are blurred and there is a smooth transition from blurred to clean parts of the image. In Figure 6 we see the distribution of faces for the small COMMONPOOL. Note that the majority of images do not contain faces.\n\nAs part of our competition pipeline, images are by default blurred during the download process. In Table 7 we can see the results of training on a set of images with the size of our medium scale after filtering with each method, with and without the application of face blurring as provided by our detector. We can see that the difference in performance is small, which suggests that the application of face blurring does not significantly affect the performance on our downstream tasks. However, we note that this design decision may be more detrimental in generative settings, especially when a generative model needs to output faces. Our competition is primarily focused on discriminative tasks, and as such when designing our dataset, we wished to prioritize the safety and privacy of individuals through blurring faces in our download tooling by default.", "md": "$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\text{Frequency} & 0 & 1 & 2 & 3 & 4 & 5 \\\\\n\\hline\n\\text{mean:} & 0.57 & \\text{median:} & 0.00 & \\text{min:} & 0 & \\text{max:} & 228 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 6: Frequency of predicted number of faces in the small COMMONPOOL.\n\nFace blurring\n\nAs an extra step to safeguard against issues of privacy that may arise from the use of data scraped from the web, we include face blurring as part of our pool creation. To create face metadata, we use the SCRFD face detector [53] to extract bounding boxes for the faces in our images. These bounding boxes are included as part of the image metadata in our pool. We make use of the pretrained SCRFD-10G model. We use the same preprocessing as the one described in the official repository of the paper, with the exception of providing 224 \u00d7 224 input images (by padding each image to square and then resizing) to limit computation costs. Invoking this model provides us with bounding boxes along with an associated score, which we then compare against a threshold of 0.3 to keep or discard this bounding box. This threshold is the default one used in the repository of SCRFD for the visualization of bounding boxes, and we found it to perform well on our data as discussed next.\n\nIn Table 6 we can see the result of face detection on a set of 3293 images from COMMONPOOL. We evaluate the detection on whether the image has visible faces or not (where images such as cartoon drawings of non-real human faces are not considered as positives), and whether the detector has detected these visible faces. We considered an image as a true positive if all the clearly visible faces in the image were detected, based on the above thresholding process. We did not do extensive box labeling. True positives are instead determined by human inspection. We compare the quality of these detections with the Amazon Rekognition system, which is the one upon which the face detections on ImageNet were based [148]. Note that in this scenario, the recall of the detectors is more important than precision (as detecting a few more bounding boxes across our pool does not affect privacy).\n\nTo utilize these bounding boxes on our data, we apply a standard blurring pipeline, as proposed by Yang et al. [148]. The result of this process is an image where the faces are blurred and there is a smooth transition from blurred to clean parts of the image. In Figure 6 we see the distribution of faces for the small COMMONPOOL. Note that the majority of images do not contain faces.\n\nAs part of our competition pipeline, images are by default blurred during the download process. In Table 7 we can see the results of training on a set of images with the size of our medium scale after filtering with each method, with and without the application of face blurring as provided by our detector. We can see that the difference in performance is small, which suggests that the application of face blurring does not significantly affect the performance on our downstream tasks. However, we note that this design decision may be more detrimental in generative settings, especially when a generative model needs to output faces. Our competition is primarily focused on discriminative tasks, and as such when designing our dataset, we wished to prioritize the safety and privacy of individuals through blurring faces in our download tooling by default."}]}, {"page": 30, "text": "Table 7: Effect of face blurring on zero-shot performance. Face blurring improves the privacy\npreservation of our dataset, while affecting model performance negligibly. Results shown for training\non a set of images with the size of our medium scale, after filtering with each method.\n  Filtering                                              Face blurring    ImageNet acc.     Avg. performance\n  CLIP score (B/32, thresh. 0.3) + English filtering           \u00d7               0.209               0.246\n                                                               \u2713               0.196               0.243\n   CLIP score (B/32, 30%)                                      \u00d7               0.287               0.301\n                                                               \u2713               0.282               0.298\nFinally, we evaluated the detector we used for potential biases. More specifically, we used the detector\non the validation set of the FairFace dataset [80]. We found that the central face of the image was\ndetected in all the images of the validation set, regardless of subgroup annotate in the dataset.\n                                                       30", "md": "|Filtering|Face blurring|ImageNet acc.|Avg. performance|\n|---|---|---|---|\n|CLIP score (B/32, thresh. 0.3) + English filtering|\u00d7|0.209|0.246|\n| |\u2713|0.196|0.243|\n|CLIP score (B/32, 30%)|\u00d7|0.287|0.301|\n| |\u2713|0.282|0.298|\n\nFinally, we evaluated the detector we used for potential biases. More specifically, we used the detector\non the validation set of the FairFace dataset [80]. We found that the central face of the image was\ndetected in all the images of the validation set, regardless of subgroup annotate in the dataset.", "images": [], "items": [{"type": "table", "rows": [["Filtering", "Face blurring", "ImageNet acc.", "Avg. performance"], ["CLIP score (B/32, thresh. 0.3) + English filtering", "\u00d7", "0.209", "0.246"], ["", "\u2713", "0.196", "0.243"], ["CLIP score (B/32, 30%)", "\u00d7", "0.287", "0.301"], ["", "\u2713", "0.282", "0.298"]], "md": "|Filtering|Face blurring|ImageNet acc.|Avg. performance|\n|---|---|---|---|\n|CLIP score (B/32, thresh. 0.3) + English filtering|\u00d7|0.209|0.246|\n| |\u2713|0.196|0.243|\n|CLIP score (B/32, 30%)|\u00d7|0.287|0.301|\n| |\u2713|0.282|0.298|", "isPerfectTable": true, "csv": "\"Filtering\",\"Face blurring\",\"ImageNet acc.\",\"Avg. performance\"\n\"CLIP score (B/32, thresh. 0.3) + English filtering\",\"\u00d7\",\"0.209\",\"0.246\"\n\"\",\"\u2713\",\"0.196\",\"0.243\"\n\"CLIP score (B/32, 30%)\",\"\u00d7\",\"0.287\",\"0.301\"\n\"\",\"\u2713\",\"0.282\",\"0.298\""}, {"type": "text", "value": "Finally, we evaluated the detector we used for potential biases. More specifically, we used the detector\non the validation set of the FairFace dataset [80]. We found that the central face of the image was\ndetected in all the images of the validation set, regardless of subgroup annotate in the dataset.", "md": "Finally, we evaluated the detector we used for potential biases. More specifically, we used the detector\non the validation set of the FairFace dataset [80]. We found that the central face of the image was\ndetected in all the images of the validation set, regardless of subgroup annotate in the dataset."}]}, {"page": 31, "text": "H      DATACOMP COMMONPOOL creation pipeline\nFigure 7: Data funnel from potential samples in Common Crawl to 13.1B image-text pairs that were\nsuitable for COMMONPOOL. We sampled uniformly 12.8B datapoints for the xlarge COMMONPOOL.\n                               Table 8: Provided metadata for COMMONPOOL.\n         Generation Time                      Label                                          Additional notes\n                                              uid\n                                              url                                          Link to the image.\n         Step 2                               text                                             Image caption.\n                                              original_width\n                                              original_height\n                                              sha256                           Safeguard for data poisoning.\n                                              clip_b32_similarity_score\n                                              clip_b32_image_features                         In separate file.\n                                              clip_b32_text_features                          In separate file.\n         Step 1                               clip_l14_similarity_score\n                                              clip_l14_image_features                         In separate file.\n                                              clip_l14_text_features                          In separate file.\n                                              face_bboxes\n                                              nsfw_image_score\n         Step 2, dropped during Step 3        nsfw_text_score\n                                              dedup_score\nCreating COMMONPOOL was a multistep process, which involved (1) parsing image urls and alt-text\nfrom Common Crawl dumps and downloading these images, (2) tagging images with metadata and\n(3) conducting safety content filtering and evaluation set duplication. In this section we provide an\noverview of the data pipeline used to create COMMONPOOL. For an overview of our \u201cdata funnel\u201d\nsee Figure 7.\n       1. For the first step, we use parse Common Crawl metadata files to harvest image-text pairs\n           (Section D). We use img2dataset [5] to obtain \u223c16.8B downloaded samples. This is the\n           first, unfiltered version of COMMONPOOL, and contains only basic information for our\n           images (i.e., the original image height, width, and alt-text caption). During this step we\n           also resize images such that their largest dimension does not exceed 512 pixels. This eases\n           storage requirements for large images, but is still larger than the 224 pixel resolution used\n           for later training stages.\n       2. For the second step, we process our unfiltered pool and create richer metadata for each\n           image-text pair. We generate the following for each sample:\n              \u2022 CLIP ViT-B/32 and CLIP ViT-L/14 image and text features, with their associated\n                 similarities.\n              \u2022 NSFW scores for the image and the text, using the analysis described in Appendix E.\n              \u2022 Deduplication score for the image, as described in Appendix F.\n                                                           31", "md": "```markdown\nH      DATACOMP COMMONPOOL creation pipeline\n\n$$\n\\text{Figure 7: Data funnel from potential samples in Common Crawl to 13.1B image-text pairs that were suitable for COMMONPOOL. We sampled uniformly 12.8B datapoints for the xlarge COMMONPOOL.}\n$$\n\n| Generation Time | Label | Additional notes |\n| --- | --- | --- |\n| uid | | |\n| url | Link to the image. | |\n| text | Image caption. | |\n| original_width | | |\n| original_height | | |\n| sha256 | Safeguard for data poisoning. | |\n| clip_b32_similarity_score | | |\n| clip_b32_image_features | In separate file. | |\n| clip_b32_text_features | In separate file. | |\n| clip_l14_similarity_score | | |\n| clip_l14_image_features | In separate file. | |\n| clip_l14_text_features | In separate file. | |\n| face_bboxes | | |\n| nsfw_image_score | | |\n| nsfw_text_score | | |\n| dedup_score | | |\n\nCreating COMMONPOOL was a multistep process, which involved:\n1. Parsing image urls and alt-text from Common Crawl dumps and downloading these images.\n2. Tagging images with metadata.\n3. Conducting safety content filtering and evaluation set duplication.\n\nIn this section we provide an overview of the data pipeline used to create COMMONPOOL. For an overview of our \"data funnel\" see Figure 7.\n\n1. For the first step, we use parse Common Crawl metadata files to harvest image-text pairs (Section D). We use img2dataset to obtain \u223c16.8B downloaded samples. This is the first, unfiltered version of COMMONPOOL, and contains only basic information for our images (i.e., the original image height, width, and alt-text caption). During this step we also resize images such that their largest dimension does not exceed 512 pixels. This eases storage requirements for large images, but is still larger than the 224 pixel resolution used for later training stages.\n\n2. For the second step, we process our unfiltered pool and create richer metadata for each image-text pair. We generate the following for each sample:\n- CLIP ViT-B/32 and CLIP ViT-L/14 image and text features, with their associated similarities.\n- NSFW scores for the image and the text.\n- Deduplication score for the image.\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nH      DATACOMP COMMONPOOL creation pipeline\n\n$$\n\\text{Figure 7: Data funnel from potential samples in Common Crawl to 13.1B image-text pairs that were suitable for COMMONPOOL. We sampled uniformly 12.8B datapoints for the xlarge COMMONPOOL.}\n$$", "md": "```markdown\nH      DATACOMP COMMONPOOL creation pipeline\n\n$$\n\\text{Figure 7: Data funnel from potential samples in Common Crawl to 13.1B image-text pairs that were suitable for COMMONPOOL. We sampled uniformly 12.8B datapoints for the xlarge COMMONPOOL.}\n$$"}, {"type": "table", "rows": [["Generation Time", "Label", "Additional notes"], ["uid", "", ""], ["url", "Link to the image.", ""], ["text", "Image caption.", ""], ["original_width", "", ""], ["original_height", "", ""], ["sha256", "Safeguard for data poisoning.", ""], ["clip_b32_similarity_score", "", ""], ["clip_b32_image_features", "In separate file.", ""], ["clip_b32_text_features", "In separate file.", ""], ["clip_l14_similarity_score", "", ""], ["clip_l14_image_features", "In separate file.", ""], ["clip_l14_text_features", "In separate file.", ""], ["face_bboxes", "", ""], ["nsfw_image_score", "", ""], ["nsfw_text_score", "", ""], ["dedup_score", "", ""]], "md": "| Generation Time | Label | Additional notes |\n| --- | --- | --- |\n| uid | | |\n| url | Link to the image. | |\n| text | Image caption. | |\n| original_width | | |\n| original_height | | |\n| sha256 | Safeguard for data poisoning. | |\n| clip_b32_similarity_score | | |\n| clip_b32_image_features | In separate file. | |\n| clip_b32_text_features | In separate file. | |\n| clip_l14_similarity_score | | |\n| clip_l14_image_features | In separate file. | |\n| clip_l14_text_features | In separate file. | |\n| face_bboxes | | |\n| nsfw_image_score | | |\n| nsfw_text_score | | |\n| dedup_score | | |", "isPerfectTable": true, "csv": "\"Generation Time\",\"Label\",\"Additional notes\"\n\"uid\",\"\",\"\"\n\"url\",\"Link to the image.\",\"\"\n\"text\",\"Image caption.\",\"\"\n\"original_width\",\"\",\"\"\n\"original_height\",\"\",\"\"\n\"sha256\",\"Safeguard for data poisoning.\",\"\"\n\"clip_b32_similarity_score\",\"\",\"\"\n\"clip_b32_image_features\",\"In separate file.\",\"\"\n\"clip_b32_text_features\",\"In separate file.\",\"\"\n\"clip_l14_similarity_score\",\"\",\"\"\n\"clip_l14_image_features\",\"In separate file.\",\"\"\n\"clip_l14_text_features\",\"In separate file.\",\"\"\n\"face_bboxes\",\"\",\"\"\n\"nsfw_image_score\",\"\",\"\"\n\"nsfw_text_score\",\"\",\"\"\n\"dedup_score\",\"\",\"\""}, {"type": "text", "value": "Creating COMMONPOOL was a multistep process, which involved:\n1. Parsing image urls and alt-text from Common Crawl dumps and downloading these images.\n2. Tagging images with metadata.\n3. Conducting safety content filtering and evaluation set duplication.\n\nIn this section we provide an overview of the data pipeline used to create COMMONPOOL. For an overview of our \"data funnel\" see Figure 7.\n\n1. For the first step, we use parse Common Crawl metadata files to harvest image-text pairs (Section D). We use img2dataset to obtain \u223c16.8B downloaded samples. This is the first, unfiltered version of COMMONPOOL, and contains only basic information for our images (i.e., the original image height, width, and alt-text caption). During this step we also resize images such that their largest dimension does not exceed 512 pixels. This eases storage requirements for large images, but is still larger than the 224 pixel resolution used for later training stages.\n\n2. For the second step, we process our unfiltered pool and create richer metadata for each image-text pair. We generate the following for each sample:\n- CLIP ViT-B/32 and CLIP ViT-L/14 image and text features, with their associated similarities.\n- NSFW scores for the image and the text.\n- Deduplication score for the image.\n```", "md": "Creating COMMONPOOL was a multistep process, which involved:\n1. Parsing image urls and alt-text from Common Crawl dumps and downloading these images.\n2. Tagging images with metadata.\n3. Conducting safety content filtering and evaluation set duplication.\n\nIn this section we provide an overview of the data pipeline used to create COMMONPOOL. For an overview of our \"data funnel\" see Figure 7.\n\n1. For the first step, we use parse Common Crawl metadata files to harvest image-text pairs (Section D). We use img2dataset to obtain \u223c16.8B downloaded samples. This is the first, unfiltered version of COMMONPOOL, and contains only basic information for our images (i.e., the original image height, width, and alt-text caption). During this step we also resize images such that their largest dimension does not exceed 512 pixels. This eases storage requirements for large images, but is still larger than the 224 pixel resolution used for later training stages.\n\n2. For the second step, we process our unfiltered pool and create richer metadata for each image-text pair. We generate the following for each sample:\n- CLIP ViT-B/32 and CLIP ViT-L/14 image and text features, with their associated similarities.\n- NSFW scores for the image and the text.\n- Deduplication score for the image.\n```"}]}, {"page": 32, "text": "            \u2022 Bounding boxes for faces detected in the image, using the method described in\n              Appendix G.\n      3. For the third and final step, we filter our image-text pairs based on the metadata generated\n         during the second stage. We filter out image-text pairs where the NSFW and deduplication\n         scores exceed the respective thresholds (Section E). From the images that pass through\n         this filtering, we keep only the desired amount (e.g., 12.8B images from the xlarge\n         COMMONPOOL). Smaller pools are telescoping subsets of larger pools. We package\n         the metadata and image urls, which is made publicly available to the participants. Note, we\n         do not release raw image data but rather image urls pointing to images.\nA summary of the metadata for each sample is found in Table 8. To validate our pipeline for\nduplication and CLIP feature correctness, we also take ImageNet train though metadata generation as\na unit test. Using the deduplication features, we detect that 100% of the images are in fact duplicates.\nAdditionally using the CLIP ViT-B/32 and CLIP ViT-L/14 image features and corresponding text\nfeatures from OpenAI\u2019s 80-prompt ensemble, we achieve 63.36% and 75.54% top-1 accuracies,\nwhich match the performance reported in the CLIP paper [111].\nWhen creating pools of different scale (i.e., number of samples), we ensure that smaller pools\nare subsets of larger pools. For instance, the small COMMONPOOL is a subset of the xlarge\nCOMMONPOOL.\nAfter COMMONPOOL is created, the participants can then download the final image-text pairs using\nthe provided files via img2dataset. To further ease the computational burden on participants, we\nadditionally provide metadata for each sample in COMMONPOOL. Note that when downloading,\nour img2dataset configuration automatically blurs faces. Hence this is an automatic step on not\nsomething participants must do ad hoc.\nI   COMMONPOOL statistics\nTo provide more information about the kinds of samples in our COMMONPOOL, we conduct additional\nanalysis on the small pool, which is an i.i.d. sample of downloaded data and a subset of the larger\npools.\nIn Figure 8 we show CLIP similarity similarity scores between images and their corresponding text.\nWe notice a flatter distribution of CLIP ViT-L/14 scores than corresponding B/32 scores.\nTurning our attention to images in COMMONPOOL, in Figure 9, we visualize the aspect ratios and\nsizes of original images (i.e., before they are downloaded and resized). In Figure 10, we display a\ndistribution of image height and width after download resizing. Notice that the majority of images\nare around 224 \u00d7 224 pixels, which is the final resized resolution used for training.\nAnalysing the textual component of each sample, we visualize frequency of the number of CLIP BPE\ntokens in the captions (Figure 11) and most common languages (Figure 12). Token counts follow\na long-tailed distribution with much more mass in the short sequence range, while English is the\npredominant language in COMMONPOOL according to fasttext and cld3.\nWe also look at url statistics. In Figure 13 we see common domain names in COMMONPOOL (e.g.,\nwordpress domains) and common suffixes (e.g., .com or .net).\n                                                  32", "md": "# Document\n\n\u2022 Bounding boxes for faces detected in the image, using the method described in Appendix G.\n\n1. For the third and final step, we filter our image-text pairs based on the metadata generated during the second stage. We filter out image-text pairs where the NSFW and deduplication scores exceed the respective thresholds (Section E). From the images that pass through this filtering, we keep only the desired amount (e.g., 12.8B images from the xlarge COMMONPOOL). Smaller pools are telescoping subsets of larger pools. We package the metadata and image urls, which is made publicly available to the participants. Note, we do not release raw image data but rather image urls pointing to images.\n\nA summary of the metadata for each sample is found in Table 8. To validate our pipeline for duplication and CLIP feature correctness, we also take ImageNet train though metadata generation as a unit test. Using the deduplication features, we detect that 100% of the images are in fact duplicates. Additionally using the CLIP ViT-B/32 and CLIP ViT-L/14 image features and corresponding text features from OpenAI\u2019s 80-prompt ensemble, we achieve 63.36% and 75.54% top-1 accuracies, which match the performance reported in the CLIP paper [111].\n\nWhen creating pools of different scale (i.e., number of samples), we ensure that smaller pools are subsets of larger pools. For instance, the small COMMONPOOL is a subset of the xlarge COMMONPOOL.\n\nAfter COMMONPOOL is created, the participants can then download the final image-text pairs using the provided files via img2dataset. To further ease the computational burden on participants, we additionally provide metadata for each sample in COMMONPOOL. Note that when downloading, our img2dataset configuration automatically blurs faces. Hence this is an automatic step on not something participants must do ad hoc.\n\n# COMMONPOOL statistics\n\nTo provide more information about the kinds of samples in our COMMONPOOL, we conduct additional analysis on the small pool, which is an i.i.d. sample of downloaded data and a subset of the larger pools.\n\nIn Figure 8 we show CLIP similarity similarity scores between images and their corresponding text. We notice a flatter distribution of CLIP ViT-L/14 scores than corresponding B/32 scores.\n\nTurning our attention to images in COMMONPOOL, in Figure 9, we visualize the aspect ratios and sizes of original images (i.e., before they are downloaded and resized). In Figure 10, we display a distribution of image height and width after download resizing. Notice that the majority of images are around 224 \u00d7 224 pixels, which is the final resized resolution used for training.\n\nAnalysing the textual component of each sample, we visualize frequency of the number of CLIP BPE tokens in the captions (Figure 11) and most common languages (Figure 12). Token counts follow a long-tailed distribution with much more mass in the short sequence range, while English is the predominant language in COMMONPOOL according to fasttext and cld3.\n\nWe also look at url statistics. In Figure 13 we see common domain names in COMMONPOOL (e.g., wordpress domains) and common suffixes (e.g., .com or .net).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "\u2022 Bounding boxes for faces detected in the image, using the method described in Appendix G.\n\n1. For the third and final step, we filter our image-text pairs based on the metadata generated during the second stage. We filter out image-text pairs where the NSFW and deduplication scores exceed the respective thresholds (Section E). From the images that pass through this filtering, we keep only the desired amount (e.g., 12.8B images from the xlarge COMMONPOOL). Smaller pools are telescoping subsets of larger pools. We package the metadata and image urls, which is made publicly available to the participants. Note, we do not release raw image data but rather image urls pointing to images.\n\nA summary of the metadata for each sample is found in Table 8. To validate our pipeline for duplication and CLIP feature correctness, we also take ImageNet train though metadata generation as a unit test. Using the deduplication features, we detect that 100% of the images are in fact duplicates. Additionally using the CLIP ViT-B/32 and CLIP ViT-L/14 image features and corresponding text features from OpenAI\u2019s 80-prompt ensemble, we achieve 63.36% and 75.54% top-1 accuracies, which match the performance reported in the CLIP paper [111].\n\nWhen creating pools of different scale (i.e., number of samples), we ensure that smaller pools are subsets of larger pools. For instance, the small COMMONPOOL is a subset of the xlarge COMMONPOOL.\n\nAfter COMMONPOOL is created, the participants can then download the final image-text pairs using the provided files via img2dataset. To further ease the computational burden on participants, we additionally provide metadata for each sample in COMMONPOOL. Note that when downloading, our img2dataset configuration automatically blurs faces. Hence this is an automatic step on not something participants must do ad hoc.", "md": "\u2022 Bounding boxes for faces detected in the image, using the method described in Appendix G.\n\n1. For the third and final step, we filter our image-text pairs based on the metadata generated during the second stage. We filter out image-text pairs where the NSFW and deduplication scores exceed the respective thresholds (Section E). From the images that pass through this filtering, we keep only the desired amount (e.g., 12.8B images from the xlarge COMMONPOOL). Smaller pools are telescoping subsets of larger pools. We package the metadata and image urls, which is made publicly available to the participants. Note, we do not release raw image data but rather image urls pointing to images.\n\nA summary of the metadata for each sample is found in Table 8. To validate our pipeline for duplication and CLIP feature correctness, we also take ImageNet train though metadata generation as a unit test. Using the deduplication features, we detect that 100% of the images are in fact duplicates. Additionally using the CLIP ViT-B/32 and CLIP ViT-L/14 image features and corresponding text features from OpenAI\u2019s 80-prompt ensemble, we achieve 63.36% and 75.54% top-1 accuracies, which match the performance reported in the CLIP paper [111].\n\nWhen creating pools of different scale (i.e., number of samples), we ensure that smaller pools are subsets of larger pools. For instance, the small COMMONPOOL is a subset of the xlarge COMMONPOOL.\n\nAfter COMMONPOOL is created, the participants can then download the final image-text pairs using the provided files via img2dataset. To further ease the computational burden on participants, we additionally provide metadata for each sample in COMMONPOOL. Note that when downloading, our img2dataset configuration automatically blurs faces. Hence this is an automatic step on not something participants must do ad hoc."}, {"type": "heading", "lvl": 1, "value": "COMMONPOOL statistics", "md": "# COMMONPOOL statistics"}, {"type": "text", "value": "To provide more information about the kinds of samples in our COMMONPOOL, we conduct additional analysis on the small pool, which is an i.i.d. sample of downloaded data and a subset of the larger pools.\n\nIn Figure 8 we show CLIP similarity similarity scores between images and their corresponding text. We notice a flatter distribution of CLIP ViT-L/14 scores than corresponding B/32 scores.\n\nTurning our attention to images in COMMONPOOL, in Figure 9, we visualize the aspect ratios and sizes of original images (i.e., before they are downloaded and resized). In Figure 10, we display a distribution of image height and width after download resizing. Notice that the majority of images are around 224 \u00d7 224 pixels, which is the final resized resolution used for training.\n\nAnalysing the textual component of each sample, we visualize frequency of the number of CLIP BPE tokens in the captions (Figure 11) and most common languages (Figure 12). Token counts follow a long-tailed distribution with much more mass in the short sequence range, while English is the predominant language in COMMONPOOL according to fasttext and cld3.\n\nWe also look at url statistics. In Figure 13 we see common domain names in COMMONPOOL (e.g., wordpress domains) and common suffixes (e.g., .com or .net).", "md": "To provide more information about the kinds of samples in our COMMONPOOL, we conduct additional analysis on the small pool, which is an i.i.d. sample of downloaded data and a subset of the larger pools.\n\nIn Figure 8 we show CLIP similarity similarity scores between images and their corresponding text. We notice a flatter distribution of CLIP ViT-L/14 scores than corresponding B/32 scores.\n\nTurning our attention to images in COMMONPOOL, in Figure 9, we visualize the aspect ratios and sizes of original images (i.e., before they are downloaded and resized). In Figure 10, we display a distribution of image height and width after download resizing. Notice that the majority of images are around 224 \u00d7 224 pixels, which is the final resized resolution used for training.\n\nAnalysing the textual component of each sample, we visualize frequency of the number of CLIP BPE tokens in the captions (Figure 11) and most common languages (Figure 12). Token counts follow a long-tailed distribution with much more mass in the short sequence range, while English is the predominant language in COMMONPOOL according to fasttext and cld3.\n\nWe also look at url statistics. In Figure 13 we see common domain names in COMMONPOOL (e.g., wordpress domains) and common suffixes (e.g., .com or .net)."}]}, {"page": 33, "text": "Figure 8: Image-text similarity score distributions using CLIP ViT-B/32 (left) and ViT-L/14 (right)\nmodels. We plot samples from the small COMMONPOOL, which are an i.i.d. sample of the xlarge\nCOMMONPOOL.\n          Figure 9: Statistics for images in the small COMMONPOOL, before applying resizing.\n                                               512px\n                                                                     224px\n                                                                     384px\nFigure 10: Image pixel heatmap. Each entry in the above heatmap represents the estimated\nprobability that a pixel is occupied. The center entry has a value of 1.0 as every image has a center\npixel. We compute the heatmap over the small COMMONPOOL. Note that image sizes are bounded\n                        Expectation value of a pixel being occupied for 12.8m pool after download (max dim no more than 512px)\nas we resize all images such that their max dimension does not exceed 512 pixels during dataset\ndownload.\n                                                                        33", "md": "Figure 8: Image-text similarity score distributions using CLIP ViT-B/32\n\nFigure 8: Image-text similarity score distributions using CLIP ViT-L/14\n\nWe plot samples from the small COMMONPOOL, which are an i.i.d. sample of the xlarge COMMONPOOL.\n\nFigure 9: Statistics for images in the small COMMONPOOL, before applying resizing\n\n512px\n224px\n384px\n\nFigure 10: Image pixel heatmap\n\nEach entry in the above heatmap represents the estimated probability that a pixel is occupied. The center entry has a value of 1.0 as every image has a center pixel. We compute the heatmap over the small COMMONPOOL. Note that image sizes are bounded.\n\nExpectation value of a pixel being occupied for 12.8m pool after download (max dim no more than 512px)\n\nas we resize all images such that their max dimension does not exceed 512 pixels during dataset download.\n\n33", "images": [{"name": "page-33-1.jpg", "height": 95, "width": 95, "x": 255, "y": 516}], "items": [{"type": "text", "value": "Figure 8: Image-text similarity score distributions using CLIP ViT-B/32\n\nFigure 8: Image-text similarity score distributions using CLIP ViT-L/14\n\nWe plot samples from the small COMMONPOOL, which are an i.i.d. sample of the xlarge COMMONPOOL.\n\nFigure 9: Statistics for images in the small COMMONPOOL, before applying resizing\n\n512px\n224px\n384px\n\nFigure 10: Image pixel heatmap\n\nEach entry in the above heatmap represents the estimated probability that a pixel is occupied. The center entry has a value of 1.0 as every image has a center pixel. We compute the heatmap over the small COMMONPOOL. Note that image sizes are bounded.\n\nExpectation value of a pixel being occupied for 12.8m pool after download (max dim no more than 512px)\n\nas we resize all images such that their max dimension does not exceed 512 pixels during dataset download.\n\n33", "md": "Figure 8: Image-text similarity score distributions using CLIP ViT-B/32\n\nFigure 8: Image-text similarity score distributions using CLIP ViT-L/14\n\nWe plot samples from the small COMMONPOOL, which are an i.i.d. sample of the xlarge COMMONPOOL.\n\nFigure 9: Statistics for images in the small COMMONPOOL, before applying resizing\n\n512px\n224px\n384px\n\nFigure 10: Image pixel heatmap\n\nEach entry in the above heatmap represents the estimated probability that a pixel is occupied. The center entry has a value of 1.0 as every image has a center pixel. We compute the heatmap over the small COMMONPOOL. Note that image sizes are bounded.\n\nExpectation value of a pixel being occupied for 12.8m pool after download (max dim no more than 512px)\n\nas we resize all images such that their max dimension does not exceed 512 pixels during dataset download.\n\n33"}]}, {"page": 34, "text": "Figure 11: Distribution of token length for alt-text in the small COMMONPOOL. The CLIP BPE\ntokenizer is used for tokenization.\n                                     fasttext                                                   cld3\nFigure 12: Counts for the top 25 most frequent languages in the small COMMONPOOL, as predicted\nby fasttext (left) and cld3 (right).\nFigure 13: Counts for the top 25 most frequent domains (left) and suffi                               xes (right) in the small\nCOMMONPOOL.\n                                                                           34", "md": "# Token Length Distribution\n\n## Figure 11: Distribution of token length for alt-text in the small COMMONPOOL\n\nThe CLIP BPE tokenizer is used for tokenization.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Token Length} & \\text{Frequency} \\\\\n\\hline\n1 & 100 \\\\\n2 & 150 \\\\\n3 & 75 \\\\\n4 & 50 \\\\\n5 & 25 \\\\\n\\hline\n\\end{array}\n$$\n\n## Figure 12: Top 25 Most Frequent Languages in the small COMMONPOOL\n\nCounts as predicted by fasttext (left) and cld3 (right).\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Rank} & \\text{Language (fasttext)} & \\text{Language (cld3)} \\\\\n\\hline\n1 & English & English \\\\\n2 & Spanish & Spanish \\\\\n3 & French & French \\\\\n4 & German & German \\\\\n5 & Chinese & Chinese \\\\\n\\hline\n\\end{array}\n$$\n\n## Figure 13: Top 25 Most Frequent Domains and Suffixes in the small COMMONPOOL\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Rank} & \\text{Domain} & \\text{Suffix} \\\\\n\\hline\n1 & example.com & .com \\\\\n2 & test.org & .org \\\\\n3 & website.net & .net \\\\\n4 & company.com & .com \\\\\n5 & blog.org & .org \\\\\n\\hline\n\\end{array}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Token Length Distribution", "md": "# Token Length Distribution"}, {"type": "heading", "lvl": 2, "value": "Figure 11: Distribution of token length for alt-text in the small COMMONPOOL", "md": "## Figure 11: Distribution of token length for alt-text in the small COMMONPOOL"}, {"type": "text", "value": "The CLIP BPE tokenizer is used for tokenization.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Token Length} & \\text{Frequency} \\\\\n\\hline\n1 & 100 \\\\\n2 & 150 \\\\\n3 & 75 \\\\\n4 & 50 \\\\\n5 & 25 \\\\\n\\hline\n\\end{array}\n$$", "md": "The CLIP BPE tokenizer is used for tokenization.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Token Length} & \\text{Frequency} \\\\\n\\hline\n1 & 100 \\\\\n2 & 150 \\\\\n3 & 75 \\\\\n4 & 50 \\\\\n5 & 25 \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "heading", "lvl": 2, "value": "Figure 12: Top 25 Most Frequent Languages in the small COMMONPOOL", "md": "## Figure 12: Top 25 Most Frequent Languages in the small COMMONPOOL"}, {"type": "text", "value": "Counts as predicted by fasttext (left) and cld3 (right).\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Rank} & \\text{Language (fasttext)} & \\text{Language (cld3)} \\\\\n\\hline\n1 & English & English \\\\\n2 & Spanish & Spanish \\\\\n3 & French & French \\\\\n4 & German & German \\\\\n5 & Chinese & Chinese \\\\\n\\hline\n\\end{array}\n$$", "md": "Counts as predicted by fasttext (left) and cld3 (right).\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Rank} & \\text{Language (fasttext)} & \\text{Language (cld3)} \\\\\n\\hline\n1 & English & English \\\\\n2 & Spanish & Spanish \\\\\n3 & French & French \\\\\n4 & German & German \\\\\n5 & Chinese & Chinese \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "heading", "lvl": 2, "value": "Figure 13: Top 25 Most Frequent Domains and Suffixes in the small COMMONPOOL", "md": "## Figure 13: Top 25 Most Frequent Domains and Suffixes in the small COMMONPOOL"}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Rank} & \\text{Domain} & \\text{Suffix} \\\\\n\\hline\n1 & example.com & .com \\\\\n2 & test.org & .org \\\\\n3 & website.net & .net \\\\\n4 & company.com & .com \\\\\n5 & blog.org & .org \\\\\n\\hline\n\\end{array}\n$$", "md": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Rank} & \\text{Domain} & \\text{Suffix} \\\\\n\\hline\n1 & example.com & .com \\\\\n2 & test.org & .org \\\\\n3 & website.net & .net \\\\\n4 & company.com & .com \\\\\n5 & blog.org & .org \\\\\n\\hline\n\\end{array}\n$$"}]}, {"page": 35, "text": "J    Efficient training on data subsets\nWhen training at large scale, it is important to use efficient access patterns to load training data. This\ntypically means that data must be loaded using large sequential reads instead of random reads in\norder to maximize throughput. In DATACOMP, this is facilitated by the WebDataset5 format which\nstores the training examples in tar files (called \u201cshards\u201d) and WebDataLoader which makes it easy to\nload data stored in this format.\nGiven an arbitrary subset of a pool, we would like to effi            ciently train on that subset. Because\nWebDataset format does not permit efficient random access (a feature inherited from tar), we must\nread through the entire pool to select the required images. There are two ways to implement this\nfiltering:\n       1. Filter during training: we apply a predicate during training data loading that discards data\n          not present in the subset.\n       2. Filter before training: we iterate over the pool, selecting the images in the subset, and\n          write them to a new WebDataset.\nAfter some profiling, we concluded that option 1 had too much overhead in the case where the subset\nis much smaller than the pool. To see why, note that if the subset is an p-fraction of the pool size,\nthen we would end up reading a 1/p factor more data than needed for training. Instead, we give an\nimplementation of option 2, which performs at most twice as many reads as needed for training.6\nOur tool, called the resharder, reads a set of uids in NumPy array format, scans through the pool,\nselecting those examples, and writes them to a new WebDataset. The resharder uses multiprocessing\nto make good use of hardware and can be distributed over many computers to further increase\nthroughput. The resharder also supports streaming data to and from cloud storage such as Amazon\nS3. The resharder is provided to participants as part of the competition tooling.\nK     Effect of duplicates in the training data\nGiven that COMMONPOOL was constructed by scraping the web for image and text pairs, there\nis a likelihood that some of our images are duplicates of each other, even if they originated from\ndifferent web sources and have different captions. Here we examine the effect of removing such\nduplicates. We used the technique proposed by Webster et al. [144], where CLIP image features are\nfirst compressed and then used to do an approximate nearest neighbor search. After this process, two\nimages x and y are considered duplicates if |dADC(x,x)\u2212dADC(x,y)|              < TADC, where TADC is some\n                                                           dADC(x,x)\nthreshold and dADC(x, x) is the distance of a vector with its quantized version used for approximate\nnearest neighbor search. For each image, we search duplicates across its 1000 nearest neighbors, and\nkeep it if it\u2019s the one with the highest CLIP ViT-L/14 similarity score across its duplicates. Results\ncan be seen in Table 9, both when this technique is used by itself and in conjunction with ViT-B/32\nfiltering. We can see that results are similar to when only using CLIP filtering.\n    5https://github.com/webdataset/webdataset\n    6Since in DATACOMP, the number of examples seen is equal to the pool size.\nTable 9: Effect of deduplication of training set for the medium size COMMONPOOL. The filtering\nperformed here is CLIP B32 score top 30% (see Table 26). Higher threshold values lead to more\nsamples being labeled as duplicates.\n      Subset                            Training dataset size   ImageNet accuracy     Average performance\n      TADC = 0.1, without filtering            99.8M                   0.195                   0.275\n      TADC = 0.2, without filtering            85.9M                   0.200                   0.277\n      TADC = 0.5, without filtering            29.6M                   0.227                   0.295\n      TADC = 0.1, with filtering               33.5M                   0.288                   0.337\n      TADC = 0.2, with filtering               30.6M                   0.289                   0.337\n      TADC = 0.5, with filtering               15.5M                   0.252                   0.311\n                                                        35", "md": "Efficient training on data subsets\n\nWhen training at large scale, it is important to use efficient access patterns to load training data. This typically means that data must be loaded using large sequential reads instead of random reads in order to maximize throughput. In DATACOMP, this is facilitated by the WebDataset5 format which stores the training examples in tar files (called \u201cshards\u201d) and WebDataLoader which makes it easy to load data stored in this format.\n\nGiven an arbitrary subset of a pool, we would like to efficiently train on that subset. Because WebDataset format does not permit efficient random access (a feature inherited from tar), we must read through the entire pool to select the required images. There are two ways to implement this filtering:\n\n1. Filter during training: we apply a predicate during training data loading that discards data not present in the subset.\n2. Filter before training: we iterate over the pool, selecting the images in the subset, and write them to a new WebDataset.\n\nAfter some profiling, we concluded that option 1 had too much overhead in the case where the subset is much smaller than the pool. To see why, note that if the subset is an $$p$$-fraction of the pool size, then we would end up reading a $$\\frac{1}{p}$$ factor more data than needed for training. Instead, we give an implementation of option 2, which performs at most twice as many reads as needed for training6.\n\nOur tool, called the resharder, reads a set of uids in NumPy array format, scans through the pool, selecting those examples, and writes them to a new WebDataset. The resharder uses multiprocessing to make good use of hardware and can be distributed over many computers to further increase throughput. The resharder also supports streaming data to and from cloud storage such as Amazon S3. The resharder is provided to participants as part of the competition tooling.\n\nEffect of duplicates in the training data\n\nGiven that COMMONPOOL was constructed by scraping the web for image and text pairs, there is a likelihood that some of our images are duplicates of each other, even if they originated from different web sources and have different captions. Here we examine the effect of removing such duplicates. We used the technique proposed by Webster et al. [144], where CLIP image features are first compressed and then used to do an approximate nearest neighbor search. After this process, two images $$x$$ and $$y$$ are considered duplicates if $$|d_{ADC}(x,x) - d_{ADC}(x,y)| < T_{ADC}$$, where $$T_{ADC}$$ is some threshold and $$d_{ADC}(x, x)$$ is the distance of a vector with its quantized version used for approximate nearest neighbor search. For each image, we search duplicates across its 1000 nearest neighbors, and keep it if it\u2019s the one with the highest CLIP ViT-L/14 similarity score across its duplicates. Results can be seen in Table 9, both when this technique is used by itself and in conjunction with ViT-B/32 filtering.\n\n5https://github.com/webdataset/webdataset\n\nSince in DATACOMP, the number of examples seen is equal to the pool size.\n\n|Subset|Training dataset size|ImageNet accuracy|Average performance|\n|---|---|---|---|\n|T_{ADC} = 0.1, without filtering|99.8M|0.195|0.275|\n|T_{ADC} = 0.2, without filtering|85.9M|0.200|0.277|\n|T_{ADC} = 0.5, without filtering|29.6M|0.227|0.295|\n|T_{ADC} = 0.1, with filtering|33.5M|0.288|0.337|\n|T_{ADC} = 0.2, with filtering|30.6M|0.289|0.337|\n|T_{ADC} = 0.5, with filtering|15.5M|0.252|0.311|\n\nTable 9: Effect of deduplication of training set for the medium size COMMONPOOL. The filtering performed here is CLIP B32 score top 30% (see Table 26). Higher threshold values lead to more samples being labeled as duplicates.", "images": [], "items": [{"type": "text", "value": "Efficient training on data subsets\n\nWhen training at large scale, it is important to use efficient access patterns to load training data. This typically means that data must be loaded using large sequential reads instead of random reads in order to maximize throughput. In DATACOMP, this is facilitated by the WebDataset5 format which stores the training examples in tar files (called \u201cshards\u201d) and WebDataLoader which makes it easy to load data stored in this format.\n\nGiven an arbitrary subset of a pool, we would like to efficiently train on that subset. Because WebDataset format does not permit efficient random access (a feature inherited from tar), we must read through the entire pool to select the required images. There are two ways to implement this filtering:\n\n1. Filter during training: we apply a predicate during training data loading that discards data not present in the subset.\n2. Filter before training: we iterate over the pool, selecting the images in the subset, and write them to a new WebDataset.\n\nAfter some profiling, we concluded that option 1 had too much overhead in the case where the subset is much smaller than the pool. To see why, note that if the subset is an $$p$$-fraction of the pool size, then we would end up reading a $$\\frac{1}{p}$$ factor more data than needed for training. Instead, we give an implementation of option 2, which performs at most twice as many reads as needed for training6.\n\nOur tool, called the resharder, reads a set of uids in NumPy array format, scans through the pool, selecting those examples, and writes them to a new WebDataset. The resharder uses multiprocessing to make good use of hardware and can be distributed over many computers to further increase throughput. The resharder also supports streaming data to and from cloud storage such as Amazon S3. The resharder is provided to participants as part of the competition tooling.\n\nEffect of duplicates in the training data\n\nGiven that COMMONPOOL was constructed by scraping the web for image and text pairs, there is a likelihood that some of our images are duplicates of each other, even if they originated from different web sources and have different captions. Here we examine the effect of removing such duplicates. We used the technique proposed by Webster et al. [144], where CLIP image features are first compressed and then used to do an approximate nearest neighbor search. After this process, two images $$x$$ and $$y$$ are considered duplicates if $$|d_{ADC}(x,x) - d_{ADC}(x,y)| < T_{ADC}$$, where $$T_{ADC}$$ is some threshold and $$d_{ADC}(x, x)$$ is the distance of a vector with its quantized version used for approximate nearest neighbor search. For each image, we search duplicates across its 1000 nearest neighbors, and keep it if it\u2019s the one with the highest CLIP ViT-L/14 similarity score across its duplicates. Results can be seen in Table 9, both when this technique is used by itself and in conjunction with ViT-B/32 filtering.\n\n5https://github.com/webdataset/webdataset\n\nSince in DATACOMP, the number of examples seen is equal to the pool size.", "md": "Efficient training on data subsets\n\nWhen training at large scale, it is important to use efficient access patterns to load training data. This typically means that data must be loaded using large sequential reads instead of random reads in order to maximize throughput. In DATACOMP, this is facilitated by the WebDataset5 format which stores the training examples in tar files (called \u201cshards\u201d) and WebDataLoader which makes it easy to load data stored in this format.\n\nGiven an arbitrary subset of a pool, we would like to efficiently train on that subset. Because WebDataset format does not permit efficient random access (a feature inherited from tar), we must read through the entire pool to select the required images. There are two ways to implement this filtering:\n\n1. Filter during training: we apply a predicate during training data loading that discards data not present in the subset.\n2. Filter before training: we iterate over the pool, selecting the images in the subset, and write them to a new WebDataset.\n\nAfter some profiling, we concluded that option 1 had too much overhead in the case where the subset is much smaller than the pool. To see why, note that if the subset is an $$p$$-fraction of the pool size, then we would end up reading a $$\\frac{1}{p}$$ factor more data than needed for training. Instead, we give an implementation of option 2, which performs at most twice as many reads as needed for training6.\n\nOur tool, called the resharder, reads a set of uids in NumPy array format, scans through the pool, selecting those examples, and writes them to a new WebDataset. The resharder uses multiprocessing to make good use of hardware and can be distributed over many computers to further increase throughput. The resharder also supports streaming data to and from cloud storage such as Amazon S3. The resharder is provided to participants as part of the competition tooling.\n\nEffect of duplicates in the training data\n\nGiven that COMMONPOOL was constructed by scraping the web for image and text pairs, there is a likelihood that some of our images are duplicates of each other, even if they originated from different web sources and have different captions. Here we examine the effect of removing such duplicates. We used the technique proposed by Webster et al. [144], where CLIP image features are first compressed and then used to do an approximate nearest neighbor search. After this process, two images $$x$$ and $$y$$ are considered duplicates if $$|d_{ADC}(x,x) - d_{ADC}(x,y)| < T_{ADC}$$, where $$T_{ADC}$$ is some threshold and $$d_{ADC}(x, x)$$ is the distance of a vector with its quantized version used for approximate nearest neighbor search. For each image, we search duplicates across its 1000 nearest neighbors, and keep it if it\u2019s the one with the highest CLIP ViT-L/14 similarity score across its duplicates. Results can be seen in Table 9, both when this technique is used by itself and in conjunction with ViT-B/32 filtering.\n\n5https://github.com/webdataset/webdataset\n\nSince in DATACOMP, the number of examples seen is equal to the pool size."}, {"type": "table", "rows": [["Subset", "Training dataset size", "ImageNet accuracy", "Average performance"], ["T_{ADC} = 0.1, without filtering", "99.8M", "0.195", "0.275"], ["T_{ADC} = 0.2, without filtering", "85.9M", "0.200", "0.277"], ["T_{ADC} = 0.5, without filtering", "29.6M", "0.227", "0.295"], ["T_{ADC} = 0.1, with filtering", "33.5M", "0.288", "0.337"], ["T_{ADC} = 0.2, with filtering", "30.6M", "0.289", "0.337"], ["T_{ADC} = 0.5, with filtering", "15.5M", "0.252", "0.311"]], "md": "|Subset|Training dataset size|ImageNet accuracy|Average performance|\n|---|---|---|---|\n|T_{ADC} = 0.1, without filtering|99.8M|0.195|0.275|\n|T_{ADC} = 0.2, without filtering|85.9M|0.200|0.277|\n|T_{ADC} = 0.5, without filtering|29.6M|0.227|0.295|\n|T_{ADC} = 0.1, with filtering|33.5M|0.288|0.337|\n|T_{ADC} = 0.2, with filtering|30.6M|0.289|0.337|\n|T_{ADC} = 0.5, with filtering|15.5M|0.252|0.311|", "isPerfectTable": true, "csv": "\"Subset\",\"Training dataset size\",\"ImageNet accuracy\",\"Average performance\"\n\"T_{ADC} = 0.1, without filtering\",\"99.8M\",\"0.195\",\"0.275\"\n\"T_{ADC} = 0.2, without filtering\",\"85.9M\",\"0.200\",\"0.277\"\n\"T_{ADC} = 0.5, without filtering\",\"29.6M\",\"0.227\",\"0.295\"\n\"T_{ADC} = 0.1, with filtering\",\"33.5M\",\"0.288\",\"0.337\"\n\"T_{ADC} = 0.2, with filtering\",\"30.6M\",\"0.289\",\"0.337\"\n\"T_{ADC} = 0.5, with filtering\",\"15.5M\",\"0.252\",\"0.311\""}, {"type": "text", "value": "Table 9: Effect of deduplication of training set for the medium size COMMONPOOL. The filtering performed here is CLIP B32 score top 30% (see Table 26). Higher threshold values lead to more samples being labeled as duplicates.", "md": "Table 9: Effect of deduplication of training set for the medium size COMMONPOOL. The filtering performed here is CLIP B32 score top 30% (see Table 26). Higher threshold values lead to more samples being labeled as duplicates."}]}, {"page": 36, "text": "Table 10: Batch size ablation at the medium scale. We compare the standard DATACOMP medium\nconfiguration, with batch size 4096 against an ablated configuration with batch size 8192 (medium:\nbatch size 2x). We find that the rankings of the baseline filtering strategies are relatively consistent.\nMore precisely, the rank correlation is 0.96 on ImageNet and 0.98 for the Average over 38 datasets.\n                 Scale                   Filtering strategy                                 Dataset     Samples  ImageNet  Average over  Delta ranking  Delta ranking\n                                                                                              size         seen             38 datasets     ImageNet       Average\n                                         No filtering                                        128M         128M     0.176        0.258            -              -\n                                         Basic filtering                                      30M         128M     0.226        0.285            -              -\n                                         Text-based                                           31M         128M     0.255        0.307            -              -\n                medium                   Image-based                                          29M         128M     0.268        0.312            -              -\n                                         LAION-2B filtering                                   13M         128M     0.230        0.292            -              -\n                                         CLIP score (L/14 30%)                                38M         128M     0.273        0.328            -              -\n                                         Image-based \u2229      CLIP score (L/14 30%)             14M         128M     0.297        0.328            -              -\n                                         No filtering                                        128M         128M     0.171        0.258            0             0\n                                         Basic filtering                                      30M         128M     0.219        0.277      +1 (worse)          0\n                                         Text-based                                           31M         128M     0.251        0.299            0        -1 (better)\n   medium:        batch size 2x          Image-based                                          29M         128M     0.260        0.299            0             0\n                                         LAION-2B filtering                                   13M         128M     0.215        0.288      -1 (better)         0\n                                         CLIP score (L/14 30%)                                38M         128M     0.271        0.324            0             0\n                                         Image-based \u2229      CLIP score (L/14 30%)             14M         128M     0.276        0.311            0        +1 (worse)\nL         Hyperparameter ablations\nRecall that in DATACOMP, we freeze the training procedure and hyperparameters to focus the\ncompetition on dataset curation. However, this leads to the natural question: do \u201cbetter\u201d datasets (i.e.,\ndatasets that lead to higher accuracy models on zero-shot downstream tasks) remain consistent when\ntraining is modified. Hence we ablate key experimental choices: batch size, model architecture, and\nnumber of training steps.\nL.1         Batch size\nWe ablate over the batch size hyperparameter, doubling the batch size at the medium scale, but holding\nall other hyperparameters constant. As see in Table 10, we find that the delta rankings are largely\nconsistent, for both ImageNet and Average performance, with rankings changing by at most plus or\nminus one position. More specifically, rank correlation before and after doubling batch size is 0.96\nfor ImageNet and 0.98 for the Average over 38 datasets metric.\nL.2         Model architecture\nWe choose to use the ViT architecture [39] because of favorable CLIP scaling trends over vanilla\nResNets [62] as reported by Radford et al. [111]. However, we still hope that better datasets for\ndownstream ViT performance will lead to better datasets to train convolutional architectures. We\nlook at the medium scale, swapping the ViT-B/32 architecture with a ConvNeXt model [93] with\nmatched giga multiplier\u2013accumulate operations (GMACs). Looking at Table 11, we see that ranking\nof different filtering methods is again relatively consistent (i.e., 1.0 rank correlation for ImageNet\nand 0.87 rank correlation for the average metric). We conclude that improvements in dataset filtering\nhave potential to improve more than just CLIP ViT model performance.\nL.3         Number of training steps\nRecall that one of our major design decisions for DATACOMP is to fix the hyperparameters associated\nwith model training, following closely hyperparameters from prior work [111]. We choose to\nfix hyperparameters to place emphasis on data curation and remove confounders arising from\nhyperparameter differences between participants. Here we ablate our hyperparameter configuration\nby training small baselines for 10\u00d7 more steps. In Figure 14 we see positive correlation for ImageNet\naccuracy for the ablated and original hyperparameter configurations. We see similar correlation for\naverage performance. See Table 12 for specific values.\n                                                                                             36", "md": "# Batch Size Ablation at the Medium Scale\n\n## Table 10: Batch size ablation at the medium scale\n\nWe compare the standard DATACOMP medium configuration, with batch size 4096 against an ablated configuration with batch size 8192 (medium: batch size 2x). We find that the rankings of the baseline filtering strategies are relatively consistent. More precisely, the rank correlation is 0.96 on ImageNet and 0.98 for the Average over 38 datasets.\n\n|Scale|Filtering strategy|Dataset|Samples size|ImageNet|Average over 38 datasets|Delta ranking ImageNet|Delta ranking Average|\n|---|---|---|---|---|---|---|---|\n|medium|No filtering|128M|128M|0.176|0.258|-|-|\n|medium|Basic filtering|30M|128M|0.226|0.285|-|-|\n|medium|Text-based|31M|128M|0.255|0.307|-|-|\n|medium|Image-based|29M|128M|0.268|0.312|-|-|\n|medium|LAION-2B filtering|13M|128M|0.230|0.292|-|-|\n|medium|CLIP score (L/14 30%)|38M|128M|0.273|0.328|-|-|\n|medium|Image-based \u2229 CLIP score (L/14 30%)|14M|128M|0.297|0.328|-|-|\n\n### Hyperparameter Ablations\n\nRecall that in DATACOMP, we freeze the training procedure and hyperparameters to focus the competition on dataset curation. However, this leads to the natural question: do \u201cbetter\u201d datasets (i.e., datasets that lead to higher accuracy models on zero-shot downstream tasks) remain consistent when training is modified. Hence we ablate key experimental choices: batch size, model architecture, and number of training steps.\n\n#### L.1 Batch size\n\nWe ablate over the batch size hyperparameter, doubling the batch size at the medium scale, but holding all other hyperparameters constant. As seen in Table 10, we find that the delta rankings are largely consistent, for both ImageNet and Average performance, with rankings changing by at most plus or minus one position. More specifically, rank correlation before and after doubling batch size is 0.96 for ImageNet and 0.98 for the Average over 38 datasets metric.\n\n#### L.2 Model architecture\n\nWe choose to use the ViT architecture [39] because of favorable CLIP scaling trends over vanilla ResNets [62] as reported by Radford et al. [111]. However, we still hope that better datasets for downstream ViT performance will lead to better datasets to train convolutional architectures. We look at the medium scale, swapping the ViT-B/32 architecture with a ConvNeXt model [93] with matched giga multiplier\u2013accumulate operations (GMACs). Looking at Table 11, we see that ranking of different filtering methods is again relatively consistent (i.e., 1.0 rank correlation for ImageNet and 0.87 rank correlation for the average metric). We conclude that improvements in dataset filtering have potential to improve more than just CLIP ViT model performance.\n\n#### L.3 Number of training steps\n\nRecall that one of our major design decisions for DATACOMP is to fix the hyperparameters associated with model training, following closely hyperparameters from prior work [111]. We choose to fix hyperparameters to place emphasis on data curation and remove confounders arising from hyperparameter differences between participants. Here we ablate our hyperparameter configuration by training small baselines for 10\u00d7 more steps. In Figure 14 we see positive correlation for ImageNet accuracy for the ablated and original hyperparameter configurations. We see similar correlation for average performance. See Table 12 for specific values.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Batch Size Ablation at the Medium Scale", "md": "# Batch Size Ablation at the Medium Scale"}, {"type": "heading", "lvl": 2, "value": "Table 10: Batch size ablation at the medium scale", "md": "## Table 10: Batch size ablation at the medium scale"}, {"type": "text", "value": "We compare the standard DATACOMP medium configuration, with batch size 4096 against an ablated configuration with batch size 8192 (medium: batch size 2x). We find that the rankings of the baseline filtering strategies are relatively consistent. More precisely, the rank correlation is 0.96 on ImageNet and 0.98 for the Average over 38 datasets.", "md": "We compare the standard DATACOMP medium configuration, with batch size 4096 against an ablated configuration with batch size 8192 (medium: batch size 2x). We find that the rankings of the baseline filtering strategies are relatively consistent. More precisely, the rank correlation is 0.96 on ImageNet and 0.98 for the Average over 38 datasets."}, {"type": "table", "rows": [["Scale", "Filtering strategy", "Dataset", "Samples size", "ImageNet", "Average over 38 datasets", "Delta ranking ImageNet", "Delta ranking Average"], ["medium", "No filtering", "128M", "128M", "0.176", "0.258", "-", "-"], ["medium", "Basic filtering", "30M", "128M", "0.226", "0.285", "-", "-"], ["medium", "Text-based", "31M", "128M", "0.255", "0.307", "-", "-"], ["medium", "Image-based", "29M", "128M", "0.268", "0.312", "-", "-"], ["medium", "LAION-2B filtering", "13M", "128M", "0.230", "0.292", "-", "-"], ["medium", "CLIP score (L/14 30%)", "38M", "128M", "0.273", "0.328", "-", "-"], ["medium", "Image-based \u2229 CLIP score (L/14 30%)", "14M", "128M", "0.297", "0.328", "-", "-"]], "md": "|Scale|Filtering strategy|Dataset|Samples size|ImageNet|Average over 38 datasets|Delta ranking ImageNet|Delta ranking Average|\n|---|---|---|---|---|---|---|---|\n|medium|No filtering|128M|128M|0.176|0.258|-|-|\n|medium|Basic filtering|30M|128M|0.226|0.285|-|-|\n|medium|Text-based|31M|128M|0.255|0.307|-|-|\n|medium|Image-based|29M|128M|0.268|0.312|-|-|\n|medium|LAION-2B filtering|13M|128M|0.230|0.292|-|-|\n|medium|CLIP score (L/14 30%)|38M|128M|0.273|0.328|-|-|\n|medium|Image-based \u2229 CLIP score (L/14 30%)|14M|128M|0.297|0.328|-|-|", "isPerfectTable": true, "csv": "\"Scale\",\"Filtering strategy\",\"Dataset\",\"Samples size\",\"ImageNet\",\"Average over 38 datasets\",\"Delta ranking ImageNet\",\"Delta ranking Average\"\n\"medium\",\"No filtering\",\"128M\",\"128M\",\"0.176\",\"0.258\",\"-\",\"-\"\n\"medium\",\"Basic filtering\",\"30M\",\"128M\",\"0.226\",\"0.285\",\"-\",\"-\"\n\"medium\",\"Text-based\",\"31M\",\"128M\",\"0.255\",\"0.307\",\"-\",\"-\"\n\"medium\",\"Image-based\",\"29M\",\"128M\",\"0.268\",\"0.312\",\"-\",\"-\"\n\"medium\",\"LAION-2B filtering\",\"13M\",\"128M\",\"0.230\",\"0.292\",\"-\",\"-\"\n\"medium\",\"CLIP score (L/14 30%)\",\"38M\",\"128M\",\"0.273\",\"0.328\",\"-\",\"-\"\n\"medium\",\"Image-based \u2229 CLIP score (L/14 30%)\",\"14M\",\"128M\",\"0.297\",\"0.328\",\"-\",\"-\""}, {"type": "heading", "lvl": 3, "value": "Hyperparameter Ablations", "md": "### Hyperparameter Ablations"}, {"type": "text", "value": "Recall that in DATACOMP, we freeze the training procedure and hyperparameters to focus the competition on dataset curation. However, this leads to the natural question: do \u201cbetter\u201d datasets (i.e., datasets that lead to higher accuracy models on zero-shot downstream tasks) remain consistent when training is modified. Hence we ablate key experimental choices: batch size, model architecture, and number of training steps.", "md": "Recall that in DATACOMP, we freeze the training procedure and hyperparameters to focus the competition on dataset curation. However, this leads to the natural question: do \u201cbetter\u201d datasets (i.e., datasets that lead to higher accuracy models on zero-shot downstream tasks) remain consistent when training is modified. Hence we ablate key experimental choices: batch size, model architecture, and number of training steps."}, {"type": "heading", "lvl": 4, "value": "L.1 Batch size", "md": "#### L.1 Batch size"}, {"type": "text", "value": "We ablate over the batch size hyperparameter, doubling the batch size at the medium scale, but holding all other hyperparameters constant. As seen in Table 10, we find that the delta rankings are largely consistent, for both ImageNet and Average performance, with rankings changing by at most plus or minus one position. More specifically, rank correlation before and after doubling batch size is 0.96 for ImageNet and 0.98 for the Average over 38 datasets metric.", "md": "We ablate over the batch size hyperparameter, doubling the batch size at the medium scale, but holding all other hyperparameters constant. As seen in Table 10, we find that the delta rankings are largely consistent, for both ImageNet and Average performance, with rankings changing by at most plus or minus one position. More specifically, rank correlation before and after doubling batch size is 0.96 for ImageNet and 0.98 for the Average over 38 datasets metric."}, {"type": "heading", "lvl": 4, "value": "L.2 Model architecture", "md": "#### L.2 Model architecture"}, {"type": "text", "value": "We choose to use the ViT architecture [39] because of favorable CLIP scaling trends over vanilla ResNets [62] as reported by Radford et al. [111]. However, we still hope that better datasets for downstream ViT performance will lead to better datasets to train convolutional architectures. We look at the medium scale, swapping the ViT-B/32 architecture with a ConvNeXt model [93] with matched giga multiplier\u2013accumulate operations (GMACs). Looking at Table 11, we see that ranking of different filtering methods is again relatively consistent (i.e., 1.0 rank correlation for ImageNet and 0.87 rank correlation for the average metric). We conclude that improvements in dataset filtering have potential to improve more than just CLIP ViT model performance.", "md": "We choose to use the ViT architecture [39] because of favorable CLIP scaling trends over vanilla ResNets [62] as reported by Radford et al. [111]. However, we still hope that better datasets for downstream ViT performance will lead to better datasets to train convolutional architectures. We look at the medium scale, swapping the ViT-B/32 architecture with a ConvNeXt model [93] with matched giga multiplier\u2013accumulate operations (GMACs). Looking at Table 11, we see that ranking of different filtering methods is again relatively consistent (i.e., 1.0 rank correlation for ImageNet and 0.87 rank correlation for the average metric). We conclude that improvements in dataset filtering have potential to improve more than just CLIP ViT model performance."}, {"type": "heading", "lvl": 4, "value": "L.3 Number of training steps", "md": "#### L.3 Number of training steps"}, {"type": "text", "value": "Recall that one of our major design decisions for DATACOMP is to fix the hyperparameters associated with model training, following closely hyperparameters from prior work [111]. We choose to fix hyperparameters to place emphasis on data curation and remove confounders arising from hyperparameter differences between participants. Here we ablate our hyperparameter configuration by training small baselines for 10\u00d7 more steps. In Figure 14 we see positive correlation for ImageNet accuracy for the ablated and original hyperparameter configurations. We see similar correlation for average performance. See Table 12 for specific values.", "md": "Recall that one of our major design decisions for DATACOMP is to fix the hyperparameters associated with model training, following closely hyperparameters from prior work [111]. We choose to fix hyperparameters to place emphasis on data curation and remove confounders arising from hyperparameter differences between participants. Here we ablate our hyperparameter configuration by training small baselines for 10\u00d7 more steps. In Figure 14 we see positive correlation for ImageNet accuracy for the ablated and original hyperparameter configurations. We see similar correlation for average performance. See Table 12 for specific values."}]}, {"page": 37, "text": "Table 11: Architure ablation at the medium scale. We compare the standard DATACOMP medium\nconfiguration, with a ViT-B/32 model against an ablated configuration (medium:                                                              ConvNeXt), which\nuses a ConvNeXt model with the same number of multiply-accumulate operations as the ViT. We\nfind that the rankings of the baseline filtering strategies are relatively consistent. More precisely, the\nrank correlation is 1.0 on ImageNet and 0.87 for the Average over 38 datasets.\n             Scale               Filtering strategy                                 Dataset     Samples       ImageNet  Average over  Delta ranking       Delta ranking\n                                                                                      size         seen                  38 datasets    ImageNet             Average\n                                 No filtering                                        128M         128M          0.176        0.254            -                   -\n                                 Basic filtering                                      30M         128M          0.226        0.280            -                   -\n                                 Text-based                                           31M         128M          0.255        0.301            -                   -\n            medium               Image-based                                          29M         128M          0.268        0.307            -                   -\n                                 LAION-2B fi      ltering                             13M         128M          0.230        0.287            -                   -\n                                 CLIP score (L/14 30%)                                38M         128M          0.273        0.323            -                   -\n                                 Image-based \u2229      CLIP score (L/14 30%)             14M         128M          0.297        0.323            -                   -\n                                 No filtering                                        128M         128M          0.178        0.255            0                  0\n                                 Basic filtering                                      30M         128M          0.232        0.272            0                  0\n                                 Text-based                                           31M         128M          0.255        0.298            0                  0\n   medium:       ConvNeXt        Image-based                                          29M         128M          0.270        0.298            0             +1 (better)\n                                 LAION-2B fi      ltering                             13M         128M          0.253        0.300            0             -2 (better)\n                                 CLIP score (L/14 30%)                                38M         128M          0.279        0.326            0             +1 (worse)\n                                 Image-based \u2229      CLIP score (L/14 30%)             14M         128M          0.323        0.331            0                  0\n                                      ImageNet                                         Average over 38 datasets\n              ImageNet acc. (10x steps)                                    Avg. performance (10x steps)\n               0.10                                                         0.200\n                                                                                                                                            Basic\n               0.08                                                         0.175                                                           CLIP score\n               0.06                                                         0.150                                                           Image-based\n                                                                                                                                            No filtering\n               0.04                                                         0.125                                                           Rand. subset\n               0.02                                                         0.100                                                           Text-based\n               0.00\n                     0.00            0.02            0.04                          0.075 0.100 0.125 0.150 0.175\n                            ImageNet acc. (standard)                                    Avg. performance (standard)\nFigure 14: (left) The effect of training for 10\u00d7 steps for for small filtering track baselines on\nImageNet. (right) Similar plot but for Avg. performance. While the ordering of some methods\nchanges quite drastically, we, in general, see a positive correlation.\n                                                                                         37", "md": "|Scale|Filtering strategy|Dataset|Samples|ImageNet|Average over 38 datasets|Delta ranking ImageNet|Delta ranking Average|\n|---|---|---|---|---|---|---|---|\n|medium|No filtering|128M|128M|0.176|0.254|-|-|\n|Basic filtering| | |30M|128M|0.226|0.280|-|-|\n|Text-based| | |31M|128M|0.255|0.301|-|-|\n|Image-based| | |29M|128M|0.268|0.307|-|-|\n|LAION-2B filtering| | |13M|128M|0.230|0.287|-|-|\n|CLIP score (L/14 30%)| | |38M|128M|0.273|0.323|-|-|\n|Image-based \u2229 CLIP score (L/14 30%)| | |14M|128M|0.297|0.323|-|-|\n|medium: ConvNeXt|No filtering|128M|128M|0.178|0.255|0|0|\n|Basic filtering| | |30M|128M|0.232|0.272|0|0|\n|Text-based| | |31M|128M|0.255|0.298|0|0|\n|Image-based| | |29M|128M|0.270|0.298|0|+1 (better)|\n|LAION-2B filtering| | |13M|128M|0.253|0.300|0|-2 (better)|\n|CLIP score (L/14 30%)| | |38M|128M|0.279|0.326|0|+1 (worse)|\n|Image-based \u2229 CLIP score (L/14 30%)| | |14M|128M|0.323|0.331|0|0|\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{ImageNet acc. (10x steps)} & \\text{Avg. performance (10x steps)} \\\\\n\\hline\n0.10 & 0.200 \\\\\n0.08 & 0.175 \\\\\n0.06 & 0.150 \\\\\n0.04 & 0.125 \\\\\n0.02 & 0.100 \\\\\n0.00 & \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n0.00 & 0.02 & 0.04 & 0.075 & 0.100 & 0.125 & 0.150 & 0.175 \\\\\n\\hline\n\\text{ImageNet acc. (standard)} & \\text{Avg. performance (standard)} \\\\\n\\hline\n\\end{array}\n$$", "images": [], "items": [{"type": "table", "rows": [["Scale", "Filtering strategy", "Dataset", "Samples", "ImageNet", "Average over 38 datasets", "Delta ranking ImageNet", "Delta ranking Average"], ["medium", "No filtering", "128M", "128M", "0.176", "0.254", "-", "-"], ["Basic filtering", "", "", "30M", "128M", "0.226", "0.280", "-", "-"], ["Text-based", "", "", "31M", "128M", "0.255", "0.301", "-", "-"], ["Image-based", "", "", "29M", "128M", "0.268", "0.307", "-", "-"], ["LAION-2B filtering", "", "", "13M", "128M", "0.230", "0.287", "-", "-"], ["CLIP score (L/14 30%)", "", "", "38M", "128M", "0.273", "0.323", "-", "-"], ["Image-based \u2229 CLIP score (L/14 30%)", "", "", "14M", "128M", "0.297", "0.323", "-", "-"], ["medium: ConvNeXt", "No filtering", "128M", "128M", "0.178", "0.255", "0", "0"], ["Basic filtering", "", "", "30M", "128M", "0.232", "0.272", "0", "0"], ["Text-based", "", "", "31M", "128M", "0.255", "0.298", "0", "0"], ["Image-based", "", "", "29M", "128M", "0.270", "0.298", "0", "+1 (better)"], ["LAION-2B filtering", "", "", "13M", "128M", "0.253", "0.300", "0", "-2 (better)"], ["CLIP score (L/14 30%)", "", "", "38M", "128M", "0.279", "0.326", "0", "+1 (worse)"], ["Image-based \u2229 CLIP score (L/14 30%)", "", "", "14M", "128M", "0.323", "0.331", "0", "0"]], "md": "|Scale|Filtering strategy|Dataset|Samples|ImageNet|Average over 38 datasets|Delta ranking ImageNet|Delta ranking Average|\n|---|---|---|---|---|---|---|---|\n|medium|No filtering|128M|128M|0.176|0.254|-|-|\n|Basic filtering| | |30M|128M|0.226|0.280|-|-|\n|Text-based| | |31M|128M|0.255|0.301|-|-|\n|Image-based| | |29M|128M|0.268|0.307|-|-|\n|LAION-2B filtering| | |13M|128M|0.230|0.287|-|-|\n|CLIP score (L/14 30%)| | |38M|128M|0.273|0.323|-|-|\n|Image-based \u2229 CLIP score (L/14 30%)| | |14M|128M|0.297|0.323|-|-|\n|medium: ConvNeXt|No filtering|128M|128M|0.178|0.255|0|0|\n|Basic filtering| | |30M|128M|0.232|0.272|0|0|\n|Text-based| | |31M|128M|0.255|0.298|0|0|\n|Image-based| | |29M|128M|0.270|0.298|0|+1 (better)|\n|LAION-2B filtering| | |13M|128M|0.253|0.300|0|-2 (better)|\n|CLIP score (L/14 30%)| | |38M|128M|0.279|0.326|0|+1 (worse)|\n|Image-based \u2229 CLIP score (L/14 30%)| | |14M|128M|0.323|0.331|0|0|", "isPerfectTable": false, "csv": "\"Scale\",\"Filtering strategy\",\"Dataset\",\"Samples\",\"ImageNet\",\"Average over 38 datasets\",\"Delta ranking ImageNet\",\"Delta ranking Average\"\n\"medium\",\"No filtering\",\"128M\",\"128M\",\"0.176\",\"0.254\",\"-\",\"-\"\n\"Basic filtering\",\"\",\"\",\"30M\",\"128M\",\"0.226\",\"0.280\",\"-\",\"-\"\n\"Text-based\",\"\",\"\",\"31M\",\"128M\",\"0.255\",\"0.301\",\"-\",\"-\"\n\"Image-based\",\"\",\"\",\"29M\",\"128M\",\"0.268\",\"0.307\",\"-\",\"-\"\n\"LAION-2B filtering\",\"\",\"\",\"13M\",\"128M\",\"0.230\",\"0.287\",\"-\",\"-\"\n\"CLIP score (L/14 30%)\",\"\",\"\",\"38M\",\"128M\",\"0.273\",\"0.323\",\"-\",\"-\"\n\"Image-based \u2229 CLIP score (L/14 30%)\",\"\",\"\",\"14M\",\"128M\",\"0.297\",\"0.323\",\"-\",\"-\"\n\"medium: ConvNeXt\",\"No filtering\",\"128M\",\"128M\",\"0.178\",\"0.255\",\"0\",\"0\"\n\"Basic filtering\",\"\",\"\",\"30M\",\"128M\",\"0.232\",\"0.272\",\"0\",\"0\"\n\"Text-based\",\"\",\"\",\"31M\",\"128M\",\"0.255\",\"0.298\",\"0\",\"0\"\n\"Image-based\",\"\",\"\",\"29M\",\"128M\",\"0.270\",\"0.298\",\"0\",\"+1 (better)\"\n\"LAION-2B filtering\",\"\",\"\",\"13M\",\"128M\",\"0.253\",\"0.300\",\"0\",\"-2 (better)\"\n\"CLIP score (L/14 30%)\",\"\",\"\",\"38M\",\"128M\",\"0.279\",\"0.326\",\"0\",\"+1 (worse)\"\n\"Image-based \u2229 CLIP score (L/14 30%)\",\"\",\"\",\"14M\",\"128M\",\"0.323\",\"0.331\",\"0\",\"0\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{ImageNet acc. (10x steps)} & \\text{Avg. performance (10x steps)} \\\\\n\\hline\n0.10 & 0.200 \\\\\n0.08 & 0.175 \\\\\n0.06 & 0.150 \\\\\n0.04 & 0.125 \\\\\n0.02 & 0.100 \\\\\n0.00 & \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n0.00 & 0.02 & 0.04 & 0.075 & 0.100 & 0.125 & 0.150 & 0.175 \\\\\n\\hline\n\\text{ImageNet acc. (standard)} & \\text{Avg. performance (standard)} \\\\\n\\hline\n\\end{array}\n$$", "md": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{ImageNet acc. (10x steps)} & \\text{Avg. performance (10x steps)} \\\\\n\\hline\n0.10 & 0.200 \\\\\n0.08 & 0.175 \\\\\n0.06 & 0.150 \\\\\n0.04 & 0.125 \\\\\n0.02 & 0.100 \\\\\n0.00 & \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n0.00 & 0.02 & 0.04 & 0.075 & 0.100 & 0.125 & 0.150 & 0.175 \\\\\n\\hline\n\\text{ImageNet acc. (standard)} & \\text{Avg. performance (standard)} \\\\\n\\hline\n\\end{array}\n$$"}]}, {"page": 38, "text": "Table 12: Experiment details when extending the number of steps by 10 times the standard amount\nfor that scale.\n  Scale        Filtering                                                                  ImageNet   ImageNet     VTAB    Retrieval  Average over\n                                                                                                    dist. shifts                      38 datasets\n               No filtering                                                                 0.102      0.093       0.204    0.147        0.196\n               Random subset(75%)                                                           0.078      0.072       0.182    0.129        0.178\n               Random subset(50%)                                                           0.045      0.049       0.161    0.104        0.150\n               Random subset(25%)                                                           0.023      0.029       0.134    0.075        0.119\n               Random subset(10%)                                                           0.010      0.018       0.119    0.069        0.101\n               Random subset(1%)                                                            0.002      0.006       0.097    0.056        0.082\n               Caption length                                                               0.085      0.080       0.198    0.136        0.184\n               Image size                                                                   0.066      0.064       0.153    0.115        0.158\n               English (fasttext)                                                           0.068      0.068       0.172    0.108        0.159\n               English (fasttext) and caption length                                        0.066      0.065       0.182    0.106        0.163\n               English (fasttext), caption length, and image size                           0.045      0.048       0.164    0.092        0.149\n               CLIP B32 score top 10%                                                       0.035      0.046       0.162    0.079        0.139\n               CLIP B32 score top 20%                                                       0.076      0.076       0.182    0.099        0.172\n               CLIP B32 score top 30%                                                       0.096      0.090       0.221    0.121        0.205\n               CLIP B32 score top 40%                                                       0.081      0.077       0.200    0.124        0.193\n               CLIP B32 score top 50%                                                       0.106      0.097       0.211    0.134        0.205\n               CLIP B32 score top 75%                                                       0.103      0.096       0.210    0.150        0.198\n   small       CLIP B32 score top 90%                                                       0.105      0.096       0.212    0.152        0.202\n               CLIP B32 threshold at 0.3 + English filter                                   0.029      0.036       0.152    0.078        0.134\n               CLIP B32 threshold at 0.28 + English filter                                  0.035      0.041       0.168    0.086        0.145\n               CLIP B32 threshold at 0.3                                                    0.076      0.078       0.199    0.102        0.182\n               CLIP L14 score top 10%                                                       0.026      0.037       0.130    0.073        0.123\n               CLIP L14 score top 20%                                                       0.060      0.064       0.161    0.096        0.153\n               CLIP L14 score top 30%                                                       0.088      0.087       0.199    0.115        0.188\n               CLIP L14 score top 40%                                                       0.100      0.096       0.217    0.122        0.207\n               CLIP L14 score top 50%                                                       0.104      0.098       0.212    0.136        0.203\n               CLIP L14 score top 75%                                                       0.103      0.095       0.189    0.146        0.191\n               CLIP L14 score top 90%                                                       0.105      0.095       0.203    0.145        0.198\n               Image-based clustering (ImageNet1k)                                          0.053      0.053       0.162    0.091        0.146\n               Image-based clustering (ImageNet21k)                                         0.063      0.059       0.173    0.108        0.167\n               Text-based clustering (ImageNet1k)                                           0.012      0.018       0.120    0.062        0.104\n               Text-based clustering (ImageNet21k)                                          0.262      0.216       0.305    0.246        0.300\n               Intersect IN1k image clustering and CLIP B32 score top 30%                   0.058      0.059       0.179    0.098        0.161\n               Intersect IN1k image clustering and CLIP L14 score top 30%                   0.049      0.051       0.171    0.090        0.150\n               Intersect IN21k image clustering and CLIP B32 score top 30%                  0.071      0.070       0.192    0.107        0.175\n               Intersect IN21k image clustering and CLIP L14 score top 30%                  0.064      0.065       0.200    0.096        0.173\n               No filtering                                                                 0.370      0.304       0.387    0.355        0.383\n               English (fasttext), caption length, and image size                           0.317      0.269       0.324    0.271        0.334\n               CLIP B32 score top 30%                                                       0.436      0.351       0.433    0.345        0.430\n               CLIP B32 score top 40%                                                       0.434      0.353       0.448    0.365        0.442\n   medium      CLIP B32 score top 50%                                                       0.426      0.352       0.439    0.377        0.433\n               CLIP B32 score top 75%                                                       0.398      0.325       0.396    0.374        0.411\n               Image-based clustering (ImageNet1k)                                          0.363      0.294       0.347    0.279        0.347\n               Image-based clustering (ImageNet21k)                                         0.374      0.303       0.372    0.318        0.372\n               Intersect IN1k image clustering and CLIP B32 score top 30%                   0.415      0.330       0.413    0.310        0.403\n               Intersect IN1k image clustering and CLIP L14 score top 30%                   0.405      0.325       0.399    0.295        0.387\n                                                                             38", "md": "|Scale|Filtering|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|small|No filtering|0.102|0.093|0.204|0.147|0.196|\n|Random subset(75%)| | |0.078|0.072|0.182|0.129|0.178|\n|Random subset(50%)| | |0.045|0.049|0.161|0.104|0.150|\n|Random subset(25%)| | |0.023|0.029|0.134|0.075|0.119|\n|Random subset(10%)| | |0.010|0.018|0.119|0.069|0.101|\n|Random subset(1%)| | |0.002|0.006|0.097|0.056|0.082|\n|Caption length| | |0.085|0.080|0.198|0.136|0.184|\n|medium|No filtering|0.370|0.304|0.387|0.355|0.383|\n|English (fasttext), caption length, and image size| | |0.317|0.269|0.324|0.271|0.334|\n|CLIP B32 score top 30%| | |0.436|0.351|0.433|0.345|0.430|\n|CLIP B32 score top 40%| | |0.434|0.353|0.448|0.365|0.442|\n|CLIP B32 score top 50%| | |0.426|0.352|0.439|0.377|0.433|\n|CLIP B32 score top 75%| | |0.398|0.325|0.396|0.374|0.411|\n|Image-based clustering (ImageNet1k)| | |0.363|0.294|0.347|0.279|0.347|\n|Image-based clustering (ImageNet21k)| | |0.374|0.303|0.372|0.318|0.372|\n|Intersect IN1k image clustering and CLIP B32 score top 30%| | |0.415|0.330|0.413|0.310|0.403|\n|Intersect IN1k image clustering and CLIP L14 score top 30%| | |0.405|0.325|0.399|0.295|0.387|", "images": [], "items": [{"type": "table", "rows": [["Scale", "Filtering", "ImageNet", "ImageNet dist. shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["small", "No filtering", "0.102", "0.093", "0.204", "0.147", "0.196"], ["Random subset(75%)", "", "", "0.078", "0.072", "0.182", "0.129", "0.178"], ["Random subset(50%)", "", "", "0.045", "0.049", "0.161", "0.104", "0.150"], ["Random subset(25%)", "", "", "0.023", "0.029", "0.134", "0.075", "0.119"], ["Random subset(10%)", "", "", "0.010", "0.018", "0.119", "0.069", "0.101"], ["Random subset(1%)", "", "", "0.002", "0.006", "0.097", "0.056", "0.082"], ["Caption length", "", "", "0.085", "0.080", "0.198", "0.136", "0.184"], ["medium", "No filtering", "0.370", "0.304", "0.387", "0.355", "0.383"], ["English (fasttext), caption length, and image size", "", "", "0.317", "0.269", "0.324", "0.271", "0.334"], ["CLIP B32 score top 30%", "", "", "0.436", "0.351", "0.433", "0.345", "0.430"], ["CLIP B32 score top 40%", "", "", "0.434", "0.353", "0.448", "0.365", "0.442"], ["CLIP B32 score top 50%", "", "", "0.426", "0.352", "0.439", "0.377", "0.433"], ["CLIP B32 score top 75%", "", "", "0.398", "0.325", "0.396", "0.374", "0.411"], ["Image-based clustering (ImageNet1k)", "", "", "0.363", "0.294", "0.347", "0.279", "0.347"], ["Image-based clustering (ImageNet21k)", "", "", "0.374", "0.303", "0.372", "0.318", "0.372"], ["Intersect IN1k image clustering and CLIP B32 score top 30%", "", "", "0.415", "0.330", "0.413", "0.310", "0.403"], ["Intersect IN1k image clustering and CLIP L14 score top 30%", "", "", "0.405", "0.325", "0.399", "0.295", "0.387"]], "md": "|Scale|Filtering|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|small|No filtering|0.102|0.093|0.204|0.147|0.196|\n|Random subset(75%)| | |0.078|0.072|0.182|0.129|0.178|\n|Random subset(50%)| | |0.045|0.049|0.161|0.104|0.150|\n|Random subset(25%)| | |0.023|0.029|0.134|0.075|0.119|\n|Random subset(10%)| | |0.010|0.018|0.119|0.069|0.101|\n|Random subset(1%)| | |0.002|0.006|0.097|0.056|0.082|\n|Caption length| | |0.085|0.080|0.198|0.136|0.184|\n|medium|No filtering|0.370|0.304|0.387|0.355|0.383|\n|English (fasttext), caption length, and image size| | |0.317|0.269|0.324|0.271|0.334|\n|CLIP B32 score top 30%| | |0.436|0.351|0.433|0.345|0.430|\n|CLIP B32 score top 40%| | |0.434|0.353|0.448|0.365|0.442|\n|CLIP B32 score top 50%| | |0.426|0.352|0.439|0.377|0.433|\n|CLIP B32 score top 75%| | |0.398|0.325|0.396|0.374|0.411|\n|Image-based clustering (ImageNet1k)| | |0.363|0.294|0.347|0.279|0.347|\n|Image-based clustering (ImageNet21k)| | |0.374|0.303|0.372|0.318|0.372|\n|Intersect IN1k image clustering and CLIP B32 score top 30%| | |0.415|0.330|0.413|0.310|0.403|\n|Intersect IN1k image clustering and CLIP L14 score top 30%| | |0.405|0.325|0.399|0.295|0.387|", "isPerfectTable": false, "csv": "\"Scale\",\"Filtering\",\"ImageNet\",\"ImageNet dist. shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"small\",\"No filtering\",\"0.102\",\"0.093\",\"0.204\",\"0.147\",\"0.196\"\n\"Random subset(75%)\",\"\",\"\",\"0.078\",\"0.072\",\"0.182\",\"0.129\",\"0.178\"\n\"Random subset(50%)\",\"\",\"\",\"0.045\",\"0.049\",\"0.161\",\"0.104\",\"0.150\"\n\"Random subset(25%)\",\"\",\"\",\"0.023\",\"0.029\",\"0.134\",\"0.075\",\"0.119\"\n\"Random subset(10%)\",\"\",\"\",\"0.010\",\"0.018\",\"0.119\",\"0.069\",\"0.101\"\n\"Random subset(1%)\",\"\",\"\",\"0.002\",\"0.006\",\"0.097\",\"0.056\",\"0.082\"\n\"Caption length\",\"\",\"\",\"0.085\",\"0.080\",\"0.198\",\"0.136\",\"0.184\"\n\"medium\",\"No filtering\",\"0.370\",\"0.304\",\"0.387\",\"0.355\",\"0.383\"\n\"English (fasttext), caption length, and image size\",\"\",\"\",\"0.317\",\"0.269\",\"0.324\",\"0.271\",\"0.334\"\n\"CLIP B32 score top 30%\",\"\",\"\",\"0.436\",\"0.351\",\"0.433\",\"0.345\",\"0.430\"\n\"CLIP B32 score top 40%\",\"\",\"\",\"0.434\",\"0.353\",\"0.448\",\"0.365\",\"0.442\"\n\"CLIP B32 score top 50%\",\"\",\"\",\"0.426\",\"0.352\",\"0.439\",\"0.377\",\"0.433\"\n\"CLIP B32 score top 75%\",\"\",\"\",\"0.398\",\"0.325\",\"0.396\",\"0.374\",\"0.411\"\n\"Image-based clustering (ImageNet1k)\",\"\",\"\",\"0.363\",\"0.294\",\"0.347\",\"0.279\",\"0.347\"\n\"Image-based clustering (ImageNet21k)\",\"\",\"\",\"0.374\",\"0.303\",\"0.372\",\"0.318\",\"0.372\"\n\"Intersect IN1k image clustering and CLIP B32 score top 30%\",\"\",\"\",\"0.415\",\"0.330\",\"0.413\",\"0.310\",\"0.403\"\n\"Intersect IN1k image clustering and CLIP L14 score top 30%\",\"\",\"\",\"0.405\",\"0.325\",\"0.399\",\"0.295\",\"0.387\""}]}, {"page": 39, "text": "M      Detector-based baselines\nWhile controlling for factors such as class balance is common in the supervised settings,\nexperimenting with analogous strategies in the context of multimodal datasets and CLIP training\nis a pertinent direction. Towards this end, we use the Detic detector [160] to annotate the medium\npool (128M samples) by extracting bounding boxes and class labels for the 1203 LVIS [54] objects\ncategories. Following the original Detic paper, we retain predictions whose confidence score exceeds\n0.5. Based on these annotations, we construct the following fi               ve strategies:\n         \u2022 Object exists: Subset for which there exists at least one detection from the 1203 LVIS\n           categories.\n         \u2022 Object centered: Subset for which there exists at least one detection from the 1203\n           LVIS categories with a bounding box center falling in the center grid cell of a 3x3 grid\n           superimposed on the image.\n         \u2022 Balancing by class: We define 1204 buckets\u20141203 buckets corresponding to the LVIS\n           classes and an additional bucket for images that do not have any detections. For each image\n           in the medium pool, we assign the image to the bucket(s) corresponding to the detected\n           classes. We then construct a dataset such that there are an equal number of samples from\n           each bucket and the total number of samples specified by a particular scale (e.g., 128M\n           samples for medium scale). Note, for rare classes there can be many repeated samples and\n           for common classes only a subset of the total samples will be in the dataset.\n         \u2022 Balancing by position: We define 26 buckets\u20140, 1, ..., 24 corresponding to 5x5 grid\n           locations in an image. An image is added to bucket(s) when it contains a bounding box\n           whose center falls in the bucket\u2019s grid cell. The 25th bucket contains images for which\n           there are no detections. We again construct a dataset such that there are an equal number of\n           samples from each bucket.\n         \u2022 Balancing by count: We define 12 buckets\u20140, 1, ..., 10 corresponding to zero to ten\n           detections in an image and a twelfth bucket corresponding to images with more than ten\n           detections. We yet again construct a dataset such that there are an equal number of samples\n           from each bucket.\nWe employ each of these strategies on the medium scale. Since the above strategies can be composed\nwith any starting pool, we additionally apply each of the above Detic-based strategies to our previous\nbest medium scale filtered pool: Image-based \u2229                 CLIP score (L/14 30%). This yields fi             ve more\ndatasets for 10 baselines in total.\nOur results are summarized in the Table 13. In summary: 1) The Image-based \u2229                          CLIP score (L/14\n30%) baseline still performs best. 2) Balancing data in the context of multimodal CLIP training\nTable 13: Detector-baseed baselines at the medium scale. We start with No filtering and Image-based\ncap CLIP score (L/14 30%) pools and apply fi            ve additional filtering and balancing strategies described\nin Appendix M. We find that even with these more sophisticated strategies, the No filtering and\nImage-based cap CLIP score (L/14 30%) still performs best at medium scale. Properly balancing\nmultimodal data remains an open direction for future work.\n                Scale     Filtering strategy                          Samples     ImageNet     Average over\n                                                                        seen                     38 datasets\n                          No filtering                                  128M        0.176          0.258\n                             \u2229 Object exists                            128M        0.181          0.263\n               medium        \u2229 Object centered                          128M        0.187          0.263\n                             \u2229 Balance by class                         128M        0.038          0.141\n                             \u2229 Balance by position                      128M        0.040          0.148\n                             \u2229 Balance by object count                  128M        0.127          0.221\n                          Image-based \u2229    CLIP score (L/14 30%)        128M        0.297          0.328\n                             \u2229 Object exists                            128M        0.289          0.319\n               medium        \u2229 Object centered                          128M        0.247          0.286\n                             \u2229 Balance by class                         128M        0.034          0.136\n                             \u2229 Balance by position                      128M        0.036          0.136\n                             \u2229 Balance by object count                  128M        0.068          0.169\n                                                            39", "md": "Detector-based baselines\n\nWhile controlling for factors such as class balance is common in the supervised settings, experimenting with analogous strategies in the context of multimodal datasets and CLIP training is a pertinent direction. Towards this end, we use the Detic detector [160] to annotate the medium pool (128M samples) by extracting bounding boxes and class labels for the 1203 LVIS [54] objects categories. Following the original Detic paper, we retain predictions whose confidence score exceeds 0.5. Based on these annotations, we construct the following five strategies:\n\n- Object exists: Subset for which there exists at least one detection from the 1203 LVIS categories.\n- Object centered: Subset for which there exists at least one detection from the 1203 LVIS categories with a bounding box center falling in the center grid cell of a 3x3 grid superimposed on the image.\n- Balancing by class: We define 1204 buckets\u20141203 buckets corresponding to the LVIS classes and an additional bucket for images that do not have any detections. For each image in the medium pool, we assign the image to the bucket(s) corresponding to the detected classes. We then construct a dataset such that there are an equal number of samples from each bucket and the total number of samples specified by a particular scale (e.g., 128M samples for medium scale). Note, for rare classes there can be many repeated samples and for common classes only a subset of the total samples will be in the dataset.\n- Balancing by position: We define 26 buckets\u20140, 1, ..., 24 corresponding to 5x5 grid locations in an image. An image is added to bucket(s) when it contains a bounding box whose center falls in the bucket\u2019s grid cell. The 25th bucket contains images for which there are no detections. We again construct a dataset such that there are an equal number of samples from each bucket.\n- Balancing by count: We define 12 buckets\u20140, 1, ..., 10 corresponding to zero to ten detections in an image and a twelfth bucket corresponding to images with more than ten detections. We yet again construct a dataset such that there are an equal number of samples from each bucket.\n\nWe employ each of these strategies on the medium scale. Since the above strategies can be composed with any starting pool, we additionally apply each of the above Detic-based strategies to our previous best medium scale filtered pool: Image-based \u2229 CLIP score (L/14 30%). This yields five more datasets for 10 baselines in total.\n\nOur results are summarized in the Table 13. In summary: 1) The Image-based \u2229 CLIP score (L/14 30%) baseline still performs best. 2) Balancing data in the context of multimodal CLIP training\n\n|Scale|Filtering strategy|Samples seen|ImageNet|Average over 38 datasets|\n|---|---|---|---|---|\n|medium|No filtering|128M|0.176|0.258|\n|\u2229 Object exists|128M|0.181|0.263| |\n|\u2229 Object centered|128M|0.187|0.263| |\n|\u2229 Balance by class|128M|0.038|0.141| |\n|\u2229 Balance by position|128M|0.040|0.148| |\n|\u2229 Balance by object count|128M|0.127|0.221| |\n|medium|Image-based \u2229 CLIP score (L/14 30%)|128M|0.297|0.328|\n|\u2229 Object exists|128M|0.289|0.319| |\n|\u2229 Object centered|128M|0.247|0.286| |\n|\u2229 Balance by class|128M|0.034|0.136| |\n|\u2229 Balance by position|128M|0.036|0.136| |\n|\u2229 Balance by object count|128M|0.068|0.169| |\n\nProperly balancing multimodal data remains an open direction for future work.", "images": [], "items": [{"type": "text", "value": "Detector-based baselines\n\nWhile controlling for factors such as class balance is common in the supervised settings, experimenting with analogous strategies in the context of multimodal datasets and CLIP training is a pertinent direction. Towards this end, we use the Detic detector [160] to annotate the medium pool (128M samples) by extracting bounding boxes and class labels for the 1203 LVIS [54] objects categories. Following the original Detic paper, we retain predictions whose confidence score exceeds 0.5. Based on these annotations, we construct the following five strategies:\n\n- Object exists: Subset for which there exists at least one detection from the 1203 LVIS categories.\n- Object centered: Subset for which there exists at least one detection from the 1203 LVIS categories with a bounding box center falling in the center grid cell of a 3x3 grid superimposed on the image.\n- Balancing by class: We define 1204 buckets\u20141203 buckets corresponding to the LVIS classes and an additional bucket for images that do not have any detections. For each image in the medium pool, we assign the image to the bucket(s) corresponding to the detected classes. We then construct a dataset such that there are an equal number of samples from each bucket and the total number of samples specified by a particular scale (e.g., 128M samples for medium scale). Note, for rare classes there can be many repeated samples and for common classes only a subset of the total samples will be in the dataset.\n- Balancing by position: We define 26 buckets\u20140, 1, ..., 24 corresponding to 5x5 grid locations in an image. An image is added to bucket(s) when it contains a bounding box whose center falls in the bucket\u2019s grid cell. The 25th bucket contains images for which there are no detections. We again construct a dataset such that there are an equal number of samples from each bucket.\n- Balancing by count: We define 12 buckets\u20140, 1, ..., 10 corresponding to zero to ten detections in an image and a twelfth bucket corresponding to images with more than ten detections. We yet again construct a dataset such that there are an equal number of samples from each bucket.\n\nWe employ each of these strategies on the medium scale. Since the above strategies can be composed with any starting pool, we additionally apply each of the above Detic-based strategies to our previous best medium scale filtered pool: Image-based \u2229 CLIP score (L/14 30%). This yields five more datasets for 10 baselines in total.\n\nOur results are summarized in the Table 13. In summary: 1) The Image-based \u2229 CLIP score (L/14 30%) baseline still performs best. 2) Balancing data in the context of multimodal CLIP training", "md": "Detector-based baselines\n\nWhile controlling for factors such as class balance is common in the supervised settings, experimenting with analogous strategies in the context of multimodal datasets and CLIP training is a pertinent direction. Towards this end, we use the Detic detector [160] to annotate the medium pool (128M samples) by extracting bounding boxes and class labels for the 1203 LVIS [54] objects categories. Following the original Detic paper, we retain predictions whose confidence score exceeds 0.5. Based on these annotations, we construct the following five strategies:\n\n- Object exists: Subset for which there exists at least one detection from the 1203 LVIS categories.\n- Object centered: Subset for which there exists at least one detection from the 1203 LVIS categories with a bounding box center falling in the center grid cell of a 3x3 grid superimposed on the image.\n- Balancing by class: We define 1204 buckets\u20141203 buckets corresponding to the LVIS classes and an additional bucket for images that do not have any detections. For each image in the medium pool, we assign the image to the bucket(s) corresponding to the detected classes. We then construct a dataset such that there are an equal number of samples from each bucket and the total number of samples specified by a particular scale (e.g., 128M samples for medium scale). Note, for rare classes there can be many repeated samples and for common classes only a subset of the total samples will be in the dataset.\n- Balancing by position: We define 26 buckets\u20140, 1, ..., 24 corresponding to 5x5 grid locations in an image. An image is added to bucket(s) when it contains a bounding box whose center falls in the bucket\u2019s grid cell. The 25th bucket contains images for which there are no detections. We again construct a dataset such that there are an equal number of samples from each bucket.\n- Balancing by count: We define 12 buckets\u20140, 1, ..., 10 corresponding to zero to ten detections in an image and a twelfth bucket corresponding to images with more than ten detections. We yet again construct a dataset such that there are an equal number of samples from each bucket.\n\nWe employ each of these strategies on the medium scale. Since the above strategies can be composed with any starting pool, we additionally apply each of the above Detic-based strategies to our previous best medium scale filtered pool: Image-based \u2229 CLIP score (L/14 30%). This yields five more datasets for 10 baselines in total.\n\nOur results are summarized in the Table 13. In summary: 1) The Image-based \u2229 CLIP score (L/14 30%) baseline still performs best. 2) Balancing data in the context of multimodal CLIP training"}, {"type": "table", "rows": [["Scale", "Filtering strategy", "Samples seen", "ImageNet", "Average over 38 datasets"], ["medium", "No filtering", "128M", "0.176", "0.258"], ["\u2229 Object exists", "128M", "0.181", "0.263", ""], ["\u2229 Object centered", "128M", "0.187", "0.263", ""], ["\u2229 Balance by class", "128M", "0.038", "0.141", ""], ["\u2229 Balance by position", "128M", "0.040", "0.148", ""], ["\u2229 Balance by object count", "128M", "0.127", "0.221", ""], ["medium", "Image-based \u2229 CLIP score (L/14 30%)", "128M", "0.297", "0.328"], ["\u2229 Object exists", "128M", "0.289", "0.319", ""], ["\u2229 Object centered", "128M", "0.247", "0.286", ""], ["\u2229 Balance by class", "128M", "0.034", "0.136", ""], ["\u2229 Balance by position", "128M", "0.036", "0.136", ""], ["\u2229 Balance by object count", "128M", "0.068", "0.169", ""]], "md": "|Scale|Filtering strategy|Samples seen|ImageNet|Average over 38 datasets|\n|---|---|---|---|---|\n|medium|No filtering|128M|0.176|0.258|\n|\u2229 Object exists|128M|0.181|0.263| |\n|\u2229 Object centered|128M|0.187|0.263| |\n|\u2229 Balance by class|128M|0.038|0.141| |\n|\u2229 Balance by position|128M|0.040|0.148| |\n|\u2229 Balance by object count|128M|0.127|0.221| |\n|medium|Image-based \u2229 CLIP score (L/14 30%)|128M|0.297|0.328|\n|\u2229 Object exists|128M|0.289|0.319| |\n|\u2229 Object centered|128M|0.247|0.286| |\n|\u2229 Balance by class|128M|0.034|0.136| |\n|\u2229 Balance by position|128M|0.036|0.136| |\n|\u2229 Balance by object count|128M|0.068|0.169| |", "isPerfectTable": true, "csv": "\"Scale\",\"Filtering strategy\",\"Samples seen\",\"ImageNet\",\"Average over 38 datasets\"\n\"medium\",\"No filtering\",\"128M\",\"0.176\",\"0.258\"\n\"\u2229 Object exists\",\"128M\",\"0.181\",\"0.263\",\"\"\n\"\u2229 Object centered\",\"128M\",\"0.187\",\"0.263\",\"\"\n\"\u2229 Balance by class\",\"128M\",\"0.038\",\"0.141\",\"\"\n\"\u2229 Balance by position\",\"128M\",\"0.040\",\"0.148\",\"\"\n\"\u2229 Balance by object count\",\"128M\",\"0.127\",\"0.221\",\"\"\n\"medium\",\"Image-based \u2229 CLIP score (L/14 30%)\",\"128M\",\"0.297\",\"0.328\"\n\"\u2229 Object exists\",\"128M\",\"0.289\",\"0.319\",\"\"\n\"\u2229 Object centered\",\"128M\",\"0.247\",\"0.286\",\"\"\n\"\u2229 Balance by class\",\"128M\",\"0.034\",\"0.136\",\"\"\n\"\u2229 Balance by position\",\"128M\",\"0.036\",\"0.136\",\"\"\n\"\u2229 Balance by object count\",\"128M\",\"0.068\",\"0.169\",\"\""}, {"type": "text", "value": "Properly balancing multimodal data remains an open direction for future work.", "md": "Properly balancing multimodal data remains an open direction for future work."}]}, {"page": 40, "text": "Table 14: Experimental configuration for each scale, including the size of the pool we provide, the\nmodel architecture and hyperparameters.\n  Scale       Model     Train compute (MACs)      Pool size  # samples seen   Learning rate   AdamW \u03b22      Warmup     Batch size\n  small     ViT-B/32          9.5 \u00d7 1016           12.8M         12.8M             5e-4           0.98         500        4096\n  medium    ViT-B/32          9.5 \u00d7 1017           128M           128M             5e-4           0.98         500        4096\n  large     ViT-B/16          2.6 \u00d7 1019           1.28B          1.28B            5e-4           0.98         500        8192\n  xlarge    ViT-L/14          1.1 \u00d7 1021           12.8B          12.8B            1e-3           0.95         10k       90112\nremains an open problem. All balancing strategies lead to divergence of the CLIP contrastive loss\nand result in poor model performance. We hypothesize that this is due to the long-tailed nature of the\ndata distribution, which leads to many repeated samples in our balanced data construction. This in\nturn, increases the likelihood that samples are contrasted with themselves in the loss computation.\nN      Training details\nThe full set of hyperparameters used for each scale is shown in Table 14.                                        For choosing\nhyperparameters, we follow the OpenCLIP library [69], an open source reproduction of OpenAI\u2019s\nCLIP. For the small, medium, and large tracks, these hyperparameters are equal to those in the\nCLIP paper, except with reduced batch size so that training runs on reasonable hardware. For the\nxlarge track, batch size is increased from that in OpenAI\u2019s CLIP to accelerate training by allowing\nthe use of many GPUs simultaneously with high utilization. For this run we also double the learning\nrate following prior work [28].\nO      Evaluation details\nModels are evaluated over a wide range of 38 tasks to measure proficiency in various domains. We\ninclude 22 of the 27 classification tasks in the test suite of Radford et al. [111], excluding the few\ndatasets that have license restrictions, are in video format, or are no longer available in their original\nform. We include 6 datasets that were designed to test generalization of models trained on ImageNet.\nWe also include a majority of the Visual Task Adaptation Benchmark, excluding 3 datasets that\nare ill-suited for zero-shot evaluation [156]. We include 3 datasets from the WILDS benchmark,\nwhich tests robustness to distribution shifts and spurious correlations [83, 127]. Finally, we include 2\nadditional datasets, Dollar Street and GeoDE, which test robustness of classification performance\nacross income levels and geographical regions [122, 114]. Furthermore, we evaluate zero-shot image\nand text retrieval on the Flickr30k and MSCOCO datasets, and image association on the WinoGAViL\ndataset [151, 26, 17]. The complete list of evaluation tasks is given in Table 15. We show a sample\nfrom each dataset in Figure 15.\nPrompt choice. Since we perform zero-shot evaluation, prompt and class name selection is important,\nand can have a significant impact on the results. To avoid heavy prompt engineering and overtuning to\nindividual models, we opt to use the prompt templates used in Radford et al. [111] whenever possible.\nMost datasets come with pre-defined class names, but some are overwritten with more descriptive\nlabels, again based on previous literature. For datasets with no precedent in zero-shot evaluation, we\nreuse prompt templates from other datasets with a similar domain and task (e.g., SVHN is evaluated\nwith MNIST prompts and class names).\nEvaluation metrics. For the majority of classification tasks, the primary evaluation metric is accuracy.\nFor certain datasets with class imbalances, we instead compute mean per-class accuracy, as done in\nRadford et al. [111]. On the WILDS benchmark datasets, we use the primary metric specified for each\ndataset on their leaderboard. Dollar Street and GeoDE test model generalization across socioeconomic\nand geographic diversity. Thus, for Dollar Street, we compute worst-group top-5 accuracy, with\ngroups defined by income level, emulating Rojas et al. [122]; for GeoDE, we compute worst-group\naccuracy, with groups defined by region (Africa, Americas, West Asia, East Asia, Southeast Asia,\nand Europe), as defined in Ramaswamy et al. [114]. For the image-text retrieval tasks, Flickr and\nMSCOCO, we compute both image and text recall (fraction of text captions for which the correct\nimage was selected and vice versa), and plot their arithmetic mean. On WinoGAViL, we compute the\n                                                                40", "md": "|Scale|Model|Train compute (MACs)|Pool size|# samples seen|Learning rate|AdamW \u03b22|Warmup|Batch size|\n|---|---|---|---|---|---|---|---|---|\n|small|ViT-B/32|9.5 \u00d7 10^16|12.8M|12.8M|5e-4|0.98|500|4096|\n|medium|ViT-B/32|9.5 \u00d7 10^17|128M|128M|5e-4|0.98|500|4096|\n|large|ViT-B/16|2.6 \u00d7 10^19|1.28B|1.28B|5e-4|0.98|500|8192|\n|xlarge|ViT-L/14|1.1 \u00d7 10^21|12.8B|12.8B|1e-3|0.95|10k|90112|\n\nRemains an open problem. All balancing strategies lead to divergence of the CLIP contrastive loss and result in poor model performance. We hypothesize that this is due to the long-tailed nature of the data distribution, which leads to many repeated samples in our balanced data construction. This in turn, increases the likelihood that samples are contrasted with themselves in the loss computation.\n\n## Training details\n\nThe full set of hyperparameters used for each scale is shown in Table 14. For choosing hyperparameters, we follow the OpenCLIP library [69], an open source reproduction of OpenAI\u2019s CLIP. For the small, medium, and large tracks, these hyperparameters are equal to those in the CLIP paper, except with reduced batch size so that training runs on reasonable hardware. For the xlarge track, batch size is increased from that in OpenAI\u2019s CLIP to accelerate training by allowing the use of many GPUs simultaneously with high utilization. For this run we also double the learning rate following prior work [28].\n\n## Evaluation details\n\nModels are evaluated over a wide range of 38 tasks to measure proficiency in various domains. We include 22 of the 27 classification tasks in the test suite of Radford et al. [111], excluding the few datasets that have license restrictions, are in video format, or are no longer available in their original form. We include 6 datasets that were designed to test generalization of models trained on ImageNet. We also include a majority of the Visual Task Adaptation Benchmark, excluding 3 datasets that are ill-suited for zero-shot evaluation [156]. We include 3 datasets from the WILDS benchmark, which tests robustness to distribution shifts and spurious correlations [83, 127]. Finally, we include 2 additional datasets, Dollar Street and GeoDE, which test robustness of classification performance across income levels and geographical regions [122, 114]. Furthermore, we evaluate zero-shot image and text retrieval on the Flickr30k and MSCOCO datasets, and image association on the WinoGAViL dataset [151, 26, 17]. The complete list of evaluation tasks is given in Table 15. We show a sample from each dataset in Figure 15.\n\n## Prompt choice\n\nSince we perform zero-shot evaluation, prompt and class name selection is important, and can have a significant impact on the results. To avoid heavy prompt engineering and overtuning to individual models, we opt to use the prompt templates used in Radford et al. [111] whenever possible. Most datasets come with pre-defined class names, but some are overwritten with more descriptive labels, again based on previous literature. For datasets with no precedent in zero-shot evaluation, we reuse prompt templates from other datasets with a similar domain and task (e.g., SVHN is evaluated with MNIST prompts and class names).\n\n## Evaluation metrics\n\nFor the majority of classification tasks, the primary evaluation metric is accuracy. For certain datasets with class imbalances, we instead compute mean per-class accuracy, as done in Radford et al. [111]. On the WILDS benchmark datasets, we use the primary metric specified for each dataset on their leaderboard. Dollar Street and GeoDE test model generalization across socioeconomic and geographic diversity. Thus, for Dollar Street, we compute worst-group top-5 accuracy, with groups defined by income level, emulating Rojas et al. [122]; for GeoDE, we compute worst-group accuracy, with groups defined by region (Africa, Americas, West Asia, East Asia, Southeast Asia, and Europe), as defined in Ramaswamy et al. [114]. For the image-text retrieval tasks, Flickr and MSCOCO, we compute both image and text recall (fraction of text captions for which the correct image was selected and vice versa), and plot their arithmetic mean. On WinoGAViL, we compute the", "images": [], "items": [{"type": "table", "rows": [["Scale", "Model", "Train compute (MACs)", "Pool size", "# samples seen", "Learning rate", "AdamW \u03b22", "Warmup", "Batch size"], ["small", "ViT-B/32", "9.5 \u00d7 10^16", "12.8M", "12.8M", "5e-4", "0.98", "500", "4096"], ["medium", "ViT-B/32", "9.5 \u00d7 10^17", "128M", "128M", "5e-4", "0.98", "500", "4096"], ["large", "ViT-B/16", "2.6 \u00d7 10^19", "1.28B", "1.28B", "5e-4", "0.98", "500", "8192"], ["xlarge", "ViT-L/14", "1.1 \u00d7 10^21", "12.8B", "12.8B", "1e-3", "0.95", "10k", "90112"]], "md": "|Scale|Model|Train compute (MACs)|Pool size|# samples seen|Learning rate|AdamW \u03b22|Warmup|Batch size|\n|---|---|---|---|---|---|---|---|---|\n|small|ViT-B/32|9.5 \u00d7 10^16|12.8M|12.8M|5e-4|0.98|500|4096|\n|medium|ViT-B/32|9.5 \u00d7 10^17|128M|128M|5e-4|0.98|500|4096|\n|large|ViT-B/16|2.6 \u00d7 10^19|1.28B|1.28B|5e-4|0.98|500|8192|\n|xlarge|ViT-L/14|1.1 \u00d7 10^21|12.8B|12.8B|1e-3|0.95|10k|90112|", "isPerfectTable": true, "csv": "\"Scale\",\"Model\",\"Train compute (MACs)\",\"Pool size\",\"# samples seen\",\"Learning rate\",\"AdamW \u03b22\",\"Warmup\",\"Batch size\"\n\"small\",\"ViT-B/32\",\"9.5 \u00d7 10^16\",\"12.8M\",\"12.8M\",\"5e-4\",\"0.98\",\"500\",\"4096\"\n\"medium\",\"ViT-B/32\",\"9.5 \u00d7 10^17\",\"128M\",\"128M\",\"5e-4\",\"0.98\",\"500\",\"4096\"\n\"large\",\"ViT-B/16\",\"2.6 \u00d7 10^19\",\"1.28B\",\"1.28B\",\"5e-4\",\"0.98\",\"500\",\"8192\"\n\"xlarge\",\"ViT-L/14\",\"1.1 \u00d7 10^21\",\"12.8B\",\"12.8B\",\"1e-3\",\"0.95\",\"10k\",\"90112\""}, {"type": "text", "value": "Remains an open problem. All balancing strategies lead to divergence of the CLIP contrastive loss and result in poor model performance. We hypothesize that this is due to the long-tailed nature of the data distribution, which leads to many repeated samples in our balanced data construction. This in turn, increases the likelihood that samples are contrasted with themselves in the loss computation.", "md": "Remains an open problem. All balancing strategies lead to divergence of the CLIP contrastive loss and result in poor model performance. We hypothesize that this is due to the long-tailed nature of the data distribution, which leads to many repeated samples in our balanced data construction. This in turn, increases the likelihood that samples are contrasted with themselves in the loss computation."}, {"type": "heading", "lvl": 2, "value": "Training details", "md": "## Training details"}, {"type": "text", "value": "The full set of hyperparameters used for each scale is shown in Table 14. For choosing hyperparameters, we follow the OpenCLIP library [69], an open source reproduction of OpenAI\u2019s CLIP. For the small, medium, and large tracks, these hyperparameters are equal to those in the CLIP paper, except with reduced batch size so that training runs on reasonable hardware. For the xlarge track, batch size is increased from that in OpenAI\u2019s CLIP to accelerate training by allowing the use of many GPUs simultaneously with high utilization. For this run we also double the learning rate following prior work [28].", "md": "The full set of hyperparameters used for each scale is shown in Table 14. For choosing hyperparameters, we follow the OpenCLIP library [69], an open source reproduction of OpenAI\u2019s CLIP. For the small, medium, and large tracks, these hyperparameters are equal to those in the CLIP paper, except with reduced batch size so that training runs on reasonable hardware. For the xlarge track, batch size is increased from that in OpenAI\u2019s CLIP to accelerate training by allowing the use of many GPUs simultaneously with high utilization. For this run we also double the learning rate following prior work [28]."}, {"type": "heading", "lvl": 2, "value": "Evaluation details", "md": "## Evaluation details"}, {"type": "text", "value": "Models are evaluated over a wide range of 38 tasks to measure proficiency in various domains. We include 22 of the 27 classification tasks in the test suite of Radford et al. [111], excluding the few datasets that have license restrictions, are in video format, or are no longer available in their original form. We include 6 datasets that were designed to test generalization of models trained on ImageNet. We also include a majority of the Visual Task Adaptation Benchmark, excluding 3 datasets that are ill-suited for zero-shot evaluation [156]. We include 3 datasets from the WILDS benchmark, which tests robustness to distribution shifts and spurious correlations [83, 127]. Finally, we include 2 additional datasets, Dollar Street and GeoDE, which test robustness of classification performance across income levels and geographical regions [122, 114]. Furthermore, we evaluate zero-shot image and text retrieval on the Flickr30k and MSCOCO datasets, and image association on the WinoGAViL dataset [151, 26, 17]. The complete list of evaluation tasks is given in Table 15. We show a sample from each dataset in Figure 15.", "md": "Models are evaluated over a wide range of 38 tasks to measure proficiency in various domains. We include 22 of the 27 classification tasks in the test suite of Radford et al. [111], excluding the few datasets that have license restrictions, are in video format, or are no longer available in their original form. We include 6 datasets that were designed to test generalization of models trained on ImageNet. We also include a majority of the Visual Task Adaptation Benchmark, excluding 3 datasets that are ill-suited for zero-shot evaluation [156]. We include 3 datasets from the WILDS benchmark, which tests robustness to distribution shifts and spurious correlations [83, 127]. Finally, we include 2 additional datasets, Dollar Street and GeoDE, which test robustness of classification performance across income levels and geographical regions [122, 114]. Furthermore, we evaluate zero-shot image and text retrieval on the Flickr30k and MSCOCO datasets, and image association on the WinoGAViL dataset [151, 26, 17]. The complete list of evaluation tasks is given in Table 15. We show a sample from each dataset in Figure 15."}, {"type": "heading", "lvl": 2, "value": "Prompt choice", "md": "## Prompt choice"}, {"type": "text", "value": "Since we perform zero-shot evaluation, prompt and class name selection is important, and can have a significant impact on the results. To avoid heavy prompt engineering and overtuning to individual models, we opt to use the prompt templates used in Radford et al. [111] whenever possible. Most datasets come with pre-defined class names, but some are overwritten with more descriptive labels, again based on previous literature. For datasets with no precedent in zero-shot evaluation, we reuse prompt templates from other datasets with a similar domain and task (e.g., SVHN is evaluated with MNIST prompts and class names).", "md": "Since we perform zero-shot evaluation, prompt and class name selection is important, and can have a significant impact on the results. To avoid heavy prompt engineering and overtuning to individual models, we opt to use the prompt templates used in Radford et al. [111] whenever possible. Most datasets come with pre-defined class names, but some are overwritten with more descriptive labels, again based on previous literature. For datasets with no precedent in zero-shot evaluation, we reuse prompt templates from other datasets with a similar domain and task (e.g., SVHN is evaluated with MNIST prompts and class names)."}, {"type": "heading", "lvl": 2, "value": "Evaluation metrics", "md": "## Evaluation metrics"}, {"type": "text", "value": "For the majority of classification tasks, the primary evaluation metric is accuracy. For certain datasets with class imbalances, we instead compute mean per-class accuracy, as done in Radford et al. [111]. On the WILDS benchmark datasets, we use the primary metric specified for each dataset on their leaderboard. Dollar Street and GeoDE test model generalization across socioeconomic and geographic diversity. Thus, for Dollar Street, we compute worst-group top-5 accuracy, with groups defined by income level, emulating Rojas et al. [122]; for GeoDE, we compute worst-group accuracy, with groups defined by region (Africa, Americas, West Asia, East Asia, Southeast Asia, and Europe), as defined in Ramaswamy et al. [114]. For the image-text retrieval tasks, Flickr and MSCOCO, we compute both image and text recall (fraction of text captions for which the correct image was selected and vice versa), and plot their arithmetic mean. On WinoGAViL, we compute the", "md": "For the majority of classification tasks, the primary evaluation metric is accuracy. For certain datasets with class imbalances, we instead compute mean per-class accuracy, as done in Radford et al. [111]. On the WILDS benchmark datasets, we use the primary metric specified for each dataset on their leaderboard. Dollar Street and GeoDE test model generalization across socioeconomic and geographic diversity. Thus, for Dollar Street, we compute worst-group top-5 accuracy, with groups defined by income level, emulating Rojas et al. [122]; for GeoDE, we compute worst-group accuracy, with groups defined by region (Africa, Americas, West Asia, East Asia, Southeast Asia, and Europe), as defined in Ramaswamy et al. [114]. For the image-text retrieval tasks, Flickr and MSCOCO, we compute both image and text recall (fraction of text captions for which the correct image was selected and vice versa), and plot their arithmetic mean. On WinoGAViL, we compute the"}]}, {"page": 41, "text": "                                                          Table 15: Evaluation tasks.\n  Task type         Dataset                         Task                             Test set size   Number of classes                 Main metric   Clean\n                    Caltech-101 [45]                Object recognition                      6,085                    102            mean per class     \u2713\n                    CIFAR-10 [86]                   Visual recognition                     10,000                     10                   accuracy    \u2713\n                    CIFAR-100 [86]                  Visual recognition                     10,000                    100                   accuracy    \u2713\n                    CLEVR Counts [76, 156]          Counting                               15,000                      8                   accuracy\n                    CLEVR Distance [76, 156]        Distance prediction                    15,000                      6                   accuracy\n                    Country211 [111, 140]           Geolocation                            21,100                    211                   accuracy    \u2713\n                    DTD [30]                        Texture classification                  1,880                     47                   accuracy    \u2713\n                    EuroSAT [63, 156]               Satellite imagery recognition           5,400                     10                   accuracy    \u2713\n                    FGVC Aircraft [95]              Aircraft recognition                    3,333                    100            mean per class     \u2713\n                    Food-101 [18]                   Food recognition                       25,250                    101                   accuracy    \u2713\n                    GTSRB [137]                     Traffic sign recognition               12,630                     43                   accuracy    \u2713\n                    ImageNet 1k [37]                Visual recognition                     50,000                  1,000                   accuracy    \u2713\n                    ImageNet Sketch [143]           Visual recognition                     50,889                  1,000                   accuracy    \u2713\n                    ImageNet V2 [121]               Visual recognition                     10,000                  1,000                   accuracy    \u2713\n                    ImageNet-A [65]                 Visual recognition                      7,500                    200                   accuracy    \u2713\n                    ImageNet-O [65]                 Visual recognition                      2,000                    200                   accuracy    \u2713\n                    ImageNet-R [64]                 Visual recognition                     30,000                    200                   accuracy    \u2713\n  Classification    KITTI distance [48, 156]        Distance prediction                       711                      4                   accuracy\n                    MNIST [89]                      Digit recognition                      10,000                     10                   accuracy    \u2713\n                    ObjectNet [13]                  Visual recognition                     18,574                    113                   accuracy    \u2713\n                    Oxford Flowers-102 [102]        Flower recognition                      6,149                    102            mean per class     \u2713\n                    Oxford-IIIT Pet [105, 156]      Pet classifi\n                                                               cation                       3,669                     37            mean per class     \u2713\n                    Pascal VOC 2007 [42]            Object recognition                     14,976                     20                   accuracy    \u2713\n                    PatchCamelyon [142, 156]        Metastatic tissue cls.                 32,768                      2                   accuracy\n                    Rendered SST2 [156]             Sentiment classification                1,821                      2                   accuracy    \u2713\n                    RESISC45 [27, 156]              Satellite imagery recognition           6,300                     45                   accuracy    \u2713\n                    Stanford Cars [85]              Vehicle recognition                     8,041                    196                   accuracy    \u2713\n                    STL-10 [31]                     Visual recognition                      8,000                     10                   accuracy    \u2713\n                    SUN-397 [146]                   Scene recognition                     108,754                    397                   accuracy    \u2713\n                    SVHN [99, 156]                  Digit recognition                       26032                     10                   accuracy    \u2713\n                    iWildCam [14, 83]               Animal recognition                     42,791                    182            macro F1 score     \u2713\n                    Camelyon17 [12, 83]             Metastatic tissue cls.                 85,054                      2                   accuracy\n                    FMoW [29, 83]                   Satellite imagery recognition          22,108                     62          worst-region acc.    \u2713\n                    Dollar Street [122]             Object recognition                      3,503                     58  worst-income top-5 acc.      \u2713\n                    GeoDE [114]                     Object recognition                     12,488                     40          worst-region acc.    \u2713\n                    Flickr30k [151]                 Image and text retrieval               31,014                   N/A                        R@1     \u2713\n  Retrieval         MSCOCO [26]                     Image and text retrieval                5,000                   N/A                        R@1     \u2713\n                    WinoGAViL [17]                  Commonsense association                 3,563                   N/A               Jaccard score    \u2713\nJaccard score (intersection-over-union) for each example, and show results for the harder samples (10\nand 12 candidates). More information on WinoGAViL evaluation can be found in Bitton et al. [17].\nClean subset. For fi             ve of our evaluation tasks (the two CLEVR tasks, the two Camelyon tasks, and\nKITTI) the zero-shot performance of all evaluated models appears to be close to that of random\nguessing, and lack correlation to the type of filtering method used (see Figure 27). Consequently, we\nstudied performance averaged only on the remaining 33 tasks, but found not substantial qualitative\ndifferences in our results. As a result, we opted to report the average on the full evaluation suite\nthroughout our study.\nZero-shot vs. fine-tuning protocols. One critical decision in DATACOMP is how exactly to evaluate\nmodels and whether or not to fine-tune models on evaluation tasks (i.e., supervised fine-tuning directly\non task training sets). We opt for zero-shot evaluation, where a models are applied to downstream\ntasks directly to 1) ease computational burden on participants and 2) measure the out-of-the-box\ngeneralization capabilities of our models. To validate this design decision, we conduct linear probes\non all models presented in Tables 3 and 18 on ImageNet. We follow a standard probing protocol\nand fine-tune the last linear layer from zero-shot initialization for 40 epochs with learning rate 1e-3,\nbatch size 256, AdamW optimizer with default settings with the exception of weight decay (that\nwe set to zero), and a cosine annealing schedule. As seen in Figure 16, zero-shot and linear probe\nperformance follow similar trends for both filtering and BYOD tracks. Moreover the Spearman rank\ncorrelation between the two protocols over the models considered is 0.99 for the filtering track and\n1.0 for BYOD. This suggests that better zero-shot models on ImageNet are correlated with better\nrepresentations of linear probe fine-tuning on ImageNet.\n                                                                              41", "md": "|Task type|Dataset|Task|Test set size|Number of classes|Main metric|Clean|\n|---|---|---|---|---|---|---|\n|Object recognition|Caltech-101 [45]|Object recognition|6,085|102|mean per class|\u2713|\n|Visual recognition|CIFAR-10 [86]|Visual recognition|10,000|10|accuracy|\u2713|\n|Visual recognition|CIFAR-100 [86]|Visual recognition|10,000|100|accuracy|\u2713|\n|Counting|CLEVR Counts [76, 156]|Counting|15,000|8|accuracy| |\n|Distance prediction|CLEVR Distance [76, 156]|Distance prediction|15,000|6|accuracy| |\n|Geolocation|Country211 [111, 140]|Geolocation|21,100|211|accuracy|\u2713|\n|Texture classification|DTD [30]|Texture classification|1,880|47|accuracy|\u2713|\n|Satellite imagery recognition|EuroSAT [63, 156]|Satellite imagery recognition|5,400|10|accuracy|\u2713|\n|Aircraft recognition|FGVC Aircraft [95]|Aircraft recognition|3,333|100|mean per class|\u2713|\n|Food recognition|Food-101 [18]|Food recognition|25,250|101|accuracy|\u2713|\n|Traffic sign recognition|GTSRB [137]|Traffic sign recognition|12,630|43|accuracy|\u2713|\n|Visual recognition|ImageNet 1k [37]|Visual recognition|50,000|1,000|accuracy|\u2713|\n|Visual recognition|ImageNet Sketch [143]|Visual recognition|50,889|1,000|accuracy|\u2713|\n|Visual recognition|ImageNet V2 [121]|Visual recognition|10,000|1,000|accuracy|\u2713|\n|Visual recognition|ImageNet-A [65]|Visual recognition|7,500|200|accuracy|\u2713|\n|Visual recognition|ImageNet-O [65]|Visual recognition|2,000|200|accuracy|\u2713|\n|Visual recognition|ImageNet-R [64]|Visual recognition|30,000|200|accuracy|\u2713|\n|Distance prediction|KITTI distance [48, 156]|Distance prediction|711|4|accuracy| |\n|Digit recognition|MNIST [89]|Digit recognition|10,000|10|accuracy|\u2713|\n|Visual recognition|ObjectNet [13]|Visual recognition|18,574|113|accuracy|\u2713|\n|Flower recognition|Oxford Flowers-102 [102]|Flower recognition|6,149|102|mean per class|\u2713|\n|Pet classification|Oxford-IIIT Pet [105, 156]|Pet classification|3,669|37|mean per class|\u2713|\n|Object recognition|Pascal VOC 2007 [42]|Object recognition|14,976|20|accuracy|\u2713|\n|Metastatic tissue cls.|PatchCamelyon [142, 156]|Metastatic tissue cls.|32,768|2|accuracy| |\n|Sentiment classification|Rendered SST2 [156]|Sentiment classification|1,821|2|accuracy|\u2713|\n|Satellite imagery recognition|RESISC45 [27, 156]|Satellite imagery recognition|6,300|45|accuracy|\u2713|\n|Vehicle recognition|Stanford Cars [85]|Vehicle recognition|8,041|196|accuracy|\u2713|\n|Visual recognition|STL-10 [31]|Visual recognition|8,000|10|accuracy|\u2713|\n|Scene recognition|SUN-397 [146]|Scene recognition|108,754|397|accuracy|\u2713|\n|Digit recognition|SVHN [99, 156]|Digit recognition|26,032|10|accuracy|\u2713|\n|Animal recognition|iWildCam [14, 83]|Animal recognition|42,791|182|macro F1 score|\u2713|\n|Metastatic tissue cls.|Camelyon17 [12, 83]|Metastatic tissue cls.|85,054|2|accuracy| |\n|Satellite imagery recognition|FMoW [29, 83]|Satellite imagery recognition|22,108|62|worst-region acc.|\u2713|\n|Object recognition|Dollar Street [122]|Object recognition|3,503|58|worst-income top-5 acc.|\u2713|\n|Object recognition|GeoDE [114]|Object recognition|12,488|40|worst-region acc.|\u2713|\n|Image and text retrieval|Flickr30k [151]|Image and text retrieval|31,014|N/A|R@1|\u2713|\n|Image and text retrieval|MSCOCO [26]|Image and text retrieval|5,000|N/A|R@1|\u2713|\n|Commonsense association|WinoGAViL [17]|Commonsense association|3,563|N/A|Jaccard score|\u2713|\n\nJaccard score (intersection-over-union) for each example, and show results for the harder samples (10 and 12 candidates). More information on WinoGAViL evaluation can be found in Bitton et al. [17].\n\nClean subset. For five of our evaluation tasks (the two CLEVR tasks, the two Camelyon tasks, and KITTI) the zero-shot performance of all evaluated models appears to be close to that of random guessing, and lack correlation to the type of filtering method used (see Figure 27). Consequently, we studied performance averaged only on the remaining 33 tasks, but found not substantial qualitative differences in our results. As a result, we opted to report the average on the full evaluation suite throughout our study.\n\nZero-shot vs. fine-tuning protocols. One critical decision in DATACOMP is how exactly to evaluate models and whether or not to fine-tune models on evaluation tasks (i.e., supervised fine-tuning directly on task training sets). We opt for zero-shot evaluation, where a models are applied to downstream tasks directly to 1) ease computational burden on participants and 2) measure the out-of-the-box generalization capabilities of our models. To validate this design decision, we conduct linear probes on all models presented in Tables 3 and 18 on ImageNet. We follow a standard probing protocol and fine-tune the last linear layer from zero-shot initialization for 40 epochs with learning rate 1e-3, batch size 256, AdamW optimizer with default settings with the exception of weight decay (that we set to zero), and a cosine annealing schedule. As seen in Figure 16, zero-shot and linear probe performance follow similar trends for both filtering and BYOD tracks. Moreover the Spearman rank correlation between the two protocols over the models considered is 0.99 for the filtering track and 1.0 for BYOD. This suggests that better zero-shot models on ImageNet are correlated with better representations of linear probe fine-tuning on ImageNet.\n\n41", "images": [], "items": [{"type": "table", "rows": [["Task type", "Dataset", "Task", "Test set size", "Number of classes", "Main metric", "Clean"], ["Object recognition", "Caltech-101 [45]", "Object recognition", "6,085", "102", "mean per class", "\u2713"], ["Visual recognition", "CIFAR-10 [86]", "Visual recognition", "10,000", "10", "accuracy", "\u2713"], ["Visual recognition", "CIFAR-100 [86]", "Visual recognition", "10,000", "100", "accuracy", "\u2713"], ["Counting", "CLEVR Counts [76, 156]", "Counting", "15,000", "8", "accuracy", ""], ["Distance prediction", "CLEVR Distance [76, 156]", "Distance prediction", "15,000", "6", "accuracy", ""], ["Geolocation", "Country211 [111, 140]", "Geolocation", "21,100", "211", "accuracy", "\u2713"], ["Texture classification", "DTD [30]", "Texture classification", "1,880", "47", "accuracy", "\u2713"], ["Satellite imagery recognition", "EuroSAT [63, 156]", "Satellite imagery recognition", "5,400", "10", "accuracy", "\u2713"], ["Aircraft recognition", "FGVC Aircraft [95]", "Aircraft recognition", "3,333", "100", "mean per class", "\u2713"], ["Food recognition", "Food-101 [18]", "Food recognition", "25,250", "101", "accuracy", "\u2713"], ["Traffic sign recognition", "GTSRB [137]", "Traffic sign recognition", "12,630", "43", "accuracy", "\u2713"], ["Visual recognition", "ImageNet 1k [37]", "Visual recognition", "50,000", "1,000", "accuracy", "\u2713"], ["Visual recognition", "ImageNet Sketch [143]", "Visual recognition", "50,889", "1,000", "accuracy", "\u2713"], ["Visual recognition", "ImageNet V2 [121]", "Visual recognition", "10,000", "1,000", "accuracy", "\u2713"], ["Visual recognition", "ImageNet-A [65]", "Visual recognition", "7,500", "200", "accuracy", "\u2713"], ["Visual recognition", "ImageNet-O [65]", "Visual recognition", "2,000", "200", "accuracy", "\u2713"], ["Visual recognition", "ImageNet-R [64]", "Visual recognition", "30,000", "200", "accuracy", "\u2713"], ["Distance prediction", "KITTI distance [48, 156]", "Distance prediction", "711", "4", "accuracy", ""], ["Digit recognition", "MNIST [89]", "Digit recognition", "10,000", "10", "accuracy", "\u2713"], ["Visual recognition", "ObjectNet [13]", "Visual recognition", "18,574", "113", "accuracy", "\u2713"], ["Flower recognition", "Oxford Flowers-102 [102]", "Flower recognition", "6,149", "102", "mean per class", "\u2713"], ["Pet classification", "Oxford-IIIT Pet [105, 156]", "Pet classification", "3,669", "37", "mean per class", "\u2713"], ["Object recognition", "Pascal VOC 2007 [42]", "Object recognition", "14,976", "20", "accuracy", "\u2713"], ["Metastatic tissue cls.", "PatchCamelyon [142, 156]", "Metastatic tissue cls.", "32,768", "2", "accuracy", ""], ["Sentiment classification", "Rendered SST2 [156]", "Sentiment classification", "1,821", "2", "accuracy", "\u2713"], ["Satellite imagery recognition", "RESISC45 [27, 156]", "Satellite imagery recognition", "6,300", "45", "accuracy", "\u2713"], ["Vehicle recognition", "Stanford Cars [85]", "Vehicle recognition", "8,041", "196", "accuracy", "\u2713"], ["Visual recognition", "STL-10 [31]", "Visual recognition", "8,000", "10", "accuracy", "\u2713"], ["Scene recognition", "SUN-397 [146]", "Scene recognition", "108,754", "397", "accuracy", "\u2713"], ["Digit recognition", "SVHN [99, 156]", "Digit recognition", "26,032", "10", "accuracy", "\u2713"], ["Animal recognition", "iWildCam [14, 83]", "Animal recognition", "42,791", "182", "macro F1 score", "\u2713"], ["Metastatic tissue cls.", "Camelyon17 [12, 83]", "Metastatic tissue cls.", "85,054", "2", "accuracy", ""], ["Satellite imagery recognition", "FMoW [29, 83]", "Satellite imagery recognition", "22,108", "62", "worst-region acc.", "\u2713"], ["Object recognition", "Dollar Street [122]", "Object recognition", "3,503", "58", "worst-income top-5 acc.", "\u2713"], ["Object recognition", "GeoDE [114]", "Object recognition", "12,488", "40", "worst-region acc.", "\u2713"], ["Image and text retrieval", "Flickr30k [151]", "Image and text retrieval", "31,014", "N/A", "R@1", "\u2713"], ["Image and text retrieval", "MSCOCO [26]", "Image and text retrieval", "5,000", "N/A", "R@1", "\u2713"], ["Commonsense association", "WinoGAViL [17]", "Commonsense association", "3,563", "N/A", "Jaccard score", "\u2713"]], "md": "|Task type|Dataset|Task|Test set size|Number of classes|Main metric|Clean|\n|---|---|---|---|---|---|---|\n|Object recognition|Caltech-101 [45]|Object recognition|6,085|102|mean per class|\u2713|\n|Visual recognition|CIFAR-10 [86]|Visual recognition|10,000|10|accuracy|\u2713|\n|Visual recognition|CIFAR-100 [86]|Visual recognition|10,000|100|accuracy|\u2713|\n|Counting|CLEVR Counts [76, 156]|Counting|15,000|8|accuracy| |\n|Distance prediction|CLEVR Distance [76, 156]|Distance prediction|15,000|6|accuracy| |\n|Geolocation|Country211 [111, 140]|Geolocation|21,100|211|accuracy|\u2713|\n|Texture classification|DTD [30]|Texture classification|1,880|47|accuracy|\u2713|\n|Satellite imagery recognition|EuroSAT [63, 156]|Satellite imagery recognition|5,400|10|accuracy|\u2713|\n|Aircraft recognition|FGVC Aircraft [95]|Aircraft recognition|3,333|100|mean per class|\u2713|\n|Food recognition|Food-101 [18]|Food recognition|25,250|101|accuracy|\u2713|\n|Traffic sign recognition|GTSRB [137]|Traffic sign recognition|12,630|43|accuracy|\u2713|\n|Visual recognition|ImageNet 1k [37]|Visual recognition|50,000|1,000|accuracy|\u2713|\n|Visual recognition|ImageNet Sketch [143]|Visual recognition|50,889|1,000|accuracy|\u2713|\n|Visual recognition|ImageNet V2 [121]|Visual recognition|10,000|1,000|accuracy|\u2713|\n|Visual recognition|ImageNet-A [65]|Visual recognition|7,500|200|accuracy|\u2713|\n|Visual recognition|ImageNet-O [65]|Visual recognition|2,000|200|accuracy|\u2713|\n|Visual recognition|ImageNet-R [64]|Visual recognition|30,000|200|accuracy|\u2713|\n|Distance prediction|KITTI distance [48, 156]|Distance prediction|711|4|accuracy| |\n|Digit recognition|MNIST [89]|Digit recognition|10,000|10|accuracy|\u2713|\n|Visual recognition|ObjectNet [13]|Visual recognition|18,574|113|accuracy|\u2713|\n|Flower recognition|Oxford Flowers-102 [102]|Flower recognition|6,149|102|mean per class|\u2713|\n|Pet classification|Oxford-IIIT Pet [105, 156]|Pet classification|3,669|37|mean per class|\u2713|\n|Object recognition|Pascal VOC 2007 [42]|Object recognition|14,976|20|accuracy|\u2713|\n|Metastatic tissue cls.|PatchCamelyon [142, 156]|Metastatic tissue cls.|32,768|2|accuracy| |\n|Sentiment classification|Rendered SST2 [156]|Sentiment classification|1,821|2|accuracy|\u2713|\n|Satellite imagery recognition|RESISC45 [27, 156]|Satellite imagery recognition|6,300|45|accuracy|\u2713|\n|Vehicle recognition|Stanford Cars [85]|Vehicle recognition|8,041|196|accuracy|\u2713|\n|Visual recognition|STL-10 [31]|Visual recognition|8,000|10|accuracy|\u2713|\n|Scene recognition|SUN-397 [146]|Scene recognition|108,754|397|accuracy|\u2713|\n|Digit recognition|SVHN [99, 156]|Digit recognition|26,032|10|accuracy|\u2713|\n|Animal recognition|iWildCam [14, 83]|Animal recognition|42,791|182|macro F1 score|\u2713|\n|Metastatic tissue cls.|Camelyon17 [12, 83]|Metastatic tissue cls.|85,054|2|accuracy| |\n|Satellite imagery recognition|FMoW [29, 83]|Satellite imagery recognition|22,108|62|worst-region acc.|\u2713|\n|Object recognition|Dollar Street [122]|Object recognition|3,503|58|worst-income top-5 acc.|\u2713|\n|Object recognition|GeoDE [114]|Object recognition|12,488|40|worst-region acc.|\u2713|\n|Image and text retrieval|Flickr30k [151]|Image and text retrieval|31,014|N/A|R@1|\u2713|\n|Image and text retrieval|MSCOCO [26]|Image and text retrieval|5,000|N/A|R@1|\u2713|\n|Commonsense association|WinoGAViL [17]|Commonsense association|3,563|N/A|Jaccard score|\u2713|", "isPerfectTable": true, "csv": "\"Task type\",\"Dataset\",\"Task\",\"Test set size\",\"Number of classes\",\"Main metric\",\"Clean\"\n\"Object recognition\",\"Caltech-101 [45]\",\"Object recognition\",\"6,085\",\"102\",\"mean per class\",\"\u2713\"\n\"Visual recognition\",\"CIFAR-10 [86]\",\"Visual recognition\",\"10,000\",\"10\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"CIFAR-100 [86]\",\"Visual recognition\",\"10,000\",\"100\",\"accuracy\",\"\u2713\"\n\"Counting\",\"CLEVR Counts [76, 156]\",\"Counting\",\"15,000\",\"8\",\"accuracy\",\"\"\n\"Distance prediction\",\"CLEVR Distance [76, 156]\",\"Distance prediction\",\"15,000\",\"6\",\"accuracy\",\"\"\n\"Geolocation\",\"Country211 [111, 140]\",\"Geolocation\",\"21,100\",\"211\",\"accuracy\",\"\u2713\"\n\"Texture classification\",\"DTD [30]\",\"Texture classification\",\"1,880\",\"47\",\"accuracy\",\"\u2713\"\n\"Satellite imagery recognition\",\"EuroSAT [63, 156]\",\"Satellite imagery recognition\",\"5,400\",\"10\",\"accuracy\",\"\u2713\"\n\"Aircraft recognition\",\"FGVC Aircraft [95]\",\"Aircraft recognition\",\"3,333\",\"100\",\"mean per class\",\"\u2713\"\n\"Food recognition\",\"Food-101 [18]\",\"Food recognition\",\"25,250\",\"101\",\"accuracy\",\"\u2713\"\n\"Traffic sign recognition\",\"GTSRB [137]\",\"Traffic sign recognition\",\"12,630\",\"43\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"ImageNet 1k [37]\",\"Visual recognition\",\"50,000\",\"1,000\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"ImageNet Sketch [143]\",\"Visual recognition\",\"50,889\",\"1,000\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"ImageNet V2 [121]\",\"Visual recognition\",\"10,000\",\"1,000\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"ImageNet-A [65]\",\"Visual recognition\",\"7,500\",\"200\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"ImageNet-O [65]\",\"Visual recognition\",\"2,000\",\"200\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"ImageNet-R [64]\",\"Visual recognition\",\"30,000\",\"200\",\"accuracy\",\"\u2713\"\n\"Distance prediction\",\"KITTI distance [48, 156]\",\"Distance prediction\",\"711\",\"4\",\"accuracy\",\"\"\n\"Digit recognition\",\"MNIST [89]\",\"Digit recognition\",\"10,000\",\"10\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"ObjectNet [13]\",\"Visual recognition\",\"18,574\",\"113\",\"accuracy\",\"\u2713\"\n\"Flower recognition\",\"Oxford Flowers-102 [102]\",\"Flower recognition\",\"6,149\",\"102\",\"mean per class\",\"\u2713\"\n\"Pet classification\",\"Oxford-IIIT Pet [105, 156]\",\"Pet classification\",\"3,669\",\"37\",\"mean per class\",\"\u2713\"\n\"Object recognition\",\"Pascal VOC 2007 [42]\",\"Object recognition\",\"14,976\",\"20\",\"accuracy\",\"\u2713\"\n\"Metastatic tissue cls.\",\"PatchCamelyon [142, 156]\",\"Metastatic tissue cls.\",\"32,768\",\"2\",\"accuracy\",\"\"\n\"Sentiment classification\",\"Rendered SST2 [156]\",\"Sentiment classification\",\"1,821\",\"2\",\"accuracy\",\"\u2713\"\n\"Satellite imagery recognition\",\"RESISC45 [27, 156]\",\"Satellite imagery recognition\",\"6,300\",\"45\",\"accuracy\",\"\u2713\"\n\"Vehicle recognition\",\"Stanford Cars [85]\",\"Vehicle recognition\",\"8,041\",\"196\",\"accuracy\",\"\u2713\"\n\"Visual recognition\",\"STL-10 [31]\",\"Visual recognition\",\"8,000\",\"10\",\"accuracy\",\"\u2713\"\n\"Scene recognition\",\"SUN-397 [146]\",\"Scene recognition\",\"108,754\",\"397\",\"accuracy\",\"\u2713\"\n\"Digit recognition\",\"SVHN [99, 156]\",\"Digit recognition\",\"26,032\",\"10\",\"accuracy\",\"\u2713\"\n\"Animal recognition\",\"iWildCam [14, 83]\",\"Animal recognition\",\"42,791\",\"182\",\"macro F1 score\",\"\u2713\"\n\"Metastatic tissue cls.\",\"Camelyon17 [12, 83]\",\"Metastatic tissue cls.\",\"85,054\",\"2\",\"accuracy\",\"\"\n\"Satellite imagery recognition\",\"FMoW [29, 83]\",\"Satellite imagery recognition\",\"22,108\",\"62\",\"worst-region acc.\",\"\u2713\"\n\"Object recognition\",\"Dollar Street [122]\",\"Object recognition\",\"3,503\",\"58\",\"worst-income top-5 acc.\",\"\u2713\"\n\"Object recognition\",\"GeoDE [114]\",\"Object recognition\",\"12,488\",\"40\",\"worst-region acc.\",\"\u2713\"\n\"Image and text retrieval\",\"Flickr30k [151]\",\"Image and text retrieval\",\"31,014\",\"N/A\",\"R@1\",\"\u2713\"\n\"Image and text retrieval\",\"MSCOCO [26]\",\"Image and text retrieval\",\"5,000\",\"N/A\",\"R@1\",\"\u2713\"\n\"Commonsense association\",\"WinoGAViL [17]\",\"Commonsense association\",\"3,563\",\"N/A\",\"Jaccard score\",\"\u2713\""}, {"type": "text", "value": "Jaccard score (intersection-over-union) for each example, and show results for the harder samples (10 and 12 candidates). More information on WinoGAViL evaluation can be found in Bitton et al. [17].\n\nClean subset. For five of our evaluation tasks (the two CLEVR tasks, the two Camelyon tasks, and KITTI) the zero-shot performance of all evaluated models appears to be close to that of random guessing, and lack correlation to the type of filtering method used (see Figure 27). Consequently, we studied performance averaged only on the remaining 33 tasks, but found not substantial qualitative differences in our results. As a result, we opted to report the average on the full evaluation suite throughout our study.\n\nZero-shot vs. fine-tuning protocols. One critical decision in DATACOMP is how exactly to evaluate models and whether or not to fine-tune models on evaluation tasks (i.e., supervised fine-tuning directly on task training sets). We opt for zero-shot evaluation, where a models are applied to downstream tasks directly to 1) ease computational burden on participants and 2) measure the out-of-the-box generalization capabilities of our models. To validate this design decision, we conduct linear probes on all models presented in Tables 3 and 18 on ImageNet. We follow a standard probing protocol and fine-tune the last linear layer from zero-shot initialization for 40 epochs with learning rate 1e-3, batch size 256, AdamW optimizer with default settings with the exception of weight decay (that we set to zero), and a cosine annealing schedule. As seen in Figure 16, zero-shot and linear probe performance follow similar trends for both filtering and BYOD tracks. Moreover the Spearman rank correlation between the two protocols over the models considered is 0.99 for the filtering track and 1.0 for BYOD. This suggests that better zero-shot models on ImageNet are correlated with better representations of linear probe fine-tuning on ImageNet.\n\n41", "md": "Jaccard score (intersection-over-union) for each example, and show results for the harder samples (10 and 12 candidates). More information on WinoGAViL evaluation can be found in Bitton et al. [17].\n\nClean subset. For five of our evaluation tasks (the two CLEVR tasks, the two Camelyon tasks, and KITTI) the zero-shot performance of all evaluated models appears to be close to that of random guessing, and lack correlation to the type of filtering method used (see Figure 27). Consequently, we studied performance averaged only on the remaining 33 tasks, but found not substantial qualitative differences in our results. As a result, we opted to report the average on the full evaluation suite throughout our study.\n\nZero-shot vs. fine-tuning protocols. One critical decision in DATACOMP is how exactly to evaluate models and whether or not to fine-tune models on evaluation tasks (i.e., supervised fine-tuning directly on task training sets). We opt for zero-shot evaluation, where a models are applied to downstream tasks directly to 1) ease computational burden on participants and 2) measure the out-of-the-box generalization capabilities of our models. To validate this design decision, we conduct linear probes on all models presented in Tables 3 and 18 on ImageNet. We follow a standard probing protocol and fine-tune the last linear layer from zero-shot initialization for 40 epochs with learning rate 1e-3, batch size 256, AdamW optimizer with default settings with the exception of weight decay (that we set to zero), and a cosine annealing schedule. As seen in Figure 16, zero-shot and linear probe performance follow similar trends for both filtering and BYOD tracks. Moreover the Spearman rank correlation between the two protocols over the models considered is 0.99 for the filtering track and 1.0 for BYOD. This suggests that better zero-shot models on ImageNet are correlated with better representations of linear probe fine-tuning on ImageNet.\n\n41"}]}, {"page": 42, "text": "                           CIFAR-10        CIFAR-100       CLEVR Counts       CLEVR Dist.      Caltech-101\n                         Camelyon17       Country211            DTD          Dollar Street       EuroSAT\n                        FGVC Aircraft         FMoW            FairFace           Flickr        Flowers-102\n                          Food-101           GTSRB             GeoDE         ImageNet 1k    ImageNet Sketch\n                        ImageNet v2       ImageNet-A        ImageNet-O       ImageNet-R            KITTI\n                            MNIST           MSCOCO           ObjectNet      Oxford-IIIT Pet    Pascal VOC\n                           8\n                       PatchCamelyon        RESISC45            SST2            STL-10           SUN397\n                            SVHN         Stanford Cars        UTKFace         WinoGAViL         iWildCam\n             Figure 15: Randomly sampled images from the evaluation datasets we consider.\nO.1      Visual Question Answering\nIn addition to our evaluation suite containing multiple classification and retrieval tasks, we conducted\nexperiments on visual question answering. More specifically, following Shen et al. [132], we use\nthe CLIP models to contrast images with prompts formed by the questions and each candidate\nanswer, without fine-tuning (i.e., in a zero-shot setting). Using the VQA v1 dataset [2], for each\ncandidate answer, we construct a text prompt that also includes the question following the template\nQuestion:         [question text] Answer:                [answer text], as in Ilharco et al. [70]. This text is then fed\nto CLIP\u2019s text encoder. As previously noted by multiple authors, CLIP models struggle on this task, potentially\ndue to the mismatch between the text in the downstream task and the captions seen during pre-training Shen\n                                                                 42", "md": "# Evaluation Datasets\n\n## Evaluation Datasets\n\n|CIFAR-10|CIFAR-100|CLEVR Counts|CLEVR Dist.|Caltech-101|\n|---|---|---|---|---|\n|Camelyon17|Country211|DTD|Dollar Street|EuroSAT|\n|FGVC Aircraft|FMoW|FairFace|Flickr|Flowers-102|\n|Food-101|GTSRB|GeoDE|ImageNet 1k|ImageNet Sketch|\n|ImageNet v2|ImageNet-A|ImageNet-O|ImageNet-R|KITTI|\n|MNIST|MSCOCO|ObjectNet|Oxford-IIIT Pet|Pascal VOC|\n|PatchCamelyon|RESISC45|SST2|STL-10|SUN397|\n|SVHN|Stanford Cars|UTKFace|WinoGAViL|iWildCam|\n\nFigure 15: Randomly sampled images from the evaluation datasets we consider.\n\n### Visual Question Answering\n\nIn addition to our evaluation suite containing multiple classification and retrieval tasks, we conducted experiments on visual question answering. More specifically, following Shen et al. [132], we use the CLIP models to contrast images with prompts formed by the questions and each candidate answer, without fine-tuning (i.e., in a zero-shot setting). Using the VQA v1 dataset [2], for each candidate answer, we construct a text prompt that also includes the question following the template:\n\n$$\\text{Question: [question text] Answer: [answer text]}$$\n\nThis text is then fed to CLIP\u2019s text encoder. As previously noted by multiple authors, CLIP models struggle on this task, potentially due to the mismatch between the text in the downstream task and the captions seen during pre-training Shen.", "images": [{"name": "page-42-0.jpg", "height": 40, "width": 40, "x": 181, "y": 91}, {"name": "page-42-1.jpg", "height": 40, "width": 40, "x": 232, "y": 91}, {"name": "page-42-5.jpg", "height": 40, "width": 40, "x": 181, "y": 154}, {"name": "page-42-11.jpg", "height": 40, "width": 40, "x": 232, "y": 216}, {"name": "page-42-10.jpg", "height": 40, "width": 40, "x": 181, "y": 216}, {"name": "page-42-4.jpg", "height": 40, "width": 40, "x": 388, "y": 91}, {"name": "page-42-2.jpg", "height": 40, "width": 41, "x": 284, "y": 91}, {"name": "page-42-3.jpg", "height": 40, "width": 40, "x": 336, "y": 91}, {"name": "page-42-6.jpg", "height": 40, "width": 40, "x": 232, "y": 154}, {"name": "page-42-7.jpg", "height": 40, "width": 41, "x": 284, "y": 154}, {"name": "page-42-8.jpg", "height": 40, "width": 40, "x": 336, "y": 154}, {"name": "page-42-9.jpg", "height": 40, "width": 40, "x": 388, "y": 154}, {"name": "page-42-15.jpg", "height": 41, "width": 40, "x": 181, "y": 278}, {"name": "page-42-14.jpg", "height": 40, "width": 40, "x": 388, "y": 216}, {"name": "page-42-16.jpg", "height": 41, "width": 40, "x": 232, "y": 278}, {"name": "page-42-17.jpg", "height": 41, "width": 41, "x": 284, "y": 278}, {"name": "page-42-18.jpg", "height": 41, "width": 40, "x": 336, "y": 278}, {"name": "page-42-12.jpg", "height": 40, "width": 41, "x": 284, "y": 216}, {"name": "page-42-20.jpg", "height": 40, "width": 40, "x": 181, "y": 342}, {"name": "page-42-13.jpg", "height": 40, "width": 40, "x": 336, "y": 216}, {"name": "page-42-19.jpg", "height": 41, "width": 40, "x": 388, "y": 278}, {"name": "page-42-24.jpg", "height": 40, "width": 40, "x": 388, "y": 342}, {"name": "page-42-22.jpg", "height": 40, "width": 41, "x": 284, "y": 342}, {"name": "page-42-21.jpg", "height": 40, "width": 40, "x": 232, "y": 342}, {"name": "page-42-23.jpg", "height": 40, "width": 40, "x": 336, "y": 342}, {"name": "page-42-25.jpg", "height": 40, "width": 40, "x": 181, "y": 404}, {"name": "page-42-27.jpg", "height": 40, "width": 41, "x": 284, "y": 404}, {"name": "page-42-26.jpg", "height": 40, "width": 40, "x": 232, "y": 404}, {"name": "page-42-28.jpg", "height": 40, "width": 40, "x": 336, "y": 404}, {"name": "page-42-30.jpg", "height": 40, "width": 40, "x": 181, "y": 467}, {"name": "page-42-33.jpg", "height": 40, "width": 40, "x": 336, "y": 467}, {"name": "page-42-37.jpg", "height": 40, "width": 41, "x": 284, "y": 530}, {"name": "page-42-35.jpg", "height": 40, "width": 40, "x": 181, "y": 530}, {"name": "page-42-32.jpg", "height": 40, "width": 41, "x": 284, "y": 467}, {"name": "page-42-29.jpg", "height": 40, "width": 40, "x": 388, "y": 404}, {"name": "page-42-31.jpg", "height": 40, "width": 40, "x": 232, "y": 467}, {"name": "page-42-34.jpg", "height": 40, "width": 40, "x": 388, "y": 467}, {"name": "page-42-36.jpg", "height": 40, "width": 40, "x": 232, "y": 530}, {"name": "page-42-38.jpg", "height": 40, "width": 40, "x": 336, "y": 530}, {"name": "page-42-39.jpg", "height": 40, "width": 40, "x": 388, "y": 530}], "items": [{"type": "heading", "lvl": 1, "value": "Evaluation Datasets", "md": "# Evaluation Datasets"}, {"type": "heading", "lvl": 2, "value": "Evaluation Datasets", "md": "## Evaluation Datasets"}, {"type": "table", "rows": [["CIFAR-10", "CIFAR-100", "CLEVR Counts", "CLEVR Dist.", "Caltech-101"], ["Camelyon17", "Country211", "DTD", "Dollar Street", "EuroSAT"], ["FGVC Aircraft", "FMoW", "FairFace", "Flickr", "Flowers-102"], ["Food-101", "GTSRB", "GeoDE", "ImageNet 1k", "ImageNet Sketch"], ["ImageNet v2", "ImageNet-A", "ImageNet-O", "ImageNet-R", "KITTI"], ["MNIST", "MSCOCO", "ObjectNet", "Oxford-IIIT Pet", "Pascal VOC"], ["PatchCamelyon", "RESISC45", "SST2", "STL-10", "SUN397"], ["SVHN", "Stanford Cars", "UTKFace", "WinoGAViL", "iWildCam"]], "md": "|CIFAR-10|CIFAR-100|CLEVR Counts|CLEVR Dist.|Caltech-101|\n|---|---|---|---|---|\n|Camelyon17|Country211|DTD|Dollar Street|EuroSAT|\n|FGVC Aircraft|FMoW|FairFace|Flickr|Flowers-102|\n|Food-101|GTSRB|GeoDE|ImageNet 1k|ImageNet Sketch|\n|ImageNet v2|ImageNet-A|ImageNet-O|ImageNet-R|KITTI|\n|MNIST|MSCOCO|ObjectNet|Oxford-IIIT Pet|Pascal VOC|\n|PatchCamelyon|RESISC45|SST2|STL-10|SUN397|\n|SVHN|Stanford Cars|UTKFace|WinoGAViL|iWildCam|", "isPerfectTable": true, "csv": "\"CIFAR-10\",\"CIFAR-100\",\"CLEVR Counts\",\"CLEVR Dist.\",\"Caltech-101\"\n\"Camelyon17\",\"Country211\",\"DTD\",\"Dollar Street\",\"EuroSAT\"\n\"FGVC Aircraft\",\"FMoW\",\"FairFace\",\"Flickr\",\"Flowers-102\"\n\"Food-101\",\"GTSRB\",\"GeoDE\",\"ImageNet 1k\",\"ImageNet Sketch\"\n\"ImageNet v2\",\"ImageNet-A\",\"ImageNet-O\",\"ImageNet-R\",\"KITTI\"\n\"MNIST\",\"MSCOCO\",\"ObjectNet\",\"Oxford-IIIT Pet\",\"Pascal VOC\"\n\"PatchCamelyon\",\"RESISC45\",\"SST2\",\"STL-10\",\"SUN397\"\n\"SVHN\",\"Stanford Cars\",\"UTKFace\",\"WinoGAViL\",\"iWildCam\""}, {"type": "text", "value": "Figure 15: Randomly sampled images from the evaluation datasets we consider.", "md": "Figure 15: Randomly sampled images from the evaluation datasets we consider."}, {"type": "heading", "lvl": 3, "value": "Visual Question Answering", "md": "### Visual Question Answering"}, {"type": "text", "value": "In addition to our evaluation suite containing multiple classification and retrieval tasks, we conducted experiments on visual question answering. More specifically, following Shen et al. [132], we use the CLIP models to contrast images with prompts formed by the questions and each candidate answer, without fine-tuning (i.e., in a zero-shot setting). Using the VQA v1 dataset [2], for each candidate answer, we construct a text prompt that also includes the question following the template:\n\n$$\\text{Question: [question text] Answer: [answer text]}$$\n\nThis text is then fed to CLIP\u2019s text encoder. As previously noted by multiple authors, CLIP models struggle on this task, potentially due to the mismatch between the text in the downstream task and the captions seen during pre-training Shen.", "md": "In addition to our evaluation suite containing multiple classification and retrieval tasks, we conducted experiments on visual question answering. More specifically, following Shen et al. [132], we use the CLIP models to contrast images with prompts formed by the questions and each candidate answer, without fine-tuning (i.e., in a zero-shot setting). Using the VQA v1 dataset [2], for each candidate answer, we construct a text prompt that also includes the question following the template:\n\n$$\\text{Question: [question text] Answer: [answer text]}$$\n\nThis text is then fed to CLIP\u2019s text encoder. As previously noted by multiple authors, CLIP models struggle on this task, potentially due to the mismatch between the text in the downstream task and the captions seen during pre-training Shen."}]}, {"page": 43, "text": "                         CommonPool                                         BYOD\n         Linear probe ImageNet                         Linear probe ImageNet\n           0.8                                           0.8                                                small\n           0.6                                           0.6                                                medium\n           0.4                                           0.4                                                large\n                                                                                                            xlarge\n           0.2                                           0.2                                                x=y\n           0.00.0     0.2     0.4     0.6     0.8        0.00.0     0.2     0.4     0.6     0.8\n                      Zero-shot ImageNet                            Zero-shot ImageNet\nFigure 16: Zero-shot ImageNet and Linear probe ImageNet performance for models from Tables\n3 and 18. Relative ordering of models demonstrates high rank correlations of 0.99 and 1.0 for\nCOMMONPOOL and BYOD respectively.\n           0.8           ImageNet                   0.6            Average\n         ImageNet accuracy                         Average accuracy                                  No filtering\n           0.6                                                                                       Basic\n                                                    0.4                                              CLIP score\n           0.4                                                                                       Image-based\n                                                                                                     Text-based\n           0.2                                      0.2                                              Rand. subset\n                                                                                                     ImageNet dist.\n           0.0\n                  0.050   0.075    0.100                   0.050    0.075    0.100\n                       VQA accuracy                              VQA accuracy\nFigure 17: Correlation between zero-shot performance on the VQA v1 dataset and results on\nImageNet and our full evaluation suite.\net al. [132], Ilharco et al. [70], Song et al. [134]. Nonetheless, we observe a strong correlation between VQA\nperformance and ImageNet accuracy (0.877) and between VQA performance and average performance on our\nfull evaluation suite. Full results are shown in Figure 17.\nP     Baseline details\nHere we provide additional details on the creation of our baseline subsets. To highlight the qualitative differences\nbetween the filtering strategies we also provide visualization for No filtering (Figure 18), Basic filtering (Figure\n19), and CLIP score (L/14 30%) (Figure 20), which can all be found in Table 3. Notice that No filtering gives\nrelatively noisy data (e.g., matching a bicycle with a caption: \u201cIMG_2187.jpg\u201d), while CLIP score samples give\nqualitatively more descriptive cations.\n                                                             43", "md": "```markdown\n|CommonPool|BYOD|\n|---|---|\n|Linear probe ImageNet|Linear probe ImageNet|\n|0.8|0.8|small|\n|0.6|0.6|medium|\n|0.4|0.4|large|\n| | |xlarge|\n|0.2|0.2|x=y|\n|0.0|0.0|0.2|0.4|0.6|0.8|0.0|0.0|0.2|0.4|0.6|0.8|\n\n$$\n\\text{Figure 16: Zero-shot ImageNet and Linear probe ImageNet performance for models from Tables 3 and 18. Relative ordering of models demonstrates high rank correlations of 0.99 and 1.0 for COMMONPOOL and BYOD respectively.}\n$$\n\n|0.8|ImageNet|0.6|Average|\n|---|---|---|---|\n|ImageNet accuracy| |Average accuracy|No filtering|\n|0.6| | |Basic|\n| | | |CLIP score|\n|0.4| | |Image-based|\n| | | |Text-based|\n|0.2|0.2| |Rand. subset|\n|0.0| |ImageNet dist.| |\n| |0.050|0.075|0.100|0.050|0.075|0.100|\n\n$$\n\\text{Figure 17: Correlation between zero-shot performance on the VQA v1 dataset and results on ImageNet and our full evaluation suite.}\n$$\n\net al. [132], Ilharco et al. [70], Song et al. [134]. Nonetheless, we observe a strong correlation between VQA performance and ImageNet accuracy (0.877) and between VQA performance and average performance on our full evaluation suite. Full results are shown in Figure 17.\n\nBaseline details\n\nHere we provide additional details on the creation of our baseline subsets. To highlight the qualitative differences between the filtering strategies we also provide visualization for No filtering (Figure 18), Basic filtering (Figure 19), and CLIP score (L/14 30%) (Figure 20), which can all be found in Table 3. Notice that No filtering gives relatively noisy data (e.g., matching a bicycle with a caption: \u201cIMG_2187.jpg\u201d), while CLIP score samples give qualitatively more descriptive captions.\n\n```", "images": [], "items": [{"type": "text", "value": "```markdown", "md": "```markdown"}, {"type": "table", "rows": [["CommonPool", "BYOD"], ["Linear probe ImageNet", "Linear probe ImageNet"], ["0.8", "0.8", "small"], ["0.6", "0.6", "medium"], ["0.4", "0.4", "large"], ["", "", "xlarge"], ["0.2", "0.2", "x=y"], ["0.0", "0.0", "0.2", "0.4", "0.6", "0.8", "0.0", "0.0", "0.2", "0.4", "0.6", "0.8"]], "md": "|CommonPool|BYOD|\n|---|---|\n|Linear probe ImageNet|Linear probe ImageNet|\n|0.8|0.8|small|\n|0.6|0.6|medium|\n|0.4|0.4|large|\n| | |xlarge|\n|0.2|0.2|x=y|\n|0.0|0.0|0.2|0.4|0.6|0.8|0.0|0.0|0.2|0.4|0.6|0.8|", "isPerfectTable": false, "csv": "\"CommonPool\",\"BYOD\"\n\"Linear probe ImageNet\",\"Linear probe ImageNet\"\n\"0.8\",\"0.8\",\"small\"\n\"0.6\",\"0.6\",\"medium\"\n\"0.4\",\"0.4\",\"large\"\n\"\",\"\",\"xlarge\"\n\"0.2\",\"0.2\",\"x=y\"\n\"0.0\",\"0.0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"0.0\",\"0.0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\""}, {"type": "text", "value": "$$\n\\text{Figure 16: Zero-shot ImageNet and Linear probe ImageNet performance for models from Tables 3 and 18. Relative ordering of models demonstrates high rank correlations of 0.99 and 1.0 for COMMONPOOL and BYOD respectively.}\n$$", "md": "$$\n\\text{Figure 16: Zero-shot ImageNet and Linear probe ImageNet performance for models from Tables 3 and 18. Relative ordering of models demonstrates high rank correlations of 0.99 and 1.0 for COMMONPOOL and BYOD respectively.}\n$$"}, {"type": "table", "rows": [["0.8", "ImageNet", "0.6", "Average"], ["ImageNet accuracy", "", "Average accuracy", "No filtering"], ["0.6", "", "", "Basic"], ["", "", "", "CLIP score"], ["0.4", "", "", "Image-based"], ["", "", "", "Text-based"], ["0.2", "0.2", "", "Rand. subset"], ["0.0", "", "ImageNet dist.", ""], ["", "0.050", "0.075", "0.100", "0.050", "0.075", "0.100"]], "md": "|0.8|ImageNet|0.6|Average|\n|---|---|---|---|\n|ImageNet accuracy| |Average accuracy|No filtering|\n|0.6| | |Basic|\n| | | |CLIP score|\n|0.4| | |Image-based|\n| | | |Text-based|\n|0.2|0.2| |Rand. subset|\n|0.0| |ImageNet dist.| |\n| |0.050|0.075|0.100|0.050|0.075|0.100|", "isPerfectTable": false, "csv": "\"0.8\",\"ImageNet\",\"0.6\",\"Average\"\n\"ImageNet accuracy\",\"\",\"Average accuracy\",\"No filtering\"\n\"0.6\",\"\",\"\",\"Basic\"\n\"\",\"\",\"\",\"CLIP score\"\n\"0.4\",\"\",\"\",\"Image-based\"\n\"\",\"\",\"\",\"Text-based\"\n\"0.2\",\"0.2\",\"\",\"Rand. subset\"\n\"0.0\",\"\",\"ImageNet dist.\",\"\"\n\"\",\"0.050\",\"0.075\",\"0.100\",\"0.050\",\"0.075\",\"0.100\""}, {"type": "text", "value": "$$\n\\text{Figure 17: Correlation between zero-shot performance on the VQA v1 dataset and results on ImageNet and our full evaluation suite.}\n$$\n\net al. [132], Ilharco et al. [70], Song et al. [134]. Nonetheless, we observe a strong correlation between VQA performance and ImageNet accuracy (0.877) and between VQA performance and average performance on our full evaluation suite. Full results are shown in Figure 17.\n\nBaseline details\n\nHere we provide additional details on the creation of our baseline subsets. To highlight the qualitative differences between the filtering strategies we also provide visualization for No filtering (Figure 18), Basic filtering (Figure 19), and CLIP score (L/14 30%) (Figure 20), which can all be found in Table 3. Notice that No filtering gives relatively noisy data (e.g., matching a bicycle with a caption: \u201cIMG_2187.jpg\u201d), while CLIP score samples give qualitatively more descriptive captions.\n\n```", "md": "$$\n\\text{Figure 17: Correlation between zero-shot performance on the VQA v1 dataset and results on ImageNet and our full evaluation suite.}\n$$\n\net al. [132], Ilharco et al. [70], Song et al. [134]. Nonetheless, we observe a strong correlation between VQA performance and ImageNet accuracy (0.877) and between VQA performance and average performance on our full evaluation suite. Full results are shown in Figure 17.\n\nBaseline details\n\nHere we provide additional details on the creation of our baseline subsets. To highlight the qualitative differences between the filtering strategies we also provide visualization for No filtering (Figure 18), Basic filtering (Figure 19), and CLIP score (L/14 30%) (Figure 20), which can all be found in Table 3. Notice that No filtering gives relatively noisy data (e.g., matching a bicycle with a caption: \u201cIMG_2187.jpg\u201d), while CLIP score samples give qualitatively more descriptive captions.\n\n```"}]}, {"page": 44, "text": "                                                                                         20110531 4665RWw [F]\n                             Organos muntoriales                                         Grotte des Demoiselles\n                                                                                         [Ganges]\n                              iPhone6s Plus/6 Plus\n                               WiFi\n                             LINKASE     with WiFi\n                                     iPhone 6s\n                             Plus/6 Plus_0\n                                                                                         Carregador port\u00e1til para\n                             IMG_2187.jpg                                                Smartphones 5000mAh 5V\n                                                                                         2.1A - JS Solu\u00e7\u00f5es em\n                                                                                         Seguran\u00e7a\n                             NACIO EUROPA EN LA                                          JUMP LEADS HEAVY DUTY\n                             EDAD MEDIA                                                  COMMERCIAL 4.5 M 700\n          LLORENS                                                                        AMP\n                LIRRERI\n                             Energy Stocks Fuel Market                                   2019-02-08\n                             Rally\nFigure 18: An i.i.d. sample from small COMMONPOOL generated after applying the No filter strategy.\nHence, these samples represent random images from COMMONPOOL.\n                                                 No filter\n                                                             44", "md": "# Document\n\n20110531 4665RWw [F]\n\nOrganos muntoriales Grotte des Demoiselles [Ganges]\n\niPhone6s Plus/6 Plus\n\nWiFi\n\nLINKASE with WiFi iPhone 6s Plus/6 Plus_0\n\nCarregador port\u00e1til para Smartphones 5000mAh 5V 2.1A - JS Solu\u00e7\u00f5es em Seguran\u00e7a\n\nNACIO EUROPA EN LA EDAD MEDIA\n\nJUMP LEADS HEAVY DUTY COMMERCIAL 4.5 M 700 AMP\n\nLLORENS LIRRERI\n\nEnergy Stocks Fuel Market Rally 2019-02-08\n\nFigure 18: An i.i.d. sample from small COMMONPOOL generated after applying the No filter strategy. Hence, these samples represent random images from COMMONPOOL.\n\nNo filter 44", "images": [{"name": "page-44-0.jpg", "height": 66, "width": 66, "x": 130, "y": 268}, {"name": "page-44-4.jpg", "height": 37, "width": 66, "x": 130, "y": 553}, {"name": "page-44-2.jpg", "height": 50, "width": 66, "x": 318, "y": 456}, {"name": "page-44-7.jpg", "height": 44, "width": 66, "x": 318, "y": 279}, {"name": "page-44-3.jpg", "height": 41, "width": 66, "x": 318, "y": 550}, {"name": "page-44-5.jpg", "height": 50, "width": 66, "x": 130, "y": 366}, {"name": "page-44-6.jpg", "height": 88, "width": 66, "x": 130, "y": 175}, {"name": "page-44-1.jpg", "height": 100, "width": 66, "x": 130, "y": 431}, {"name": "page-44-8.jpg", "height": 66, "width": 66, "x": 318, "y": 358}, {"name": "page-44-9.jpg", "height": 100, "width": 66, "x": 318, "y": 169}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "20110531 4665RWw [F]\n\nOrganos muntoriales Grotte des Demoiselles [Ganges]\n\niPhone6s Plus/6 Plus\n\nWiFi\n\nLINKASE with WiFi iPhone 6s Plus/6 Plus_0\n\nCarregador port\u00e1til para Smartphones 5000mAh 5V 2.1A - JS Solu\u00e7\u00f5es em Seguran\u00e7a\n\nNACIO EUROPA EN LA EDAD MEDIA\n\nJUMP LEADS HEAVY DUTY COMMERCIAL 4.5 M 700 AMP\n\nLLORENS LIRRERI\n\nEnergy Stocks Fuel Market Rally 2019-02-08\n\nFigure 18: An i.i.d. sample from small COMMONPOOL generated after applying the No filter strategy. Hence, these samples represent random images from COMMONPOOL.\n\nNo filter 44", "md": "20110531 4665RWw [F]\n\nOrganos muntoriales Grotte des Demoiselles [Ganges]\n\niPhone6s Plus/6 Plus\n\nWiFi\n\nLINKASE with WiFi iPhone 6s Plus/6 Plus_0\n\nCarregador port\u00e1til para Smartphones 5000mAh 5V 2.1A - JS Solu\u00e7\u00f5es em Seguran\u00e7a\n\nNACIO EUROPA EN LA EDAD MEDIA\n\nJUMP LEADS HEAVY DUTY COMMERCIAL 4.5 M 700 AMP\n\nLLORENS LIRRERI\n\nEnergy Stocks Fuel Market Rally 2019-02-08\n\nFigure 18: An i.i.d. sample from small COMMONPOOL generated after applying the No filter strategy. Hence, these samples represent random images from COMMONPOOL.\n\nNo filter 44"}]}, {"page": 45, "text": "                                                                                             Implication in the\n                                                                                             classroom:\n                               status report templates -                                     O Step 3: We Must Work it\n                               12+ free word documents                                       Out\n                               download | free,                                              Say and mean \"We have to\n                               Powerpoint templates                                          work it out\". The behaviour\n                                                                                             cannot\n                                                                                             co...\n                               City 39 mm Quartz                                             Astro ATA 3050 INSERTION\n                                                                                             TOOL, 16/20 GA\n                               1006: Rookwood pink mat                                       WN | 2 Corinthians\n                               vase, 1929, 2382, 6&quot;                                     10:1-18 | Meekness or\n                                                                                             Boldness?\n                               Chaussure De Running                                          Shopping Fairy Crochet\n                               Junior Asics Gt-1000 Gs                                       Pattern, crochet wings,\n                               Bleu/vert - Asics - 37                                        crochet shopping bags,\n                                                                                             crochet doll\n                               Luxardo, Maraschino                                           Essay Outlines Exles by\n                               Cherries, 14 Fl Oz :                                          Writing Center Workshops\n                               Grocery & Gourmet Food                                        The Outline\nFigure 19: An i.i.d. sample from small COMMONPOOL generated after applying the Basic filter\nstrategy.\n                                                        Basic\n                                                                  45", "md": "Implication in the classroom:\n\nstatus report templates - 12+ free word documents download | free, Powerpoint templates\n\nSay and mean \"We have to work it out\". The behaviour cannot co...\n\nCity 39 mm Quartz Astro ATA 3050 INSERTION TOOL, 16/20 GA\n\n1006: Rookwood pink mat vase, 1929, 2382, 6&quot;\n\nChaussure De Running Junior Asics Gt-1000 Gs Bleu/vert - Asics - 37\n\nLuxardo, Maraschino Cherries, 14 Fl Oz : Grocery & Gourmet Food\n\nFigure 19: An i.i.d. sample from small COMMONPOOL generated after applying the Basic filter strategy.\n\nBasic 45", "images": [{"name": "page-45-3.jpg", "height": 66, "width": 66, "x": 131, "y": 432}, {"name": "page-45-2.jpg", "height": 53, "width": 66, "x": 318, "y": 439}, {"name": "page-45-0.jpg", "height": 52, "width": 66, "x": 131, "y": 177}, {"name": "page-45-6.jpg", "height": 38, "width": 66, "x": 318, "y": 357}, {"name": "page-45-1.jpg", "height": 66, "width": 66, "x": 131, "y": 522}, {"name": "page-45-5.jpg", "height": 66, "width": 66, "x": 131, "y": 342}, {"name": "page-45-4.jpg", "height": 66, "width": 66, "x": 131, "y": 252}, {"name": "page-45-9.jpg", "height": 86, "width": 66, "x": 318, "y": 512}, {"name": "page-45-8.jpg", "height": 50, "width": 66, "x": 318, "y": 178}, {"name": "page-45-7.jpg", "height": 66, "width": 66, "x": 318, "y": 252}], "items": [{"type": "text", "value": "Implication in the classroom:\n\nstatus report templates - 12+ free word documents download | free, Powerpoint templates\n\nSay and mean \"We have to work it out\". The behaviour cannot co...\n\nCity 39 mm Quartz Astro ATA 3050 INSERTION TOOL, 16/20 GA\n\n1006: Rookwood pink mat vase, 1929, 2382, 6&quot;\n\nChaussure De Running Junior Asics Gt-1000 Gs Bleu/vert - Asics - 37\n\nLuxardo, Maraschino Cherries, 14 Fl Oz : Grocery & Gourmet Food\n\nFigure 19: An i.i.d. sample from small COMMONPOOL generated after applying the Basic filter strategy.\n\nBasic 45", "md": "Implication in the classroom:\n\nstatus report templates - 12+ free word documents download | free, Powerpoint templates\n\nSay and mean \"We have to work it out\". The behaviour cannot co...\n\nCity 39 mm Quartz Astro ATA 3050 INSERTION TOOL, 16/20 GA\n\n1006: Rookwood pink mat vase, 1929, 2382, 6&quot;\n\nChaussure De Running Junior Asics Gt-1000 Gs Bleu/vert - Asics - 37\n\nLuxardo, Maraschino Cherries, 14 Fl Oz : Grocery & Gourmet Food\n\nFigure 19: An i.i.d. sample from small COMMONPOOL generated after applying the Basic filter strategy.\n\nBasic 45"}]}, {"page": 46, "text": "       DIVEREELEMEYT\n                             Sacred Geometry Egg Of\n                             Life, Sacred Geometry                                       The Martian Monster And\n                             Symbols, Golden Ratio,                                      Other Stories (The EC\n                             Flower Of Life, Wicca,                                      Comics Library) ()\n                             Magick, Tattoos, Geometric\n                             Nature, Geometric Mandala\n                                                                                         Mesmerizing Black, Silver &\n                                                                                         Pink Handmade Modern\n                             Porsche Cayman S                                            Metal Wall Art Sculpture -\n                                                                                         Metallic One of a Kind\n                                                                                         Abstract Painting - OOAK\n                                                                                         546 by Jon Allen\n                             ALPINESTARS Radar Short                                     Under Armour Heatgear\n                             navy blue                                                   Gotta Have It Shorty -\n                                                                                         Women's at Foot Locker\n                             Tripod Delphin TPX3 Silver                                  Football Manager 2016\n                    Foiyilk  Mett.jpg auf                                                Profitable forex trading\n                             www.funpot.net                                              systems\nFigure 20: An i.i.d. sample from small COMMONPOOL generated after applying the CLIP score\n(L/14 30%)\n                                                         strategy.\n                                               Clip fi        lter\n                                                             46", "md": "DIVEREELEMEYT\n\nSacred Geometry Egg Of Life, Sacred Geometry The Martian Monster And Symbols, Golden Ratio, The EC Flower Of Life, Wicca, Magick, Tattoos, Geometric Nature, Geometric Mandala\n\nMesmerizing Black, Silver & Pink Handmade Modern Porsche Cayman S Metal Wall Art Sculpture - Metallic One of a Kind Abstract Painting - OOAK 546 by Jon Allen\n\nALPINESTARS Radar Short navy blue\n\nUnder Armour Heatgear Gotta Have It Shorty - Women's at Foot Locker\n\nTripod Delphin TPX3 Silver Football Manager 2016\n\nProfitable forex trading systems\n\nFigure 20: An i.i.d. sample from small COMMONPOOL generated after applying the CLIP score (L/14 30%) strategy.\n\nwww.funpot.net", "images": [{"name": "page-46-1.jpg", "height": 95, "width": 66, "x": 130, "y": 340}, {"name": "page-46-0.jpg", "height": 42, "width": 66, "x": 308, "y": 457}, {"name": "page-46-7.jpg", "height": 47, "width": 66, "x": 130, "y": 275}, {"name": "page-46-3.jpg", "height": 94, "width": 66, "x": 308, "y": 169}, {"name": "page-46-4.jpg", "height": 53, "width": 66, "x": 308, "y": 272}, {"name": "page-46-5.jpg", "height": 33, "width": 66, "x": 130, "y": 551}, {"name": "page-46-2.jpg", "height": 41, "width": 66, "x": 308, "y": 547}, {"name": "page-46-8.jpg", "height": 66, "width": 66, "x": 130, "y": 444}, {"name": "page-46-9.jpg", "height": 100, "width": 66, "x": 130, "y": 166}, {"name": "page-46-6.jpg", "height": 66, "width": 66, "x": 308, "y": 355}], "items": [{"type": "text", "value": "DIVEREELEMEYT\n\nSacred Geometry Egg Of Life, Sacred Geometry The Martian Monster And Symbols, Golden Ratio, The EC Flower Of Life, Wicca, Magick, Tattoos, Geometric Nature, Geometric Mandala\n\nMesmerizing Black, Silver & Pink Handmade Modern Porsche Cayman S Metal Wall Art Sculpture - Metallic One of a Kind Abstract Painting - OOAK 546 by Jon Allen\n\nALPINESTARS Radar Short navy blue\n\nUnder Armour Heatgear Gotta Have It Shorty - Women's at Foot Locker\n\nTripod Delphin TPX3 Silver Football Manager 2016\n\nProfitable forex trading systems\n\nFigure 20: An i.i.d. sample from small COMMONPOOL generated after applying the CLIP score (L/14 30%) strategy.\n\nwww.funpot.net", "md": "DIVEREELEMEYT\n\nSacred Geometry Egg Of Life, Sacred Geometry The Martian Monster And Symbols, Golden Ratio, The EC Flower Of Life, Wicca, Magick, Tattoos, Geometric Nature, Geometric Mandala\n\nMesmerizing Black, Silver & Pink Handmade Modern Porsche Cayman S Metal Wall Art Sculpture - Metallic One of a Kind Abstract Painting - OOAK 546 by Jon Allen\n\nALPINESTARS Radar Short navy blue\n\nUnder Armour Heatgear Gotta Have It Shorty - Women's at Foot Locker\n\nTripod Delphin TPX3 Silver Football Manager 2016\n\nProfitable forex trading systems\n\nFigure 20: An i.i.d. sample from small COMMONPOOL generated after applying the CLIP score (L/14 30%) strategy.\n\nwww.funpot.net"}]}, {"page": 47, "text": "P.1      Filtering track\nBasic filtering. For language detection, we use Fasttext 0.92, version lid.176, and cld3 - library gcld3 3.0.13.\nWe count the number of words in each caption by splitting using whitespaces.\nCLIP thresholds. We use OpenAI pretrained CLIP ViT-B/32 and ViT-L/14 models [111] to compute the\ncosine similarity text and image tower outputs as the CLIP scores. On the small and medium pools, we also\nexperiment with baselines that filter out samples in the top few percentiles of CLIP scores. Specifically, we\ntry baselines that use samples with top {1,2,5}-30% CLIP scores (ViT-B/32 model), and the performance is\nsightly better on the small pool (at most 0.5 gain of averaged accuracy) while slightly worse on the medium\npool (0.4-0.8 loss of averaged accuracy). In Table 16, we show how the CLIP score thresholds relate to the\nfraction of the pool retained by the filter.\nText-based filtering. Each synset is represented by a synset offset that can be used to retrieve the synset\nfrom WordNet. In order to verify if a caption has a word corresponding to a synset from our set we iterate over\nevery word and retrieve the synsets that this word can describe (using nltk.corpus WordNet). Following that, we\nretrieve the most likely lemma representing that synset, fi                  nd its synset offset, and check if the number is part of\nthe IN21K or IN1K sets.7\nText-based sampling. This baseline uses text only to filter labels which mention concepts (synsets) appearing\nin IN21K, and applies a temperature parameter to control how equally-represented different concepts are in\nthe dataset. For synset j, let Nj be the number of examples containing words matched to that synset, where as\nbefore for each word we only match the most likely synset. Furthermore, for image-text pair i let Ti be the set of\nsynset matched to the caption.\nThe probability of sampling example i is proportional to either                         1                j      (average synset score in the\n                                                                                      |Ti|      j\u2208Ti N \u03b1\u22121\ndata point) or maxj\u2208Ti N \u03b1\u22121       j      (maximum synset score in the data point), where \u03b1 is a \u201ctemperature\u201d parameter\ncontrolling the flatness of the distribution. We sample examples with replacement but discard any example\nrepeated more than 100 times.\nImage-based filtering. We now provide a detailed description of the Image-based filtering procedure. First,\nsince the core of the procedure concerns only image content, we begin with basic text-bsaed filtering: we remove\nfrom the pool only all examples with non-English captions (as determined by fasttext), and all examples whose\ncaptions have less than two words or less than six characters.\nNext, we use clustering of image embeddings to select a subset of examples whose image content is related to a\nclean training set of interest. Let e1, . . . , eM denote the CLIP image embeddings of the remaining examples in\nthe pool. We cluster these embeddings into K = 105 clusters using Faiss with 20 iterations, and let c1, . . . , cK\ndenote the resulting cluster centers. Due to memory constraints, for the large and xlarge pools, we perform\nthe clustering on a random subset of about 160M examples (that pass the basic text-based filtering). For an\nembedding vector v, let\n                                                           I(v) = arg max  i\u2264K \u27e8v, ci\u27e9\ndenote the index of the cluster center nearest to v as measured by inner product. Let f1, . . . , fN denote the CLIP\nimage embeddings of a clean supervised training set (we experiment with either ImageNet 1K or ImageNet\n21K), and let\n                                                         S = {I(fi) | 1 \u2264         i \u2264   N}\nbe the set of cluster indices who are nearest neighbors to some clean training set image. We then keep only\nimages in the pool whose nearest cluster center is in S. That is, out of the M examples passing the text-based\nfiltering, the output subset keeps the examples with indices\n                                                        {1 \u2264    j \u2264   M | I(ej) \u2208        S}.\nImage-based sampling. In addition to filtering methods, we experiment with cluster-based sampling methods.\nFirst, we compute the score of i-th cluster si as the number of ImageNet data assigned to this cluster. Then,                             s\u03b1\nfor parameter \u03b1 > 0 we define a distribution over the pool by sampling cluster i with probability                                         jis\u03b1\n                                                                                                                                             j and\nuniformly sampling an example for the cluster, rejecting any example repeated more than 100 times. We try\n5 different \u03b1, i.e., {0, 0.2, 0.5, 1.0, 2.0}, and the best average accuracy is obtained when \u03b1 = 0.2, while the\nperformance is still worse than the image-based filtering on the small and medium pool. We therefore do not\ninclude this line of baselines in the experiments of large pool.\n     7For the ImageNet 21K synsets, we have used the list in https://storage.googleapis.com/bit_\nmodels/imagenet21k_wordnet_ids.txt\n                                                                         47", "md": "# Filtering Track\n\n# Filtering track\n\nBasic filtering. For language detection, we use Fasttext 0.92, version lid.176, and cld3 - library gcld3 3.0.13. We count the number of words in each caption by splitting using whitespaces.\n\nCLIP thresholds. We use OpenAI pretrained CLIP ViT-B/32 and ViT-L/14 models to compute the cosine similarity text and image tower outputs as the CLIP scores. On the small and medium pools, we also experiment with baselines that filter out samples in the top few percentiles of CLIP scores. Specifically, we try baselines that use samples with top {1,2,5}-30% CLIP scores (ViT-B/32 model), and the performance is slightly better on the small pool (at most 0.5 gain of averaged accuracy) while slightly worse on the medium pool (0.4-0.8 loss of averaged accuracy). In Table 16, we show how the CLIP score thresholds relate to the fraction of the pool retained by the filter.\n\nText-based filtering. Each synset is represented by a synset offset that can be used to retrieve the synset from WordNet. In order to verify if a caption has a word corresponding to a synset from our set we iterate over every word and retrieve the synsets that this word can describe (using nltk.corpus WordNet). Following that, we retrieve the most likely lemma representing that synset, find its synset offset, and check if the number is part of the IN21K or IN1K sets.\n\nText-based sampling. This baseline uses text only to filter labels which mention concepts (synsets) appearing in IN21K, and applies a temperature parameter to control how equally-represented different concepts are in the dataset. For synset j, let Nj be the number of examples containing words matched to that synset, where as before for each word we only match the most likely synset. Furthermore, for image-text pair i let Ti be the set of synset matched to the caption.\n\nThe probability of sampling example i is proportional to either $$\\frac{1}{|T_i|} \\sum_{j \\in T_i} N_j^{\\alpha-1}$$ (average synset score in the data point) or $$\\max_{j \\in T_i} N_j^{\\alpha-1}$$ (maximum synset score in the data point), where \u03b1 is a \u201ctemperature\u201d parameter controlling the flatness of the distribution. We sample examples with replacement but discard any example repeated more than 100 times.\n\nImage-based filtering. We now provide a detailed description of the Image-based filtering procedure. First, since the core of the procedure concerns only image content, we begin with basic text-based filtering: we remove from the pool only all examples with non-English captions (as determined by fasttext), and all examples whose captions have less than two words or less than six characters.\n\nNext, we use clustering of image embeddings to select a subset of examples whose image content is related to a clean training set of interest. Let e1, . . . , eM denote the CLIP image embeddings of the remaining examples in the pool. We cluster these embeddings into K = 105 clusters using Faiss with 20 iterations, and let c1, . . . , cK denote the resulting cluster centers. Due to memory constraints, for the large and xlarge pools, we perform the clustering on a random subset of about 160M examples (that pass the basic text-based filtering). For an embedding vector v, let $$I(v) = \\arg \\max_{i \\leq K} \\langle v, c_i \\rangle$$ denote the index of the cluster center nearest to v as measured by inner product. Let f1, . . . , fN denote the CLIP image embeddings of a clean supervised training set (we experiment with either ImageNet 1K or ImageNet 21K), and let $$S = \\{I(f_i) | 1 \\leq i \\leq N\\}$$ be the set of cluster indices who are nearest neighbors to some clean training set image. We then keep only images in the pool whose nearest cluster center is in S. That is, out of the M examples passing the text-based filtering, the output subset keeps the examples with indices $$\\{1 \\leq j \\leq M | I(e_j) \\in S\\}$$.\n\nImage-based sampling. In addition to filtering methods, we experiment with cluster-based sampling methods. First, we compute the score of i-th cluster si as the number of ImageNet data assigned to this cluster. Then, for parameter \u03b1 > 0 we define a distribution over the pool by sampling cluster i with probability $$\\frac{s_i^\\alpha}{\\sum_j s_j^\\alpha}$$ and uniformly sampling an example for the cluster, rejecting any example repeated more than 100 times. We try 5 different \u03b1, i.e., {0, 0.2, 0.5, 1.0, 2.0}, and the best average accuracy is obtained when \u03b1 = 0.2, while the performance is still worse than the image-based filtering on the small and medium pool. We therefore do not include this line of baselines in the experiments of large pool.\n\nFor the ImageNet 21K synsets, we have used the list in https://storage.googleapis.com/bit_models/imagenet21k_wordnet_ids.txt", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Filtering Track", "md": "# Filtering Track"}, {"type": "heading", "lvl": 1, "value": "Filtering track", "md": "# Filtering track"}, {"type": "text", "value": "Basic filtering. For language detection, we use Fasttext 0.92, version lid.176, and cld3 - library gcld3 3.0.13. We count the number of words in each caption by splitting using whitespaces.\n\nCLIP thresholds. We use OpenAI pretrained CLIP ViT-B/32 and ViT-L/14 models to compute the cosine similarity text and image tower outputs as the CLIP scores. On the small and medium pools, we also experiment with baselines that filter out samples in the top few percentiles of CLIP scores. Specifically, we try baselines that use samples with top {1,2,5}-30% CLIP scores (ViT-B/32 model), and the performance is slightly better on the small pool (at most 0.5 gain of averaged accuracy) while slightly worse on the medium pool (0.4-0.8 loss of averaged accuracy). In Table 16, we show how the CLIP score thresholds relate to the fraction of the pool retained by the filter.\n\nText-based filtering. Each synset is represented by a synset offset that can be used to retrieve the synset from WordNet. In order to verify if a caption has a word corresponding to a synset from our set we iterate over every word and retrieve the synsets that this word can describe (using nltk.corpus WordNet). Following that, we retrieve the most likely lemma representing that synset, find its synset offset, and check if the number is part of the IN21K or IN1K sets.\n\nText-based sampling. This baseline uses text only to filter labels which mention concepts (synsets) appearing in IN21K, and applies a temperature parameter to control how equally-represented different concepts are in the dataset. For synset j, let Nj be the number of examples containing words matched to that synset, where as before for each word we only match the most likely synset. Furthermore, for image-text pair i let Ti be the set of synset matched to the caption.\n\nThe probability of sampling example i is proportional to either $$\\frac{1}{|T_i|} \\sum_{j \\in T_i} N_j^{\\alpha-1}$$ (average synset score in the data point) or $$\\max_{j \\in T_i} N_j^{\\alpha-1}$$ (maximum synset score in the data point), where \u03b1 is a \u201ctemperature\u201d parameter controlling the flatness of the distribution. We sample examples with replacement but discard any example repeated more than 100 times.\n\nImage-based filtering. We now provide a detailed description of the Image-based filtering procedure. First, since the core of the procedure concerns only image content, we begin with basic text-based filtering: we remove from the pool only all examples with non-English captions (as determined by fasttext), and all examples whose captions have less than two words or less than six characters.\n\nNext, we use clustering of image embeddings to select a subset of examples whose image content is related to a clean training set of interest. Let e1, . . . , eM denote the CLIP image embeddings of the remaining examples in the pool. We cluster these embeddings into K = 105 clusters using Faiss with 20 iterations, and let c1, . . . , cK denote the resulting cluster centers. Due to memory constraints, for the large and xlarge pools, we perform the clustering on a random subset of about 160M examples (that pass the basic text-based filtering). For an embedding vector v, let $$I(v) = \\arg \\max_{i \\leq K} \\langle v, c_i \\rangle$$ denote the index of the cluster center nearest to v as measured by inner product. Let f1, . . . , fN denote the CLIP image embeddings of a clean supervised training set (we experiment with either ImageNet 1K or ImageNet 21K), and let $$S = \\{I(f_i) | 1 \\leq i \\leq N\\}$$ be the set of cluster indices who are nearest neighbors to some clean training set image. We then keep only images in the pool whose nearest cluster center is in S. That is, out of the M examples passing the text-based filtering, the output subset keeps the examples with indices $$\\{1 \\leq j \\leq M | I(e_j) \\in S\\}$$.\n\nImage-based sampling. In addition to filtering methods, we experiment with cluster-based sampling methods. First, we compute the score of i-th cluster si as the number of ImageNet data assigned to this cluster. Then, for parameter \u03b1 > 0 we define a distribution over the pool by sampling cluster i with probability $$\\frac{s_i^\\alpha}{\\sum_j s_j^\\alpha}$$ and uniformly sampling an example for the cluster, rejecting any example repeated more than 100 times. We try 5 different \u03b1, i.e., {0, 0.2, 0.5, 1.0, 2.0}, and the best average accuracy is obtained when \u03b1 = 0.2, while the performance is still worse than the image-based filtering on the small and medium pool. We therefore do not include this line of baselines in the experiments of large pool.\n\nFor the ImageNet 21K synsets, we have used the list in https://storage.googleapis.com/bit_models/imagenet21k_wordnet_ids.txt", "md": "Basic filtering. For language detection, we use Fasttext 0.92, version lid.176, and cld3 - library gcld3 3.0.13. We count the number of words in each caption by splitting using whitespaces.\n\nCLIP thresholds. We use OpenAI pretrained CLIP ViT-B/32 and ViT-L/14 models to compute the cosine similarity text and image tower outputs as the CLIP scores. On the small and medium pools, we also experiment with baselines that filter out samples in the top few percentiles of CLIP scores. Specifically, we try baselines that use samples with top {1,2,5}-30% CLIP scores (ViT-B/32 model), and the performance is slightly better on the small pool (at most 0.5 gain of averaged accuracy) while slightly worse on the medium pool (0.4-0.8 loss of averaged accuracy). In Table 16, we show how the CLIP score thresholds relate to the fraction of the pool retained by the filter.\n\nText-based filtering. Each synset is represented by a synset offset that can be used to retrieve the synset from WordNet. In order to verify if a caption has a word corresponding to a synset from our set we iterate over every word and retrieve the synsets that this word can describe (using nltk.corpus WordNet). Following that, we retrieve the most likely lemma representing that synset, find its synset offset, and check if the number is part of the IN21K or IN1K sets.\n\nText-based sampling. This baseline uses text only to filter labels which mention concepts (synsets) appearing in IN21K, and applies a temperature parameter to control how equally-represented different concepts are in the dataset. For synset j, let Nj be the number of examples containing words matched to that synset, where as before for each word we only match the most likely synset. Furthermore, for image-text pair i let Ti be the set of synset matched to the caption.\n\nThe probability of sampling example i is proportional to either $$\\frac{1}{|T_i|} \\sum_{j \\in T_i} N_j^{\\alpha-1}$$ (average synset score in the data point) or $$\\max_{j \\in T_i} N_j^{\\alpha-1}$$ (maximum synset score in the data point), where \u03b1 is a \u201ctemperature\u201d parameter controlling the flatness of the distribution. We sample examples with replacement but discard any example repeated more than 100 times.\n\nImage-based filtering. We now provide a detailed description of the Image-based filtering procedure. First, since the core of the procedure concerns only image content, we begin with basic text-based filtering: we remove from the pool only all examples with non-English captions (as determined by fasttext), and all examples whose captions have less than two words or less than six characters.\n\nNext, we use clustering of image embeddings to select a subset of examples whose image content is related to a clean training set of interest. Let e1, . . . , eM denote the CLIP image embeddings of the remaining examples in the pool. We cluster these embeddings into K = 105 clusters using Faiss with 20 iterations, and let c1, . . . , cK denote the resulting cluster centers. Due to memory constraints, for the large and xlarge pools, we perform the clustering on a random subset of about 160M examples (that pass the basic text-based filtering). For an embedding vector v, let $$I(v) = \\arg \\max_{i \\leq K} \\langle v, c_i \\rangle$$ denote the index of the cluster center nearest to v as measured by inner product. Let f1, . . . , fN denote the CLIP image embeddings of a clean supervised training set (we experiment with either ImageNet 1K or ImageNet 21K), and let $$S = \\{I(f_i) | 1 \\leq i \\leq N\\}$$ be the set of cluster indices who are nearest neighbors to some clean training set image. We then keep only images in the pool whose nearest cluster center is in S. That is, out of the M examples passing the text-based filtering, the output subset keeps the examples with indices $$\\{1 \\leq j \\leq M | I(e_j) \\in S\\}$$.\n\nImage-based sampling. In addition to filtering methods, we experiment with cluster-based sampling methods. First, we compute the score of i-th cluster si as the number of ImageNet data assigned to this cluster. Then, for parameter \u03b1 > 0 we define a distribution over the pool by sampling cluster i with probability $$\\frac{s_i^\\alpha}{\\sum_j s_j^\\alpha}$$ and uniformly sampling an example for the cluster, rejecting any example repeated more than 100 times. We try 5 different \u03b1, i.e., {0, 0.2, 0.5, 1.0, 2.0}, and the best average accuracy is obtained when \u03b1 = 0.2, while the performance is still worse than the image-based filtering on the small and medium pool. We therefore do not include this line of baselines in the experiments of large pool.\n\nFor the ImageNet 21K synsets, we have used the list in https://storage.googleapis.com/bit_models/imagenet21k_wordnet_ids.txt"}]}, {"page": 48, "text": "Table 16: CLIP threshold filtering configurations. \u201cFraction\u201d denotes the size of the filtered subset\nrelative to the pool.\n                                 CLIP model En. filtering Threshold Fraction\n                                 ViT-B/32           \u2717         0.384       1%\n                                 ViT-B/32           \u2717         0.358       3%\n                                 ViT-B/32           \u2713         0.300      10.2%\n                                 ViT-B/32           \u2717         0.325       10%\n                                 ViT-B/32           \u2713          0.28      7.4%\n                                 ViT-B/32           \u2717         0.300       20%\n                                 ViT-B/32           \u2717         0.281       30%\n                                 ViT-B/32           \u2717         0.263       40%\n                                 ViT-B/32           \u2717         0.247       50%\n                                 ViT-B/32           \u2717         0.215       75%\n                                 ViT-B/32           \u2717         0.193       90%\n                                 ViT-L/14           \u2717         0.364       1%\n                                 ViT-L/14           \u2717         0.334       3%\n                                 ViT-L/14           \u2713         0.300      5.4%\n                                 ViT-L/14           \u2717         0.295       10%\n                                 ViT-L/14           \u2713         0.280      3.3%\n                                 ViT-L/14           \u2717         0.266       20%\n                                 ViT-L/14           \u2717         0.243       30%\n                                 ViT-L/14           \u2717         0.222       40%\n                                 ViT-L/14           \u2717         0.203       50%\n                                 ViT-L/14           \u2717         0.160       75%\n                                 ViT-L/14           \u2717         0.129       90%\nImageNet distance filtering. We rank the samples in the pool by the minimum embedding distance (1 minus\ncosine similarity) between its image and the ImageNet images; both embeddings are obtained from OpenAI\npretrained CLIP ViT-L/14 model [111]. Then we select top images by different fractions as in image-based\nfiltering methods.\nP.2    BYOD track\nWe experiment with the following data sources:\n\u2022 CC12M [24]: images and HTML alt-text crawled and filtered from web pages.\n\u2022 YFCC15M: this is the 15M subset of the YFCC100M dataset [140] that Radford et al. [111] used for dataset\n  ablation in their CLIP paper.\n\u2022 RedCaps [38]: 12M images and corresponding captions were crawled from 350 manually curated subreddits\n  between 2008 and 2020.\n\u2022 Shutterstock: 106M images and captions were obtained from the Shutterstock website in 2021 [101]. We\n  use the \u201cphotos\u201d subset of this dataset, with 58M samples, which we found performed best, unless specified\n  otherwise.\n\u2022 WIT [136]: Image-text pairs from Wikipedia pages. We use the attribution fields as captions, which we found\n  performed best.\n\u2022 COYO [20]: A collection of 700M image-text pairs from Common Crawl.\n\u2022 LAION-2B [129]: A 2.32 billion english subset of LAION-5B.\n\u2022 LAION-COCO: A dataset with 600M images from LAION-5B and synthetic captions.8\n\u2022 LAION-A: According to laion.ai, LAION-A is a 900M subset of LAION-2B [129] with the aesthetic filtering\n  procedure used in LAION-aesthetic9 and pHash deduplication [72].\nIn Table 17, we use some heuristics to measure the quality of some external data sources. First, following Nguyen\net al. [101], we train a CLIP model on a 5M random subset from each source, and evaluate the performance of\nthe resulting models on ImageNet and ImageNet-derived distributions \u2014 ImageNet-V2 [121], ImageNet-R [64],\n    8https://laion.ai/blog/laion-coco/\n    9https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md\n                                                       48", "md": "|CLIP model|En. filtering|Threshold|Fraction|\n|---|---|---|---|\n|ViT-B/32|\u2717|0.384|1%|\n|ViT-B/32|\u2717|0.358|3%|\n|ViT-B/32|\u2713|0.300|10.2%|\n|ViT-B/32|\u2717|0.325|10%|\n|ViT-B/32|\u2713|0.28|7.4%|\n|ViT-B/32|\u2717|0.300|20%|\n|ViT-B/32|\u2717|0.281|30%|\n|ViT-B/32|\u2717|0.263|40%|\n|ViT-B/32|\u2717|0.247|50%|\n|ViT-B/32|\u2717|0.215|75%|\n|ViT-B/32|\u2717|0.193|90%|\n|ViT-L/14|\u2717|0.364|1%|\n|ViT-L/14|\u2717|0.334|3%|\n|ViT-L/14|\u2713|0.300|5.4%|\n|ViT-L/14|\u2717|0.295|10%|\n|ViT-L/14|\u2713|0.280|3.3%|\n|ViT-L/14|\u2717|0.266|20%|\n|ViT-L/14|\u2717|0.243|30%|\n|ViT-L/14|\u2717|0.222|40%|\n|ViT-L/14|\u2717|0.203|50%|\n|ViT-L/14|\u2717|0.160|75%|\n|ViT-L/14|\u2717|0.129|90%|\n\nImageNet distance filtering. We rank the samples in the pool by the minimum embedding distance (1 minus cosine similarity) between its image and the ImageNet images; both embeddings are obtained from OpenAI pretrained CLIP ViT-L/14 model [111]. Then we select top images by different fractions as in image-based filtering methods.\n\n## BYOD track\n\nWe experiment with the following data sources:\n\n- CC12M [24]: images and HTML alt-text crawled and filtered from web pages.\n- YFCC15M: this is the 15M subset of the YFCC100M dataset [140] that Radford et al. [111] used for dataset ablation in their CLIP paper.\n- RedCaps [38]: 12M images and corresponding captions were crawled from 350 manually curated subreddits between 2008 and 2020.\n- Shutterstock: 106M images and captions were obtained from the Shutterstock website in 2021 [101]. We use the \u201cphotos\u201d subset of this dataset, with 58M samples, which we found performed best, unless specified otherwise.\n- WIT [136]: Image-text pairs from Wikipedia pages. We use the attribution fields as captions, which we found performed best.\n- COYO [20]: A collection of 700M image-text pairs from Common Crawl.\n- LAION-2B [129]: A 2.32 billion English subset of LAION-5B.\n- LAION-COCO: A dataset with 600M images from LAION-5B and synthetic captions.\n- LAION-A: According to laion.ai, LAION-A is a 900M subset of LAION-2B [129] with the aesthetic filtering procedure used in LAION-aesthetic and pHash deduplication.\n\n|Data Source|Quality Heuristics|\n|---|---|\n|CC12M|...|\n|YFCC15M|...|\n|RedCaps|...|\n|Shutterstock|...|\n|WIT|...|\n|COYO|...|\n|LAION-2B|...|\n|LAION-COCO|...|\n|LAION-A|...|", "images": [], "items": [{"type": "table", "rows": [["CLIP model", "En. filtering", "Threshold", "Fraction"], ["ViT-B/32", "\u2717", "0.384", "1%"], ["ViT-B/32", "\u2717", "0.358", "3%"], ["ViT-B/32", "\u2713", "0.300", "10.2%"], ["ViT-B/32", "\u2717", "0.325", "10%"], ["ViT-B/32", "\u2713", "0.28", "7.4%"], ["ViT-B/32", "\u2717", "0.300", "20%"], ["ViT-B/32", "\u2717", "0.281", "30%"], ["ViT-B/32", "\u2717", "0.263", "40%"], ["ViT-B/32", "\u2717", "0.247", "50%"], ["ViT-B/32", "\u2717", "0.215", "75%"], ["ViT-B/32", "\u2717", "0.193", "90%"], ["ViT-L/14", "\u2717", "0.364", "1%"], ["ViT-L/14", "\u2717", "0.334", "3%"], ["ViT-L/14", "\u2713", "0.300", "5.4%"], ["ViT-L/14", "\u2717", "0.295", "10%"], ["ViT-L/14", "\u2713", "0.280", "3.3%"], ["ViT-L/14", "\u2717", "0.266", "20%"], ["ViT-L/14", "\u2717", "0.243", "30%"], ["ViT-L/14", "\u2717", "0.222", "40%"], ["ViT-L/14", "\u2717", "0.203", "50%"], ["ViT-L/14", "\u2717", "0.160", "75%"], ["ViT-L/14", "\u2717", "0.129", "90%"]], "md": "|CLIP model|En. filtering|Threshold|Fraction|\n|---|---|---|---|\n|ViT-B/32|\u2717|0.384|1%|\n|ViT-B/32|\u2717|0.358|3%|\n|ViT-B/32|\u2713|0.300|10.2%|\n|ViT-B/32|\u2717|0.325|10%|\n|ViT-B/32|\u2713|0.28|7.4%|\n|ViT-B/32|\u2717|0.300|20%|\n|ViT-B/32|\u2717|0.281|30%|\n|ViT-B/32|\u2717|0.263|40%|\n|ViT-B/32|\u2717|0.247|50%|\n|ViT-B/32|\u2717|0.215|75%|\n|ViT-B/32|\u2717|0.193|90%|\n|ViT-L/14|\u2717|0.364|1%|\n|ViT-L/14|\u2717|0.334|3%|\n|ViT-L/14|\u2713|0.300|5.4%|\n|ViT-L/14|\u2717|0.295|10%|\n|ViT-L/14|\u2713|0.280|3.3%|\n|ViT-L/14|\u2717|0.266|20%|\n|ViT-L/14|\u2717|0.243|30%|\n|ViT-L/14|\u2717|0.222|40%|\n|ViT-L/14|\u2717|0.203|50%|\n|ViT-L/14|\u2717|0.160|75%|\n|ViT-L/14|\u2717|0.129|90%|", "isPerfectTable": true, "csv": "\"CLIP model\",\"En. filtering\",\"Threshold\",\"Fraction\"\n\"ViT-B/32\",\"\u2717\",\"0.384\",\"1%\"\n\"ViT-B/32\",\"\u2717\",\"0.358\",\"3%\"\n\"ViT-B/32\",\"\u2713\",\"0.300\",\"10.2%\"\n\"ViT-B/32\",\"\u2717\",\"0.325\",\"10%\"\n\"ViT-B/32\",\"\u2713\",\"0.28\",\"7.4%\"\n\"ViT-B/32\",\"\u2717\",\"0.300\",\"20%\"\n\"ViT-B/32\",\"\u2717\",\"0.281\",\"30%\"\n\"ViT-B/32\",\"\u2717\",\"0.263\",\"40%\"\n\"ViT-B/32\",\"\u2717\",\"0.247\",\"50%\"\n\"ViT-B/32\",\"\u2717\",\"0.215\",\"75%\"\n\"ViT-B/32\",\"\u2717\",\"0.193\",\"90%\"\n\"ViT-L/14\",\"\u2717\",\"0.364\",\"1%\"\n\"ViT-L/14\",\"\u2717\",\"0.334\",\"3%\"\n\"ViT-L/14\",\"\u2713\",\"0.300\",\"5.4%\"\n\"ViT-L/14\",\"\u2717\",\"0.295\",\"10%\"\n\"ViT-L/14\",\"\u2713\",\"0.280\",\"3.3%\"\n\"ViT-L/14\",\"\u2717\",\"0.266\",\"20%\"\n\"ViT-L/14\",\"\u2717\",\"0.243\",\"30%\"\n\"ViT-L/14\",\"\u2717\",\"0.222\",\"40%\"\n\"ViT-L/14\",\"\u2717\",\"0.203\",\"50%\"\n\"ViT-L/14\",\"\u2717\",\"0.160\",\"75%\"\n\"ViT-L/14\",\"\u2717\",\"0.129\",\"90%\""}, {"type": "text", "value": "ImageNet distance filtering. We rank the samples in the pool by the minimum embedding distance (1 minus cosine similarity) between its image and the ImageNet images; both embeddings are obtained from OpenAI pretrained CLIP ViT-L/14 model [111]. Then we select top images by different fractions as in image-based filtering methods.", "md": "ImageNet distance filtering. We rank the samples in the pool by the minimum embedding distance (1 minus cosine similarity) between its image and the ImageNet images; both embeddings are obtained from OpenAI pretrained CLIP ViT-L/14 model [111]. Then we select top images by different fractions as in image-based filtering methods."}, {"type": "heading", "lvl": 2, "value": "BYOD track", "md": "## BYOD track"}, {"type": "text", "value": "We experiment with the following data sources:\n\n- CC12M [24]: images and HTML alt-text crawled and filtered from web pages.\n- YFCC15M: this is the 15M subset of the YFCC100M dataset [140] that Radford et al. [111] used for dataset ablation in their CLIP paper.\n- RedCaps [38]: 12M images and corresponding captions were crawled from 350 manually curated subreddits between 2008 and 2020.\n- Shutterstock: 106M images and captions were obtained from the Shutterstock website in 2021 [101]. We use the \u201cphotos\u201d subset of this dataset, with 58M samples, which we found performed best, unless specified otherwise.\n- WIT [136]: Image-text pairs from Wikipedia pages. We use the attribution fields as captions, which we found performed best.\n- COYO [20]: A collection of 700M image-text pairs from Common Crawl.\n- LAION-2B [129]: A 2.32 billion English subset of LAION-5B.\n- LAION-COCO: A dataset with 600M images from LAION-5B and synthetic captions.\n- LAION-A: According to laion.ai, LAION-A is a 900M subset of LAION-2B [129] with the aesthetic filtering procedure used in LAION-aesthetic and pHash deduplication.", "md": "We experiment with the following data sources:\n\n- CC12M [24]: images and HTML alt-text crawled and filtered from web pages.\n- YFCC15M: this is the 15M subset of the YFCC100M dataset [140] that Radford et al. [111] used for dataset ablation in their CLIP paper.\n- RedCaps [38]: 12M images and corresponding captions were crawled from 350 manually curated subreddits between 2008 and 2020.\n- Shutterstock: 106M images and captions were obtained from the Shutterstock website in 2021 [101]. We use the \u201cphotos\u201d subset of this dataset, with 58M samples, which we found performed best, unless specified otherwise.\n- WIT [136]: Image-text pairs from Wikipedia pages. We use the attribution fields as captions, which we found performed best.\n- COYO [20]: A collection of 700M image-text pairs from Common Crawl.\n- LAION-2B [129]: A 2.32 billion English subset of LAION-5B.\n- LAION-COCO: A dataset with 600M images from LAION-5B and synthetic captions.\n- LAION-A: According to laion.ai, LAION-A is a 900M subset of LAION-2B [129] with the aesthetic filtering procedure used in LAION-aesthetic and pHash deduplication."}, {"type": "table", "rows": [["Data Source", "Quality Heuristics"], ["CC12M", "..."], ["YFCC15M", "..."], ["RedCaps", "..."], ["Shutterstock", "..."], ["WIT", "..."], ["COYO", "..."], ["LAION-2B", "..."], ["LAION-COCO", "..."], ["LAION-A", "..."]], "md": "|Data Source|Quality Heuristics|\n|---|---|\n|CC12M|...|\n|YFCC15M|...|\n|RedCaps|...|\n|Shutterstock|...|\n|WIT|...|\n|COYO|...|\n|LAION-2B|...|\n|LAION-COCO|...|\n|LAION-A|...|", "isPerfectTable": true, "csv": "\"Data Source\",\"Quality Heuristics\"\n\"CC12M\",\"...\"\n\"YFCC15M\",\"...\"\n\"RedCaps\",\"...\"\n\"Shutterstock\",\"...\"\n\"WIT\",\"...\"\n\"COYO\",\"...\"\n\"LAION-2B\",\"...\"\n\"LAION-COCO\",\"...\"\n\"LAION-A\",\"...\""}]}, {"page": 49, "text": "                           Table 17: Measuring the quality of external data sources\n  Dataset        Dataset size  ImageNet acc.         Avg. accuracy         Avg. cos. sim. (B/32)   Avg. cos. sim. (L/14)\n                                                ImageNet and OOD sets\n  CC12M              10M            27.8                  34.0                     0.306                   0.268\n  YFCC15M            15M            22.6                  24.6                     0.262                   0.198\n  RedCaps            11M            26.8                  31.5                     0.281                   0.240\n  Shutterstock       15M            21.0                  28.3                     0.314                   0.273\n                       1.0 Dollar Street Performance            1.0     GeoDE Performance\n                      Worst-group top-5 accuracy               Worst-region accuracy\n                       0.8                                      0.8\n                       0.6                                      0.6\n                       0.4                                      0.4\n                       0.2                                      0.2\n                       0.00.00   0.25    0.50    0.75   1.00    0.00.00    0.25   0.50    0.75    1.00\n                               Average top-5 accuracy                      Average accuracy\n                                  Basic              Text-based        ImageNet ResNet-50\n                                  Image-based        CLIP score        Fine-tuned ResNet-50\n Figure 21: Comparison of average and worst-group scores for Dollar Street and GeoDE diversity\n datasets. On Dollar Street, our overall higher-performing models display a larger worst-group\n performance gap (corresponding to lower income households). GeoDE does not show this trend.\n ImageNet-Sketch [143] and ObjectNet [13]. Moreover, for each data source, we use OpenAI\u2019s pretrained CLIP\n ViT-B/32 and ViT-L/14 models to compute the cosine similarity between image and text embeddings of a data\n point, and obtain the average cosine similarity score for the whole dataset.\n P.2.1    Additional results\nWe present a series of results for the BYOD track in Table 18.\n Q     Fairness and biases\n To study the biases displayed by our models, we include two diversity-related datasets, Dollar Street [122]\n and GeoDE [114], in our evaluation suite, and perform further analysis on the face datasets FairFace [80] and\n UTKFace [159] with demographic labels, following Radford et al. [111].\n Q.1    Diversity\nWe break down model performance on the Dollar Street and GeoDE datasets in Figure 21. Dollar Street consists\n of images of household items taken in homes around the world, and represents a wide socioeconomic range\n that includes homes with no Internet access [122]. The objects belong to ImageNet categories, and the task is\n image classification. Standard ImageNet-trained models achieve monotonically increasing performance levels\n with higher household income levels [122]. Here we use the income-based subgroups defined in Rojas et al.\n [122], and find a similar bias as discovered in their paper. While our trained models show a smaller worst-group\n performance gap than an ImageNet-trained ResNet-50, they underperform a model fine-tuned on Dollar Street.\n Models with higher average accuracy show a larger worst-group gap, which future work should try to address.\n GeoDE consists of images of everyday items and objects, which again fall into ImageNet categories. The dataset\n represents six world regions equally, and primarily aims to promote geographic diversity of datasets [114].\n Both ImageNet models and our models show less bias under this distribution compared to Dollar Street, with a\n smaller worst-group accuracy gap. The trends show that performance across all regions improves steadily with\n increased scale, and the performance approaches that of a model fine-tuned on GeoDE. While we know that\n classifiers trained specifically on ImageNet can display geographic biases [114], these biases are not apparent\n in our GeoDE model evaluations. Future work is needed to investigate the extent to which our models have\n geographic biases not evaluated in GeoDE.\n                                                            49", "md": "|Dataset|Dataset size|ImageNet acc.|Avg. accuracy|Avg. cos. sim. (B/32)|Avg. cos. sim. (L/14)|\n|---|---|---|---|---|---|\n|CC12M|10M|27.8|34.0|0.306|0.268|\n|YFCC15M|15M|22.6|24.6|0.262|0.198|\n|RedCaps|11M|26.8|31.5|0.281|0.240|\n|Shutterstock|15M|21.0|28.3|0.314|0.273|\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{1.0 Dollar Street Performance} & 1.0 & \\text{GeoDE Performance} \\\\\n\\text{Worst-group top-5 accuracy} & \\text{Worst-region accuracy} \\\\\n0.8 & 0.8 \\\\\n0.6 & 0.6 \\\\\n0.4 & 0.4 \\\\\n0.2 & 0.2 \\\\\n0.00 & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 & 0.00 & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 \\\\\n\\text{Average top-5 accuracy} & \\text{Average accuracy} \\\\\n\\text{Basic} & \\text{Text-based} & \\text{ImageNet ResNet-50} \\\\\n\\text{Image-based} & \\text{CLIP score} & \\text{Fine-tuned ResNet-50} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 21: Comparison of average and worst-group scores for Dollar Street and GeoDE diversity datasets. On Dollar Street, our overall higher-performing models display a larger worst-group performance gap (corresponding to lower income households). GeoDE does not show this trend. ImageNet-Sketch [143] and ObjectNet [13]. Moreover, for each data source, we use OpenAI\u2019s pretrained CLIP ViT-B/32 and ViT-L/14 models to compute the cosine similarity between image and text embeddings of a data point, and obtain the average cosine similarity score for the whole dataset.\n\nP.2.1 Additional results\n\nWe present a series of results for the BYOD track in Table 18.\n\nQ Fairness and biases\n\nTo study the biases displayed by our models, we include two diversity-related datasets, Dollar Street [122] and GeoDE [114], in our evaluation suite, and perform further analysis on the face datasets FairFace [80] and UTKFace [159] with demographic labels, following Radford et al. [111].\n\nQ.1 Diversity\n\nWe break down model performance on the Dollar Street and GeoDE datasets in Figure 21. Dollar Street consists of images of household items taken in homes around the world, and represents a wide socioeconomic range that includes homes with no Internet access [122]. The objects belong to ImageNet categories, and the task is image classification. Standard ImageNet-trained models achieve monotonically increasing performance levels with higher household income levels [122]. Here we use the income-based subgroups defined in Rojas et al. [122], and find a similar bias as discovered in their paper. While our trained models show a smaller worst-group performance gap than an ImageNet-trained ResNet-50, they underperform a model fine-tuned on Dollar Street. Models with higher average accuracy show a larger worst-group gap, which future work should try to address.\n\nGeoDE consists of images of everyday items and objects, which again fall into ImageNet categories. The dataset represents six world regions equally, and primarily aims to promote geographic diversity of datasets [114]. Both ImageNet models and our models show less bias under this distribution compared to Dollar Street, with a smaller worst-group accuracy gap. The trends show that performance across all regions improves steadily with increased scale, and the performance approaches that of a model fine-tuned on GeoDE. While we know that classifiers trained specifically on ImageNet can display geographic biases [114], these biases are not apparent in our GeoDE model evaluations. Future work is needed to investigate the extent to which our models have geographic biases not evaluated in GeoDE.", "images": [], "items": [{"type": "table", "rows": [["Dataset", "Dataset size", "ImageNet acc.", "Avg. accuracy", "Avg. cos. sim. (B/32)", "Avg. cos. sim. (L/14)"], ["CC12M", "10M", "27.8", "34.0", "0.306", "0.268"], ["YFCC15M", "15M", "22.6", "24.6", "0.262", "0.198"], ["RedCaps", "11M", "26.8", "31.5", "0.281", "0.240"], ["Shutterstock", "15M", "21.0", "28.3", "0.314", "0.273"]], "md": "|Dataset|Dataset size|ImageNet acc.|Avg. accuracy|Avg. cos. sim. (B/32)|Avg. cos. sim. (L/14)|\n|---|---|---|---|---|---|\n|CC12M|10M|27.8|34.0|0.306|0.268|\n|YFCC15M|15M|22.6|24.6|0.262|0.198|\n|RedCaps|11M|26.8|31.5|0.281|0.240|\n|Shutterstock|15M|21.0|28.3|0.314|0.273|", "isPerfectTable": true, "csv": "\"Dataset\",\"Dataset size\",\"ImageNet acc.\",\"Avg. accuracy\",\"Avg. cos. sim. (B/32)\",\"Avg. cos. sim. (L/14)\"\n\"CC12M\",\"10M\",\"27.8\",\"34.0\",\"0.306\",\"0.268\"\n\"YFCC15M\",\"15M\",\"22.6\",\"24.6\",\"0.262\",\"0.198\"\n\"RedCaps\",\"11M\",\"26.8\",\"31.5\",\"0.281\",\"0.240\"\n\"Shutterstock\",\"15M\",\"21.0\",\"28.3\",\"0.314\",\"0.273\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{1.0 Dollar Street Performance} & 1.0 & \\text{GeoDE Performance} \\\\\n\\text{Worst-group top-5 accuracy} & \\text{Worst-region accuracy} \\\\\n0.8 & 0.8 \\\\\n0.6 & 0.6 \\\\\n0.4 & 0.4 \\\\\n0.2 & 0.2 \\\\\n0.00 & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 & 0.00 & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 \\\\\n\\text{Average top-5 accuracy} & \\text{Average accuracy} \\\\\n\\text{Basic} & \\text{Text-based} & \\text{ImageNet ResNet-50} \\\\\n\\text{Image-based} & \\text{CLIP score} & \\text{Fine-tuned ResNet-50} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 21: Comparison of average and worst-group scores for Dollar Street and GeoDE diversity datasets. On Dollar Street, our overall higher-performing models display a larger worst-group performance gap (corresponding to lower income households). GeoDE does not show this trend. ImageNet-Sketch [143] and ObjectNet [13]. Moreover, for each data source, we use OpenAI\u2019s pretrained CLIP ViT-B/32 and ViT-L/14 models to compute the cosine similarity between image and text embeddings of a data point, and obtain the average cosine similarity score for the whole dataset.\n\nP.2.1 Additional results\n\nWe present a series of results for the BYOD track in Table 18.\n\nQ Fairness and biases\n\nTo study the biases displayed by our models, we include two diversity-related datasets, Dollar Street [122] and GeoDE [114], in our evaluation suite, and perform further analysis on the face datasets FairFace [80] and UTKFace [159] with demographic labels, following Radford et al. [111].\n\nQ.1 Diversity\n\nWe break down model performance on the Dollar Street and GeoDE datasets in Figure 21. Dollar Street consists of images of household items taken in homes around the world, and represents a wide socioeconomic range that includes homes with no Internet access [122]. The objects belong to ImageNet categories, and the task is image classification. Standard ImageNet-trained models achieve monotonically increasing performance levels with higher household income levels [122]. Here we use the income-based subgroups defined in Rojas et al. [122], and find a similar bias as discovered in their paper. While our trained models show a smaller worst-group performance gap than an ImageNet-trained ResNet-50, they underperform a model fine-tuned on Dollar Street. Models with higher average accuracy show a larger worst-group gap, which future work should try to address.\n\nGeoDE consists of images of everyday items and objects, which again fall into ImageNet categories. The dataset represents six world regions equally, and primarily aims to promote geographic diversity of datasets [114]. Both ImageNet models and our models show less bias under this distribution compared to Dollar Street, with a smaller worst-group accuracy gap. The trends show that performance across all regions improves steadily with increased scale, and the performance approaches that of a model fine-tuned on GeoDE. While we know that classifiers trained specifically on ImageNet can display geographic biases [114], these biases are not apparent in our GeoDE model evaluations. Future work is needed to investigate the extent to which our models have geographic biases not evaluated in GeoDE.", "md": "$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{1.0 Dollar Street Performance} & 1.0 & \\text{GeoDE Performance} \\\\\n\\text{Worst-group top-5 accuracy} & \\text{Worst-region accuracy} \\\\\n0.8 & 0.8 \\\\\n0.6 & 0.6 \\\\\n0.4 & 0.4 \\\\\n0.2 & 0.2 \\\\\n0.00 & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 & 0.00 & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 \\\\\n\\text{Average top-5 accuracy} & \\text{Average accuracy} \\\\\n\\text{Basic} & \\text{Text-based} & \\text{ImageNet ResNet-50} \\\\\n\\text{Image-based} & \\text{CLIP score} & \\text{Fine-tuned ResNet-50} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 21: Comparison of average and worst-group scores for Dollar Street and GeoDE diversity datasets. On Dollar Street, our overall higher-performing models display a larger worst-group performance gap (corresponding to lower income households). GeoDE does not show this trend. ImageNet-Sketch [143] and ObjectNet [13]. Moreover, for each data source, we use OpenAI\u2019s pretrained CLIP ViT-B/32 and ViT-L/14 models to compute the cosine similarity between image and text embeddings of a data point, and obtain the average cosine similarity score for the whole dataset.\n\nP.2.1 Additional results\n\nWe present a series of results for the BYOD track in Table 18.\n\nQ Fairness and biases\n\nTo study the biases displayed by our models, we include two diversity-related datasets, Dollar Street [122] and GeoDE [114], in our evaluation suite, and perform further analysis on the face datasets FairFace [80] and UTKFace [159] with demographic labels, following Radford et al. [111].\n\nQ.1 Diversity\n\nWe break down model performance on the Dollar Street and GeoDE datasets in Figure 21. Dollar Street consists of images of household items taken in homes around the world, and represents a wide socioeconomic range that includes homes with no Internet access [122]. The objects belong to ImageNet categories, and the task is image classification. Standard ImageNet-trained models achieve monotonically increasing performance levels with higher household income levels [122]. Here we use the income-based subgroups defined in Rojas et al. [122], and find a similar bias as discovered in their paper. While our trained models show a smaller worst-group performance gap than an ImageNet-trained ResNet-50, they underperform a model fine-tuned on Dollar Street. Models with higher average accuracy show a larger worst-group gap, which future work should try to address.\n\nGeoDE consists of images of everyday items and objects, which again fall into ImageNet categories. The dataset represents six world regions equally, and primarily aims to promote geographic diversity of datasets [114]. Both ImageNet models and our models show less bias under this distribution compared to Dollar Street, with a smaller worst-group accuracy gap. The trends show that performance across all regions improves steadily with increased scale, and the performance approaches that of a model fine-tuned on GeoDE. While we know that classifiers trained specifically on ImageNet can display geographic biases [114], these biases are not apparent in our GeoDE model evaluations. Future work is needed to investigate the extent to which our models have geographic biases not evaluated in GeoDE."}]}, {"page": 50, "text": "Table 18: Zero-shot performance for select baselines in the BYOD track. Unless specified otherwise,\nCOMMONPOOL means our pool filtered with CLIP score (L/14, 30%).\n  Scale       Data source      Training                                                         ImageNet      ImageNet      VTAB    Retrieval  Average over\n                               dataset size                                                                   dist. shifts                       38 datasets\n                    #0         CC12M                                                              0.099          0.080       0.223    0.197         0.205\n                    #1         LAION15M                                                           0.083          0.076       0.210    0.144         0.189\n                    #2         RedCaps                                                            0.076          0.066       0.177    0.141         0.168\n                    #3         Shutterstock 15M                                                   0.083          0.070       0.214    0.159         0.185\n   small            #4         YFCC15M                                                            0.071          0.046       0.182    0.147         0.164\n                    #5         #0 + #1 + #2                                                       0.097          0.084       0.208    0.161         0.195\n                    #6         #0 + #1 + #3                                                       0.091          0.081       0.222    0.138         0.202\n                    #7         #0 + #2 + #3 + #4                                                  0.095          0.075       0.205    0.164         0.186\n                    #8         #0\u20134                                                               0.093          0.076       0.205    0.162         0.193\n                    #9         CC12M                                                              0.245          0.189       0.283    0.289         0.272\n                   #10         LAION15M                                                           0.270          0.215       0.317    0.255         0.306\n                   #11         RedCaps                                                            0.237          0.166       0.271    0.178         0.263\n                   #12         Shutterstock 15M                                                   0.229          0.191       0.316    0.260         0.290\n                   #13         YFCC15M                                                            0.232          0.137       0.263    0.245         0.257\n                   #14         #9 + #10 + #11                                                     0.376          0.287       0.387    0.323         0.366\n                   #15         #9 + #10 + #12                                                     0.342          0.278       0.362    0.345         0.357\n                   #16         #9 + #11 + #12 + #13                                               0.360          0.268       0.365    0.275         0.345\n   medium          #17         #9\u201313                                                              0.371          0.285       0.408    0.280         0.367\n                   #18         Shutterstock illustration                                          0.053          0.094       0.205    0.125         0.180\n                   #19         Shutterstock photo                                                 0.342          0.209       0.364    0.350         0.331\n                   #20         Shutterstock vectors                                               0.072          0.151       0.216    0.148         0.208\n                   #21         Shutterstock full                                                  0.313          0.254       0.353    0.331         0.342\n                   #22         WIT full                                                           0.096          0.063       0.196    0.104         0.177\n                   #23         WIT English                                                        0.051          0.038       0.145    0.083         0.143\n                   #24         COYO                                                               0.272          0.235       0.301    0.254         0.304\n                   #25         LAION-COCO                                                         0.209          0.205       0.293    0.359         0.297\n                   #26         Shutterstock illustration                                          0.337          0.203       0.307    0.322         0.306\n                   #27         Shutterstock photo                                                 0.485          0.304       0.432    0.427         0.398\n                   #28         Shutterstock vectors                                               0.126          0.223       0.244    0.191         0.246\n                   #29         Shutterstock full                                                  0.500          0.412       0.472    0.451         0.456\n                   #30         COYO                                                               0.547          0.456       0.475    0.549         0.486\n                   #31         LAION-COCO                                                         0.355          0.351       0.395    0.494         0.398\n                   #32         COYO + LAION-COCO                                                  0.528          0.458       0.479    0.589         0.498\n                   #33         LAION-A                                                            0.611          0.474       0.501    0.542         0.505\n                   #34         LAION-2B                                                           0.585          0.472       0.504    0.525         0.515\n                   #35         COMMONPOOL + #9\u201313                                                 0.602          0.498       0.541    0.416         0.537\n                   #36         COMMONPOOL + #9\u201313 (2x upsampled)                                  0.613          0.507       0.559    0.433         0.543\n   large           #37         COMMONPOOL + #9\u201313 (4x upsampled)                                  0.615          0.514       0.553    0.427         0.543\n                   #38         COMMONPOOL + #9\u201313 (6x upsampled)                                  0.620          0.519       0.558    0.437         0.549\n                   #39         COMMONPOOL + #9\u201313 (8x upsampled)                                  0.624          0.520       0.533    0.443         0.537\n                   #40         COMMONPOOL + #9\u201313 (10x upsampled)                                 0.621          0.520       0.540    0.441         0.537\n                   #41         COMMONPOOL + COYO                                                  0.561          0.472       0.504    0.508         0.513\n                   #42         COMMONPOOL + LAION-A                                               0.607          0.480       0.531    0.514         0.527\n                   #43         COMMONPOOL + LAION-COCO                                            0.522          0.457       0.513    0.498         0.514\n                   #44         COMMONPOOL + #9+#11+#13+#19                                        0.609          0.508       0.546    0.439         0.536\n                   #45         COMMONPOOL + #9+#11+#13+#19 (2x upsampled)                         0.621          0.509       0.547    0.458         0.541\n                   #46         COMMONPOOL + #9+#11+#13+#19 (4x upsampled)                         0.632          0.515       0.533    0.452         0.532\n                   #47         COMMONPOOL + #9+#11+#13+#19 (6x upsampled)                         0.635          0.515       0.535    0.471         0.532\n                   #48         COMMONPOOL + #9+#11+#13+#19 (8x upsampled)                         0.633          0.515       0.523    0.464         0.530\n                   #49         COMMONPOOL + #9+#11+#13+#19 (10x upsampled)                        0.630          0.513       0.523    0.356         0.521\n                   #50         LAION-2B                                                           0.757          0.631       0.611    0.619         0.621\n   xlarge          #51         COMMONPOOL + #9+#11+#13+#19                                        0.766          0.660       0.662    0.539         0.659\n                   #52         COMMONPOOL + #9+#11+#13+#19 (6x upsampled)                         0.776          0.671       0.633    0.552         0.649\n                   #53         COMMONPOOL + #9+#11+#13+#19 (18x upsampled)                        0.771          0.667       0.629    0.554         0.643\nQ.2       Fairness\nEmulating Radford et al. [111], we evaluate our best models from the filtering and BYOD tracks on the human\nface datasets FairFace and UTKFace, using zero-shot classification to predict the race, gender, and age annotated\nin these datasets. Following Hanna et al. [59] and Hundt et al. [68], we acknowledge that these evaluations can\nbe problematic as race and gender should not be considered fixed categories, but rather fluid attributes that may\nchange for individuals, based on they way they identify at any given moment\u2014regardless of appearance. We\ninclude these evaluations for continuity with prior work and as a probe into model behaviour, but hope future\nwork will consider improved face fairness evaluation. We also note that race, gender, and age classification are\nnot the intended end-goals of the models or benchmark, and we do not condone the use of COMMONPOOL or\nmodels trained on COMMONPOOL data for any decisions involving people.\n                                                                                50", "md": "|Scale|Data source|Training dataset size|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|---|\n|small|#0|CC12M|0.099|0.080|0.223|0.197|0.205|\n| |#1|LAION15M|0.083|0.076|0.210|0.144|0.189|\n| |#2|RedCaps|0.076|0.066|0.177|0.141|0.168|\n| |#3|Shutterstock 15M|0.083|0.070|0.214|0.159|0.185|\n| |#4|YFCC15M|0.071|0.046|0.182|0.147|0.164|\n| |#5|#0 + #1 + #2|0.097|0.084|0.208|0.161|0.195|\n| |#6|#0 + #1 + #3|0.091|0.081|0.222|0.138|0.202|\n| |#7|#0 + #2 + #3 + #4|0.095|0.075|0.205|0.164|0.186|\n| |#8|#0\u20134|0.093|0.076|0.205|0.162|0.193|\n|medium|#9|CC12M|0.245|0.189|0.283|0.289|0.272|\n| |#10|LAION15M|0.270|0.215|0.317|0.255|0.306|\n| |#11|RedCaps|0.237|0.166|0.271|0.178|0.263|\n| |#12|Shutterstock 15M|0.229|0.191|0.316|0.260|0.290|\n| |#13|YFCC15M|0.232|0.137|0.263|0.245|0.257|\n| |#14|#9 + #10 + #11|0.376|0.287|0.387|0.323|0.366|\n| |#15|#9 + #10 + #12|0.342|0.278|0.362|0.345|0.357|\n| |#16|#9 + #11 + #12 + #13|0.360|0.268|0.365|0.275|0.345|\n| |#17|#9\u201313|0.371|0.285|0.408|0.280|0.367|\n| |#18|Shutterstock illustration|0.053|0.094|0.205|0.125|0.180|\n| |#19|Shutterstock photo|0.342|0.209|0.364|0.350|0.331|\n| |#20|Shutterstock vectors|0.072|0.151|0.216|0.148|0.208|\n| |#21|Shutterstock full|0.313|0.254|0.353|0.331|0.342|\n| |#22|WIT full|0.096|0.063|0.196|0.104|0.177|\n| |#23|WIT English|0.051|0.038|0.145|0.083|0.143|\n| |#24|COYO|0.272|0.235|0.301|0.254|0.304|\n| |#25|LAION-COCO|0.209|0.205|0.293|0.359|0.297|\n| |#26|Shutterstock illustration|0.337|0.203|0.307|0.322|0.306|\n| |#27|Shutterstock photo|0.485|0.304|0.432|0.427|0.398|\n| |#28|Shutterstock vectors|0.126|0.223|0.244|0.191|0.246|\n| |#29|Shutterstock full|0.500|0.412|0.472|0.451|0.456|\n| |#30|COYO|0.547|0.456|0.475|0.549|0.486|\n| |#31|LAION-COCO|0.355|0.351|0.395|0.494|0.398|\n| |#32|COYO + LAION-COCO|0.528|0.458|0.479|0.589|0.498|\n| |#33|LAION-A|0.611|0.474|0.501|0.542|0.505|\n| |#34|LAION-2B|0.585|0.472|0.504|0.525|0.515|\n| |#35|COMMONPOOL + #9\u201313|0.602|0.498|0.541|0.416|0.537|\n| |#36|COMMONPOOL + #9\u201313 (2x upsampled)|0.613|0.507|0.559|0.433|0.543|\n| |#37|COMMONPOOL + #9\u201313 (4x upsampled)|0.615|0.514|0.553|0.427|0.543|\n| |#38|COMMONPOOL + #9\u201313 (6x upsampled)|0.620|0.519|0.558|0.437|0.549|\n| |#39|COMMONPOOL + #9\u201313 (8x upsampled)|0.624|0.520|0.533|0.443|0.537|\n| |#40|COMMONPOOL + #9\u201313 (10x upsampled)|0.621|0.520|0.540|0.441|0.537|\n| |#41|COMMONPOOL + COYO|0.561|0.472|0.504|0.508|0.513|\n| |#42|COMMONPOOL + LAION-A|0.607|0.480|0.531|0.514|0.527|\n| |#43|COMMONPOOL + LAION-COCO|0.522|0.457|0.513|0.498|0.514|\n| |#44|COMMONPOOL + #9+#11+#13+#19|0.609|0.508|0.546|0.439|0.536|\n| |#45|COMMONPOOL + #9+#11+#13+#19 (2x upsampled)|0.621|0.509|0.547|0.458|0.541|\n| |#46|COMMONPOOL + #9+#11+#13+#19 (4x upsampled)|0.632|0.515|0.533|0.452|0.532|\n| |#47|COMMONPOOL + #9+#11+#13+#19 (6x upsampled)|0.635|0.515|0.535|0.471|0.532|\n| |#48|COMMONPOOL + #9+#11+#13+#19 (8x upsampled)|0.633|0.515|0.523|0.464|0.530|\n| |#49|COMMONPOOL + #9+#11+#13+#19 (10x upsampled)|0.630|0.513|0.523|0.356|0.521|\n|xlarge|#50|LAION-2B|0.757|0.631|0.611|0.619|0.621|\n| |#51|COMMONPOOL + #9+#11+#13+#19|0.766|0.660|0.662|0.539|0.659|\n| |#52|COMMONPOOL + #9+#11+#13+#19 (6x upsampled)|0.776|0.671|0.633|0.552|0.649|\n| |#53|COMMONPOOL + #9+#11+#13+#19 (18x upsampled)|0.771|0.667|0.629|0.554|0.643|\n\nQ.2 Fairness\n\nEmulating Radford et al. [111], we evaluate our best models from the filtering and BYOD tracks on the human face datasets FairFace and UTKFace, using zero-shot classification to predict the race, gender, and age annotated in these datasets. Following Hanna et al. [59] and Hundt et al. [68], we acknowledge that these evaluations can be problematic as race and gender should not be considered fixed categories, but rather fluid attributes that may change for individuals, based on they way they identify at any given moment\u2014regardless of appearance. We include these evaluations for continuity with prior work and as a probe into model behaviour, but hope future work will consider improved face fairness evaluation. We also note that race, gender, and age classification are not the intended end-goals of the models or benchmark, and we do not condone the use of COMMONPOOL or models trained on COMMONPOOL data for any decisions involving people.\n\n```", "images": [], "items": [{"type": "table", "rows": [["Scale", "Data source", "Training dataset size", "ImageNet", "ImageNet dist. shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["small", "#0", "CC12M", "0.099", "0.080", "0.223", "0.197", "0.205"], ["", "#1", "LAION15M", "0.083", "0.076", "0.210", "0.144", "0.189"], ["", "#2", "RedCaps", "0.076", "0.066", "0.177", "0.141", "0.168"], ["", "#3", "Shutterstock 15M", "0.083", "0.070", "0.214", "0.159", "0.185"], ["", "#4", "YFCC15M", "0.071", "0.046", "0.182", "0.147", "0.164"], ["", "#5", "#0 + #1 + #2", "0.097", "0.084", "0.208", "0.161", "0.195"], ["", "#6", "#0 + #1 + #3", "0.091", "0.081", "0.222", "0.138", "0.202"], ["", "#7", "#0 + #2 + #3 + #4", "0.095", "0.075", "0.205", "0.164", "0.186"], ["", "#8", "#0\u20134", "0.093", "0.076", "0.205", "0.162", "0.193"], ["medium", "#9", "CC12M", "0.245", "0.189", "0.283", "0.289", "0.272"], ["", "#10", "LAION15M", "0.270", "0.215", "0.317", "0.255", "0.306"], ["", "#11", "RedCaps", "0.237", "0.166", "0.271", "0.178", "0.263"], ["", "#12", "Shutterstock 15M", "0.229", "0.191", "0.316", "0.260", "0.290"], ["", "#13", "YFCC15M", "0.232", "0.137", "0.263", "0.245", "0.257"], ["", "#14", "#9 + #10 + #11", "0.376", "0.287", "0.387", "0.323", "0.366"], ["", "#15", "#9 + #10 + #12", "0.342", "0.278", "0.362", "0.345", "0.357"], ["", "#16", "#9 + #11 + #12 + #13", "0.360", "0.268", "0.365", "0.275", "0.345"], ["", "#17", "#9\u201313", "0.371", "0.285", "0.408", "0.280", "0.367"], ["", "#18", "Shutterstock illustration", "0.053", "0.094", "0.205", "0.125", "0.180"], ["", "#19", "Shutterstock photo", "0.342", "0.209", "0.364", "0.350", "0.331"], ["", "#20", "Shutterstock vectors", "0.072", "0.151", "0.216", "0.148", "0.208"], ["", "#21", "Shutterstock full", "0.313", "0.254", "0.353", "0.331", "0.342"], ["", "#22", "WIT full", "0.096", "0.063", "0.196", "0.104", "0.177"], ["", "#23", "WIT English", "0.051", "0.038", "0.145", "0.083", "0.143"], ["", "#24", "COYO", "0.272", "0.235", "0.301", "0.254", "0.304"], ["", "#25", "LAION-COCO", "0.209", "0.205", "0.293", "0.359", "0.297"], ["", "#26", "Shutterstock illustration", "0.337", "0.203", "0.307", "0.322", "0.306"], ["", "#27", "Shutterstock photo", "0.485", "0.304", "0.432", "0.427", "0.398"], ["", "#28", "Shutterstock vectors", "0.126", "0.223", "0.244", "0.191", "0.246"], ["", "#29", "Shutterstock full", "0.500", "0.412", "0.472", "0.451", "0.456"], ["", "#30", "COYO", "0.547", "0.456", "0.475", "0.549", "0.486"], ["", "#31", "LAION-COCO", "0.355", "0.351", "0.395", "0.494", "0.398"], ["", "#32", "COYO + LAION-COCO", "0.528", "0.458", "0.479", "0.589", "0.498"], ["", "#33", "LAION-A", "0.611", "0.474", "0.501", "0.542", "0.505"], ["", "#34", "LAION-2B", "0.585", "0.472", "0.504", "0.525", "0.515"], ["", "#35", "COMMONPOOL + #9\u201313", "0.602", "0.498", "0.541", "0.416", "0.537"], ["", "#36", "COMMONPOOL + #9\u201313 (2x upsampled)", "0.613", "0.507", "0.559", "0.433", "0.543"], ["", "#37", "COMMONPOOL + #9\u201313 (4x upsampled)", "0.615", "0.514", "0.553", "0.427", "0.543"], ["", "#38", "COMMONPOOL + #9\u201313 (6x upsampled)", "0.620", "0.519", "0.558", "0.437", "0.549"], ["", "#39", "COMMONPOOL + #9\u201313 (8x upsampled)", "0.624", "0.520", "0.533", "0.443", "0.537"], ["", "#40", "COMMONPOOL + #9\u201313 (10x upsampled)", "0.621", "0.520", "0.540", "0.441", "0.537"], ["", "#41", "COMMONPOOL + COYO", "0.561", "0.472", "0.504", "0.508", "0.513"], ["", "#42", "COMMONPOOL + LAION-A", "0.607", "0.480", "0.531", "0.514", "0.527"], ["", "#43", "COMMONPOOL + LAION-COCO", "0.522", "0.457", "0.513", "0.498", "0.514"], ["", "#44", "COMMONPOOL + #9+#11+#13+#19", "0.609", "0.508", "0.546", "0.439", "0.536"], ["", "#45", "COMMONPOOL + #9+#11+#13+#19 (2x upsampled)", "0.621", "0.509", "0.547", "0.458", "0.541"], ["", "#46", "COMMONPOOL + #9+#11+#13+#19 (4x upsampled)", "0.632", "0.515", "0.533", "0.452", "0.532"], ["", "#47", "COMMONPOOL + #9+#11+#13+#19 (6x upsampled)", "0.635", "0.515", "0.535", "0.471", "0.532"], ["", "#48", "COMMONPOOL + #9+#11+#13+#19 (8x upsampled)", "0.633", "0.515", "0.523", "0.464", "0.530"], ["", "#49", "COMMONPOOL + #9+#11+#13+#19 (10x upsampled)", "0.630", "0.513", "0.523", "0.356", "0.521"], ["xlarge", "#50", "LAION-2B", "0.757", "0.631", "0.611", "0.619", "0.621"], ["", "#51", "COMMONPOOL + #9+#11+#13+#19", "0.766", "0.660", "0.662", "0.539", "0.659"], ["", "#52", "COMMONPOOL + #9+#11+#13+#19 (6x upsampled)", "0.776", "0.671", "0.633", "0.552", "0.649"], ["", "#53", "COMMONPOOL + #9+#11+#13+#19 (18x upsampled)", "0.771", "0.667", "0.629", "0.554", "0.643"]], "md": "|Scale|Data source|Training dataset size|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|---|\n|small|#0|CC12M|0.099|0.080|0.223|0.197|0.205|\n| |#1|LAION15M|0.083|0.076|0.210|0.144|0.189|\n| |#2|RedCaps|0.076|0.066|0.177|0.141|0.168|\n| |#3|Shutterstock 15M|0.083|0.070|0.214|0.159|0.185|\n| |#4|YFCC15M|0.071|0.046|0.182|0.147|0.164|\n| |#5|#0 + #1 + #2|0.097|0.084|0.208|0.161|0.195|\n| |#6|#0 + #1 + #3|0.091|0.081|0.222|0.138|0.202|\n| |#7|#0 + #2 + #3 + #4|0.095|0.075|0.205|0.164|0.186|\n| |#8|#0\u20134|0.093|0.076|0.205|0.162|0.193|\n|medium|#9|CC12M|0.245|0.189|0.283|0.289|0.272|\n| |#10|LAION15M|0.270|0.215|0.317|0.255|0.306|\n| |#11|RedCaps|0.237|0.166|0.271|0.178|0.263|\n| |#12|Shutterstock 15M|0.229|0.191|0.316|0.260|0.290|\n| |#13|YFCC15M|0.232|0.137|0.263|0.245|0.257|\n| |#14|#9 + #10 + #11|0.376|0.287|0.387|0.323|0.366|\n| |#15|#9 + #10 + #12|0.342|0.278|0.362|0.345|0.357|\n| |#16|#9 + #11 + #12 + #13|0.360|0.268|0.365|0.275|0.345|\n| |#17|#9\u201313|0.371|0.285|0.408|0.280|0.367|\n| |#18|Shutterstock illustration|0.053|0.094|0.205|0.125|0.180|\n| |#19|Shutterstock photo|0.342|0.209|0.364|0.350|0.331|\n| |#20|Shutterstock vectors|0.072|0.151|0.216|0.148|0.208|\n| |#21|Shutterstock full|0.313|0.254|0.353|0.331|0.342|\n| |#22|WIT full|0.096|0.063|0.196|0.104|0.177|\n| |#23|WIT English|0.051|0.038|0.145|0.083|0.143|\n| |#24|COYO|0.272|0.235|0.301|0.254|0.304|\n| |#25|LAION-COCO|0.209|0.205|0.293|0.359|0.297|\n| |#26|Shutterstock illustration|0.337|0.203|0.307|0.322|0.306|\n| |#27|Shutterstock photo|0.485|0.304|0.432|0.427|0.398|\n| |#28|Shutterstock vectors|0.126|0.223|0.244|0.191|0.246|\n| |#29|Shutterstock full|0.500|0.412|0.472|0.451|0.456|\n| |#30|COYO|0.547|0.456|0.475|0.549|0.486|\n| |#31|LAION-COCO|0.355|0.351|0.395|0.494|0.398|\n| |#32|COYO + LAION-COCO|0.528|0.458|0.479|0.589|0.498|\n| |#33|LAION-A|0.611|0.474|0.501|0.542|0.505|\n| |#34|LAION-2B|0.585|0.472|0.504|0.525|0.515|\n| |#35|COMMONPOOL + #9\u201313|0.602|0.498|0.541|0.416|0.537|\n| |#36|COMMONPOOL + #9\u201313 (2x upsampled)|0.613|0.507|0.559|0.433|0.543|\n| |#37|COMMONPOOL + #9\u201313 (4x upsampled)|0.615|0.514|0.553|0.427|0.543|\n| |#38|COMMONPOOL + #9\u201313 (6x upsampled)|0.620|0.519|0.558|0.437|0.549|\n| |#39|COMMONPOOL + #9\u201313 (8x upsampled)|0.624|0.520|0.533|0.443|0.537|\n| |#40|COMMONPOOL + #9\u201313 (10x upsampled)|0.621|0.520|0.540|0.441|0.537|\n| |#41|COMMONPOOL + COYO|0.561|0.472|0.504|0.508|0.513|\n| |#42|COMMONPOOL + LAION-A|0.607|0.480|0.531|0.514|0.527|\n| |#43|COMMONPOOL + LAION-COCO|0.522|0.457|0.513|0.498|0.514|\n| |#44|COMMONPOOL + #9+#11+#13+#19|0.609|0.508|0.546|0.439|0.536|\n| |#45|COMMONPOOL + #9+#11+#13+#19 (2x upsampled)|0.621|0.509|0.547|0.458|0.541|\n| |#46|COMMONPOOL + #9+#11+#13+#19 (4x upsampled)|0.632|0.515|0.533|0.452|0.532|\n| |#47|COMMONPOOL + #9+#11+#13+#19 (6x upsampled)|0.635|0.515|0.535|0.471|0.532|\n| |#48|COMMONPOOL + #9+#11+#13+#19 (8x upsampled)|0.633|0.515|0.523|0.464|0.530|\n| |#49|COMMONPOOL + #9+#11+#13+#19 (10x upsampled)|0.630|0.513|0.523|0.356|0.521|\n|xlarge|#50|LAION-2B|0.757|0.631|0.611|0.619|0.621|\n| |#51|COMMONPOOL + #9+#11+#13+#19|0.766|0.660|0.662|0.539|0.659|\n| |#52|COMMONPOOL + #9+#11+#13+#19 (6x upsampled)|0.776|0.671|0.633|0.552|0.649|\n| |#53|COMMONPOOL + #9+#11+#13+#19 (18x upsampled)|0.771|0.667|0.629|0.554|0.643|", "isPerfectTable": true, "csv": "\"Scale\",\"Data source\",\"Training dataset size\",\"ImageNet\",\"ImageNet dist. shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"small\",\"#0\",\"CC12M\",\"0.099\",\"0.080\",\"0.223\",\"0.197\",\"0.205\"\n\"\",\"#1\",\"LAION15M\",\"0.083\",\"0.076\",\"0.210\",\"0.144\",\"0.189\"\n\"\",\"#2\",\"RedCaps\",\"0.076\",\"0.066\",\"0.177\",\"0.141\",\"0.168\"\n\"\",\"#3\",\"Shutterstock 15M\",\"0.083\",\"0.070\",\"0.214\",\"0.159\",\"0.185\"\n\"\",\"#4\",\"YFCC15M\",\"0.071\",\"0.046\",\"0.182\",\"0.147\",\"0.164\"\n\"\",\"#5\",\"#0 + #1 + #2\",\"0.097\",\"0.084\",\"0.208\",\"0.161\",\"0.195\"\n\"\",\"#6\",\"#0 + #1 + #3\",\"0.091\",\"0.081\",\"0.222\",\"0.138\",\"0.202\"\n\"\",\"#7\",\"#0 + #2 + #3 + #4\",\"0.095\",\"0.075\",\"0.205\",\"0.164\",\"0.186\"\n\"\",\"#8\",\"#0\u20134\",\"0.093\",\"0.076\",\"0.205\",\"0.162\",\"0.193\"\n\"medium\",\"#9\",\"CC12M\",\"0.245\",\"0.189\",\"0.283\",\"0.289\",\"0.272\"\n\"\",\"#10\",\"LAION15M\",\"0.270\",\"0.215\",\"0.317\",\"0.255\",\"0.306\"\n\"\",\"#11\",\"RedCaps\",\"0.237\",\"0.166\",\"0.271\",\"0.178\",\"0.263\"\n\"\",\"#12\",\"Shutterstock 15M\",\"0.229\",\"0.191\",\"0.316\",\"0.260\",\"0.290\"\n\"\",\"#13\",\"YFCC15M\",\"0.232\",\"0.137\",\"0.263\",\"0.245\",\"0.257\"\n\"\",\"#14\",\"#9 + #10 + #11\",\"0.376\",\"0.287\",\"0.387\",\"0.323\",\"0.366\"\n\"\",\"#15\",\"#9 + #10 + #12\",\"0.342\",\"0.278\",\"0.362\",\"0.345\",\"0.357\"\n\"\",\"#16\",\"#9 + #11 + #12 + #13\",\"0.360\",\"0.268\",\"0.365\",\"0.275\",\"0.345\"\n\"\",\"#17\",\"#9\u201313\",\"0.371\",\"0.285\",\"0.408\",\"0.280\",\"0.367\"\n\"\",\"#18\",\"Shutterstock illustration\",\"0.053\",\"0.094\",\"0.205\",\"0.125\",\"0.180\"\n\"\",\"#19\",\"Shutterstock photo\",\"0.342\",\"0.209\",\"0.364\",\"0.350\",\"0.331\"\n\"\",\"#20\",\"Shutterstock vectors\",\"0.072\",\"0.151\",\"0.216\",\"0.148\",\"0.208\"\n\"\",\"#21\",\"Shutterstock full\",\"0.313\",\"0.254\",\"0.353\",\"0.331\",\"0.342\"\n\"\",\"#22\",\"WIT full\",\"0.096\",\"0.063\",\"0.196\",\"0.104\",\"0.177\"\n\"\",\"#23\",\"WIT English\",\"0.051\",\"0.038\",\"0.145\",\"0.083\",\"0.143\"\n\"\",\"#24\",\"COYO\",\"0.272\",\"0.235\",\"0.301\",\"0.254\",\"0.304\"\n\"\",\"#25\",\"LAION-COCO\",\"0.209\",\"0.205\",\"0.293\",\"0.359\",\"0.297\"\n\"\",\"#26\",\"Shutterstock illustration\",\"0.337\",\"0.203\",\"0.307\",\"0.322\",\"0.306\"\n\"\",\"#27\",\"Shutterstock photo\",\"0.485\",\"0.304\",\"0.432\",\"0.427\",\"0.398\"\n\"\",\"#28\",\"Shutterstock vectors\",\"0.126\",\"0.223\",\"0.244\",\"0.191\",\"0.246\"\n\"\",\"#29\",\"Shutterstock full\",\"0.500\",\"0.412\",\"0.472\",\"0.451\",\"0.456\"\n\"\",\"#30\",\"COYO\",\"0.547\",\"0.456\",\"0.475\",\"0.549\",\"0.486\"\n\"\",\"#31\",\"LAION-COCO\",\"0.355\",\"0.351\",\"0.395\",\"0.494\",\"0.398\"\n\"\",\"#32\",\"COYO + LAION-COCO\",\"0.528\",\"0.458\",\"0.479\",\"0.589\",\"0.498\"\n\"\",\"#33\",\"LAION-A\",\"0.611\",\"0.474\",\"0.501\",\"0.542\",\"0.505\"\n\"\",\"#34\",\"LAION-2B\",\"0.585\",\"0.472\",\"0.504\",\"0.525\",\"0.515\"\n\"\",\"#35\",\"COMMONPOOL + #9\u201313\",\"0.602\",\"0.498\",\"0.541\",\"0.416\",\"0.537\"\n\"\",\"#36\",\"COMMONPOOL + #9\u201313 (2x upsampled)\",\"0.613\",\"0.507\",\"0.559\",\"0.433\",\"0.543\"\n\"\",\"#37\",\"COMMONPOOL + #9\u201313 (4x upsampled)\",\"0.615\",\"0.514\",\"0.553\",\"0.427\",\"0.543\"\n\"\",\"#38\",\"COMMONPOOL + #9\u201313 (6x upsampled)\",\"0.620\",\"0.519\",\"0.558\",\"0.437\",\"0.549\"\n\"\",\"#39\",\"COMMONPOOL + #9\u201313 (8x upsampled)\",\"0.624\",\"0.520\",\"0.533\",\"0.443\",\"0.537\"\n\"\",\"#40\",\"COMMONPOOL + #9\u201313 (10x upsampled)\",\"0.621\",\"0.520\",\"0.540\",\"0.441\",\"0.537\"\n\"\",\"#41\",\"COMMONPOOL + COYO\",\"0.561\",\"0.472\",\"0.504\",\"0.508\",\"0.513\"\n\"\",\"#42\",\"COMMONPOOL + LAION-A\",\"0.607\",\"0.480\",\"0.531\",\"0.514\",\"0.527\"\n\"\",\"#43\",\"COMMONPOOL + LAION-COCO\",\"0.522\",\"0.457\",\"0.513\",\"0.498\",\"0.514\"\n\"\",\"#44\",\"COMMONPOOL + #9+#11+#13+#19\",\"0.609\",\"0.508\",\"0.546\",\"0.439\",\"0.536\"\n\"\",\"#45\",\"COMMONPOOL + #9+#11+#13+#19 (2x upsampled)\",\"0.621\",\"0.509\",\"0.547\",\"0.458\",\"0.541\"\n\"\",\"#46\",\"COMMONPOOL + #9+#11+#13+#19 (4x upsampled)\",\"0.632\",\"0.515\",\"0.533\",\"0.452\",\"0.532\"\n\"\",\"#47\",\"COMMONPOOL + #9+#11+#13+#19 (6x upsampled)\",\"0.635\",\"0.515\",\"0.535\",\"0.471\",\"0.532\"\n\"\",\"#48\",\"COMMONPOOL + #9+#11+#13+#19 (8x upsampled)\",\"0.633\",\"0.515\",\"0.523\",\"0.464\",\"0.530\"\n\"\",\"#49\",\"COMMONPOOL + #9+#11+#13+#19 (10x upsampled)\",\"0.630\",\"0.513\",\"0.523\",\"0.356\",\"0.521\"\n\"xlarge\",\"#50\",\"LAION-2B\",\"0.757\",\"0.631\",\"0.611\",\"0.619\",\"0.621\"\n\"\",\"#51\",\"COMMONPOOL + #9+#11+#13+#19\",\"0.766\",\"0.660\",\"0.662\",\"0.539\",\"0.659\"\n\"\",\"#52\",\"COMMONPOOL + #9+#11+#13+#19 (6x upsampled)\",\"0.776\",\"0.671\",\"0.633\",\"0.552\",\"0.649\"\n\"\",\"#53\",\"COMMONPOOL + #9+#11+#13+#19 (18x upsampled)\",\"0.771\",\"0.667\",\"0.629\",\"0.554\",\"0.643\""}, {"type": "text", "value": "Q.2 Fairness\n\nEmulating Radford et al. [111], we evaluate our best models from the filtering and BYOD tracks on the human face datasets FairFace and UTKFace, using zero-shot classification to predict the race, gender, and age annotated in these datasets. Following Hanna et al. [59] and Hundt et al. [68], we acknowledge that these evaluations can be problematic as race and gender should not be considered fixed categories, but rather fluid attributes that may change for individuals, based on they way they identify at any given moment\u2014regardless of appearance. We include these evaluations for continuity with prior work and as a probe into model behaviour, but hope future work will consider improved face fairness evaluation. We also note that race, gender, and age classification are not the intended end-goals of the models or benchmark, and we do not condone the use of COMMONPOOL or models trained on COMMONPOOL data for any decisions involving people.\n\n```", "md": "Q.2 Fairness\n\nEmulating Radford et al. [111], we evaluate our best models from the filtering and BYOD tracks on the human face datasets FairFace and UTKFace, using zero-shot classification to predict the race, gender, and age annotated in these datasets. Following Hanna et al. [59] and Hundt et al. [68], we acknowledge that these evaluations can be problematic as race and gender should not be considered fixed categories, but rather fluid attributes that may change for individuals, based on they way they identify at any given moment\u2014regardless of appearance. We include these evaluations for continuity with prior work and as a probe into model behaviour, but hope future work will consider improved face fairness evaluation. We also note that race, gender, and age classification are not the intended end-goals of the models or benchmark, and we do not condone the use of COMMONPOOL or models trained on COMMONPOOL data for any decisions involving people.\n\n```"}]}, {"page": 51, "text": "Table 19: Overall race, gender, and age classification accuracy of our two best xlarge baselines,\nImage-based \u2229       CLIP score (L/14 30%) for the filtering track and COMMONPOOL, CLIP score +\n4 external sources (upsampled 6x) for the BYOD track. Race classification was binary (white or\nnon-white) as in Karkkainen & Joo [80].\n                                    Dataset      Track       Race   Gender     Age\n                                    FairFace     Filtering   86.4     91.7     34.3\n                                                 BYOD        76.5     93.9     33.8\n                                    UTKFace      Filtering   86.2     93.8     39.5\n                                                 BYOD        86.1     95.5     38.6\nTable 20: Gender classification accuracy of our two best xlarge baselines, Image-based \u2229                   CLIP score\n(L/14 30%) for the filtering track and COMMONPOOL, CLIP score + 4 external sources (upsampled\n6x) for the BYOD track.\n                                                       FairFace\n  Track       Gender                                                Race\n                        Black   White    Indian   Latino/Hispanic    Middle Eastern     Southeast Asian    East Asian\n   Filtering  Male       79.3    91.3     90.8          90.4               95.7               83.0            80.7\n              Female     95.4    96.6     94.2          96.6               96.5               97.2            98.2\n   BYOD       Male       89.2    94.8     93.2          93.4               97.4               90.2            90.6\n              Female     89.2    96.0     94.2          96.0               96.2               97.1            97.0\n                                                       UTKFace\n                           Track        Gender                      Race\n                                                  Black    White    Indian   Asian    Other\n                            Filtering   Male       95.4     92.5     91.7     73.1     84.2\n                                        Female     97.3     98.7     97.4     98.3     97.4\n                            BYOD        Male       96.8     95.9     94.7     85.7     90.4\n                                        Female     96.3     97.7     96.8     95.9     95.6\nAs described in Appendix G, our filleting track models are trained on images with faces blurred. Nevertheless,\nthese models still perform significantly above random chance on face classification. We hypothesize that this is\ndue to a combination of faces bypassing our face blurring filter in the training data, contextual clues outside of the\nface region, or signal associated with skin color. The BYOD track model performs even better than the filtering\ntrack model. We hypothesize that this is because BYOD data is used off-the-shelf and hence contains non-blurred\nfaces. In Table 19, we present overall accuracy for these three traits. Note that race is treated as a binary variable\n(white or non-white) to enable comparison to prior results, gender is a binary variable (male or female) according\nto annotations, and age is binned into 9 ranges according to the annotation precision of FairFace. The BYOD\nmodel, performs better at distinguishing the annotated gender, but is worse at distinguishing annotated race and\nage.\nWe further break down these statistics over the intersection of race and gender, examining gender classification\naccuracies in Table 20. We find that there are drastic differences in accuracy across different annotated subgroups,\nvarying by both race and gender. The filtering models shows a tendency to misclassify Black, Southeast Asian,\nand East Asian males as females at 20.7%, 17%, and 19.3% respectively on FairFace. Furthermore, we find that\nwhile the BYOD model improves accuracy, on FairFace most of this improvement is on men (ranging from\n1.7pp gain to 9.9pp gain), while on women, BYOD offers little change (ranging from 0.6pp gain to 6.2pp drop).\nFollowing Radford et al. [111], we also examined associations of particular demographics with potentially\nharmful language. We replicate their setup with two classification tasks: (1) including race-gender intersection\nclasses (e.g. \u201cblack woman\u201d, \u201cindian man\u201d, etc.) and several harmful crime-related terms (\u201cthief\u201d, \u201ccriminal\u201d,\n\u201csuspicious person\u201d); (2) including the same race-gender intersection classes and non-human terms (\u201canimal\u201d,\n\u201cgorilla\u201d, \u201cchimpanzee\u201d, \u201corangutan\u201d). We compute the frequency of misclassification of people into one of the\nharmful categories and run these experiments on FairFace and UTKFace separately. The results are shown in\nTable 21. Unlike in Radford et al. [111], we find that our models have a very small probability of classifying\nhuman faces as non-human, with a max score across all subgroups of 0.1%. However, a significant proportion\nof people are misclassified as criminal. This again highlights the importance of dataset curation and the risks\nassociated with zero-shot classification on models trained on web-scraped datasets.\n                                                           51", "md": "**Table 19: Overall race, gender, and age classification accuracy of our two best xlarge baselines**\n|Dataset|Track|Race|Gender|Age|\n|---|---|---|---|---|\n|FairFace|Filtering|86.4|91.7|34.3|\n| |BYOD|76.5|93.9|33.8|\n|UTKFace|Filtering|86.2|93.8|39.5|\n| |BYOD|86.1|95.5|38.6|\n\n|Track|Gender|Black|White|Indian|Latino/Hispanic|Middle Eastern|Southeast Asian|East Asian|\n|---|---|---|---|---|---|---|---|---|\n|Filtering|Male|79.3|91.3|90.8|90.4|95.7|83.0|80.7|\n| |Female|95.4|96.6|94.2|96.6|96.5|97.2|98.2|\n|BYOD|Male|89.2|94.8|93.2|93.4|97.4|90.2|90.6|\n| |Female|89.2|96.0|94.2|96.0|96.2|97.1|97.0|\n\nAs described in Appendix G, our filtering track models are trained on images with faces blurred. Nevertheless, these models still perform significantly above random chance on face classification. We hypothesize that this is due to a combination of faces bypassing our face blurring filter in the training data, contextual clues outside of the face region, or signal associated with skin color. The BYOD track model performs even better than the filtering track model. We hypothesize that this is because BYOD data is used off-the-shelf and hence contains non-blurred faces. In Table 19, we present overall accuracy for these three traits. Note that race is treated as a binary variable (white or non-white) to enable comparison to prior results, gender is a binary variable (male or female) according to annotations, and age is binned into 9 ranges according to the annotation precision of FairFace. The BYOD model performs better at distinguishing the annotated gender, but is worse at distinguishing annotated race and age.\n\nWe further break down these statistics over the intersection of race and gender, examining gender classification accuracies in Table 20. We find that there are drastic differences in accuracy across different annotated subgroups, varying by both race and gender. The filtering models show a tendency to misclassify Black, Southeast Asian, and East Asian males as females at 20.7%, 17%, and 19.3% respectively on FairFace. Furthermore, we find that while the BYOD model improves accuracy, on FairFace most of this improvement is on men (ranging from 1.7pp gain to 9.9pp gain), while on women, BYOD offers little change (ranging from 0.6pp gain to 6.2pp drop).\n\nFollowing Radford et al. [111], we also examined associations of particular demographics with potentially harmful language. We replicate their setup with two classification tasks: (1) including race-gender intersection classes (e.g. \"black woman\", \"indian man\", etc.) and several harmful crime-related terms (\"thief\", \"criminal\", \"suspicious person\"); (2) including the same race-gender intersection classes and non-human terms (\"animal\", \"gorilla\", \"chimpanzee\", \"orangutan\"). We compute the frequency of misclassification of people into one of the harmful categories and run these experiments on FairFace and UTKFace separately. The results are shown in Table 21. Unlike in Radford et al. [111], we find that our models have a very small probability of classifying human faces as non-human, with a max score across all subgroups of 0.1%. However, a significant proportion of people are misclassified as criminal. This again highlights the importance of dataset curation and the risks associated with zero-shot classification on models trained on web-scraped datasets.", "images": [], "items": [{"type": "text", "value": "**Table 19: Overall race, gender, and age classification accuracy of our two best xlarge baselines**", "md": "**Table 19: Overall race, gender, and age classification accuracy of our two best xlarge baselines**"}, {"type": "table", "rows": [["Dataset", "Track", "Race", "Gender", "Age"], ["FairFace", "Filtering", "86.4", "91.7", "34.3"], ["", "BYOD", "76.5", "93.9", "33.8"], ["UTKFace", "Filtering", "86.2", "93.8", "39.5"], ["", "BYOD", "86.1", "95.5", "38.6"]], "md": "|Dataset|Track|Race|Gender|Age|\n|---|---|---|---|---|\n|FairFace|Filtering|86.4|91.7|34.3|\n| |BYOD|76.5|93.9|33.8|\n|UTKFace|Filtering|86.2|93.8|39.5|\n| |BYOD|86.1|95.5|38.6|", "isPerfectTable": true, "csv": "\"Dataset\",\"Track\",\"Race\",\"Gender\",\"Age\"\n\"FairFace\",\"Filtering\",\"86.4\",\"91.7\",\"34.3\"\n\"\",\"BYOD\",\"76.5\",\"93.9\",\"33.8\"\n\"UTKFace\",\"Filtering\",\"86.2\",\"93.8\",\"39.5\"\n\"\",\"BYOD\",\"86.1\",\"95.5\",\"38.6\""}, {"type": "table", "rows": [["Track", "Gender", "Black", "White", "Indian", "Latino/Hispanic", "Middle Eastern", "Southeast Asian", "East Asian"], ["Filtering", "Male", "79.3", "91.3", "90.8", "90.4", "95.7", "83.0", "80.7"], ["", "Female", "95.4", "96.6", "94.2", "96.6", "96.5", "97.2", "98.2"], ["BYOD", "Male", "89.2", "94.8", "93.2", "93.4", "97.4", "90.2", "90.6"], ["", "Female", "89.2", "96.0", "94.2", "96.0", "96.2", "97.1", "97.0"]], "md": "|Track|Gender|Black|White|Indian|Latino/Hispanic|Middle Eastern|Southeast Asian|East Asian|\n|---|---|---|---|---|---|---|---|---|\n|Filtering|Male|79.3|91.3|90.8|90.4|95.7|83.0|80.7|\n| |Female|95.4|96.6|94.2|96.6|96.5|97.2|98.2|\n|BYOD|Male|89.2|94.8|93.2|93.4|97.4|90.2|90.6|\n| |Female|89.2|96.0|94.2|96.0|96.2|97.1|97.0|", "isPerfectTable": true, "csv": "\"Track\",\"Gender\",\"Black\",\"White\",\"Indian\",\"Latino/Hispanic\",\"Middle Eastern\",\"Southeast Asian\",\"East Asian\"\n\"Filtering\",\"Male\",\"79.3\",\"91.3\",\"90.8\",\"90.4\",\"95.7\",\"83.0\",\"80.7\"\n\"\",\"Female\",\"95.4\",\"96.6\",\"94.2\",\"96.6\",\"96.5\",\"97.2\",\"98.2\"\n\"BYOD\",\"Male\",\"89.2\",\"94.8\",\"93.2\",\"93.4\",\"97.4\",\"90.2\",\"90.6\"\n\"\",\"Female\",\"89.2\",\"96.0\",\"94.2\",\"96.0\",\"96.2\",\"97.1\",\"97.0\""}, {"type": "text", "value": "As described in Appendix G, our filtering track models are trained on images with faces blurred. Nevertheless, these models still perform significantly above random chance on face classification. We hypothesize that this is due to a combination of faces bypassing our face blurring filter in the training data, contextual clues outside of the face region, or signal associated with skin color. The BYOD track model performs even better than the filtering track model. We hypothesize that this is because BYOD data is used off-the-shelf and hence contains non-blurred faces. In Table 19, we present overall accuracy for these three traits. Note that race is treated as a binary variable (white or non-white) to enable comparison to prior results, gender is a binary variable (male or female) according to annotations, and age is binned into 9 ranges according to the annotation precision of FairFace. The BYOD model performs better at distinguishing the annotated gender, but is worse at distinguishing annotated race and age.\n\nWe further break down these statistics over the intersection of race and gender, examining gender classification accuracies in Table 20. We find that there are drastic differences in accuracy across different annotated subgroups, varying by both race and gender. The filtering models show a tendency to misclassify Black, Southeast Asian, and East Asian males as females at 20.7%, 17%, and 19.3% respectively on FairFace. Furthermore, we find that while the BYOD model improves accuracy, on FairFace most of this improvement is on men (ranging from 1.7pp gain to 9.9pp gain), while on women, BYOD offers little change (ranging from 0.6pp gain to 6.2pp drop).\n\nFollowing Radford et al. [111], we also examined associations of particular demographics with potentially harmful language. We replicate their setup with two classification tasks: (1) including race-gender intersection classes (e.g. \"black woman\", \"indian man\", etc.) and several harmful crime-related terms (\"thief\", \"criminal\", \"suspicious person\"); (2) including the same race-gender intersection classes and non-human terms (\"animal\", \"gorilla\", \"chimpanzee\", \"orangutan\"). We compute the frequency of misclassification of people into one of the harmful categories and run these experiments on FairFace and UTKFace separately. The results are shown in Table 21. Unlike in Radford et al. [111], we find that our models have a very small probability of classifying human faces as non-human, with a max score across all subgroups of 0.1%. However, a significant proportion of people are misclassified as criminal. This again highlights the importance of dataset curation and the risks associated with zero-shot classification on models trained on web-scraped datasets.", "md": "As described in Appendix G, our filtering track models are trained on images with faces blurred. Nevertheless, these models still perform significantly above random chance on face classification. We hypothesize that this is due to a combination of faces bypassing our face blurring filter in the training data, contextual clues outside of the face region, or signal associated with skin color. The BYOD track model performs even better than the filtering track model. We hypothesize that this is because BYOD data is used off-the-shelf and hence contains non-blurred faces. In Table 19, we present overall accuracy for these three traits. Note that race is treated as a binary variable (white or non-white) to enable comparison to prior results, gender is a binary variable (male or female) according to annotations, and age is binned into 9 ranges according to the annotation precision of FairFace. The BYOD model performs better at distinguishing the annotated gender, but is worse at distinguishing annotated race and age.\n\nWe further break down these statistics over the intersection of race and gender, examining gender classification accuracies in Table 20. We find that there are drastic differences in accuracy across different annotated subgroups, varying by both race and gender. The filtering models show a tendency to misclassify Black, Southeast Asian, and East Asian males as females at 20.7%, 17%, and 19.3% respectively on FairFace. Furthermore, we find that while the BYOD model improves accuracy, on FairFace most of this improvement is on men (ranging from 1.7pp gain to 9.9pp gain), while on women, BYOD offers little change (ranging from 0.6pp gain to 6.2pp drop).\n\nFollowing Radford et al. [111], we also examined associations of particular demographics with potentially harmful language. We replicate their setup with two classification tasks: (1) including race-gender intersection classes (e.g. \"black woman\", \"indian man\", etc.) and several harmful crime-related terms (\"thief\", \"criminal\", \"suspicious person\"); (2) including the same race-gender intersection classes and non-human terms (\"animal\", \"gorilla\", \"chimpanzee\", \"orangutan\"). We compute the frequency of misclassification of people into one of the harmful categories and run these experiments on FairFace and UTKFace separately. The results are shown in Table 21. Unlike in Radford et al. [111], we find that our models have a very small probability of classifying human faces as non-human, with a max score across all subgroups of 0.1%. However, a significant proportion of people are misclassified as criminal. This again highlights the importance of dataset curation and the risks associated with zero-shot classification on models trained on web-scraped datasets."}]}, {"page": 52, "text": "Table 21: Harmful misclassification rates of our two best xlarge baselines, Image-based \u2229                     CLIP score\n(L/14 30%) for the filtering track and COMMONPOOL, CLIP score + 4 external sources (upsampled\n6x) for the BYOD track. While very few samples are misclassified as non-human, the filter track\nmodel assigns a crime-related label to a significant portion of people, and this is exacerbated by the\nBYOD model in many cases.\n                                                          FairFace\n  Track                                                                   Race\n                              Black   White    Indian   Latino/Hispanic    Middle Eastern    Southeast Asian   East Asian\n  Filtering  Crime-related     4.4     24.3      8.8          14.3               23.7               7.4            8.6\n             Non-human         0.0      0.0      0.0           0.0               0.0                0.0            0.0\n  BYOD       Crime-related     18.4    16.8     21.5          22.9               20.9               35.3          30.9\n             Non-human         0.0      0.1      0.0           0.1               0.0                0.1            0.1\n                                                         UTKFace\n                         Track                                             Race\n                                                        Black    White    Indian    Asian    Other\n                         Filtering   Crime-related       6.8      16.1      9.1       6.9     13.9\n                                     Non-human           0.0       0.2      0.0       0.1      0.0\n                         BYOD        Crime-related       12.8     10.8      15.2     13.2     18.6\n                                     Non-human           0.0       0.2      0.0       0.0      0.0\n                                                             52", "md": "|Track| | | | | |Race|\n|---|---|---|---|---|---|---|\n| | |Black|White|Indian|Latino/Hispanic|Middle Eastern|Southeast Asian|East Asian|\n|Filtering|Crime-related|4.4|24.3|8.8|14.3|23.7|7.4|8.6|\n| |Non-human|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n|BYOD|Crime-related|18.4|16.8|21.5|22.9|20.9|35.3|30.9|\n| |Non-human|0.0|0.1|0.0|0.1|0.0|0.1|0.1|\n\n|Track| | | |Race|\n|---|---|---|---|---|\n| | |Black|White|Indian|Asian|Other|\n|Filtering|Crime-related|6.8|16.1|9.1|6.9|13.9|\n| |Non-human|0.0|0.2|0.0|0.1|0.0|\n|BYOD|Crime-related|12.8|10.8|15.2|13.2|18.6|\n| |Non-human|0.0|0.2|0.0|0.0|0.0|", "images": [], "items": [{"type": "table", "rows": [["Track", "", "", "", "", "", "Race"], ["", "", "Black", "White", "Indian", "Latino/Hispanic", "Middle Eastern", "Southeast Asian", "East Asian"], ["Filtering", "Crime-related", "4.4", "24.3", "8.8", "14.3", "23.7", "7.4", "8.6"], ["", "Non-human", "0.0", "0.0", "0.0", "0.0", "0.0", "0.0", "0.0"], ["BYOD", "Crime-related", "18.4", "16.8", "21.5", "22.9", "20.9", "35.3", "30.9"], ["", "Non-human", "0.0", "0.1", "0.0", "0.1", "0.0", "0.1", "0.1"]], "md": "|Track| | | | | |Race|\n|---|---|---|---|---|---|---|\n| | |Black|White|Indian|Latino/Hispanic|Middle Eastern|Southeast Asian|East Asian|\n|Filtering|Crime-related|4.4|24.3|8.8|14.3|23.7|7.4|8.6|\n| |Non-human|0.0|0.0|0.0|0.0|0.0|0.0|0.0|\n|BYOD|Crime-related|18.4|16.8|21.5|22.9|20.9|35.3|30.9|\n| |Non-human|0.0|0.1|0.0|0.1|0.0|0.1|0.1|", "isPerfectTable": false, "csv": "\"Track\",\"\",\"\",\"\",\"\",\"\",\"Race\"\n\"\",\"\",\"Black\",\"White\",\"Indian\",\"Latino/Hispanic\",\"Middle Eastern\",\"Southeast Asian\",\"East Asian\"\n\"Filtering\",\"Crime-related\",\"4.4\",\"24.3\",\"8.8\",\"14.3\",\"23.7\",\"7.4\",\"8.6\"\n\"\",\"Non-human\",\"0.0\",\"0.0\",\"0.0\",\"0.0\",\"0.0\",\"0.0\",\"0.0\"\n\"BYOD\",\"Crime-related\",\"18.4\",\"16.8\",\"21.5\",\"22.9\",\"20.9\",\"35.3\",\"30.9\"\n\"\",\"Non-human\",\"0.0\",\"0.1\",\"0.0\",\"0.1\",\"0.0\",\"0.1\",\"0.1\""}, {"type": "table", "rows": [["Track", "", "", "", "Race"], ["", "", "Black", "White", "Indian", "Asian", "Other"], ["Filtering", "Crime-related", "6.8", "16.1", "9.1", "6.9", "13.9"], ["", "Non-human", "0.0", "0.2", "0.0", "0.1", "0.0"], ["BYOD", "Crime-related", "12.8", "10.8", "15.2", "13.2", "18.6"], ["", "Non-human", "0.0", "0.2", "0.0", "0.0", "0.0"]], "md": "|Track| | | |Race|\n|---|---|---|---|---|\n| | |Black|White|Indian|Asian|Other|\n|Filtering|Crime-related|6.8|16.1|9.1|6.9|13.9|\n| |Non-human|0.0|0.2|0.0|0.1|0.0|\n|BYOD|Crime-related|12.8|10.8|15.2|13.2|18.6|\n| |Non-human|0.0|0.2|0.0|0.0|0.0|", "isPerfectTable": false, "csv": "\"Track\",\"\",\"\",\"\",\"Race\"\n\"\",\"\",\"Black\",\"White\",\"Indian\",\"Asian\",\"Other\"\n\"Filtering\",\"Crime-related\",\"6.8\",\"16.1\",\"9.1\",\"6.9\",\"13.9\"\n\"\",\"Non-human\",\"0.0\",\"0.2\",\"0.0\",\"0.1\",\"0.0\"\n\"BYOD\",\"Crime-related\",\"12.8\",\"10.8\",\"15.2\",\"13.2\",\"18.6\"\n\"\",\"Non-human\",\"0.0\",\"0.2\",\"0.0\",\"0.0\",\"0.0\""}]}, {"page": 53, "text": "R      Extra figures and tables\n   ImageNet acc. (medium)\n      small vs medium (ImageNet)                      medium vs large (ImageNet)                        small vs large (ImageNet)\n    0.3                                           ImageNet acc. (large)                          ImageNet acc. (large)\n    0.2                                            0.55                                            0.55\n                                                   0.50                                            0.50\n    0.1                                            0.45                                            0.45\n       0.00       0.02       0.04                       0.10    0.15    0.20     0.25                          0.02         0.04\n             ImageNet acc. (small)                          ImageNet acc. (medium)                           ImageNet acc. (small)\n   Avg. pref. metric (medium)\n        small vs medium (Average)                      medium vs large (Average)                         small vs large (Average)\n                                                  Avg. pref. metric (large)                      Avg. pref. metric (large)\n    0.3                                            0.50                                            0.50\n    0.2                                            0.45                                            0.45\n                                                   0.40                                            0.40\n    0.1        0.10           0.15                            0.20     0.25     0.30                     0.100    0.125     0.150    0.175\n            Avg. pref. metric (small)                      Avg. pref. metric (medium)                       Avg. pref. metric (small)\n                    Basic                Image-based                No filtering          Rand. subset            Text-based\n                    CLIP score           ImageNet dist.\nFigure 22:         Improving downstream performance at smaller scales correlates positively with\nperformance gains at larger scales. These trends suggests that dataset filtering can be studied\neffectively at smaller scales, even with less computational resources.\nTable 22: Rank correlation between the performance obtained with various filtering strategies at two\ndifferent scales. Our experimental suggest that the ranking is relatively consistent between scales,\nespecially for the adjacent scale pairs.\n                 Metric                         small vs medium            small vs large           medium vs large\n                 ImageNet acc.                         0.895                      0.811                     0.847\n                 Average pref. metric                  0.854                      0.708                     0.876\n                                                                     53", "md": "# Extra figures and tables\n\n## ImageNet Accuracy Comparison\n\n| |Small vs Medium (ImageNet)|Medium vs Large (ImageNet)|Small vs Large (ImageNet)|\n|---|---|---|---|\n|0.3| |ImageNet acc. (large)|ImageNet acc. (large)|\n|0.2| |0.55|0.55|\n| | |0.50|0.50|\n|0.1| |0.45|0.45|\n|0.00|0.02|0.10|0.02|\n| |0.04|0.15|0.04|\n\n## Average Preference Metric Comparison (Medium)\n\n| |Small vs Medium (Average)|Medium vs Large (Average)|Small vs Large (Average)|\n|---|---|---|---|\n| |Avg. pref. metric (large)|Avg. pref. metric (large)| |\n|0.3|0.50|0.50| |\n|0.2|0.45|0.45| |\n| |0.40|0.40| |\n|0.1|0.10|0.20|0.100|\n| |0.15|0.25|0.125|\n| | |0.30|0.150|\n| | | |0.175|\n\n## Improving Downstream Performance\n\nImproving downstream performance at smaller scales correlates positively with performance gains at larger scales. These trends suggest that dataset filtering can be studied effectively at smaller scales, even with fewer computational resources.\n\n## Rank Correlation Table\n\nRank correlation between the performance obtained with various filtering strategies at two different scales. Our experiments suggest that the ranking is relatively consistent between scales, especially for the adjacent scale pairs.\n\n|Metric|Small vs Medium|Small vs Large|Medium vs Large|\n|---|---|---|---|\n|ImageNet acc.|0.895|0.811|0.847|\n|Average pref. metric|0.854|0.708|0.876|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Extra figures and tables", "md": "# Extra figures and tables"}, {"type": "heading", "lvl": 2, "value": "ImageNet Accuracy Comparison", "md": "## ImageNet Accuracy Comparison"}, {"type": "table", "rows": [["", "Small vs Medium (ImageNet)", "Medium vs Large (ImageNet)", "Small vs Large (ImageNet)"], ["0.3", "", "ImageNet acc. (large)", "ImageNet acc. (large)"], ["0.2", "", "0.55", "0.55"], ["", "", "0.50", "0.50"], ["0.1", "", "0.45", "0.45"], ["0.00", "0.02", "0.10", "0.02"], ["", "0.04", "0.15", "0.04"]], "md": "| |Small vs Medium (ImageNet)|Medium vs Large (ImageNet)|Small vs Large (ImageNet)|\n|---|---|---|---|\n|0.3| |ImageNet acc. (large)|ImageNet acc. (large)|\n|0.2| |0.55|0.55|\n| | |0.50|0.50|\n|0.1| |0.45|0.45|\n|0.00|0.02|0.10|0.02|\n| |0.04|0.15|0.04|", "isPerfectTable": true, "csv": "\"\",\"Small vs Medium (ImageNet)\",\"Medium vs Large (ImageNet)\",\"Small vs Large (ImageNet)\"\n\"0.3\",\"\",\"ImageNet acc. (large)\",\"ImageNet acc. (large)\"\n\"0.2\",\"\",\"0.55\",\"0.55\"\n\"\",\"\",\"0.50\",\"0.50\"\n\"0.1\",\"\",\"0.45\",\"0.45\"\n\"0.00\",\"0.02\",\"0.10\",\"0.02\"\n\"\",\"0.04\",\"0.15\",\"0.04\""}, {"type": "heading", "lvl": 2, "value": "Average Preference Metric Comparison (Medium)", "md": "## Average Preference Metric Comparison (Medium)"}, {"type": "table", "rows": [["", "Small vs Medium (Average)", "Medium vs Large (Average)", "Small vs Large (Average)"], ["", "Avg. pref. metric (large)", "Avg. pref. metric (large)", ""], ["0.3", "0.50", "0.50", ""], ["0.2", "0.45", "0.45", ""], ["", "0.40", "0.40", ""], ["0.1", "0.10", "0.20", "0.100"], ["", "0.15", "0.25", "0.125"], ["", "", "0.30", "0.150"], ["", "", "", "0.175"]], "md": "| |Small vs Medium (Average)|Medium vs Large (Average)|Small vs Large (Average)|\n|---|---|---|---|\n| |Avg. pref. metric (large)|Avg. pref. metric (large)| |\n|0.3|0.50|0.50| |\n|0.2|0.45|0.45| |\n| |0.40|0.40| |\n|0.1|0.10|0.20|0.100|\n| |0.15|0.25|0.125|\n| | |0.30|0.150|\n| | | |0.175|", "isPerfectTable": true, "csv": "\"\",\"Small vs Medium (Average)\",\"Medium vs Large (Average)\",\"Small vs Large (Average)\"\n\"\",\"Avg. pref. metric (large)\",\"Avg. pref. metric (large)\",\"\"\n\"0.3\",\"0.50\",\"0.50\",\"\"\n\"0.2\",\"0.45\",\"0.45\",\"\"\n\"\",\"0.40\",\"0.40\",\"\"\n\"0.1\",\"0.10\",\"0.20\",\"0.100\"\n\"\",\"0.15\",\"0.25\",\"0.125\"\n\"\",\"\",\"0.30\",\"0.150\"\n\"\",\"\",\"\",\"0.175\""}, {"type": "heading", "lvl": 2, "value": "Improving Downstream Performance", "md": "## Improving Downstream Performance"}, {"type": "text", "value": "Improving downstream performance at smaller scales correlates positively with performance gains at larger scales. These trends suggest that dataset filtering can be studied effectively at smaller scales, even with fewer computational resources.", "md": "Improving downstream performance at smaller scales correlates positively with performance gains at larger scales. These trends suggest that dataset filtering can be studied effectively at smaller scales, even with fewer computational resources."}, {"type": "heading", "lvl": 2, "value": "Rank Correlation Table", "md": "## Rank Correlation Table"}, {"type": "text", "value": "Rank correlation between the performance obtained with various filtering strategies at two different scales. Our experiments suggest that the ranking is relatively consistent between scales, especially for the adjacent scale pairs.", "md": "Rank correlation between the performance obtained with various filtering strategies at two different scales. Our experiments suggest that the ranking is relatively consistent between scales, especially for the adjacent scale pairs."}, {"type": "table", "rows": [["Metric", "Small vs Medium", "Small vs Large", "Medium vs Large"], ["ImageNet acc.", "0.895", "0.811", "0.847"], ["Average pref. metric", "0.854", "0.708", "0.876"]], "md": "|Metric|Small vs Medium|Small vs Large|Medium vs Large|\n|---|---|---|---|\n|ImageNet acc.|0.895|0.811|0.847|\n|Average pref. metric|0.854|0.708|0.876|", "isPerfectTable": true, "csv": "\"Metric\",\"Small vs Medium\",\"Small vs Large\",\"Medium vs Large\"\n\"ImageNet acc.\",\"0.895\",\"0.811\",\"0.847\"\n\"Average pref. metric\",\"0.854\",\"0.708\",\"0.876\""}]}, {"page": 54, "text": "                         ImageNet                               Average over 38 datasets\n    ImageNet accuracy                                   Average performance\n      0.05                                               0.16                                                      No filtering\n      0.04                                               0.14                                                      Basic\n      0.03                                                                                                         CLIP score\n                                                         0.12                                                      Image-based\n      0.02                                                                                                         Text-based\n      0.01                                               0.10                                                      Rand. subset\n      0.00                  106               107        0.08                   106              107\n                 Dataset size (log scale)                            Dataset size (log scale)\n      0.3              ImageNet                         0.35   Average over 38 datasets\n    ImageNet accuracy                                  Average performance\n                                                        0.30                                                       No filtering\n      0.2                                               0.25                                                       Basic\n                                                                                                                   CLIP score\n                                                        0.20                                                       Image-based\n      0.1                                                                                                          Text-based\n                                                        0.15                                                       Rand. subset\n      0.0                  107               108        0.10                   107               108\n               Dataset size (log scale)                             Dataset size (log scale)\n      0.6              ImageNet                                Average over 38 datasets\n                                                        Average performance\n    ImageNet accuracy                                     0.5\n      0.5                                                                                                          No filtering\n                                                                                                                   Basic\n      0.4                                                 0.4                                                      CLIP score\n                                                                                                                   Image-based\n      0.3                                                 0.3                                                      Text-based\n      0.2                                                                                                          Rand. subset\n                           108               109                               108               109\n               Dataset size (log scale)                             Dataset size (log scale)\nFigure 23: Performance as a function of the number of training samples from the small (top), medium\n(middle), and large (bottom) scales. There is a significant variance in accuracy even when accounting\nfor the size of the training set.\n      Table 23: Comparison of ViT-B/32 and ViT-B/16 models across different training datasets.\n   Model         Training Dataset         Training       Training      ImageNet        ImageNet       VTAB    Retrieval      Average over\n                                        dataset size       steps                      dist. shifts                            38 datasets\n   ViT B/32       DATACOMP-1B               1.4B            13B          0.692           0.551         0.577    0.538            0.579\n   ViT B/32       OpenAI\u2019s WIT              0.4B            13B          0.633           0.485         0.526    0.501            0.525\n   ViT B/32         LAION-2B                2.3B            34B          0.666           0.522         0.561    0.560            0.569\n   ViT B/16       DATACOMP-1B               1.4B            13B          0.735           0.608         0.621    0.578            0.615\n   ViT B/16       OpenAI\u2019s WIT              0.4B            13B          0.683           0.559         0.546    0.527            0.563\n   ViT B/16         LAION-2B                2.3B            34B          0.702           0.566         0.572    0.583            0.587\n                                                                       54", "md": "# Performance Comparison\n\n## Performance Comparison\n\n### ImageNet vs. Average over 38 datasets\n\n|ImageNet accuracy|Average performance|\n|---|---|\n|0.05|0.16|\n|0.04|0.14|\n|0.03|CLIP score|\n| |0.12|\n|0.02|Image-based|\n| |Text-based|\n|0.01|0.10|\n|0.00|106|107|0.08|106|107|\n|Dataset size (log scale)|Dataset size (log scale)|\n\n### ImageNet vs. Average over 38 datasets\n\n|ImageNet accuracy|Average performance|\n|---|---|\n|0.3|0.35|\n|0.2|0.25|\n| |CLIP score|\n| |0.20|\n|0.1|Image-based|\n| |Text-based|\n|0.0|107|108|0.10|107|108|\n|Dataset size (log scale)|Dataset size (log scale)|\n\n### ImageNet vs. Average over 38 datasets\n\n|ImageNet accuracy|Average performance|\n|---|---|\n|0.5|No filtering|\n|0.4|Basic|\n|0.3|CLIP score|\n|0.2|Image-based|\n|0.3|Text-based|\n|0.2|Rand. subset|\n|108|109|108|109|\n|Dataset size (log scale)|Dataset size (log scale)|\n\n### Performance as a function of the number of training samples\n\nFigure 23: Performance as a function of the number of training samples from the small (top), medium (middle), and large (bottom) scales. There is a significant variance in accuracy even when accounting for the size of the training set.\n\n### Comparison of ViT-B/32 and ViT-B/16 models\n\n|Model|Training Dataset|Training dataset size|Training steps|ImageNet dist.|ImageNet shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|---|---|\n|ViT B/32|DATACOMP-1B|1.4B|13B|0.692|0.551|0.577|0.538|0.579|\n|ViT B/32|OpenAI\u2019s WIT|0.4B|13B|0.633|0.485|0.526|0.501|0.525|\n|ViT B/32|LAION-2B|2.3B|34B|0.666|0.522|0.561|0.560|0.569|\n|ViT B/16|DATACOMP-1B|1.4B|13B|0.735|0.608|0.621|0.578|0.615|\n|ViT B/16|OpenAI\u2019s WIT|0.4B|13B|0.683|0.559|0.546|0.527|0.563|\n|ViT B/16|LAION-2B|2.3B|34B|0.702|0.566|0.572|0.583|0.587|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Performance Comparison", "md": "# Performance Comparison"}, {"type": "heading", "lvl": 2, "value": "Performance Comparison", "md": "## Performance Comparison"}, {"type": "heading", "lvl": 3, "value": "ImageNet vs. Average over 38 datasets", "md": "### ImageNet vs. Average over 38 datasets"}, {"type": "table", "rows": [["ImageNet accuracy", "Average performance"], ["0.05", "0.16"], ["0.04", "0.14"], ["0.03", "CLIP score"], ["", "0.12"], ["0.02", "Image-based"], ["", "Text-based"], ["0.01", "0.10"], ["0.00", "106", "107", "0.08", "106", "107"], ["Dataset size (log scale)", "Dataset size (log scale)"]], "md": "|ImageNet accuracy|Average performance|\n|---|---|\n|0.05|0.16|\n|0.04|0.14|\n|0.03|CLIP score|\n| |0.12|\n|0.02|Image-based|\n| |Text-based|\n|0.01|0.10|\n|0.00|106|107|0.08|106|107|\n|Dataset size (log scale)|Dataset size (log scale)|", "isPerfectTable": false, "csv": "\"ImageNet accuracy\",\"Average performance\"\n\"0.05\",\"0.16\"\n\"0.04\",\"0.14\"\n\"0.03\",\"CLIP score\"\n\"\",\"0.12\"\n\"0.02\",\"Image-based\"\n\"\",\"Text-based\"\n\"0.01\",\"0.10\"\n\"0.00\",\"106\",\"107\",\"0.08\",\"106\",\"107\"\n\"Dataset size (log scale)\",\"Dataset size (log scale)\""}, {"type": "heading", "lvl": 3, "value": "ImageNet vs. Average over 38 datasets", "md": "### ImageNet vs. Average over 38 datasets"}, {"type": "table", "rows": [["ImageNet accuracy", "Average performance"], ["0.3", "0.35"], ["0.2", "0.25"], ["", "CLIP score"], ["", "0.20"], ["0.1", "Image-based"], ["", "Text-based"], ["0.0", "107", "108", "0.10", "107", "108"], ["Dataset size (log scale)", "Dataset size (log scale)"]], "md": "|ImageNet accuracy|Average performance|\n|---|---|\n|0.3|0.35|\n|0.2|0.25|\n| |CLIP score|\n| |0.20|\n|0.1|Image-based|\n| |Text-based|\n|0.0|107|108|0.10|107|108|\n|Dataset size (log scale)|Dataset size (log scale)|", "isPerfectTable": false, "csv": "\"ImageNet accuracy\",\"Average performance\"\n\"0.3\",\"0.35\"\n\"0.2\",\"0.25\"\n\"\",\"CLIP score\"\n\"\",\"0.20\"\n\"0.1\",\"Image-based\"\n\"\",\"Text-based\"\n\"0.0\",\"107\",\"108\",\"0.10\",\"107\",\"108\"\n\"Dataset size (log scale)\",\"Dataset size (log scale)\""}, {"type": "heading", "lvl": 3, "value": "ImageNet vs. Average over 38 datasets", "md": "### ImageNet vs. Average over 38 datasets"}, {"type": "table", "rows": [["ImageNet accuracy", "Average performance"], ["0.5", "No filtering"], ["0.4", "Basic"], ["0.3", "CLIP score"], ["0.2", "Image-based"], ["0.3", "Text-based"], ["0.2", "Rand. subset"], ["108", "109", "108", "109"], ["Dataset size (log scale)", "Dataset size (log scale)"]], "md": "|ImageNet accuracy|Average performance|\n|---|---|\n|0.5|No filtering|\n|0.4|Basic|\n|0.3|CLIP score|\n|0.2|Image-based|\n|0.3|Text-based|\n|0.2|Rand. subset|\n|108|109|108|109|\n|Dataset size (log scale)|Dataset size (log scale)|", "isPerfectTable": false, "csv": "\"ImageNet accuracy\",\"Average performance\"\n\"0.5\",\"No filtering\"\n\"0.4\",\"Basic\"\n\"0.3\",\"CLIP score\"\n\"0.2\",\"Image-based\"\n\"0.3\",\"Text-based\"\n\"0.2\",\"Rand. subset\"\n\"108\",\"109\",\"108\",\"109\"\n\"Dataset size (log scale)\",\"Dataset size (log scale)\""}, {"type": "heading", "lvl": 3, "value": "Performance as a function of the number of training samples", "md": "### Performance as a function of the number of training samples"}, {"type": "text", "value": "Figure 23: Performance as a function of the number of training samples from the small (top), medium (middle), and large (bottom) scales. There is a significant variance in accuracy even when accounting for the size of the training set.", "md": "Figure 23: Performance as a function of the number of training samples from the small (top), medium (middle), and large (bottom) scales. There is a significant variance in accuracy even when accounting for the size of the training set."}, {"type": "heading", "lvl": 3, "value": "Comparison of ViT-B/32 and ViT-B/16 models", "md": "### Comparison of ViT-B/32 and ViT-B/16 models"}, {"type": "table", "rows": [["Model", "Training Dataset", "Training dataset size", "Training steps", "ImageNet dist.", "ImageNet shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["ViT B/32", "DATACOMP-1B", "1.4B", "13B", "0.692", "0.551", "0.577", "0.538", "0.579"], ["ViT B/32", "OpenAI\u2019s WIT", "0.4B", "13B", "0.633", "0.485", "0.526", "0.501", "0.525"], ["ViT B/32", "LAION-2B", "2.3B", "34B", "0.666", "0.522", "0.561", "0.560", "0.569"], ["ViT B/16", "DATACOMP-1B", "1.4B", "13B", "0.735", "0.608", "0.621", "0.578", "0.615"], ["ViT B/16", "OpenAI\u2019s WIT", "0.4B", "13B", "0.683", "0.559", "0.546", "0.527", "0.563"], ["ViT B/16", "LAION-2B", "2.3B", "34B", "0.702", "0.566", "0.572", "0.583", "0.587"]], "md": "|Model|Training Dataset|Training dataset size|Training steps|ImageNet dist.|ImageNet shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|---|---|\n|ViT B/32|DATACOMP-1B|1.4B|13B|0.692|0.551|0.577|0.538|0.579|\n|ViT B/32|OpenAI\u2019s WIT|0.4B|13B|0.633|0.485|0.526|0.501|0.525|\n|ViT B/32|LAION-2B|2.3B|34B|0.666|0.522|0.561|0.560|0.569|\n|ViT B/16|DATACOMP-1B|1.4B|13B|0.735|0.608|0.621|0.578|0.615|\n|ViT B/16|OpenAI\u2019s WIT|0.4B|13B|0.683|0.559|0.546|0.527|0.563|\n|ViT B/16|LAION-2B|2.3B|34B|0.702|0.566|0.572|0.583|0.587|", "isPerfectTable": true, "csv": "\"Model\",\"Training Dataset\",\"Training dataset size\",\"Training steps\",\"ImageNet dist.\",\"ImageNet shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"ViT B/32\",\"DATACOMP-1B\",\"1.4B\",\"13B\",\"0.692\",\"0.551\",\"0.577\",\"0.538\",\"0.579\"\n\"ViT B/32\",\"OpenAI\u2019s WIT\",\"0.4B\",\"13B\",\"0.633\",\"0.485\",\"0.526\",\"0.501\",\"0.525\"\n\"ViT B/32\",\"LAION-2B\",\"2.3B\",\"34B\",\"0.666\",\"0.522\",\"0.561\",\"0.560\",\"0.569\"\n\"ViT B/16\",\"DATACOMP-1B\",\"1.4B\",\"13B\",\"0.735\",\"0.608\",\"0.621\",\"0.578\",\"0.615\"\n\"ViT B/16\",\"OpenAI\u2019s WIT\",\"0.4B\",\"13B\",\"0.683\",\"0.559\",\"0.546\",\"0.527\",\"0.563\"\n\"ViT B/16\",\"LAION-2B\",\"2.3B\",\"34B\",\"0.702\",\"0.566\",\"0.572\",\"0.583\",\"0.587\""}]}, {"page": 55, "text": "                                    fasttext                   0.40             gcld3\n                    detected as English     ViT-B/32          detected as English       ViT-B/32\n                  Percentage of text                       Percentage of text\n                     0.7                    ViT-L/14           0.35                     ViT-L/14\n                                                               0.30\n                     0.6\n                                                               0.25\n                     0.5                                       0.20\n                             0.25    0.50   0.75   1.00                  0.25   0.50   0.75    1.00\n                            Fraction remaining after                   Fraction remaining after\n                               CLIP score filtering                       CLIP score filtering\nFigure 24: We examine the percentage of texts classified as English after taking the top fraction (on\nthe x-axis) of the large billion pool as sorted by CLIP similarity score. We see that doing CLIP\nfiltering implicitly does some English filtering, as image-text pairs with a higher CLIP score are more\nfrequently classified as English.\n                      clean                                       full\n   Average pref. metric (clean)               Average pref. metric (full)\n     0.6                                       0.6                                               No filtering\n                                                                                                 Basic\n                                                                                                 CLIP score\n     0.4                                       0.4                                               Image-based\n                                                                                                 Text-based\n     0.2                                       0.2                                               Rand. subset\n                                                                                                 ImageNet dist.\n         0.00     0.25     0.50     0.75           0.00     0.25      0.50     0.75\n               ImageNet accuracy                         ImageNet accuracy\nFigure 25: Correlation between ImageNet accuracy and average performance on our suite of\nevaluation tasks. While ImageNet accuracy strongly correlates with the average performance (both\non the clean subset and the full suite), the same is not true for all individual datasets we study, as\nshown in Appendix R.\n                      0.6\n                     Avg. accuracy on 5 OOD sets\n                      0.5\n                      0.4\n                      0.3\n                      0.2\n                      0.1\n                      0.0 0.0        0.1        0.2        0.3        0.4        0.5        0.6\n                                                   ImageNet accuracy\n                          x=y                          Basic                    Text-based\n                          ImageNet models              CLIP score               Rand. subset\n                          No filtering                 Image-based              ImageNet dist.\nFigure 26: Zero-shot CLIP models trained with various filtering strategies form a reliable trend\nrelating accuracy on ImageNet and related distribution shifts, exhibiting higher effective robustness\nwhen compared to ImageNet-trained models from Taori et al. [139].\n                                                        55", "md": "# OCR Text Analysis\n\n## Figure 24\n\nWe examine the percentage of texts classified as English after taking the top fraction (on the x-axis) of the large billion pool as sorted by CLIP similarity score. We see that doing CLIP filtering implicitly does some English filtering, as image-text pairs with a higher CLIP score are more frequently classified as English.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{Fraction remaining after CLIP score filtering} & 0.25 & 0.50 & 0.75 & 1.00 \\\\\n\\hline\n\\text{Percentage of text (ViT-L/14)} & 0.35 & 0.30 & & \\\\\n\\hline\n\\text{Percentage of text (ViT-B/32)} & 0.40 & & & \\\\\n\\hline\n\\end{array}\n$$\n\n## Figure 25\n\nCorrelation between ImageNet accuracy and average performance on our suite of evaluation tasks. While ImageNet accuracy strongly correlates with the average performance (both on the clean subset and the full suite), the same is not true for all individual datasets we study, as shown in Appendix R.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{ImageNet accuracy} & 0.00 & 0.25 & 0.50 & 0.75 \\\\\n\\hline\n\\text{ImageNet accuracy} & & & & \\\\\n\\hline\n\\end{array}\n$$\n\n## Figure 26\n\nZero-shot CLIP models trained with various filtering strategies form a reliable trend relating accuracy on ImageNet and related distribution shifts, exhibiting higher effective robustness when compared to ImageNet-trained models from Taori et al. [139].\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{ImageNet accuracy} & 0.6 \\\\\n\\hline\n\\text{Avg. accuracy on 5 OOD sets} & 0.4 \\\\\n\\hline\n\\end{array}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text Analysis", "md": "# OCR Text Analysis"}, {"type": "heading", "lvl": 2, "value": "Figure 24", "md": "## Figure 24"}, {"type": "text", "value": "We examine the percentage of texts classified as English after taking the top fraction (on the x-axis) of the large billion pool as sorted by CLIP similarity score. We see that doing CLIP filtering implicitly does some English filtering, as image-text pairs with a higher CLIP score are more frequently classified as English.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{Fraction remaining after CLIP score filtering} & 0.25 & 0.50 & 0.75 & 1.00 \\\\\n\\hline\n\\text{Percentage of text (ViT-L/14)} & 0.35 & 0.30 & & \\\\\n\\hline\n\\text{Percentage of text (ViT-B/32)} & 0.40 & & & \\\\\n\\hline\n\\end{array}\n$$", "md": "We examine the percentage of texts classified as English after taking the top fraction (on the x-axis) of the large billion pool as sorted by CLIP similarity score. We see that doing CLIP filtering implicitly does some English filtering, as image-text pairs with a higher CLIP score are more frequently classified as English.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{Fraction remaining after CLIP score filtering} & 0.25 & 0.50 & 0.75 & 1.00 \\\\\n\\hline\n\\text{Percentage of text (ViT-L/14)} & 0.35 & 0.30 & & \\\\\n\\hline\n\\text{Percentage of text (ViT-B/32)} & 0.40 & & & \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "heading", "lvl": 2, "value": "Figure 25", "md": "## Figure 25"}, {"type": "text", "value": "Correlation between ImageNet accuracy and average performance on our suite of evaluation tasks. While ImageNet accuracy strongly correlates with the average performance (both on the clean subset and the full suite), the same is not true for all individual datasets we study, as shown in Appendix R.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{ImageNet accuracy} & 0.00 & 0.25 & 0.50 & 0.75 \\\\\n\\hline\n\\text{ImageNet accuracy} & & & & \\\\\n\\hline\n\\end{array}\n$$", "md": "Correlation between ImageNet accuracy and average performance on our suite of evaluation tasks. While ImageNet accuracy strongly correlates with the average performance (both on the clean subset and the full suite), the same is not true for all individual datasets we study, as shown in Appendix R.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{ImageNet accuracy} & 0.00 & 0.25 & 0.50 & 0.75 \\\\\n\\hline\n\\text{ImageNet accuracy} & & & & \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "heading", "lvl": 2, "value": "Figure 26", "md": "## Figure 26"}, {"type": "text", "value": "Zero-shot CLIP models trained with various filtering strategies form a reliable trend relating accuracy on ImageNet and related distribution shifts, exhibiting higher effective robustness when compared to ImageNet-trained models from Taori et al. [139].\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{ImageNet accuracy} & 0.6 \\\\\n\\hline\n\\text{Avg. accuracy on 5 OOD sets} & 0.4 \\\\\n\\hline\n\\end{array}\n$$", "md": "Zero-shot CLIP models trained with various filtering strategies form a reliable trend relating accuracy on ImageNet and related distribution shifts, exhibiting higher effective robustness when compared to ImageNet-trained models from Taori et al. [139].\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{ImageNet accuracy} & 0.6 \\\\\n\\hline\n\\text{Avg. accuracy on 5 OOD sets} & 0.4 \\\\\n\\hline\n\\end{array}\n$$"}]}, {"page": 56, "text": "Table 24: Comparison at the xlarge scale between a 400M subset of COMMONPOOL and OpenAI\u2019s\nWIT which also contains 400M samples. Our 400M subset is created by intersecting IN1k image\nclustering with English cld3 filtering, then taking the top 400M samples sorted by CLIP L14 score.\nOur model does better across the various evaluation groupings.\n     Model                                                    Training Dataset                                                         Training            Training            ImageNet                    ImageNet             VTAB                Retrieval                 Average over\n                                                                                                                                    dataset size               steps                                       dist. shifts                                                         38 datasets\n     ViT L/14                  top 400M by CLIP L14 of Image-based \u2229                                                cld3                 400M                  13B                  0.763                      0.657             0.641                  0.595                         0.638\n     ViT L/14                                                   OpenAI\u2019s WIT                                                             400M                  13B                  0.755                      0.649             0.586                  0.543                         0.617\n     1.0                              CIFAR-10                                        1.0                          CIFAR-100                                 1.0                      CLEVR Counts                                 1.0                         CLEVR Distance\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n    Accuracy                                                                        Accuracy                                                               Accuracy                                                              Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                              0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                            Caltech-101                                       1.0                        Camelyon17                                  1.0                        Country211                                 1.0                      Describable Textures\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n    Mean per class\n                                                                                    Accuracy                                                               Accuracy                                                              Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                              0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                           Dollar Street                                      1.0                           EuroSAT                                  1.0                       FGVC Aircraft                               1.0                               FMoW\n    Worst-income top-5 acc.\n                                                                                                                                                                                                                                 Worst-region acc.\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n                                                                                                                                                           Mean per class\n                                                                                    Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                              0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                      Flickr image retrieval                                  1.0                    Flickr text retrieval                           1.0                         Food-101                                  1.0                               GTSRB\n    Image retrieval R@1                                                             ext retrieval R@1\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n                                                                                                                                                           Accuracy                                                              Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                            T 0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                               GeoDE                                          1.0                     ImageNet Sketch                                1.0                       ImageNet v2                                 1.0                            ImageNet-A\n    Worst-region acc.\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n                                                                                    Accuracy                                                               Accuracy                                                              Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                              0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                            ImageNet-O                                        1.0                         ImageNet-R                                 1.0                 KITTI Vehicle Distance                            1.0                               MNIST\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n    Accuracy                                                                        Accuracy                                                               Accuracy                                                              Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                              0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                   MSCOCO image retrieval                                     1.0                  MSCOCO text retrieval                             1.0                         ObjectNet                                 1.0                       Oxford Flowers-102\n    Image retrieval R@1                                                             ext retrieval R@1\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n                                                                                                                                                                                                                                 Mean per class\n                                                                                                                                                           Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                            T 0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                          Oxford-IIIT Pet                                     1.0                     Pascal VOC 2007                                1.0                     PatchCamelyon                                 1.0                             RESISC45\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n    Mean per class\n                                                                                    Accuracy                                                               Accuracy                                                              Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                              0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                          Rendered SST2                                       1.0                            STL-10                                  1.0                          SUN397                                   1.0                                SVHN\n     0.8                                                                              0.8                                                                    0.8                                                                   0.8\n    Accuracy                                                                        Accuracy                                                               Accuracy                                                              Accuracy\n     0.6                                                                              0.6                                                                    0.6                                                                   0.6\n     0.4                                                                              0.4                                                                    0.4                                                                   0.4\n     0.2                                                                              0.2                                                                    0.2                                                                   0.2\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8     0.0        0.1     0.2      0.3     0.4      0.5     0.6  0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy                                                         ImageNet accuracy\n     1.0                           Stanford Cars                                      1.0                         WinoGAViL                                  1.0                         iWildCam\n     0.8                                                                              0.8                                                                    0.8                                                                                                    No filtering\n                                                                                    Jaccard Index                                                          Macro F1 score                                                                                           Basic\n    Accuracy                                                                                                                                                                                                                                                        CLIP score\n     0.6                                                                              0.6                                                                    0.6                                                                                                    Image-based\n     0.4                                                                              0.4                                                                    0.4                                                                                                    Text-based\n                                                                                                                                                                                                                                                                    Rand. subset\n     0.2                                                                              0.2                                                                    0.2                                                                                                    ImageNet dist.\n                                                                                                                                                                                                                                                                    Random performance\n     0.0        0.1      0.2     0.3      0.4     0.5     0.6      0.7     0.8        0.0        0.1   0.2     0.3     0.4      0.5     0.6      0.7  0.8    0.0        0.1  0.2     0.3     0.4      0.5     0.6     0.7  0.8\n                                 ImageNet accuracy                                                             ImageNet accuracy                                                     ImageNet accuracy\nFigure 27: Zero-shot performance on other datasets is often positively correlated with that on\nImageNet, but not always. In cases where ImageNet shows close to zero correlation with other\ndatasets, performance on that dataset is often close to random chance.", "md": "|Model|Training Dataset|Training dataset size|Training steps|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|---|\n|ViT L/14|top 400M by CLIP L14 of Image-based \u2229 cld3|400M| | |13B|0.763|0.657|0.641|0.595|0.638|\n|ViT L/14|OpenAI\u2019s WIT| | | |400M|13B|0.755|0.649|0.586|0.543|0.617|\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n1.0 & CIFAR-10 & 1.0 & CIFAR-100 & 1.0 & CLEVR Counts & 1.0 & CLEVR Distance \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Caltech-101 & 1.0 & Camelyon17 & 1.0 & Country211 & 1.0 & Describable Textures \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Mean per class} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Dollar Street & 1.0 & EuroSAT & 1.0 & FGVC Aircraft & 1.0 & FMoW \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Worst-income top-5 acc.} & & & & \\text{Mean per class} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Flickr image retrieval & 1.0 & Flickr text retrieval & 1.0 & Food-101 & 1.0 & GTSRB \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Image retrieval R@1} & \\text{ext retrieval R@1} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & T 0.2 & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & GeoDE & 1.0 & ImageNet Sketch & 1.0 & ImageNet v2 & 1.0 & ImageNet-A \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Worst-region acc.} & & & & \\text{Accuracy} & \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & ImageNet-O & 1.0 & ImageNet-R & 1.0 & KITTI Vehicle Distance & 1.0 & MNIST \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & MSCOCO image retrieval & 1.0 & MSCOCO text retrieval & 1.0 & ObjectNet & 1.0 & Oxford Flowers-102 \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Image retrieval R@1} & \\text{ext retrieval R@1} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & T 0.2 & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Oxford-IIIT Pet & 1.0 & Pascal VOC 2007 & 1.0 & PatchCamelyon & 1.0 & RESISC45 \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Mean per class} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & &", "images": [], "items": [{"type": "table", "rows": [["Model", "Training Dataset", "Training dataset size", "Training steps", "ImageNet dist. shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["ViT L/14", "top 400M by CLIP L14 of Image-based \u2229 cld3", "400M", "", "", "13B", "0.763", "0.657", "0.641", "0.595", "0.638"], ["ViT L/14", "OpenAI\u2019s WIT", "", "", "", "400M", "13B", "0.755", "0.649", "0.586", "0.543", "0.617"]], "md": "|Model|Training Dataset|Training dataset size|Training steps|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|---|\n|ViT L/14|top 400M by CLIP L14 of Image-based \u2229 cld3|400M| | |13B|0.763|0.657|0.641|0.595|0.638|\n|ViT L/14|OpenAI\u2019s WIT| | | |400M|13B|0.755|0.649|0.586|0.543|0.617|", "isPerfectTable": false, "csv": "\"Model\",\"Training Dataset\",\"Training dataset size\",\"Training steps\",\"ImageNet dist. shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"ViT L/14\",\"top 400M by CLIP L14 of Image-based \u2229 cld3\",\"400M\",\"\",\"\",\"13B\",\"0.763\",\"0.657\",\"0.641\",\"0.595\",\"0.638\"\n\"ViT L/14\",\"OpenAI\u2019s WIT\",\"\",\"\",\"\",\"400M\",\"13B\",\"0.755\",\"0.649\",\"0.586\",\"0.543\",\"0.617\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n1.0 & CIFAR-10 & 1.0 & CIFAR-100 & 1.0 & CLEVR Counts & 1.0 & CLEVR Distance \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Caltech-101 & 1.0 & Camelyon17 & 1.0 & Country211 & 1.0 & Describable Textures \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Mean per class} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Dollar Street & 1.0 & EuroSAT & 1.0 & FGVC Aircraft & 1.0 & FMoW \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Worst-income top-5 acc.} & & & & \\text{Mean per class} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Flickr image retrieval & 1.0 & Flickr text retrieval & 1.0 & Food-101 & 1.0 & GTSRB \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Image retrieval R@1} & \\text{ext retrieval R@1} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & T 0.2 & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & GeoDE & 1.0 & ImageNet Sketch & 1.0 & ImageNet v2 & 1.0 & ImageNet-A \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Worst-region acc.} & & & & \\text{Accuracy} & \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & ImageNet-O & 1.0 & ImageNet-R & 1.0 & KITTI Vehicle Distance & 1.0 & MNIST \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & MSCOCO image retrieval & 1.0 & MSCOCO text retrieval & 1.0 & ObjectNet & 1.0 & Oxford Flowers-102 \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Image retrieval R@1} & \\text{ext retrieval R@1} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & T 0.2 & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Oxford-IIIT Pet & 1.0 & Pascal VOC 2007 & 1.0 & PatchCamelyon & 1.0 & RESISC45 \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Mean per class} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & &", "md": "$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n1.0 & CIFAR-10 & 1.0 & CIFAR-100 & 1.0 & CLEVR Counts & 1.0 & CLEVR Distance \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Caltech-101 & 1.0 & Camelyon17 & 1.0 & Country211 & 1.0 & Describable Textures \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Mean per class} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Dollar Street & 1.0 & EuroSAT & 1.0 & FGVC Aircraft & 1.0 & FMoW \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Worst-income top-5 acc.} & & & & \\text{Mean per class} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Flickr image retrieval & 1.0 & Flickr text retrieval & 1.0 & Food-101 & 1.0 & GTSRB \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Image retrieval R@1} & \\text{ext retrieval R@1} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & T 0.2 & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & GeoDE & 1.0 & ImageNet Sketch & 1.0 & ImageNet v2 & 1.0 & ImageNet-A \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Worst-region acc.} & & & & \\text{Accuracy} & \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & ImageNet-O & 1.0 & ImageNet-R & 1.0 & KITTI Vehicle Distance & 1.0 & MNIST \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & MSCOCO image retrieval & 1.0 & MSCOCO text retrieval & 1.0 & ObjectNet & 1.0 & Oxford Flowers-102 \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Image retrieval R@1} & \\text{ext retrieval R@1} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & T 0.2 & 0.2 & & 0.2 & & 0.2 & \\\\\n0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\\\\n\\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} & \\text{ImageNet accuracy} \\\\\n1.0 & Oxford-IIIT Pet & 1.0 & Pascal VOC 2007 & 1.0 & PatchCamelyon & 1.0 & RESISC45 \\\\\n0.8 & & 0.8 & & 0.8 & & 0.8 & \\\\\n\\text{Mean per class} & \\text{Accuracy} & \\text{Accuracy} & \\text{Accuracy} \\\\\n0.6 & & 0.6 & & 0.6 & & 0.6 & \\\\\n0.4 & & 0.4 & & 0.4 & & 0.4 & \\\\\n0.2 & & 0.2 & &"}]}, {"page": 57, "text": "                              Table 25: Baseline results for the filtering track, small scale.\nFiltering                                                                    Training        ImageNet  ImageNet      VTAB   Retrieval  Average over\n                                                                            dataset size               dist. shifts                      38 datasets\nNo filtering                                                                   12.8M           0.025      0.033      0.145    0.114         0.133\nRandom subset (75%)                                                            9.6M            0.028      0.037      0.153    0.110         0.140\nRandom subset (50%)                                                            6.4M            0.027      0.037      0.147    0.111         0.137\nRandom subset (25%)                                                            3.2M            0.022      0.032      0.130    0.099         0.126\nRandom subset (10%)                                                            1.3M            0.010      0.018      0.116    0.077         0.103\nRandom subset (1%)                                                             128K            0.002      0.005      0.095    0.049         0.078\nCaption length                                                                 8.7M            0.034      0.040      0.148    0.109         0.143\nImage size                                                                     7.8M            0.027      0.036      0.154    0.119         0.138\nEnglish (fasttext)                                                             6.3M            0.038      0.045      0.164    0.124         0.154\nEnglish (fasttext) and caption length                                          4.8M            0.041      0.048      0.159    0.123         0.154\nEnglish (fasttext), caption length, and image size                             3.0M            0.038      0.043      0.150    0.118         0.142\nEnglish (cld3)                                                                 2.6M            0.032      0.039      0.143    0.111         0.142\nEnglish (cld3) and caption length                                              2.3M            0.031      0.038      0.153    0.111         0.142\nEnglish (cld3), caption length, and image size                                 1.5M            0.023      0.030      0.154    0.092         0.141\nCLIP B32 score top 1%                                                          129K            0.003      0.007      0.114    0.050         0.086\nCLIP B32 score top 3%                                                          384K            0.006      0.014      0.104    0.055         0.089\nCLIP B32 score top 10%                                                         1.3M            0.026      0.035      0.147    0.083         0.126\nCLIP B32 score top 20%                                                         2.6M            0.051      0.056      0.173    0.114         0.161\nCLIP B32 score top 30%                                                         3.8M            0.045      0.052      0.180    0.120         0.167\nCLIP B32 score top 40%                                                         5.1M            0.052      0.057      0.173    0.123         0.167\nCLIP B32 score top 50%                                                         6.4M            0.047      0.053      0.174    0.124         0.165\nCLIP B32 score top 75%                                                         9.6M            0.033      0.043      0.161    0.121         0.151\nCLIP B32 score top 90%                                                         11.5M           0.028      0.039      0.140    0.114         0.136\nCLIP B32 threshold at 0.3 + English filter                                     942K            0.022      0.032      0.138    0.077         0.122\nCLIP B32 threshold at 0.28 + English filter                                    1.3M            0.031      0.040      0.136    0.092         0.133\nCLIP B32 threshold at 0.3                                                      2.6M            0.052      0.056      0.166    0.114         0.161\nCLIP B32 score 1% to 30%                                                       3.7M            0.053      0.058      0.185    0.113         0.170\nCLIP B32 score 2% to 30%                                                       3.6M            0.056      0.059      0.173    0.120         0.161\nCLIP B32 score 5% to 30%                                                       3.2M            0.052      0.055      0.177    0.115         0.169\nCLIP L14 score top 1%                                                          128K            0.002      0.007      0.111    0.050         0.080\nCLIP L14 score top 3%                                                          386K            0.004      0.009      0.110    0.052         0.088\nCLIP L14 score top 10%                                                         1.3M            0.021      0.033      0.131    0.075         0.119\nCLIP L14 score top 20%                                                         2.6M            0.042      0.051      0.165    0.100         0.151\nCLIP L14 score top 30%                                                         3.8M            0.051      0.055      0.190    0.119         0.173\nCLIP L14 score top 40%                                                         5.1M            0.050      0.054      0.173    0.119         0.168\nCLIP L14 score top 50%                                                         6.4M            0.045      0.052      0.164    0.122         0.160\nCLIP L14 score top 75%                                                         9.6M            0.035      0.043      0.164    0.120         0.151\nCLIP L14 score top 90%                                                         11.5M           0.031      0.038      0.154    0.116         0.144\nImage-based clustering (ImageNet1k)                                            2.9M            0.043      0.047      0.178    0.121         0.159\nImage-based clustering (ImageNet21k)                                           4.5M            0.035      0.045      0.154    0.122         0.148\nImage-based sampling, \u03b1=0                                                      12.8M           0.019      0.030      0.144    0.095         0.127\nImage-based sampling, \u03b1=0.2                                                    12.8M           0.031      0.036      0.133    0.100         0.131\nImage-based sampling, \u03b1=0.5                                                    12.8M           0.032      0.038      0.129    0.096         0.125\nImage-based sampling, \u03b1=1                                                      12.8M           0.021      0.028      0.128    0.078         0.116\nImage-based sampling, \u03b1=2                                                      12.8M           0.011      0.017      0.116    0.065         0.099\nImageNet distance (L14, top 30%) and English                                   2.0M            0.031      0.039      0.163    0.103         0.145\nImageNet distance (L14, top 20%)                                               2.6M            0.030      0.035      0.155    0.102         0.136\nImageNet distance (L14, top 30%)                                               3.9M            0.034      0.041      0.151    0.106         0.139\nImageNet distance (L14, top 40%)                                               5.1M            0.036      0.040      0.151    0.118         0.143\nText-based clustering (ImageNet1k)                                             427K            0.009      0.016      0.120    0.056         0.096\nText-based clustering (ImageNet21k)                                            3.2M            0.046      0.052      0.169    0.125         0.157\nText-based sampling with average score, \u03b1=0                                    12.8M           0.011      0.020      0.128    0.079         0.112\nText-based sampling with average score, \u03b1=0.5                                  12.8M           0.023      0.035      0.127    0.092         0.128\nText-based sampling with average score, \u03b1=1                                    12.8M           0.040      0.044      0.163    0.115         0.155\nText-based sampling with average score, \u03b1=1.2                                  12.8M           0.038      0.045      0.150    0.112         0.143\nText-based sampling with max score, \u03b1=0                                        12.8M           0.012      0.020      0.126    0.074         0.107\nText-based sampling with max score, \u03b1=0.5                                      12.8M           0.025      0.033      0.134    0.093         0.129\nText-based sampling with max score, \u03b1=1                                        12.8M           0.040      0.046      0.159    0.116         0.150\nText-based sampling with max score, \u03b1=1.2                                      12.8M           0.040      0.050      0.161    0.113         0.152\nIntersect IN1k image clustering and CLIP B32 score top 30%                     1.4M            0.049      0.053      0.150    0.103         0.148\nIntersect IN1k image clustering and CLIP L14 score top 30%                     1.4M            0.039      0.045      0.162    0.094         0.145\nIntersect IN21k image clustering and CLIP B32 score top 30%                    2.1M            0.052      0.057      0.179    0.112         0.167\nIntersect IN21k image clustering and CLIP L14 score top 30%                    2.1M            0.047      0.053      0.176    0.110         0.163\n                                                                                57", "md": "|Filtering|Training dataset size|ImageNet dist.|ImageNet shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|No filtering|12.8M|0.025|0.033|0.145|0.114|0.133|\n|Random subset (75%)|9.6M|0.028|0.037|0.153|0.110|0.140|\n|Random subset (50%)|6.4M|0.027|0.037|0.147|0.111|0.137|\n|Random subset (25%)|3.2M|0.022|0.032|0.130|0.099|0.126|\n|Random subset (10%)|1.3M|0.010|0.018|0.116|0.077|0.103|\n|Random subset (1%)|128K|0.002|0.005|0.095|0.049|0.078|\n|Caption length|8.7M|0.034|0.040|0.148|0.109|0.143|\n|Image size|7.8M|0.027|0.036|0.154|0.119|0.138|\n|English (fasttext)|6.3M|0.038|0.045|0.164|0.124|0.154|\n|English (fasttext) and caption length|4.8M|0.041|0.048|0.159|0.123|0.154|\n|English (fasttext), caption length, and image size|3.0M|0.038|0.043|0.150|0.118|0.142|\n|English (cld3)|2.6M|0.032|0.039|0.143|0.111|0.142|\n|English (cld3) and caption length|2.3M|0.031|0.038|0.153|0.111|0.142|\n|English (cld3), caption length, and image size|1.5M|0.023|0.030|0.154|0.092|0.141|\n|CLIP B32 score top 1%|129K|0.003|0.007|0.114|0.050|0.086|\n|CLIP B32 score top 3%|384K|0.006|0.014|0.104|0.055|0.089|\n|CLIP B32 score top 10%|1.3M|0.026|0.035|0.147|0.083|0.126|\n|CLIP B32 score top 20%|2.6M|0.051|0.056|0.173|0.114|0.161|\n|CLIP B32 score top 30%|3.8M|0.045|0.052|0.180|0.120|0.167|\n|CLIP B32 score top 40%|5.1M|0.052|0.057|0.173|0.123|0.167|\n|CLIP B32 score top 50%|6.4M|0.047|0.053|0.174|0.124|0.165|\n|CLIP B32 score top 75%|9.6M|0.033|0.043|0.161|0.121|0.151|\n|CLIP B32 score top 90%|11.5M|0.028|0.039|0.140|0.114|0.136|\n|CLIP B32 threshold at 0.3 + English filter|942K|0.022|0.032|0.138|0.077|0.122|\n|CLIP B32 threshold at 0.28 + English filter|1.3M|0.031|0.040|0.136|0.092|0.133|\n|CLIP B32 threshold at 0.3|2.6M|0.052|0.056|0.166|0.114|0.161|\n|CLIP B32 score 1% to 30%|3.7M|0.053|0.058|0.185|0.113|0.170|\n|CLIP B32 score 2% to 30%|3.6M|0.056|0.059|0.173|0.120|0.161|\n|CLIP B32 score 5% to 30%|3.2M|0.052|0.055|0.177|0.115|0.169|\n|CLIP L14 score top 1%|128K|0.002|0.007|0.111|0.050|0.080|\n|CLIP L14 score top 3%|386K|0.004|0.009|0.110|0.052|0.088|\n|CLIP L14 score top 10%|1.3M|0.021|0.033|0.131|0.075|0.119|\n|CLIP L14 score top 20%|2.6M|0.042|0.051|0.165|0.100|0.151|\n|CLIP L14 score top 30%|3.8M|0.051|0.055|0.190|0.119|0.173|\n|CLIP L14 score top 40%|5.1M|0.050|0.054|0.173|0.119|0.168|\n|CLIP L14 score top 50%|6.4M|0.045|0.052|0.164|0.122|0.160|\n|CLIP L14 score top 75%|9.6M|0.035|0.043|0.164|0.120|0.151|\n|CLIP L14 score top 90%|11.5M|0.031|0.038|0.154|0.116|0.144|\n|Image-based clustering (ImageNet1k)|2.9M|0.043|0.047|0.178|0.121|0.159|\n|Image-based clustering (ImageNet21k)|4.5M|0.035|0.045|0.154|0.122|0.148|\n|Image-based sampling, \u03b1=0|12.8M|0.019|0.030|0.144|0.095|0.127|\n|Image-based sampling, \u03b1=0.2|12.8M|0.031|0.036|0.133|0.100|0.131|\n|Image-based sampling, \u03b1=0.5|12.8M|0.032|0.038|0.129|0.096|0.125|\n|Image-based sampling, \u03b1=1|12.8M|0.021|0.028|0.128|0.078|0.116|\n|Image-based sampling, \u03b1=2|12.8M|0.011|0.017|0.116|0.065|0.099|\n|ImageNet distance (L14, top 30%) and English|2.0M|0.031|0.039|0.163|0.103|0.145|\n|ImageNet distance (L14, top 20%)|2.6M|0.030|0.035|0.155|0.102|0.136|\n|ImageNet distance (L14, top 30%)| |3.9M|0.034|0.041|0.151|0.106|0.139|ImageNet distance (L14, top 40%)|5.1M|0.036|0.040|0.151|0.118|0.143|\n|Text-based clustering (ImageNet1k)|427K|0.009|0.016|0.120|0.056|0.096|\n|Text-based clustering (ImageNet21k)|3.2M|0.046|0.052|0.169|0.125|0.157|\n|Text-based sampling with average score, \u03b1=0|12.8M|0.011|0.020|0.128|0.079|0.112|\n|Text-based sampling with average score, \u03b1=0.5|12.8M|0.023|0.035|0.127|0.092|0.128|\n|Text-based sampling with average score, \u03b1=1|12.8M|0.040|0.044|0.163|0.115|0.155|\n|Text-based sampling with average score, \u03b1=1.2|12.8M|0.038|0.045|0.150|0.112|0.143|\n|Text-based sampling with max score, \u03b1=0|12.8M|0.012|0.020|0.126|0.074|0.107|\n|Text-based sampling with max score, \u03b1=0.5|12.8M|0.025|0.033|0.134|0.093|0.129|\n|Text-based sampling with max score, \u03b1=1|12.8M|0.040|0.046|0.159|0.116|0.150|\n|Text-based sampling with max score, \u03b1=1.2|12.8M|0.040|0.050|0.161|0.113|0.152|\n|Intersect IN1k image clustering and CLIP B32 score top 30%|1.4M|0.049|0.053|0.150|0.103|0.148|\n|Intersect IN1k image clustering and CLIP L14 score top 30%|1.4M|0.039|0.045|0.162|0.094|0.145|\n|Intersect IN21k image clustering and CLIP B32 score top 30%|2.1M|0.052|0.057|0.179|0.112|0.167|\n|Intersect IN21k image clustering and CLIP L14 score top 30%|2.1M|0.047|0.053|0.176|0.110|0.163|", "images": [], "items": [{"type": "table", "rows": [["Filtering", "Training dataset size", "ImageNet dist.", "ImageNet shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["No filtering", "12.8M", "0.025", "0.033", "0.145", "0.114", "0.133"], ["Random subset (75%)", "9.6M", "0.028", "0.037", "0.153", "0.110", "0.140"], ["Random subset (50%)", "6.4M", "0.027", "0.037", "0.147", "0.111", "0.137"], ["Random subset (25%)", "3.2M", "0.022", "0.032", "0.130", "0.099", "0.126"], ["Random subset (10%)", "1.3M", "0.010", "0.018", "0.116", "0.077", "0.103"], ["Random subset (1%)", "128K", "0.002", "0.005", "0.095", "0.049", "0.078"], ["Caption length", "8.7M", "0.034", "0.040", "0.148", "0.109", "0.143"], ["Image size", "7.8M", "0.027", "0.036", "0.154", "0.119", "0.138"], ["English (fasttext)", "6.3M", "0.038", "0.045", "0.164", "0.124", "0.154"], ["English (fasttext) and caption length", "4.8M", "0.041", "0.048", "0.159", "0.123", "0.154"], ["English (fasttext), caption length, and image size", "3.0M", "0.038", "0.043", "0.150", "0.118", "0.142"], ["English (cld3)", "2.6M", "0.032", "0.039", "0.143", "0.111", "0.142"], ["English (cld3) and caption length", "2.3M", "0.031", "0.038", "0.153", "0.111", "0.142"], ["English (cld3), caption length, and image size", "1.5M", "0.023", "0.030", "0.154", "0.092", "0.141"], ["CLIP B32 score top 1%", "129K", "0.003", "0.007", "0.114", "0.050", "0.086"], ["CLIP B32 score top 3%", "384K", "0.006", "0.014", "0.104", "0.055", "0.089"], ["CLIP B32 score top 10%", "1.3M", "0.026", "0.035", "0.147", "0.083", "0.126"], ["CLIP B32 score top 20%", "2.6M", "0.051", "0.056", "0.173", "0.114", "0.161"], ["CLIP B32 score top 30%", "3.8M", "0.045", "0.052", "0.180", "0.120", "0.167"], ["CLIP B32 score top 40%", "5.1M", "0.052", "0.057", "0.173", "0.123", "0.167"], ["CLIP B32 score top 50%", "6.4M", "0.047", "0.053", "0.174", "0.124", "0.165"], ["CLIP B32 score top 75%", "9.6M", "0.033", "0.043", "0.161", "0.121", "0.151"], ["CLIP B32 score top 90%", "11.5M", "0.028", "0.039", "0.140", "0.114", "0.136"], ["CLIP B32 threshold at 0.3 + English filter", "942K", "0.022", "0.032", "0.138", "0.077", "0.122"], ["CLIP B32 threshold at 0.28 + English filter", "1.3M", "0.031", "0.040", "0.136", "0.092", "0.133"], ["CLIP B32 threshold at 0.3", "2.6M", "0.052", "0.056", "0.166", "0.114", "0.161"], ["CLIP B32 score 1% to 30%", "3.7M", "0.053", "0.058", "0.185", "0.113", "0.170"], ["CLIP B32 score 2% to 30%", "3.6M", "0.056", "0.059", "0.173", "0.120", "0.161"], ["CLIP B32 score 5% to 30%", "3.2M", "0.052", "0.055", "0.177", "0.115", "0.169"], ["CLIP L14 score top 1%", "128K", "0.002", "0.007", "0.111", "0.050", "0.080"], ["CLIP L14 score top 3%", "386K", "0.004", "0.009", "0.110", "0.052", "0.088"], ["CLIP L14 score top 10%", "1.3M", "0.021", "0.033", "0.131", "0.075", "0.119"], ["CLIP L14 score top 20%", "2.6M", "0.042", "0.051", "0.165", "0.100", "0.151"], ["CLIP L14 score top 30%", "3.8M", "0.051", "0.055", "0.190", "0.119", "0.173"], ["CLIP L14 score top 40%", "5.1M", "0.050", "0.054", "0.173", "0.119", "0.168"], ["CLIP L14 score top 50%", "6.4M", "0.045", "0.052", "0.164", "0.122", "0.160"], ["CLIP L14 score top 75%", "9.6M", "0.035", "0.043", "0.164", "0.120", "0.151"], ["CLIP L14 score top 90%", "11.5M", "0.031", "0.038", "0.154", "0.116", "0.144"], ["Image-based clustering (ImageNet1k)", "2.9M", "0.043", "0.047", "0.178", "0.121", "0.159"], ["Image-based clustering (ImageNet21k)", "4.5M", "0.035", "0.045", "0.154", "0.122", "0.148"], ["Image-based sampling, \u03b1=0", "12.8M", "0.019", "0.030", "0.144", "0.095", "0.127"], ["Image-based sampling, \u03b1=0.2", "12.8M", "0.031", "0.036", "0.133", "0.100", "0.131"], ["Image-based sampling, \u03b1=0.5", "12.8M", "0.032", "0.038", "0.129", "0.096", "0.125"], ["Image-based sampling, \u03b1=1", "12.8M", "0.021", "0.028", "0.128", "0.078", "0.116"], ["Image-based sampling, \u03b1=2", "12.8M", "0.011", "0.017", "0.116", "0.065", "0.099"], ["ImageNet distance (L14, top 30%) and English", "2.0M", "0.031", "0.039", "0.163", "0.103", "0.145"], ["ImageNet distance (L14, top 20%)", "2.6M", "0.030", "0.035", "0.155", "0.102", "0.136"], ["ImageNet distance (L14, top 30%)", "", "3.9M", "0.034", "0.041", "0.151", "0.106", "0.139", "ImageNet distance (L14, top 40%)", "5.1M", "0.036", "0.040", "0.151", "0.118", "0.143"], ["Text-based clustering (ImageNet1k)", "427K", "0.009", "0.016", "0.120", "0.056", "0.096"], ["Text-based clustering (ImageNet21k)", "3.2M", "0.046", "0.052", "0.169", "0.125", "0.157"], ["Text-based sampling with average score, \u03b1=0", "12.8M", "0.011", "0.020", "0.128", "0.079", "0.112"], ["Text-based sampling with average score, \u03b1=0.5", "12.8M", "0.023", "0.035", "0.127", "0.092", "0.128"], ["Text-based sampling with average score, \u03b1=1", "12.8M", "0.040", "0.044", "0.163", "0.115", "0.155"], ["Text-based sampling with average score, \u03b1=1.2", "12.8M", "0.038", "0.045", "0.150", "0.112", "0.143"], ["Text-based sampling with max score, \u03b1=0", "12.8M", "0.012", "0.020", "0.126", "0.074", "0.107"], ["Text-based sampling with max score, \u03b1=0.5", "12.8M", "0.025", "0.033", "0.134", "0.093", "0.129"], ["Text-based sampling with max score, \u03b1=1", "12.8M", "0.040", "0.046", "0.159", "0.116", "0.150"], ["Text-based sampling with max score, \u03b1=1.2", "12.8M", "0.040", "0.050", "0.161", "0.113", "0.152"], ["Intersect IN1k image clustering and CLIP B32 score top 30%", "1.4M", "0.049", "0.053", "0.150", "0.103", "0.148"], ["Intersect IN1k image clustering and CLIP L14 score top 30%", "1.4M", "0.039", "0.045", "0.162", "0.094", "0.145"], ["Intersect IN21k image clustering and CLIP B32 score top 30%", "2.1M", "0.052", "0.057", "0.179", "0.112", "0.167"], ["Intersect IN21k image clustering and CLIP L14 score top 30%", "2.1M", "0.047", "0.053", "0.176", "0.110", "0.163"]], "md": "|Filtering|Training dataset size|ImageNet dist.|ImageNet shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|No filtering|12.8M|0.025|0.033|0.145|0.114|0.133|\n|Random subset (75%)|9.6M|0.028|0.037|0.153|0.110|0.140|\n|Random subset (50%)|6.4M|0.027|0.037|0.147|0.111|0.137|\n|Random subset (25%)|3.2M|0.022|0.032|0.130|0.099|0.126|\n|Random subset (10%)|1.3M|0.010|0.018|0.116|0.077|0.103|\n|Random subset (1%)|128K|0.002|0.005|0.095|0.049|0.078|\n|Caption length|8.7M|0.034|0.040|0.148|0.109|0.143|\n|Image size|7.8M|0.027|0.036|0.154|0.119|0.138|\n|English (fasttext)|6.3M|0.038|0.045|0.164|0.124|0.154|\n|English (fasttext) and caption length|4.8M|0.041|0.048|0.159|0.123|0.154|\n|English (fasttext), caption length, and image size|3.0M|0.038|0.043|0.150|0.118|0.142|\n|English (cld3)|2.6M|0.032|0.039|0.143|0.111|0.142|\n|English (cld3) and caption length|2.3M|0.031|0.038|0.153|0.111|0.142|\n|English (cld3), caption length, and image size|1.5M|0.023|0.030|0.154|0.092|0.141|\n|CLIP B32 score top 1%|129K|0.003|0.007|0.114|0.050|0.086|\n|CLIP B32 score top 3%|384K|0.006|0.014|0.104|0.055|0.089|\n|CLIP B32 score top 10%|1.3M|0.026|0.035|0.147|0.083|0.126|\n|CLIP B32 score top 20%|2.6M|0.051|0.056|0.173|0.114|0.161|\n|CLIP B32 score top 30%|3.8M|0.045|0.052|0.180|0.120|0.167|\n|CLIP B32 score top 40%|5.1M|0.052|0.057|0.173|0.123|0.167|\n|CLIP B32 score top 50%|6.4M|0.047|0.053|0.174|0.124|0.165|\n|CLIP B32 score top 75%|9.6M|0.033|0.043|0.161|0.121|0.151|\n|CLIP B32 score top 90%|11.5M|0.028|0.039|0.140|0.114|0.136|\n|CLIP B32 threshold at 0.3 + English filter|942K|0.022|0.032|0.138|0.077|0.122|\n|CLIP B32 threshold at 0.28 + English filter|1.3M|0.031|0.040|0.136|0.092|0.133|\n|CLIP B32 threshold at 0.3|2.6M|0.052|0.056|0.166|0.114|0.161|\n|CLIP B32 score 1% to 30%|3.7M|0.053|0.058|0.185|0.113|0.170|\n|CLIP B32 score 2% to 30%|3.6M|0.056|0.059|0.173|0.120|0.161|\n|CLIP B32 score 5% to 30%|3.2M|0.052|0.055|0.177|0.115|0.169|\n|CLIP L14 score top 1%|128K|0.002|0.007|0.111|0.050|0.080|\n|CLIP L14 score top 3%|386K|0.004|0.009|0.110|0.052|0.088|\n|CLIP L14 score top 10%|1.3M|0.021|0.033|0.131|0.075|0.119|\n|CLIP L14 score top 20%|2.6M|0.042|0.051|0.165|0.100|0.151|\n|CLIP L14 score top 30%|3.8M|0.051|0.055|0.190|0.119|0.173|\n|CLIP L14 score top 40%|5.1M|0.050|0.054|0.173|0.119|0.168|\n|CLIP L14 score top 50%|6.4M|0.045|0.052|0.164|0.122|0.160|\n|CLIP L14 score top 75%|9.6M|0.035|0.043|0.164|0.120|0.151|\n|CLIP L14 score top 90%|11.5M|0.031|0.038|0.154|0.116|0.144|\n|Image-based clustering (ImageNet1k)|2.9M|0.043|0.047|0.178|0.121|0.159|\n|Image-based clustering (ImageNet21k)|4.5M|0.035|0.045|0.154|0.122|0.148|\n|Image-based sampling, \u03b1=0|12.8M|0.019|0.030|0.144|0.095|0.127|\n|Image-based sampling, \u03b1=0.2|12.8M|0.031|0.036|0.133|0.100|0.131|\n|Image-based sampling, \u03b1=0.5|12.8M|0.032|0.038|0.129|0.096|0.125|\n|Image-based sampling, \u03b1=1|12.8M|0.021|0.028|0.128|0.078|0.116|\n|Image-based sampling, \u03b1=2|12.8M|0.011|0.017|0.116|0.065|0.099|\n|ImageNet distance (L14, top 30%) and English|2.0M|0.031|0.039|0.163|0.103|0.145|\n|ImageNet distance (L14, top 20%)|2.6M|0.030|0.035|0.155|0.102|0.136|\n|ImageNet distance (L14, top 30%)| |3.9M|0.034|0.041|0.151|0.106|0.139|ImageNet distance (L14, top 40%)|5.1M|0.036|0.040|0.151|0.118|0.143|\n|Text-based clustering (ImageNet1k)|427K|0.009|0.016|0.120|0.056|0.096|\n|Text-based clustering (ImageNet21k)|3.2M|0.046|0.052|0.169|0.125|0.157|\n|Text-based sampling with average score, \u03b1=0|12.8M|0.011|0.020|0.128|0.079|0.112|\n|Text-based sampling with average score, \u03b1=0.5|12.8M|0.023|0.035|0.127|0.092|0.128|\n|Text-based sampling with average score, \u03b1=1|12.8M|0.040|0.044|0.163|0.115|0.155|\n|Text-based sampling with average score, \u03b1=1.2|12.8M|0.038|0.045|0.150|0.112|0.143|\n|Text-based sampling with max score, \u03b1=0|12.8M|0.012|0.020|0.126|0.074|0.107|\n|Text-based sampling with max score, \u03b1=0.5|12.8M|0.025|0.033|0.134|0.093|0.129|\n|Text-based sampling with max score, \u03b1=1|12.8M|0.040|0.046|0.159|0.116|0.150|\n|Text-based sampling with max score, \u03b1=1.2|12.8M|0.040|0.050|0.161|0.113|0.152|\n|Intersect IN1k image clustering and CLIP B32 score top 30%|1.4M|0.049|0.053|0.150|0.103|0.148|\n|Intersect IN1k image clustering and CLIP L14 score top 30%|1.4M|0.039|0.045|0.162|0.094|0.145|\n|Intersect IN21k image clustering and CLIP B32 score top 30%|2.1M|0.052|0.057|0.179|0.112|0.167|\n|Intersect IN21k image clustering and CLIP L14 score top 30%|2.1M|0.047|0.053|0.176|0.110|0.163|", "isPerfectTable": false, "csv": "\"Filtering\",\"Training dataset size\",\"ImageNet dist.\",\"ImageNet shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"No filtering\",\"12.8M\",\"0.025\",\"0.033\",\"0.145\",\"0.114\",\"0.133\"\n\"Random subset (75%)\",\"9.6M\",\"0.028\",\"0.037\",\"0.153\",\"0.110\",\"0.140\"\n\"Random subset (50%)\",\"6.4M\",\"0.027\",\"0.037\",\"0.147\",\"0.111\",\"0.137\"\n\"Random subset (25%)\",\"3.2M\",\"0.022\",\"0.032\",\"0.130\",\"0.099\",\"0.126\"\n\"Random subset (10%)\",\"1.3M\",\"0.010\",\"0.018\",\"0.116\",\"0.077\",\"0.103\"\n\"Random subset (1%)\",\"128K\",\"0.002\",\"0.005\",\"0.095\",\"0.049\",\"0.078\"\n\"Caption length\",\"8.7M\",\"0.034\",\"0.040\",\"0.148\",\"0.109\",\"0.143\"\n\"Image size\",\"7.8M\",\"0.027\",\"0.036\",\"0.154\",\"0.119\",\"0.138\"\n\"English (fasttext)\",\"6.3M\",\"0.038\",\"0.045\",\"0.164\",\"0.124\",\"0.154\"\n\"English (fasttext) and caption length\",\"4.8M\",\"0.041\",\"0.048\",\"0.159\",\"0.123\",\"0.154\"\n\"English (fasttext), caption length, and image size\",\"3.0M\",\"0.038\",\"0.043\",\"0.150\",\"0.118\",\"0.142\"\n\"English (cld3)\",\"2.6M\",\"0.032\",\"0.039\",\"0.143\",\"0.111\",\"0.142\"\n\"English (cld3) and caption length\",\"2.3M\",\"0.031\",\"0.038\",\"0.153\",\"0.111\",\"0.142\"\n\"English (cld3), caption length, and image size\",\"1.5M\",\"0.023\",\"0.030\",\"0.154\",\"0.092\",\"0.141\"\n\"CLIP B32 score top 1%\",\"129K\",\"0.003\",\"0.007\",\"0.114\",\"0.050\",\"0.086\"\n\"CLIP B32 score top 3%\",\"384K\",\"0.006\",\"0.014\",\"0.104\",\"0.055\",\"0.089\"\n\"CLIP B32 score top 10%\",\"1.3M\",\"0.026\",\"0.035\",\"0.147\",\"0.083\",\"0.126\"\n\"CLIP B32 score top 20%\",\"2.6M\",\"0.051\",\"0.056\",\"0.173\",\"0.114\",\"0.161\"\n\"CLIP B32 score top 30%\",\"3.8M\",\"0.045\",\"0.052\",\"0.180\",\"0.120\",\"0.167\"\n\"CLIP B32 score top 40%\",\"5.1M\",\"0.052\",\"0.057\",\"0.173\",\"0.123\",\"0.167\"\n\"CLIP B32 score top 50%\",\"6.4M\",\"0.047\",\"0.053\",\"0.174\",\"0.124\",\"0.165\"\n\"CLIP B32 score top 75%\",\"9.6M\",\"0.033\",\"0.043\",\"0.161\",\"0.121\",\"0.151\"\n\"CLIP B32 score top 90%\",\"11.5M\",\"0.028\",\"0.039\",\"0.140\",\"0.114\",\"0.136\"\n\"CLIP B32 threshold at 0.3 + English filter\",\"942K\",\"0.022\",\"0.032\",\"0.138\",\"0.077\",\"0.122\"\n\"CLIP B32 threshold at 0.28 + English filter\",\"1.3M\",\"0.031\",\"0.040\",\"0.136\",\"0.092\",\"0.133\"\n\"CLIP B32 threshold at 0.3\",\"2.6M\",\"0.052\",\"0.056\",\"0.166\",\"0.114\",\"0.161\"\n\"CLIP B32 score 1% to 30%\",\"3.7M\",\"0.053\",\"0.058\",\"0.185\",\"0.113\",\"0.170\"\n\"CLIP B32 score 2% to 30%\",\"3.6M\",\"0.056\",\"0.059\",\"0.173\",\"0.120\",\"0.161\"\n\"CLIP B32 score 5% to 30%\",\"3.2M\",\"0.052\",\"0.055\",\"0.177\",\"0.115\",\"0.169\"\n\"CLIP L14 score top 1%\",\"128K\",\"0.002\",\"0.007\",\"0.111\",\"0.050\",\"0.080\"\n\"CLIP L14 score top 3%\",\"386K\",\"0.004\",\"0.009\",\"0.110\",\"0.052\",\"0.088\"\n\"CLIP L14 score top 10%\",\"1.3M\",\"0.021\",\"0.033\",\"0.131\",\"0.075\",\"0.119\"\n\"CLIP L14 score top 20%\",\"2.6M\",\"0.042\",\"0.051\",\"0.165\",\"0.100\",\"0.151\"\n\"CLIP L14 score top 30%\",\"3.8M\",\"0.051\",\"0.055\",\"0.190\",\"0.119\",\"0.173\"\n\"CLIP L14 score top 40%\",\"5.1M\",\"0.050\",\"0.054\",\"0.173\",\"0.119\",\"0.168\"\n\"CLIP L14 score top 50%\",\"6.4M\",\"0.045\",\"0.052\",\"0.164\",\"0.122\",\"0.160\"\n\"CLIP L14 score top 75%\",\"9.6M\",\"0.035\",\"0.043\",\"0.164\",\"0.120\",\"0.151\"\n\"CLIP L14 score top 90%\",\"11.5M\",\"0.031\",\"0.038\",\"0.154\",\"0.116\",\"0.144\"\n\"Image-based clustering (ImageNet1k)\",\"2.9M\",\"0.043\",\"0.047\",\"0.178\",\"0.121\",\"0.159\"\n\"Image-based clustering (ImageNet21k)\",\"4.5M\",\"0.035\",\"0.045\",\"0.154\",\"0.122\",\"0.148\"\n\"Image-based sampling, \u03b1=0\",\"12.8M\",\"0.019\",\"0.030\",\"0.144\",\"0.095\",\"0.127\"\n\"Image-based sampling, \u03b1=0.2\",\"12.8M\",\"0.031\",\"0.036\",\"0.133\",\"0.100\",\"0.131\"\n\"Image-based sampling, \u03b1=0.5\",\"12.8M\",\"0.032\",\"0.038\",\"0.129\",\"0.096\",\"0.125\"\n\"Image-based sampling, \u03b1=1\",\"12.8M\",\"0.021\",\"0.028\",\"0.128\",\"0.078\",\"0.116\"\n\"Image-based sampling, \u03b1=2\",\"12.8M\",\"0.011\",\"0.017\",\"0.116\",\"0.065\",\"0.099\"\n\"ImageNet distance (L14, top 30%) and English\",\"2.0M\",\"0.031\",\"0.039\",\"0.163\",\"0.103\",\"0.145\"\n\"ImageNet distance (L14, top 20%)\",\"2.6M\",\"0.030\",\"0.035\",\"0.155\",\"0.102\",\"0.136\"\n\"ImageNet distance (L14, top 30%)\",\"\",\"3.9M\",\"0.034\",\"0.041\",\"0.151\",\"0.106\",\"0.139\",\"ImageNet distance (L14, top 40%)\",\"5.1M\",\"0.036\",\"0.040\",\"0.151\",\"0.118\",\"0.143\"\n\"Text-based clustering (ImageNet1k)\",\"427K\",\"0.009\",\"0.016\",\"0.120\",\"0.056\",\"0.096\"\n\"Text-based clustering (ImageNet21k)\",\"3.2M\",\"0.046\",\"0.052\",\"0.169\",\"0.125\",\"0.157\"\n\"Text-based sampling with average score, \u03b1=0\",\"12.8M\",\"0.011\",\"0.020\",\"0.128\",\"0.079\",\"0.112\"\n\"Text-based sampling with average score, \u03b1=0.5\",\"12.8M\",\"0.023\",\"0.035\",\"0.127\",\"0.092\",\"0.128\"\n\"Text-based sampling with average score, \u03b1=1\",\"12.8M\",\"0.040\",\"0.044\",\"0.163\",\"0.115\",\"0.155\"\n\"Text-based sampling with average score, \u03b1=1.2\",\"12.8M\",\"0.038\",\"0.045\",\"0.150\",\"0.112\",\"0.143\"\n\"Text-based sampling with max score, \u03b1=0\",\"12.8M\",\"0.012\",\"0.020\",\"0.126\",\"0.074\",\"0.107\"\n\"Text-based sampling with max score, \u03b1=0.5\",\"12.8M\",\"0.025\",\"0.033\",\"0.134\",\"0.093\",\"0.129\"\n\"Text-based sampling with max score, \u03b1=1\",\"12.8M\",\"0.040\",\"0.046\",\"0.159\",\"0.116\",\"0.150\"\n\"Text-based sampling with max score, \u03b1=1.2\",\"12.8M\",\"0.040\",\"0.050\",\"0.161\",\"0.113\",\"0.152\"\n\"Intersect IN1k image clustering and CLIP B32 score top 30%\",\"1.4M\",\"0.049\",\"0.053\",\"0.150\",\"0.103\",\"0.148\"\n\"Intersect IN1k image clustering and CLIP L14 score top 30%\",\"1.4M\",\"0.039\",\"0.045\",\"0.162\",\"0.094\",\"0.145\"\n\"Intersect IN21k image clustering and CLIP B32 score top 30%\",\"2.1M\",\"0.052\",\"0.057\",\"0.179\",\"0.112\",\"0.167\"\n\"Intersect IN21k image clustering and CLIP L14 score top 30%\",\"2.1M\",\"0.047\",\"0.053\",\"0.176\",\"0.110\",\"0.163\""}]}, {"page": 58, "text": "                             Table 26: Baseline results for the filtering track, medium scale.\nFiltering                                                                    Training        ImageNet  ImageNet      VTAB   Retrieval  Average over\n                                                                            dataset size               dist. shifts                      38 datasets\nNo filtering                                                                   128M            0.176      0.152      0.259    0.219         0.258\nRandom subset (75%)                                                            96.0M           0.175      0.154      0.265    0.219         0.257\nRandom subset (50%)                                                            64.0M           0.171      0.151      0.258    0.216         0.252\nRandom subset (25%)                                                            32.0M           0.155      0.136      0.246    0.203         0.240\nRandom subset (10%)                                                            12.8M           0.107      0.095      0.210    0.144         0.200\nRandom subset (1%)                                                             1.3M            0.009      0.017      0.102    0.065         0.090\nCaption length                                                                 87.5M           0.199      0.172      0.275    0.236         0.275\nImage size                                                                     77.8M           0.189      0.163      0.248    0.231         0.259\nEnglish (fasttext)                                                             63.0M           0.214      0.182      0.290    0.246         0.285\nEnglish (fasttext) and caption length                                          47.8M           0.226      0.193      0.284    0.251         0.285\nEnglish (fasttext), caption length, and image size                             29.8M           0.226      0.193      0.297    0.253         0.294\nEnglish (cld3)                                                                 25.6M           0.200      0.175      0.296    0.235         0.279\nEnglish (cld3) and caption length                                              22.9M           0.204      0.175      0.287    0.243         0.278\nEnglish (cld3), caption length, and image size                                 14.6M           0.179      0.159      0.243    0.216         0.247\nCLIP B32 score top 1%                                                          1.3M            0.025      0.037      0.140    0.076         0.126\nCLIP B32 score top 3%                                                          3.9M            0.093      0.096      0.205    0.128         0.188\nCLIP B32 score top 10%                                                         12.8M           0.231      0.199      0.305    0.206         0.298\nCLIP B32 score top 20%                                                         25.7M           0.279      0.234      0.337    0.241         0.330\nCLIP B32 score top 30%                                                         38.4M           0.285      0.240      0.355    0.253         0.338\nCLIP B32 score top 40%                                                         51.3M           0.273      0.227      0.333    0.257         0.324\nCLIP B32 score top 50%                                                         64.0M           0.256      0.219      0.322    0.259         0.316\nCLIP B32 score top 75%                                                         96.1M           0.211      0.180      0.301    0.238         0.290\nCLIP B32 score top 90%                                                         115M            0.189      0.165      0.279    0.229         0.274\nCLIP B32 threshold at 0.3 + English filter                                     9.4M            0.208      0.184      0.292    0.210         0.276\nCLIP B32 threshold at 0.28 + English filter                                    13.0M           0.230      0.198      0.307    0.233         0.292\nCLIP B32 threshold at 0.3                                                      25.9M           0.282      0.233      0.340    0.243         0.333\nCLIP B32 score 1% to 30%                                                       37.1M           0.287      0.238      0.347    0.253         0.334\nCLIP B32 score 2% to 30%                                                       35.9M           0.288      0.238      0.338    0.248         0.330\nCLIP B32 score 5% to 30%                                                       32.0M           0.281      0.230      0.352    0.254         0.339\nCLIP L14 score top 1%                                                          1.3M            0.014      0.025      0.136    0.062         0.109\nCLIP L14 score top 3%                                                          3.9M            0.065      0.077      0.176    0.103         0.160\nCLIP L14 score top 10%                                                         12.8M           0.198      0.183      0.283    0.188         0.277\nCLIP L14 score top 20%                                                         25.7M           0.260      0.225      0.326    0.235         0.322\nCLIP L14 score top 30%                                                         38.4M           0.273      0.230      0.338    0.251         0.328\nCLIP L14 score top 40%                                                         51.2M           0.262      0.226      0.330    0.260         0.327\nCLIP L14 score top 50%                                                         64.1M           0.254      0.218      0.322    0.262         0.315\nCLIP L14 score top 75%                                                         96.1M           0.212      0.180      0.287    0.242         0.285\nCLIP L14 score top 90%                                                         115M            0.188      0.164      0.258    0.225         0.266\nImage-based clustering (ImageNet1k)                                            29.2M           0.268      0.213      0.319    0.256         0.312\nImage-based clustering (ImageNet21k)                                           45.1M           0.238      0.198      0.304    0.252         0.312\nImage-based sampling, \u03b1=0                                                      128M            0.170      0.150      0.266    0.209         0.254\nImage-based sampling, \u03b1=0.2                                                    128M            0.249      0.193      0.292    0.221         0.284\nImage-based sampling, \u03b1=0.5                                                    128M            0.269      0.196      0.301    0.216         0.284\nImage-based sampling, \u03b1=1                                                      128M            0.207      0.145      0.264    0.166         0.239\nImage-based sampling, \u03b1=2                                                      128M            0.118      0.082      0.207    0.110         0.180\nImageNet distance (L14, top 30%) and English                                   19.8M           0.212      0.158      0.272    0.178         0.259\nImageNet distance (L/14, top 20%)                                              25.8M           0.193      0.138      0.276    0.176         0.252\nImageNet distance (L/14, top 30%)                                              38.5M           0.212      0.159      0.283    0.201         0.269\nImageNet distance (L/14, top 40%)                                              51.3M           0.212      0.165      0.273    0.212         0.270\nText-based clustering (ImageNet1k)                                             4.3M            0.099      0.090      0.173    0.109         0.166\nText-based clustering (ImageNet21k)                                            31.7M           0.255      0.215      0.328    0.249         0.307\nText-based sampling with average score, \u03b1=0                                    128M            0.136      0.110      0.213    0.140         0.209\nText-based sampling with average score, \u03b1=0.5                                  128M            0.222      0.178      0.273    0.206         0.269\nText-based sampling with average score, \u03b1=1                                    128M            0.245      0.204      0.302    0.251         0.293\nText-based sampling with average score, \u03b1=1.2                                  128M            0.231      0.200      0.298    0.240         0.289\nText-based sampling with max score, \u03b1=0                                        128M            0.140      0.116      0.242    0.138         0.225\nText-based sampling with max score, \u03b1=0.5                                      128M            0.229      0.190      0.290    0.205         0.283\nText-based sampling with max score, \u03b1=1                                        128M            0.247      0.209      0.300    0.241         0.295\nText-based sampling with max score, \u03b1=1.2                                      128M            0.235      0.200      0.298    0.239         0.290\nIntersect IN1k image clustering and CLIP B32 score top 30%                     14.2M           0.305      0.243      0.342    0.250         0.328\nIntersect IN1k image clustering and CLIP L14 score top 30%                     14.0M           0.297      0.239      0.346    0.231         0.328\nIntersect IN21k image clustering and CLIP B32 score top 30%                    21.1M           0.298      0.244      0.347    0.256         0.336\nIntersect IN21k image clustering and CLIP L14 score top 30%                    20.8M           0.290      0.241      0.339    0.244         0.328\n                                                                                58", "md": "|Filtering|Training dataset size|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|No filtering|128M|0.176|0.152|0.259|0.219|0.258|\n|Random subset (75%)|96.0M|0.175|0.154|0.265|0.219|0.257|\n|Random subset (50%)|64.0M|0.171|0.151|0.258|0.216|0.252|\n|Random subset (25%)|32.0M|0.155|0.136|0.246|0.203|0.240|\n|Random subset (10%)|12.8M|0.107|0.095|0.210|0.144|0.200|\n|Random subset (1%)|1.3M|0.009|0.017|0.102|0.065|0.090|\n|Caption length|87.5M|0.199|0.172|0.275|0.236|0.275|\n|Image size|77.8M|0.189|0.163|0.248|0.231|0.259|\n|English (fasttext)|63.0M|0.214|0.182|0.290|0.246|0.285|\n|English (fasttext) and caption length|47.8M|0.226|0.193|0.284|0.251|0.285|\n|English (fasttext), caption length, and image size|29.8M|0.226|0.193|0.297|0.253|0.294|\n|English (cld3)|25.6M|0.200|0.175|0.296|0.235|0.279|\n|English (cld3) and caption length|22.9M|0.204|0.175|0.287|0.243|0.278|\n|English (cld3), caption length, and image size|14.6M|0.179|0.159|0.243|0.216|0.247|\n|CLIP B32 score top 1%|1.3M|0.025|0.037|0.140|0.076|0.126|\n|CLIP B32 score top 3%|3.9M|0.093|0.096|0.205|0.128|0.188|\n|CLIP B32 score top 10%|12.8M|0.231|0.199|0.305|0.206|0.298|\n|CLIP B32 score top 20%|25.7M|0.279|0.234|0.337|0.241|0.330|\n|CLIP B32 score top 30%|38.4M|0.285|0.240|0.355|0.253|0.338|\n|CLIP B32 score top 40%|51.3M|0.273|0.227|0.333|0.257|0.324|\n|CLIP B32 score top 50%|64.0M|0.256|0.219|0.322|0.259|0.316|\n|CLIP B32 score top 75%|96.1M|0.211|0.180|0.301|0.238|0.290|\n|CLIP B32 score top 90%|115M|0.189|0.165|0.279|0.229|0.274|\n|CLIP B32 threshold at 0.3 + English filter|9.4M|0.208|0.184|0.292|0.210|0.276|\n|CLIP B32 threshold at 0.28 + English filter|13.0M|0.230|0.198|0.307|0.233|0.292|\n|CLIP B32 threshold at 0.3|25.9M|0.282|0.233|0.340|0.243|0.333|\n|CLIP B32 score 1% to 30%|37.1M|0.287|0.238|0.347|0.253|0.334|\n|CLIP B32 score 2% to 30%|35.9M|0.288|0.238|0.338|0.248|0.330|\n|CLIP B32 score 5% to 30%|32.0M|0.281|0.230|0.352|0.254|0.339|\n|CLIP L14 score top 1%|1.3M|0.014|0.025|0.136|0.062|0.109|\n|CLIP L14 score top 3%|3.9M|0.065|0.077|0.176|0.103|0.160|\n|CLIP L14 score top 10%|12.8M|0.198|0.183|0.283|0.188|0.277|\n|CLIP L14 score top 20%|25.7M|0.260|0.225|0.326|0.235|0.322|\n|CLIP L14 score top 30%|38.4M|0.273|0.230|0.338|0.251|0.328|\n|CLIP L14 score top 40%|51.2M|0.262|0.226|0.330|0.260|0.327|\n|CLIP L14 score top 50%|64.1M|0.254|0.218|0.322|0.262|0.315|\n|CLIP L14 score top 75%|96.1M|0.212|0.180|0.287|0.242|0.285|\n|CLIP L14 score top 90%|115M|0.188|0.164|0.258|0.225|0.266|\n|Image-based clustering (ImageNet1k)|29.2M|0.268|0.213|0.319|0.256|0.312|\n|Image-based clustering (ImageNet21k)|45.1M|0.238|0.198|0.304|0.252|0.312|\n|Image-based sampling, \u03b1=0|128M|0.170|0.150|0.266|0.209|0.254|\n|Image-based sampling, \u03b1=0.2|128M|0.249|0.193|0.292|0.221|0.284|\n|Image-based sampling, \u03b1=0.5|128M|0.269|0.196|0.301|0.216|0.284|\n|Image-based sampling, \u03b1=1|128M|0.207|0.145|0.264|0.166|0.239|\n|Image-based sampling, \u03b1=2|128M|0.118|0.082|0.207|0.110|0.180|\n|ImageNet distance (L14, top 30%) and English|19.8M|0.212|0.158|0.272|0.178|0.259|\n|ImageNet distance (L/14, top 20%)|25.8M|0.193|0.138|0.276|0.176|0.252|\n|ImageNet distance (L/14, top 30%)| |38.5M|0.212|0.159|0.283|0.201|0.269|ImageNet distance (L/14, top 40%)|51.3M|0.212|0.165|0.273|0.212|0.270|\n|Text-based clustering (ImageNet1k)|4.3M|0.099|0.090|0.173|0.109|0.166|\n|Text-based clustering (ImageNet21k)|31.7M|0.255|0.215|0.328|0.249|0.307|\n|Text-based sampling with average score, \u03b1=0|128M|0.136|0.110|0.213|0.140|0.209|\n|Text-based sampling with average score, \u03b1=0.5|128M|0.222|0.178|0.273|0.206|0.269|\n|Text-based sampling with average score, \u03b1=1|128M|0.245|0.204|0.302|0.251|0.293|\n|Text-based sampling with average score, \u03b1=1.2|128M|0.231|0.200|0.298|0.240|0.289|\n|Text-based sampling with max score, \u03b1=0|128M|0.140|0.116|0.242|0.138|0.225|\n|Text-based sampling with max score, \u03b1=0.5|128M|0.229|0.190|0.290|0.205|0.283|\n|Text-based sampling with max score, \u03b1=1|128M|0.247|0.209|0.300|0.241|0.295|\n|Text-based sampling with max score, \u03b1=1.2|128M|0.235|0.200|0.298|0.239|0.290|\n|Intersect IN1k image clustering and CLIP B32 score top 30%|14.2M|0.305|0.243|0.342|0.250|0.328|\n|Intersect IN1k image clustering and CLIP L14 score top 30%|14.0M|0.297|0.239|0.346|0.231|0.328|\n|Intersect IN21k image clustering and CLIP B32 score top 30%|21.1M|0.298|0.244|0.347|0.256|0.336|\n|Intersect IN21k image clustering and CLIP L14 score top 30%|20.8M|0.290|0.241|0.339|0.244|0.328|", "images": [], "items": [{"type": "table", "rows": [["Filtering", "Training dataset size", "ImageNet", "ImageNet dist. shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["No filtering", "128M", "0.176", "0.152", "0.259", "0.219", "0.258"], ["Random subset (75%)", "96.0M", "0.175", "0.154", "0.265", "0.219", "0.257"], ["Random subset (50%)", "64.0M", "0.171", "0.151", "0.258", "0.216", "0.252"], ["Random subset (25%)", "32.0M", "0.155", "0.136", "0.246", "0.203", "0.240"], ["Random subset (10%)", "12.8M", "0.107", "0.095", "0.210", "0.144", "0.200"], ["Random subset (1%)", "1.3M", "0.009", "0.017", "0.102", "0.065", "0.090"], ["Caption length", "87.5M", "0.199", "0.172", "0.275", "0.236", "0.275"], ["Image size", "77.8M", "0.189", "0.163", "0.248", "0.231", "0.259"], ["English (fasttext)", "63.0M", "0.214", "0.182", "0.290", "0.246", "0.285"], ["English (fasttext) and caption length", "47.8M", "0.226", "0.193", "0.284", "0.251", "0.285"], ["English (fasttext), caption length, and image size", "29.8M", "0.226", "0.193", "0.297", "0.253", "0.294"], ["English (cld3)", "25.6M", "0.200", "0.175", "0.296", "0.235", "0.279"], ["English (cld3) and caption length", "22.9M", "0.204", "0.175", "0.287", "0.243", "0.278"], ["English (cld3), caption length, and image size", "14.6M", "0.179", "0.159", "0.243", "0.216", "0.247"], ["CLIP B32 score top 1%", "1.3M", "0.025", "0.037", "0.140", "0.076", "0.126"], ["CLIP B32 score top 3%", "3.9M", "0.093", "0.096", "0.205", "0.128", "0.188"], ["CLIP B32 score top 10%", "12.8M", "0.231", "0.199", "0.305", "0.206", "0.298"], ["CLIP B32 score top 20%", "25.7M", "0.279", "0.234", "0.337", "0.241", "0.330"], ["CLIP B32 score top 30%", "38.4M", "0.285", "0.240", "0.355", "0.253", "0.338"], ["CLIP B32 score top 40%", "51.3M", "0.273", "0.227", "0.333", "0.257", "0.324"], ["CLIP B32 score top 50%", "64.0M", "0.256", "0.219", "0.322", "0.259", "0.316"], ["CLIP B32 score top 75%", "96.1M", "0.211", "0.180", "0.301", "0.238", "0.290"], ["CLIP B32 score top 90%", "115M", "0.189", "0.165", "0.279", "0.229", "0.274"], ["CLIP B32 threshold at 0.3 + English filter", "9.4M", "0.208", "0.184", "0.292", "0.210", "0.276"], ["CLIP B32 threshold at 0.28 + English filter", "13.0M", "0.230", "0.198", "0.307", "0.233", "0.292"], ["CLIP B32 threshold at 0.3", "25.9M", "0.282", "0.233", "0.340", "0.243", "0.333"], ["CLIP B32 score 1% to 30%", "37.1M", "0.287", "0.238", "0.347", "0.253", "0.334"], ["CLIP B32 score 2% to 30%", "35.9M", "0.288", "0.238", "0.338", "0.248", "0.330"], ["CLIP B32 score 5% to 30%", "32.0M", "0.281", "0.230", "0.352", "0.254", "0.339"], ["CLIP L14 score top 1%", "1.3M", "0.014", "0.025", "0.136", "0.062", "0.109"], ["CLIP L14 score top 3%", "3.9M", "0.065", "0.077", "0.176", "0.103", "0.160"], ["CLIP L14 score top 10%", "12.8M", "0.198", "0.183", "0.283", "0.188", "0.277"], ["CLIP L14 score top 20%", "25.7M", "0.260", "0.225", "0.326", "0.235", "0.322"], ["CLIP L14 score top 30%", "38.4M", "0.273", "0.230", "0.338", "0.251", "0.328"], ["CLIP L14 score top 40%", "51.2M", "0.262", "0.226", "0.330", "0.260", "0.327"], ["CLIP L14 score top 50%", "64.1M", "0.254", "0.218", "0.322", "0.262", "0.315"], ["CLIP L14 score top 75%", "96.1M", "0.212", "0.180", "0.287", "0.242", "0.285"], ["CLIP L14 score top 90%", "115M", "0.188", "0.164", "0.258", "0.225", "0.266"], ["Image-based clustering (ImageNet1k)", "29.2M", "0.268", "0.213", "0.319", "0.256", "0.312"], ["Image-based clustering (ImageNet21k)", "45.1M", "0.238", "0.198", "0.304", "0.252", "0.312"], ["Image-based sampling, \u03b1=0", "128M", "0.170", "0.150", "0.266", "0.209", "0.254"], ["Image-based sampling, \u03b1=0.2", "128M", "0.249", "0.193", "0.292", "0.221", "0.284"], ["Image-based sampling, \u03b1=0.5", "128M", "0.269", "0.196", "0.301", "0.216", "0.284"], ["Image-based sampling, \u03b1=1", "128M", "0.207", "0.145", "0.264", "0.166", "0.239"], ["Image-based sampling, \u03b1=2", "128M", "0.118", "0.082", "0.207", "0.110", "0.180"], ["ImageNet distance (L14, top 30%) and English", "19.8M", "0.212", "0.158", "0.272", "0.178", "0.259"], ["ImageNet distance (L/14, top 20%)", "25.8M", "0.193", "0.138", "0.276", "0.176", "0.252"], ["ImageNet distance (L/14, top 30%)", "", "38.5M", "0.212", "0.159", "0.283", "0.201", "0.269", "ImageNet distance (L/14, top 40%)", "51.3M", "0.212", "0.165", "0.273", "0.212", "0.270"], ["Text-based clustering (ImageNet1k)", "4.3M", "0.099", "0.090", "0.173", "0.109", "0.166"], ["Text-based clustering (ImageNet21k)", "31.7M", "0.255", "0.215", "0.328", "0.249", "0.307"], ["Text-based sampling with average score, \u03b1=0", "128M", "0.136", "0.110", "0.213", "0.140", "0.209"], ["Text-based sampling with average score, \u03b1=0.5", "128M", "0.222", "0.178", "0.273", "0.206", "0.269"], ["Text-based sampling with average score, \u03b1=1", "128M", "0.245", "0.204", "0.302", "0.251", "0.293"], ["Text-based sampling with average score, \u03b1=1.2", "128M", "0.231", "0.200", "0.298", "0.240", "0.289"], ["Text-based sampling with max score, \u03b1=0", "128M", "0.140", "0.116", "0.242", "0.138", "0.225"], ["Text-based sampling with max score, \u03b1=0.5", "128M", "0.229", "0.190", "0.290", "0.205", "0.283"], ["Text-based sampling with max score, \u03b1=1", "128M", "0.247", "0.209", "0.300", "0.241", "0.295"], ["Text-based sampling with max score, \u03b1=1.2", "128M", "0.235", "0.200", "0.298", "0.239", "0.290"], ["Intersect IN1k image clustering and CLIP B32 score top 30%", "14.2M", "0.305", "0.243", "0.342", "0.250", "0.328"], ["Intersect IN1k image clustering and CLIP L14 score top 30%", "14.0M", "0.297", "0.239", "0.346", "0.231", "0.328"], ["Intersect IN21k image clustering and CLIP B32 score top 30%", "21.1M", "0.298", "0.244", "0.347", "0.256", "0.336"], ["Intersect IN21k image clustering and CLIP L14 score top 30%", "20.8M", "0.290", "0.241", "0.339", "0.244", "0.328"]], "md": "|Filtering|Training dataset size|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|No filtering|128M|0.176|0.152|0.259|0.219|0.258|\n|Random subset (75%)|96.0M|0.175|0.154|0.265|0.219|0.257|\n|Random subset (50%)|64.0M|0.171|0.151|0.258|0.216|0.252|\n|Random subset (25%)|32.0M|0.155|0.136|0.246|0.203|0.240|\n|Random subset (10%)|12.8M|0.107|0.095|0.210|0.144|0.200|\n|Random subset (1%)|1.3M|0.009|0.017|0.102|0.065|0.090|\n|Caption length|87.5M|0.199|0.172|0.275|0.236|0.275|\n|Image size|77.8M|0.189|0.163|0.248|0.231|0.259|\n|English (fasttext)|63.0M|0.214|0.182|0.290|0.246|0.285|\n|English (fasttext) and caption length|47.8M|0.226|0.193|0.284|0.251|0.285|\n|English (fasttext), caption length, and image size|29.8M|0.226|0.193|0.297|0.253|0.294|\n|English (cld3)|25.6M|0.200|0.175|0.296|0.235|0.279|\n|English (cld3) and caption length|22.9M|0.204|0.175|0.287|0.243|0.278|\n|English (cld3), caption length, and image size|14.6M|0.179|0.159|0.243|0.216|0.247|\n|CLIP B32 score top 1%|1.3M|0.025|0.037|0.140|0.076|0.126|\n|CLIP B32 score top 3%|3.9M|0.093|0.096|0.205|0.128|0.188|\n|CLIP B32 score top 10%|12.8M|0.231|0.199|0.305|0.206|0.298|\n|CLIP B32 score top 20%|25.7M|0.279|0.234|0.337|0.241|0.330|\n|CLIP B32 score top 30%|38.4M|0.285|0.240|0.355|0.253|0.338|\n|CLIP B32 score top 40%|51.3M|0.273|0.227|0.333|0.257|0.324|\n|CLIP B32 score top 50%|64.0M|0.256|0.219|0.322|0.259|0.316|\n|CLIP B32 score top 75%|96.1M|0.211|0.180|0.301|0.238|0.290|\n|CLIP B32 score top 90%|115M|0.189|0.165|0.279|0.229|0.274|\n|CLIP B32 threshold at 0.3 + English filter|9.4M|0.208|0.184|0.292|0.210|0.276|\n|CLIP B32 threshold at 0.28 + English filter|13.0M|0.230|0.198|0.307|0.233|0.292|\n|CLIP B32 threshold at 0.3|25.9M|0.282|0.233|0.340|0.243|0.333|\n|CLIP B32 score 1% to 30%|37.1M|0.287|0.238|0.347|0.253|0.334|\n|CLIP B32 score 2% to 30%|35.9M|0.288|0.238|0.338|0.248|0.330|\n|CLIP B32 score 5% to 30%|32.0M|0.281|0.230|0.352|0.254|0.339|\n|CLIP L14 score top 1%|1.3M|0.014|0.025|0.136|0.062|0.109|\n|CLIP L14 score top 3%|3.9M|0.065|0.077|0.176|0.103|0.160|\n|CLIP L14 score top 10%|12.8M|0.198|0.183|0.283|0.188|0.277|\n|CLIP L14 score top 20%|25.7M|0.260|0.225|0.326|0.235|0.322|\n|CLIP L14 score top 30%|38.4M|0.273|0.230|0.338|0.251|0.328|\n|CLIP L14 score top 40%|51.2M|0.262|0.226|0.330|0.260|0.327|\n|CLIP L14 score top 50%|64.1M|0.254|0.218|0.322|0.262|0.315|\n|CLIP L14 score top 75%|96.1M|0.212|0.180|0.287|0.242|0.285|\n|CLIP L14 score top 90%|115M|0.188|0.164|0.258|0.225|0.266|\n|Image-based clustering (ImageNet1k)|29.2M|0.268|0.213|0.319|0.256|0.312|\n|Image-based clustering (ImageNet21k)|45.1M|0.238|0.198|0.304|0.252|0.312|\n|Image-based sampling, \u03b1=0|128M|0.170|0.150|0.266|0.209|0.254|\n|Image-based sampling, \u03b1=0.2|128M|0.249|0.193|0.292|0.221|0.284|\n|Image-based sampling, \u03b1=0.5|128M|0.269|0.196|0.301|0.216|0.284|\n|Image-based sampling, \u03b1=1|128M|0.207|0.145|0.264|0.166|0.239|\n|Image-based sampling, \u03b1=2|128M|0.118|0.082|0.207|0.110|0.180|\n|ImageNet distance (L14, top 30%) and English|19.8M|0.212|0.158|0.272|0.178|0.259|\n|ImageNet distance (L/14, top 20%)|25.8M|0.193|0.138|0.276|0.176|0.252|\n|ImageNet distance (L/14, top 30%)| |38.5M|0.212|0.159|0.283|0.201|0.269|ImageNet distance (L/14, top 40%)|51.3M|0.212|0.165|0.273|0.212|0.270|\n|Text-based clustering (ImageNet1k)|4.3M|0.099|0.090|0.173|0.109|0.166|\n|Text-based clustering (ImageNet21k)|31.7M|0.255|0.215|0.328|0.249|0.307|\n|Text-based sampling with average score, \u03b1=0|128M|0.136|0.110|0.213|0.140|0.209|\n|Text-based sampling with average score, \u03b1=0.5|128M|0.222|0.178|0.273|0.206|0.269|\n|Text-based sampling with average score, \u03b1=1|128M|0.245|0.204|0.302|0.251|0.293|\n|Text-based sampling with average score, \u03b1=1.2|128M|0.231|0.200|0.298|0.240|0.289|\n|Text-based sampling with max score, \u03b1=0|128M|0.140|0.116|0.242|0.138|0.225|\n|Text-based sampling with max score, \u03b1=0.5|128M|0.229|0.190|0.290|0.205|0.283|\n|Text-based sampling with max score, \u03b1=1|128M|0.247|0.209|0.300|0.241|0.295|\n|Text-based sampling with max score, \u03b1=1.2|128M|0.235|0.200|0.298|0.239|0.290|\n|Intersect IN1k image clustering and CLIP B32 score top 30%|14.2M|0.305|0.243|0.342|0.250|0.328|\n|Intersect IN1k image clustering and CLIP L14 score top 30%|14.0M|0.297|0.239|0.346|0.231|0.328|\n|Intersect IN21k image clustering and CLIP B32 score top 30%|21.1M|0.298|0.244|0.347|0.256|0.336|\n|Intersect IN21k image clustering and CLIP L14 score top 30%|20.8M|0.290|0.241|0.339|0.244|0.328|", "isPerfectTable": false, "csv": "\"Filtering\",\"Training dataset size\",\"ImageNet\",\"ImageNet dist. shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"No filtering\",\"128M\",\"0.176\",\"0.152\",\"0.259\",\"0.219\",\"0.258\"\n\"Random subset (75%)\",\"96.0M\",\"0.175\",\"0.154\",\"0.265\",\"0.219\",\"0.257\"\n\"Random subset (50%)\",\"64.0M\",\"0.171\",\"0.151\",\"0.258\",\"0.216\",\"0.252\"\n\"Random subset (25%)\",\"32.0M\",\"0.155\",\"0.136\",\"0.246\",\"0.203\",\"0.240\"\n\"Random subset (10%)\",\"12.8M\",\"0.107\",\"0.095\",\"0.210\",\"0.144\",\"0.200\"\n\"Random subset (1%)\",\"1.3M\",\"0.009\",\"0.017\",\"0.102\",\"0.065\",\"0.090\"\n\"Caption length\",\"87.5M\",\"0.199\",\"0.172\",\"0.275\",\"0.236\",\"0.275\"\n\"Image size\",\"77.8M\",\"0.189\",\"0.163\",\"0.248\",\"0.231\",\"0.259\"\n\"English (fasttext)\",\"63.0M\",\"0.214\",\"0.182\",\"0.290\",\"0.246\",\"0.285\"\n\"English (fasttext) and caption length\",\"47.8M\",\"0.226\",\"0.193\",\"0.284\",\"0.251\",\"0.285\"\n\"English (fasttext), caption length, and image size\",\"29.8M\",\"0.226\",\"0.193\",\"0.297\",\"0.253\",\"0.294\"\n\"English (cld3)\",\"25.6M\",\"0.200\",\"0.175\",\"0.296\",\"0.235\",\"0.279\"\n\"English (cld3) and caption length\",\"22.9M\",\"0.204\",\"0.175\",\"0.287\",\"0.243\",\"0.278\"\n\"English (cld3), caption length, and image size\",\"14.6M\",\"0.179\",\"0.159\",\"0.243\",\"0.216\",\"0.247\"\n\"CLIP B32 score top 1%\",\"1.3M\",\"0.025\",\"0.037\",\"0.140\",\"0.076\",\"0.126\"\n\"CLIP B32 score top 3%\",\"3.9M\",\"0.093\",\"0.096\",\"0.205\",\"0.128\",\"0.188\"\n\"CLIP B32 score top 10%\",\"12.8M\",\"0.231\",\"0.199\",\"0.305\",\"0.206\",\"0.298\"\n\"CLIP B32 score top 20%\",\"25.7M\",\"0.279\",\"0.234\",\"0.337\",\"0.241\",\"0.330\"\n\"CLIP B32 score top 30%\",\"38.4M\",\"0.285\",\"0.240\",\"0.355\",\"0.253\",\"0.338\"\n\"CLIP B32 score top 40%\",\"51.3M\",\"0.273\",\"0.227\",\"0.333\",\"0.257\",\"0.324\"\n\"CLIP B32 score top 50%\",\"64.0M\",\"0.256\",\"0.219\",\"0.322\",\"0.259\",\"0.316\"\n\"CLIP B32 score top 75%\",\"96.1M\",\"0.211\",\"0.180\",\"0.301\",\"0.238\",\"0.290\"\n\"CLIP B32 score top 90%\",\"115M\",\"0.189\",\"0.165\",\"0.279\",\"0.229\",\"0.274\"\n\"CLIP B32 threshold at 0.3 + English filter\",\"9.4M\",\"0.208\",\"0.184\",\"0.292\",\"0.210\",\"0.276\"\n\"CLIP B32 threshold at 0.28 + English filter\",\"13.0M\",\"0.230\",\"0.198\",\"0.307\",\"0.233\",\"0.292\"\n\"CLIP B32 threshold at 0.3\",\"25.9M\",\"0.282\",\"0.233\",\"0.340\",\"0.243\",\"0.333\"\n\"CLIP B32 score 1% to 30%\",\"37.1M\",\"0.287\",\"0.238\",\"0.347\",\"0.253\",\"0.334\"\n\"CLIP B32 score 2% to 30%\",\"35.9M\",\"0.288\",\"0.238\",\"0.338\",\"0.248\",\"0.330\"\n\"CLIP B32 score 5% to 30%\",\"32.0M\",\"0.281\",\"0.230\",\"0.352\",\"0.254\",\"0.339\"\n\"CLIP L14 score top 1%\",\"1.3M\",\"0.014\",\"0.025\",\"0.136\",\"0.062\",\"0.109\"\n\"CLIP L14 score top 3%\",\"3.9M\",\"0.065\",\"0.077\",\"0.176\",\"0.103\",\"0.160\"\n\"CLIP L14 score top 10%\",\"12.8M\",\"0.198\",\"0.183\",\"0.283\",\"0.188\",\"0.277\"\n\"CLIP L14 score top 20%\",\"25.7M\",\"0.260\",\"0.225\",\"0.326\",\"0.235\",\"0.322\"\n\"CLIP L14 score top 30%\",\"38.4M\",\"0.273\",\"0.230\",\"0.338\",\"0.251\",\"0.328\"\n\"CLIP L14 score top 40%\",\"51.2M\",\"0.262\",\"0.226\",\"0.330\",\"0.260\",\"0.327\"\n\"CLIP L14 score top 50%\",\"64.1M\",\"0.254\",\"0.218\",\"0.322\",\"0.262\",\"0.315\"\n\"CLIP L14 score top 75%\",\"96.1M\",\"0.212\",\"0.180\",\"0.287\",\"0.242\",\"0.285\"\n\"CLIP L14 score top 90%\",\"115M\",\"0.188\",\"0.164\",\"0.258\",\"0.225\",\"0.266\"\n\"Image-based clustering (ImageNet1k)\",\"29.2M\",\"0.268\",\"0.213\",\"0.319\",\"0.256\",\"0.312\"\n\"Image-based clustering (ImageNet21k)\",\"45.1M\",\"0.238\",\"0.198\",\"0.304\",\"0.252\",\"0.312\"\n\"Image-based sampling, \u03b1=0\",\"128M\",\"0.170\",\"0.150\",\"0.266\",\"0.209\",\"0.254\"\n\"Image-based sampling, \u03b1=0.2\",\"128M\",\"0.249\",\"0.193\",\"0.292\",\"0.221\",\"0.284\"\n\"Image-based sampling, \u03b1=0.5\",\"128M\",\"0.269\",\"0.196\",\"0.301\",\"0.216\",\"0.284\"\n\"Image-based sampling, \u03b1=1\",\"128M\",\"0.207\",\"0.145\",\"0.264\",\"0.166\",\"0.239\"\n\"Image-based sampling, \u03b1=2\",\"128M\",\"0.118\",\"0.082\",\"0.207\",\"0.110\",\"0.180\"\n\"ImageNet distance (L14, top 30%) and English\",\"19.8M\",\"0.212\",\"0.158\",\"0.272\",\"0.178\",\"0.259\"\n\"ImageNet distance (L/14, top 20%)\",\"25.8M\",\"0.193\",\"0.138\",\"0.276\",\"0.176\",\"0.252\"\n\"ImageNet distance (L/14, top 30%)\",\"\",\"38.5M\",\"0.212\",\"0.159\",\"0.283\",\"0.201\",\"0.269\",\"ImageNet distance (L/14, top 40%)\",\"51.3M\",\"0.212\",\"0.165\",\"0.273\",\"0.212\",\"0.270\"\n\"Text-based clustering (ImageNet1k)\",\"4.3M\",\"0.099\",\"0.090\",\"0.173\",\"0.109\",\"0.166\"\n\"Text-based clustering (ImageNet21k)\",\"31.7M\",\"0.255\",\"0.215\",\"0.328\",\"0.249\",\"0.307\"\n\"Text-based sampling with average score, \u03b1=0\",\"128M\",\"0.136\",\"0.110\",\"0.213\",\"0.140\",\"0.209\"\n\"Text-based sampling with average score, \u03b1=0.5\",\"128M\",\"0.222\",\"0.178\",\"0.273\",\"0.206\",\"0.269\"\n\"Text-based sampling with average score, \u03b1=1\",\"128M\",\"0.245\",\"0.204\",\"0.302\",\"0.251\",\"0.293\"\n\"Text-based sampling with average score, \u03b1=1.2\",\"128M\",\"0.231\",\"0.200\",\"0.298\",\"0.240\",\"0.289\"\n\"Text-based sampling with max score, \u03b1=0\",\"128M\",\"0.140\",\"0.116\",\"0.242\",\"0.138\",\"0.225\"\n\"Text-based sampling with max score, \u03b1=0.5\",\"128M\",\"0.229\",\"0.190\",\"0.290\",\"0.205\",\"0.283\"\n\"Text-based sampling with max score, \u03b1=1\",\"128M\",\"0.247\",\"0.209\",\"0.300\",\"0.241\",\"0.295\"\n\"Text-based sampling with max score, \u03b1=1.2\",\"128M\",\"0.235\",\"0.200\",\"0.298\",\"0.239\",\"0.290\"\n\"Intersect IN1k image clustering and CLIP B32 score top 30%\",\"14.2M\",\"0.305\",\"0.243\",\"0.342\",\"0.250\",\"0.328\"\n\"Intersect IN1k image clustering and CLIP L14 score top 30%\",\"14.0M\",\"0.297\",\"0.239\",\"0.346\",\"0.231\",\"0.328\"\n\"Intersect IN21k image clustering and CLIP B32 score top 30%\",\"21.1M\",\"0.298\",\"0.244\",\"0.347\",\"0.256\",\"0.336\"\n\"Intersect IN21k image clustering and CLIP L14 score top 30%\",\"20.8M\",\"0.290\",\"0.241\",\"0.339\",\"0.244\",\"0.328\""}]}, {"page": 59, "text": "                              Table 27: Baseline results for the filtering track, large scale.\nFiltering                                                                    Training        ImageNet   ImageNet       VTAB   Retrieval  Average over\n                                                                            dataset size                dist. shifts                       38 datasets\nNo filtering                                                                   1.28B           0.459       0.378       0.426    0.419         0.437\nRandom subset (75%)                                                            960M            0.456       0.379       0.435    0.415         0.442\nRandom subset (50%)                                                            640M            0.453       0.377       0.427    0.413         0.433\nRandom subset (25%)                                                            320M            0.447       0.373       0.424    0.407         0.434\nRandom subset (10%)                                                            128M            0.426       0.350       0.417    0.396         0.442\nRandom subset (1%)                                                             12.8M           0.135       0.118       0.219    0.135         0.218\nCaption length                                                                 874M            0.474       0.392       0.438    0.443         0.445\nImage size                                                                     777M            0.466       0.375       0.421    0.438         0.429\nEnglish (fasttext)                                                             630M            0.500       0.414       0.449    0.460         0.462\nEnglish (fasttext), caption length, and image size                             298M            0.516       0.423       0.446    0.480         0.458\nEnglish (cld3)                                                                 256M            0.486       0.405       0.462    0.472         0.458\nCLIP B32 score top 10%                                                         128M            0.543       0.440       0.471    0.435         0.483\nCLIP B32 score top 20%                                                         257M            0.578       0.465       0.516    0.463         0.515\nCLIP B32 score top 30%                                                         384M            0.578       0.466       0.525    0.475         0.527\nCLIP B32 score top 40%                                                         512M            0.560       0.454       0.512    0.478         0.511\nCLIP B32 score top 50%                                                         640M            0.546       0.450       0.504    0.484         0.505\nCLIP B32 threshold at 0.3 + English filter                                     94.3M           0.553       0.447       0.511    0.482         0.502\nCLIP B32 threshold at 0.28 + English filter                                    130M            0.553       0.453       0.510    0.495         0.501\nCLIP B32 threshold at 0.3                                                      258M            0.579       0.464       0.501    0.465         0.505\nCLIP L14 score top 10%                                                         128M            0.528       0.444       0.482    0.413         0.486\nCLIP L14 score top 20%                                                         257M            0.570       0.466       0.524    0.455         0.521\nCLIP L14 score top 30%                                                         384M            0.578       0.474       0.538    0.466         0.529\nCLIP L14 score top 40%                                                         512M            0.564       0.462       0.533    0.468         0.529\nCLIP L14 score top 50%                                                         641M            0.548       0.455       0.539    0.469         0.528\nImage-based clustering (ImageNet1k)                                            294M            0.572       0.454       0.483    0.481         0.481\nImage-based clustering (ImageNet21k)                                           450M            0.527       0.433       0.468    0.463         0.471\nText-based clustering (ImageNet1k)                                             42.7M           0.419       0.355       0.340    0.309         0.361\nText-based clustering (ImageNet21k)                                            317M            0.561       0.465       0.465    0.479         0.476\nIntersect IN1k image clustering and CLIP B32 score top 30%                     143M            0.632       0.498       0.525    0.504         0.528\nIntersect IN1k image clustering and CLIP L14 score top 30%                     140M            0.631       0.508       0.546    0.498         0.537\nIntersect IN21k image clustering and CLIP B32 score top 30%                    211M            0.605       0.481       0.531    0.494         0.519\nIntersect IN21k image clustering and CLIP L14 score top 30%                    208M            0.506       0.416       0.466    0.424         0.471\n                             Table 28: Baseline results for the filtering track, xlarge scale.\nFiltering                                                                    Training       ImageNet    ImageNet      VTAB    Retrieval  Average over\n                                                                           dataset size                dist. shifts                        38 datasets\nNo filtering                                                                  12.8B            0.723      0.612        0.611    0.569         0.621\nCLIP B32 score top 30%                                                        3.84B            0.764      0.640        0.628    0.599         0.638\nCLIP B32 threshold at 0.28 + English filter                                    1.3B            0.755      0.637        0.624    0.620         0.636\nCLIP L14 score top 20%                                                        2.56B            0.761      0.649        0.630    0.575         0.636\nCLIP L14 score top 25%                                                         3.2B            0.768      0.656        0.621    0.585         0.637\nCLIP L14 score top 30%                                                        3.84B            0.764      0.655        0.643    0.588         0.650\nIntersect IN1k image clustering and CLIP L14 score top 30%                    1.38B            0.792      0.679        0.652    0.608         0.663\n                                                                                59", "md": "|Filtering|Training dataset size|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|No filtering|1.28B|0.459|0.378|0.426|0.419|0.437|\n|Random subset (75%)|960M|0.456|0.379|0.435|0.415|0.442|\n|Random subset (50%)|640M|0.453|0.377|0.427|0.413|0.433|\n|Random subset (25%)|320M|0.447|0.373|0.424|0.407|0.434|\n|Random subset (10%)|128M|0.426|0.350|0.417|0.396|0.442|\n|Random subset (1%)|12.8M|0.135|0.118|0.219|0.135|0.218|\n|Caption length|874M|0.474|0.392|0.438|0.443|0.445|\n|Image size|777M|0.466|0.375|0.421|0.438|0.429|\n|English (fasttext)|630M|0.500|0.414|0.449|0.460|0.462|\n|English (fasttext), caption length, and image size|298M|0.516|0.423|0.446|0.480|0.458|\n|English (cld3)|256M|0.486|0.405|0.462|0.472|0.458|\n|CLIP B32 score top 10%|128M|0.543|0.440|0.471|0.435|0.483|\n|CLIP B32 score top 20%|257M|0.578|0.465|0.516|0.463|0.515|\n|CLIP B32 score top 30%|384M|0.578|0.466|0.525|0.475|0.527|\n|CLIP B32 score top 40%|512M|0.560|0.454|0.512|0.478|0.511|\n|CLIP B32 score top 50%|640M|0.546|0.450|0.504|0.484|0.505|\n|CLIP B32 threshold at 0.3 + English filter|94.3M|0.553|0.447|0.511|0.482|0.502|\n|CLIP B32 threshold at 0.28 + English filter|130M|0.553|0.453|0.510|0.495|0.501|\n|CLIP B32 threshold at 0.3|258M|0.579|0.464|0.501|0.465|0.505|\n|CLIP L14 score top 10%|128M|0.528|0.444|0.482|0.413|0.486|\n|CLIP L14 score top 20%|257M|0.570|0.466|0.524|0.455|0.521|\n|CLIP L14 score top 30%|384M|0.578|0.474|0.538|0.466|0.529|\n|CLIP L14 score top 40%|512M|0.564|0.462|0.533|0.468|0.529|\n|CLIP L14 score top 50%|641M|0.548|0.455|0.539|0.469|0.528|\n|Image-based clustering (ImageNet1k)|294M|0.572|0.454|0.483|0.481|0.481|\n|Image-based clustering (ImageNet21k)|450M|0.527|0.433|0.468|0.463|0.471|\n|Text-based clustering (ImageNet1k)|42.7M|0.419|0.355|0.340|0.309|0.361|\n|Text-based clustering (ImageNet21k)|317M|0.561|0.465|0.465|0.479|0.476|\n|Intersect IN1k image clustering and CLIP B32 score top 30%|143M|0.632|0.498|0.525|0.504|0.528|\n|Intersect IN1k image clustering and CLIP L14 score top 30%|140M|0.631|0.508|0.546|0.498|0.537|\n|Intersect IN21k image clustering and CLIP B32 score top 30%|211M|0.605|0.481|0.531|0.494|0.519|\n|Intersect IN21k image clustering and CLIP L14 score top 30%|208M|0.506|0.416|0.466|0.424|0.471|\n\n|Filtering|Training dataset size|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|No filtering|12.8B|0.723|0.612|0.611|0.569|0.621|\n|CLIP B32 score top 30%|3.84B|0.764|0.640|0.628|0.599|0.638|\n|CLIP B32 threshold at 0.28 + English filter|1.3B|0.755|0.637|0.624|0.620|0.636|\n|CLIP L14 score top 20%|2.56B|0.761|0.649|0.630|0.575|0.636|\n|CLIP L14 score top 25%|3.2B|0.768|0.656|0.621|0.585|0.637|\n|CLIP L14 score top 30%|3.84B|0.764|0.655|0.643|0.588|0.650|\n|Intersect IN1k image clustering and CLIP L14 score top 30%|1.38B|0.792|0.679|0.652|0.608|0.663|", "images": [], "items": [{"type": "table", "rows": [["Filtering", "Training dataset size", "ImageNet", "ImageNet dist. shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["No filtering", "1.28B", "0.459", "0.378", "0.426", "0.419", "0.437"], ["Random subset (75%)", "960M", "0.456", "0.379", "0.435", "0.415", "0.442"], ["Random subset (50%)", "640M", "0.453", "0.377", "0.427", "0.413", "0.433"], ["Random subset (25%)", "320M", "0.447", "0.373", "0.424", "0.407", "0.434"], ["Random subset (10%)", "128M", "0.426", "0.350", "0.417", "0.396", "0.442"], ["Random subset (1%)", "12.8M", "0.135", "0.118", "0.219", "0.135", "0.218"], ["Caption length", "874M", "0.474", "0.392", "0.438", "0.443", "0.445"], ["Image size", "777M", "0.466", "0.375", "0.421", "0.438", "0.429"], ["English (fasttext)", "630M", "0.500", "0.414", "0.449", "0.460", "0.462"], ["English (fasttext), caption length, and image size", "298M", "0.516", "0.423", "0.446", "0.480", "0.458"], ["English (cld3)", "256M", "0.486", "0.405", "0.462", "0.472", "0.458"], ["CLIP B32 score top 10%", "128M", "0.543", "0.440", "0.471", "0.435", "0.483"], ["CLIP B32 score top 20%", "257M", "0.578", "0.465", "0.516", "0.463", "0.515"], ["CLIP B32 score top 30%", "384M", "0.578", "0.466", "0.525", "0.475", "0.527"], ["CLIP B32 score top 40%", "512M", "0.560", "0.454", "0.512", "0.478", "0.511"], ["CLIP B32 score top 50%", "640M", "0.546", "0.450", "0.504", "0.484", "0.505"], ["CLIP B32 threshold at 0.3 + English filter", "94.3M", "0.553", "0.447", "0.511", "0.482", "0.502"], ["CLIP B32 threshold at 0.28 + English filter", "130M", "0.553", "0.453", "0.510", "0.495", "0.501"], ["CLIP B32 threshold at 0.3", "258M", "0.579", "0.464", "0.501", "0.465", "0.505"], ["CLIP L14 score top 10%", "128M", "0.528", "0.444", "0.482", "0.413", "0.486"], ["CLIP L14 score top 20%", "257M", "0.570", "0.466", "0.524", "0.455", "0.521"], ["CLIP L14 score top 30%", "384M", "0.578", "0.474", "0.538", "0.466", "0.529"], ["CLIP L14 score top 40%", "512M", "0.564", "0.462", "0.533", "0.468", "0.529"], ["CLIP L14 score top 50%", "641M", "0.548", "0.455", "0.539", "0.469", "0.528"], ["Image-based clustering (ImageNet1k)", "294M", "0.572", "0.454", "0.483", "0.481", "0.481"], ["Image-based clustering (ImageNet21k)", "450M", "0.527", "0.433", "0.468", "0.463", "0.471"], ["Text-based clustering (ImageNet1k)", "42.7M", "0.419", "0.355", "0.340", "0.309", "0.361"], ["Text-based clustering (ImageNet21k)", "317M", "0.561", "0.465", "0.465", "0.479", "0.476"], ["Intersect IN1k image clustering and CLIP B32 score top 30%", "143M", "0.632", "0.498", "0.525", "0.504", "0.528"], ["Intersect IN1k image clustering and CLIP L14 score top 30%", "140M", "0.631", "0.508", "0.546", "0.498", "0.537"], ["Intersect IN21k image clustering and CLIP B32 score top 30%", "211M", "0.605", "0.481", "0.531", "0.494", "0.519"], ["Intersect IN21k image clustering and CLIP L14 score top 30%", "208M", "0.506", "0.416", "0.466", "0.424", "0.471"]], "md": "|Filtering|Training dataset size|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|No filtering|1.28B|0.459|0.378|0.426|0.419|0.437|\n|Random subset (75%)|960M|0.456|0.379|0.435|0.415|0.442|\n|Random subset (50%)|640M|0.453|0.377|0.427|0.413|0.433|\n|Random subset (25%)|320M|0.447|0.373|0.424|0.407|0.434|\n|Random subset (10%)|128M|0.426|0.350|0.417|0.396|0.442|\n|Random subset (1%)|12.8M|0.135|0.118|0.219|0.135|0.218|\n|Caption length|874M|0.474|0.392|0.438|0.443|0.445|\n|Image size|777M|0.466|0.375|0.421|0.438|0.429|\n|English (fasttext)|630M|0.500|0.414|0.449|0.460|0.462|\n|English (fasttext), caption length, and image size|298M|0.516|0.423|0.446|0.480|0.458|\n|English (cld3)|256M|0.486|0.405|0.462|0.472|0.458|\n|CLIP B32 score top 10%|128M|0.543|0.440|0.471|0.435|0.483|\n|CLIP B32 score top 20%|257M|0.578|0.465|0.516|0.463|0.515|\n|CLIP B32 score top 30%|384M|0.578|0.466|0.525|0.475|0.527|\n|CLIP B32 score top 40%|512M|0.560|0.454|0.512|0.478|0.511|\n|CLIP B32 score top 50%|640M|0.546|0.450|0.504|0.484|0.505|\n|CLIP B32 threshold at 0.3 + English filter|94.3M|0.553|0.447|0.511|0.482|0.502|\n|CLIP B32 threshold at 0.28 + English filter|130M|0.553|0.453|0.510|0.495|0.501|\n|CLIP B32 threshold at 0.3|258M|0.579|0.464|0.501|0.465|0.505|\n|CLIP L14 score top 10%|128M|0.528|0.444|0.482|0.413|0.486|\n|CLIP L14 score top 20%|257M|0.570|0.466|0.524|0.455|0.521|\n|CLIP L14 score top 30%|384M|0.578|0.474|0.538|0.466|0.529|\n|CLIP L14 score top 40%|512M|0.564|0.462|0.533|0.468|0.529|\n|CLIP L14 score top 50%|641M|0.548|0.455|0.539|0.469|0.528|\n|Image-based clustering (ImageNet1k)|294M|0.572|0.454|0.483|0.481|0.481|\n|Image-based clustering (ImageNet21k)|450M|0.527|0.433|0.468|0.463|0.471|\n|Text-based clustering (ImageNet1k)|42.7M|0.419|0.355|0.340|0.309|0.361|\n|Text-based clustering (ImageNet21k)|317M|0.561|0.465|0.465|0.479|0.476|\n|Intersect IN1k image clustering and CLIP B32 score top 30%|143M|0.632|0.498|0.525|0.504|0.528|\n|Intersect IN1k image clustering and CLIP L14 score top 30%|140M|0.631|0.508|0.546|0.498|0.537|\n|Intersect IN21k image clustering and CLIP B32 score top 30%|211M|0.605|0.481|0.531|0.494|0.519|\n|Intersect IN21k image clustering and CLIP L14 score top 30%|208M|0.506|0.416|0.466|0.424|0.471|", "isPerfectTable": true, "csv": "\"Filtering\",\"Training dataset size\",\"ImageNet\",\"ImageNet dist. shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"No filtering\",\"1.28B\",\"0.459\",\"0.378\",\"0.426\",\"0.419\",\"0.437\"\n\"Random subset (75%)\",\"960M\",\"0.456\",\"0.379\",\"0.435\",\"0.415\",\"0.442\"\n\"Random subset (50%)\",\"640M\",\"0.453\",\"0.377\",\"0.427\",\"0.413\",\"0.433\"\n\"Random subset (25%)\",\"320M\",\"0.447\",\"0.373\",\"0.424\",\"0.407\",\"0.434\"\n\"Random subset (10%)\",\"128M\",\"0.426\",\"0.350\",\"0.417\",\"0.396\",\"0.442\"\n\"Random subset (1%)\",\"12.8M\",\"0.135\",\"0.118\",\"0.219\",\"0.135\",\"0.218\"\n\"Caption length\",\"874M\",\"0.474\",\"0.392\",\"0.438\",\"0.443\",\"0.445\"\n\"Image size\",\"777M\",\"0.466\",\"0.375\",\"0.421\",\"0.438\",\"0.429\"\n\"English (fasttext)\",\"630M\",\"0.500\",\"0.414\",\"0.449\",\"0.460\",\"0.462\"\n\"English (fasttext), caption length, and image size\",\"298M\",\"0.516\",\"0.423\",\"0.446\",\"0.480\",\"0.458\"\n\"English (cld3)\",\"256M\",\"0.486\",\"0.405\",\"0.462\",\"0.472\",\"0.458\"\n\"CLIP B32 score top 10%\",\"128M\",\"0.543\",\"0.440\",\"0.471\",\"0.435\",\"0.483\"\n\"CLIP B32 score top 20%\",\"257M\",\"0.578\",\"0.465\",\"0.516\",\"0.463\",\"0.515\"\n\"CLIP B32 score top 30%\",\"384M\",\"0.578\",\"0.466\",\"0.525\",\"0.475\",\"0.527\"\n\"CLIP B32 score top 40%\",\"512M\",\"0.560\",\"0.454\",\"0.512\",\"0.478\",\"0.511\"\n\"CLIP B32 score top 50%\",\"640M\",\"0.546\",\"0.450\",\"0.504\",\"0.484\",\"0.505\"\n\"CLIP B32 threshold at 0.3 + English filter\",\"94.3M\",\"0.553\",\"0.447\",\"0.511\",\"0.482\",\"0.502\"\n\"CLIP B32 threshold at 0.28 + English filter\",\"130M\",\"0.553\",\"0.453\",\"0.510\",\"0.495\",\"0.501\"\n\"CLIP B32 threshold at 0.3\",\"258M\",\"0.579\",\"0.464\",\"0.501\",\"0.465\",\"0.505\"\n\"CLIP L14 score top 10%\",\"128M\",\"0.528\",\"0.444\",\"0.482\",\"0.413\",\"0.486\"\n\"CLIP L14 score top 20%\",\"257M\",\"0.570\",\"0.466\",\"0.524\",\"0.455\",\"0.521\"\n\"CLIP L14 score top 30%\",\"384M\",\"0.578\",\"0.474\",\"0.538\",\"0.466\",\"0.529\"\n\"CLIP L14 score top 40%\",\"512M\",\"0.564\",\"0.462\",\"0.533\",\"0.468\",\"0.529\"\n\"CLIP L14 score top 50%\",\"641M\",\"0.548\",\"0.455\",\"0.539\",\"0.469\",\"0.528\"\n\"Image-based clustering (ImageNet1k)\",\"294M\",\"0.572\",\"0.454\",\"0.483\",\"0.481\",\"0.481\"\n\"Image-based clustering (ImageNet21k)\",\"450M\",\"0.527\",\"0.433\",\"0.468\",\"0.463\",\"0.471\"\n\"Text-based clustering (ImageNet1k)\",\"42.7M\",\"0.419\",\"0.355\",\"0.340\",\"0.309\",\"0.361\"\n\"Text-based clustering (ImageNet21k)\",\"317M\",\"0.561\",\"0.465\",\"0.465\",\"0.479\",\"0.476\"\n\"Intersect IN1k image clustering and CLIP B32 score top 30%\",\"143M\",\"0.632\",\"0.498\",\"0.525\",\"0.504\",\"0.528\"\n\"Intersect IN1k image clustering and CLIP L14 score top 30%\",\"140M\",\"0.631\",\"0.508\",\"0.546\",\"0.498\",\"0.537\"\n\"Intersect IN21k image clustering and CLIP B32 score top 30%\",\"211M\",\"0.605\",\"0.481\",\"0.531\",\"0.494\",\"0.519\"\n\"Intersect IN21k image clustering and CLIP L14 score top 30%\",\"208M\",\"0.506\",\"0.416\",\"0.466\",\"0.424\",\"0.471\""}, {"type": "table", "rows": [["Filtering", "Training dataset size", "ImageNet", "ImageNet dist. shifts", "VTAB", "Retrieval", "Average over 38 datasets"], ["No filtering", "12.8B", "0.723", "0.612", "0.611", "0.569", "0.621"], ["CLIP B32 score top 30%", "3.84B", "0.764", "0.640", "0.628", "0.599", "0.638"], ["CLIP B32 threshold at 0.28 + English filter", "1.3B", "0.755", "0.637", "0.624", "0.620", "0.636"], ["CLIP L14 score top 20%", "2.56B", "0.761", "0.649", "0.630", "0.575", "0.636"], ["CLIP L14 score top 25%", "3.2B", "0.768", "0.656", "0.621", "0.585", "0.637"], ["CLIP L14 score top 30%", "3.84B", "0.764", "0.655", "0.643", "0.588", "0.650"], ["Intersect IN1k image clustering and CLIP L14 score top 30%", "1.38B", "0.792", "0.679", "0.652", "0.608", "0.663"]], "md": "|Filtering|Training dataset size|ImageNet|ImageNet dist. shifts|VTAB|Retrieval|Average over 38 datasets|\n|---|---|---|---|---|---|---|\n|No filtering|12.8B|0.723|0.612|0.611|0.569|0.621|\n|CLIP B32 score top 30%|3.84B|0.764|0.640|0.628|0.599|0.638|\n|CLIP B32 threshold at 0.28 + English filter|1.3B|0.755|0.637|0.624|0.620|0.636|\n|CLIP L14 score top 20%|2.56B|0.761|0.649|0.630|0.575|0.636|\n|CLIP L14 score top 25%|3.2B|0.768|0.656|0.621|0.585|0.637|\n|CLIP L14 score top 30%|3.84B|0.764|0.655|0.643|0.588|0.650|\n|Intersect IN1k image clustering and CLIP L14 score top 30%|1.38B|0.792|0.679|0.652|0.608|0.663|", "isPerfectTable": true, "csv": "\"Filtering\",\"Training dataset size\",\"ImageNet\",\"ImageNet dist. shifts\",\"VTAB\",\"Retrieval\",\"Average over 38 datasets\"\n\"No filtering\",\"12.8B\",\"0.723\",\"0.612\",\"0.611\",\"0.569\",\"0.621\"\n\"CLIP B32 score top 30%\",\"3.84B\",\"0.764\",\"0.640\",\"0.628\",\"0.599\",\"0.638\"\n\"CLIP B32 threshold at 0.28 + English filter\",\"1.3B\",\"0.755\",\"0.637\",\"0.624\",\"0.620\",\"0.636\"\n\"CLIP L14 score top 20%\",\"2.56B\",\"0.761\",\"0.649\",\"0.630\",\"0.575\",\"0.636\"\n\"CLIP L14 score top 25%\",\"3.2B\",\"0.768\",\"0.656\",\"0.621\",\"0.585\",\"0.637\"\n\"CLIP L14 score top 30%\",\"3.84B\",\"0.764\",\"0.655\",\"0.643\",\"0.588\",\"0.650\"\n\"Intersect IN1k image clustering and CLIP L14 score top 30%\",\"1.38B\",\"0.792\",\"0.679\",\"0.652\",\"0.608\",\"0.663\""}]}, {"page": 60, "text": "S  Datasheet\n        S.1    Motivation\n    Q1 For what purpose was the dataset created? Was there a specific task in mind? Was there a specific\n        gap that needed to be filled? Please provide a description.\n            \u2022 The purpose of DATACOMP and the associated COMMONPOOL dataset is to enable study of\n              what makes a strong image-text dataset, which supports a broad range of applications. Prior work\n              mainly focuses on data curation in the context of supervised datasets and smaller scales. For a\n              fuller treatment see Section 2. In our initial release of DATACOMP we focus on 38 downstream\n              image classification and image retrieval tasks. For details see Section 3.5 and Appendix O.\n    Q2 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\n        company, institution, organization)?\n            \u2022 DATACOMP and COMMONPOOL were created by a group of researchers with the following\n              affiliations, listed in alphabetical order: Allen Institute for Artificial Intelligence (AI2), Apple,\n              Columbia University, Google Research, Graz University of Technology, Hebrew University,\n              Juelich Supercomputing Center, LAION, Research Center Juelich, StabilityAI, Tel Aviv\n              University, University of Illinois Urbana-Champaign, University of Texas at Austin, University\n              of Washington.\n    Q3 Who funded the creation of the dataset? If there is an associated grant, please provide the name of\n        the grantor and the grant name and number.\n            \u2022 Compute for this research was generously provided by StabilityAI. For more specific\n              acknowledgments, see the acknowledgment section at the end of the main paper.\n    Q4 Any other comments?\n            \u2022 We hope that COMMONPOOL will help to facilitate data-centric questions in ML and AI towards\n              the next generation of web-scale datasets, that 1) yield higher accuracy models and 2) models\n              that are safer and more equitable.\n        S.2    Composition\n    Q5 What do the instances that comprise the dataset represent (e.g., documents, photos, people,\n        countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and\n        interactions between them; nodes and edges)? Please provide a description.\n            \u2022 Each instance is a pair of url and corresponding image alt-text. The url points to an image that a\n              user can then try to download. Each sample is also tagged with metadata, discussed in Q25.\n    Q6 How many instances are there in total (of each type, if appropriate)?\n            \u2022 There are 12.8B instances in COMMONPOOL. For breakdowns and statistics see Appendix I.\n    Q7 Does the dataset contain all possible instances or is it a sample (not necessarily random) of\n        instances from a larger set? If the dataset is a sample, then what is the larger set? Is the\n        sample representative of the larger set (e.g., geographic coverage)? If so, please describe how\n        this representativeness was validated/verified. If it is not representative of the larger set, please\n        describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or\n        unavailable).\n            \u2022 We find \u223c88B possible samples in common crawl. These samples are globally shuffled to ensure\n              i.i.d. sampling for all sampling based parts of the downstream pipeline. Of these samples we\n              attempt to download \u223c40B samples. Due to various download issues, such as dead links and\n              throttling, we are able to successfully download \u223c16.8B samples. After NSFW filtering and\n              evaluation set deduplication we end up with \u223c13.1B viable samples, from which we randomly\n              sample 12.8B for COMMONPOOL. For a complete treatment and visualization of our data\n              processing funnel, see Appendix H. For each sample we also release metadata shown in Table 8.\n    Q8 What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features?\n        In either case, please provide a description.\n            \u2022 Each sample contains an image url for download and an associated alt-text caption. Additionally,\n              each sample contains metadata fields shown in Table 8 (e.g., image aspect ratio and CLIP\n              features).\n    Q9 Is there a label or target associated with each instance? If so, please provide a description.\n                                                       60", "md": "# Datasheet\n\n## S.1 Motivation\n\nQ1 For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\n- The purpose of DATACOMP and the associated COMMONPOOL dataset is to enable study of what makes a strong image-text dataset, which supports a broad range of applications. Prior work mainly focuses on data curation in the context of supervised datasets and smaller scales. For a fuller treatment see Section 2. In our initial release of DATACOMP we focus on 38 downstream image classification and image retrieval tasks. For details see Section 3.5 and Appendix O.\n\nQ2 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\n\n- DATACOMP and COMMONPOOL were created by a group of researchers with the following affiliations, listed in alphabetical order: Allen Institute for Artificial Intelligence (AI2), Apple, Columbia University, Google Research, Graz University of Technology, Hebrew University, Juelich Supercomputing Center, LAION, Research Center Juelich, StabilityAI, Tel Aviv University, University of Illinois Urbana-Champaign, University of Texas at Austin, University of Washington.\n\nQ3 Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\n\n- Compute for this research was generously provided by StabilityAI. For more specific acknowledgments, see the acknowledgment section at the end of the main paper.\n\nQ4 Any other comments?\n\n- We hope that COMMONPOOL will help to facilitate data-centric questions in ML and AI towards the next generation of web-scale datasets, that 1) yield higher accuracy models and 2) models that are safer and more equitable.\n\n## S.2 Composition\n\nQ5 What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\n- Each instance is a pair of url and corresponding image alt-text. The url points to an image that a user can then try to download. Each sample is also tagged with metadata, discussed in Q25.\n\nQ6 How many instances are there in total (of each type, if appropriate)?\n\n- There are 12.8B instances in COMMONPOOL. For breakdowns and statistics see Appendix I.\n\nQ7 Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\n\n- We find $\\sim$88B possible samples in common crawl. These samples are globally shuffled to ensure i.i.d. sampling for all sampling based parts of the downstream pipeline. Of these samples we attempt to download $\\sim$40B samples. Due to various download issues, such as dead links and throttling, we are able to successfully download $\\sim$16.8B samples. After NSFW filtering and evaluation set deduplication we end up with $\\sim$13.1B viable samples, from which we randomly sample 12.8B for COMMONPOOL. For a complete treatment and visualization of our data processing funnel, see Appendix H. For each sample we also release metadata shown in Table 8.\n\nQ8 What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.\n\n- Each sample contains an image url for download and an associated alt-text caption. Additionally, each sample contains metadata fields shown in Table 8 (e.g., image aspect ratio and CLIP features).\n\nQ9 Is there a label or target associated with each instance? If so, please provide a description.\n\n- 60", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Datasheet", "md": "# Datasheet"}, {"type": "heading", "lvl": 2, "value": "S.1 Motivation", "md": "## S.1 Motivation"}, {"type": "text", "value": "Q1 For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\n- The purpose of DATACOMP and the associated COMMONPOOL dataset is to enable study of what makes a strong image-text dataset, which supports a broad range of applications. Prior work mainly focuses on data curation in the context of supervised datasets and smaller scales. For a fuller treatment see Section 2. In our initial release of DATACOMP we focus on 38 downstream image classification and image retrieval tasks. For details see Section 3.5 and Appendix O.\n\nQ2 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\n\n- DATACOMP and COMMONPOOL were created by a group of researchers with the following affiliations, listed in alphabetical order: Allen Institute for Artificial Intelligence (AI2), Apple, Columbia University, Google Research, Graz University of Technology, Hebrew University, Juelich Supercomputing Center, LAION, Research Center Juelich, StabilityAI, Tel Aviv University, University of Illinois Urbana-Champaign, University of Texas at Austin, University of Washington.\n\nQ3 Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\n\n- Compute for this research was generously provided by StabilityAI. For more specific acknowledgments, see the acknowledgment section at the end of the main paper.\n\nQ4 Any other comments?\n\n- We hope that COMMONPOOL will help to facilitate data-centric questions in ML and AI towards the next generation of web-scale datasets, that 1) yield higher accuracy models and 2) models that are safer and more equitable.", "md": "Q1 For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\n- The purpose of DATACOMP and the associated COMMONPOOL dataset is to enable study of what makes a strong image-text dataset, which supports a broad range of applications. Prior work mainly focuses on data curation in the context of supervised datasets and smaller scales. For a fuller treatment see Section 2. In our initial release of DATACOMP we focus on 38 downstream image classification and image retrieval tasks. For details see Section 3.5 and Appendix O.\n\nQ2 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\n\n- DATACOMP and COMMONPOOL were created by a group of researchers with the following affiliations, listed in alphabetical order: Allen Institute for Artificial Intelligence (AI2), Apple, Columbia University, Google Research, Graz University of Technology, Hebrew University, Juelich Supercomputing Center, LAION, Research Center Juelich, StabilityAI, Tel Aviv University, University of Illinois Urbana-Champaign, University of Texas at Austin, University of Washington.\n\nQ3 Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\n\n- Compute for this research was generously provided by StabilityAI. For more specific acknowledgments, see the acknowledgment section at the end of the main paper.\n\nQ4 Any other comments?\n\n- We hope that COMMONPOOL will help to facilitate data-centric questions in ML and AI towards the next generation of web-scale datasets, that 1) yield higher accuracy models and 2) models that are safer and more equitable."}, {"type": "heading", "lvl": 2, "value": "S.2 Composition", "md": "## S.2 Composition"}, {"type": "text", "value": "Q5 What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\n- Each instance is a pair of url and corresponding image alt-text. The url points to an image that a user can then try to download. Each sample is also tagged with metadata, discussed in Q25.\n\nQ6 How many instances are there in total (of each type, if appropriate)?\n\n- There are 12.8B instances in COMMONPOOL. For breakdowns and statistics see Appendix I.\n\nQ7 Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\n\n- We find $\\sim$88B possible samples in common crawl. These samples are globally shuffled to ensure i.i.d. sampling for all sampling based parts of the downstream pipeline. Of these samples we attempt to download $\\sim$40B samples. Due to various download issues, such as dead links and throttling, we are able to successfully download $\\sim$16.8B samples. After NSFW filtering and evaluation set deduplication we end up with $\\sim$13.1B viable samples, from which we randomly sample 12.8B for COMMONPOOL. For a complete treatment and visualization of our data processing funnel, see Appendix H. For each sample we also release metadata shown in Table 8.\n\nQ8 What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.\n\n- Each sample contains an image url for download and an associated alt-text caption. Additionally, each sample contains metadata fields shown in Table 8 (e.g., image aspect ratio and CLIP features).\n\nQ9 Is there a label or target associated with each instance? If so, please provide a description.\n\n- 60", "md": "Q5 What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\n- Each instance is a pair of url and corresponding image alt-text. The url points to an image that a user can then try to download. Each sample is also tagged with metadata, discussed in Q25.\n\nQ6 How many instances are there in total (of each type, if appropriate)?\n\n- There are 12.8B instances in COMMONPOOL. For breakdowns and statistics see Appendix I.\n\nQ7 Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\n\n- We find $\\sim$88B possible samples in common crawl. These samples are globally shuffled to ensure i.i.d. sampling for all sampling based parts of the downstream pipeline. Of these samples we attempt to download $\\sim$40B samples. Due to various download issues, such as dead links and throttling, we are able to successfully download $\\sim$16.8B samples. After NSFW filtering and evaluation set deduplication we end up with $\\sim$13.1B viable samples, from which we randomly sample 12.8B for COMMONPOOL. For a complete treatment and visualization of our data processing funnel, see Appendix H. For each sample we also release metadata shown in Table 8.\n\nQ8 What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.\n\n- Each sample contains an image url for download and an associated alt-text caption. Additionally, each sample contains metadata fields shown in Table 8 (e.g., image aspect ratio and CLIP features).\n\nQ9 Is there a label or target associated with each instance? If so, please provide a description.\n\n- 60"}]}, {"page": 61, "text": "         \u2022 We do not provide any category labels; however, the text associated with each image can be\n            considered a soft, noisy label for each sample. Such labels are common in modern image-text\n            training paradigms (e.g., image-text representation alignment, image captioning objectives,\n            text-conditional image generation objectives, etc.).\nQ10 Is any information missing from individual instances? If so, please provide a description, explaining\n      why this information is missing (e.g., because it was unavailable). This does not include intentionally\n      removed information, but might include, e.g., redacted text.\n         \u2022 No, each sample is an image-text pair.\nQ11 Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings, social\n      network links)? If so, please describe how these relationships are made explicit.\n         \u2022 No, the dataset is released as it is with no explicit attempt to establish relationships between\n            instances.\nQ12 Are there recommended data splits (e.g., training, development/validation, testing)? If so, please\n      provide a description of these splits, explaining the rationale behind them.\n         \u2022 No. The test tasks are existing image classification tasks. We run a deduplication model to try to\n            prevent test set contamination in COMMONPOOL.\nQ13 Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a\n      description.\n         \u2022 COMMONPOOL is sourced from Common Crawl, which can be thought of as a snapshot of the\n            internet. Hence, there can be considerable noise (e.g., alt-text being unrelated to its associated\n            image), duplicate data, etc.\nQ14 Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\n      websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees\n      that they will exist, and remain constant, over time; b) are there official archival versions of the\n      complete dataset (i.e., including the external resources as they existed at the time the dataset was\n      created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources\n      that might apply to a future user? Please provide descriptions of all external resources and any\n      restrictions associated with them, as well as links or other access points, as appropriate.\n         \u2022 The data is not self-contained and rather links other external resources on the internet. Links\n            point to resources distributed across the internet. There is no guarantee that the resources will\n            exist in perpetuity or that that the resources will not change. To mitigate against data poisoning\n            in future COMMONPOOL downloads, we release SHA256 hashes of images. Due to the size of\n            the dataset, it is not possible to provide it in an archival form.\nQ15 Does the dataset contain data that might be considered confidential (e.g., data that is protected by\n      legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals\u2019\n      non-public communications)? If so, please provide a description.\n         \u2022 The dataset is comprised of data that was readily available on the public internet at the time of\n            our download. However, it is possible that the dataset contains confidential information (e.g.,\n            private data that is hosted publicly for nefarious reasons or out of ignorance of said data being\n            confidential).\nQ16 Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\n      or might otherwise cause anxiety? If so, please describe why.\n         \u2022 Considering the plurality of people and their backgrounds across the world, it is highly likely that\n            there is content in COMMONPOOL that may upset people. Common Crawl scrapes the internet,\n            which has pornographic, hateful, racist, sexist, and otherwise abhorrent and toxic material.\n            While we attempt to do thorough NSFW filtering, these methods are not 100% accurate. At\n            the 12.8B scale at which we operate, it is highly likely that there is still toxic content in the\n            dataset. We consider the dataset as a research artifact and hope future work will look critically at\n            COMMONPOOL in the hopes of developing even better safety filters.\nQ17 Does the dataset relate to people? If not, you may skip the remaining questions in this section.\n         \u2022 People may appear in the dataset; however, in an effort to preserve privacy, our downloading\n            tooling automatically blurs all detected faces in COMMONPOOL images.\nQ18 Does the dataset identify any subpopulations (e.g., by age, gender)?\n         \u2022 While COMMONPOOL does not explicitly identify subpopulations in its metadata, it is plausible\n            to extract such information for some images using the corresponding textual caption.\n                                                     61", "md": "# Dataset Information\n\n## Dataset Information\n\nQ10 Is any information missing from individual instances?\n\nNo, each sample is an image-text pair.\n\nQ11 Are relationships between individual instances made explicit?\n\nNo, the dataset is released as it is with no explicit attempt to establish relationships between instances.\n\nQ12 Are there recommended data splits?\n\nNo. The test tasks are existing image classification tasks. We run a deduplication model to try to prevent test set contamination in COMMONPOOL.\n\nQ13 Are there any errors, sources of noise, or redundancies in the dataset?\n\nCOMMONPOOL is sourced from Common Crawl, which can be thought of as a snapshot of the internet. Hence, there can be considerable noise (e.g., alt-text being unrelated to its associated image), duplicate data, etc.\n\nQ14 Is the dataset self-contained, or does it link to or otherwise rely on external resources?\n\nThe data is not self-contained and rather links other external resources on the internet. Links point to resources distributed across the internet. There is no guarantee that the resources will exist in perpetuity or that the resources will not change. To mitigate against data poisoning in future COMMONPOOL downloads, we release SHA256 hashes of images. Due to the size of the dataset, it is not possible to provide it in an archival form.\n\nQ15 Does the dataset contain data that might be considered confidential?\n\nThe dataset is comprised of data that was readily available on the public internet at the time of our download. However, it is possible that the dataset contains confidential information (e.g., private data that is hosted publicly for nefarious reasons or out of ignorance of said data being confidential).\n\nQ16 Does the dataset contain data that might be offensive?\n\nConsidering the plurality of people and their backgrounds across the world, it is highly likely that there is content in COMMONPOOL that may upset people. Common Crawl scrapes the internet, which has pornographic, hateful, racist, sexist, and otherwise abhorrent and toxic material. While we attempt to do thorough NSFW filtering, these methods are not 100% accurate. At the 12.8B scale at which we operate, it is highly likely that there is still toxic content in the dataset. We consider the dataset as a research artifact and hope future work will look critically at COMMONPOOL in the hopes of developing even better safety filters.\n\nQ17 Does the dataset relate to people?\n\nPeople may appear in the dataset; however, in an effort to preserve privacy, our downloading tooling automatically blurs all detected faces in COMMONPOOL images.\n\nQ18 Does the dataset identify any subpopulations?\n\nWhile COMMONPOOL does not explicitly identify subpopulations in its metadata, it is plausible to extract such information for some images using the corresponding textual caption.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Dataset Information", "md": "# Dataset Information"}, {"type": "heading", "lvl": 2, "value": "Dataset Information", "md": "## Dataset Information"}, {"type": "text", "value": "Q10 Is any information missing from individual instances?\n\nNo, each sample is an image-text pair.\n\nQ11 Are relationships between individual instances made explicit?\n\nNo, the dataset is released as it is with no explicit attempt to establish relationships between instances.\n\nQ12 Are there recommended data splits?\n\nNo. The test tasks are existing image classification tasks. We run a deduplication model to try to prevent test set contamination in COMMONPOOL.\n\nQ13 Are there any errors, sources of noise, or redundancies in the dataset?\n\nCOMMONPOOL is sourced from Common Crawl, which can be thought of as a snapshot of the internet. Hence, there can be considerable noise (e.g., alt-text being unrelated to its associated image), duplicate data, etc.\n\nQ14 Is the dataset self-contained, or does it link to or otherwise rely on external resources?\n\nThe data is not self-contained and rather links other external resources on the internet. Links point to resources distributed across the internet. There is no guarantee that the resources will exist in perpetuity or that the resources will not change. To mitigate against data poisoning in future COMMONPOOL downloads, we release SHA256 hashes of images. Due to the size of the dataset, it is not possible to provide it in an archival form.\n\nQ15 Does the dataset contain data that might be considered confidential?\n\nThe dataset is comprised of data that was readily available on the public internet at the time of our download. However, it is possible that the dataset contains confidential information (e.g., private data that is hosted publicly for nefarious reasons or out of ignorance of said data being confidential).\n\nQ16 Does the dataset contain data that might be offensive?\n\nConsidering the plurality of people and their backgrounds across the world, it is highly likely that there is content in COMMONPOOL that may upset people. Common Crawl scrapes the internet, which has pornographic, hateful, racist, sexist, and otherwise abhorrent and toxic material. While we attempt to do thorough NSFW filtering, these methods are not 100% accurate. At the 12.8B scale at which we operate, it is highly likely that there is still toxic content in the dataset. We consider the dataset as a research artifact and hope future work will look critically at COMMONPOOL in the hopes of developing even better safety filters.\n\nQ17 Does the dataset relate to people?\n\nPeople may appear in the dataset; however, in an effort to preserve privacy, our downloading tooling automatically blurs all detected faces in COMMONPOOL images.\n\nQ18 Does the dataset identify any subpopulations?\n\nWhile COMMONPOOL does not explicitly identify subpopulations in its metadata, it is plausible to extract such information for some images using the corresponding textual caption.", "md": "Q10 Is any information missing from individual instances?\n\nNo, each sample is an image-text pair.\n\nQ11 Are relationships between individual instances made explicit?\n\nNo, the dataset is released as it is with no explicit attempt to establish relationships between instances.\n\nQ12 Are there recommended data splits?\n\nNo. The test tasks are existing image classification tasks. We run a deduplication model to try to prevent test set contamination in COMMONPOOL.\n\nQ13 Are there any errors, sources of noise, or redundancies in the dataset?\n\nCOMMONPOOL is sourced from Common Crawl, which can be thought of as a snapshot of the internet. Hence, there can be considerable noise (e.g., alt-text being unrelated to its associated image), duplicate data, etc.\n\nQ14 Is the dataset self-contained, or does it link to or otherwise rely on external resources?\n\nThe data is not self-contained and rather links other external resources on the internet. Links point to resources distributed across the internet. There is no guarantee that the resources will exist in perpetuity or that the resources will not change. To mitigate against data poisoning in future COMMONPOOL downloads, we release SHA256 hashes of images. Due to the size of the dataset, it is not possible to provide it in an archival form.\n\nQ15 Does the dataset contain data that might be considered confidential?\n\nThe dataset is comprised of data that was readily available on the public internet at the time of our download. However, it is possible that the dataset contains confidential information (e.g., private data that is hosted publicly for nefarious reasons or out of ignorance of said data being confidential).\n\nQ16 Does the dataset contain data that might be offensive?\n\nConsidering the plurality of people and their backgrounds across the world, it is highly likely that there is content in COMMONPOOL that may upset people. Common Crawl scrapes the internet, which has pornographic, hateful, racist, sexist, and otherwise abhorrent and toxic material. While we attempt to do thorough NSFW filtering, these methods are not 100% accurate. At the 12.8B scale at which we operate, it is highly likely that there is still toxic content in the dataset. We consider the dataset as a research artifact and hope future work will look critically at COMMONPOOL in the hopes of developing even better safety filters.\n\nQ17 Does the dataset relate to people?\n\nPeople may appear in the dataset; however, in an effort to preserve privacy, our downloading tooling automatically blurs all detected faces in COMMONPOOL images.\n\nQ18 Does the dataset identify any subpopulations?\n\nWhile COMMONPOOL does not explicitly identify subpopulations in its metadata, it is plausible to extract such information for some images using the corresponding textual caption."}]}, {"page": 62, "text": "Q19 Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly\n      (i.e., in combination with other data) from the dataset? If so, please describe how.\n          \u2022 We conjecture that even with our face blurring procedure, it may still be possible to identify\n            individuals. Face blurring relies of a face detection model, which could fail (See Appendix\n            G for experimental validation of the employed detector). It is also possible to identify certain\n            celebrities or athletes, who may wear distinctive clothing that is associated with them. It is also\n            likely that names are contained in textual captions, though it is not guaranteed that these names\n            correspond to people in images due to the inherent noisiness of internet captions.\nQ20 Does the dataset contain data that might be considered sensitive in any way (e.g., data that\n      reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union\n      memberships, or locations; financial or health data; biometric or genetic data; forms of\n      government identification, such as social security numbers; criminal history)? If so, please\n      provide a description.\n          \u2022 Yes. COMMONPOOL is created using images and corresponding alt-text that are available on\n            the public internet. Given the 12.8B scale of COMMONPOOL, it is highly likely that there is\n            sensitive data in the dataset. To mitigate against making sensitive content more accessible, we\n            1) run NSFW image filtering and 2) NSFW text filtering when generating COMMONPOOL,\n            discarding all samples that are flagged. Additionally we 3) provide automatic face blurring in\n            our COMMONPOOL download scripts to blur all detected faces.\nQ21 Any other comments?\n          \u2022 COMMONPOOL is a research artifact, and we hope it will be useful for those studying how to\n            make internet-scale datasets safer.\n      S.3    Collection Process\nQ22 How was the data associated with each instance acquired? Was the data directly observable (e.g.,\n      raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived\n      from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was\n      reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If\n      so, please describe how.\n          \u2022 Data is directly downloaded from the public internet.\nQ23 What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or\n      sensor, manual human curation, software program, software API)? How were these mechanisms\n      or procedures validated?\n          \u2022 We iterate on the LAION-5B data collection process, making an effort to emphasize safety. We\n            ran python based processing scripts to parse Common Crawl dumps, download images, filter\n            our NSFW content, deduplicate samples against downstream tests sets, blur faces, and compute\n            CLIP features. We ran processes on 100s of AWS CPU nodes for Common Crawl parsing and\n            data download. Other steps were run on one of StabilityAI\u2019s GPU cluster. For software links see\n            Q37. For software validation related to NSFW content filtering and face blurring see Appendices\n            E and G respectively. In brief, for NSFW image filtering, we validate against commercial APIs\n            and on the NSFW test set introduced in LAION-5B. For face detection (used for face blurring),\n            we evaluate against commercial APIs. We find strong performance for both modules.\nQ24 If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,\n      probabilistic with specific sampling probabilities)?\n          \u2022 See Q7.\nQ25 Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and\n      how were they compensated (e.g., how much were crowdworkers paid)?\n          \u2022 The researching authors were involved in the data collection as an open source effort. No\n            researchers were compensated specifically for their involvement in this project.\nQ26 Over what timeframe was the data collected? Does this timeframe match the creation timeframe\n      of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please\n      describe the timeframe in which the data associated with the instances was created.\n          \u2022 Data was downloaded between December 2022 and March 2023. The urls are collected from\n            Common Crawl dumps between 2014 and 2022. Common Crawl dumps may include urls from\n            the early days of the internet. Hence, the download/collection timeframe does not match the\n            creation timeframe. Additionally, future users of COMMONPOOL and its subsets will have to\n            download data themselves using our tooling.\n                                                    62", "md": "# Data Collection Information\n\n## Q19 Is it possible to identify individuals from the dataset?\n\nWe conjecture that even with our face blurring procedure, it may still be possible to identify individuals. Face blurring relies on a face detection model, which could fail. It is also possible to identify certain celebrities or athletes based on distinctive clothing. Names may be contained in textual captions, but it is not guaranteed that these names correspond to people in images due to the noisiness of internet captions.\n\n## Q20 Does the dataset contain sensitive data?\n\nYes, the dataset COMMONPOOL is created using images and alt-text from the public internet. Given the large scale of COMMONPOOL, it is highly likely that sensitive data is present. To mitigate this, NSFW image and text filtering is applied during dataset generation, and automatic face blurring is provided in the download scripts.\n\n## Q21 Any other comments?\n\nCOMMONPOOL is a research artifact intended to assist in studying how to make internet-scale datasets safer.\n\n## Q22 How was the data associated with each instance acquired?\n\nData is directly downloaded from the public internet.\n\n## Q23 What mechanisms were used to collect the data?\n\nThe data collection process involved python-based processing scripts to parse Common Crawl dumps, download images, filter NSFW content, deduplicate samples, blur faces, and compute CLIP features. Processes were run on AWS CPU nodes and StabilityAI's GPU cluster. Validation for NSFW content filtering and face blurring is detailed in Appendices E and G.\n\n## Q24 Sampling Strategy\n\nRefer to Q7 for information on the sampling strategy.\n\n## Q25 Who was involved in the data collection process?\n\nThe researching authors were involved in the data collection as part of an open-source effort. No researchers were compensated for their involvement.\n\n## Q26 Data Collection Timeframe\n\nData was downloaded between December 2022 and March 2023, while the URLs were collected from Common Crawl dumps between 2014 and 2022. The creation timeframe of the data associated with the instances may vary due to the nature of Common Crawl dumps.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Data Collection Information", "md": "# Data Collection Information"}, {"type": "heading", "lvl": 2, "value": "Q19 Is it possible to identify individuals from the dataset?", "md": "## Q19 Is it possible to identify individuals from the dataset?"}, {"type": "text", "value": "We conjecture that even with our face blurring procedure, it may still be possible to identify individuals. Face blurring relies on a face detection model, which could fail. It is also possible to identify certain celebrities or athletes based on distinctive clothing. Names may be contained in textual captions, but it is not guaranteed that these names correspond to people in images due to the noisiness of internet captions.", "md": "We conjecture that even with our face blurring procedure, it may still be possible to identify individuals. Face blurring relies on a face detection model, which could fail. It is also possible to identify certain celebrities or athletes based on distinctive clothing. Names may be contained in textual captions, but it is not guaranteed that these names correspond to people in images due to the noisiness of internet captions."}, {"type": "heading", "lvl": 2, "value": "Q20 Does the dataset contain sensitive data?", "md": "## Q20 Does the dataset contain sensitive data?"}, {"type": "text", "value": "Yes, the dataset COMMONPOOL is created using images and alt-text from the public internet. Given the large scale of COMMONPOOL, it is highly likely that sensitive data is present. To mitigate this, NSFW image and text filtering is applied during dataset generation, and automatic face blurring is provided in the download scripts.", "md": "Yes, the dataset COMMONPOOL is created using images and alt-text from the public internet. Given the large scale of COMMONPOOL, it is highly likely that sensitive data is present. To mitigate this, NSFW image and text filtering is applied during dataset generation, and automatic face blurring is provided in the download scripts."}, {"type": "heading", "lvl": 2, "value": "Q21 Any other comments?", "md": "## Q21 Any other comments?"}, {"type": "text", "value": "COMMONPOOL is a research artifact intended to assist in studying how to make internet-scale datasets safer.", "md": "COMMONPOOL is a research artifact intended to assist in studying how to make internet-scale datasets safer."}, {"type": "heading", "lvl": 2, "value": "Q22 How was the data associated with each instance acquired?", "md": "## Q22 How was the data associated with each instance acquired?"}, {"type": "text", "value": "Data is directly downloaded from the public internet.", "md": "Data is directly downloaded from the public internet."}, {"type": "heading", "lvl": 2, "value": "Q23 What mechanisms were used to collect the data?", "md": "## Q23 What mechanisms were used to collect the data?"}, {"type": "text", "value": "The data collection process involved python-based processing scripts to parse Common Crawl dumps, download images, filter NSFW content, deduplicate samples, blur faces, and compute CLIP features. Processes were run on AWS CPU nodes and StabilityAI's GPU cluster. Validation for NSFW content filtering and face blurring is detailed in Appendices E and G.", "md": "The data collection process involved python-based processing scripts to parse Common Crawl dumps, download images, filter NSFW content, deduplicate samples, blur faces, and compute CLIP features. Processes were run on AWS CPU nodes and StabilityAI's GPU cluster. Validation for NSFW content filtering and face blurring is detailed in Appendices E and G."}, {"type": "heading", "lvl": 2, "value": "Q24 Sampling Strategy", "md": "## Q24 Sampling Strategy"}, {"type": "text", "value": "Refer to Q7 for information on the sampling strategy.", "md": "Refer to Q7 for information on the sampling strategy."}, {"type": "heading", "lvl": 2, "value": "Q25 Who was involved in the data collection process?", "md": "## Q25 Who was involved in the data collection process?"}, {"type": "text", "value": "The researching authors were involved in the data collection as part of an open-source effort. No researchers were compensated for their involvement.", "md": "The researching authors were involved in the data collection as part of an open-source effort. No researchers were compensated for their involvement."}, {"type": "heading", "lvl": 2, "value": "Q26 Data Collection Timeframe", "md": "## Q26 Data Collection Timeframe"}, {"type": "text", "value": "Data was downloaded between December 2022 and March 2023, while the URLs were collected from Common Crawl dumps between 2014 and 2022. The creation timeframe of the data associated with the instances may vary due to the nature of Common Crawl dumps.", "md": "Data was downloaded between December 2022 and March 2023, while the URLs were collected from Common Crawl dumps between 2014 and 2022. The creation timeframe of the data associated with the instances may vary due to the nature of Common Crawl dumps."}]}, {"page": 63, "text": "Q27 Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please\n      provide a description of these review processes, including the outcomes, as well as a link or other\n      access point to any supporting documentation.\n         \u2022 Our dataset collection process iterates on the LAION-5B process, which found IRB review was\n            not necessary as they \u201cdo not intervene with the people depicted in the data as well as the data\n            being public.\u201d [129]. Additionally, the NeurIPS ethics review found no serious ethical issues\n            with LAION-5B. We take even more stringent safety measures than the original LAION-5B\n            dataset, in that we filter out data that is flagged as NSFW by our detection pipeline and blur\n            detected faces in COMMONPOOL, automatically in our released download tooling. All this being\n            said, a formal ethics review has not been conducted to date.\nQ28 Does the dataset relate to people? If not, you may skip the remaining questions in this section.\n         \u2022 Yes.    People may appear in the dataset.        Detected faces are blurred when downloading\n            COMMONPOOL with our tooling.\nQ29 Did you collect the data from the individuals in question directly, or obtain it via third parties or\n      other sources (e.g., websites)?\n         \u2022 We collect data from websites across the internet.\nQ30 Were the individuals in question notified about the data collection? If so, please describe (or show\n      with screenshots or other information) how notice was provided, and provide a link or other access\n      point to, or otherwise reproduce, the exact language of the notification itself.\n         \u2022 Individuals were not notified about the data collection.\nQ31 Did the individuals in question consent to the collection and use of their data? If so, please\n      describe (or show with screenshots or other information) how consent was requested and provided,\n      and provide a link or other access point to, or otherwise reproduce, the exact language to which the\n      individuals consented.\n         \u2022 Following our usage of Common Crawl and https://github.com/rom1504/img2dataset\n            for download images, we respect robots.txt files, which specify parts of websites that a\n            crawler may access. It is, however, possible that images of people, medical images, etc. were\n            uploaded to the internet without a person\u2019s consent. To mitigate against such safety concerns we\n            make an effort to do rigorous NSFW fi     ltering and blur all detected faces automatically in our\n            download tooling.\nQ32 If consent was obtained, were the consenting individuals provided with a mechanism to revoke\n      their consent in the future or for certain uses? If so, please provide a description, as well as a link\n      or other access point to the mechanism (if appropriate).\n         \u2022 In conjunction with LAION, we use https://laion.ai/dataset-requests/ to monitor\n            user takedown requests. We will also make an effort to provide a user with the url at which their\n            sensitive content is hosted\u2014if they do not have this information already\u2014, so they can take\n            further action as they see fit (e.g., contacting the host to request that the content is taken down\n            from the internet).\nQ33 Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data\n      protection impact analysis) been conducted? If so, please provide a description of this analysis,\n      including the outcomes, as well as a link or other access point to any supporting documentation.\n         \u2022 We conduct a fairness evaluation on models trained on COMMONPOOL and its derivative. See\n            Appendix Q for details. Birhane et al. [15] conduct an extensive study in the context of LAION-\n            400M, which is an image-text dataset also sourced from Common Crawl, finding a plethora of\n            dangerous and unsafe content. Our dataset differs from LAION-400M in that we conduct NSFW\n            preprocessing and face blurring for detected faces. COMMONPOOL only contains samples that\n            pass our NSFW safety checks and our download tooling automatically blurs detected faces.\n            However, since COMMONPOOL is created from the internet, it is still likely that it contains some\n            harmful data.\nQ34 Any other comments?\n         \u2022 We hope that future work will use COMMONPOOL to study how to construct safer, web-scale\n            datasets.\n      S.4    Preprocessing, Cleaning, and/or Labeling\nQ35 Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing,\n      tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing\n      of missing values)? If so, please provide a description. If not, you may skip the remainder of the\n      questions in this section.\n                                                    63", "md": "## Q27 Were any ethical review processes conducted (e.g., by an institutional review board)?\n\nIf so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\n\n- Our dataset collection process iterates on the LAION-5B process, which found IRB review was not necessary as they \u201cdo not intervene with the people depicted in the data as well as the data being public.\u201d [129]. Additionally, the NeurIPS ethics review found no serious ethical issues with LAION-5B. We take even more stringent safety measures than the original LAION-5B dataset, in that we filter out data that is flagged as NSFW by our detection pipeline and blur detected faces in COMMONPOOL, automatically in our released download tooling. All this being said, a formal ethics review has not been conducted to date.\n\n## Q28 Does the dataset relate to people?\n\nIf not, you may skip the remaining questions in this section.\n\n- Yes. People may appear in the dataset. Detected faces are blurred when downloading COMMONPOOL with our tooling.\n\n## Q29 Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\n\n- We collect data from websites across the internet.\n\n## Q30 Were the individuals in question notified about the data collection?\n\nIf so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\n\n- Individuals were not notified about the data collection.\n\n## Q31 Did the individuals in question consent to the collection and use of their data?\n\nIf so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\n\n- Following our usage of Common Crawl and https://github.com/rom1504/img2dataset for download images, we respect robots.txt files, which specify parts of websites that a crawler may access. It is, however, possible that images of people, medical images, etc. were uploaded to the internet without a person\u2019s consent. To mitigate against such safety concerns we make an effort to do rigorous NSFW filtering and blur all detected faces automatically in our download tooling.\n\n## Q32 If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?\n\nIf so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\n\n- In conjunction with LAION, we use https://laion.ai/dataset-requests/ to monitor user takedown requests. We will also make an effort to provide a user with the url at which their sensitive content is hosted\u2014if they do not have this information already\u2014, so they can take further action as they see fit (e.g., contacting the host to request that the content is taken down from the internet).\n\n## Q33 Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?\n\nIf so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.\n\n- We conduct a fairness evaluation on models trained on COMMONPOOL and its derivative. See Appendix Q for details. Birhane et al. [15] conduct an extensive study in the context of LAION-400M, which is an image-text dataset also sourced from Common Crawl, finding a plethora of dangerous and unsafe content. Our dataset differs from LAION-400M in that we conduct NSFW preprocessing and face blurring for detected faces. COMMONPOOL only contains samples that pass our NSFW safety checks and our download tooling automatically blurs detected faces. However, since COMMONPOOL is created from the internet, it is still likely that it contains some harmful data.\n\n## Q34 Any other comments?\n\n- We hope that future work will use COMMONPOOL to study how to construct safer, web-scale datasets.\n\n## S.4 Preprocessing, Cleaning, and/or Labeling\n\n## Q35 Was any preprocessing/cleaning/labeling of the data done?\n\nIf so, please provide a description. If not, you may skip the remainder of the questions in this section.\n\n- Preprocessing, cleaning, and labeling were done on the data. For a detailed description, please refer to the dataset documentation.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Q27 Were any ethical review processes conducted (e.g., by an institutional review board)?", "md": "## Q27 Were any ethical review processes conducted (e.g., by an institutional review board)?"}, {"type": "text", "value": "If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\n\n- Our dataset collection process iterates on the LAION-5B process, which found IRB review was not necessary as they \u201cdo not intervene with the people depicted in the data as well as the data being public.\u201d [129]. Additionally, the NeurIPS ethics review found no serious ethical issues with LAION-5B. We take even more stringent safety measures than the original LAION-5B dataset, in that we filter out data that is flagged as NSFW by our detection pipeline and blur detected faces in COMMONPOOL, automatically in our released download tooling. All this being said, a formal ethics review has not been conducted to date.", "md": "If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\n\n- Our dataset collection process iterates on the LAION-5B process, which found IRB review was not necessary as they \u201cdo not intervene with the people depicted in the data as well as the data being public.\u201d [129]. Additionally, the NeurIPS ethics review found no serious ethical issues with LAION-5B. We take even more stringent safety measures than the original LAION-5B dataset, in that we filter out data that is flagged as NSFW by our detection pipeline and blur detected faces in COMMONPOOL, automatically in our released download tooling. All this being said, a formal ethics review has not been conducted to date."}, {"type": "heading", "lvl": 2, "value": "Q28 Does the dataset relate to people?", "md": "## Q28 Does the dataset relate to people?"}, {"type": "text", "value": "If not, you may skip the remaining questions in this section.\n\n- Yes. People may appear in the dataset. Detected faces are blurred when downloading COMMONPOOL with our tooling.", "md": "If not, you may skip the remaining questions in this section.\n\n- Yes. People may appear in the dataset. Detected faces are blurred when downloading COMMONPOOL with our tooling."}, {"type": "heading", "lvl": 2, "value": "Q29 Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?", "md": "## Q29 Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?"}, {"type": "text", "value": "- We collect data from websites across the internet.", "md": "- We collect data from websites across the internet."}, {"type": "heading", "lvl": 2, "value": "Q30 Were the individuals in question notified about the data collection?", "md": "## Q30 Were the individuals in question notified about the data collection?"}, {"type": "text", "value": "If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\n\n- Individuals were not notified about the data collection.", "md": "If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\n\n- Individuals were not notified about the data collection."}, {"type": "heading", "lvl": 2, "value": "Q31 Did the individuals in question consent to the collection and use of their data?", "md": "## Q31 Did the individuals in question consent to the collection and use of their data?"}, {"type": "text", "value": "If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\n\n- Following our usage of Common Crawl and https://github.com/rom1504/img2dataset for download images, we respect robots.txt files, which specify parts of websites that a crawler may access. It is, however, possible that images of people, medical images, etc. were uploaded to the internet without a person\u2019s consent. To mitigate against such safety concerns we make an effort to do rigorous NSFW filtering and blur all detected faces automatically in our download tooling.", "md": "If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\n\n- Following our usage of Common Crawl and https://github.com/rom1504/img2dataset for download images, we respect robots.txt files, which specify parts of websites that a crawler may access. It is, however, possible that images of people, medical images, etc. were uploaded to the internet without a person\u2019s consent. To mitigate against such safety concerns we make an effort to do rigorous NSFW filtering and blur all detected faces automatically in our download tooling."}, {"type": "heading", "lvl": 2, "value": "Q32 If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?", "md": "## Q32 If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?"}, {"type": "text", "value": "If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\n\n- In conjunction with LAION, we use https://laion.ai/dataset-requests/ to monitor user takedown requests. We will also make an effort to provide a user with the url at which their sensitive content is hosted\u2014if they do not have this information already\u2014, so they can take further action as they see fit (e.g., contacting the host to request that the content is taken down from the internet).", "md": "If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\n\n- In conjunction with LAION, we use https://laion.ai/dataset-requests/ to monitor user takedown requests. We will also make an effort to provide a user with the url at which their sensitive content is hosted\u2014if they do not have this information already\u2014, so they can take further action as they see fit (e.g., contacting the host to request that the content is taken down from the internet)."}, {"type": "heading", "lvl": 2, "value": "Q33 Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?", "md": "## Q33 Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?"}, {"type": "text", "value": "If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.\n\n- We conduct a fairness evaluation on models trained on COMMONPOOL and its derivative. See Appendix Q for details. Birhane et al. [15] conduct an extensive study in the context of LAION-400M, which is an image-text dataset also sourced from Common Crawl, finding a plethora of dangerous and unsafe content. Our dataset differs from LAION-400M in that we conduct NSFW preprocessing and face blurring for detected faces. COMMONPOOL only contains samples that pass our NSFW safety checks and our download tooling automatically blurs detected faces. However, since COMMONPOOL is created from the internet, it is still likely that it contains some harmful data.", "md": "If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.\n\n- We conduct a fairness evaluation on models trained on COMMONPOOL and its derivative. See Appendix Q for details. Birhane et al. [15] conduct an extensive study in the context of LAION-400M, which is an image-text dataset also sourced from Common Crawl, finding a plethora of dangerous and unsafe content. Our dataset differs from LAION-400M in that we conduct NSFW preprocessing and face blurring for detected faces. COMMONPOOL only contains samples that pass our NSFW safety checks and our download tooling automatically blurs detected faces. However, since COMMONPOOL is created from the internet, it is still likely that it contains some harmful data."}, {"type": "heading", "lvl": 2, "value": "Q34 Any other comments?", "md": "## Q34 Any other comments?"}, {"type": "text", "value": "- We hope that future work will use COMMONPOOL to study how to construct safer, web-scale datasets.", "md": "- We hope that future work will use COMMONPOOL to study how to construct safer, web-scale datasets."}, {"type": "heading", "lvl": 2, "value": "S.4 Preprocessing, Cleaning, and/or Labeling", "md": "## S.4 Preprocessing, Cleaning, and/or Labeling"}, {"type": "heading", "lvl": 2, "value": "Q35 Was any preprocessing/cleaning/labeling of the data done?", "md": "## Q35 Was any preprocessing/cleaning/labeling of the data done?"}, {"type": "text", "value": "If so, please provide a description. If not, you may skip the remainder of the questions in this section.\n\n- Preprocessing, cleaning, and labeling were done on the data. For a detailed description, please refer to the dataset documentation.", "md": "If so, please provide a description. If not, you may skip the remainder of the questions in this section.\n\n- Preprocessing, cleaning, and labeling were done on the data. For a detailed description, please refer to the dataset documentation."}]}, {"page": 64, "text": "          \u2022 Yes. See Q7. For more details see Appendix H.\nQ36 Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support\n      unanticipated future uses)? If so, please provide a link or other access point to the \u201craw\u201d data.\n          \u2022 Raw data is not available or distributed due to safety considerations. We distribute only urls that\n             are in the dataset on HuggingFace\u2014and not urls of images our preprocessing flagged as NSFW.\nQ37 Is the software used to preprocess/clean/label the instances available? If so, please provide a link\n      or other access point.\n          \u2022 We use the following, open-source software to aid in data processing:\n              \u2013 Apache Spark: https://spark.apache.org\n              \u2013 Ray: https://www.ray.io\n              \u2013 img2dataset: https://github.com/rom1504/img2dataset\n              \u2013 OpenAI CLIP: https://github.com/openai/CLIP\n              \u2013 Near            dedulicate          detector:                  https://github.com/lyakaap/\n                 ISC21-Descriptor-Track-1st\n              \u2013 Face detector: https://github.com/deepinsight/insightface\n              \u2013 Detoxify, for detecting toxic language: https://github.com/unitaryai/detoxify\n              \u2013 A modified version of the following NSFW image detector: https://github.com/\n                 LAION-AI/CLIP-based-NSFW-Detector. Specifically, we use the dataset used to train\n                 this model to train our own 4-layer MLP classifier.\nQ38 Any other comments?\n          \u2022 COMMONPOOL and DATACOMP would not be possible without tools developed by the open-\n             source community.\n      S.5     Uses\nQ39 Has the dataset been used for any tasks already? If so, please provide a description.\n          \u2022 The full dataset (and subsets) have been used to train several CLIP models at various scales and\n             compute budgets as presented in our main paper. We evaluate these models zero-shot on 38\n             downstream image classification and retrieval tasks. See Section 3.5 and Appendix O for more\n             details.\nQ40 Is there a repository that links to any or all papers or systems that use the dataset? If so, please\n      provide a link or other access point.\n          \u2022 No. However, there is a leaderboard associated with DATACOMP. Interested parties can\n             investigate the submissions and further study publications that make use of our data. See:\n             https://www.datacomp.ai/leaderboard.html.\nQ41 What (other) tasks could the dataset be used for?\n          \u2022 The dataset could also be used for training image captioning models and language-conditional\n             image generation models. Note: generative image models trained on COMMONPOOL are not\n             expected to generate recognizable human faces as our download tooling automatically blurs\n             detected faces. COMMONPOOL could be used for sociological studies, for example, examining\n             societal biases or to better understand what is on the public internet.\nQ42 Is there anything about the composition of the dataset or the way it was collected and\n      preprocessed/cleaned/labeled that might impact future uses? For example, is there anything\n      that a future user might need to know to avoid uses that could result in unfair treatment of individuals\n      or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial\n      harms, legal risks) If so, please provide a description. Is there anything a future user could do to\n      mitigate these undesirable harms?\n          \u2022 COMMONPOOL and its derivatives are not intended for production ready products, including but\n             not limited to those related to race, gender identity or expression, ethnicity, sexual orientation,\n             age, socioeconomic status, disability, religion, national origin or creed. COMMONPOOL is not\n             suitable for any software that makes decisions involving people. COMMONPOOL is collected\n             from the internet and hence reflects many of the biases, unfairness, and stereotypes currently\n             existing in our societies. COMMONPOOL is intended as a research artifact to study multimodal\n             dataset curation and the effect of data curation strategies on downstream models.\nQ43 Are there tasks for which the dataset should not be used? If so, please provide a description.\n                                                       64", "md": "# Dataset Information\n\n## Dataset Information\n\nQ36 Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data?\n\nRaw data is not available or distributed due to safety considerations. Only URLs in the dataset on HuggingFace are distributed.\n\nQ37 Is the software used to preprocess/clean/label the instances available?\n\n- Apache Spark: https://spark.apache.org\n- Ray: https://www.ray.io\n- img2dataset: https://github.com/rom1504/img2dataset\n- OpenAI CLIP: https://github.com/openai/CLIP\n- Near deduplicate detector: https://github.com/lyakaap/ISC21-Descriptor-Track-1st\n- Face detector: https://github.com/deepinsight/insightface\n- Detoxify, for detecting toxic language: https://github.com/unitaryai/detoxify\n- A modified version of NSFW image detector: https://github.com/LAION-AI/CLIP-based-NSFW-Detector\n\nQ38 Any other comments?\n\nCOMMONPOOL and DATACOMP would not be possible without tools developed by the open-source community.\n\nQ39 Has the dataset been used for any tasks already?\n\nThe dataset has been used to train several CLIP models and evaluated on 38 downstream image classification and retrieval tasks.\n\nQ40 Is there a repository that links to any or all papers or systems that use the dataset?\n\nNo repository available, but there is a leaderboard associated with DATACOMP for further investigation.\n\nQ41 What other tasks could the dataset be used for?\n\nThe dataset could be used for training image captioning models and language-conditional image generation models.\n\nQ42 Is there anything about the composition of the dataset or the way it was collected that might impact future uses?\n\nCOMMONPOOL is not intended for production-ready products involving people and reflects biases existing in societies.\n\nQ43 Are there tasks for which the dataset should not be used?\n\nThe dataset should not be used for production-ready products involving people.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Dataset Information", "md": "# Dataset Information"}, {"type": "heading", "lvl": 2, "value": "Dataset Information", "md": "## Dataset Information"}, {"type": "text", "value": "Q36 Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data?\n\nRaw data is not available or distributed due to safety considerations. Only URLs in the dataset on HuggingFace are distributed.\n\nQ37 Is the software used to preprocess/clean/label the instances available?\n\n- Apache Spark: https://spark.apache.org\n- Ray: https://www.ray.io\n- img2dataset: https://github.com/rom1504/img2dataset\n- OpenAI CLIP: https://github.com/openai/CLIP\n- Near deduplicate detector: https://github.com/lyakaap/ISC21-Descriptor-Track-1st\n- Face detector: https://github.com/deepinsight/insightface\n- Detoxify, for detecting toxic language: https://github.com/unitaryai/detoxify\n- A modified version of NSFW image detector: https://github.com/LAION-AI/CLIP-based-NSFW-Detector\n\nQ38 Any other comments?\n\nCOMMONPOOL and DATACOMP would not be possible without tools developed by the open-source community.\n\nQ39 Has the dataset been used for any tasks already?\n\nThe dataset has been used to train several CLIP models and evaluated on 38 downstream image classification and retrieval tasks.\n\nQ40 Is there a repository that links to any or all papers or systems that use the dataset?\n\nNo repository available, but there is a leaderboard associated with DATACOMP for further investigation.\n\nQ41 What other tasks could the dataset be used for?\n\nThe dataset could be used for training image captioning models and language-conditional image generation models.\n\nQ42 Is there anything about the composition of the dataset or the way it was collected that might impact future uses?\n\nCOMMONPOOL is not intended for production-ready products involving people and reflects biases existing in societies.\n\nQ43 Are there tasks for which the dataset should not be used?\n\nThe dataset should not be used for production-ready products involving people.", "md": "Q36 Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data?\n\nRaw data is not available or distributed due to safety considerations. Only URLs in the dataset on HuggingFace are distributed.\n\nQ37 Is the software used to preprocess/clean/label the instances available?\n\n- Apache Spark: https://spark.apache.org\n- Ray: https://www.ray.io\n- img2dataset: https://github.com/rom1504/img2dataset\n- OpenAI CLIP: https://github.com/openai/CLIP\n- Near deduplicate detector: https://github.com/lyakaap/ISC21-Descriptor-Track-1st\n- Face detector: https://github.com/deepinsight/insightface\n- Detoxify, for detecting toxic language: https://github.com/unitaryai/detoxify\n- A modified version of NSFW image detector: https://github.com/LAION-AI/CLIP-based-NSFW-Detector\n\nQ38 Any other comments?\n\nCOMMONPOOL and DATACOMP would not be possible without tools developed by the open-source community.\n\nQ39 Has the dataset been used for any tasks already?\n\nThe dataset has been used to train several CLIP models and evaluated on 38 downstream image classification and retrieval tasks.\n\nQ40 Is there a repository that links to any or all papers or systems that use the dataset?\n\nNo repository available, but there is a leaderboard associated with DATACOMP for further investigation.\n\nQ41 What other tasks could the dataset be used for?\n\nThe dataset could be used for training image captioning models and language-conditional image generation models.\n\nQ42 Is there anything about the composition of the dataset or the way it was collected that might impact future uses?\n\nCOMMONPOOL is not intended for production-ready products involving people and reflects biases existing in societies.\n\nQ43 Are there tasks for which the dataset should not be used?\n\nThe dataset should not be used for production-ready products involving people."}]}, {"page": 65, "text": "          \u2022 COMMONPOOL in its current form or the subsets presented in this paper should not be used in\n            software that makes decisions related to people. The known biases (Appendix Q) make deploying\n            software, especially widely decimated production-level products, built on COMMONPOOL\n            incredibly irresponsible.     COMMONPOOL is designed as a research artifact for academic\n            exploration. We also do not condone the use of COMMONPOOL in surveillance or military\n            applications.\nQ44 Any other comments?\n          \u2022 Our goal with COMMONPOOL and DATACOMP was to put a benchmark in place so the\n            community can start measuring dataset progress along many different axes (e.g., model\n            performance on diverse tasks). We believe this is crucial to develop more performant and\n            safer datasets.\n      S.6    Distribution\nQ45 Will the dataset be distributed to third parties outside of the entity (e.g., company, institution,\n      organization) on behalf of which the dataset was created? If so, please provide a description.\n          \u2022 Yes. We use HuggingFace datasets for public release.\nQ46 How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have\n      a digital object identifier (DOI)?\n          \u2022 The dataset will be distributed via HuggingFace datasets at https://huggingface.co/\n            datasets/mlfoundations/datacomp_pools/tree/main\nQ47 When will the dataset be distributed?\n          \u2022 DATACOMP will be available starting May 2023.\nQ48 Will the dataset be distributed under a copyright or other intellectual property (IP) license,\n      and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and\n      provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU,\n      as well as any fees associated with these restrictions.\n          \u2022 We distribute the url-text sample and metadata under a standard CC-BY-4.0 licence.\nQ49 Have any third parties imposed IP-based or other restrictions on the data associated with\n      the instances? If so, please describe these restrictions, and provide a link or other access point\n      to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these\n      restrictions.\n          \u2022 We do not copyright samples in the dataset.\nQ50 Do any export controls or other regulatory restrictions apply to the dataset or to individual\n      instances? If so, please describe these restrictions, and provide a link or other access point to, or\n      otherwise reproduce, any supporting documentation.\n          \u2022 The dataset is provided as an index of url-text pairs.\nQ51 Any other comments?\n          \u2022 We provide several subsets of COMMONPOOL (between 12.8M samples and the full dataset of\n            12.8B samples). Hence, it is possible to download and experiment with subset of the data.\n      S.7    Maintenance\nQ52 Who will be supporting/hosting/maintaining the dataset?\n          \u2022 HuggingFace currently hosts the url-text pairs and metadata. The DATACOMP team will be\n            responsible for maintaining the dataset.\nQ53 How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\n          \u2022 We can be contacted at contact@datacomp.ai.\nQ54 Is there an erratum? If so, please provide a link or other access point.\n          \u2022 Currently there are no errata. If issues are discovered, we will communicate with the public via\n            our website https://datacomp.ai.\nQ55 Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\n      If so, please describe how often, by whom, and how updates will be communicated to users (e.g.,\n      mailing list, GitHub)?\n                                                      65", "md": "# Data Distribution and Maintenance Information\n\n## Data Distribution Information\n\nQ45: Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nYes. We use HuggingFace datasets for public release.\n\nQ46: How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\nThe dataset will be distributed via HuggingFace datasets at HuggingFace Datasets\n\nQ47: When will the dataset be distributed?\n\nDATACOMP will be available starting May 2023.\n\nQ48: Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\n\nWe distribute the url-text sample and metadata under a standard CC-BY-4.0 licence.\n\nQ49: Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\n\nWe do not copyright samples in the dataset.\n\nQ50: Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\n\nThe dataset is provided as an index of url-text pairs.\n\nQ51: Any other comments?\n\nWe provide several subsets of COMMONPOOL (between 12.8M samples and the full dataset of 12.8B samples). Hence, it is possible to download and experiment with a subset of the data.\n\n## Data Maintenance Information\n\nQ52: Who will be supporting/hosting/maintaining the dataset?\n\nHuggingFace currently hosts the url-text pairs and metadata. The DATACOMP team will be responsible for maintaining the dataset.\n\nQ53: How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\n\nWe can be contacted at contact@datacomp.ai.\n\nQ54: Is there an erratum? If so, please provide a link or other access point.\n\nCurrently there are no errata. If issues are discovered, we will communicate with the public via our website Datacomp Website.\n\nQ55: Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?\n\nUpdates will be communicated through our website and other relevant channels.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Data Distribution and Maintenance Information", "md": "# Data Distribution and Maintenance Information"}, {"type": "heading", "lvl": 2, "value": "Data Distribution Information", "md": "## Data Distribution Information"}, {"type": "text", "value": "Q45: Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nYes. We use HuggingFace datasets for public release.\n\nQ46: How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\nThe dataset will be distributed via HuggingFace datasets at HuggingFace Datasets\n\nQ47: When will the dataset be distributed?\n\nDATACOMP will be available starting May 2023.\n\nQ48: Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\n\nWe distribute the url-text sample and metadata under a standard CC-BY-4.0 licence.\n\nQ49: Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\n\nWe do not copyright samples in the dataset.\n\nQ50: Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\n\nThe dataset is provided as an index of url-text pairs.\n\nQ51: Any other comments?\n\nWe provide several subsets of COMMONPOOL (between 12.8M samples and the full dataset of 12.8B samples). Hence, it is possible to download and experiment with a subset of the data.", "md": "Q45: Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nYes. We use HuggingFace datasets for public release.\n\nQ46: How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\nThe dataset will be distributed via HuggingFace datasets at HuggingFace Datasets\n\nQ47: When will the dataset be distributed?\n\nDATACOMP will be available starting May 2023.\n\nQ48: Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\n\nWe distribute the url-text sample and metadata under a standard CC-BY-4.0 licence.\n\nQ49: Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\n\nWe do not copyright samples in the dataset.\n\nQ50: Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\n\nThe dataset is provided as an index of url-text pairs.\n\nQ51: Any other comments?\n\nWe provide several subsets of COMMONPOOL (between 12.8M samples and the full dataset of 12.8B samples). Hence, it is possible to download and experiment with a subset of the data."}, {"type": "heading", "lvl": 2, "value": "Data Maintenance Information", "md": "## Data Maintenance Information"}, {"type": "text", "value": "Q52: Who will be supporting/hosting/maintaining the dataset?\n\nHuggingFace currently hosts the url-text pairs and metadata. The DATACOMP team will be responsible for maintaining the dataset.\n\nQ53: How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\n\nWe can be contacted at contact@datacomp.ai.\n\nQ54: Is there an erratum? If so, please provide a link or other access point.\n\nCurrently there are no errata. If issues are discovered, we will communicate with the public via our website Datacomp Website.\n\nQ55: Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?\n\nUpdates will be communicated through our website and other relevant channels.", "md": "Q52: Who will be supporting/hosting/maintaining the dataset?\n\nHuggingFace currently hosts the url-text pairs and metadata. The DATACOMP team will be responsible for maintaining the dataset.\n\nQ53: How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\n\nWe can be contacted at contact@datacomp.ai.\n\nQ54: Is there an erratum? If so, please provide a link or other access point.\n\nCurrently there are no errata. If issues are discovered, we will communicate with the public via our website Datacomp Website.\n\nQ55: Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?\n\nUpdates will be communicated through our website and other relevant channels."}]}, {"page": 66, "text": "         \u2022 At the present time there is no intention to update COMMONPOOL for scientific reasons. However,\n            we will respond to user takedown requests (see Q56). COMMONPOOL is inherently noisy and\n            the purpose of releasing it is to encourage researchers in the community to study dataset cleaning\n            in the context of image-text samples.\nQ56 If the dataset relates to people, are there applicable limits on the retention of the data associated\n      with the instances (e.g., were individuals in question told that their data would be retained for a\n      fixed period of time and then deleted)? If so, please describe these limits and explain how they will\n      be enforced.\n         \u2022 We will use the following website, https://laion.ai/dataset-requests, for user\n            takedown requests, where \u201cSample ID\u201d is the sample uid.\nQ57 Will older versions of the dataset continue to be supported/hosted/maintained? If so, please\n      describe how. If not, please describe how its obsolescence will be communicated to users.\n         \u2022 This is the first version of DATACOMP and the associated COMMONPOOL dataset. We do not\n            intend to maintain deprecated version of COMMONPOOL. We will communicate deprication\n            notices through our website: https://datacomp.ai.\nQ58 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\n      them to do so? If so, please provide a description. Will these contributions be validated/verified?\n      If so, please describe how. If not, why not? Is there a process for communicating/distributing these\n      contributions to other users? If so, please provide a description.\n         \u2022 All alterations to the dataset will be handled on a case-by-case basis.\nQ59 Any other comments?\n         \u2022 We encourage community members to contact us at contact@datacomp.ai with inquiries\n            related to dataset maintainence.\n                                                   66", "md": "\u2022 At the present time there is no intention to update COMMONPOOL for scientific reasons. However,\nwe will respond to user takedown requests (see Q56). COMMONPOOL is inherently noisy and\nthe purpose of releasing it is to encourage researchers in the community to study dataset cleaning\nin the context of image-text samples.\n\nQ56 If the dataset relates to people, are there applicable limits on the retention of the data associated\nwith the instances (e.g., were individuals in question told that their data would be retained for a\nfixed period of time and then deleted)? If so, please describe these limits and explain how they will\nbe enforced.\n\n\u2022 We will use the following website, https://laion.ai/dataset-requests, for user\ntakedown requests, where \u201cSample ID\u201d is the sample uid.\n\nQ57 Will older versions of the dataset continue to be supported/hosted/maintained? If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users.\n\n\u2022 This is the first version of DATACOMP and the associated COMMONPOOL dataset. We do not\nintend to maintain deprecated version of COMMONPOOL. We will communicate deprication\nnotices through our website: https://datacomp.ai.\n\nQ58 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so? If so, please provide a description. Will these contributions be validated/verified?\nIf so, please describe how. If not, why not? Is there a process for communicating/distributing these\ncontributions to other users? If so, please provide a description.\n\n\u2022 All alterations to the dataset will be handled on a case-by-case basis.\n\nQ59 Any other comments?\n\n\u2022 We encourage community members to contact us at contact@datacomp.ai with inquiries\nrelated to dataset maintainence.\n\n66", "images": [], "items": [{"type": "text", "value": "\u2022 At the present time there is no intention to update COMMONPOOL for scientific reasons. However,\nwe will respond to user takedown requests (see Q56). COMMONPOOL is inherently noisy and\nthe purpose of releasing it is to encourage researchers in the community to study dataset cleaning\nin the context of image-text samples.\n\nQ56 If the dataset relates to people, are there applicable limits on the retention of the data associated\nwith the instances (e.g., were individuals in question told that their data would be retained for a\nfixed period of time and then deleted)? If so, please describe these limits and explain how they will\nbe enforced.\n\n\u2022 We will use the following website, https://laion.ai/dataset-requests, for user\ntakedown requests, where \u201cSample ID\u201d is the sample uid.\n\nQ57 Will older versions of the dataset continue to be supported/hosted/maintained? If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users.\n\n\u2022 This is the first version of DATACOMP and the associated COMMONPOOL dataset. We do not\nintend to maintain deprecated version of COMMONPOOL. We will communicate deprication\nnotices through our website: https://datacomp.ai.\n\nQ58 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so? If so, please provide a description. Will these contributions be validated/verified?\nIf so, please describe how. If not, why not? Is there a process for communicating/distributing these\ncontributions to other users? If so, please provide a description.\n\n\u2022 All alterations to the dataset will be handled on a case-by-case basis.\n\nQ59 Any other comments?\n\n\u2022 We encourage community members to contact us at contact@datacomp.ai with inquiries\nrelated to dataset maintainence.\n\n66", "md": "\u2022 At the present time there is no intention to update COMMONPOOL for scientific reasons. However,\nwe will respond to user takedown requests (see Q56). COMMONPOOL is inherently noisy and\nthe purpose of releasing it is to encourage researchers in the community to study dataset cleaning\nin the context of image-text samples.\n\nQ56 If the dataset relates to people, are there applicable limits on the retention of the data associated\nwith the instances (e.g., were individuals in question told that their data would be retained for a\nfixed period of time and then deleted)? If so, please describe these limits and explain how they will\nbe enforced.\n\n\u2022 We will use the following website, https://laion.ai/dataset-requests, for user\ntakedown requests, where \u201cSample ID\u201d is the sample uid.\n\nQ57 Will older versions of the dataset continue to be supported/hosted/maintained? If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users.\n\n\u2022 This is the first version of DATACOMP and the associated COMMONPOOL dataset. We do not\nintend to maintain deprecated version of COMMONPOOL. We will communicate deprication\nnotices through our website: https://datacomp.ai.\n\nQ58 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so? If so, please provide a description. Will these contributions be validated/verified?\nIf so, please describe how. If not, why not? Is there a process for communicating/distributing these\ncontributions to other users? If so, please provide a description.\n\n\u2022 All alterations to the dataset will be handled on a case-by-case basis.\n\nQ59 Any other comments?\n\n\u2022 We encourage community members to contact us at contact@datacomp.ai with inquiries\nrelated to dataset maintainence.\n\n66"}]}], "job_id": "8aa4e856-11a5-4669-b6ef-946dc69c0af8", "file_path": "./corpus/2304.14108.pdf"}