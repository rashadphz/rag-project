{"pages": [{"page": 1, "text": "                                                    Consistent Diffusion Models:\n                        Mitigating Sampling Drift by Learning to be Consistent\n                                Giannis Daras\u2217                                                     Yuval Dagan\u2217\n                  Department of Computer Science                            Electrical Engineering and Computer Science\n                    University of Texas at Austin                                    University of California, Berkeley\narXiv:2302.09057v1  [cs.LG]  17 Feb 2023\n                         Alexandros G. Dimakis                                           Constantinos Daskalakis\n                           Department of ECE                             Electrical Engineering and Computer Science\n                    University of Texas at Austin                             Massachusetts Institute of Technology\n                                                                   February 20, 2023\n                                                                           Abstract\n                       Imperfect score-matching leads to a shift between the training and the sampling distribution of\n                   diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield\n                   sampling iterates that drift away from the training distribution. Yet, the standard training objective via\n                   Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted\n                   data, we propose to enforce a consistency property which states that predictions of the model on its own\n                   generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on\n                   some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score\n                   is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-\n                   the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in\n                   AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm.\n            1      Introduction\n            The diffusion-based (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020) approach to generative\n            models has been successful across various modalities, including images (Ramesh et al., 2022; Saharia et al.,\n            2022; Dhariwal and Nichol, 2021; Nichol and Dhariwal, 2021; Kim et al., 2022; Song et al., 2021b; Ruiz et al.,\n            2022; Gal et al., 2022; Daras and Dimakis, 2022; Daras et al., 2022a), videos (Ho et al., 2022a,b; Hong et al.,\n            2022), audio (Kong et al., 2021), 3D structures (Poole et al., 2022), proteins (Anand and Achim, 2022; Trippe\n            et al., 2022; Schneuing et al., 2022; Corso et al., 2022), and medical applications (Jalal et al., 2021; Arvinte\n            et al., 2022).\n                Diffusion models generate data by first drawing a sample from a noisy distribution and slowly denoising\n            this sample to ultimately obtain a sample from the target distribution. This is achieved by sampling, in\n            reverse from time t = 1 down to t = 0, a stochastic process {xt}t\u2208[0,1] wherein x0 is distributed according to\n            the target distribution p0 and, for all t,\n                                                        xt \u223c   pt where pt := p0 \u2295         N(0, \u03c32 t Id).                         (1)\n            That is, pt is the distribution resulting from corrupting a sample from p0 with noise sampled from N(0, \u03c32          t Id),\n            where \u03c3t is an increasing function such that \u03c30 = 0 and \u03c31 is sufficiently large so that p1 is nearly\n            indistinguishable from pure noise. We note that diffusion models have been generalized to other types of\n               \u2217These authors contributed equally to this work\n                                                                                1", "md": "# Consistent Diffusion Models\n\n# Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\n\nGiannis Daras* - Department of Computer Science, University of Texas at Austin\n\nYuval Dagan* - Electrical Engineering and Computer Science, University of California, Berkeley\n\nAlexandros G. Dimakis - Department of ECE, University of Texas at Austin\n\nConstantinos Daskalakis - Electrical Engineering and Computer Science, Massachusetts Institute of Technology\n\narXiv:2302.09057v1 [cs.LG] 17 Feb 2023\n\nFebruary 20, 2023\n\n## Abstract\n\nImperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a consistency property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm.\n\n## Introduction\n\nThe diffusion-based approach to generative models has been successful across various modalities, including images, videos, audio, 3D structures, proteins, and medical applications.\n\nDiffusion models generate data by first drawing a sample from a noisy distribution and slowly denoising this sample to ultimately obtain a sample from the target distribution. This is achieved by sampling, in reverse from time $$t = 1$$ down to $$t = 0$$, a stochastic process $$\\{x_t\\}_{t\\in[0,1]}$$ wherein $$x_0$$ is distributed according to the target distribution $$p_0$$ and, for all $$t$$,\n\n$$x_t \\sim p_t \\text{ where } p_t := p_0 \\oplus N(0, \\sigma^2_t I_d). \\quad (1)$$\n\nThat is, $$p_t$$ is the distribution resulting from corrupting a sample from $$p_0$$ with noise sampled from $$N(0, \\sigma^2_t I_d)$$, where $$\\sigma_t$$ is an increasing function such that $$\\sigma_0 = 0$$ and $$\\sigma_1$$ is sufficiently large so that $$p_1$$ is nearly indistinguishable from pure noise. We note that diffusion models have been generalized to other types of.\n\n*These authors contributed equally to this work", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Consistent Diffusion Models", "md": "# Consistent Diffusion Models"}, {"type": "heading", "lvl": 1, "value": "Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent", "md": "# Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent"}, {"type": "text", "value": "Giannis Daras* - Department of Computer Science, University of Texas at Austin\n\nYuval Dagan* - Electrical Engineering and Computer Science, University of California, Berkeley\n\nAlexandros G. Dimakis - Department of ECE, University of Texas at Austin\n\nConstantinos Daskalakis - Electrical Engineering and Computer Science, Massachusetts Institute of Technology\n\narXiv:2302.09057v1 [cs.LG] 17 Feb 2023\n\nFebruary 20, 2023", "md": "Giannis Daras* - Department of Computer Science, University of Texas at Austin\n\nYuval Dagan* - Electrical Engineering and Computer Science, University of California, Berkeley\n\nAlexandros G. Dimakis - Department of ECE, University of Texas at Austin\n\nConstantinos Daskalakis - Electrical Engineering and Computer Science, Massachusetts Institute of Technology\n\narXiv:2302.09057v1 [cs.LG] 17 Feb 2023\n\nFebruary 20, 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Imperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a consistency property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm.", "md": "Imperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a consistency property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "The diffusion-based approach to generative models has been successful across various modalities, including images, videos, audio, 3D structures, proteins, and medical applications.\n\nDiffusion models generate data by first drawing a sample from a noisy distribution and slowly denoising this sample to ultimately obtain a sample from the target distribution. This is achieved by sampling, in reverse from time $$t = 1$$ down to $$t = 0$$, a stochastic process $$\\{x_t\\}_{t\\in[0,1]}$$ wherein $$x_0$$ is distributed according to the target distribution $$p_0$$ and, for all $$t$$,\n\n$$x_t \\sim p_t \\text{ where } p_t := p_0 \\oplus N(0, \\sigma^2_t I_d). \\quad (1)$$\n\nThat is, $$p_t$$ is the distribution resulting from corrupting a sample from $$p_0$$ with noise sampled from $$N(0, \\sigma^2_t I_d)$$, where $$\\sigma_t$$ is an increasing function such that $$\\sigma_0 = 0$$ and $$\\sigma_1$$ is sufficiently large so that $$p_1$$ is nearly indistinguishable from pure noise. We note that diffusion models have been generalized to other types of.\n\n*These authors contributed equally to this work", "md": "The diffusion-based approach to generative models has been successful across various modalities, including images, videos, audio, 3D structures, proteins, and medical applications.\n\nDiffusion models generate data by first drawing a sample from a noisy distribution and slowly denoising this sample to ultimately obtain a sample from the target distribution. This is achieved by sampling, in reverse from time $$t = 1$$ down to $$t = 0$$, a stochastic process $$\\{x_t\\}_{t\\in[0,1]}$$ wherein $$x_0$$ is distributed according to the target distribution $$p_0$$ and, for all $$t$$,\n\n$$x_t \\sim p_t \\text{ where } p_t := p_0 \\oplus N(0, \\sigma^2_t I_d). \\quad (1)$$\n\nThat is, $$p_t$$ is the distribution resulting from corrupting a sample from $$p_0$$ with noise sampled from $$N(0, \\sigma^2_t I_d)$$, where $$\\sigma_t$$ is an increasing function such that $$\\sigma_0 = 0$$ and $$\\sigma_1$$ is sufficiently large so that $$p_1$$ is nearly indistinguishable from pure noise. We note that diffusion models have been generalized to other types of.\n\n*These authors contributed equally to this work"}]}, {"page": 2, "text": "corruptions by the recent works of Daras et al. (2022b); Bansal et al. (2022); Hoogeboom and Salimans\n(2022); Deasy et al. (2021); Nachmani et al. (2021).\n      In order to sample from a diffusion model, i.e. sample the afore-described process in reverse time, it\nsuffices to know the score function s(x, t) = \u2207x log p(x, t), where p(x, t) is the density of xt \u223c                                                                      pt. Indeed,\ngiven a sample xt \u223c                 pt, one can use the score function at xt, i.e. s(xt, t), to generate a sample from pt\u2212dt by\ntaking an infinitesimal step of a stochastic or an ordinary differential equation (Song et al., 2021b,a), or by\nusing Langevin dynamics (Grenander and Miller, 1994; Song and Ermon, 2020).1 Hence, in order to train\na diffusion model to sample from a target distribution of interest p\u2217                                                  0 it suffices to learn the score function\ns\u2217(x, t) using samples from the corrupted distributions p\u2217                                         t resulting from p\u2217            0 and a particular noise schedule\n\u03c3t. Notice that those samples can be easily drawn given samples from p\u2217                                                      0.\nThe Sampling Drift Challenge:                                     Unfortunately the true score function s\u2217(x, t) is not perfectly learned\nduring training. Thus, at generation time, the samples xt drawn using the learned score function, s(x, t), in\nthe ways discussed above, drift astray in distribution from the true corrupted distributions p\u2217                                                                      t . This drift\nbecomes larger for smaller t due to compounding of errors and is accentuated by the fact that the further\naway a sample xt is from the likely support of the true p\u2217                                           t the larger is also the error \u2225s(xt, t) \u2212                            s\u2217(xt, t)\u2225\nbetween the learned and the true score function at xt, which feeds into an even larger drift between the\ndistribution of xt\u2032 from p\u2217                t\u2032 for t\u2032 < t; see e.g. (Sehwag et al., 2022; Ho et al., 2020; Nichol and Dhariwal, 2021;\nChen et al., 2022a). These challenges motivate the question:\nQuestion 1. How can one train diffusion models to improve the error \u2225s(x, t) \u2212                                                           s\u2217(x, t)\u2225      between the learned\nand true score function on inputs (x, t) where x is unlikely under the target noisy distribution p\u2217                                                                    t ?\n      A direct approach to this challenge is to train our model to minimize the afore-described error on pairs\n(x, t) where x is sampled from distributions other than p\u2217                                       t . However, there is no straightforward way to do so,\nbecause we do not have direct access to the values of the true score function s\u2217(x, t).\n      This motivates us to propose a novel training method to mitigate sampling drift by enforcing that the\nlearned score function satisfies an invariant, that we call \u201cconsistency property.\u201d This property relates\nmultiple inputs to s(\u00b7, \u00b7) and can be optimized without using any samples from the target distribution p\u2217                                                                        0. As\nwe will show theoretically, enforcing this consistency in conjunction with minimizing a very weakened form of\nthe standard score matching objective (for a single t and an open set of x\u2019s) suffices to learn the correct score\neverywhere. We also provide experiments illustrating that regularizing the standard score matching objective\nusing our consistency property leads to state-of-the-art models.\nOur Approach:                     The true score function s\u2217(x, t) is closely related to another function, called optimal\ndenoiser, which predicts a clean sample x0 \u223c                                    p\u22170 from a noisy observation xt = x0 + \u03c3t\u03b7 where the noise is\n\u03b7 \u223c     N(0, Id). The optimal denoiser (under the \u21132 loss) is the conditional expectation:\n                                                                        h\u2217(x, t) := E[x0 | xt = x],\nand the true score function can be obtained from the optimal denoiser as follows: s\u2217(x, t) = (h\u2217(x, t) \u2212                                                                        x)/\u03c32   t .\nIndeed, the standard training technique, via score-matching, explicitly trains for the score through the denoiser\nh\u2217    (Vincent, 2011; Efron, 2011; Meng et al., 2021; Kim and Ye, 2021; Luo, 2022).\n      We are now ready to state our consistency property. We will say that a (denoising) function h(x, t) is\nconsistent iff\n                                                                    \u2200t, \u2200x : E[x0|xt = x] = h(x, t),\nwhere the expectation is with respect to a sample from the learned reverse process, defined in terms of the\nimplied score function s(x, t) = (h(x, t) \u2212                             x)/\u03c32   t , when this is initialized at xt = x and run backwards in time\nto sample x0. See Eq. (3) for the precise stochastic differential equation and its justification. In particular, h\nis called consistent if the prediction h(x, t) of the conditional expectation of the clean image x0 given xt = x\nequals the expected value of an image that is generated by the learned reversed process, starting from xt = x.\n     1Some of these methods, such as Langevin dynamics, require also to know the score function in the neighborhood of xt.\n                                                                                            2", "md": "Corruptions by the recent works of Daras et al. (2022b); Bansal et al. (2022); Hoogeboom and Salimans\n(2022); Deasy et al. (2021); Nachmani et al. (2021).\n\nIn order to sample from a diffusion model, i.e. sample the afore-described process in reverse time, it\nsuffices to know the score function \\(s(x, t) = \\nabla_x \\log p(x, t)\\), where \\(p(x, t)\\) is the density of \\(x_t \\sim p_t\\). Indeed,\ngiven a sample \\(x_t \\sim p_t\\), one can use the score function at \\(x_t\\), i.e. \\(s(x_t, t)\\), to generate a sample from \\(p_{t-dt}\\) by\ntaking an infinitesimal step of a stochastic or an ordinary differential equation (Song et al., 2021b,a), or by\nusing Langevin dynamics (Grenander and Miller, 1994; Song and Ermon, 2020). Hence, in order to train\na diffusion model to sample from a target distribution of interest \\(p^*_0\\) it suffices to learn the score function\n\\(s^*(x, t)\\) using samples from the corrupted distributions \\(p^*_t\\) resulting from \\(p^*_0\\) and a particular noise schedule\n\\(\\sigma_t\\). Notice that those samples can be easily drawn given samples from \\(p^*_0\\).\n\nThe Sampling Drift Challenge: Unfortunately the true score function \\(s^*(x, t)\\) is not perfectly learned\nduring training. Thus, at generation time, the samples \\(x_t\\) drawn using the learned score function, \\(s(x, t)\\), in\nthe ways discussed above, drift astray in distribution from the true corrupted distributions \\(p^*_t\\). This drift\nbecomes larger for smaller \\(t\\) due to compounding of errors and is accentuated by the fact that the further\naway a sample \\(x_t\\) is from the likely support of the true \\(p^*_t\\) the larger is also the error \\(\\|s(x_t, t) - s^*(x_t, t)\\|\\)\nbetween the learned and the true score function at \\(x_t\\), which feeds into an even larger drift between the\ndistribution of \\(x_{t'}\\) from \\(p^*_{t'}\\) for \\(t' < t\\); see e.g. (Sehwag et al., 2022; Ho et al., 2020; Nichol and Dhariwal, 2021;\nChen et al., 2022a). These challenges motivate the question:\n\nQuestion 1. How can one train diffusion models to improve the error \\(\\|s(x, t) - s^*(x, t)\\|\\) between the learned\nand true score function on inputs \\((x, t)\\) where \\(x\\) is unlikely under the target noisy distribution \\(p^*_t\\)?\n\nA direct approach to this challenge is to train our model to minimize the afore-described error on pairs\n\\((x, t)\\) where \\(x\\) is sampled from distributions other than \\(p^*_t\\). However, there is no straightforward way to do so,\nbecause we do not have direct access to the values of the true score function \\(s^*(x, t)\\).\n\nThis motivates us to propose a novel training method to mitigate sampling drift by enforcing that the\nlearned score function satisfies an invariant, that we call \u201cconsistency property.\u201d This property relates\nmultiple inputs to \\(s(\\cdot, \\cdot)\\) and can be optimized without using any samples from the target distribution \\(p^*_0\\). As\nwe will show theoretically, enforcing this consistency in conjunction with minimizing a very weakened form of\nthe standard score matching objective (for a single \\(t\\) and an open set of \\(x\\)\u2019s) suffices to learn the correct score\neverywhere. We also provide experiments illustrating that regularizing the standard score matching objective\nusing our consistency property leads to state-of-the-art models.\n\nOur Approach: The true score function \\(s^*(x, t)\\) is closely related to another function, called optimal\ndenoiser, which predicts a clean sample \\(x_0 \\sim p^*_0\\) from a noisy observation \\(x_t = x_0 + \\sigma_t \\eta\\) where the noise is\n\\(\\eta \\sim N(0, I_d)\\). The optimal denoiser (under the \\(\\ell_2\\) loss) is the conditional expectation:\n\\(h^*(x, t) := E[x_0 | x_t = x]\\),\nand the true score function can be obtained from the optimal denoiser as follows: \\(s^*(x, t) = (h^*(x, t) - x)/\\sigma^2_t\\).\nIndeed, the standard training technique, via score-matching, explicitly trains for the score through the denoiser\n\\(h^*\\) (Vincent, 2011; Efron, 2011; Meng et al., 2021; Kim and Ye, 2021; Luo, 2022).\n\nWe are now ready to state our consistency property. We will say that a (denoising) function \\(h(x, t)\\) is\nconsistent iff\n\\(\\forall t, \\forall x : E[x_0|x_t = x] = h(x, t)\\),\nwhere the expectation is with respect to a sample from the learned reverse process, defined in terms of the\nimplied score function \\(s(x, t) = (h(x, t) - x)/\\sigma^2_t\\), when this is initialized at \\(x_t = x\\) and run backwards in time\nto sample \\(x_0\\). See Eq. (3) for the precise stochastic differential equation and its justification. In particular, \\(h\\)\nis called consistent if the prediction \\(h(x, t)\\) of the conditional expectation of the clean image \\(x_0\\) given \\(x_t = x\\)\nequals the expected value of an image that is generated by the learned reversed process, starting from \\(x_t = x\\).\n\nSome of these methods, such as Langevin dynamics, require also to know the score function in the neighborhood of \\(x_t\\).", "images": [], "items": [{"type": "text", "value": "Corruptions by the recent works of Daras et al. (2022b); Bansal et al. (2022); Hoogeboom and Salimans\n(2022); Deasy et al. (2021); Nachmani et al. (2021).\n\nIn order to sample from a diffusion model, i.e. sample the afore-described process in reverse time, it\nsuffices to know the score function \\(s(x, t) = \\nabla_x \\log p(x, t)\\), where \\(p(x, t)\\) is the density of \\(x_t \\sim p_t\\). Indeed,\ngiven a sample \\(x_t \\sim p_t\\), one can use the score function at \\(x_t\\), i.e. \\(s(x_t, t)\\), to generate a sample from \\(p_{t-dt}\\) by\ntaking an infinitesimal step of a stochastic or an ordinary differential equation (Song et al., 2021b,a), or by\nusing Langevin dynamics (Grenander and Miller, 1994; Song and Ermon, 2020). Hence, in order to train\na diffusion model to sample from a target distribution of interest \\(p^*_0\\) it suffices to learn the score function\n\\(s^*(x, t)\\) using samples from the corrupted distributions \\(p^*_t\\) resulting from \\(p^*_0\\) and a particular noise schedule\n\\(\\sigma_t\\). Notice that those samples can be easily drawn given samples from \\(p^*_0\\).\n\nThe Sampling Drift Challenge: Unfortunately the true score function \\(s^*(x, t)\\) is not perfectly learned\nduring training. Thus, at generation time, the samples \\(x_t\\) drawn using the learned score function, \\(s(x, t)\\), in\nthe ways discussed above, drift astray in distribution from the true corrupted distributions \\(p^*_t\\). This drift\nbecomes larger for smaller \\(t\\) due to compounding of errors and is accentuated by the fact that the further\naway a sample \\(x_t\\) is from the likely support of the true \\(p^*_t\\) the larger is also the error \\(\\|s(x_t, t) - s^*(x_t, t)\\|\\)\nbetween the learned and the true score function at \\(x_t\\), which feeds into an even larger drift between the\ndistribution of \\(x_{t'}\\) from \\(p^*_{t'}\\) for \\(t' < t\\); see e.g. (Sehwag et al., 2022; Ho et al., 2020; Nichol and Dhariwal, 2021;\nChen et al., 2022a). These challenges motivate the question:\n\nQuestion 1. How can one train diffusion models to improve the error \\(\\|s(x, t) - s^*(x, t)\\|\\) between the learned\nand true score function on inputs \\((x, t)\\) where \\(x\\) is unlikely under the target noisy distribution \\(p^*_t\\)?\n\nA direct approach to this challenge is to train our model to minimize the afore-described error on pairs\n\\((x, t)\\) where \\(x\\) is sampled from distributions other than \\(p^*_t\\). However, there is no straightforward way to do so,\nbecause we do not have direct access to the values of the true score function \\(s^*(x, t)\\).\n\nThis motivates us to propose a novel training method to mitigate sampling drift by enforcing that the\nlearned score function satisfies an invariant, that we call \u201cconsistency property.\u201d This property relates\nmultiple inputs to \\(s(\\cdot, \\cdot)\\) and can be optimized without using any samples from the target distribution \\(p^*_0\\). As\nwe will show theoretically, enforcing this consistency in conjunction with minimizing a very weakened form of\nthe standard score matching objective (for a single \\(t\\) and an open set of \\(x\\)\u2019s) suffices to learn the correct score\neverywhere. We also provide experiments illustrating that regularizing the standard score matching objective\nusing our consistency property leads to state-of-the-art models.\n\nOur Approach: The true score function \\(s^*(x, t)\\) is closely related to another function, called optimal\ndenoiser, which predicts a clean sample \\(x_0 \\sim p^*_0\\) from a noisy observation \\(x_t = x_0 + \\sigma_t \\eta\\) where the noise is\n\\(\\eta \\sim N(0, I_d)\\). The optimal denoiser (under the \\(\\ell_2\\) loss) is the conditional expectation:\n\\(h^*(x, t) := E[x_0 | x_t = x]\\),\nand the true score function can be obtained from the optimal denoiser as follows: \\(s^*(x, t) = (h^*(x, t) - x)/\\sigma^2_t\\).\nIndeed, the standard training technique, via score-matching, explicitly trains for the score through the denoiser\n\\(h^*\\) (Vincent, 2011; Efron, 2011; Meng et al., 2021; Kim and Ye, 2021; Luo, 2022).\n\nWe are now ready to state our consistency property. We will say that a (denoising) function \\(h(x, t)\\) is\nconsistent iff\n\\(\\forall t, \\forall x : E[x_0|x_t = x] = h(x, t)\\),\nwhere the expectation is with respect to a sample from the learned reverse process, defined in terms of the\nimplied score function \\(s(x, t) = (h(x, t) - x)/\\sigma^2_t\\), when this is initialized at \\(x_t = x\\) and run backwards in time\nto sample \\(x_0\\). See Eq. (3) for the precise stochastic differential equation and its justification. In particular, \\(h\\)\nis called consistent if the prediction \\(h(x, t)\\) of the conditional expectation of the clean image \\(x_0\\) given \\(x_t = x\\)\nequals the expected value of an image that is generated by the learned reversed process, starting from \\(x_t = x\\).\n\nSome of these methods, such as Langevin dynamics, require also to know the score function in the neighborhood of \\(x_t\\).", "md": "Corruptions by the recent works of Daras et al. (2022b); Bansal et al. (2022); Hoogeboom and Salimans\n(2022); Deasy et al. (2021); Nachmani et al. (2021).\n\nIn order to sample from a diffusion model, i.e. sample the afore-described process in reverse time, it\nsuffices to know the score function \\(s(x, t) = \\nabla_x \\log p(x, t)\\), where \\(p(x, t)\\) is the density of \\(x_t \\sim p_t\\). Indeed,\ngiven a sample \\(x_t \\sim p_t\\), one can use the score function at \\(x_t\\), i.e. \\(s(x_t, t)\\), to generate a sample from \\(p_{t-dt}\\) by\ntaking an infinitesimal step of a stochastic or an ordinary differential equation (Song et al., 2021b,a), or by\nusing Langevin dynamics (Grenander and Miller, 1994; Song and Ermon, 2020). Hence, in order to train\na diffusion model to sample from a target distribution of interest \\(p^*_0\\) it suffices to learn the score function\n\\(s^*(x, t)\\) using samples from the corrupted distributions \\(p^*_t\\) resulting from \\(p^*_0\\) and a particular noise schedule\n\\(\\sigma_t\\). Notice that those samples can be easily drawn given samples from \\(p^*_0\\).\n\nThe Sampling Drift Challenge: Unfortunately the true score function \\(s^*(x, t)\\) is not perfectly learned\nduring training. Thus, at generation time, the samples \\(x_t\\) drawn using the learned score function, \\(s(x, t)\\), in\nthe ways discussed above, drift astray in distribution from the true corrupted distributions \\(p^*_t\\). This drift\nbecomes larger for smaller \\(t\\) due to compounding of errors and is accentuated by the fact that the further\naway a sample \\(x_t\\) is from the likely support of the true \\(p^*_t\\) the larger is also the error \\(\\|s(x_t, t) - s^*(x_t, t)\\|\\)\nbetween the learned and the true score function at \\(x_t\\), which feeds into an even larger drift between the\ndistribution of \\(x_{t'}\\) from \\(p^*_{t'}\\) for \\(t' < t\\); see e.g. (Sehwag et al., 2022; Ho et al., 2020; Nichol and Dhariwal, 2021;\nChen et al., 2022a). These challenges motivate the question:\n\nQuestion 1. How can one train diffusion models to improve the error \\(\\|s(x, t) - s^*(x, t)\\|\\) between the learned\nand true score function on inputs \\((x, t)\\) where \\(x\\) is unlikely under the target noisy distribution \\(p^*_t\\)?\n\nA direct approach to this challenge is to train our model to minimize the afore-described error on pairs\n\\((x, t)\\) where \\(x\\) is sampled from distributions other than \\(p^*_t\\). However, there is no straightforward way to do so,\nbecause we do not have direct access to the values of the true score function \\(s^*(x, t)\\).\n\nThis motivates us to propose a novel training method to mitigate sampling drift by enforcing that the\nlearned score function satisfies an invariant, that we call \u201cconsistency property.\u201d This property relates\nmultiple inputs to \\(s(\\cdot, \\cdot)\\) and can be optimized without using any samples from the target distribution \\(p^*_0\\). As\nwe will show theoretically, enforcing this consistency in conjunction with minimizing a very weakened form of\nthe standard score matching objective (for a single \\(t\\) and an open set of \\(x\\)\u2019s) suffices to learn the correct score\neverywhere. We also provide experiments illustrating that regularizing the standard score matching objective\nusing our consistency property leads to state-of-the-art models.\n\nOur Approach: The true score function \\(s^*(x, t)\\) is closely related to another function, called optimal\ndenoiser, which predicts a clean sample \\(x_0 \\sim p^*_0\\) from a noisy observation \\(x_t = x_0 + \\sigma_t \\eta\\) where the noise is\n\\(\\eta \\sim N(0, I_d)\\). The optimal denoiser (under the \\(\\ell_2\\) loss) is the conditional expectation:\n\\(h^*(x, t) := E[x_0 | x_t = x]\\),\nand the true score function can be obtained from the optimal denoiser as follows: \\(s^*(x, t) = (h^*(x, t) - x)/\\sigma^2_t\\).\nIndeed, the standard training technique, via score-matching, explicitly trains for the score through the denoiser\n\\(h^*\\) (Vincent, 2011; Efron, 2011; Meng et al., 2021; Kim and Ye, 2021; Luo, 2022).\n\nWe are now ready to state our consistency property. We will say that a (denoising) function \\(h(x, t)\\) is\nconsistent iff\n\\(\\forall t, \\forall x : E[x_0|x_t = x] = h(x, t)\\),\nwhere the expectation is with respect to a sample from the learned reverse process, defined in terms of the\nimplied score function \\(s(x, t) = (h(x, t) - x)/\\sigma^2_t\\), when this is initialized at \\(x_t = x\\) and run backwards in time\nto sample \\(x_0\\). See Eq. (3) for the precise stochastic differential equation and its justification. In particular, \\(h\\)\nis called consistent if the prediction \\(h(x, t)\\) of the conditional expectation of the clean image \\(x_0\\) given \\(x_t = x\\)\nequals the expected value of an image that is generated by the learned reversed process, starting from \\(x_t = x\\).\n\nSome of these methods, such as Langevin dynamics, require also to know the score function in the neighborhood of \\(x_t\\)."}]}, {"page": 3, "text": "     While there are several other properties that the score function of a diffusion process must satisfy, e.g. the\nFokker-Planck equation (Lai et al., 2022), our first theoretical result is that the consistency of h(x, t) suffices\n(in conjunction with the conservativeness of its score function s(x, t) = (h(x, t) \u2212                                  x)/\u03c32 t ) to guarantee that s\nmust be the score function of a diffusion process (and must thus satisfy any other property that a diffusion\nprocess must satisfy). If additionally s(x, t) equals the score function s\u2217(x, t) of a target diffusion process at a\nsingle time t = t0 and an open subset of x \u2208                     Rd, then it equals s\u2217          everywhere. Intuitively, this suggests that\nlearning the score in-sample for a single t = t0, and satisfying the consistency and conservativeness properties\noff-sample, also yields a correct estimate off-sample. This can be summarized as follows below:\nTheorem 1.1 (informal). If some denoiser h(x, t) is consistent and its corresponding score function s(x, t) =\n(h(x, t) \u2212     x)/\u03c32  t is a conservative field, then s(x, t) is the score function of a diffusion process, i.e. the\ngeneration process using score function s is the inverse of a diffusion process. If additionally s(x, t) = s\u2217(x, t)\nfor a single t = t0 and all x in an open subset of Rd, where s\u2217                                is the score function of a target diffusion\nprocess, then s(x, t) = s\u2217(x, t) everywhere, i.e. to learn the score function everywhere it suffices to learn it for\na single t0 and an open subset of x\u2019s.\n     We propose a loss function to train for the consistency property and we show experimentally that\nregularizing the standard score matching objective using our consistency property leads to better models.\nSummary of Contributions:\n     1. We identify an invariant property, consistency of the denoiser h, that any perfectly trained model should\n         satisfy.\n     2. We prove that if the denoiser h(x, t) is consistent and its implied score function s(x, t) = (h(x, t)\u2212x)/\u03c32                                        t\n         is a conservative field, then s(x, t) is the score function of some diffusion process, even if there are\n         learning errors with respect to the score of the target process, which generates the training data.\n     3. We prove that if these two properties are satisfied, then optimizing perfectly the score for a single t = t0\n         and an open subset S \u2286              Rd, guarantees that the score is learned perfectly everywhere.\n     4. We propose a novel training objective that enforces the consistency property. Our new objective\n         optimizes the network to have consistent predictions on data points from the learned distribution.\n     5. We show experimentally that, paired with the original Denoising Score Matching (DSM) loss, our\n         objective achieves a new state-of-the-art on conditional and unconditional generation in CIFAR10 and\n         baseline improvements in AFHQ and FFHQ.\n     6. We open-source our code and models: https://github.com/giannisdaras/cdm.\n2        Background\nDiffusion processes, score functions and denoising.                                     Diffusion models are trained by solving a super-\nvised regression problem (Song and Ermon, 2019; Ho et al., 2020). The function that one aims to learn, called\nthe score function, defined below, is equivalent (up to a linear transformation) to a denoising function (Efron,\n2011; Vincent, 2011), whose goal is to denoise an image that was injected with noise. In particular, for some\ntarget distribution p0, one\u2019s goal is to learn the following function h: Rd \u00d7 [0, 1] \u2192                                   Rd:\n                                         h(x, t) = E[x0 | xt = x]; x0 \u223c             p0, xt \u223c      N(x0, \u03c32   t Id).                                     (2)\nIn other words, the goal is to predict the expected \u201cclean\u201d image x0 given a corrupted version of it, assuming\nthat the image was sampled from p0 and its corruption was done by adding to it noise from N(0, \u03c32                                           t Id), where\n\u03c32t is a non-negative and increasing function of t. Given such a function h, we can generate samples from p0\n                                                                             3", "md": "While there are several other properties that the score function of a diffusion process must satisfy, e.g. the Fokker-Planck equation (Lai et al., 2022), our first theoretical result is that the consistency of \\( h(x, t) \\) suffices (in conjunction with the conservativeness of its score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\)) to guarantee that \\( s \\) must be the score function of a diffusion process (and must thus satisfy any other property that a diffusion process must satisfy). If additionally \\( s(x, t) \\) equals the score function \\( s^*(x, t) \\) of a target diffusion process at a single time \\( t = t_0 \\) and an open subset of \\( x \\in \\mathbb{R}^d \\), then it equals \\( s^* \\) everywhere. Intuitively, this suggests that learning the score in-sample for a single \\( t = t_0 \\), and satisfying the consistency and conservativeness properties off-sample, also yields a correct estimate off-sample. This can be summarized as follows below:\n\nTheorem 1.1 (informal). If some denoiser \\( h(x, t) \\) is consistent and its corresponding score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\) is a conservative field, then \\( s(x, t) \\) is the score function of a diffusion process, i.e. the generation process using score function \\( s \\) is the inverse of a diffusion process. If additionally \\( s(x, t) = s^*(x, t) \\) for a single \\( t = t_0 \\) and all \\( x \\) in an open subset of \\( \\mathbb{R}^d \\), where \\( s^* \\) is the score function of a target diffusion process, then \\( s(x, t) = s^*(x, t) \\) everywhere, i.e. to learn the score function everywhere it suffices to learn it for a single \\( t_0 \\) and an open subset of \\( x \\)'s.\n\nWe propose a loss function to train for the consistency property and we show experimentally that regularizing the standard score matching objective using our consistency property leads to better models.\n\nSummary of Contributions:\n\n1. We identify an invariant property, consistency of the denoiser \\( h \\), that any perfectly trained model should satisfy.\n2. We prove that if the denoiser \\( h(x, t) \\) is consistent and its implied score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\) is a conservative field, then \\( s(x, t) \\) is the score function of some diffusion process, even if there are learning errors with respect to the score of the target process, which generates the training data.\n3. We prove that if these two properties are satisfied, then optimizing perfectly the score for a single \\( t = t_0 \\) and an open subset \\( S \\subseteq \\mathbb{R}^d \\), guarantees that the score is learned perfectly everywhere.\n4. We propose a novel training objective that enforces the consistency property. Our new objective optimizes the network to have consistent predictions on data points from the learned distribution.\n5. We show experimentally that, paired with the original Denoising Score Matching (DSM) loss, our objective achieves a new state-of-the-art on conditional and unconditional generation in CIFAR10 and baseline improvements in AFHQ and FFHQ.\n6. We open-source our code and models: https://github.com/giannisdaras/cdm.\n\n## Background\n\nDiffusion processes, score functions and denoising. Diffusion models are trained by solving a supervised regression problem (Song and Ermon, 2019; Ho et al., 2020). The function that one aims to learn, called the score function, defined below, is equivalent (up to a linear transformation) to a denoising function (Efron, 2011; Vincent, 2011), whose goal is to denoise an image that was injected with noise. In particular, for some target distribution \\( p_0 \\), one\u2019s goal is to learn the following function \\( h: \\mathbb{R}^d \\times [0, 1] \\rightarrow \\mathbb{R}^d \\):\n\n\\[\nh(x, t) = E[x_0 | x_t = x]; x_0 \\sim p_0, x_t \\sim N(x_0, \\sigma^2 t I_d). \\quad (2)\n\\]\n\nIn other words, the goal is to predict the expected \u201cclean\u201d image \\( x_0 \\) given a corrupted version of it, assuming that the image was sampled from \\( p_0 \\) and its corruption was done by adding to it noise from \\( N(0, \\sigma^2 t I_d) \\), where \\( \\sigma^2 t \\) is a non-negative and increasing function of \\( t \\). Given such a function \\( h \\), we can generate samples from \\( p_0 \\).", "images": [], "items": [{"type": "text", "value": "While there are several other properties that the score function of a diffusion process must satisfy, e.g. the Fokker-Planck equation (Lai et al., 2022), our first theoretical result is that the consistency of \\( h(x, t) \\) suffices (in conjunction with the conservativeness of its score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\)) to guarantee that \\( s \\) must be the score function of a diffusion process (and must thus satisfy any other property that a diffusion process must satisfy). If additionally \\( s(x, t) \\) equals the score function \\( s^*(x, t) \\) of a target diffusion process at a single time \\( t = t_0 \\) and an open subset of \\( x \\in \\mathbb{R}^d \\), then it equals \\( s^* \\) everywhere. Intuitively, this suggests that learning the score in-sample for a single \\( t = t_0 \\), and satisfying the consistency and conservativeness properties off-sample, also yields a correct estimate off-sample. This can be summarized as follows below:\n\nTheorem 1.1 (informal). If some denoiser \\( h(x, t) \\) is consistent and its corresponding score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\) is a conservative field, then \\( s(x, t) \\) is the score function of a diffusion process, i.e. the generation process using score function \\( s \\) is the inverse of a diffusion process. If additionally \\( s(x, t) = s^*(x, t) \\) for a single \\( t = t_0 \\) and all \\( x \\) in an open subset of \\( \\mathbb{R}^d \\), where \\( s^* \\) is the score function of a target diffusion process, then \\( s(x, t) = s^*(x, t) \\) everywhere, i.e. to learn the score function everywhere it suffices to learn it for a single \\( t_0 \\) and an open subset of \\( x \\)'s.\n\nWe propose a loss function to train for the consistency property and we show experimentally that regularizing the standard score matching objective using our consistency property leads to better models.\n\nSummary of Contributions:\n\n1. We identify an invariant property, consistency of the denoiser \\( h \\), that any perfectly trained model should satisfy.\n2. We prove that if the denoiser \\( h(x, t) \\) is consistent and its implied score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\) is a conservative field, then \\( s(x, t) \\) is the score function of some diffusion process, even if there are learning errors with respect to the score of the target process, which generates the training data.\n3. We prove that if these two properties are satisfied, then optimizing perfectly the score for a single \\( t = t_0 \\) and an open subset \\( S \\subseteq \\mathbb{R}^d \\), guarantees that the score is learned perfectly everywhere.\n4. We propose a novel training objective that enforces the consistency property. Our new objective optimizes the network to have consistent predictions on data points from the learned distribution.\n5. We show experimentally that, paired with the original Denoising Score Matching (DSM) loss, our objective achieves a new state-of-the-art on conditional and unconditional generation in CIFAR10 and baseline improvements in AFHQ and FFHQ.\n6. We open-source our code and models: https://github.com/giannisdaras/cdm.", "md": "While there are several other properties that the score function of a diffusion process must satisfy, e.g. the Fokker-Planck equation (Lai et al., 2022), our first theoretical result is that the consistency of \\( h(x, t) \\) suffices (in conjunction with the conservativeness of its score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\)) to guarantee that \\( s \\) must be the score function of a diffusion process (and must thus satisfy any other property that a diffusion process must satisfy). If additionally \\( s(x, t) \\) equals the score function \\( s^*(x, t) \\) of a target diffusion process at a single time \\( t = t_0 \\) and an open subset of \\( x \\in \\mathbb{R}^d \\), then it equals \\( s^* \\) everywhere. Intuitively, this suggests that learning the score in-sample for a single \\( t = t_0 \\), and satisfying the consistency and conservativeness properties off-sample, also yields a correct estimate off-sample. This can be summarized as follows below:\n\nTheorem 1.1 (informal). If some denoiser \\( h(x, t) \\) is consistent and its corresponding score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\) is a conservative field, then \\( s(x, t) \\) is the score function of a diffusion process, i.e. the generation process using score function \\( s \\) is the inverse of a diffusion process. If additionally \\( s(x, t) = s^*(x, t) \\) for a single \\( t = t_0 \\) and all \\( x \\) in an open subset of \\( \\mathbb{R}^d \\), where \\( s^* \\) is the score function of a target diffusion process, then \\( s(x, t) = s^*(x, t) \\) everywhere, i.e. to learn the score function everywhere it suffices to learn it for a single \\( t_0 \\) and an open subset of \\( x \\)'s.\n\nWe propose a loss function to train for the consistency property and we show experimentally that regularizing the standard score matching objective using our consistency property leads to better models.\n\nSummary of Contributions:\n\n1. We identify an invariant property, consistency of the denoiser \\( h \\), that any perfectly trained model should satisfy.\n2. We prove that if the denoiser \\( h(x, t) \\) is consistent and its implied score function \\( s(x, t) = \\frac{{h(x, t) - x}}{{\\sigma^2 t}} \\) is a conservative field, then \\( s(x, t) \\) is the score function of some diffusion process, even if there are learning errors with respect to the score of the target process, which generates the training data.\n3. We prove that if these two properties are satisfied, then optimizing perfectly the score for a single \\( t = t_0 \\) and an open subset \\( S \\subseteq \\mathbb{R}^d \\), guarantees that the score is learned perfectly everywhere.\n4. We propose a novel training objective that enforces the consistency property. Our new objective optimizes the network to have consistent predictions on data points from the learned distribution.\n5. We show experimentally that, paired with the original Denoising Score Matching (DSM) loss, our objective achieves a new state-of-the-art on conditional and unconditional generation in CIFAR10 and baseline improvements in AFHQ and FFHQ.\n6. We open-source our code and models: https://github.com/giannisdaras/cdm."}, {"type": "heading", "lvl": 2, "value": "Background", "md": "## Background"}, {"type": "text", "value": "Diffusion processes, score functions and denoising. Diffusion models are trained by solving a supervised regression problem (Song and Ermon, 2019; Ho et al., 2020). The function that one aims to learn, called the score function, defined below, is equivalent (up to a linear transformation) to a denoising function (Efron, 2011; Vincent, 2011), whose goal is to denoise an image that was injected with noise. In particular, for some target distribution \\( p_0 \\), one\u2019s goal is to learn the following function \\( h: \\mathbb{R}^d \\times [0, 1] \\rightarrow \\mathbb{R}^d \\):\n\n\\[\nh(x, t) = E[x_0 | x_t = x]; x_0 \\sim p_0, x_t \\sim N(x_0, \\sigma^2 t I_d). \\quad (2)\n\\]\n\nIn other words, the goal is to predict the expected \u201cclean\u201d image \\( x_0 \\) given a corrupted version of it, assuming that the image was sampled from \\( p_0 \\) and its corruption was done by adding to it noise from \\( N(0, \\sigma^2 t I_d) \\), where \\( \\sigma^2 t \\) is a non-negative and increasing function of \\( t \\). Given such a function \\( h \\), we can generate samples from \\( p_0 \\).", "md": "Diffusion processes, score functions and denoising. Diffusion models are trained by solving a supervised regression problem (Song and Ermon, 2019; Ho et al., 2020). The function that one aims to learn, called the score function, defined below, is equivalent (up to a linear transformation) to a denoising function (Efron, 2011; Vincent, 2011), whose goal is to denoise an image that was injected with noise. In particular, for some target distribution \\( p_0 \\), one\u2019s goal is to learn the following function \\( h: \\mathbb{R}^d \\times [0, 1] \\rightarrow \\mathbb{R}^d \\):\n\n\\[\nh(x, t) = E[x_0 | x_t = x]; x_0 \\sim p_0, x_t \\sim N(x_0, \\sigma^2 t I_d). \\quad (2)\n\\]\n\nIn other words, the goal is to predict the expected \u201cclean\u201d image \\( x_0 \\) given a corrupted version of it, assuming that the image was sampled from \\( p_0 \\) and its corruption was done by adding to it noise from \\( N(0, \\sigma^2 t I_d) \\), where \\( \\sigma^2 t \\) is a non-negative and increasing function of \\( t \\). Given such a function \\( h \\), we can generate samples from \\( p_0 \\)."}]}, {"page": 4, "text": " by solving a Stochastic Differential Equation (SDE) that depends on h (Song et al., 2021b). Specifically, one\n starts by sampling x1 from some fixed distribution and then runs the following SDE backwards in time:\n                                                       dxt = \u2212g(t)2 h(xt, t) \u2212     \u03c32t     xt  dt + g(t)dBt,                                                       (3)\nwhere Bt is a reverse-time Brownian motion and g(t)2 = d\u03c32                                   dtt. To explain how Eq. (3) was derived, consider\n the forward SDE that starts with a clean image x0 and slowly injects noise:\n                                                                   dxt = g(t)dBt, x0 \u223c              p0.                                                            (4)\nWe notice here that the xt under Eq. (4) is N(x0, \u03c32                            t Id), where x0 \u223c          p0, so it has the same distribution that it\n has in Eq. (2). Remarkably, such SDEs are reversible in time (Anderson, 1982). Hence, the diffusion process\n of Eq. (4) can be viewed as a reversed-time diffusion:\n                                                      dxt = \u2212g(t)2\u2207x log p(xt, t)dt + g(t)dBt,                                                                     (5)\nwhere p(xt, t) is the density of xt at time t. We note that s(x, t) := \u2207x log p(x, t) is called the score function\n of xt at time t. Using Tweedie\u2019s lemma (Efron, 2011), one obtains the following relationship between the\n denoising function h and the score function:\n Substituting Eq. (6) in Eq. (5), one obtains Eq. (3).          \u2207x log p(x, t) = h(x, t) \u2212      \u03c32t     x.                                                         (6)\n Training via denoising score matching.                                  The standard way to train for h is via denoising score matching.\nThis is performed by obtaining samples of x0 \u223c                               p0 and xt \u223c         N(x0, \u03c32    t Id) and training to minimize\n                             Ex0\u223cp0,xt\u223cN(x0,\u03c32         t Id)L1 t,xt,x0(\u03b8) = Ex0\u223cp0,xt\u223cN(x0,\u03c32              t Id) \u2225h\u03b8(xt, t) \u2212        x0\u22252 ,\nwhere the optimization is over some family of functions, {h\u03b8}\u03b8\u2208\u0398. It was shown by Vincent (2011) that\n optimizing Eq. (2) is equivalent to optimizing h in mean-squared-error on a random point xt that is a noisy\n image, xt \u223c        N(x0, \u03c32    t Id) where x0 \u223c           p0:  Ext \u2225h\u03b8(xt, t) \u2212         h\u2217(xt, t)\u22252 ,\nwhere h\u2217        is the true denoising function from Eq. (2).\n 3       Theory\nWe define below the consistency property that a function h should satisfy. This states that the output of h(x, t)\n(which is meant to approximate the conditional expectation of x0 conditioned on xt = x) is indeed consistent\nwith the average point x0 generated using h and conditioning on xt = x. Recall from the previous section\n that generation according to h conditionning on xt = x is done by running the following SDE backwards in\n time conditioning on xt = x:\n                                                      dxt = \u2212g(t)2 h(xt, t) \u2212      \u03c32t     xt dt + g(t)2dBt,                                                       (7)\nThe consistency property is therefore defined as follows:\nProperty 1 (Consistency.). A function h: Rd \u00d7 [0, 1] \u2192                                      Rd is said to be consistent iff for all t \u2208                 (0, 1] and\n all x \u2208    Rd,                                                   h(x, t) = Eh[x0 | xt = x],                                                                       (8)\nwhere Eh[x0 | xt = x] corresponds to the conditional expectation of x0 in the process that starts with xt = x\n and samples x0 by running the SDE of Eq. (7) backwards in time (where note that the SDE uses h).\n                                                                                    4", "md": "# Math Equations in HTML\n\nBy solving a Stochastic Differential Equation (SDE) that depends on h (Song et al., 2021b). Specifically, one starts by sampling x1 from some fixed distribution and then runs the following SDE backwards in time:\n\n$$\ndxt = -g(t)^2 h(xt, t) - \\sigma^2 t xt dt + g(t)dBt, \\quad (3)\n$$\nwhere Bt is a reverse-time Brownian motion and \\(g(t)^2 = \\frac{d\\sigma^2}{dt}\\). To explain how Eq. (3) was derived, consider the forward SDE that starts with a clean image x0 and slowly injects noise:\n\n$$\ndxt = g(t)dBt, \\quad x0 \\sim p0. \\quad (4)\n$$\nWe notice here that the xt under Eq. (4) is \\(N(x0, \\sigma^2 t I_d)\\), where \\(x0 \\sim p0\\), so it has the same distribution that it has in Eq. (2). Remarkably, such SDEs are reversible in time (Anderson, 1982). Hence, the diffusion process of Eq. (4) can be viewed as a reversed-time diffusion:\n\n$$\ndxt = -g(t)^2\\nabla_x \\log p(xt, t)dt + g(t)dBt, \\quad (5)\n$$\nwhere p(xt, t) is the density of xt at time t. We note that \\(s(x, t) := \\nabla_x \\log p(x, t)\\) is called the score function of xt at time t. Using Tweedie\u2019s lemma (Efron, 2011), one obtains the following relationship between the denoising function h and the score function:\n\nSubstituting Eq. (6) in Eq. (5), one obtains Eq. (3).\n\n$$\n\\nabla_x \\log p(x, t) = h(x, t) - \\sigma^2 t x. \\quad (6)\n$$\nTraining via denoising score matching. The standard way to train for h is via denoising score matching. This is performed by obtaining samples of \\(x0 \\sim p0\\) and \\(xt \\sim N(x0, \\sigma^2 t I_d)\\) and training to minimize\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x0 \\sim p0, xt \\sim N(x0, \\sigma^2 t I_d)} L1(t, xt, x0)(\\theta) \\\\\n&= \\mathbb{E}_{x0 \\sim p0, xt \\sim N(x0, \\sigma^2 t I_d)} \\|h_{\\theta}(xt, t) - x0\\|^2,\n\\end{align*}\n$$\nwhere the optimization is over some family of functions, \\(\\{h_{\\theta}\\}_{\\theta \\in \\Theta}\\). It was shown by Vincent (2011) that optimizing Eq. (2) is equivalent to optimizing h in mean-squared-error on a random point xt that is a noisy image, \\(xt \\sim N(x0, \\sigma^2 t I_d)\\) where \\(x0 \\sim p0\\):\n\n$$\n\\mathbb{E}_{xt} \\|h_{\\theta}(xt, t) - h^*(xt, t)\\|^2,\n$$\nwhere \\(h^*\\) is the true denoising function from Eq. (2).\n\n### Theory\n\nWe define below the consistency property that a function h should satisfy. This states that the output of h(x, t) (which is meant to approximate the conditional expectation of x0 conditioned on xt = x) is indeed consistent with the average point x0 generated using h and conditioning on xt = x. Recall from the previous section that generation according to h conditioning on xt = x is done by running the following SDE backwards in time conditioning on xt = x:\n\n$$\ndxt = -g(t)^2 h(xt, t) - \\sigma^2 t xt dt + g(t)^2 dBt, \\quad (7)\n$$\nThe consistency property is therefore defined as follows:\n\nProperty 1 (Consistency). A function \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\) is said to be consistent iff for all \\(t \\in (0, 1]\\) and all \\(x \\in \\mathbb{R}^d\\),\n\n$$\nh(x, t) = \\mathbb{E}[x0 | xt = x], \\quad (8)\n$$\nwhere \\(\\mathbb{E}[x0 | xt = x]\\) corresponds to the conditional expectation of x0 in the process that starts with xt = x and samples x0 by running the SDE of Eq. (7) backwards in time (where note that the SDE uses h).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations in HTML", "md": "# Math Equations in HTML"}, {"type": "text", "value": "By solving a Stochastic Differential Equation (SDE) that depends on h (Song et al., 2021b). Specifically, one starts by sampling x1 from some fixed distribution and then runs the following SDE backwards in time:\n\n$$\ndxt = -g(t)^2 h(xt, t) - \\sigma^2 t xt dt + g(t)dBt, \\quad (3)\n$$\nwhere Bt is a reverse-time Brownian motion and \\(g(t)^2 = \\frac{d\\sigma^2}{dt}\\). To explain how Eq. (3) was derived, consider the forward SDE that starts with a clean image x0 and slowly injects noise:\n\n$$\ndxt = g(t)dBt, \\quad x0 \\sim p0. \\quad (4)\n$$\nWe notice here that the xt under Eq. (4) is \\(N(x0, \\sigma^2 t I_d)\\), where \\(x0 \\sim p0\\), so it has the same distribution that it has in Eq. (2). Remarkably, such SDEs are reversible in time (Anderson, 1982). Hence, the diffusion process of Eq. (4) can be viewed as a reversed-time diffusion:\n\n$$\ndxt = -g(t)^2\\nabla_x \\log p(xt, t)dt + g(t)dBt, \\quad (5)\n$$\nwhere p(xt, t) is the density of xt at time t. We note that \\(s(x, t) := \\nabla_x \\log p(x, t)\\) is called the score function of xt at time t. Using Tweedie\u2019s lemma (Efron, 2011), one obtains the following relationship between the denoising function h and the score function:\n\nSubstituting Eq. (6) in Eq. (5), one obtains Eq. (3).\n\n$$\n\\nabla_x \\log p(x, t) = h(x, t) - \\sigma^2 t x. \\quad (6)\n$$\nTraining via denoising score matching. The standard way to train for h is via denoising score matching. This is performed by obtaining samples of \\(x0 \\sim p0\\) and \\(xt \\sim N(x0, \\sigma^2 t I_d)\\) and training to minimize\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x0 \\sim p0, xt \\sim N(x0, \\sigma^2 t I_d)} L1(t, xt, x0)(\\theta) \\\\\n&= \\mathbb{E}_{x0 \\sim p0, xt \\sim N(x0, \\sigma^2 t I_d)} \\|h_{\\theta}(xt, t) - x0\\|^2,\n\\end{align*}\n$$\nwhere the optimization is over some family of functions, \\(\\{h_{\\theta}\\}_{\\theta \\in \\Theta}\\). It was shown by Vincent (2011) that optimizing Eq. (2) is equivalent to optimizing h in mean-squared-error on a random point xt that is a noisy image, \\(xt \\sim N(x0, \\sigma^2 t I_d)\\) where \\(x0 \\sim p0\\):\n\n$$\n\\mathbb{E}_{xt} \\|h_{\\theta}(xt, t) - h^*(xt, t)\\|^2,\n$$\nwhere \\(h^*\\) is the true denoising function from Eq. (2).", "md": "By solving a Stochastic Differential Equation (SDE) that depends on h (Song et al., 2021b). Specifically, one starts by sampling x1 from some fixed distribution and then runs the following SDE backwards in time:\n\n$$\ndxt = -g(t)^2 h(xt, t) - \\sigma^2 t xt dt + g(t)dBt, \\quad (3)\n$$\nwhere Bt is a reverse-time Brownian motion and \\(g(t)^2 = \\frac{d\\sigma^2}{dt}\\). To explain how Eq. (3) was derived, consider the forward SDE that starts with a clean image x0 and slowly injects noise:\n\n$$\ndxt = g(t)dBt, \\quad x0 \\sim p0. \\quad (4)\n$$\nWe notice here that the xt under Eq. (4) is \\(N(x0, \\sigma^2 t I_d)\\), where \\(x0 \\sim p0\\), so it has the same distribution that it has in Eq. (2). Remarkably, such SDEs are reversible in time (Anderson, 1982). Hence, the diffusion process of Eq. (4) can be viewed as a reversed-time diffusion:\n\n$$\ndxt = -g(t)^2\\nabla_x \\log p(xt, t)dt + g(t)dBt, \\quad (5)\n$$\nwhere p(xt, t) is the density of xt at time t. We note that \\(s(x, t) := \\nabla_x \\log p(x, t)\\) is called the score function of xt at time t. Using Tweedie\u2019s lemma (Efron, 2011), one obtains the following relationship between the denoising function h and the score function:\n\nSubstituting Eq. (6) in Eq. (5), one obtains Eq. (3).\n\n$$\n\\nabla_x \\log p(x, t) = h(x, t) - \\sigma^2 t x. \\quad (6)\n$$\nTraining via denoising score matching. The standard way to train for h is via denoising score matching. This is performed by obtaining samples of \\(x0 \\sim p0\\) and \\(xt \\sim N(x0, \\sigma^2 t I_d)\\) and training to minimize\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x0 \\sim p0, xt \\sim N(x0, \\sigma^2 t I_d)} L1(t, xt, x0)(\\theta) \\\\\n&= \\mathbb{E}_{x0 \\sim p0, xt \\sim N(x0, \\sigma^2 t I_d)} \\|h_{\\theta}(xt, t) - x0\\|^2,\n\\end{align*}\n$$\nwhere the optimization is over some family of functions, \\(\\{h_{\\theta}\\}_{\\theta \\in \\Theta}\\). It was shown by Vincent (2011) that optimizing Eq. (2) is equivalent to optimizing h in mean-squared-error on a random point xt that is a noisy image, \\(xt \\sim N(x0, \\sigma^2 t I_d)\\) where \\(x0 \\sim p0\\):\n\n$$\n\\mathbb{E}_{xt} \\|h_{\\theta}(xt, t) - h^*(xt, t)\\|^2,\n$$\nwhere \\(h^*\\) is the true denoising function from Eq. (2)."}, {"type": "heading", "lvl": 3, "value": "Theory", "md": "### Theory"}, {"type": "text", "value": "We define below the consistency property that a function h should satisfy. This states that the output of h(x, t) (which is meant to approximate the conditional expectation of x0 conditioned on xt = x) is indeed consistent with the average point x0 generated using h and conditioning on xt = x. Recall from the previous section that generation according to h conditioning on xt = x is done by running the following SDE backwards in time conditioning on xt = x:\n\n$$\ndxt = -g(t)^2 h(xt, t) - \\sigma^2 t xt dt + g(t)^2 dBt, \\quad (7)\n$$\nThe consistency property is therefore defined as follows:\n\nProperty 1 (Consistency). A function \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\) is said to be consistent iff for all \\(t \\in (0, 1]\\) and all \\(x \\in \\mathbb{R}^d\\),\n\n$$\nh(x, t) = \\mathbb{E}[x0 | xt = x], \\quad (8)\n$$\nwhere \\(\\mathbb{E}[x0 | xt = x]\\) corresponds to the conditional expectation of x0 in the process that starts with xt = x and samples x0 by running the SDE of Eq. (7) backwards in time (where note that the SDE uses h).", "md": "We define below the consistency property that a function h should satisfy. This states that the output of h(x, t) (which is meant to approximate the conditional expectation of x0 conditioned on xt = x) is indeed consistent with the average point x0 generated using h and conditioning on xt = x. Recall from the previous section that generation according to h conditioning on xt = x is done by running the following SDE backwards in time conditioning on xt = x:\n\n$$\ndxt = -g(t)^2 h(xt, t) - \\sigma^2 t xt dt + g(t)^2 dBt, \\quad (7)\n$$\nThe consistency property is therefore defined as follows:\n\nProperty 1 (Consistency). A function \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\) is said to be consistent iff for all \\(t \\in (0, 1]\\) and all \\(x \\in \\mathbb{R}^d\\),\n\n$$\nh(x, t) = \\mathbb{E}[x0 | xt = x], \\quad (8)\n$$\nwhere \\(\\mathbb{E}[x0 | xt = x]\\) corresponds to the conditional expectation of x0 in the process that starts with xt = x and samples x0 by running the SDE of Eq. (7) backwards in time (where note that the SDE uses h)."}]}, {"page": 5, "text": "      The following Lemma states that Property 1 holds if and only if the model prediction, h(x, t), is consistent\nwith the average output of h on samples that are generated using h and conditioning on xt = x, i.e. that\n h(xt, t) is a reverse-Martingale under the same process of Eq. (7).\n Lemma 3.1. Property 1 holds if and only if the following two properties hold:\n      \u2022 The function h is a reverse-Martingale, namely: for all t > t\u2032 and for any x:\n                                                                  h(x, t) = Eh[h(xt\u2032, t\u2032) | xt = x],\n         where the expectation is over xt\u2032 that is sampled according to Eq. (7) with the same function h, given\n         the initial condition xt = x.\n      \u2022 For all x \u2208         Rd, h(x, 0) = x.\n      The proof of this Lemma is included in the Appendix. Further, we introduce one more property that will\n be required for our theoretical results: the learned vector-field should be conservative.\nProperty 2 (Conservative vector field / Score Property.). Let h: Rd \u00d7 [0, 1] \u2192                                                             Rd. We say that h\n induces a conservative vector field (or that is satisfies the score property) if for any t \u2208                                                  (0, 1] there exists\n some probability density p(\u00b7, t) such that                      h(x, t) \u2212     x\n                                                                       \u03c32t         = \u2207    log p(x, t).\n      We note that the optimal denoiser, i.e. h defined as in Eq. (2) satisfies both of the properties we introduced.\n In the paper, we will focus on enforcing the consistency property and we are going to assume conservativeness\n for our theoretical results. This assumption can be relieved to hold only at a single t \u2208                                                (0, 1] using results of\n Lai et al. (2022).\n      Next, we show the theoretical consequences of enforcing Properties 1 and 2. First, we show that this\n enforces h to indeed correspond to a denoising function, namely, h satisfies Eq. (2) for some distribution p\u2032                                                           0\n over x0. Yet, this does not imply that p0 is the correct underlying distribution that we are trying to learn.\n Indeed, these properties can apply to any distribution p0. Yet, we can show that if we learn h correctly for\n some inputs and if these properties apply everywhere then h is learned correctly everywhere.\nTheorem 3.2. Let h: Rd \u00d7 [0, 1] \u2192                          Rd be a continuous function. Then:\n     1. The function h satisfies both Properties 1 and 2 if and only if h is defined by Eq. (2) for some distribution\n          p0.\n     2. Assume that h satisfies Properties 1 and 2. Further, let h\u2217                                        be another function that corresponds to\n         Eq. (2) with some initial distribution p\u2217                    0. Assume that h = h\u2217               on some open set U \u2286                Rd and some fixed\n          t0 \u2208   (0, 1], namely, h(x, t0) = h\u2217(x, t0) for all x \u2208                        U. Then, h\u2217(x, t) = h(x, t) for all x and all t.\n Proof overview. We start with the first part of the theorem. We assume that h satisfies Properties 1 and\n 2 and we will show that h is defined by Eq. (2) for some distribution p0 (while the other direction in the\n equivalence follows trivially from the definitions of these properties). Motivated by Eq. (6), define the function\n s: Rd \u00d7 (0, 1] according to                                         s(x, t) = h(x, t) \u2212  \u03c32t     x.                                                                  (9)\nWe will first show that s satisfies the partial differential equation\n                                                                 \u2202s                  Jss + 1             ,                                                          (10)\n                                                                 \u2202t = g(t)2                    2\u25b3s\n                                                                                    5", "md": "The following Lemma states that Property 1 holds if and only if the model prediction, \\(h(x, t)\\), is consistent with the average output of \\(h\\) on samples that are generated using \\(h\\) and conditioning on \\(x_t = x\\), i.e. that \\(h(x_t, t)\\) is a reverse-Martingale under the same process of Eq. (7).\n\nLemma 3.1. Property 1 holds if and only if the following two properties hold:\n- The function \\(h\\) is a reverse-Martingale, namely: for all \\(t > t'\\) and for any \\(x\\):\n$$h(x, t) = E[h(x_{t'}, t') | x_t = x],$$\nwhere the expectation is over \\(x_{t'}\\) that is sampled according to Eq. (7) with the same function \\(h\\), given the initial condition \\(x_t = x\\).\n- For all \\(x \\in \\mathbb{R}^d\\), \\(h(x, 0) = x\\).\n\nThe proof of this Lemma is included in the Appendix. Further, we introduce one more property that will be required for our theoretical results: the learned vector-field should be conservative.\n\nProperty 2 (Conservative vector field / Score Property.). Let \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\). We say that \\(h\\) induces a conservative vector field (or that it satisfies the score property) if for any \\(t \\in (0, 1]\\) there exists some probability density \\(p(\\cdot, t)\\) such that\n$$\\frac{h(x, t) - x}{\\sigma^2 t} = \\nabla \\log p(x, t).$$\n\nWe note that the optimal denoiser, i.e. \\(h\\) defined as in Eq. (2) satisfies both of the properties we introduced. In the paper, we will focus on enforcing the consistency property and we are going to assume conservativeness for our theoretical results. This assumption can be relieved to hold only at a single \\(t \\in (0, 1]\\) using results of Lai et al. (2022).\n\nNext, we show the theoretical consequences of enforcing Properties 1 and 2. First, we show that this enforces \\(h\\) to indeed correspond to a denoising function, namely, \\(h\\) satisfies Eq. (2) for some distribution \\(p'_0\\) over \\(x_0\\). Yet, this does not imply that \\(p_0\\) is the correct underlying distribution that we are trying to learn. Indeed, these properties can apply to any distribution \\(p_0\\). Yet, we can show that if we learn \\(h\\) correctly for some inputs and if these properties apply everywhere then \\(h\\) is learned correctly everywhere.\n\nTheorem 3.2. Let \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\) be a continuous function. Then:\n1. The function \\(h\\) satisfies both Properties 1 and 2 if and only if \\(h\\) is defined by Eq. (2) for some distribution \\(p_0\\).\n2. Assume that \\(h\\) satisfies Properties 1 and 2. Further, let \\(h^*\\) be another function that corresponds to Eq. (2) with some initial distribution \\(p^*_0\\). Assume that \\(h = h^*\\) on some open set \\(U \\subseteq \\mathbb{R}^d\\) and some fixed \\(t_0 \\in (0, 1]\\), namely, \\(h(x, t_0) = h^*(x, t_0)\\) for all \\(x \\in U\\). Then, \\(h^*(x, t) = h(x, t)\\) for all \\(x\\) and all \\(t\\).\n\nProof overview. We start with the first part of the theorem. We assume that \\(h\\) satisfies Properties 1 and 2 and we will show that \\(h\\) is defined by Eq. (2) for some distribution \\(p_0\\) (while the other direction in the equivalence follows trivially from the definitions of these properties). Motivated by Eq. (6), define the function \\(s: \\mathbb{R}^d \\times (0, 1]\\) according to\n$$s(x, t) = h(x, t) - \\frac{\\sigma^2 t}{x}.$$\n\nWe will first show that \\(s\\) satisfies the partial differential equation\n$$\\frac{\\partial s}{\\partial t} = g(t)^2 Jss + \\frac{1}{2} \\nabla^2 s.$$", "images": [], "items": [{"type": "text", "value": "The following Lemma states that Property 1 holds if and only if the model prediction, \\(h(x, t)\\), is consistent with the average output of \\(h\\) on samples that are generated using \\(h\\) and conditioning on \\(x_t = x\\), i.e. that \\(h(x_t, t)\\) is a reverse-Martingale under the same process of Eq. (7).\n\nLemma 3.1. Property 1 holds if and only if the following two properties hold:\n- The function \\(h\\) is a reverse-Martingale, namely: for all \\(t > t'\\) and for any \\(x\\):\n$$h(x, t) = E[h(x_{t'}, t') | x_t = x],$$\nwhere the expectation is over \\(x_{t'}\\) that is sampled according to Eq. (7) with the same function \\(h\\), given the initial condition \\(x_t = x\\).\n- For all \\(x \\in \\mathbb{R}^d\\), \\(h(x, 0) = x\\).\n\nThe proof of this Lemma is included in the Appendix. Further, we introduce one more property that will be required for our theoretical results: the learned vector-field should be conservative.\n\nProperty 2 (Conservative vector field / Score Property.). Let \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\). We say that \\(h\\) induces a conservative vector field (or that it satisfies the score property) if for any \\(t \\in (0, 1]\\) there exists some probability density \\(p(\\cdot, t)\\) such that\n$$\\frac{h(x, t) - x}{\\sigma^2 t} = \\nabla \\log p(x, t).$$\n\nWe note that the optimal denoiser, i.e. \\(h\\) defined as in Eq. (2) satisfies both of the properties we introduced. In the paper, we will focus on enforcing the consistency property and we are going to assume conservativeness for our theoretical results. This assumption can be relieved to hold only at a single \\(t \\in (0, 1]\\) using results of Lai et al. (2022).\n\nNext, we show the theoretical consequences of enforcing Properties 1 and 2. First, we show that this enforces \\(h\\) to indeed correspond to a denoising function, namely, \\(h\\) satisfies Eq. (2) for some distribution \\(p'_0\\) over \\(x_0\\). Yet, this does not imply that \\(p_0\\) is the correct underlying distribution that we are trying to learn. Indeed, these properties can apply to any distribution \\(p_0\\). Yet, we can show that if we learn \\(h\\) correctly for some inputs and if these properties apply everywhere then \\(h\\) is learned correctly everywhere.\n\nTheorem 3.2. Let \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\) be a continuous function. Then:\n1. The function \\(h\\) satisfies both Properties 1 and 2 if and only if \\(h\\) is defined by Eq. (2) for some distribution \\(p_0\\).\n2. Assume that \\(h\\) satisfies Properties 1 and 2. Further, let \\(h^*\\) be another function that corresponds to Eq. (2) with some initial distribution \\(p^*_0\\). Assume that \\(h = h^*\\) on some open set \\(U \\subseteq \\mathbb{R}^d\\) and some fixed \\(t_0 \\in (0, 1]\\), namely, \\(h(x, t_0) = h^*(x, t_0)\\) for all \\(x \\in U\\). Then, \\(h^*(x, t) = h(x, t)\\) for all \\(x\\) and all \\(t\\).\n\nProof overview. We start with the first part of the theorem. We assume that \\(h\\) satisfies Properties 1 and 2 and we will show that \\(h\\) is defined by Eq. (2) for some distribution \\(p_0\\) (while the other direction in the equivalence follows trivially from the definitions of these properties). Motivated by Eq. (6), define the function \\(s: \\mathbb{R}^d \\times (0, 1]\\) according to\n$$s(x, t) = h(x, t) - \\frac{\\sigma^2 t}{x}.$$\n\nWe will first show that \\(s\\) satisfies the partial differential equation\n$$\\frac{\\partial s}{\\partial t} = g(t)^2 Jss + \\frac{1}{2} \\nabla^2 s.$$", "md": "The following Lemma states that Property 1 holds if and only if the model prediction, \\(h(x, t)\\), is consistent with the average output of \\(h\\) on samples that are generated using \\(h\\) and conditioning on \\(x_t = x\\), i.e. that \\(h(x_t, t)\\) is a reverse-Martingale under the same process of Eq. (7).\n\nLemma 3.1. Property 1 holds if and only if the following two properties hold:\n- The function \\(h\\) is a reverse-Martingale, namely: for all \\(t > t'\\) and for any \\(x\\):\n$$h(x, t) = E[h(x_{t'}, t') | x_t = x],$$\nwhere the expectation is over \\(x_{t'}\\) that is sampled according to Eq. (7) with the same function \\(h\\), given the initial condition \\(x_t = x\\).\n- For all \\(x \\in \\mathbb{R}^d\\), \\(h(x, 0) = x\\).\n\nThe proof of this Lemma is included in the Appendix. Further, we introduce one more property that will be required for our theoretical results: the learned vector-field should be conservative.\n\nProperty 2 (Conservative vector field / Score Property.). Let \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\). We say that \\(h\\) induces a conservative vector field (or that it satisfies the score property) if for any \\(t \\in (0, 1]\\) there exists some probability density \\(p(\\cdot, t)\\) such that\n$$\\frac{h(x, t) - x}{\\sigma^2 t} = \\nabla \\log p(x, t).$$\n\nWe note that the optimal denoiser, i.e. \\(h\\) defined as in Eq. (2) satisfies both of the properties we introduced. In the paper, we will focus on enforcing the consistency property and we are going to assume conservativeness for our theoretical results. This assumption can be relieved to hold only at a single \\(t \\in (0, 1]\\) using results of Lai et al. (2022).\n\nNext, we show the theoretical consequences of enforcing Properties 1 and 2. First, we show that this enforces \\(h\\) to indeed correspond to a denoising function, namely, \\(h\\) satisfies Eq. (2) for some distribution \\(p'_0\\) over \\(x_0\\). Yet, this does not imply that \\(p_0\\) is the correct underlying distribution that we are trying to learn. Indeed, these properties can apply to any distribution \\(p_0\\). Yet, we can show that if we learn \\(h\\) correctly for some inputs and if these properties apply everywhere then \\(h\\) is learned correctly everywhere.\n\nTheorem 3.2. Let \\(h: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d\\) be a continuous function. Then:\n1. The function \\(h\\) satisfies both Properties 1 and 2 if and only if \\(h\\) is defined by Eq. (2) for some distribution \\(p_0\\).\n2. Assume that \\(h\\) satisfies Properties 1 and 2. Further, let \\(h^*\\) be another function that corresponds to Eq. (2) with some initial distribution \\(p^*_0\\). Assume that \\(h = h^*\\) on some open set \\(U \\subseteq \\mathbb{R}^d\\) and some fixed \\(t_0 \\in (0, 1]\\), namely, \\(h(x, t_0) = h^*(x, t_0)\\) for all \\(x \\in U\\). Then, \\(h^*(x, t) = h(x, t)\\) for all \\(x\\) and all \\(t\\).\n\nProof overview. We start with the first part of the theorem. We assume that \\(h\\) satisfies Properties 1 and 2 and we will show that \\(h\\) is defined by Eq. (2) for some distribution \\(p_0\\) (while the other direction in the equivalence follows trivially from the definitions of these properties). Motivated by Eq. (6), define the function \\(s: \\mathbb{R}^d \\times (0, 1]\\) according to\n$$s(x, t) = h(x, t) - \\frac{\\sigma^2 t}{x}.$$\n\nWe will first show that \\(s\\) satisfies the partial differential equation\n$$\\frac{\\partial s}{\\partial t} = g(t)^2 Jss + \\frac{1}{2} \\nabla^2 s.$$"}]}, {"page": 6, "text": "where Js \u2208          Rd\u00d7d is the Jacobian of s, (Js)ij = \u2202si                   xj and each coordinate i of \u25b3s \u2208                        Rd is the Laplacian of\n coordinate i of s, (\u25b3s)i =  n                         \u22022 si\n                                                j=1    \u2202x2j . In order to obtain Eq. (10), first, we use a generalization of Ito\u2019s\n lemma, which states that for an SDE\n                                                                dxt = \u00b5(xt, t)dt + g(t)dBtx                                                                    (11)\n and for f : Rd \u00d7 [0, 1] \u2192             Rd, f(xt, t) satisfies the SDE\n                                             df(xt, t) =       \u2202f\u2202t + Jf\u00b5 \u2212          g(t)2   \u25b3f       dt + g(t)JfdBt.\n                                                                                        2\n If f is a reverse-Martingale then the term that multiplies dt has to equal zero, namely,\n                                                                 \u2202f\n                                                                  \u2202t + Jf\u00b5 \u2212         g(t)2   \u25b3f = 0.\n                                                                                        2\n By Lemma 3.1, h(xt, t) is a reverse Martingale, therefore we can substitute f = h and substitute \u00b5 = \u2212g(t)2s\n according to Eq. (7), to deduce that\n                                                              \u2202h\n                                                              \u2202t \u2212     g(t)2Jhs \u2212        g(t)2   \u25b3h = 0.\n                                                                                            2\n Substituting h(x, t) = \u03c32            t s(x, t) + x according to Eq. (6) yields Eq. (10) as required.\n      Next, we show that any s\u2032 that is the score-function (i.e. gradient of log probability) of some diffusion\n process that follows the SDE Eq. (4), also satisfies Eq. (10). To obtain this, one can use the Fokker-Planck\n equation, whose special case states that the density function p(x, t) of any stochastic process that satisfies\n the SDE Eq. (4) satisfies the PDE\n                                                                          \u2202p\n                                                                           \u2202t = g(t)2  2    \u25b3p\nwhere \u25b3        corresponds to the Laplacian operator. Using this one can obtain a PDE for \u2207x log p which happens\n to be exactly Eq. (10) if the process is defined by Eq. (4).\n      Next, we use Property 2 to deduce that there exists some densities p(\u00b7, t) for t \u2208                                              [0, 1] such that\n                                                         s(x, t) = h(x, t) \u2212  \u03c32t     x  = \u2207x log p(x, t).\n Denote by p\u2032(x, t) the score function of the diffusion process that is defined by the SDE of Eq. (4) with the\n initial condition that p(x, 0) = p\u2032(x, 0) for all x. Denote by s\u2032(x, t) = \u2207x log p\u2032(x, t) the score function of p\u2032.\nAs we proved above, both s and s\u2032 satisfy the PDE Eq. (10) and the same initial condition at t = 0. By the\n uniqueness of the PDE, it holds that s(x, t) = s\u2032(x, t) for all t \u2265                                  t0. Denote by h\u2217            the function that satisfies\n Eq. (2) with the initial condition x0 \u223c                      p0. By Eq. (6),\n                                                                    s\u2032(x, t) = h\u2217(x, t) \u2212  \u03c32t     x.\n By Eq. (9) and since s = s\u2032, it follows that h = h\u2217                             and this is what we wanted to prove.\n      We proceed with proving part 2 of the theorem. We use the notion of an analytic function on Rd: that is\n a function f : Rd \u2192            R such that at any x0 \u2208               Rd, the Taylor series of f centered at x0 converges for all x \u2208                            Rd\n to f(x). We use the property that an analytic function is uniquely determined by its value on any open subset:\nIf f and g are analytic functions that identify in some open subset U \u2282                                          Rd then f = g everywhere. We prove\n this statement in the remainder of this paragraph, as follows: Represent f and g as Taylor series around\n some x0 \u2208        U. The Taylor series of f and g identify: indeed, these series are functions of the derivatives of f\n and g which are functions of only the values in U. Since f and g equal their Taylor series, they are equal.\n                                                                                    6", "md": "where \\( J_s \\in \\mathbb{R}^{d \\times d} \\) is the Jacobian of \\( s \\), \\( (J_s)_{ij} = \\frac{\\partial s_i}{\\partial x_j} \\) and each coordinate \\( i \\) of \\( \\nabla s \\in \\mathbb{R}^d \\) is the Laplacian of coordinate \\( i \\) of \\( s \\), \\( (\\nabla s)_i = \\sum_{j=1}^{n} \\frac{\\partial^2 s_i}{\\partial x_{2j}} \\). In order to obtain Eq. (10), first, we use a generalization of Ito's lemma, which states that for an SDE\n\\[ dx_t = \\mu(x_t, t) dt + g(t) dB_t \\quad (11) \\]\nand for \\( f : \\mathbb{R}^d \\times [0, 1] \\rightarrow \\mathbb{R} \\), \\( f(x_t, t) \\) satisfies the SDE\n\\[ df(x_t, t) = \\frac{\\partial f}{\\partial t} + Jf\\mu - \\frac{g(t)^2}{2} \\nabla f dt + g(t) Jf dB_t. \\]\nIf \\( f \\) is a reverse-Martingale then the term that multiplies \\( dt \\) has to equal zero, namely,\n\\[ \\frac{\\partial f}{\\partial t} + Jf\\mu - \\frac{g(t)^2}{2} \\nabla f = 0. \\]\nBy Lemma 3.1, \\( h(x_t, t) \\) is a reverse Martingale, therefore we can substitute \\( f = h \\) and substitute \\( \\mu = -g(t)^2s \\) according to Eq. (7), to deduce that\n\\[ \\frac{\\partial h}{\\partial t} - g(t)^2 Jh s - \\frac{g(t)^2}{2} \\nabla h = 0. \\]\nSubstituting \\( h(x, t) = \\sigma^2 t s(x, t) + x \\) according to Eq. (6) yields Eq. (10) as required.\n\nNext, we show that any \\( s' \\) that is the score-function (i.e. gradient of log probability) of some diffusion process that follows the SDE Eq. (4), also satisfies Eq. (10). To obtain this, one can use the Fokker-Planck equation, whose special case states that the density function \\( p(x, t) \\) of any stochastic process that satisfies the SDE Eq. (4) satisfies the PDE\n\\[ \\frac{\\partial p}{\\partial t} = g(t)^2 \\nabla^2 p \\]\nwhere \\( \\nabla^2 \\) corresponds to the Laplacian operator. Using this one can obtain a PDE for \\( \\nabla x \\log p \\) which happens to be exactly Eq. (10) if the process is defined by Eq. (4).\n\nNext, we use Property 2 to deduce that there exists some densities \\( p(\\cdot, t) \\) for \\( t \\in [0, 1] \\) such that\n\\[ s(x, t) = h(x, t) - \\sigma^2 t x = \\nabla x \\log p(x, t). \\]\nDenote by \\( p'(x, t) \\) the score function of the diffusion process that is defined by the SDE of Eq. (4) with the initial condition that \\( p(x, 0) = p'(x, 0) \\) for all \\( x \\). Denote by \\( s'(x, t) = \\nabla x \\log p'(x, t) \\) the score function of \\( p' \\).\n\nAs we proved above, both \\( s \\) and \\( s' \\) satisfy the PDE Eq. (10) and the same initial condition at \\( t = 0 \\). By the uniqueness of the PDE, it holds that \\( s(x, t) = s'(x, t) \\) for all \\( t \\geq t_0 \\). Denote by \\( h^* \\) the function that satisfies Eq. (2) with the initial condition \\( x_0 \\sim p_0 \\). By Eq. (6),\n\\[ s'(x, t) = h^*(x, t) - \\sigma^2 t x. \\]\nBy Eq. (9) and since \\( s = s' \\), it follows that \\( h = h^* \\) and this is what we wanted to prove.\n\nWe proceed with proving part 2 of the theorem. We use the notion of an analytic function on \\( \\mathbb{R}^d \\): that is a function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) such that at any \\( x_0 \\in \\mathbb{R}^d \\), the Taylor series of \\( f \\) centered at \\( x_0 \\) converges for all \\( x \\in \\mathbb{R}^d \\) to \\( f(x) \\). We use the property that an analytic function is uniquely determined by its value on any open subset: If \\( f \\) and \\( g \\) are analytic functions that identify in some open subset \\( U \\subset \\mathbb{R}^d \\) then \\( f = g \\) everywhere. We prove this statement in the remainder of this paragraph, as follows: Represent \\( f \\) and \\( g \\) as Taylor series around some \\( x_0 \\in U \\). The Taylor series of \\( f \\) and \\( g \\) identify: indeed, these series are functions of the derivatives of \\( f \\) and \\( g \\) which are functions of only the values in \\( U \\). Since \\( f \\) and \\( g \\) equal their Taylor series, they are equal.", "images": [], "items": [{"type": "text", "value": "where \\( J_s \\in \\mathbb{R}^{d \\times d} \\) is the Jacobian of \\( s \\), \\( (J_s)_{ij} = \\frac{\\partial s_i}{\\partial x_j} \\) and each coordinate \\( i \\) of \\( \\nabla s \\in \\mathbb{R}^d \\) is the Laplacian of coordinate \\( i \\) of \\( s \\), \\( (\\nabla s)_i = \\sum_{j=1}^{n} \\frac{\\partial^2 s_i}{\\partial x_{2j}} \\). In order to obtain Eq. (10), first, we use a generalization of Ito's lemma, which states that for an SDE\n\\[ dx_t = \\mu(x_t, t) dt + g(t) dB_t \\quad (11) \\]\nand for \\( f : \\mathbb{R}^d \\times [0, 1] \\rightarrow \\mathbb{R} \\), \\( f(x_t, t) \\) satisfies the SDE\n\\[ df(x_t, t) = \\frac{\\partial f}{\\partial t} + Jf\\mu - \\frac{g(t)^2}{2} \\nabla f dt + g(t) Jf dB_t. \\]\nIf \\( f \\) is a reverse-Martingale then the term that multiplies \\( dt \\) has to equal zero, namely,\n\\[ \\frac{\\partial f}{\\partial t} + Jf\\mu - \\frac{g(t)^2}{2} \\nabla f = 0. \\]\nBy Lemma 3.1, \\( h(x_t, t) \\) is a reverse Martingale, therefore we can substitute \\( f = h \\) and substitute \\( \\mu = -g(t)^2s \\) according to Eq. (7), to deduce that\n\\[ \\frac{\\partial h}{\\partial t} - g(t)^2 Jh s - \\frac{g(t)^2}{2} \\nabla h = 0. \\]\nSubstituting \\( h(x, t) = \\sigma^2 t s(x, t) + x \\) according to Eq. (6) yields Eq. (10) as required.\n\nNext, we show that any \\( s' \\) that is the score-function (i.e. gradient of log probability) of some diffusion process that follows the SDE Eq. (4), also satisfies Eq. (10). To obtain this, one can use the Fokker-Planck equation, whose special case states that the density function \\( p(x, t) \\) of any stochastic process that satisfies the SDE Eq. (4) satisfies the PDE\n\\[ \\frac{\\partial p}{\\partial t} = g(t)^2 \\nabla^2 p \\]\nwhere \\( \\nabla^2 \\) corresponds to the Laplacian operator. Using this one can obtain a PDE for \\( \\nabla x \\log p \\) which happens to be exactly Eq. (10) if the process is defined by Eq. (4).\n\nNext, we use Property 2 to deduce that there exists some densities \\( p(\\cdot, t) \\) for \\( t \\in [0, 1] \\) such that\n\\[ s(x, t) = h(x, t) - \\sigma^2 t x = \\nabla x \\log p(x, t). \\]\nDenote by \\( p'(x, t) \\) the score function of the diffusion process that is defined by the SDE of Eq. (4) with the initial condition that \\( p(x, 0) = p'(x, 0) \\) for all \\( x \\). Denote by \\( s'(x, t) = \\nabla x \\log p'(x, t) \\) the score function of \\( p' \\).\n\nAs we proved above, both \\( s \\) and \\( s' \\) satisfy the PDE Eq. (10) and the same initial condition at \\( t = 0 \\). By the uniqueness of the PDE, it holds that \\( s(x, t) = s'(x, t) \\) for all \\( t \\geq t_0 \\). Denote by \\( h^* \\) the function that satisfies Eq. (2) with the initial condition \\( x_0 \\sim p_0 \\). By Eq. (6),\n\\[ s'(x, t) = h^*(x, t) - \\sigma^2 t x. \\]\nBy Eq. (9) and since \\( s = s' \\), it follows that \\( h = h^* \\) and this is what we wanted to prove.\n\nWe proceed with proving part 2 of the theorem. We use the notion of an analytic function on \\( \\mathbb{R}^d \\): that is a function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) such that at any \\( x_0 \\in \\mathbb{R}^d \\), the Taylor series of \\( f \\) centered at \\( x_0 \\) converges for all \\( x \\in \\mathbb{R}^d \\) to \\( f(x) \\). We use the property that an analytic function is uniquely determined by its value on any open subset: If \\( f \\) and \\( g \\) are analytic functions that identify in some open subset \\( U \\subset \\mathbb{R}^d \\) then \\( f = g \\) everywhere. We prove this statement in the remainder of this paragraph, as follows: Represent \\( f \\) and \\( g \\) as Taylor series around some \\( x_0 \\in U \\). The Taylor series of \\( f \\) and \\( g \\) identify: indeed, these series are functions of the derivatives of \\( f \\) and \\( g \\) which are functions of only the values in \\( U \\). Since \\( f \\) and \\( g \\) equal their Taylor series, they are equal.", "md": "where \\( J_s \\in \\mathbb{R}^{d \\times d} \\) is the Jacobian of \\( s \\), \\( (J_s)_{ij} = \\frac{\\partial s_i}{\\partial x_j} \\) and each coordinate \\( i \\) of \\( \\nabla s \\in \\mathbb{R}^d \\) is the Laplacian of coordinate \\( i \\) of \\( s \\), \\( (\\nabla s)_i = \\sum_{j=1}^{n} \\frac{\\partial^2 s_i}{\\partial x_{2j}} \\). In order to obtain Eq. (10), first, we use a generalization of Ito's lemma, which states that for an SDE\n\\[ dx_t = \\mu(x_t, t) dt + g(t) dB_t \\quad (11) \\]\nand for \\( f : \\mathbb{R}^d \\times [0, 1] \\rightarrow \\mathbb{R} \\), \\( f(x_t, t) \\) satisfies the SDE\n\\[ df(x_t, t) = \\frac{\\partial f}{\\partial t} + Jf\\mu - \\frac{g(t)^2}{2} \\nabla f dt + g(t) Jf dB_t. \\]\nIf \\( f \\) is a reverse-Martingale then the term that multiplies \\( dt \\) has to equal zero, namely,\n\\[ \\frac{\\partial f}{\\partial t} + Jf\\mu - \\frac{g(t)^2}{2} \\nabla f = 0. \\]\nBy Lemma 3.1, \\( h(x_t, t) \\) is a reverse Martingale, therefore we can substitute \\( f = h \\) and substitute \\( \\mu = -g(t)^2s \\) according to Eq. (7), to deduce that\n\\[ \\frac{\\partial h}{\\partial t} - g(t)^2 Jh s - \\frac{g(t)^2}{2} \\nabla h = 0. \\]\nSubstituting \\( h(x, t) = \\sigma^2 t s(x, t) + x \\) according to Eq. (6) yields Eq. (10) as required.\n\nNext, we show that any \\( s' \\) that is the score-function (i.e. gradient of log probability) of some diffusion process that follows the SDE Eq. (4), also satisfies Eq. (10). To obtain this, one can use the Fokker-Planck equation, whose special case states that the density function \\( p(x, t) \\) of any stochastic process that satisfies the SDE Eq. (4) satisfies the PDE\n\\[ \\frac{\\partial p}{\\partial t} = g(t)^2 \\nabla^2 p \\]\nwhere \\( \\nabla^2 \\) corresponds to the Laplacian operator. Using this one can obtain a PDE for \\( \\nabla x \\log p \\) which happens to be exactly Eq. (10) if the process is defined by Eq. (4).\n\nNext, we use Property 2 to deduce that there exists some densities \\( p(\\cdot, t) \\) for \\( t \\in [0, 1] \\) such that\n\\[ s(x, t) = h(x, t) - \\sigma^2 t x = \\nabla x \\log p(x, t). \\]\nDenote by \\( p'(x, t) \\) the score function of the diffusion process that is defined by the SDE of Eq. (4) with the initial condition that \\( p(x, 0) = p'(x, 0) \\) for all \\( x \\). Denote by \\( s'(x, t) = \\nabla x \\log p'(x, t) \\) the score function of \\( p' \\).\n\nAs we proved above, both \\( s \\) and \\( s' \\) satisfy the PDE Eq. (10) and the same initial condition at \\( t = 0 \\). By the uniqueness of the PDE, it holds that \\( s(x, t) = s'(x, t) \\) for all \\( t \\geq t_0 \\). Denote by \\( h^* \\) the function that satisfies Eq. (2) with the initial condition \\( x_0 \\sim p_0 \\). By Eq. (6),\n\\[ s'(x, t) = h^*(x, t) - \\sigma^2 t x. \\]\nBy Eq. (9) and since \\( s = s' \\), it follows that \\( h = h^* \\) and this is what we wanted to prove.\n\nWe proceed with proving part 2 of the theorem. We use the notion of an analytic function on \\( \\mathbb{R}^d \\): that is a function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) such that at any \\( x_0 \\in \\mathbb{R}^d \\), the Taylor series of \\( f \\) centered at \\( x_0 \\) converges for all \\( x \\in \\mathbb{R}^d \\) to \\( f(x) \\). We use the property that an analytic function is uniquely determined by its value on any open subset: If \\( f \\) and \\( g \\) are analytic functions that identify in some open subset \\( U \\subset \\mathbb{R}^d \\) then \\( f = g \\) everywhere. We prove this statement in the remainder of this paragraph, as follows: Represent \\( f \\) and \\( g \\) as Taylor series around some \\( x_0 \\in U \\). The Taylor series of \\( f \\) and \\( g \\) identify: indeed, these series are functions of the derivatives of \\( f \\) and \\( g \\) which are functions of only the values in \\( U \\). Since \\( f \\) and \\( g \\) equal their Taylor series, they are equal."}]}, {"page": 7, "text": "      Next, we will show that for any diffusion process that is defined by Eq. (4), the probability density of\n p(x, t0) at any time t0 > 0 is analytic as a function of x. Recall that the distribution of x0 is defined in Eq. (4)\n as p0 and it holds that the distribution of xt                       0 is obtained from p0 by adding a Gaussian noise N(0, \u03c32                                t I) and\n its density at any x equals\n                                              p(x, t0) =                \u221a    1       exp      \u2212(x \u2212  2\u03c32 a)2      dp0(a).\n                                                                a\u2208Rd       2\u03c0\u03c3t   0                      t\n Since the function exp(\u2212(x \u2212                   a)2/(2\u03c32    t )) is analytic, one could deduce that p(x, t0) is also analytic. Further,\n p(x, t0) > 0 for all x which implies that there is no singularity for log p(x, t0) which can be used to deduce\n that log p(x, t0) is also analytic and further that \u2207x log p(x, t0) is analytic as well.\n      We use the first part of the theorem to deduce that s is the score function of some diffusion process hence\n it is analytic. By assumption, s identifies with some target score function s\u2217                                          in some open subset U \u2286                  Rd at\n some t0, which, by the fact that s(x, t0) and s\u2217(x, t0) are analytic, implies that s(x, t0) = s\u2217(x, t0) for all x.\n Finally, since s and s\u2217              both satisfy the PDE Eq. (10) and they satsify the same initial condition at t0, it\n holds that by uniqueness of the PDE s(x, t) = s\u2217(x, t) for all x and t.\n 4       Method\nTheorem 3.2 motivates enforcing the consistency property on the learned model. We notice that the consistency\n equation Eq. (8) may be expensive to train for, because it requires one to generate whole trajectories. Rather,\nwe use the equivalent Martingale assumption of Lemma 3.1, which can be observed locally with only partial\n trajectories:2 We suggest the following loss function, for some fixed t, t\u2032 and x:\n                                              L2 t,t\u2032,x(\u03b8) = (E\u03b8[h\u03b8(xt\u2032, t\u2032) | xt = x] \u2212                 h\u03b8(x, t))2 /2,\nwhere the expectation E\u03b8[\u00b7 | xt = x] is taken according to process Eq. (7) parameterized by h\u03b8 with the initial\n condition xt = x. Differentiating this expectation, one gets the following (see Section B.1 for full derivation):\n    \u2207L2   t,t\u2032,x(\u03b8) = E\u03b8 [h\u03b8(xt\u2032, t\u2032) \u2212            h\u03b8(xt, t) | xt = x]\u22a4          E\u03b8   h\u03b8(xt\u2032, t\u2032)\u2207\u03b8 log (p\u03b8(xt\u2032 | xt = x)) +\nwhere p\u03b8 corresponds to the same probability measure where the expectation E\u03b8 is taken from and \u2207\u03b8h\u03b8         \u2207\u03b8h\u03b8(xt\u2032, t\u2032) \u2212        \u2207\u03b8h\u03b8(xt, t)          xt = x       ,\n corresponds to the Jacobian matrix of h\u03b8 where the derivatives are taken with respect to \u03b8. Notice, however,\n that computing the expectation accurately might require a large number of samples. Instead, it is possible\n to obtain a stochastic gradient of this target by taking two samples, xt\u2032 and xt\u2032, independently, from the\n conditional distribution of xt\u2032 conditioned on xt = x and replace each of the two expectations in the formula\n above with one of these two samples.\n      We further notice the gradient of the consistency loss can be written as\n    \u2207\u03b8L2   t,t\u2032,x(\u03b8) = 1   2\u2207\u03b8 \u2225E\u03b8[h\u03b8(xt\u2032, t\u2032)] \u2212             h\u03b8(x, t)\u22252 + E\u03b8 [h\u03b8(xt\u2032, t\u2032) \u2212              h\u03b8(x, t)]\u22a4      E\u03b8 []\u2207\u03b8 log (p(xt\u2032)) h\u03b8(xt\u2032, t\u2032)]\n In order to save on computation time, we trained by taking gradient steps with respect to only the first\n summand in this decomposition and notice that if the consistency property is preserved then this term\n becomes zero, which implies that no update is made, as desired.\n      It remains to determine how to select t, t\u2032 and xt\u2032. Notice that t has to vary throughout the whole range\n of [0, 1] whereas t\u2032 can either vary over [0, t], however, it sufficient to take t\u2032 \u2208                                     [t \u2212   \u03f5, t]. However, the further\n     2According to Lemma 3.1, in order to completely train for Property 1, one has to also enforce h(x, 0) = x, however, this is\n taken care from the denoising score matching objective Eq. (2).\n                                                                                    7", "md": "# Math Equations and Text\n\n## Text and Math Equations\n\nNext, we will show that for any diffusion process that is defined by Eq. (4), the probability density of\n\\( p(x, t_0) \\) at any time \\( t_0 > 0 \\) is analytic as a function of \\( x \\). Recall that the distribution of \\( x_0 \\) is defined in Eq. (4)\nas \\( p_0 \\) and it holds that the distribution of \\( x_{t_0} \\) is obtained from \\( p_0 \\) by adding a Gaussian noise \\( N(0, \\sigma^2_t I) \\) and\nits density at any \\( x \\) equals\n\n$$\np(x, t_0) = \\int_{a\\in\\mathbb{R}^d} \\frac{1}{\\sqrt{2\\pi\\sigma^2_t}} \\exp\\left(-\\frac{(x - 2\\sigma^2 a)^2}{t}\\right) dp_0(a).\n$$\n\nSince the function \\( \\exp\\left(-\\frac{(x - a)^2}{2\\sigma^2_t}\\right) \\) is analytic, one could deduce that \\( p(x, t_0) \\) is also analytic. Further,\n\\( p(x, t_0) > 0 \\) for all \\( x \\) which implies that there is no singularity for \\( \\log p(x, t_0) \\) which can be used to deduce\nthat \\( \\log p(x, t_0) \\) is also analytic and further that \\( \\nabla_x \\log p(x, t_0) \\) is analytic as well.\n\nWe use the first part of the theorem to deduce that \\( s \\) is the score function of some diffusion process hence\nit is analytic. By assumption, \\( s \\) identifies with some target score function \\( s^* \\) in some open subset \\( U \\subseteq \\mathbb{R}^d \\) at\nsome \\( t_0 \\), which, by the fact that \\( s(x, t_0) \\) and \\( s^*(x, t_0) \\) are analytic, implies that \\( s(x, t_0) = s^*(x, t_0) \\) for all \\( x \\).\nFinally, since \\( s \\) and \\( s^* \\) both satisfy the PDE Eq. (10) and they satisfy the same initial condition at \\( t_0 \\), it\nholds that by uniqueness of the PDE \\( s(x, t) = s^*(x, t) \\) for all \\( x \\) and \\( t \\).\n\n### Method\n\nTheorem 3.2 motivates enforcing the consistency property on the learned model. We notice that the consistency\nequation Eq. (8) may be expensive to train for, because it requires one to generate whole trajectories. Rather,\nwe use the equivalent Martingale assumption of Lemma 3.1, which can be observed locally with only partial\ntrajectories: We suggest the following loss function, for some fixed \\( t, t' \\) and \\( x \\):\n\n$$\nL_{t,t',x}(\\theta) = \\left(E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t)\\right)^2 / 2,\n$$\n\nwhere the expectation \\( E_{\\theta}[\\cdot | x_t = x] \\) is taken according to process Eq. (7) parameterized by \\( h_{\\theta} \\) with the initial\ncondition \\( x_t = x \\). Differentiating this expectation, one gets the following:\n\n$$\n\\nabla L_{t,t',x}(\\theta) = E_{\\theta} \\left[h_{\\theta}(x_{t'}, t') - h_{\\theta}(x_t, t) | x_t = x\\right]^T E_{\\theta} [h_{\\theta}(x_{t'}, t') \\nabla \\log (p_{\\theta}(x_{t'} | x_t = x))] +\n$$\n\nwhere \\( p_{\\theta} \\) corresponds to the same probability measure where the expectation \\( E_{\\theta} \\) is taken from and \\( \\nabla \\theta h_{\\theta} \\) corresponds to the Jacobian matrix of \\( h_{\\theta} \\) where the derivatives are taken with respect to \\( \\theta \\). Notice, however,\nthat computing the expectation accurately might require a large number of samples. Instead, it is possible\nto obtain a stochastic gradient of this target by taking two samples, \\( x_{t'} \\) and \\( x_{t'} \\), independently, from the\nconditional distribution of \\( x_{t'} \\) conditioned on \\( x_t = x \\) and replace each of the two expectations in the formula\nabove with one of these two samples.\n\nWe further notice the gradient of the consistency loss can be written as\n\n$$\n\\nabla \\theta L_{t,t',x}(\\theta) = \\frac{1}{2} \\nabla \\theta \\left\\|E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t)\\right\\|^2 + E_{\\theta} \\left[h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t)\\right]^T E_{\\theta} \\left[\\nabla \\theta \\log (p(x_{t'})) h_{\\theta}(x_{t'}, t')\\right]$$\n\nIn order to save on computation time, we trained by taking gradient steps with respect to only the first\nsummand in this decomposition and notice that if the consistency property is preserved then this term\nbecomes zero, which implies that no update is made, as desired.\n\nIt remains to determine how to select \\( t, t' \\) and \\( x_{t'} \\). Notice that \\( t \\) has to vary throughout the whole range\nof [0, 1] whereas \\( t' \\) can either vary over [0, t], however, it is sufficient to take \\( t' \\in [t - \\epsilon, t] \\). However, the further\n\nAccording to Lemma 3.1, in order to completely train for Property 1, one has to also enforce \\( h(x, 0) = x \\), however, this is\ntaken care from the denoising score matching objective Eq. (2).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Text and Math Equations", "md": "## Text and Math Equations"}, {"type": "text", "value": "Next, we will show that for any diffusion process that is defined by Eq. (4), the probability density of\n\\( p(x, t_0) \\) at any time \\( t_0 > 0 \\) is analytic as a function of \\( x \\). Recall that the distribution of \\( x_0 \\) is defined in Eq. (4)\nas \\( p_0 \\) and it holds that the distribution of \\( x_{t_0} \\) is obtained from \\( p_0 \\) by adding a Gaussian noise \\( N(0, \\sigma^2_t I) \\) and\nits density at any \\( x \\) equals\n\n$$\np(x, t_0) = \\int_{a\\in\\mathbb{R}^d} \\frac{1}{\\sqrt{2\\pi\\sigma^2_t}} \\exp\\left(-\\frac{(x - 2\\sigma^2 a)^2}{t}\\right) dp_0(a).\n$$\n\nSince the function \\( \\exp\\left(-\\frac{(x - a)^2}{2\\sigma^2_t}\\right) \\) is analytic, one could deduce that \\( p(x, t_0) \\) is also analytic. Further,\n\\( p(x, t_0) > 0 \\) for all \\( x \\) which implies that there is no singularity for \\( \\log p(x, t_0) \\) which can be used to deduce\nthat \\( \\log p(x, t_0) \\) is also analytic and further that \\( \\nabla_x \\log p(x, t_0) \\) is analytic as well.\n\nWe use the first part of the theorem to deduce that \\( s \\) is the score function of some diffusion process hence\nit is analytic. By assumption, \\( s \\) identifies with some target score function \\( s^* \\) in some open subset \\( U \\subseteq \\mathbb{R}^d \\) at\nsome \\( t_0 \\), which, by the fact that \\( s(x, t_0) \\) and \\( s^*(x, t_0) \\) are analytic, implies that \\( s(x, t_0) = s^*(x, t_0) \\) for all \\( x \\).\nFinally, since \\( s \\) and \\( s^* \\) both satisfy the PDE Eq. (10) and they satisfy the same initial condition at \\( t_0 \\), it\nholds that by uniqueness of the PDE \\( s(x, t) = s^*(x, t) \\) for all \\( x \\) and \\( t \\).", "md": "Next, we will show that for any diffusion process that is defined by Eq. (4), the probability density of\n\\( p(x, t_0) \\) at any time \\( t_0 > 0 \\) is analytic as a function of \\( x \\). Recall that the distribution of \\( x_0 \\) is defined in Eq. (4)\nas \\( p_0 \\) and it holds that the distribution of \\( x_{t_0} \\) is obtained from \\( p_0 \\) by adding a Gaussian noise \\( N(0, \\sigma^2_t I) \\) and\nits density at any \\( x \\) equals\n\n$$\np(x, t_0) = \\int_{a\\in\\mathbb{R}^d} \\frac{1}{\\sqrt{2\\pi\\sigma^2_t}} \\exp\\left(-\\frac{(x - 2\\sigma^2 a)^2}{t}\\right) dp_0(a).\n$$\n\nSince the function \\( \\exp\\left(-\\frac{(x - a)^2}{2\\sigma^2_t}\\right) \\) is analytic, one could deduce that \\( p(x, t_0) \\) is also analytic. Further,\n\\( p(x, t_0) > 0 \\) for all \\( x \\) which implies that there is no singularity for \\( \\log p(x, t_0) \\) which can be used to deduce\nthat \\( \\log p(x, t_0) \\) is also analytic and further that \\( \\nabla_x \\log p(x, t_0) \\) is analytic as well.\n\nWe use the first part of the theorem to deduce that \\( s \\) is the score function of some diffusion process hence\nit is analytic. By assumption, \\( s \\) identifies with some target score function \\( s^* \\) in some open subset \\( U \\subseteq \\mathbb{R}^d \\) at\nsome \\( t_0 \\), which, by the fact that \\( s(x, t_0) \\) and \\( s^*(x, t_0) \\) are analytic, implies that \\( s(x, t_0) = s^*(x, t_0) \\) for all \\( x \\).\nFinally, since \\( s \\) and \\( s^* \\) both satisfy the PDE Eq. (10) and they satisfy the same initial condition at \\( t_0 \\), it\nholds that by uniqueness of the PDE \\( s(x, t) = s^*(x, t) \\) for all \\( x \\) and \\( t \\)."}, {"type": "heading", "lvl": 3, "value": "Method", "md": "### Method"}, {"type": "text", "value": "Theorem 3.2 motivates enforcing the consistency property on the learned model. We notice that the consistency\nequation Eq. (8) may be expensive to train for, because it requires one to generate whole trajectories. Rather,\nwe use the equivalent Martingale assumption of Lemma 3.1, which can be observed locally with only partial\ntrajectories: We suggest the following loss function, for some fixed \\( t, t' \\) and \\( x \\):\n\n$$\nL_{t,t',x}(\\theta) = \\left(E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t)\\right)^2 / 2,\n$$\n\nwhere the expectation \\( E_{\\theta}[\\cdot | x_t = x] \\) is taken according to process Eq. (7) parameterized by \\( h_{\\theta} \\) with the initial\ncondition \\( x_t = x \\). Differentiating this expectation, one gets the following:\n\n$$\n\\nabla L_{t,t',x}(\\theta) = E_{\\theta} \\left[h_{\\theta}(x_{t'}, t') - h_{\\theta}(x_t, t) | x_t = x\\right]^T E_{\\theta} [h_{\\theta}(x_{t'}, t') \\nabla \\log (p_{\\theta}(x_{t'} | x_t = x))] +\n$$\n\nwhere \\( p_{\\theta} \\) corresponds to the same probability measure where the expectation \\( E_{\\theta} \\) is taken from and \\( \\nabla \\theta h_{\\theta} \\) corresponds to the Jacobian matrix of \\( h_{\\theta} \\) where the derivatives are taken with respect to \\( \\theta \\). Notice, however,\nthat computing the expectation accurately might require a large number of samples. Instead, it is possible\nto obtain a stochastic gradient of this target by taking two samples, \\( x_{t'} \\) and \\( x_{t'} \\), independently, from the\nconditional distribution of \\( x_{t'} \\) conditioned on \\( x_t = x \\) and replace each of the two expectations in the formula\nabove with one of these two samples.\n\nWe further notice the gradient of the consistency loss can be written as\n\n$$\n\\nabla \\theta L_{t,t',x}(\\theta) = \\frac{1}{2} \\nabla \\theta \\left\\|E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t)\\right\\|^2 + E_{\\theta} \\left[h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t)\\right]^T E_{\\theta} \\left[\\nabla \\theta \\log (p(x_{t'})) h_{\\theta}(x_{t'}, t')\\right]$$\n\nIn order to save on computation time, we trained by taking gradient steps with respect to only the first\nsummand in this decomposition and notice that if the consistency property is preserved then this term\nbecomes zero, which implies that no update is made, as desired.\n\nIt remains to determine how to select \\( t, t' \\) and \\( x_{t'} \\). Notice that \\( t \\) has to vary throughout the whole range\nof [0, 1] whereas \\( t' \\) can either vary over [0, t], however, it is sufficient to take \\( t' \\in [t - \\epsilon, t] \\). However, the further\n\nAccording to Lemma 3.1, in order to completely train for Property 1, one has to also enforce \\( h(x, 0) = x \\), however, this is\ntaken care from the denoising score matching objective Eq. (2).", "md": "Theorem 3.2 motivates enforcing the consistency property on the learned model. We notice that the consistency\nequation Eq. (8) may be expensive to train for, because it requires one to generate whole trajectories. Rather,\nwe use the equivalent Martingale assumption of Lemma 3.1, which can be observed locally with only partial\ntrajectories: We suggest the following loss function, for some fixed \\( t, t' \\) and \\( x \\):\n\n$$\nL_{t,t',x}(\\theta) = \\left(E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t)\\right)^2 / 2,\n$$\n\nwhere the expectation \\( E_{\\theta}[\\cdot | x_t = x] \\) is taken according to process Eq. (7) parameterized by \\( h_{\\theta} \\) with the initial\ncondition \\( x_t = x \\). Differentiating this expectation, one gets the following:\n\n$$\n\\nabla L_{t,t',x}(\\theta) = E_{\\theta} \\left[h_{\\theta}(x_{t'}, t') - h_{\\theta}(x_t, t) | x_t = x\\right]^T E_{\\theta} [h_{\\theta}(x_{t'}, t') \\nabla \\log (p_{\\theta}(x_{t'} | x_t = x))] +\n$$\n\nwhere \\( p_{\\theta} \\) corresponds to the same probability measure where the expectation \\( E_{\\theta} \\) is taken from and \\( \\nabla \\theta h_{\\theta} \\) corresponds to the Jacobian matrix of \\( h_{\\theta} \\) where the derivatives are taken with respect to \\( \\theta \\). Notice, however,\nthat computing the expectation accurately might require a large number of samples. Instead, it is possible\nto obtain a stochastic gradient of this target by taking two samples, \\( x_{t'} \\) and \\( x_{t'} \\), independently, from the\nconditional distribution of \\( x_{t'} \\) conditioned on \\( x_t = x \\) and replace each of the two expectations in the formula\nabove with one of these two samples.\n\nWe further notice the gradient of the consistency loss can be written as\n\n$$\n\\nabla \\theta L_{t,t',x}(\\theta) = \\frac{1}{2} \\nabla \\theta \\left\\|E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t)\\right\\|^2 + E_{\\theta} \\left[h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t)\\right]^T E_{\\theta} \\left[\\nabla \\theta \\log (p(x_{t'})) h_{\\theta}(x_{t'}, t')\\right]$$\n\nIn order to save on computation time, we trained by taking gradient steps with respect to only the first\nsummand in this decomposition and notice that if the consistency property is preserved then this term\nbecomes zero, which implies that no update is made, as desired.\n\nIt remains to determine how to select \\( t, t' \\) and \\( x_{t'} \\). Notice that \\( t \\) has to vary throughout the whole range\nof [0, 1] whereas \\( t' \\) can either vary over [0, t], however, it is sufficient to take \\( t' \\in [t - \\epsilon, t] \\). However, the further\n\nAccording to Lemma 3.1, in order to completely train for Property 1, one has to also enforce \\( h(x, 0) = x \\), however, this is\ntaken care from the denoising score matching objective Eq. (2)."}]}, {"page": 8, "text": " away t and t\u2032 are, we need to run more steps of the reverse SDE to avoid large discretization errors. Instead,\nwe enforce the property only on small time windows using that consistency over small intervals implies global\n consistency. We notice that xt can be chosen arbitrarily and two possible choises are to sample it from the\n target noisy distribution pt or from the model.\nRemark 4.1. It is important to sample xt\u2032 conditioned on xt according to the specific SDE Eq. (7). While a\nvariety of alternative SDEs exist which preserve the same marginal distribution at any t, they might not\n preseve the conditionals.\n 5       Experiments\n For all our experiments, we rely on the official open-sourced code and the training and evaluation hyper-\n parameters from the paper \u201cElucidating the Design Space of Diffusion-Based Generative Models\u201d (Karras\n et al., 2022) that, to the best of our knowledge, holds the current state-of-the-art on conditional generation on\n CIFAR-10 and unconditional generation on CIFAR-10, AFHQ (64x64 resolution), FFHQ (64x64 resolution).\nWe refer to the models trained with our regularization as \u201cCDM (Ours)\u201d and to models trained with vanilla\n Denoising Score Matching (DSM) as \u201cEDM\u201d models. \u201cCDM\u201d models are trained with the weighted objective:\n                        Lours   (\u03b8) = Et       Ex0\u223cp0,xt\u223cN (x0,\u03c32                t,xt,x0(\u03b8) + \u03bbExt\u223cptEt\u2032\u223cU[t\u2212\u03f5,t]L2                t,t\u2032,xt(\u03b8)     ,\n                          \u03bb                                              t Id)L1\nwhile the \u201cEDM\u201d models are trained only with the first term of the outer expectation. We also denote in the\n name whether the models have been trained with the Variance Preserving (VP) Song et al. (2021b); Ho et al.\n(2020) or the Variance Exploding Song et al. (2021b); Song and Ermon (2020, 2019), e.g. we write EDM-VP.\n Finally, for completeness, we also report scores from the models of Song et al. (2021b), following the practice\n of the EDM paper. We refer to the latter baselines as \u201cNCSNv3\u201d baselines.\n      We train diffusion models, with and without our regularization, for conditional generation on CIFAR-10\n and unconditional generation on CIFAR-10 and AFHQ (64x64 resolution). For the re-trained models on\n CIFAR-10, we use exactly the same training hyperparameters as in Karras et al. (2022) and we verify that\n our re-trained models match (within 1%) the FID numbers mentioned in the paper. For AFHQ, we had to\n drop the batch size from the suggested value of 512 to 256 to fit in memory, which increased the FID from\n1.96 (reported value) to 2.29. All models were trained for 200k iterations, as in Karras et al. (2022). Finally,\nwe retrain a baseline model on FFHQ for 150k iterations and we finetune it for 5k steps using our proposed\n objective.\n Implementation Choices and Computational Requirements.                                                           As mentioned, when enforcing the\n Consistency Property, we are free to choose t\u2032 anywhere in the interval [0, t]. When t, t\u2032 are far away, sampling\n x\u2032t from the distribution p\u03b8            t\u2032(x\u2032 t|xt) requires many sampling steps (to reduce discretization errors). Since this\n needs to be done for every Gradient Descent update, the training time increases significantly. Instead, we\n notice that local consistency implies global consistency. Hence, we first fix the number of sampling steps to\n run in every training iteration and then we sample t\u2032 uniformly in the interval [t \u2212                                                \u03f5, t] for some specified \u03f5.\n For all our experiments, we fix the number of sampling steps to 6 which roughly increases the training time\n needed by 1.5x. We train all our models on a DGX server with 8 A100 GPUs with 80GBs of memory each.\n 5.1        Consistency Property Testing\nWe are now ready to present our results. The first thing that we check is whether regularizing for the\n Consistency Property actually leads to models that are more consistent. Specifically, we want to check that\n the model trained with Lours             \u03bb     achieves lower consistency error, i.e. lower L2                         t,t\u2032,xt. To check this, we do the\n following two tests: i) we fix t = 1 and we show how L2                                t,t\u2032,xt changes as t\u2032 changes in [0, 1], ii) we fix t\u2032 = 0\n and we show how the loss is changing as you change t in [0, 1]. Intuitively, the first test shows how the\nviolation of the consistency property splits across the sampling process and the second test shows how much\n                                                                                    8", "md": "away \\( t \\) and \\( t' \\) are, we need to run more steps of the reverse SDE to avoid large discretization errors. Instead, we enforce the property only on small time windows using that consistency over small intervals implies global consistency. We notice that \\( x_t \\) can be chosen arbitrarily and two possible choices are to sample it from the target noisy distribution \\( p_t \\) or from the model.\n\nRemark 4.1. It is important to sample \\( x_{t'} \\) conditioned on \\( x_t \\) according to the specific SDE Eq. (7). While a variety of alternative SDEs exist which preserve the same marginal distribution at any \\( t \\), they might not preserve the conditionals.\n\n## 5 Experiments\n\nFor all our experiments, we rely on the official open-sourced code and the training and evaluation hyper-parameters from the paper \u201cElucidating the Design Space of Diffusion-Based Generative Models\u201d (Karras et al., 2022) that, to the best of our knowledge, holds the current state-of-the-art on conditional generation on CIFAR-10 and unconditional generation on CIFAR-10, AFHQ (64x64 resolution), FFHQ (64x64 resolution). We refer to the models trained with our regularization as \u201cCDM (Ours)\u201d and to models trained with vanilla Denoising Score Matching (DSM) as \u201cEDM\u201d models. \u201cCDM\u201d models are trained with the weighted objective:\n\n\\[\nL_{\\text{ours}}(\\theta) = E_t E_{x_0 \\sim p_0, x_t \\sim N(x_0, \\sigma^2_t)} L_{2,t,t',x_t,x_0}(\\theta) + \\lambda E_t E_{x_t' \\sim U[t-\\epsilon, t]} L_{2,t,t',x_t}(\\theta)\n\\]\n\nwhile the \u201cEDM\u201d models are trained only with the first term of the outer expectation. We also denote in the name whether the models have been trained with the Variance Preserving (VP) Song et al. (2021b); Ho et al. (2020) or the Variance Exploding Song et al. (2021b); Song and Ermon (2020, 2019), e.g. we write EDM-VP. Finally, for completeness, we also report scores from the models of Song et al. (2021b), following the practice of the EDM paper. We refer to the latter baselines as \u201cNCSNv3\u201d baselines.\n\nWe train diffusion models, with and without our regularization, for conditional generation on CIFAR-10 and unconditional generation on CIFAR-10 and AFHQ (64x64 resolution). For the re-trained models on CIFAR-10, we use exactly the same training hyperparameters as in Karras et al. (2022) and we verify that our re-trained models match (within 1%) the FID numbers mentioned in the paper. For AFHQ, we had to drop the batch size from the suggested value of 512 to 256 to fit in memory, which increased the FID from 1.96 (reported value) to 2.29. All models were trained for 200k iterations, as in Karras et al. (2022). Finally, we retrain a baseline model on FFHQ for 150k iterations and we finetune it for 5k steps using our proposed objective.\n\nImplementation Choices and Computational Requirements. As mentioned, when enforcing the Consistency Property, we are free to choose \\( t' \\) anywhere in the interval [0, \\( t \\)]. When \\( t, t' \\) are far away, sampling \\( x_t' \\) from the distribution \\( p_{\\theta_t'}(x_t' | x_t) \\) requires many sampling steps (to reduce discretization errors). Since this needs to be done for every Gradient Descent update, the training time increases significantly. Instead, we notice that local consistency implies global consistency. Hence, we first fix the number of sampling steps to run in every training iteration and then we sample \\( t' \\) uniformly in the interval [\\( t - \\epsilon, t \\)] for some specified \\( \\epsilon \\). For all our experiments, we fix the number of sampling steps to 6 which roughly increases the training time needed by 1.5x. We train all our models on a DGX server with 8 A100 GPUs with 80GBs of memory each.\n\n### 5.1 Consistency Property Testing\n\nWe are now ready to present our results. The first thing that we check is whether regularizing for the Consistency Property actually leads to models that are more consistent. Specifically, we want to check that the model trained with \\( L_{\\text{ours}}^\\lambda \\) achieves lower consistency error, i.e. lower \\( L_{2,t,t',x_t} \\). To check this, we do the following two tests: i) we fix \\( t = 1 \\) and we show how \\( L_{2,t,t',x_t} \\) changes as \\( t' \\) changes in [0, 1], ii) we fix \\( t' = 0 \\) and we show how the loss is changing as you change \\( t \\) in [0, 1]. Intuitively, the first test shows how the violation of the consistency property splits across the sampling process and the second test shows how much", "images": [], "items": [{"type": "text", "value": "away \\( t \\) and \\( t' \\) are, we need to run more steps of the reverse SDE to avoid large discretization errors. Instead, we enforce the property only on small time windows using that consistency over small intervals implies global consistency. We notice that \\( x_t \\) can be chosen arbitrarily and two possible choices are to sample it from the target noisy distribution \\( p_t \\) or from the model.\n\nRemark 4.1. It is important to sample \\( x_{t'} \\) conditioned on \\( x_t \\) according to the specific SDE Eq. (7). While a variety of alternative SDEs exist which preserve the same marginal distribution at any \\( t \\), they might not preserve the conditionals.", "md": "away \\( t \\) and \\( t' \\) are, we need to run more steps of the reverse SDE to avoid large discretization errors. Instead, we enforce the property only on small time windows using that consistency over small intervals implies global consistency. We notice that \\( x_t \\) can be chosen arbitrarily and two possible choices are to sample it from the target noisy distribution \\( p_t \\) or from the model.\n\nRemark 4.1. It is important to sample \\( x_{t'} \\) conditioned on \\( x_t \\) according to the specific SDE Eq. (7). While a variety of alternative SDEs exist which preserve the same marginal distribution at any \\( t \\), they might not preserve the conditionals."}, {"type": "heading", "lvl": 2, "value": "5 Experiments", "md": "## 5 Experiments"}, {"type": "text", "value": "For all our experiments, we rely on the official open-sourced code and the training and evaluation hyper-parameters from the paper \u201cElucidating the Design Space of Diffusion-Based Generative Models\u201d (Karras et al., 2022) that, to the best of our knowledge, holds the current state-of-the-art on conditional generation on CIFAR-10 and unconditional generation on CIFAR-10, AFHQ (64x64 resolution), FFHQ (64x64 resolution). We refer to the models trained with our regularization as \u201cCDM (Ours)\u201d and to models trained with vanilla Denoising Score Matching (DSM) as \u201cEDM\u201d models. \u201cCDM\u201d models are trained with the weighted objective:\n\n\\[\nL_{\\text{ours}}(\\theta) = E_t E_{x_0 \\sim p_0, x_t \\sim N(x_0, \\sigma^2_t)} L_{2,t,t',x_t,x_0}(\\theta) + \\lambda E_t E_{x_t' \\sim U[t-\\epsilon, t]} L_{2,t,t',x_t}(\\theta)\n\\]\n\nwhile the \u201cEDM\u201d models are trained only with the first term of the outer expectation. We also denote in the name whether the models have been trained with the Variance Preserving (VP) Song et al. (2021b); Ho et al. (2020) or the Variance Exploding Song et al. (2021b); Song and Ermon (2020, 2019), e.g. we write EDM-VP. Finally, for completeness, we also report scores from the models of Song et al. (2021b), following the practice of the EDM paper. We refer to the latter baselines as \u201cNCSNv3\u201d baselines.\n\nWe train diffusion models, with and without our regularization, for conditional generation on CIFAR-10 and unconditional generation on CIFAR-10 and AFHQ (64x64 resolution). For the re-trained models on CIFAR-10, we use exactly the same training hyperparameters as in Karras et al. (2022) and we verify that our re-trained models match (within 1%) the FID numbers mentioned in the paper. For AFHQ, we had to drop the batch size from the suggested value of 512 to 256 to fit in memory, which increased the FID from 1.96 (reported value) to 2.29. All models were trained for 200k iterations, as in Karras et al. (2022). Finally, we retrain a baseline model on FFHQ for 150k iterations and we finetune it for 5k steps using our proposed objective.\n\nImplementation Choices and Computational Requirements. As mentioned, when enforcing the Consistency Property, we are free to choose \\( t' \\) anywhere in the interval [0, \\( t \\)]. When \\( t, t' \\) are far away, sampling \\( x_t' \\) from the distribution \\( p_{\\theta_t'}(x_t' | x_t) \\) requires many sampling steps (to reduce discretization errors). Since this needs to be done for every Gradient Descent update, the training time increases significantly. Instead, we notice that local consistency implies global consistency. Hence, we first fix the number of sampling steps to run in every training iteration and then we sample \\( t' \\) uniformly in the interval [\\( t - \\epsilon, t \\)] for some specified \\( \\epsilon \\). For all our experiments, we fix the number of sampling steps to 6 which roughly increases the training time needed by 1.5x. We train all our models on a DGX server with 8 A100 GPUs with 80GBs of memory each.", "md": "For all our experiments, we rely on the official open-sourced code and the training and evaluation hyper-parameters from the paper \u201cElucidating the Design Space of Diffusion-Based Generative Models\u201d (Karras et al., 2022) that, to the best of our knowledge, holds the current state-of-the-art on conditional generation on CIFAR-10 and unconditional generation on CIFAR-10, AFHQ (64x64 resolution), FFHQ (64x64 resolution). We refer to the models trained with our regularization as \u201cCDM (Ours)\u201d and to models trained with vanilla Denoising Score Matching (DSM) as \u201cEDM\u201d models. \u201cCDM\u201d models are trained with the weighted objective:\n\n\\[\nL_{\\text{ours}}(\\theta) = E_t E_{x_0 \\sim p_0, x_t \\sim N(x_0, \\sigma^2_t)} L_{2,t,t',x_t,x_0}(\\theta) + \\lambda E_t E_{x_t' \\sim U[t-\\epsilon, t]} L_{2,t,t',x_t}(\\theta)\n\\]\n\nwhile the \u201cEDM\u201d models are trained only with the first term of the outer expectation. We also denote in the name whether the models have been trained with the Variance Preserving (VP) Song et al. (2021b); Ho et al. (2020) or the Variance Exploding Song et al. (2021b); Song and Ermon (2020, 2019), e.g. we write EDM-VP. Finally, for completeness, we also report scores from the models of Song et al. (2021b), following the practice of the EDM paper. We refer to the latter baselines as \u201cNCSNv3\u201d baselines.\n\nWe train diffusion models, with and without our regularization, for conditional generation on CIFAR-10 and unconditional generation on CIFAR-10 and AFHQ (64x64 resolution). For the re-trained models on CIFAR-10, we use exactly the same training hyperparameters as in Karras et al. (2022) and we verify that our re-trained models match (within 1%) the FID numbers mentioned in the paper. For AFHQ, we had to drop the batch size from the suggested value of 512 to 256 to fit in memory, which increased the FID from 1.96 (reported value) to 2.29. All models were trained for 200k iterations, as in Karras et al. (2022). Finally, we retrain a baseline model on FFHQ for 150k iterations and we finetune it for 5k steps using our proposed objective.\n\nImplementation Choices and Computational Requirements. As mentioned, when enforcing the Consistency Property, we are free to choose \\( t' \\) anywhere in the interval [0, \\( t \\)]. When \\( t, t' \\) are far away, sampling \\( x_t' \\) from the distribution \\( p_{\\theta_t'}(x_t' | x_t) \\) requires many sampling steps (to reduce discretization errors). Since this needs to be done for every Gradient Descent update, the training time increases significantly. Instead, we notice that local consistency implies global consistency. Hence, we first fix the number of sampling steps to run in every training iteration and then we sample \\( t' \\) uniformly in the interval [\\( t - \\epsilon, t \\)] for some specified \\( \\epsilon \\). For all our experiments, we fix the number of sampling steps to 6 which roughly increases the training time needed by 1.5x. We train all our models on a DGX server with 8 A100 GPUs with 80GBs of memory each."}, {"type": "heading", "lvl": 3, "value": "5.1 Consistency Property Testing", "md": "### 5.1 Consistency Property Testing"}, {"type": "text", "value": "We are now ready to present our results. The first thing that we check is whether regularizing for the Consistency Property actually leads to models that are more consistent. Specifically, we want to check that the model trained with \\( L_{\\text{ours}}^\\lambda \\) achieves lower consistency error, i.e. lower \\( L_{2,t,t',x_t} \\). To check this, we do the following two tests: i) we fix \\( t = 1 \\) and we show how \\( L_{2,t,t',x_t} \\) changes as \\( t' \\) changes in [0, 1], ii) we fix \\( t' = 0 \\) and we show how the loss is changing as you change \\( t \\) in [0, 1]. Intuitively, the first test shows how the violation of the consistency property splits across the sampling process and the second test shows how much", "md": "We are now ready to present our results. The first thing that we check is whether regularizing for the Consistency Property actually leads to models that are more consistent. Specifically, we want to check that the model trained with \\( L_{\\text{ours}}^\\lambda \\) achieves lower consistency error, i.e. lower \\( L_{2,t,t',x_t} \\). To check this, we do the following two tests: i) we fix \\( t = 1 \\) and we show how \\( L_{2,t,t',x_t} \\) changes as \\( t' \\) changes in [0, 1], ii) we fix \\( t' = 0 \\) and we show how the loss is changing as you change \\( t \\) in [0, 1]. Intuitively, the first test shows how the violation of the consistency property splits across the sampling process and the second test shows how much"}]}, {"page": 9, "text": "you finally (t\u2032 = 0) violate the property if the violation started at time t. The results are shown in Figures 1a,\n1b, respectively, for the models trained on AFHQ. We include additional results for CIFAR-10, FFHQ in\nFigures 4, 5, 6, 7 of the Appendix. As shown, indeed regularizing for the Consistency Loss drops the L2                                                                                                                   t,t\u2032,xt\nas expected.\n                                    Consistency Property Testing (AFHQ)                                                                                       Consistency Property Testing (AFHQ)\n                                        Backward Sampler (1000) steps                                                                                            Backward Sampler (1000) steps\n                      Baseline                                                                                                                                                                                  Baseline\n                      Ours                                                                                                                                                                                      Ours\n        0.005                                                                                                                    0.005\n                                                                                                                              h (x1, 1.0)||2\n    h (xt, t)||2\n        0.004                                                                                                                    0.004\n                                                                                                                               p (xt|x1)[h (xt, t)]\n        0.003                                                                                                                    0.003\n      p (x0|xt)[x0]\n        0.002                                                                                                                    0.002\n       x0\n      ||                                                                                                                        xt\n        0.001                                                                                                                    0.001\n                                                                                                                               ||\n        0.000                                                                                                                    0.000\n                  0         10        20         30        40        50        60         70        80                                      0         10        20        30        40         50  60      70        80\n                                                             t                                                                                                                        t\n(a) Consistency Property Testing on AFHQ. The plot                                                                       (b) Consistency Property Testing on AFHQ. The plot\nillustrates how the Consistency Loss, L2                                              t,t\u2032,xt    , behaves                illustrates how the Consistency Loss, L2                                     t,t\u2032,xt     , behaves\nfor t\u2032 = 0, as t changes.                                                                                                 for t = 0, as t\u2032 changes.\n                                                                     Figure 1: Consistency Property Testing on AFHQ.\nPerformance. We evaluate performance of the models trained from scratch. Following the methodology\nof Karras et al. (2022), we generate 150k images from each model and we report the minimum FID\ncomputed on three sets of 50k images each. We keep checkpoints during training and we report FID for\n30k, 70k, 100k, 150k, 180k and 200k iterations in Table 1. We also report the best FID found for each model,\nafter evaluating checkpoints every 5k iterations (i.e. we evaluate 40 models spanning 200k steps of training).\nAs shown in the Table, the proposed consistency regularization yields improvements throughout the training.\nIn the case of CIFAR-10 (conditional and unconditional) where the re-trained baseline was trained with\nexactly the same hyperparameters as the models in the EDM Karras et al. (2022) paper, our CDM models\nachieve a new state-of-the-art.\n         We further show that our consistency regularization can be applied on top of a pre-trained model.\nSpecifically, we train a baseline EDM-VP model on FFHQ 64x64 for 150k using vanilla Denoising Score\nMatching. We then do 5k steps of finetuning, with and without our consistency regularization and we\nmeasure the FID score of both models. The baseline model achieves FID 2.68 while the model finetuned with\nconsistency regularization achieves 2.61. This experiment shows the potential of applying our consistency\nregularization to pre-trained models, potentially even at large scale, e.g. we could apply this idea with\ntext-to-image models such as Stable Diffusion Rombach et al. (2022). We leave this direction for future work.\n         Uncurated samples from our best models on AFHQ, CIFAR-10 and FFHQ are given in Figures 2a, 2b, 8.\nOne benefit of the deterministic samplers is the unique identifiability property (Song et al., 2021b). Intuitively,\nthis means that by using the same noise and the same deterministic sampler, we can directly compare visually\nmodels that might have been trained in completely different ways. We select a couple of images from Figure\n2a (AFHQ generations) and we compare the generated images from our model with the ones from the EDM\nbaseline for the same noises. The results are shown in Figure 3. As shown, the consistency regularization fixes\nseveral geometric inconsistencies for the picked images. We underline that the shown images are examples for\n                                                                                                                      9", "md": "# Consistency Property Testing\n\n## Consistency Property Testing (AFHQ)\n\n### Backward Sampler (1000) steps\n\n|Baseline|Ours|\n|---|---|\n|0.005|0.005|\n|0.004|0.004|\n|0.003|0.003|\n|0.002|0.002|\n|0.001|0.001|\n|0.000|0.000|\n\n(a) Consistency Property Testing on AFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t' = 0, as t changes.\n\n### Backward Sampler (1000) steps\n\n|Baseline|Ours|\n|---|---|\n|0.005|0.005|\n|0.004|0.004|\n|0.003|0.003|\n|0.002|0.002|\n|0.001|0.001|\n|0.000|0.000|\n\n(b) Consistency Property Testing on AFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t = 0, as t' changes.\n\nFigure 1: Consistency Property Testing on AFHQ.\n\n## Performance\n\nWe evaluate performance of the models trained from scratch. Following the methodology of Karras et al. (2022), we generate 150k images from each model and we report the minimum FID computed on three sets of 50k images each. We keep checkpoints during training and we report FID for 30k, 70k, 100k, 150k, 180k and 200k iterations in Table 1. We also report the best FID found for each model, after evaluating checkpoints every 5k iterations (i.e. we evaluate 40 models spanning 200k steps of training). As shown in the Table, the proposed consistency regularization yields improvements throughout the training.\n\nIn the case of CIFAR-10 (conditional and unconditional) where the re-trained baseline was trained with exactly the same hyperparameters as the models in the EDM Karras et al. (2022) paper, our CDM models achieve a new state-of-the-art.\n\nWe further show that our consistency regularization can be applied on top of a pre-trained model. Specifically, we train a baseline EDM-VP model on FFHQ 64x64 for 150k using vanilla Denoising Score Matching. We then do 5k steps of finetuning, with and without our consistency regularization and we measure the FID score of both models. The baseline model achieves FID 2.68 while the model finetuned with consistency regularization achieves 2.61. This experiment shows the potential of applying our consistency regularization to pre-trained models, potentially even at large scale, e.g. we could apply this idea with text-to-image models such as Stable Diffusion Rombach et al. (2022). We leave this direction for future work.\n\nUncurated samples from our best models on AFHQ, CIFAR-10 and FFHQ are given in Figures 2a, 2b, 8.\n\nOne benefit of the deterministic samplers is the unique identifiability property (Song et al., 2021b). Intuitively, this means that by using the same noise and the same deterministic sampler, we can directly compare visually models that might have been trained in completely different ways. We select a couple of images from Figure 2a (AFHQ generations) and we compare the generated images from our model with the ones from the EDM baseline for the same noises. The results are shown in Figure 3. As shown, the consistency regularization fixes several geometric inconsistencies for the picked images. We underline that the shown images are examples for", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Consistency Property Testing", "md": "# Consistency Property Testing"}, {"type": "heading", "lvl": 2, "value": "Consistency Property Testing (AFHQ)", "md": "## Consistency Property Testing (AFHQ)"}, {"type": "heading", "lvl": 3, "value": "Backward Sampler (1000) steps", "md": "### Backward Sampler (1000) steps"}, {"type": "table", "rows": [["Baseline", "Ours"], ["0.005", "0.005"], ["0.004", "0.004"], ["0.003", "0.003"], ["0.002", "0.002"], ["0.001", "0.001"], ["0.000", "0.000"]], "md": "|Baseline|Ours|\n|---|---|\n|0.005|0.005|\n|0.004|0.004|\n|0.003|0.003|\n|0.002|0.002|\n|0.001|0.001|\n|0.000|0.000|", "isPerfectTable": true, "csv": "\"Baseline\",\"Ours\"\n\"0.005\",\"0.005\"\n\"0.004\",\"0.004\"\n\"0.003\",\"0.003\"\n\"0.002\",\"0.002\"\n\"0.001\",\"0.001\"\n\"0.000\",\"0.000\""}, {"type": "text", "value": "(a) Consistency Property Testing on AFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t' = 0, as t changes.", "md": "(a) Consistency Property Testing on AFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t' = 0, as t changes."}, {"type": "heading", "lvl": 3, "value": "Backward Sampler (1000) steps", "md": "### Backward Sampler (1000) steps"}, {"type": "table", "rows": [["Baseline", "Ours"], ["0.005", "0.005"], ["0.004", "0.004"], ["0.003", "0.003"], ["0.002", "0.002"], ["0.001", "0.001"], ["0.000", "0.000"]], "md": "|Baseline|Ours|\n|---|---|\n|0.005|0.005|\n|0.004|0.004|\n|0.003|0.003|\n|0.002|0.002|\n|0.001|0.001|\n|0.000|0.000|", "isPerfectTable": true, "csv": "\"Baseline\",\"Ours\"\n\"0.005\",\"0.005\"\n\"0.004\",\"0.004\"\n\"0.003\",\"0.003\"\n\"0.002\",\"0.002\"\n\"0.001\",\"0.001\"\n\"0.000\",\"0.000\""}, {"type": "text", "value": "(b) Consistency Property Testing on AFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t = 0, as t' changes.\n\nFigure 1: Consistency Property Testing on AFHQ.", "md": "(b) Consistency Property Testing on AFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t = 0, as t' changes.\n\nFigure 1: Consistency Property Testing on AFHQ."}, {"type": "heading", "lvl": 2, "value": "Performance", "md": "## Performance"}, {"type": "text", "value": "We evaluate performance of the models trained from scratch. Following the methodology of Karras et al. (2022), we generate 150k images from each model and we report the minimum FID computed on three sets of 50k images each. We keep checkpoints during training and we report FID for 30k, 70k, 100k, 150k, 180k and 200k iterations in Table 1. We also report the best FID found for each model, after evaluating checkpoints every 5k iterations (i.e. we evaluate 40 models spanning 200k steps of training). As shown in the Table, the proposed consistency regularization yields improvements throughout the training.\n\nIn the case of CIFAR-10 (conditional and unconditional) where the re-trained baseline was trained with exactly the same hyperparameters as the models in the EDM Karras et al. (2022) paper, our CDM models achieve a new state-of-the-art.\n\nWe further show that our consistency regularization can be applied on top of a pre-trained model. Specifically, we train a baseline EDM-VP model on FFHQ 64x64 for 150k using vanilla Denoising Score Matching. We then do 5k steps of finetuning, with and without our consistency regularization and we measure the FID score of both models. The baseline model achieves FID 2.68 while the model finetuned with consistency regularization achieves 2.61. This experiment shows the potential of applying our consistency regularization to pre-trained models, potentially even at large scale, e.g. we could apply this idea with text-to-image models such as Stable Diffusion Rombach et al. (2022). We leave this direction for future work.\n\nUncurated samples from our best models on AFHQ, CIFAR-10 and FFHQ are given in Figures 2a, 2b, 8.\n\nOne benefit of the deterministic samplers is the unique identifiability property (Song et al., 2021b). Intuitively, this means that by using the same noise and the same deterministic sampler, we can directly compare visually models that might have been trained in completely different ways. We select a couple of images from Figure 2a (AFHQ generations) and we compare the generated images from our model with the ones from the EDM baseline for the same noises. The results are shown in Figure 3. As shown, the consistency regularization fixes several geometric inconsistencies for the picked images. We underline that the shown images are examples for", "md": "We evaluate performance of the models trained from scratch. Following the methodology of Karras et al. (2022), we generate 150k images from each model and we report the minimum FID computed on three sets of 50k images each. We keep checkpoints during training and we report FID for 30k, 70k, 100k, 150k, 180k and 200k iterations in Table 1. We also report the best FID found for each model, after evaluating checkpoints every 5k iterations (i.e. we evaluate 40 models spanning 200k steps of training). As shown in the Table, the proposed consistency regularization yields improvements throughout the training.\n\nIn the case of CIFAR-10 (conditional and unconditional) where the re-trained baseline was trained with exactly the same hyperparameters as the models in the EDM Karras et al. (2022) paper, our CDM models achieve a new state-of-the-art.\n\nWe further show that our consistency regularization can be applied on top of a pre-trained model. Specifically, we train a baseline EDM-VP model on FFHQ 64x64 for 150k using vanilla Denoising Score Matching. We then do 5k steps of finetuning, with and without our consistency regularization and we measure the FID score of both models. The baseline model achieves FID 2.68 while the model finetuned with consistency regularization achieves 2.61. This experiment shows the potential of applying our consistency regularization to pre-trained models, potentially even at large scale, e.g. we could apply this idea with text-to-image models such as Stable Diffusion Rombach et al. (2022). We leave this direction for future work.\n\nUncurated samples from our best models on AFHQ, CIFAR-10 and FFHQ are given in Figures 2a, 2b, 8.\n\nOne benefit of the deterministic samplers is the unique identifiability property (Song et al., 2021b). Intuitively, this means that by using the same noise and the same deterministic sampler, we can directly compare visually models that might have been trained in completely different ways. We select a couple of images from Figure 2a (AFHQ generations) and we compare the generated images from our model with the ones from the EDM baseline for the same noises. The results are shown in Figure 3. As shown, the consistency regularization fixes several geometric inconsistencies for the picked images. We underline that the shown images are examples for"}]}, {"page": 10, "text": "(a) Uncurated images by our model trained on AFHQ.                (b) Uncurated images by our conditional CIFAR-10 model.\nFID: 2.21, NFEs: 79.                                              FID: 1.77, NFEs: 35.\n                 Figure 2: Comparison of uncurated images generated by two different models.\n               Model                                         30k      70k    100k     150k    180k    200k     Best\n       CDM-VP (Ours)                                         3.00    2.44    2.30     2.31    2.25    2.44     2.21\n      EDM-VP (retrained)                                     3.27    2.41     2.61    2.43    2.29     2.61    2.26\n      EDM-VP (reported)\u22173                  AFHQ                                                                1.96\n      EDM-VE (reported)\u2217                                                                                       2.16\n    NCSNv3-VP (reported)\u2217                                                                                      2.58\n    NCSNv3-VE (reported)\u2217                                                                                      18.52\n       CDM-VP (Ours)                                         2.44    1.94    1.88     1.88    1.80    1.82     1.77\n      EDM-VP (retrained)                                     2.50    1.99     1.94    1.85    1.86     1.90    1.82\n       EDM-VP (reported)             CIFAR10 (cond.)                                                           1.79\n       EDM-VE (reported)                                                                                       1.79\n     NCSNv3-VP (reported)                                                                                      2.48\n     NCSNv3-VE (reported)                                                                                      3.11\n       CDM-VP (Ours)                                         2.83    2.21    2.14     2.08    1.99    2.03     1.95\n      EDM-VP (retrained)                                     2.90    2.32     2.15    2.09    2.01     2.13    2.01\n       EDM-VP (reported)           CIFAR10 (uncond.)                                                           1.97\n       EDM-VE (reported)                                                                                       1.98\n     NCSNv3-VP (reported)                                                                                      3.01\n     NCSNv3-VE (reported)                                                                                      3.77\nTable 1: FID results for deterministic sampling, using the Karras et al. (2022) second-order samplers. For\nthe CIFAR-10 models, we do 35 function evaluations and for AFHQ 79.\nwhich consistency regularization helped and that potentially there are images for which the baseline models\ngive more realistic results.\nAblation Study for Theoretical Predictions. One interesting implication of Theorem 3.2 is that it\nsuggests that we only need to learn the score perfectly on some fixed t0 and then the consistency property\nimplies that the score is learned everywhere (for all t and in the whole space). This motivates the following\n                                                           10", "md": "(a) Uncurated images by our model trained on AFHQ. (b) Uncurated images by our conditional CIFAR-10 model.\nFID: 2.21, NFEs: 79. FID: 1.77, NFEs: 35.\n\n$$\n\\begin{array}{lccccccc}\n\\text{Model} & 30k & 70k & 100k & 150k & 180k & 200k & \\text{Best} \\\\\n\\hline\n\\text{CDM-VP (Ours)} & 3.00 & 2.44 & 2.30 & 2.31 & 2.25 & 2.44 & 2.21 \\\\\n\\text{EDM-VP (retrained)} & 3.27 & 2.41 & 2.61 & 2.43 & 2.29 & 2.61 & 2.26 \\\\\n\\text{EDM-VP (reported)}^*3 & \\text{AFHQ} & & & & & & 1.96 \\\\\n\\text{EDM-VE (reported)}^* & & & & & & & 2.16 \\\\\n\\text{NCSNv3-VP (reported)}^* & & & & & & & 2.58 \\\\\n\\text{NCSNv3-VE (reported)}^* & & & & & & & 18.52 \\\\\n\\text{CDM-VP (Ours)} & 2.44 & 1.94 & 1.88 & 1.88 & 1.80 & 1.82 & 1.77 \\\\\n\\text{EDM-VP (retrained)} & 2.50 & 1.99 & 1.94 & 1.85 & 1.86 & 1.90 & 1.82 \\\\\n\\text{EDM-VP (reported)} & \\text{CIFAR10 (cond.)} & & & & & & 1.79 \\\\\n\\text{EDM-VE (reported)} & & & & & & & 1.79 \\\\\n\\text{NCSNv3-VP (reported)} & & & & & & & 2.48 \\\\\n\\text{NCSNv3-VE (reported)} & & & & & & & 3.11 \\\\\n\\text{CDM-VP (Ours)} & 2.83 & 2.21 & 2.14 & 2.08 & 1.99 & 2.03 & 1.95 \\\\\n\\text{EDM-VP (retrained)} & 2.90 & 2.32 & 2.15 & 2.09 & 2.01 & 2.13 & 2.01 \\\\\n\\text{EDM-VP (reported)} & \\text{CIFAR10 (uncond.)} & & & & & & 1.97 \\\\\n\\text{EDM-VE (reported)} & & & & & & & 1.98 \\\\\n\\text{NCSNv3-VP (reported)} & & & & & & & 3.01 \\\\\n\\text{NCSNv3-VE (reported)} & & & & & & & 3.77 \\\\\n\\end{array}\n$$\n\nTable 1: FID results for deterministic sampling, using the Karras et al. (2022) second-order samplers. For the CIFAR-10 models, we do 35 function evaluations and for AFHQ 79.\n\nWhich consistency regularization helped and that potentially there are images for which the baseline models give more realistic results.\n\nAblation Study for Theoretical Predictions. One interesting implication of Theorem 3.2 is that it suggests that we only need to learn the score perfectly on some fixed \\( t_0 \\) and then the consistency property implies that the score is learned everywhere (for all \\( t \\) and in the whole space). This motivates the following:\n\n10", "images": [{"name": "page-10-0.jpg", "height": 211, "width": 211, "x": 72, "y": 72}, {"name": "page-10-1.jpg", "height": 211, "width": 211, "x": 329, "y": 73}], "items": [{"type": "text", "value": "(a) Uncurated images by our model trained on AFHQ. (b) Uncurated images by our conditional CIFAR-10 model.\nFID: 2.21, NFEs: 79. FID: 1.77, NFEs: 35.\n\n$$\n\\begin{array}{lccccccc}\n\\text{Model} & 30k & 70k & 100k & 150k & 180k & 200k & \\text{Best} \\\\\n\\hline\n\\text{CDM-VP (Ours)} & 3.00 & 2.44 & 2.30 & 2.31 & 2.25 & 2.44 & 2.21 \\\\\n\\text{EDM-VP (retrained)} & 3.27 & 2.41 & 2.61 & 2.43 & 2.29 & 2.61 & 2.26 \\\\\n\\text{EDM-VP (reported)}^*3 & \\text{AFHQ} & & & & & & 1.96 \\\\\n\\text{EDM-VE (reported)}^* & & & & & & & 2.16 \\\\\n\\text{NCSNv3-VP (reported)}^* & & & & & & & 2.58 \\\\\n\\text{NCSNv3-VE (reported)}^* & & & & & & & 18.52 \\\\\n\\text{CDM-VP (Ours)} & 2.44 & 1.94 & 1.88 & 1.88 & 1.80 & 1.82 & 1.77 \\\\\n\\text{EDM-VP (retrained)} & 2.50 & 1.99 & 1.94 & 1.85 & 1.86 & 1.90 & 1.82 \\\\\n\\text{EDM-VP (reported)} & \\text{CIFAR10 (cond.)} & & & & & & 1.79 \\\\\n\\text{EDM-VE (reported)} & & & & & & & 1.79 \\\\\n\\text{NCSNv3-VP (reported)} & & & & & & & 2.48 \\\\\n\\text{NCSNv3-VE (reported)} & & & & & & & 3.11 \\\\\n\\text{CDM-VP (Ours)} & 2.83 & 2.21 & 2.14 & 2.08 & 1.99 & 2.03 & 1.95 \\\\\n\\text{EDM-VP (retrained)} & 2.90 & 2.32 & 2.15 & 2.09 & 2.01 & 2.13 & 2.01 \\\\\n\\text{EDM-VP (reported)} & \\text{CIFAR10 (uncond.)} & & & & & & 1.97 \\\\\n\\text{EDM-VE (reported)} & & & & & & & 1.98 \\\\\n\\text{NCSNv3-VP (reported)} & & & & & & & 3.01 \\\\\n\\text{NCSNv3-VE (reported)} & & & & & & & 3.77 \\\\\n\\end{array}\n$$\n\nTable 1: FID results for deterministic sampling, using the Karras et al. (2022) second-order samplers. For the CIFAR-10 models, we do 35 function evaluations and for AFHQ 79.\n\nWhich consistency regularization helped and that potentially there are images for which the baseline models give more realistic results.\n\nAblation Study for Theoretical Predictions. One interesting implication of Theorem 3.2 is that it suggests that we only need to learn the score perfectly on some fixed \\( t_0 \\) and then the consistency property implies that the score is learned everywhere (for all \\( t \\) and in the whole space). This motivates the following:\n\n10", "md": "(a) Uncurated images by our model trained on AFHQ. (b) Uncurated images by our conditional CIFAR-10 model.\nFID: 2.21, NFEs: 79. FID: 1.77, NFEs: 35.\n\n$$\n\\begin{array}{lccccccc}\n\\text{Model} & 30k & 70k & 100k & 150k & 180k & 200k & \\text{Best} \\\\\n\\hline\n\\text{CDM-VP (Ours)} & 3.00 & 2.44 & 2.30 & 2.31 & 2.25 & 2.44 & 2.21 \\\\\n\\text{EDM-VP (retrained)} & 3.27 & 2.41 & 2.61 & 2.43 & 2.29 & 2.61 & 2.26 \\\\\n\\text{EDM-VP (reported)}^*3 & \\text{AFHQ} & & & & & & 1.96 \\\\\n\\text{EDM-VE (reported)}^* & & & & & & & 2.16 \\\\\n\\text{NCSNv3-VP (reported)}^* & & & & & & & 2.58 \\\\\n\\text{NCSNv3-VE (reported)}^* & & & & & & & 18.52 \\\\\n\\text{CDM-VP (Ours)} & 2.44 & 1.94 & 1.88 & 1.88 & 1.80 & 1.82 & 1.77 \\\\\n\\text{EDM-VP (retrained)} & 2.50 & 1.99 & 1.94 & 1.85 & 1.86 & 1.90 & 1.82 \\\\\n\\text{EDM-VP (reported)} & \\text{CIFAR10 (cond.)} & & & & & & 1.79 \\\\\n\\text{EDM-VE (reported)} & & & & & & & 1.79 \\\\\n\\text{NCSNv3-VP (reported)} & & & & & & & 2.48 \\\\\n\\text{NCSNv3-VE (reported)} & & & & & & & 3.11 \\\\\n\\text{CDM-VP (Ours)} & 2.83 & 2.21 & 2.14 & 2.08 & 1.99 & 2.03 & 1.95 \\\\\n\\text{EDM-VP (retrained)} & 2.90 & 2.32 & 2.15 & 2.09 & 2.01 & 2.13 & 2.01 \\\\\n\\text{EDM-VP (reported)} & \\text{CIFAR10 (uncond.)} & & & & & & 1.97 \\\\\n\\text{EDM-VE (reported)} & & & & & & & 1.98 \\\\\n\\text{NCSNv3-VP (reported)} & & & & & & & 3.01 \\\\\n\\text{NCSNv3-VE (reported)} & & & & & & & 3.77 \\\\\n\\end{array}\n$$\n\nTable 1: FID results for deterministic sampling, using the Karras et al. (2022) second-order samplers. For the CIFAR-10 models, we do 35 function evaluations and for AFHQ 79.\n\nWhich consistency regularization helped and that potentially there are images for which the baseline models give more realistic results.\n\nAblation Study for Theoretical Predictions. One interesting implication of Theorem 3.2 is that it suggests that we only need to learn the score perfectly on some fixed \\( t_0 \\) and then the consistency property implies that the score is learned everywhere (for all \\( t \\) and in the whole space). This motivates the following:\n\n10"}]}, {"page": 11, "text": " Figure 3: Visual comparison of EDM model (top) and CDM model (Ours, bottom) using deterministic\n sampling initiated with the same noise. As seen, the consistency regularization fixes several geometric\n inconsistencies and artifacts in the generated images.\n                                                 Model                       FID\n                                            EDM (baseline)                   5.81\n                                           CDM, all times t                  5.45\n                                           CDM, for some t                   6.59\n                               CDM, for some t, early stopped sampling       14.52\n Table 2: Ablation study on removing the DSM loss for some t. Table reports FID results after 10k steps of\n training in CIFAR-10.\n experiment: instead of using as our loss the weighted sum of DSM and our consistency regularization for all\n t, we will not use DSM for t \u2264  tthreshold, for some tthreshold that we test our theory for.\n    We pick tthreshold such that for 20% of the diffusion (on the side of clean images), we do not train with\n DSM. For the rest 80% we train with both DSM and our consistency regularization. Since this is only an\n ablation study, we train for only 10k steps on (conditional) CIFAR-10. We report FID numbers for three\n models: i) training with only DSM, ii) training with DSM and consistency regularization everywhere, iii)\n training with DSM for 80% of times t and consistency regularization everywhere. In our reported models,\n we also include FID of an early stopped sampling of the latter model, i.e. we do not run the sampling for\n t < tthreshold and we just output h\u03b8(xtthreshold, tthreshold). The numbers are summarized in Table 2. As shown,\n the theory is predictive since early stopping the generation at time t gives significantly worse results than\n continuing the sampling through the times that were never explicitly trained for approximating the score (i.e.\n we did not use DSM for those times). That said, the best results are obtained by combining DSM and our\n consistency regularization everywhere, which is what we did for all the other experiments in the paper.\n 6    Related Work\n The fact that imperfect learning of the score function introduces a shift between the training and the sampling\n distribution has been well known. Chen et al. (2022a) analyze how the l2 error in the approximation of\n the score function propagates to Total Variation distance error bounds between the true and the learned\n distribution. Several methods for mitigating this issue have been proposed, but the majority of the attempts\n focus on changing the sampling process Song et al. (2021b); Karras et al. (2022); Jolicoeur-Martineau et al.\n(2021); Sehwag et al. (2022). A related work is the Analog-Bits paper Chen et al. (2022b) that conditions the\n model during training with past model predictions.\n    Karras et al. (2022) discusses potential violations of invariances, such as the non-conservativity of the\n induced vector field, due to imperfect score matching. However, they do not formally test or enforce this\n                                                       11", "md": "# Research Paper Summary\n\n## Visual Comparison of EDM and CDM Models\n\nFigure 3: Visual comparison of EDM model (top) and CDM model (Ours, bottom) using deterministic sampling initiated with the same noise. As seen, the consistency regularization fixes several geometric inconsistencies and artifacts in the generated images.\n\n## Model Comparison\n\n|Model|FID|\n|---|---|\n|EDM (baseline)|5.81|\n|CDM, all times t|5.45|\n|CDM, for some t|6.59|\n|CDM, for some t, early stopped sampling|14.52|\n\n## Ablation Study on Removing the DSM Loss\n\nTable 2: Ablation study on removing the DSM loss for some t. Table reports FID results after 10k steps of training in CIFAR-10.\n\nExperiment: instead of using as our loss the weighted sum of DSM and our consistency regularization for all t, we will not use DSM for t \u2264 tthreshold, for some tthreshold that we test our theory for.\n\nWe pick tthreshold such that for 20% of the diffusion (on the side of clean images), we do not train with DSM. For the rest 80% we train with both DSM and our consistency regularization. Since this is only an ablation study, we train for only 10k steps on (conditional) CIFAR-10. We report FID numbers for three models: i) training with only DSM, ii) training with DSM and consistency regularization everywhere, iii) training with DSM for 80% of times t and consistency regularization everywhere. In our reported models, we also include FID of an early stopped sampling of the latter model, i.e. we do not run the sampling for t < tthreshold and we just output h\u03b8(xtthreshold, tthreshold). The numbers are summarized in Table 2. As shown, the theory is predictive since early stopping the generation at time t gives significantly worse results than continuing the sampling through the times that were never explicitly trained for approximating the score (i.e. we did not use DSM for those times). That said, the best results are obtained by combining DSM and our consistency regularization everywhere, which is what we did for all the other experiments in the paper.\n\n## Related Work\n\nThe fact that imperfect learning of the score function introduces a shift between the training and the sampling distribution has been well known. Chen et al. (2022a) analyze how the l2 error in the approximation of the score function propagates to Total Variation distance error bounds between the true and the learned distribution. Several methods for mitigating this issue have been proposed, but the majority of the attempts focus on changing the sampling process Song et al. (2021b); Karras et al. (2022); Jolicoeur-Martineau et al. (2021); Sehwag et al. (2022). A related work is the Analog-Bits paper Chen et al. (2022b) that conditions the model during training with past model predictions.\n\nKarras et al. (2022) discusses potential violations of invariances, such as the non-conservativity of the induced vector field, due to imperfect score matching. However, they do not formally test or enforce this.", "images": [{"name": "page-11-1.jpg", "height": 66, "width": 328, "x": 142, "y": 138}, {"name": "page-11-0.jpg", "height": 66, "width": 328, "x": 142, "y": 71}], "items": [{"type": "heading", "lvl": 1, "value": "Research Paper Summary", "md": "# Research Paper Summary"}, {"type": "heading", "lvl": 2, "value": "Visual Comparison of EDM and CDM Models", "md": "## Visual Comparison of EDM and CDM Models"}, {"type": "text", "value": "Figure 3: Visual comparison of EDM model (top) and CDM model (Ours, bottom) using deterministic sampling initiated with the same noise. As seen, the consistency regularization fixes several geometric inconsistencies and artifacts in the generated images.", "md": "Figure 3: Visual comparison of EDM model (top) and CDM model (Ours, bottom) using deterministic sampling initiated with the same noise. As seen, the consistency regularization fixes several geometric inconsistencies and artifacts in the generated images."}, {"type": "heading", "lvl": 2, "value": "Model Comparison", "md": "## Model Comparison"}, {"type": "table", "rows": [["Model", "FID"], ["EDM (baseline)", "5.81"], ["CDM, all times t", "5.45"], ["CDM, for some t", "6.59"], ["CDM, for some t, early stopped sampling", "14.52"]], "md": "|Model|FID|\n|---|---|\n|EDM (baseline)|5.81|\n|CDM, all times t|5.45|\n|CDM, for some t|6.59|\n|CDM, for some t, early stopped sampling|14.52|", "isPerfectTable": true, "csv": "\"Model\",\"FID\"\n\"EDM (baseline)\",\"5.81\"\n\"CDM, all times t\",\"5.45\"\n\"CDM, for some t\",\"6.59\"\n\"CDM, for some t, early stopped sampling\",\"14.52\""}, {"type": "heading", "lvl": 2, "value": "Ablation Study on Removing the DSM Loss", "md": "## Ablation Study on Removing the DSM Loss"}, {"type": "text", "value": "Table 2: Ablation study on removing the DSM loss for some t. Table reports FID results after 10k steps of training in CIFAR-10.\n\nExperiment: instead of using as our loss the weighted sum of DSM and our consistency regularization for all t, we will not use DSM for t \u2264 tthreshold, for some tthreshold that we test our theory for.\n\nWe pick tthreshold such that for 20% of the diffusion (on the side of clean images), we do not train with DSM. For the rest 80% we train with both DSM and our consistency regularization. Since this is only an ablation study, we train for only 10k steps on (conditional) CIFAR-10. We report FID numbers for three models: i) training with only DSM, ii) training with DSM and consistency regularization everywhere, iii) training with DSM for 80% of times t and consistency regularization everywhere. In our reported models, we also include FID of an early stopped sampling of the latter model, i.e. we do not run the sampling for t < tthreshold and we just output h\u03b8(xtthreshold, tthreshold). The numbers are summarized in Table 2. As shown, the theory is predictive since early stopping the generation at time t gives significantly worse results than continuing the sampling through the times that were never explicitly trained for approximating the score (i.e. we did not use DSM for those times). That said, the best results are obtained by combining DSM and our consistency regularization everywhere, which is what we did for all the other experiments in the paper.", "md": "Table 2: Ablation study on removing the DSM loss for some t. Table reports FID results after 10k steps of training in CIFAR-10.\n\nExperiment: instead of using as our loss the weighted sum of DSM and our consistency regularization for all t, we will not use DSM for t \u2264 tthreshold, for some tthreshold that we test our theory for.\n\nWe pick tthreshold such that for 20% of the diffusion (on the side of clean images), we do not train with DSM. For the rest 80% we train with both DSM and our consistency regularization. Since this is only an ablation study, we train for only 10k steps on (conditional) CIFAR-10. We report FID numbers for three models: i) training with only DSM, ii) training with DSM and consistency regularization everywhere, iii) training with DSM for 80% of times t and consistency regularization everywhere. In our reported models, we also include FID of an early stopped sampling of the latter model, i.e. we do not run the sampling for t < tthreshold and we just output h\u03b8(xtthreshold, tthreshold). The numbers are summarized in Table 2. As shown, the theory is predictive since early stopping the generation at time t gives significantly worse results than continuing the sampling through the times that were never explicitly trained for approximating the score (i.e. we did not use DSM for those times). That said, the best results are obtained by combining DSM and our consistency regularization everywhere, which is what we did for all the other experiments in the paper."}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "The fact that imperfect learning of the score function introduces a shift between the training and the sampling distribution has been well known. Chen et al. (2022a) analyze how the l2 error in the approximation of the score function propagates to Total Variation distance error bounds between the true and the learned distribution. Several methods for mitigating this issue have been proposed, but the majority of the attempts focus on changing the sampling process Song et al. (2021b); Karras et al. (2022); Jolicoeur-Martineau et al. (2021); Sehwag et al. (2022). A related work is the Analog-Bits paper Chen et al. (2022b) that conditions the model during training with past model predictions.\n\nKarras et al. (2022) discusses potential violations of invariances, such as the non-conservativity of the induced vector field, due to imperfect score matching. However, they do not formally test or enforce this.", "md": "The fact that imperfect learning of the score function introduces a shift between the training and the sampling distribution has been well known. Chen et al. (2022a) analyze how the l2 error in the approximation of the score function propagates to Total Variation distance error bounds between the true and the learned distribution. Several methods for mitigating this issue have been proposed, but the majority of the attempts focus on changing the sampling process Song et al. (2021b); Karras et al. (2022); Jolicoeur-Martineau et al. (2021); Sehwag et al. (2022). A related work is the Analog-Bits paper Chen et al. (2022b) that conditions the model during training with past model predictions.\n\nKarras et al. (2022) discusses potential violations of invariances, such as the non-conservativity of the induced vector field, due to imperfect score matching. However, they do not formally test or enforce this."}]}, {"page": 12, "text": "property. Lai et al. (2022) study the problem of regularizing diffusion models to satisfy the Fokker-Planck\nequation. While we show in Theorem 3.2 that perfect conservative training enforces the Fokker-Planck\nequation, we notice that their training method is different: they suggest to enforce the equation locally by\nusing the finite differences method to approximate the derivatives. Further, they do not train on drifted\ndata. Instead, we notice that our consistency loss is well suited to handle drifted data since it operates across\ntrajectories generated by the model. Finally, they show benchmark improvements on MNIST whereas we\nachieve state-of-the-art performance and benchmark improvements in more challenging datasets such as\nCIFAR-10 and AFHQ.\n7     Conclusions and Future Work\nWe proposed a novel objective that enforces the trained network to have self-consistent predictions over time.\nWe optimize this objective with points from the sampling distribution, effectively reducing the sampling drift\nobserved in prior empirical works. Theoretically, we show that the consistency property implies that we\nare sampling from the reverse of some diffusion process. Together with the assumption that the network\nhas learned perfectly the score for some time t0 and some open set U, we can prove that the consistency\nproperty implies that we learn the score perfectly everywhere. Empirically, we use our objective to obtain\nstate-of-the-art for CIFAR-10 and baseline improvements on AFHQ and FFHQ.\n   There are limitations of our method and several directions for future work. The proposed regularization\nincreases the training time by approximately 1.5x. It would be interesting to explore how to enforce consistency\nin more effective ways in future work. Further, our method does not test nor enforce that the induced\nvector-field is conservative, which is a key theoretical assumption. Our method guarantees only indirectly\nimprove the performance in the samples from the learned distribution by enforcing some invariant. Finally,\nour theoretical result assumes perfect learning of the score in some subset of Rd. An important next step\nwould be to understand how errors propagate if the score-function is only approximately learned.\n8     Acknowledgments\nThis research has been supported by NSF Grants CCF 1763702, AF 1901292, CNS 2148141, Tripods CCF\n1934932, IFML CCF 2019844, the Texas Advanced Computing Center (TACC) and research gifts by Western\nDigital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Archie Straiton Endowed\nFaculty Fellowship. Giannis Daras has been supported by the Onassis Fellowship, the Bodossaki Fellowship\nand the Leventis Fellowship. Constantinos Daskalakis has been supported by NSF Awards CCF-1901292,\nDMS-2022448 and DMS2134108, a Simons Investigator Award, the Simons Collaboration on the Theory of\nAlgorithmic Fairness and a DSTA grant.\nReferences\nAnand, N. and Achim, T. (2022). Protein structure and sequence generation with equivariant denoising\n  diffusion probabilistic models. arXiv preprint arXiv:2205.15019.\nAnderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications,\n  12(3):313\u2013326.\nArvinte, M., Jalal, A., Daras, G., Price, E., Dimakis, A., and Tamir, J. I. (2022). Single-shot adaptation using\n  score-based models for mri reconstruction. In International Society for Magnetic Resonance in Medicine,\n  Annual Meeting.\nBansal, A., Borgnia, E., Chu, H.-M., Li, J. S., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., and\n  Goldstein, T. (2022). Cold Diffusion: Inverting arbitrary image transforms without noise. arXiv preprint\n  arXiv:2208.09392.\n                                                       12", "md": "# Research Paper Summary\n\n## Property\n\nLai et al. (2022) study the problem of regularizing diffusion models to satisfy the Fokker-Planck equation. While we show in Theorem 3.2 that perfect conservative training enforces the Fokker-Planck equation, we notice that their training method is different: they suggest to enforce the equation locally by using the finite differences method to approximate the derivatives. Further, they do not train on drifted data. Instead, we notice that our consistency loss is well suited to handle drifted data since it operates across trajectories generated by the model. Finally, they show benchmark improvements on MNIST whereas we achieve state-of-the-art performance and benchmark improvements in more challenging datasets such as CIFAR-10 and AFHQ.\n\n## Conclusions and Future Work\n\nWe proposed a novel objective that enforces the trained network to have self-consistent predictions over time. We optimize this objective with points from the sampling distribution, effectively reducing the sampling drift observed in prior empirical works. Theoretically, we show that the consistency property implies that we are sampling from the reverse of some diffusion process. Together with the assumption that the network has learned perfectly the score for some time $$t_0$$ and some open set $$U$$, we can prove that the consistency property implies that we learn the score perfectly everywhere. Empirically, we use our objective to obtain state-of-the-art for CIFAR-10 and baseline improvements on AFHQ and FFHQ.\n\nThere are limitations of our method and several directions for future work. The proposed regularization increases the training time by approximately 1.5x. It would be interesting to explore how to enforce consistency in more effective ways in future work. Further, our method does not test nor enforce that the induced vector-field is conservative, which is a key theoretical assumption. Our method guarantees only indirectly improve the performance in the samples from the learned distribution by enforcing some invariant. Finally, our theoretical result assumes perfect learning of the score in some subset of $$\\mathbb{R}^d$$. An important next step would be to understand how errors propagate if the score-function is only approximately learned.\n\n## Acknowledgments\n\nThis research has been supported by NSF Grants CCF 1763702, AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Archie Straiton Endowed Faculty Fellowship. Giannis Daras has been supported by the Onassis Fellowship, the Bodossaki Fellowship and the Leventis Fellowship. Constantinos Daskalakis has been supported by NSF Awards CCF-1901292, DMS-2022448 and DMS2134108, a Simons Investigator Award, the Simons Collaboration on the Theory of Algorithmic Fairness and a DSTA grant.\n\n## References\n\nAnand, N. and Achim, T. (2022). Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. arXiv preprint arXiv:2205.15019.\n\nAnderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326.\n\nArvinte, M., Jalal, A., Daras, G., Price, E., Dimakis, A., and Tamir, J. I. (2022). Single-shot adaptation using score-based models for MRI reconstruction. In International Society for Magnetic Resonance in Medicine, Annual Meeting.\n\nBansal, A., Borgnia, E., Chu, H.-M., Li, J. S., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., and Goldstein, T. (2022). Cold Diffusion: Inverting arbitrary image transforms without noise. arXiv preprint arXiv:2208.09392.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Paper Summary", "md": "# Research Paper Summary"}, {"type": "heading", "lvl": 2, "value": "Property", "md": "## Property"}, {"type": "text", "value": "Lai et al. (2022) study the problem of regularizing diffusion models to satisfy the Fokker-Planck equation. While we show in Theorem 3.2 that perfect conservative training enforces the Fokker-Planck equation, we notice that their training method is different: they suggest to enforce the equation locally by using the finite differences method to approximate the derivatives. Further, they do not train on drifted data. Instead, we notice that our consistency loss is well suited to handle drifted data since it operates across trajectories generated by the model. Finally, they show benchmark improvements on MNIST whereas we achieve state-of-the-art performance and benchmark improvements in more challenging datasets such as CIFAR-10 and AFHQ.", "md": "Lai et al. (2022) study the problem of regularizing diffusion models to satisfy the Fokker-Planck equation. While we show in Theorem 3.2 that perfect conservative training enforces the Fokker-Planck equation, we notice that their training method is different: they suggest to enforce the equation locally by using the finite differences method to approximate the derivatives. Further, they do not train on drifted data. Instead, we notice that our consistency loss is well suited to handle drifted data since it operates across trajectories generated by the model. Finally, they show benchmark improvements on MNIST whereas we achieve state-of-the-art performance and benchmark improvements in more challenging datasets such as CIFAR-10 and AFHQ."}, {"type": "heading", "lvl": 2, "value": "Conclusions and Future Work", "md": "## Conclusions and Future Work"}, {"type": "text", "value": "We proposed a novel objective that enforces the trained network to have self-consistent predictions over time. We optimize this objective with points from the sampling distribution, effectively reducing the sampling drift observed in prior empirical works. Theoretically, we show that the consistency property implies that we are sampling from the reverse of some diffusion process. Together with the assumption that the network has learned perfectly the score for some time $$t_0$$ and some open set $$U$$, we can prove that the consistency property implies that we learn the score perfectly everywhere. Empirically, we use our objective to obtain state-of-the-art for CIFAR-10 and baseline improvements on AFHQ and FFHQ.\n\nThere are limitations of our method and several directions for future work. The proposed regularization increases the training time by approximately 1.5x. It would be interesting to explore how to enforce consistency in more effective ways in future work. Further, our method does not test nor enforce that the induced vector-field is conservative, which is a key theoretical assumption. Our method guarantees only indirectly improve the performance in the samples from the learned distribution by enforcing some invariant. Finally, our theoretical result assumes perfect learning of the score in some subset of $$\\mathbb{R}^d$$. An important next step would be to understand how errors propagate if the score-function is only approximately learned.", "md": "We proposed a novel objective that enforces the trained network to have self-consistent predictions over time. We optimize this objective with points from the sampling distribution, effectively reducing the sampling drift observed in prior empirical works. Theoretically, we show that the consistency property implies that we are sampling from the reverse of some diffusion process. Together with the assumption that the network has learned perfectly the score for some time $$t_0$$ and some open set $$U$$, we can prove that the consistency property implies that we learn the score perfectly everywhere. Empirically, we use our objective to obtain state-of-the-art for CIFAR-10 and baseline improvements on AFHQ and FFHQ.\n\nThere are limitations of our method and several directions for future work. The proposed regularization increases the training time by approximately 1.5x. It would be interesting to explore how to enforce consistency in more effective ways in future work. Further, our method does not test nor enforce that the induced vector-field is conservative, which is a key theoretical assumption. Our method guarantees only indirectly improve the performance in the samples from the learned distribution by enforcing some invariant. Finally, our theoretical result assumes perfect learning of the score in some subset of $$\\mathbb{R}^d$$. An important next step would be to understand how errors propagate if the score-function is only approximately learned."}, {"type": "heading", "lvl": 2, "value": "Acknowledgments", "md": "## Acknowledgments"}, {"type": "text", "value": "This research has been supported by NSF Grants CCF 1763702, AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Archie Straiton Endowed Faculty Fellowship. Giannis Daras has been supported by the Onassis Fellowship, the Bodossaki Fellowship and the Leventis Fellowship. Constantinos Daskalakis has been supported by NSF Awards CCF-1901292, DMS-2022448 and DMS2134108, a Simons Investigator Award, the Simons Collaboration on the Theory of Algorithmic Fairness and a DSTA grant.", "md": "This research has been supported by NSF Grants CCF 1763702, AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Archie Straiton Endowed Faculty Fellowship. Giannis Daras has been supported by the Onassis Fellowship, the Bodossaki Fellowship and the Leventis Fellowship. Constantinos Daskalakis has been supported by NSF Awards CCF-1901292, DMS-2022448 and DMS2134108, a Simons Investigator Award, the Simons Collaboration on the Theory of Algorithmic Fairness and a DSTA grant."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "Anand, N. and Achim, T. (2022). Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. arXiv preprint arXiv:2205.15019.\n\nAnderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326.\n\nArvinte, M., Jalal, A., Daras, G., Price, E., Dimakis, A., and Tamir, J. I. (2022). Single-shot adaptation using score-based models for MRI reconstruction. In International Society for Magnetic Resonance in Medicine, Annual Meeting.\n\nBansal, A., Borgnia, E., Chu, H.-M., Li, J. S., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., and Goldstein, T. (2022). Cold Diffusion: Inverting arbitrary image transforms without noise. arXiv preprint arXiv:2208.09392.", "md": "Anand, N. and Achim, T. (2022). Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. arXiv preprint arXiv:2205.15019.\n\nAnderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326.\n\nArvinte, M., Jalal, A., Daras, G., Price, E., Dimakis, A., and Tamir, J. I. (2022). Single-shot adaptation using score-based models for MRI reconstruction. In International Society for Magnetic Resonance in Medicine, Annual Meeting.\n\nBansal, A., Borgnia, E., Chu, H.-M., Li, J. S., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., and Goldstein, T. (2022). Cold Diffusion: Inverting arbitrary image transforms without noise. arXiv preprint arXiv:2208.09392."}]}, {"page": 13, "text": "Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2022a). Sampling is as easy as learning the\n   score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215.\nChen, T., Zhang, R., and Hinton, G. (2022b). Analog bits: Generating discrete data using diffusion models\n   with self-conditioning. arXiv preprint arXiv:2208.04202.\nCorso, G., St\u00a8ark, H., Jing, B., Barzilay, R., and Jaakkola, T. (2022). Diffdock: Diffusion steps, twists, and\n   turns for molecular docking. arXiv preprint arXiv:2210.01776.\nDaras, G., Dagan, Y., Dimakis, A. G., and Daskalakis, C. (2022a). Score-guided intermediate layer optimiza-\n   tion: Fast langevin mixing for inverse problem. arXiv preprint arXiv:2206.09104.\nDaras, G., Delbracio, M., Talebi, H., Dimakis, A. G., and Milanfar, P. (2022b). Soft diffusion: Score matching\n   for general corruptions. arXiv preprint arXiv:2209.05442.\nDaras, G. and Dimakis, A. G. (2022). Multiresolution textual inversion. arXiv preprint arXiv:2211.17115.\nDeasy, J., Simidjievski, N., and Li` o, P. (2021). Heavy-tailed denoising score matching. arXiv preprint\n   arXiv:2112.09788.\nDhariwal, P. and Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in Neural\n   Information Processing Systems, 34:8780\u20138794.\nEfron, B. (2011). Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association,\n   106(496):1602\u20131614.\nGal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. (2022). An\n   image is worth one word: Personalizing text-to-image generation using textual inversion.\nGrenander, U. and Miller, M. I. (1994). Representations of knowledge in complex systems. Journal of the\n   Royal Statistical Society: Series B (Methodological), 56(4):549\u2013581.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M.,\n   Fleet, D. J., et al. (2022a). Imagen video: High definition video generation with diffusion models. arXiv\n   preprint arXiv:2210.02303.\nHo, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural\n   Information Processing Systems, 33:6840\u20136851.\nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. (2022b). Video diffusion models.\n   arXiv:2204.03458.\nHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. (2022). Cogvideo: Large-scale pretraining for\n   text-to-video generation via transformers. arXiv preprint arXiv:2205.15868.\nHoogeboom, E. and Salimans, T. (2022). Blurring diffusion models. arXiv preprint arXiv:2209.05557.\nJalal, A., Arvinte, M., Daras, G., Price, E., Dimakis, A. G., and Tamir, J. (2021). Robust compressed sensing\n   mri with deep generative priors. Advances in Neural Information Processing Systems, 34:14938\u201314954.\nJolicoeur-Martineau, A., Li, K., Pich\u00b4e-Taillefer, R., Kachman, T., and Mitliagkas, I. (2021). Gotta go fast\n   when generating data with score-based models. arXiv preprint arXiv:2105.14080.\nKarras, T., Aittala, M., Aila, T., and Laine, S. (2022). Elucidating the design space of diffusion-based\n   generative models. arXiv preprint arXiv:2206.00364.\nKim, D., Shin, S., Song, K., Kang, W., and Moon, I.-C. (2022). Soft truncation: A universal training\n   technique of score-based diffusion model for high precision score estimation. In International Conference\n   on Machine Learning, pages 11201\u201311228. PMLR.\n                                                      13", "md": "# Research Papers\n\n# List of Research Papers\n\n- Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2022a). Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215.\n- Chen, T., Zhang, R., and Hinton, G. (2022b). Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202.\n- Corso, G., St\u00a8ark, H., Jing, B., Barzilay, R., and Jaakkola, T. (2022). Diffdock: Diffusion steps, twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776.\n- Daras, G., Dagan, Y., Dimakis, A. G., and Daskalakis, C. (2022a). Score-guided intermediate layer optimization: Fast langevin mixing for inverse problem. arXiv preprint arXiv:2206.09104.\n- Daras, G., Delbracio, M., Talebi, H., Dimakis, A. G., and Milanfar, P. (2022b). Soft diffusion: Score matching for general corruptions. arXiv preprint arXiv:2209.05442.\n- Daras, G. and Dimakis, A. G. (2022). Multiresolution textual inversion. arXiv preprint arXiv:2211.17115.\n- Deasy, J., Simidjievski, N., and Li` o, P. (2021). Heavy-tailed denoising score matching. arXiv preprint arXiv:2112.09788.\n- Dhariwal, P. and Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794.\n- Efron, B. (2011). Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106(496):1602\u20131614.\n- Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion.\n- Grenander, U. and Miller, M. I. (1994). Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549\u2013581.\n- Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. (2022a). Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303.\n- Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851.\n- Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. (2022b). Video diffusion models. arXiv:2204.03458.\n- Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. (2022). Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868.\n- Hoogeboom, E. and Salimans, T. (2022). Blurring diffusion models. arXiv preprint arXiv:2209.05557.\n- Jalal, A., Arvinte, M., Daras, G., Price, E., Dimakis, A. G., and Tamir, J. (2021). Robust compressed sensing mri with deep generative priors. Advances in Neural Information Processing Systems, 34:14938\u201314954.\n- Jolicoeur-Martineau, A., Li, K., Pich\u00b4e-Taillefer, R., Kachman, T., and Mitliagkas, I. (2021). Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080.\n- Karras, T., Aittala, M., Aila, T., and Laine, S. (2022). Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364.\n- Kim, D., Shin, S., Song, K., Kang, W., and Moon, I.-C. (2022). Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. In International Conference on Machine Learning, pages 11201\u201311228. PMLR.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Papers", "md": "# Research Papers"}, {"type": "heading", "lvl": 1, "value": "List of Research Papers", "md": "# List of Research Papers"}, {"type": "text", "value": "- Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2022a). Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215.\n- Chen, T., Zhang, R., and Hinton, G. (2022b). Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202.\n- Corso, G., St\u00a8ark, H., Jing, B., Barzilay, R., and Jaakkola, T. (2022). Diffdock: Diffusion steps, twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776.\n- Daras, G., Dagan, Y., Dimakis, A. G., and Daskalakis, C. (2022a). Score-guided intermediate layer optimization: Fast langevin mixing for inverse problem. arXiv preprint arXiv:2206.09104.\n- Daras, G., Delbracio, M., Talebi, H., Dimakis, A. G., and Milanfar, P. (2022b). Soft diffusion: Score matching for general corruptions. arXiv preprint arXiv:2209.05442.\n- Daras, G. and Dimakis, A. G. (2022). Multiresolution textual inversion. arXiv preprint arXiv:2211.17115.\n- Deasy, J., Simidjievski, N., and Li` o, P. (2021). Heavy-tailed denoising score matching. arXiv preprint arXiv:2112.09788.\n- Dhariwal, P. and Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794.\n- Efron, B. (2011). Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106(496):1602\u20131614.\n- Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion.\n- Grenander, U. and Miller, M. I. (1994). Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549\u2013581.\n- Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. (2022a). Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303.\n- Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851.\n- Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. (2022b). Video diffusion models. arXiv:2204.03458.\n- Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. (2022). Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868.\n- Hoogeboom, E. and Salimans, T. (2022). Blurring diffusion models. arXiv preprint arXiv:2209.05557.\n- Jalal, A., Arvinte, M., Daras, G., Price, E., Dimakis, A. G., and Tamir, J. (2021). Robust compressed sensing mri with deep generative priors. Advances in Neural Information Processing Systems, 34:14938\u201314954.\n- Jolicoeur-Martineau, A., Li, K., Pich\u00b4e-Taillefer, R., Kachman, T., and Mitliagkas, I. (2021). Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080.\n- Karras, T., Aittala, M., Aila, T., and Laine, S. (2022). Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364.\n- Kim, D., Shin, S., Song, K., Kang, W., and Moon, I.-C. (2022). Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. In International Conference on Machine Learning, pages 11201\u201311228. PMLR.", "md": "- Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2022a). Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215.\n- Chen, T., Zhang, R., and Hinton, G. (2022b). Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202.\n- Corso, G., St\u00a8ark, H., Jing, B., Barzilay, R., and Jaakkola, T. (2022). Diffdock: Diffusion steps, twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776.\n- Daras, G., Dagan, Y., Dimakis, A. G., and Daskalakis, C. (2022a). Score-guided intermediate layer optimization: Fast langevin mixing for inverse problem. arXiv preprint arXiv:2206.09104.\n- Daras, G., Delbracio, M., Talebi, H., Dimakis, A. G., and Milanfar, P. (2022b). Soft diffusion: Score matching for general corruptions. arXiv preprint arXiv:2209.05442.\n- Daras, G. and Dimakis, A. G. (2022). Multiresolution textual inversion. arXiv preprint arXiv:2211.17115.\n- Deasy, J., Simidjievski, N., and Li` o, P. (2021). Heavy-tailed denoising score matching. arXiv preprint arXiv:2112.09788.\n- Dhariwal, P. and Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794.\n- Efron, B. (2011). Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106(496):1602\u20131614.\n- Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion.\n- Grenander, U. and Miller, M. I. (1994). Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549\u2013581.\n- Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. (2022a). Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303.\n- Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851.\n- Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. (2022b). Video diffusion models. arXiv:2204.03458.\n- Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. (2022). Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868.\n- Hoogeboom, E. and Salimans, T. (2022). Blurring diffusion models. arXiv preprint arXiv:2209.05557.\n- Jalal, A., Arvinte, M., Daras, G., Price, E., Dimakis, A. G., and Tamir, J. (2021). Robust compressed sensing mri with deep generative priors. Advances in Neural Information Processing Systems, 34:14938\u201314954.\n- Jolicoeur-Martineau, A., Li, K., Pich\u00b4e-Taillefer, R., Kachman, T., and Mitliagkas, I. (2021). Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080.\n- Karras, T., Aittala, M., Aila, T., and Laine, S. (2022). Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364.\n- Kim, D., Shin, S., Song, K., Kang, W., and Moon, I.-C. (2022). Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. In International Conference on Machine Learning, pages 11201\u201311228. PMLR."}]}, {"page": 14, "text": "Kim, K. and Ye, J. C. (2021). Noise2score: tweedie\u2019s approach to self-supervised image denoising without\n  clean images. Advances in Neural Information Processing Systems, 34:864\u2013874.\nKong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2021). Diffwave: A versatile diffusion model for\n  audio synthesis. In International Conference on Learning Representations.\nLai, C.-H., Takida, Y., Murata, N., Uesaka, T., Mitsufuji, Y., and Ermon, S. (2022). Regularizing score-based\n  models with score fokker-planck equations. In NeurIPS 2022 Workshop on Score-Based Methods.\nLuo, C. (2022). Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970.\nMeng, C., Song, Y., Li, W., and Ermon, S. (2021). Estimating high order gradients of the data distribution\n  by denoising. Advances in Neural Information Processing Systems, 34:25359\u201325369.\nNachmani, E., Roman, R. S., and Wolf, L. (2021). Denoising diffusion gamma models. arXiv preprint\n  arXiv:2110.05948.\nNichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International\n  Conference on Machine Learning, pages 8162\u20138171. PMLR.\nPoole, B., Jain, A., Barron, J. T., and Mildenhall, B. (2022). Dreamfusion: Text-to-3d using 2d diffusion.\n  arXiv.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical text-conditional image\n  generation with clip latents. arXiv preprint arXiv:2204.06125.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image synthesis\n  with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition, pages 10684\u201310695.\nRuiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. (2022). Dreambooth: Fine tuning\n  text-to-image diffusion models for subject-driven generation.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K.,\n  Mahdavi, S. S., Lopes, R. G., et al. (2022). Photorealistic text-to-image diffusion models with deep language\n  understanding. arXiv preprint arXiv:2205.11487.\nSchneuing, A., Du, Y., Harris, C., Jamasb, A., Igashov, I., Du, W., Blundell, T., Li\u00b4       o, P., Gomes, C.,\n  Welling, M., et al. (2022). Structure-based drug design with equivariant diffusion models. arXiv preprint\n  arXiv:2210.13695.\nSehwag, V., Hazirbas, C., Gordo, A., Ozgenel, F., and Canton, C. (2022). Generating high fidelity data from\n  low-density regions using diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition, pages 11492\u201311501.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning\n  using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265.\n  PMLR.\nSong, J., Meng, C., and Ermon, S. (2021a). Denoising diffusion implicit models. In International Conference\n  on Learning Representations.\nSong, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution.\n  Advances in Neural Information Processing Systems, 32.\nSong, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models. Advances in\n  neural information processing systems, 33:12438\u201312448.\n                                                      14", "md": "# References\n\n# References\n\n- Kim, K. and Ye, J. C. (2021). *Noise2score: tweedie\u2019s approach to self-supervised image denoising without clean images. Advances in Neural Information Processing Systems*, 34:864\u2013874.\n- Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2021). *Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations*.\n- Lai, C.-H., Takida, Y., Murata, N., Uesaka, T., Mitsufuji, Y., and Ermon, S. (2022). *Regularizing score-based models with score fokker-planck equations. In NeurIPS 2022 Workshop on Score-Based Methods*.\n- Luo, C. (2022). *Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970*.\n- Meng, C., Song, Y., Li, W., and Ermon, S. (2021). *Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems*, 34:25359\u201325369.\n- Nachmani, E., Roman, R. S., and Wolf, L. (2021). *Denoising diffusion gamma models. arXiv preprint arXiv:2110.05948*.\n- Nichol, A. Q. and Dhariwal, P. (2021). *Improved denoising diffusion probabilistic models. In International Conference on Machine Learning*, pages 8162\u20138171. PMLR.\n- Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. (2022). *Dreamfusion: Text-to-3d using 2d diffusion. arXiv*.\n- Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). *Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125*.\n- Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). *High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 10684\u201310695.\n- Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. (2022). *Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation*.\n- Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. (2022). *Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487*.\n- Schneuing, A., Du, Y., Harris, C., Jamasb, A., Igashov, I., Du, W., Blundell, T., Li\u00b4 o, P., Gomes, C., Welling, M., et al. (2022). *Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695*.\n- Sehwag, V., Hazirbas, C., Gordo, A., Ozgenel, F., and Canton, C. (2022). *Generating high fidelity data from low-density regions using diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11492\u201311501.\n- Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). *Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning*, pages 2256\u20132265. PMLR.\n- Song, J., Meng, C., and Ermon, S. (2021a). *Denoising diffusion implicit models. In International Conference on Learning Representations*.\n- Song, Y. and Ermon, S. (2019). *Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems*, 32.\n- Song, Y. and Ermon, S. (2020). *Improved techniques for training score-based generative models. Advances in neural information processing systems*, 33:12438\u201312448.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- Kim, K. and Ye, J. C. (2021). *Noise2score: tweedie\u2019s approach to self-supervised image denoising without clean images. Advances in Neural Information Processing Systems*, 34:864\u2013874.\n- Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2021). *Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations*.\n- Lai, C.-H., Takida, Y., Murata, N., Uesaka, T., Mitsufuji, Y., and Ermon, S. (2022). *Regularizing score-based models with score fokker-planck equations. In NeurIPS 2022 Workshop on Score-Based Methods*.\n- Luo, C. (2022). *Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970*.\n- Meng, C., Song, Y., Li, W., and Ermon, S. (2021). *Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems*, 34:25359\u201325369.\n- Nachmani, E., Roman, R. S., and Wolf, L. (2021). *Denoising diffusion gamma models. arXiv preprint arXiv:2110.05948*.\n- Nichol, A. Q. and Dhariwal, P. (2021). *Improved denoising diffusion probabilistic models. In International Conference on Machine Learning*, pages 8162\u20138171. PMLR.\n- Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. (2022). *Dreamfusion: Text-to-3d using 2d diffusion. arXiv*.\n- Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). *Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125*.\n- Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). *High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 10684\u201310695.\n- Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. (2022). *Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation*.\n- Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. (2022). *Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487*.\n- Schneuing, A., Du, Y., Harris, C., Jamasb, A., Igashov, I., Du, W., Blundell, T., Li\u00b4 o, P., Gomes, C., Welling, M., et al. (2022). *Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695*.\n- Sehwag, V., Hazirbas, C., Gordo, A., Ozgenel, F., and Canton, C. (2022). *Generating high fidelity data from low-density regions using diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11492\u201311501.\n- Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). *Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning*, pages 2256\u20132265. PMLR.\n- Song, J., Meng, C., and Ermon, S. (2021a). *Denoising diffusion implicit models. In International Conference on Learning Representations*.\n- Song, Y. and Ermon, S. (2019). *Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems*, 32.\n- Song, Y. and Ermon, S. (2020). *Improved techniques for training score-based generative models. Advances in neural information processing systems*, 33:12438\u201312448.", "md": "- Kim, K. and Ye, J. C. (2021). *Noise2score: tweedie\u2019s approach to self-supervised image denoising without clean images. Advances in Neural Information Processing Systems*, 34:864\u2013874.\n- Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2021). *Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations*.\n- Lai, C.-H., Takida, Y., Murata, N., Uesaka, T., Mitsufuji, Y., and Ermon, S. (2022). *Regularizing score-based models with score fokker-planck equations. In NeurIPS 2022 Workshop on Score-Based Methods*.\n- Luo, C. (2022). *Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970*.\n- Meng, C., Song, Y., Li, W., and Ermon, S. (2021). *Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems*, 34:25359\u201325369.\n- Nachmani, E., Roman, R. S., and Wolf, L. (2021). *Denoising diffusion gamma models. arXiv preprint arXiv:2110.05948*.\n- Nichol, A. Q. and Dhariwal, P. (2021). *Improved denoising diffusion probabilistic models. In International Conference on Machine Learning*, pages 8162\u20138171. PMLR.\n- Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. (2022). *Dreamfusion: Text-to-3d using 2d diffusion. arXiv*.\n- Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). *Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125*.\n- Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). *High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 10684\u201310695.\n- Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. (2022). *Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation*.\n- Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. (2022). *Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487*.\n- Schneuing, A., Du, Y., Harris, C., Jamasb, A., Igashov, I., Du, W., Blundell, T., Li\u00b4 o, P., Gomes, C., Welling, M., et al. (2022). *Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695*.\n- Sehwag, V., Hazirbas, C., Gordo, A., Ozgenel, F., and Canton, C. (2022). *Generating high fidelity data from low-density regions using diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11492\u201311501.\n- Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). *Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning*, pages 2256\u20132265. PMLR.\n- Song, J., Meng, C., and Ermon, S. (2021a). *Denoising diffusion implicit models. In International Conference on Learning Representations*.\n- Song, Y. and Ermon, S. (2019). *Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems*, 32.\n- Song, Y. and Ermon, S. (2020). *Improved techniques for training score-based generative models. Advances in neural information processing systems*, 33:12438\u201312448."}]}, {"page": 15, "text": "Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021b). Score-based\n  generative modeling through stochastic differential equations. In International Conference on Learning\n  Representations.\nTrippe, B. L., Yim, J., Tischer, D., Broderick, T., Baker, D., Barzilay, R., and Jaakkola, T. (2022). Diffusion\n  probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem.         arXiv preprint\n  arXiv:2206.04119.\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural computation,\n  23(7):1661\u20131674.\n                                                      15", "md": "Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021b). Score-based\ngenerative modeling through stochastic differential equations. In International Conference on Learning\nRepresentations.\n\nTrippe, B. L., Yim, J., Tischer, D., Broderick, T., Baker, D., Barzilay, R., and Jaakkola, T. (2022). Diffusion\nprobabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. arXiv preprint\narXiv:2206.04119.\n\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural computation,\n23(7):1661\u20131674.\n\n15", "images": [], "items": [{"type": "text", "value": "Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021b). Score-based\ngenerative modeling through stochastic differential equations. In International Conference on Learning\nRepresentations.\n\nTrippe, B. L., Yim, J., Tischer, D., Broderick, T., Baker, D., Barzilay, R., and Jaakkola, T. (2022). Diffusion\nprobabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. arXiv preprint\narXiv:2206.04119.\n\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural computation,\n23(7):1661\u20131674.\n\n15", "md": "Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021b). Score-based\ngenerative modeling through stochastic differential equations. In International Conference on Learning\nRepresentations.\n\nTrippe, B. L., Yim, J., Tischer, D., Broderick, T., Baker, D., Barzilay, R., and Jaakkola, T. (2022). Diffusion\nprobabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. arXiv preprint\narXiv:2206.04119.\n\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural computation,\n23(7):1661\u20131674.\n\n15"}]}, {"page": 16, "text": " A         Proof of Theorem 3.2\n In Section A.1 we present some preliminaries to the proof, in Section A.2 we include the proof, with proofs of\n some lemmas ommitted and in the remaining sections we prove these lemmas.\n A.1         Preliminaries\n Preliminaries on diffusion processes                                In the next definition we define for a function F : Rd \u2192                              Rd its\nJacobian JF , its divergence \u2207                  \u00b7 F and its Laplacian \u25b3F that is computed separately on each coordinate of F                                        :\n Definition A.1. Given a function F = (f1, . . . , fn): Rd \u2192            (JF )ij = \u2202fi(x) \u2202xj   Rd,.denote by JF : Rd \u2192                 Rd\u00d7d its Jacobian:\nThe divergence of F is defined as                                  \u2207   \u00b7 F (x) :=       n   \u2202fi(x)     .\n                                                                                      i=1      \u2202xi\n Denote by \u25b3F : Rd \u2192                 Rd the function whose ith entry is the Laplacian of fi:\n                                                                                       n    \u22022fi(x).\n                                                                  (\u25b3F    (x))i =      j=1      \u2202x2 j\n If F is a function of both x \u2208                   Rd and t \u2208         R, then JF , \u25b3f and \u2207                 \u00b7 F correspond to F as a function of x,\nwhereas t is kept fixed. In particular,\n                       (JF (x, t))ij = \u2202fi(x, t)           ,     (\u25b3F    (x, t))i =       n    \u22022 fi(x, t)   ,     \u2207  \u00b7 F =       n   \u2202fi(x, t)    .\n                                                  \u2202xj                                  j=1        \u2202x2 j                        i=1       \u2202xi\n      We use the celebrated Ito\u2019s lemma and some of its immediate generalizations:\n Lemma A.2 (Ito\u2019s Lemma). Let xt be a stochastic process xt \u2208    dxt = \u00b5(xt, t)dt + g(t)dBt,           Rd, that is defined by the following SDE:\nwhere Bt is a standard Brownian motion. Let f : Rd \u00d7 R \u2192                                        R. Then,\n                                   df(xt, t) =       d   f                                       \u25b3f       dt + g(t)\u2207xf \u22a4dBt.\n                                                        dt + \u2207xf \u22a4\u00b5(xt, t) + g(t)2          2\nFurther, if F : Rd \u00d7 R \u2192                Rd is a multi-valued function, then\n                                           dF(xt, t) =         dFdt + JF \u00b5 + g(t)2       2    \u25b3F       dt + g(t)JF dBt.\nLastly, if xt is instead defined with a reverse noise,\n                                                                dxt = \u00b5(xt, t)dt + g(t)dBt,\nthen the multi-valued Ito\u2019s lemma is modified as follows:\n                                           dF(xt, t) =        dF dt + JF \u00b5 \u2212          g(t)2   \u25b3F       dt + g(t)JF dBt.                                       (12)\n                                                                                        2\n                                                                                    16", "md": "# Proof of Theorem 3.2\n\n## Proof of Theorem 3.2\n\nIn Section A.1 we present some preliminaries to the proof, in Section A.2 we include the proof, with proofs of\nsome lemmas omitted and in the remaining sections we prove these lemmas.\n\n### A.1 Preliminaries\n\nPreliminaries on diffusion processes\n\nIn the next definition we define for a function \\( F : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) its Jacobian \\( JF \\), its divergence \\( \\nabla \\cdot F \\) and its Laplacian \\( \\Delta F \\) that is computed separately on each coordinate of \\( F \\):\n\nDefinition A.1. Given a function \\( F = (f_1, \\ldots, f_n) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^n \\), denote by \\( JF : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d \\times d} \\) its Jacobian:\nThe divergence of \\( F \\) is defined as \\( \\nabla \\cdot F(x) := \\sum_{i=1}^{n} \\frac{\\partial f_i(x)}{\\partial x_i} \\).\nDenote by \\( \\Delta F : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) the function whose \\( i \\)th entry is the Laplacian of \\( f_i \\):\n\\( (\\Delta F(x))_i = \\sum_{j=1}^{n} \\frac{\\partial^2 f_i(x)}{\\partial x_j^2} \\).\n\nIf \\( F \\) is a function of both \\( x \\in \\mathbb{R}^d \\) and \\( t \\in \\mathbb{R} \\), then \\( JF \\), \\( \\Delta f \\) and \\( \\nabla \\cdot F \\) correspond to \\( F \\) as a function of \\( x \\), whereas \\( t \\) is kept fixed. In particular,\n\\( (JF(x, t))_{ij} = \\frac{\\partial f_i(x, t)}{\\partial x_j} \\), \\( (\\Delta F(x, t))_i = \\sum_{j=1}^{n} \\frac{\\partial^2 f_i(x, t)}{\\partial x_j^2} \\), \\( \\nabla \\cdot F = \\sum_{i=1}^{n} \\frac{\\partial f_i(x, t)}{\\partial x_i} \\).\n\nWe use the celebrated Ito's lemma and some of its immediate generalizations:\n\nLemma A.2 (Ito's Lemma). Let \\( x_t \\) be a stochastic process \\( x_t \\in \\mathbb{R}^d = dx_t = \\mu(x_t, t)dt + g(t)dB_t \\), \\( \\mathbb{R}^d \\), that is defined by the following SDE:\nwhere \\( B_t \\) is a standard Brownian motion. Let \\( f : \\mathbb{R}^d \\times \\mathbb{R} \\rightarrow \\mathbb{R} \\). Then,\n\\( df(x_t, t) = \\frac{d f}{dt} + \\Delta f dt + g(t) \\nabla_x f^T dB_t. \\)\n\nFurther, if \\( F : \\mathbb{R}^d \\times \\mathbb{R} \\rightarrow \\mathbb{R}^d \\) is a multi-valued function, then\n\\( dF(x_t, t) = \\frac{d F}{dt} + JF \\mu + g(t)^2 \\Delta F dt + g(t)JF dB_t. \\)\n\nLastly, if \\( x_t \\) is instead defined with a reverse noise,\n\\( dx_t = \\mu(x_t, t)dt + g(t)dB_t \\),\nthen the multi-valued Ito's lemma is modified as follows:\n\\( dF(x_t, t) = \\frac{d F}{dt} + JF \\mu - g(t)^2 \\Delta F dt + g(t)JF dB_t. \\) (12)\n\n16", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Proof of Theorem 3.2", "md": "# Proof of Theorem 3.2"}, {"type": "heading", "lvl": 2, "value": "Proof of Theorem 3.2", "md": "## Proof of Theorem 3.2"}, {"type": "text", "value": "In Section A.1 we present some preliminaries to the proof, in Section A.2 we include the proof, with proofs of\nsome lemmas omitted and in the remaining sections we prove these lemmas.", "md": "In Section A.1 we present some preliminaries to the proof, in Section A.2 we include the proof, with proofs of\nsome lemmas omitted and in the remaining sections we prove these lemmas."}, {"type": "heading", "lvl": 3, "value": "A.1 Preliminaries", "md": "### A.1 Preliminaries"}, {"type": "text", "value": "Preliminaries on diffusion processes\n\nIn the next definition we define for a function \\( F : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) its Jacobian \\( JF \\), its divergence \\( \\nabla \\cdot F \\) and its Laplacian \\( \\Delta F \\) that is computed separately on each coordinate of \\( F \\):\n\nDefinition A.1. Given a function \\( F = (f_1, \\ldots, f_n) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^n \\), denote by \\( JF : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d \\times d} \\) its Jacobian:\nThe divergence of \\( F \\) is defined as \\( \\nabla \\cdot F(x) := \\sum_{i=1}^{n} \\frac{\\partial f_i(x)}{\\partial x_i} \\).\nDenote by \\( \\Delta F : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) the function whose \\( i \\)th entry is the Laplacian of \\( f_i \\):\n\\( (\\Delta F(x))_i = \\sum_{j=1}^{n} \\frac{\\partial^2 f_i(x)}{\\partial x_j^2} \\).\n\nIf \\( F \\) is a function of both \\( x \\in \\mathbb{R}^d \\) and \\( t \\in \\mathbb{R} \\), then \\( JF \\), \\( \\Delta f \\) and \\( \\nabla \\cdot F \\) correspond to \\( F \\) as a function of \\( x \\), whereas \\( t \\) is kept fixed. In particular,\n\\( (JF(x, t))_{ij} = \\frac{\\partial f_i(x, t)}{\\partial x_j} \\), \\( (\\Delta F(x, t))_i = \\sum_{j=1}^{n} \\frac{\\partial^2 f_i(x, t)}{\\partial x_j^2} \\), \\( \\nabla \\cdot F = \\sum_{i=1}^{n} \\frac{\\partial f_i(x, t)}{\\partial x_i} \\).\n\nWe use the celebrated Ito's lemma and some of its immediate generalizations:\n\nLemma A.2 (Ito's Lemma). Let \\( x_t \\) be a stochastic process \\( x_t \\in \\mathbb{R}^d = dx_t = \\mu(x_t, t)dt + g(t)dB_t \\), \\( \\mathbb{R}^d \\), that is defined by the following SDE:\nwhere \\( B_t \\) is a standard Brownian motion. Let \\( f : \\mathbb{R}^d \\times \\mathbb{R} \\rightarrow \\mathbb{R} \\). Then,\n\\( df(x_t, t) = \\frac{d f}{dt} + \\Delta f dt + g(t) \\nabla_x f^T dB_t. \\)\n\nFurther, if \\( F : \\mathbb{R}^d \\times \\mathbb{R} \\rightarrow \\mathbb{R}^d \\) is a multi-valued function, then\n\\( dF(x_t, t) = \\frac{d F}{dt} + JF \\mu + g(t)^2 \\Delta F dt + g(t)JF dB_t. \\)\n\nLastly, if \\( x_t \\) is instead defined with a reverse noise,\n\\( dx_t = \\mu(x_t, t)dt + g(t)dB_t \\),\nthen the multi-valued Ito's lemma is modified as follows:\n\\( dF(x_t, t) = \\frac{d F}{dt} + JF \\mu - g(t)^2 \\Delta F dt + g(t)JF dB_t. \\) (12)\n\n16", "md": "Preliminaries on diffusion processes\n\nIn the next definition we define for a function \\( F : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) its Jacobian \\( JF \\), its divergence \\( \\nabla \\cdot F \\) and its Laplacian \\( \\Delta F \\) that is computed separately on each coordinate of \\( F \\):\n\nDefinition A.1. Given a function \\( F = (f_1, \\ldots, f_n) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^n \\), denote by \\( JF : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d \\times d} \\) its Jacobian:\nThe divergence of \\( F \\) is defined as \\( \\nabla \\cdot F(x) := \\sum_{i=1}^{n} \\frac{\\partial f_i(x)}{\\partial x_i} \\).\nDenote by \\( \\Delta F : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) the function whose \\( i \\)th entry is the Laplacian of \\( f_i \\):\n\\( (\\Delta F(x))_i = \\sum_{j=1}^{n} \\frac{\\partial^2 f_i(x)}{\\partial x_j^2} \\).\n\nIf \\( F \\) is a function of both \\( x \\in \\mathbb{R}^d \\) and \\( t \\in \\mathbb{R} \\), then \\( JF \\), \\( \\Delta f \\) and \\( \\nabla \\cdot F \\) correspond to \\( F \\) as a function of \\( x \\), whereas \\( t \\) is kept fixed. In particular,\n\\( (JF(x, t))_{ij} = \\frac{\\partial f_i(x, t)}{\\partial x_j} \\), \\( (\\Delta F(x, t))_i = \\sum_{j=1}^{n} \\frac{\\partial^2 f_i(x, t)}{\\partial x_j^2} \\), \\( \\nabla \\cdot F = \\sum_{i=1}^{n} \\frac{\\partial f_i(x, t)}{\\partial x_i} \\).\n\nWe use the celebrated Ito's lemma and some of its immediate generalizations:\n\nLemma A.2 (Ito's Lemma). Let \\( x_t \\) be a stochastic process \\( x_t \\in \\mathbb{R}^d = dx_t = \\mu(x_t, t)dt + g(t)dB_t \\), \\( \\mathbb{R}^d \\), that is defined by the following SDE:\nwhere \\( B_t \\) is a standard Brownian motion. Let \\( f : \\mathbb{R}^d \\times \\mathbb{R} \\rightarrow \\mathbb{R} \\). Then,\n\\( df(x_t, t) = \\frac{d f}{dt} + \\Delta f dt + g(t) \\nabla_x f^T dB_t. \\)\n\nFurther, if \\( F : \\mathbb{R}^d \\times \\mathbb{R} \\rightarrow \\mathbb{R}^d \\) is a multi-valued function, then\n\\( dF(x_t, t) = \\frac{d F}{dt} + JF \\mu + g(t)^2 \\Delta F dt + g(t)JF dB_t. \\)\n\nLastly, if \\( x_t \\) is instead defined with a reverse noise,\n\\( dx_t = \\mu(x_t, t)dt + g(t)dB_t \\),\nthen the multi-valued Ito's lemma is modified as follows:\n\\( dF(x_t, t) = \\frac{d F}{dt} + JF \\mu - g(t)^2 \\Delta F dt + g(t)JF dB_t. \\) (12)\n\n16"}]}, {"page": 17, "text": "      Lastly, we present the Fokker-Planck equation which states that the probability distribution that corre-\n sponds to diffusion processes satisfy a certain partial differential equation:\n Lemma A.3 (Fokker-Planck equation). Let xt be defined by\n                                                                 dxt = \u00b5(xt, t)dt + g(t)dBt,\nwhere xt, \u00b5(x, t) \u2208           Rd and Bt is a Brownian motion in Rd. Denote by p(x, t) the density at point x on time t.\nThen,                   \u2202                                                            \u25b3p(x, t) = \u2212p\u2207            \u00b7 \u00b5 \u2212    \u00b5\u2207    \u00b7 p + g(t)2     \u25b3p.\n                       \u2202tp(x, t) = \u2212\u2207           \u00b7 (\u00b5(x, t)p(x, t)) + g(t)2      2                                                        2\n Preliminaries on analytic functions\n Definition A.4. A function f : Rd \u2192                         R is analytic on Rd if for any x0, x \u2208                    Rd, the Taylor series of f around\n x0, evaluated at x, converges to f(x). We say that F = (f1, . . . , fn): Rd \u2192                                         Rd is an analytic function if fi is\n analytic for all i \u2208          {1, . . . , n}.\n      The following holds:\n Lemma A.5. If F, G: Rd \u2192                        Rd are two analytic functions and if F = G for all x \u2208                                       U where U \u2286    Rd,\n U \u0338= 0, is an open set, then F = G on all Rd.\n      This is a well known result and a proof sketch was given in Section 3.\n The heat equation.                    The following is a Folklore lemma on the uniqueness of the solutions to the heat\n equation:\n Lemma A.6. Let p and p\u2032 be two continuous functions on Rd \u00d7 [t0, 1] that satisfy the heat equation\n                                                                          \u2202p\nFurther, assume that p(\u00b7, t0) = p\u2032(\u00b7, t0). Then, p = p\u2032 for all t \u2208       \u2202t = g(t)2  2    \u25b3p.       [t0, 1].                                               (13)\n A.2         Main proof\n In what appears below we denote\n                                                                     s(x, t) := h(x, t) \u2212  \u03c32t     x.                                                       (14)\nWe start by claiming that if h satisfies Property 1, then s satisfies the PDE Eq. (10): (proof in Section A.3)\n Lemma A.7. Let h satisfy Property 1 and define s according to Eq. (14). Then, s satisfies Eq. (10).\n      Next, we claim that the score function of any diffusion process satisfies the PDE Eq. (10): (proof in\n Section A.4)\n Lemma A.8. Let s be the score function of some diffusion process that is defined by Eq. (4). Then, s satisfies\nthe PDE Eq. (10).\n      To complete the first part of the proof, denote by p(\u00b7, t) the probability distribution such that s(x, t) =\n \u2207  log p(x, t), whose existence follows from Property 2. We would like to argue that {p(\u00b7, t)}t\u2208(0,1] corresponds\n the probability density of the diffusion\n                                                                          dxt = g(t)dBt.                                                                    (15)\n It suffices to show that for any t0 > 0, {p(\u00b7, t)}t\u2208(t0,1] corresponds to the same diffusion. To show the\n latter, let t0 \u2208       (0, 1) and consider the diffusion process according to Eq. (15) with the initial condition that\n                                                                                    17", "md": "# Math Equations\n\n## Fokker-Planck Equation\n\nLemma A.3 (Fokker-Planck equation). Let \\( x_t \\) be defined by\n\n$$\ndx_t = \\mu(x_t, t)dt + g(t)dB_t,\n$$\nwhere \\( x_t, \\mu(x, t) \\in \\mathbb{R}^d \\) and \\( B_t \\) is a Brownian motion in \\( \\mathbb{R}^d \\). Denote by \\( p(x, t) \\) the density at point \\( x \\) on time \\( t \\).\n\nThen,\n\n$$\n\\begin{align*}\n\\frac{\\partial p(x, t)}{\\partial t} &= -\\nabla \\cdot (\\mu(x, t)p(x, t)) + g(t)^2 \\nabla^2 p.\n\\end{align*}\n$$\n\n## Preliminaries on Analytic Functions\n\nDefinition A.4. A function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) is analytic on \\( \\mathbb{R}^d \\) if for any \\( x_0, x \\in \\mathbb{R}^d \\), the Taylor series of \\( f \\) around \\( x_0 \\), evaluated at \\( x \\), converges to \\( f(x) \\). We say that \\( F = (f_1, . . . , f_n) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) is an analytic function if \\( f_i \\) is analytic for all \\( i \\in \\{1, . . . , n\\} \\).\n\nThe following holds:\n\nLemma A.5. If \\( F, G : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) are two analytic functions and if \\( F = G \\) for all \\( x \\in U \\) where \\( U \\subseteq \\mathbb{R}^d \\), \\( U \\neq \\emptyset \\), is an open set, then \\( F = G \\) on all \\( \\mathbb{R}^d \\).\n\n## The Heat Equation\n\nLemma A.6. Let \\( p \\) and \\( p' \\) be two continuous functions on \\( \\mathbb{R}^d \\times [t_0, 1] \\) that satisfy the heat equation\n\n$$\n\\frac{\\partial p}{\\partial t} = g(t)^2 \\nabla^2 p. \\quad [t_0, 1].\n$$\n\n## Main Proof\n\nIn what appears below we denote\n\n$$\ns(x, t) := h(x, t) - \\sigma^2 t x. \\quad (14)\n$$\nWe start by claiming that if \\( h \\) satisfies Property 1, then \\( s \\) satisfies the PDE Eq. (10): (proof in Section A.3)\n\nLemma A.7. Let \\( h \\) satisfy Property 1 and define \\( s \\) according to Eq. (14). Then, \\( s \\) satisfies Eq. (10).\n\nNext, we claim that the score function of any diffusion process satisfies the PDE Eq. (10): (proof in Section A.4)\n\nLemma A.8. Let \\( s \\) be the score function of some diffusion process that is defined by Eq. (4). Then, \\( s \\) satisfies the PDE Eq. (10).\n\nTo complete the first part of the proof, denote by \\( p(\\cdot, t) \\) the probability distribution such that \\( s(x, t) = \\nabla \\log p(x, t) \\), whose existence follows from Property 2. We would like to argue that \\( \\{ p(\\cdot, t) \\}_{t \\in (0,1]} \\) corresponds to the probability density of the diffusion\n\n$$\ndx_t = g(t)dB_t. \\quad (15)\n$$\nIt suffices to show that for any \\( t_0 > 0 \\), \\( \\{ p(\\cdot, t) \\}_{t \\in (t_0,1]} \\) corresponds to the same diffusion. To show the latter, let \\( t_0 \\in (0, 1) \\) and consider the diffusion process according to Eq. (15) with the initial condition that\n\n$$\n17\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "heading", "lvl": 2, "value": "Fokker-Planck Equation", "md": "## Fokker-Planck Equation"}, {"type": "text", "value": "Lemma A.3 (Fokker-Planck equation). Let \\( x_t \\) be defined by\n\n$$\ndx_t = \\mu(x_t, t)dt + g(t)dB_t,\n$$\nwhere \\( x_t, \\mu(x, t) \\in \\mathbb{R}^d \\) and \\( B_t \\) is a Brownian motion in \\( \\mathbb{R}^d \\). Denote by \\( p(x, t) \\) the density at point \\( x \\) on time \\( t \\).\n\nThen,\n\n$$\n\\begin{align*}\n\\frac{\\partial p(x, t)}{\\partial t} &= -\\nabla \\cdot (\\mu(x, t)p(x, t)) + g(t)^2 \\nabla^2 p.\n\\end{align*}\n$$", "md": "Lemma A.3 (Fokker-Planck equation). Let \\( x_t \\) be defined by\n\n$$\ndx_t = \\mu(x_t, t)dt + g(t)dB_t,\n$$\nwhere \\( x_t, \\mu(x, t) \\in \\mathbb{R}^d \\) and \\( B_t \\) is a Brownian motion in \\( \\mathbb{R}^d \\). Denote by \\( p(x, t) \\) the density at point \\( x \\) on time \\( t \\).\n\nThen,\n\n$$\n\\begin{align*}\n\\frac{\\partial p(x, t)}{\\partial t} &= -\\nabla \\cdot (\\mu(x, t)p(x, t)) + g(t)^2 \\nabla^2 p.\n\\end{align*}\n$$"}, {"type": "heading", "lvl": 2, "value": "Preliminaries on Analytic Functions", "md": "## Preliminaries on Analytic Functions"}, {"type": "text", "value": "Definition A.4. A function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) is analytic on \\( \\mathbb{R}^d \\) if for any \\( x_0, x \\in \\mathbb{R}^d \\), the Taylor series of \\( f \\) around \\( x_0 \\), evaluated at \\( x \\), converges to \\( f(x) \\). We say that \\( F = (f_1, . . . , f_n) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) is an analytic function if \\( f_i \\) is analytic for all \\( i \\in \\{1, . . . , n\\} \\).\n\nThe following holds:\n\nLemma A.5. If \\( F, G : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) are two analytic functions and if \\( F = G \\) for all \\( x \\in U \\) where \\( U \\subseteq \\mathbb{R}^d \\), \\( U \\neq \\emptyset \\), is an open set, then \\( F = G \\) on all \\( \\mathbb{R}^d \\).", "md": "Definition A.4. A function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) is analytic on \\( \\mathbb{R}^d \\) if for any \\( x_0, x \\in \\mathbb{R}^d \\), the Taylor series of \\( f \\) around \\( x_0 \\), evaluated at \\( x \\), converges to \\( f(x) \\). We say that \\( F = (f_1, . . . , f_n) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) is an analytic function if \\( f_i \\) is analytic for all \\( i \\in \\{1, . . . , n\\} \\).\n\nThe following holds:\n\nLemma A.5. If \\( F, G : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d \\) are two analytic functions and if \\( F = G \\) for all \\( x \\in U \\) where \\( U \\subseteq \\mathbb{R}^d \\), \\( U \\neq \\emptyset \\), is an open set, then \\( F = G \\) on all \\( \\mathbb{R}^d \\)."}, {"type": "heading", "lvl": 2, "value": "The Heat Equation", "md": "## The Heat Equation"}, {"type": "text", "value": "Lemma A.6. Let \\( p \\) and \\( p' \\) be two continuous functions on \\( \\mathbb{R}^d \\times [t_0, 1] \\) that satisfy the heat equation\n\n$$\n\\frac{\\partial p}{\\partial t} = g(t)^2 \\nabla^2 p. \\quad [t_0, 1].\n$$", "md": "Lemma A.6. Let \\( p \\) and \\( p' \\) be two continuous functions on \\( \\mathbb{R}^d \\times [t_0, 1] \\) that satisfy the heat equation\n\n$$\n\\frac{\\partial p}{\\partial t} = g(t)^2 \\nabla^2 p. \\quad [t_0, 1].\n$$"}, {"type": "heading", "lvl": 2, "value": "Main Proof", "md": "## Main Proof"}, {"type": "text", "value": "In what appears below we denote\n\n$$\ns(x, t) := h(x, t) - \\sigma^2 t x. \\quad (14)\n$$\nWe start by claiming that if \\( h \\) satisfies Property 1, then \\( s \\) satisfies the PDE Eq. (10): (proof in Section A.3)\n\nLemma A.7. Let \\( h \\) satisfy Property 1 and define \\( s \\) according to Eq. (14). Then, \\( s \\) satisfies Eq. (10).\n\nNext, we claim that the score function of any diffusion process satisfies the PDE Eq. (10): (proof in Section A.4)\n\nLemma A.8. Let \\( s \\) be the score function of some diffusion process that is defined by Eq. (4). Then, \\( s \\) satisfies the PDE Eq. (10).\n\nTo complete the first part of the proof, denote by \\( p(\\cdot, t) \\) the probability distribution such that \\( s(x, t) = \\nabla \\log p(x, t) \\), whose existence follows from Property 2. We would like to argue that \\( \\{ p(\\cdot, t) \\}_{t \\in (0,1]} \\) corresponds to the probability density of the diffusion\n\n$$\ndx_t = g(t)dB_t. \\quad (15)\n$$\nIt suffices to show that for any \\( t_0 > 0 \\), \\( \\{ p(\\cdot, t) \\}_{t \\in (t_0,1]} \\) corresponds to the same diffusion. To show the latter, let \\( t_0 \\in (0, 1) \\) and consider the diffusion process according to Eq. (15) with the initial condition that\n\n$$\n17\n$$", "md": "In what appears below we denote\n\n$$\ns(x, t) := h(x, t) - \\sigma^2 t x. \\quad (14)\n$$\nWe start by claiming that if \\( h \\) satisfies Property 1, then \\( s \\) satisfies the PDE Eq. (10): (proof in Section A.3)\n\nLemma A.7. Let \\( h \\) satisfy Property 1 and define \\( s \\) according to Eq. (14). Then, \\( s \\) satisfies Eq. (10).\n\nNext, we claim that the score function of any diffusion process satisfies the PDE Eq. (10): (proof in Section A.4)\n\nLemma A.8. Let \\( s \\) be the score function of some diffusion process that is defined by Eq. (4). Then, \\( s \\) satisfies the PDE Eq. (10).\n\nTo complete the first part of the proof, denote by \\( p(\\cdot, t) \\) the probability distribution such that \\( s(x, t) = \\nabla \\log p(x, t) \\), whose existence follows from Property 2. We would like to argue that \\( \\{ p(\\cdot, t) \\}_{t \\in (0,1]} \\) corresponds to the probability density of the diffusion\n\n$$\ndx_t = g(t)dB_t. \\quad (15)\n$$\nIt suffices to show that for any \\( t_0 > 0 \\), \\( \\{ p(\\cdot, t) \\}_{t \\in (t_0,1]} \\) corresponds to the same diffusion. To show the latter, let \\( t_0 \\in (0, 1) \\) and consider the diffusion process according to Eq. (15) with the initial condition that\n\n$$\n17\n$$"}]}, {"page": 18, "text": " xt0 \u223c  p(\u00b7, t0). Denote its score function by s\u2032 and notice that it satisfies the PDE Eq. (10) and the initial\n condition s\u2032(x, t0) = \u2207x log p(x, t0) = s(x, t0), where the first equality follows from the definition of a score\n function and the second from the construction of p(x, t0). Further, recall that s(x, t) satisfies the same PDE\n Eq. (10) by Lemma A.3. Next we will show that s = s\u2032 for all t \u2208          [t0, 1], and this will follow from the following\n lemma: (proof in Section A.5)\n Lemma A.9. Let s and s\u2032 be two solutions for the PDE (10) on the domain Rd \u00d7 [t0, 1] that satisfy the\n same initial condition at t0: s(x, t0) = s\u2032(x, t0) for all x. Further, assume that for all t \u2208           [t0, 1] there exist\n probability densities p(\u00b7, t) and p\u2032(\u00b7, t) such that s(x, t) = \u2207x log p(x, t) and s\u2032(x, t) = \u2207x log p\u2032(x, t) for all x.\nThen, s = s\u2032 on all of Rd \u00d7 [t0, 1].\n     Then, by uniqueness of the PDE one obtains that s = s\u2032 for all t \u2208               [t0, 1]. Hence, s is the score of a\n diffusion for all t \u2265 t0 and this holds for any t0 > 0, hence this holds for any t > 0. This concludes the proof\n of the first part of the theorem.\n     For the second part, let s\u2217   denote some score function of a diffusion process that satisfies Eq. (4). Assume\n that for some t0 > 0 and some open subset U \u2286             Rd, s = s\u2217, namely s(x, t0) = s\u2217(x, t0) for all t0 > 0 and\n all x \u2208 U. First, we would like to argue that if s(x, t) is the score function of some diffusion process that\n satisfies Eq. (4), then for any t0 > 0 it holds that s(x, t0) is an analytic function (proof in Section A.6)\n Lemma A.10. Let xt obey the SDE Eq. (4) with the initial condition x0 \u223c                    \u00b50. Let t > 0 and let s(x, t)\ndenote the score function of xt, namely, s(x, t) = \u2207x log p(x, t) where p(x, t) is the density of xt. Assume\nthat \u00b50 is a bounded-support distribution. Then, s(x, t) is an analytic function.\n     Since both s and s\u2217     are scores of diffusion processes, then s(x, t0) and s\u2217(x, t0) are analytic functions.\n Using the fact that s = s\u2217    on U \u00d7 {t0} and using Lemma A.5 we derive that s(x, t0) = s\u2217(x, t0) for all x. Let\n p and p\u2217   denote the densities that correspond to the score functions s and s\u2217             and by definition of a score\n function, we obtain that for all x,\n                                 \u2207log p(x, t0) = s(x, t0) = s\u2217(x, t0) = \u2207log p\u2217(x, t0),\n which implies, by integration, that\n                                              log p(x, t0) = log p\u2217(x, t0) + c\n for some constant c \u2208    R. However, c = 0. Indeed,\n                 1 =     p(x, t0)dx =      elog p(x,t0)dx =    elog p\u2217(x,t0)+cdx =     p\u2217(x, t0)ecdx = ec,\n which implies that c = 0 as required. As a consequence, the following lemma implies that p(x, 0) = p\u2217(x, 0)\n for all x (proof in Section A.7):\n Lemma A.11. Let xt and yt be stochastic processes that follow Eq. (4) with initial conditions x0 \u223c                  \u00b50 and\n y0 \u223c  \u00b5\u2032\n        0 and assume that \u00b50 and \u00b5\u2032     0 are bounded-support. Assume that for some t0 > 0, xt          0 and yt0 have the\n same distribution. Then, \u00b50 = \u00b5\u2032     0.\n \u02dc   Without loss of generality, one can replace 0 with any \u02dc       t \u2208 (0, t0), to obtain that p(x, \u02dct) = p\u2217(x, \u02dct) for any\n t \u2208 [0, t0]. Now, p(x, t0) is analytic, from Lemma A.5, hence it is continuous. Consequently, Lemma A.6\n implies that p = p\u2217      in Rd \u00d7 [t0, 1]. This concludes that p = p\u2217           in all the domain, which implies that\n s = \u2207log p = \u2207log p\u2217      = s\u2217, as required.\n A.3      Proof of Lemma A.7\n We use Ito\u2019s lemma, and in particular Eq. (12), to get a PDE for the function h(xt, t) where xt satisfies the\n stochastic process\n                                            dxt = \u2212g(t)2s(xt, t)dt + g(t)dBt.\n                                                              18", "md": "# Math Equations\n\n$$s'(x, t_0) = \\nabla_x \\log p(x, t_0) = s(x, t_0)$$\n\n$$s(x, t) = \\nabla_x \\log p(x, t)$$\n\n$$s'(x, t) = \\nabla_x \\log p'(x, t)$$\n\n$$\\nabla \\log p(x, t_0) = s(x, t_0) = s'(x, t_0) = \\nabla \\log p'(x, t_0)$$\n\n$$\\log p(x, t_0) = \\log p'(x, t_0) + c$$\n\n$$1 = \\int p(x, t_0)dx = \\int e^{\\log p(x,t_0)}dx = \\int e^{\\log p'(x,t_0)+c}dx = \\int p'(x, t_0)e^cdx = e^c$$\n\n$$p(x, 0) = p'(x, 0)$$\n\n$$p(x, \\tilde{t}) = p^*(x, \\tilde{t})$$\n\n$$s = \\nabla \\log p = \\nabla \\log p^* = s^*$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "$$s'(x, t_0) = \\nabla_x \\log p(x, t_0) = s(x, t_0)$$\n\n$$s(x, t) = \\nabla_x \\log p(x, t)$$\n\n$$s'(x, t) = \\nabla_x \\log p'(x, t)$$\n\n$$\\nabla \\log p(x, t_0) = s(x, t_0) = s'(x, t_0) = \\nabla \\log p'(x, t_0)$$\n\n$$\\log p(x, t_0) = \\log p'(x, t_0) + c$$\n\n$$1 = \\int p(x, t_0)dx = \\int e^{\\log p(x,t_0)}dx = \\int e^{\\log p'(x,t_0)+c}dx = \\int p'(x, t_0)e^cdx = e^c$$\n\n$$p(x, 0) = p'(x, 0)$$\n\n$$p(x, \\tilde{t}) = p^*(x, \\tilde{t})$$\n\n$$s = \\nabla \\log p = \\nabla \\log p^* = s^*$$", "md": "$$s'(x, t_0) = \\nabla_x \\log p(x, t_0) = s(x, t_0)$$\n\n$$s(x, t) = \\nabla_x \\log p(x, t)$$\n\n$$s'(x, t) = \\nabla_x \\log p'(x, t)$$\n\n$$\\nabla \\log p(x, t_0) = s(x, t_0) = s'(x, t_0) = \\nabla \\log p'(x, t_0)$$\n\n$$\\log p(x, t_0) = \\log p'(x, t_0) + c$$\n\n$$1 = \\int p(x, t_0)dx = \\int e^{\\log p(x,t_0)}dx = \\int e^{\\log p'(x,t_0)+c}dx = \\int p'(x, t_0)e^cdx = e^c$$\n\n$$p(x, 0) = p'(x, 0)$$\n\n$$p(x, \\tilde{t}) = p^*(x, \\tilde{t})$$\n\n$$s = \\nabla \\log p = \\nabla \\log p^* = s^*$$"}]}, {"page": 19, "text": " Ito\u2019s formula yields that\n                                           dh(xt, t) =       \u2202h \u2202t \u2212     g(t)2JF s \u2212       g(t)2   \u25b3h       dt + \u03c3Jhd \u00af    Bt.\n                                                                                              2\n Since (h, s) satisfies Property 1 and using Lemma 3.1, h is a reverse martingale which implies that the term\n that multiplies dt has to equal zero. In particular, we have that\n                                                              \u2202h\n                                                              \u2202t \u2212     g(t)2Jhs \u2212        g(t)2   \u25b3h = 0.                                       (16)\n                                                                                            2\n By Eq. (14),\nTherefore,                                                                   s = h \u2212  \u03c32t x  .\n                                                                           h = x + \u03c32      t s.\n Substituting this in Eq. (16) and using the relation d\u03c32                               t /dt = g(t)2 that follows from Eq. (14), one obtains\n that\n                                             0 = \u2202  \u2202t(x + \u03c32     t s) \u2212   g(t)2Jx+\u03c32     t ss \u2212    g(t)2   \u25b3(x + \u03c32     t s)\n                                                                                                       2\n                                                = g(t)2s + \u03c32      t  \u2202s                        t Js)s \u2212     g(t)2\u03c32  t  \u25b3s\n                                                                      \u2202t \u2212    g(t)2(I + \u03c32                        2\n                                                = \u03c32  t \u2202s                t Jss \u2212     g(t)2\u03c32   t \u25b3s.\n Dividing by \u03c32       t , we get that                   \u2202t \u2212  \u2202sg(t)2\u03c32                    2\n                                                              \u2202t \u2212     g(t)2Jss \u2212       g(t)2    \u25b3s = 0,\n                                                                                           2\nwhich is what we wanted to prove.\n A.4         Proof of Lemma A.8\nWe present as a consequence of the Fokker-Plank equation (Lemma A.3) a PDE for the log density log p:\n Lemma A.12. Let xt be defined by\n                                                                 dxt = \u00b5(xt, t)dt + g(t)dBt.\nThen,                                \u2202  log p   = \u2212\u2207       \u00b7 \u00b5 \u2212   \u00b5\u2207    \u00b7 log p + g(t)2\u2225\u2207          log p\u22252    + g(t)2\u25b3        log p\n                                        \u2202t                                                       2                          2\n Proof. We would like to replace the partial derivatives of p that appears in Lemma A.3 with the partial\n derivatives of log p. Using the formula\n                                                                          \u2202  log p   = 1    \u2202p\n                                                                             \u2202t          p  \u2202t ,\n one obtains that\n                                                                          \u2202p\n                                                                           \u2202t = p\u2202      log p   .\n                                                                                         \u2202t\n Similarly,\n                                                                           \u2202p    = p\u2202     log p                                                (17)\n                                                                          \u2202xi            \u2202xi\n                                                                                    19", "md": "Ito\u2019s formula yields that\n\n$$\ndh(x_t, t) = \\frac{\\partial h}{\\partial t} - g(t)^2 JF_s - g(t)^2 \\Delta h dt + \\sigma Jh \\bar{B}_t.\n$$\nSince (h, s) satisfies Property 1 and using Lemma 3.1, h is a reverse martingale which implies that the term that multiplies dt has to equal zero. In particular, we have that\n\n$$\n\\frac{\\partial h}{\\partial t} - g(t)^2 Jhs - g(t)^2 \\Delta h = 0. \\quad (16)\n$$\nBy Eq. (14),\n\nTherefore,\n\n$$\ns = h - \\sigma^2 t x, \\quad h = x + \\sigma^2 t s.\n$$\nSubstituting this in Eq. (16) and using the relation $\\frac{d\\sigma^2_t}{dt} = g(t)^2$ that follows from Eq. (14), one obtains that\n\n$$\n\\begin{align*}\n0 & = \\frac{\\partial}{\\partial t}(x + \\sigma^2 t s) - g(t)^2 J(x + \\sigma^2 t s) - g(t)^2 \\Delta(x + \\sigma^2 t s) \\\\\n& = g(t)^2 s + \\sigma^2 t \\frac{\\partial s}{\\partial t} - g(t)^2 \\sigma^2 t \\Delta s \\\\\n& = \\sigma^2 t \\frac{\\partial s}{\\partial t} - g(t)^2 \\sigma^2 t \\Delta s.\n\\end{align*}\n$$\nDividing by $\\sigma^2 t$, we get that\n\n$$\n\\frac{\\partial t}{\\partial t} - \\frac{\\partial s}{\\partial t} g(t)^2 \\sigma^2 - g(t)^2 Jss - g(t)^2 \\Delta s = 0,\n$$\nwhich is what we wanted to prove.\n\nA.4 Proof of Lemma A.8\n\nWe present as a consequence of the Fokker-Plank equation (Lemma A.3) a PDE for the log density $\\log p$:\n\nLemma A.12. Let $x_t$ be defined by\n\n$$\ndx_t = \\mu(x_t, t) dt + g(t) dB_t.\n$$\nThen,\n\n$$\n\\frac{\\partial \\log p}{\\partial t} = -\\nabla \\cdot \\mu - \\mu \\nabla \\cdot \\log p + g(t)^2 \\|\\nabla \\log p\\|^2 + g(t)^2 \\Delta \\log p\n$$\nProof. We would like to replace the partial derivatives of $p$ that appears in Lemma A.3 with the partial derivatives of $\\log p$. Using the formula\n\n$$\n\\frac{\\partial \\log p}{\\partial t} = \\frac{1}{p} \\frac{\\partial p}{\\partial t},\n$$\none obtains that\n\n$$\n\\frac{\\partial p}{\\partial t} = p \\frac{\\partial \\log p}{\\partial t}.\n$$\nSimilarly,\n\n$$\n\\frac{\\partial p}{\\partial x_i} = p \\frac{\\partial \\log p}{\\partial x_i} \\quad (17)\n$$", "images": [], "items": [{"type": "text", "value": "Ito\u2019s formula yields that\n\n$$\ndh(x_t, t) = \\frac{\\partial h}{\\partial t} - g(t)^2 JF_s - g(t)^2 \\Delta h dt + \\sigma Jh \\bar{B}_t.\n$$\nSince (h, s) satisfies Property 1 and using Lemma 3.1, h is a reverse martingale which implies that the term that multiplies dt has to equal zero. In particular, we have that\n\n$$\n\\frac{\\partial h}{\\partial t} - g(t)^2 Jhs - g(t)^2 \\Delta h = 0. \\quad (16)\n$$\nBy Eq. (14),\n\nTherefore,\n\n$$\ns = h - \\sigma^2 t x, \\quad h = x + \\sigma^2 t s.\n$$\nSubstituting this in Eq. (16) and using the relation $\\frac{d\\sigma^2_t}{dt} = g(t)^2$ that follows from Eq. (14), one obtains that\n\n$$\n\\begin{align*}\n0 & = \\frac{\\partial}{\\partial t}(x + \\sigma^2 t s) - g(t)^2 J(x + \\sigma^2 t s) - g(t)^2 \\Delta(x + \\sigma^2 t s) \\\\\n& = g(t)^2 s + \\sigma^2 t \\frac{\\partial s}{\\partial t} - g(t)^2 \\sigma^2 t \\Delta s \\\\\n& = \\sigma^2 t \\frac{\\partial s}{\\partial t} - g(t)^2 \\sigma^2 t \\Delta s.\n\\end{align*}\n$$\nDividing by $\\sigma^2 t$, we get that\n\n$$\n\\frac{\\partial t}{\\partial t} - \\frac{\\partial s}{\\partial t} g(t)^2 \\sigma^2 - g(t)^2 Jss - g(t)^2 \\Delta s = 0,\n$$\nwhich is what we wanted to prove.\n\nA.4 Proof of Lemma A.8\n\nWe present as a consequence of the Fokker-Plank equation (Lemma A.3) a PDE for the log density $\\log p$:\n\nLemma A.12. Let $x_t$ be defined by\n\n$$\ndx_t = \\mu(x_t, t) dt + g(t) dB_t.\n$$\nThen,\n\n$$\n\\frac{\\partial \\log p}{\\partial t} = -\\nabla \\cdot \\mu - \\mu \\nabla \\cdot \\log p + g(t)^2 \\|\\nabla \\log p\\|^2 + g(t)^2 \\Delta \\log p\n$$\nProof. We would like to replace the partial derivatives of $p$ that appears in Lemma A.3 with the partial derivatives of $\\log p$. Using the formula\n\n$$\n\\frac{\\partial \\log p}{\\partial t} = \\frac{1}{p} \\frac{\\partial p}{\\partial t},\n$$\none obtains that\n\n$$\n\\frac{\\partial p}{\\partial t} = p \\frac{\\partial \\log p}{\\partial t}.\n$$\nSimilarly,\n\n$$\n\\frac{\\partial p}{\\partial x_i} = p \\frac{\\partial \\log p}{\\partial x_i} \\quad (17)\n$$", "md": "Ito\u2019s formula yields that\n\n$$\ndh(x_t, t) = \\frac{\\partial h}{\\partial t} - g(t)^2 JF_s - g(t)^2 \\Delta h dt + \\sigma Jh \\bar{B}_t.\n$$\nSince (h, s) satisfies Property 1 and using Lemma 3.1, h is a reverse martingale which implies that the term that multiplies dt has to equal zero. In particular, we have that\n\n$$\n\\frac{\\partial h}{\\partial t} - g(t)^2 Jhs - g(t)^2 \\Delta h = 0. \\quad (16)\n$$\nBy Eq. (14),\n\nTherefore,\n\n$$\ns = h - \\sigma^2 t x, \\quad h = x + \\sigma^2 t s.\n$$\nSubstituting this in Eq. (16) and using the relation $\\frac{d\\sigma^2_t}{dt} = g(t)^2$ that follows from Eq. (14), one obtains that\n\n$$\n\\begin{align*}\n0 & = \\frac{\\partial}{\\partial t}(x + \\sigma^2 t s) - g(t)^2 J(x + \\sigma^2 t s) - g(t)^2 \\Delta(x + \\sigma^2 t s) \\\\\n& = g(t)^2 s + \\sigma^2 t \\frac{\\partial s}{\\partial t} - g(t)^2 \\sigma^2 t \\Delta s \\\\\n& = \\sigma^2 t \\frac{\\partial s}{\\partial t} - g(t)^2 \\sigma^2 t \\Delta s.\n\\end{align*}\n$$\nDividing by $\\sigma^2 t$, we get that\n\n$$\n\\frac{\\partial t}{\\partial t} - \\frac{\\partial s}{\\partial t} g(t)^2 \\sigma^2 - g(t)^2 Jss - g(t)^2 \\Delta s = 0,\n$$\nwhich is what we wanted to prove.\n\nA.4 Proof of Lemma A.8\n\nWe present as a consequence of the Fokker-Plank equation (Lemma A.3) a PDE for the log density $\\log p$:\n\nLemma A.12. Let $x_t$ be defined by\n\n$$\ndx_t = \\mu(x_t, t) dt + g(t) dB_t.\n$$\nThen,\n\n$$\n\\frac{\\partial \\log p}{\\partial t} = -\\nabla \\cdot \\mu - \\mu \\nabla \\cdot \\log p + g(t)^2 \\|\\nabla \\log p\\|^2 + g(t)^2 \\Delta \\log p\n$$\nProof. We would like to replace the partial derivatives of $p$ that appears in Lemma A.3 with the partial derivatives of $\\log p$. Using the formula\n\n$$\n\\frac{\\partial \\log p}{\\partial t} = \\frac{1}{p} \\frac{\\partial p}{\\partial t},\n$$\none obtains that\n\n$$\n\\frac{\\partial p}{\\partial t} = p \\frac{\\partial \\log p}{\\partial t}.\n$$\nSimilarly,\n\n$$\n\\frac{\\partial p}{\\partial x_i} = p \\frac{\\partial \\log p}{\\partial x_i} \\quad (17)\n$$"}]}, {"page": 20, "text": "which also implies that\n                                                          \u2207p = p\u2207        log p,      \u2207   \u00b7 p = p\u2207      \u00b7 log p.\n Differentiating Eq. (17) again with respect to xi and applying Eq. (17) once more, one obtains that\n                       \u22022  p  =     \u2202       p\u2202   log p      = \u2202p      \u2202  log p   + p\u22022 log p        = p     \u2202      log p    2   + \u22022 log p         .\n                       \u2202x2 i       \u2202x  i        \u2202xi             \u2202xi      \u2202xi              \u2202x2 i                    \u2202xi                \u2202x2  i\n Summing over i, one obtains that\n                                    \u25b3p = p        n     \u2202      log p   2   + \u22022 log p          = p\u2225\u2207      log p\u22252 + p\u25b3         log p.                 (18)\n                                                 i=1          \u2202xi                 \u2202x2 i\n Substituting the partials derivatives of p inside the Fokker-Planck equation in Lemma A.3, one obtains that\n                                p\u2202   log p   = \u2212p\u2207        \u00b7 \u00b5 \u2212   \u00b5(p\u2207     \u00b7 log p) + g(t)2          p\u2225\u2207    log p\u22252 + p\u25b3        log p     .\n                                     \u2202t                                                      2\n Dividing by p, one gets that\n                                    \u2202  log p    = \u2212\u2207      \u00b7 \u00b5 \u2212   \u00b5\u2207    \u00b7 log p + g(t)2\u2225\u2207          log p\u22252     + g(t)2\u25b3       log p.\n                                        \u2202t                                                       2                         2\n as required.\n      We are ready to prove Lemma A.8: Substituting \u00b5 = 0 in Lemma A.12, on obtains that\n                                                      \u2202 log p    = g(t)2\u2225\u2207        log p\u22252     + g(t)2\u25b3       log p.\n                                                         \u2202t                     2                         2\nTaking the gradient with respect to x, one obtains that\n                                                 \u2207\u2202    log p   = g(t)2\u2207\u2225\u2207          log p\u22252     + g(t)2\u2207\u25b3          log p.                              (19)\n                                                       \u2202t                       2                            2\n Since \u2202/\u2202xi commutes with \u2202/\u2202t, it holds that\n                                                               \u2207\u2202     log p   = \u2202                                                                     (20)\n recalling that by definition s = \u2207                   log p. Further, \u2202t          \u2202t\u2207     log p = \u2202s  \u2202t ,\n                         \u2202   \u2225\u2207    log p\u22252 =        n     \u2202     \u2202    log p 2     = 2     n    \u22022 log p    \u2202  log p   = 2(Hlog p\u2207         log p)i,\n                        \u2202xi                       j=1   \u2202xi         \u2202xj                 j=1   \u2202xi\u2202xj        \u2202xj\nwhere for any function f : Rd \u2192                     R, Hf is the Hessian function of f that is defined by\n                                                                        (Hf)ij =          \u22022  f\n                                                                                       \u2202xi\u2202xj\nThis implies that\n Further, notice that                                         \u2207\u2225\u2207      log p\u22252 = 2Hlog p\u2207           log p.\nwhich implies that                                                           Hf = J\u2207f,\n                                                       \u2207\u2225\u2207      log p\u22252 = 2J\u2207log p\u2207            log p = 2Jss.                                          (21)\n                                                                                    20", "md": "# Math Equations\n\nwhich also implies that\n\n$$\\nabla p = p\\nabla \\log p, \\quad \\nabla \\cdot p = p\\nabla \\cdot \\log p.$$\n\nDifferentiating Eq. (17) again with respect to \\(x_i\\) and applying Eq. (17) once more, one obtains that\n\n$$\\frac{\\partial^2 p}{\\partial x_i^2} = \\frac{\\partial}{\\partial x_i} p\\frac{\\partial \\log p}{\\partial x_i} = \\frac{\\partial p}{\\partial \\log p} + p\\frac{\\partial^2 \\log p}{\\partial x_i^2} = p\\left(\\frac{\\partial \\log p}{\\partial x_i}\\right)^2 + \\frac{\\partial^2 \\log p}{\\partial x_i}.$$\n\nSumming over \\(i\\), one obtains that\n\n$$\\Delta p = p \\sum_{i=1}^{n} \\frac{\\partial \\log p}{\\partial x_i}^2 + \\frac{\\partial^2 \\log p}{\\partial x_i} = p\\|\\nabla \\log p\\|^2 + p\\Delta \\log p. \\quad (18)$$\n\nSubstituting the partial derivatives of \\(p\\) inside the Fokker-Planck equation in Lemma A.3, one obtains that\n\n$$p\\frac{\\partial \\log p}{\\partial t} = -p\\nabla \\cdot \\mu - \\mu(p\\nabla \\cdot \\log p) + g(t)^2 p\\|\\nabla \\log p\\|^2 + p\\Delta \\log p.$$\n\nDividing by \\(p\\), one gets that\n\n$$\\frac{\\partial \\log p}{\\partial t} = -\\nabla \\cdot \\mu - \\mu\\nabla \\cdot \\log p + g(t)^2\\|\\nabla \\log p\\|^2 + g(t)^2\\Delta \\log p.$$\n\nAs required.\n\nWe are ready to prove Lemma A.8: Substituting \\(\\mu = 0\\) in Lemma A.12, one obtains that\n\n$$\\frac{\\partial \\log p}{\\partial t} = g(t)^2\\|\\nabla \\log p\\|^2 + g(t)^2\\Delta \\log p.$$\n\nTaking the gradient with respect to \\(x\\), one obtains that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = g(t)^2\\nabla\\|\\nabla \\log p\\|^2 + g(t)^2\\nabla\\Delta \\log p. \\quad (19)$$\n\nSince \\(\\frac{\\partial}{\\partial x_i}\\) commutes with \\(\\frac{\\partial}{\\partial t}\\), it holds that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = \\frac{\\partial}{\\partial t}(\\nabla \\log p)$$\n\nRecalling that by definition \\(s = \\nabla \\log p\\). Further,\n\n$$\\frac{\\partial \\|\\nabla \\log p\\|^2}{\\partial x_i} = \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_i} \\frac{\\partial \\log p}{\\partial x_j} = 2 \\sum_{j=1}^{n} \\frac{\\partial^2 \\log p}{\\partial x_i \\partial x_j} \\frac{\\partial \\log p}{\\partial x_j} = 2(H\\log p\\nabla \\log p)_i,$$\n\nwhere for any function \\(f : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\), \\(Hf\\) is the Hessian function of \\(f\\) that is defined by\n\n$$\\left(Hf\\right)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$$\n\nThis implies that\n\nFurther, notice that\n\n$$\\nabla\\|\\nabla \\log p\\|^2 = 2H\\log p\\nabla \\log p.$$\n\nwhich implies that\n\n$$Hf = J\\nabla f, \\quad \\nabla\\|\\nabla \\log p\\|^2 = 2J\\nabla \\log p\\nabla \\log p = 2Jss. \\quad (21)$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "which also implies that\n\n$$\\nabla p = p\\nabla \\log p, \\quad \\nabla \\cdot p = p\\nabla \\cdot \\log p.$$\n\nDifferentiating Eq. (17) again with respect to \\(x_i\\) and applying Eq. (17) once more, one obtains that\n\n$$\\frac{\\partial^2 p}{\\partial x_i^2} = \\frac{\\partial}{\\partial x_i} p\\frac{\\partial \\log p}{\\partial x_i} = \\frac{\\partial p}{\\partial \\log p} + p\\frac{\\partial^2 \\log p}{\\partial x_i^2} = p\\left(\\frac{\\partial \\log p}{\\partial x_i}\\right)^2 + \\frac{\\partial^2 \\log p}{\\partial x_i}.$$\n\nSumming over \\(i\\), one obtains that\n\n$$\\Delta p = p \\sum_{i=1}^{n} \\frac{\\partial \\log p}{\\partial x_i}^2 + \\frac{\\partial^2 \\log p}{\\partial x_i} = p\\|\\nabla \\log p\\|^2 + p\\Delta \\log p. \\quad (18)$$\n\nSubstituting the partial derivatives of \\(p\\) inside the Fokker-Planck equation in Lemma A.3, one obtains that\n\n$$p\\frac{\\partial \\log p}{\\partial t} = -p\\nabla \\cdot \\mu - \\mu(p\\nabla \\cdot \\log p) + g(t)^2 p\\|\\nabla \\log p\\|^2 + p\\Delta \\log p.$$\n\nDividing by \\(p\\), one gets that\n\n$$\\frac{\\partial \\log p}{\\partial t} = -\\nabla \\cdot \\mu - \\mu\\nabla \\cdot \\log p + g(t)^2\\|\\nabla \\log p\\|^2 + g(t)^2\\Delta \\log p.$$\n\nAs required.\n\nWe are ready to prove Lemma A.8: Substituting \\(\\mu = 0\\) in Lemma A.12, one obtains that\n\n$$\\frac{\\partial \\log p}{\\partial t} = g(t)^2\\|\\nabla \\log p\\|^2 + g(t)^2\\Delta \\log p.$$\n\nTaking the gradient with respect to \\(x\\), one obtains that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = g(t)^2\\nabla\\|\\nabla \\log p\\|^2 + g(t)^2\\nabla\\Delta \\log p. \\quad (19)$$\n\nSince \\(\\frac{\\partial}{\\partial x_i}\\) commutes with \\(\\frac{\\partial}{\\partial t}\\), it holds that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = \\frac{\\partial}{\\partial t}(\\nabla \\log p)$$\n\nRecalling that by definition \\(s = \\nabla \\log p\\). Further,\n\n$$\\frac{\\partial \\|\\nabla \\log p\\|^2}{\\partial x_i} = \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_i} \\frac{\\partial \\log p}{\\partial x_j} = 2 \\sum_{j=1}^{n} \\frac{\\partial^2 \\log p}{\\partial x_i \\partial x_j} \\frac{\\partial \\log p}{\\partial x_j} = 2(H\\log p\\nabla \\log p)_i,$$\n\nwhere for any function \\(f : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\), \\(Hf\\) is the Hessian function of \\(f\\) that is defined by\n\n$$\\left(Hf\\right)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$$\n\nThis implies that\n\nFurther, notice that\n\n$$\\nabla\\|\\nabla \\log p\\|^2 = 2H\\log p\\nabla \\log p.$$\n\nwhich implies that\n\n$$Hf = J\\nabla f, \\quad \\nabla\\|\\nabla \\log p\\|^2 = 2J\\nabla \\log p\\nabla \\log p = 2Jss. \\quad (21)$$", "md": "which also implies that\n\n$$\\nabla p = p\\nabla \\log p, \\quad \\nabla \\cdot p = p\\nabla \\cdot \\log p.$$\n\nDifferentiating Eq. (17) again with respect to \\(x_i\\) and applying Eq. (17) once more, one obtains that\n\n$$\\frac{\\partial^2 p}{\\partial x_i^2} = \\frac{\\partial}{\\partial x_i} p\\frac{\\partial \\log p}{\\partial x_i} = \\frac{\\partial p}{\\partial \\log p} + p\\frac{\\partial^2 \\log p}{\\partial x_i^2} = p\\left(\\frac{\\partial \\log p}{\\partial x_i}\\right)^2 + \\frac{\\partial^2 \\log p}{\\partial x_i}.$$\n\nSumming over \\(i\\), one obtains that\n\n$$\\Delta p = p \\sum_{i=1}^{n} \\frac{\\partial \\log p}{\\partial x_i}^2 + \\frac{\\partial^2 \\log p}{\\partial x_i} = p\\|\\nabla \\log p\\|^2 + p\\Delta \\log p. \\quad (18)$$\n\nSubstituting the partial derivatives of \\(p\\) inside the Fokker-Planck equation in Lemma A.3, one obtains that\n\n$$p\\frac{\\partial \\log p}{\\partial t} = -p\\nabla \\cdot \\mu - \\mu(p\\nabla \\cdot \\log p) + g(t)^2 p\\|\\nabla \\log p\\|^2 + p\\Delta \\log p.$$\n\nDividing by \\(p\\), one gets that\n\n$$\\frac{\\partial \\log p}{\\partial t} = -\\nabla \\cdot \\mu - \\mu\\nabla \\cdot \\log p + g(t)^2\\|\\nabla \\log p\\|^2 + g(t)^2\\Delta \\log p.$$\n\nAs required.\n\nWe are ready to prove Lemma A.8: Substituting \\(\\mu = 0\\) in Lemma A.12, one obtains that\n\n$$\\frac{\\partial \\log p}{\\partial t} = g(t)^2\\|\\nabla \\log p\\|^2 + g(t)^2\\Delta \\log p.$$\n\nTaking the gradient with respect to \\(x\\), one obtains that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = g(t)^2\\nabla\\|\\nabla \\log p\\|^2 + g(t)^2\\nabla\\Delta \\log p. \\quad (19)$$\n\nSince \\(\\frac{\\partial}{\\partial x_i}\\) commutes with \\(\\frac{\\partial}{\\partial t}\\), it holds that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = \\frac{\\partial}{\\partial t}(\\nabla \\log p)$$\n\nRecalling that by definition \\(s = \\nabla \\log p\\). Further,\n\n$$\\frac{\\partial \\|\\nabla \\log p\\|^2}{\\partial x_i} = \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_i} \\frac{\\partial \\log p}{\\partial x_j} = 2 \\sum_{j=1}^{n} \\frac{\\partial^2 \\log p}{\\partial x_i \\partial x_j} \\frac{\\partial \\log p}{\\partial x_j} = 2(H\\log p\\nabla \\log p)_i,$$\n\nwhere for any function \\(f : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\), \\(Hf\\) is the Hessian function of \\(f\\) that is defined by\n\n$$\\left(Hf\\right)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$$\n\nThis implies that\n\nFurther, notice that\n\n$$\\nabla\\|\\nabla \\log p\\|^2 = 2H\\log p\\nabla \\log p.$$\n\nwhich implies that\n\n$$Hf = J\\nabla f, \\quad \\nabla\\|\\nabla \\log p\\|^2 = 2J\\nabla \\log p\\nabla \\log p = 2Jss. \\quad (21)$$"}]}, {"page": 21, "text": " Lastly, we get that by the commutative property of partial derivatives,\n                                                                \u2207\u25b3     log p = \u25b3\u2207        log p = \u25b3s.                      (22)\n Substituting Eq. (20), Eq. (21) and Eq. (22) in Eq. (19), one obtains that\n                                                                 \u2202s\n as required.                                                    \u2202t = g(t)2Jss + g(t)2\u25b3s         2       ,\n A.5         Proof of Lemma A.9\nWe will prove that p and p\u2032 satisfy the same PDE (which is the heat equation). Recall that s and s\u2032 satisfy\n                                                \u2202s                 Jss + 1              = g(t)2        \u2207\u2225s\u22252 + \u25b3s\n By substituting s = \u2207              log p,      \u2202t = g(t)2                   2\u25b3s               2\n                                                   \u2202\u2207    log p   = g(t)2        \u2207\u2225\u2207     log p\u22252 + \u25b3\u2207          log p    .\n                                                        \u2202t              2\n By exchanging the order of derivatives, we obtain that\n                                                    \u2207\u2202    log p    = \u2207g(t)2         \u2225\u2207    log p\u22252 + \u25b3       log p     .\n                                                           \u2202t                2\n By integrating, this implies that\n                                                   \u2202 log p    = g(t)2       \u2225\u2207    log p\u22252 + \u25b3        log p    + c(t),\n                                                      \u2202t             2\nwhere c(t) depends only on t. Eq. (18) shows that\n By substituting this in the equation above, we obtain that     \u25b3   log p = \u25b3p    p \u2212     \u2225\u2207   log p\u22252.\n                                                                 \u2202  log p    = g(t)2     \u25b3p\n                                                                     \u2202t             2      p + c(t).\n By multiplying both sides with p, we get that\n                                                            \u2202p                     = g(t)2     \u25b3p + c(t).                 (23)\n                                                            \u2202t = p\u2202       log p\n                                                                           \u2202t             2\n Since p is a probability distribution,                                   Rd p(x, t)dx = 1,\n therefore,                                             \u2202p(x, t)     dx = \u2202\n                                                             \u2202t               \u2202t    Rd p(x, t)dx = \u22021      \u2202t = 0.\n Integrating over Eq. (23) we obtain that\n                                                     0 =         g(t)2   \u25b3p + c(t)dx = 0 +                 c(t)dx,\n                                                                   2\n                                                                                    21", "md": "# Math Equations\n\nLastly, we get that by the commutative property of partial derivatives,\n\n$$\\nabla \\triangle \\log p = \\triangle \\nabla \\log p = \\triangle s. \\quad (22)$$\nSubstituting Eq. (20), Eq. (21) and Eq. (22) in Eq. (19), one obtains that\n\n$$\\frac{\\partial s}{\\partial t} = g(t)^2 Jss + g(t)^2 \\triangle s^2,$$\nProof of Lemma A.9\n\nWe will prove that p and p' satisfy the same PDE (which is the heat equation). Recall that s and s' satisfy\n\n$$\\frac{\\partial s}{\\partial t} = Jss + 1 = g(t)^2 \\nabla \\|s\\|^2 + \\triangle s$$\nBy substituting $$s = \\nabla \\log p,$$\n\n$$\\frac{\\partial}{\\partial t} \\nabla \\log p = g(t)^2 \\nabla \\| \\nabla \\log p \\|^2 + \\triangle \\nabla \\log p.$$\nBy exchanging the order of derivatives, we obtain that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = \\nabla g(t)^2 \\| \\nabla \\log p \\|^2 + \\triangle \\log p.$$\nBy integrating, this implies that\n\n$$\\frac{\\partial \\log p}{\\partial t} = g(t)^2 \\| \\nabla \\log p \\|^2 + \\triangle \\log p + c(t),$$\nwhere c(t) depends only on t. Eq. (18) shows that\n\nBy substituting this in the equation above, we obtain that\n\n$$\\triangle \\log p = \\triangle p \\frac{p - \\| \\nabla \\log p \\|^2}{\\frac{\\partial \\log p}{\\partial t} = g(t)^2 \\triangle p + c(t). \\quad (23)$$\nSince p is a probability distribution,\n\n$$\\int_{\\mathbb{R}^d} p(x, t)dx = 1,$$\ntherefore,\n\n$$\\frac{\\partial p(x, t)}{\\partial t} dx = \\frac{\\partial}{\\partial t} \\int_{\\mathbb{R}^d} p(x, t)dx = \\frac{\\partial 1}{\\partial t} = 0.$$\nIntegrating over Eq. (23) we obtain that\n\n$$0 = \\int g(t)^2 \\triangle p + c(t)dx = 0 + \\int c(t)dx,$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Lastly, we get that by the commutative property of partial derivatives,\n\n$$\\nabla \\triangle \\log p = \\triangle \\nabla \\log p = \\triangle s. \\quad (22)$$\nSubstituting Eq. (20), Eq. (21) and Eq. (22) in Eq. (19), one obtains that\n\n$$\\frac{\\partial s}{\\partial t} = g(t)^2 Jss + g(t)^2 \\triangle s^2,$$\nProof of Lemma A.9\n\nWe will prove that p and p' satisfy the same PDE (which is the heat equation). Recall that s and s' satisfy\n\n$$\\frac{\\partial s}{\\partial t} = Jss + 1 = g(t)^2 \\nabla \\|s\\|^2 + \\triangle s$$\nBy substituting $$s = \\nabla \\log p,$$\n\n$$\\frac{\\partial}{\\partial t} \\nabla \\log p = g(t)^2 \\nabla \\| \\nabla \\log p \\|^2 + \\triangle \\nabla \\log p.$$\nBy exchanging the order of derivatives, we obtain that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = \\nabla g(t)^2 \\| \\nabla \\log p \\|^2 + \\triangle \\log p.$$\nBy integrating, this implies that\n\n$$\\frac{\\partial \\log p}{\\partial t} = g(t)^2 \\| \\nabla \\log p \\|^2 + \\triangle \\log p + c(t),$$\nwhere c(t) depends only on t. Eq. (18) shows that\n\nBy substituting this in the equation above, we obtain that\n\n$$\\triangle \\log p = \\triangle p \\frac{p - \\| \\nabla \\log p \\|^2}{\\frac{\\partial \\log p}{\\partial t} = g(t)^2 \\triangle p + c(t). \\quad (23)$$\nSince p is a probability distribution,\n\n$$\\int_{\\mathbb{R}^d} p(x, t)dx = 1,$$\ntherefore,\n\n$$\\frac{\\partial p(x, t)}{\\partial t} dx = \\frac{\\partial}{\\partial t} \\int_{\\mathbb{R}^d} p(x, t)dx = \\frac{\\partial 1}{\\partial t} = 0.$$\nIntegrating over Eq. (23) we obtain that\n\n$$0 = \\int g(t)^2 \\triangle p + c(t)dx = 0 + \\int c(t)dx,$$", "md": "Lastly, we get that by the commutative property of partial derivatives,\n\n$$\\nabla \\triangle \\log p = \\triangle \\nabla \\log p = \\triangle s. \\quad (22)$$\nSubstituting Eq. (20), Eq. (21) and Eq. (22) in Eq. (19), one obtains that\n\n$$\\frac{\\partial s}{\\partial t} = g(t)^2 Jss + g(t)^2 \\triangle s^2,$$\nProof of Lemma A.9\n\nWe will prove that p and p' satisfy the same PDE (which is the heat equation). Recall that s and s' satisfy\n\n$$\\frac{\\partial s}{\\partial t} = Jss + 1 = g(t)^2 \\nabla \\|s\\|^2 + \\triangle s$$\nBy substituting $$s = \\nabla \\log p,$$\n\n$$\\frac{\\partial}{\\partial t} \\nabla \\log p = g(t)^2 \\nabla \\| \\nabla \\log p \\|^2 + \\triangle \\nabla \\log p.$$\nBy exchanging the order of derivatives, we obtain that\n\n$$\\nabla \\frac{\\partial \\log p}{\\partial t} = \\nabla g(t)^2 \\| \\nabla \\log p \\|^2 + \\triangle \\log p.$$\nBy integrating, this implies that\n\n$$\\frac{\\partial \\log p}{\\partial t} = g(t)^2 \\| \\nabla \\log p \\|^2 + \\triangle \\log p + c(t),$$\nwhere c(t) depends only on t. Eq. (18) shows that\n\nBy substituting this in the equation above, we obtain that\n\n$$\\triangle \\log p = \\triangle p \\frac{p - \\| \\nabla \\log p \\|^2}{\\frac{\\partial \\log p}{\\partial t} = g(t)^2 \\triangle p + c(t). \\quad (23)$$\nSince p is a probability distribution,\n\n$$\\int_{\\mathbb{R}^d} p(x, t)dx = 1,$$\ntherefore,\n\n$$\\frac{\\partial p(x, t)}{\\partial t} dx = \\frac{\\partial}{\\partial t} \\int_{\\mathbb{R}^d} p(x, t)dx = \\frac{\\partial 1}{\\partial t} = 0.$$\nIntegrating over Eq. (23) we obtain that\n\n$$0 = \\int g(t)^2 \\triangle p + c(t)dx = 0 + \\int c(t)dx,$$"}]}, {"page": 22, "text": "where the last equation holds since the integral of a Laplacian of probability density integrates to 0. It follows\nthat c(t) = 0 which implies that\n                                                                                \u2202p\n                                                                                \u2202t = g(t)2   2     \u25b3p,                                                                            (24)\nand the same PDE holds where p\u2032 replaces p, and this follows without loss of generality. Further, since log p\nand log p\u2032 are differentiable, it holds that p(\u00b7, t) and p\u2032(\u00b7, t) are continuous for all fixed t. This implies that p\nand p\u2032 are continuous as functions of x and t since they both satisfy the heat equation Eq. (13). Consequently,\nLemma A.6 implies that p = p\u2032 on Rd \u00d7 [t0, 1]. Finally, s = \u2207                                              log p = \u2207        log p\u2032 = s\u2032, as required.\nA.6          Proof of Lemma A.10\nFirst, recall that since xt satisfies Eq. (4) with the initial condition x0 \u223c                                             \u00b50, then xt \u223c          \u00b50 +N(0, \u03c32         t I), namely,\nxt is the addition of a random variable drawn from \u00b50 and an independent Gaussian N(0, \u03c32                                                                      t I). Therefore,\nthe density of xt, which we denote by p(x, t), equals\n                                                     p(x, a) = Ea\u223c\u00b50               \u221a    1       exp      \u2212\u2225x \u2212   2\u03c32  a\u22252         .\n                                                                                       2\u03c0\u03c3t                          t\nUsing the equation\n                                                                        \u2207x log f(x) = \u2207xf(x)        f(x) ,\nwe get that                                                                                     Ea\u223c\u00b50         \u221a   1     x\u2212a             \u2212\u2225x\u2212a\u22252\n                                                                                                                2\u03c0\u03c3t     \u03c32t exp              2\u03c32 t\n                             s(x, a) = \u2207x log p(x, a) = \u2207xp(x, a)           p(x, a)         =       Ea\u223c\u00b50        \u221a   1              \u2212\u2225x\u2212a\u222522\u03c32                                    (25)\n                                                                                                                    2\u03c0\u03c3t exp                  t\nBy using the fact that the Taylor formula for ex equals\n                                                                                  ex =      \u221e     ei\n                                                                                           i=0    i! ,\nwe obtain that the right hand side of Eq. (25) equals\n                        Ea\u223c\u00b50         \u221a   1     x\u2212a     \u221e  i=0    (\u22121)i      \u2225x\u2212a\u22252      i          Ea\u223c\u00b50         x\u2212a     \u221e  i=0    (\u22121)i      \u2225x\u2212a\u22252      i\n                                        2\u03c0\u03c3t     \u03c32t                i!          2\u03c32 t                              \u03c32t                i!          2\u03c32 t\n                            Ea\u223c\u00b50        \u221a   1       \u221e  i=0   (\u22121)i       \u2225x\u2212a\u22252      i         =       Ea\u223c\u00b50        \u221e   i=0    (\u22121)i       \u2225x\u2212a\u22252      i                         (26)\n                                            2\u03c0\u03c3t                 i!          2\u03c32t                                                  i!         2\u03c32 t\nWe will use the following property of analytic functions: if f and g are analytic functions over Rd and g(x) \u0338= 0\nfor all x then f/g is analytic over Rd. Since the denominator at the right hand side of Eq. (26) is nonzero, it\nsuffices to prove that the numerator and the denominator are analytic. We will prove for the denominator and\nthe proof for the numerator is nearly identical. By assumption of this lemma, the support of \u00b50 is bounded,\nhence there is some M > 0 such that \u2225x\u2225                                 \u2264   M for any x in the support. Then,\n                                                    (\u22121)i        \u2225x \u2212       a\u22252    i     \u2264    1   x2 + a2          i   = M 2i\n                                                         i!             2\u03c32                   i!         \u03c32                 \u03c32i\n                                                                            t                              t                  t i!.\nThis bound is independent on a, and summing these abvolute values of coefficients for i \u2208                                                                   N, one obtains a\nconvergent series. Hence we can replace the summation and the expectation in the denominator at the right\nhand side of Eq. (26) to get that it equals\n                                                                 \u221e     (\u22121)i    Ea\u223c\u00b50        \u2225x \u2212         a\u22252    i      .                                                         (27)\n                                                                i=0       i!                         2\u03c32  t\n                                                                                          22", "md": "where the last equation holds since the integral of a Laplacian of probability density integrates to 0. It follows\nthat \\(c(t) = 0\\) which implies that\n\n\\[\\frac{\\partial p}{\\partial t} = g(t)^2 \\Delta p, \\quad (24)\\]\nand the same PDE holds where \\(p'\\) replaces \\(p\\), and this follows without loss of generality. Further, since \\(\\log p\\)\nand \\(\\log p'\\) are differentiable, it holds that \\(p(\\cdot, t)\\) and \\(p'(\\cdot, t)\\) are continuous for all fixed \\(t\\). This implies that \\(p\\)\nand \\(p'\\) are continuous as functions of \\(x\\) and \\(t\\) since they both satisfy the heat equation Eq. (13). Consequently,\nLemma A.6 implies that \\(p = p'\\) on \\(\\mathbb{R}^d \\times [t_0, 1]\\). Finally, \\(s = \\nabla \\log p = \\nabla \\log p' = s'\\), as required.\n\nA.6 Proof of Lemma A.10\n\nFirst, recall that since \\(x_t\\) satisfies Eq. (4) with the initial condition \\(x_0 \\sim \\mu_0\\), then \\(x_t \\sim \\mu_0 + \\mathcal{N}(0, \\sigma^2 t I)\\), namely,\n\\(x_t\\) is the addition of a random variable drawn from \\(\\mu_0\\) and an independent Gaussian \\(\\mathcal{N}(0, \\sigma^2 t I)\\). Therefore,\nthe density of \\(x_t\\), which we denote by \\(p(x, t)\\), equals\n\n\\[p(x, t) = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x - \\frac{2\\sigma^2}{t} a\\|^2}{2\\sigma^2 t} \\right) \\right].\\]\nUsing the equation\n\n\\[\\nabla_x \\log f(x) = \\nabla_x f(x) \\frac{1}{f(x)},\\]\nwe get that\n\n\\[s(x, a) = \\nabla_x \\log p(x, a) = \\nabla_x p(x, a) \\frac{1}{p(x, a)} = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x-a\\|^2}{2\\sigma^2} \\right) \\right]. \\quad (25)\\]\nBy using the fact that the Taylor formula for \\(e^x\\) equals\n\n\\[e^x = \\sum_{i=0}^{\\infty} \\frac{x^i}{i!},\\]\nwe obtain that the right hand side of Eq. (25) equals\n\n\\[\\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x-a\\|^2}{2\\sigma^2 t} \\right) \\sum_{i=0}^{\\infty} \\frac{(-1)^i \\|x-a\\|^2}{i! 2\\sigma^2 t} \\right] = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\sum_{i=0}^{\\infty} \\frac{(-1)^i \\|x-a\\|^2}{i! 2\\sigma^2 t} \\right]. \\quad (26)\\]\nWe will use the following property of analytic functions: if \\(f\\) and \\(g\\) are analytic functions over \\(\\mathbb{R}^d\\) and \\(g(x) \\neq 0\\)\nfor all \\(x\\) then \\(f/g\\) is analytic over \\(\\mathbb{R}^d\\). Since the denominator at the right hand side of Eq. (26) is nonzero, it\nsuffices to prove that the numerator and the denominator are analytic. We will prove for the denominator and\nthe proof for the numerator is nearly identical. By assumption of this lemma, the support of \\(\\mu_0\\) is bounded,\nhence there is some \\(M > 0\\) such that \\(\\|x\\| \\leq M\\) for any \\(x\\) in the support. Then,\n\n\\[\\frac{(-1)^i \\|x - a\\|^2}{i! 2\\sigma^2} \\leq \\frac{1}{x^2 + a^2} \\leq \\frac{M^{2i}}{\\sigma^2 (\\sigma^2)^i t^i i!}.\\]\nThis bound is independent on \\(a\\), and summing these absolute values of coefficients for \\(i \\in \\mathbb{N}\\), one obtains a\nconvergent series. Hence we can replace the summation and the expectation in the denominator at the right\nhand side of Eq. (26) to get that it equals\n\n\\[\\sum_{i=0}^{\\infty} (-1)^i \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\|x - a\\|^2 \\right]. \\quad (27)\\]", "images": [], "items": [{"type": "text", "value": "where the last equation holds since the integral of a Laplacian of probability density integrates to 0. It follows\nthat \\(c(t) = 0\\) which implies that\n\n\\[\\frac{\\partial p}{\\partial t} = g(t)^2 \\Delta p, \\quad (24)\\]\nand the same PDE holds where \\(p'\\) replaces \\(p\\), and this follows without loss of generality. Further, since \\(\\log p\\)\nand \\(\\log p'\\) are differentiable, it holds that \\(p(\\cdot, t)\\) and \\(p'(\\cdot, t)\\) are continuous for all fixed \\(t\\). This implies that \\(p\\)\nand \\(p'\\) are continuous as functions of \\(x\\) and \\(t\\) since they both satisfy the heat equation Eq. (13). Consequently,\nLemma A.6 implies that \\(p = p'\\) on \\(\\mathbb{R}^d \\times [t_0, 1]\\). Finally, \\(s = \\nabla \\log p = \\nabla \\log p' = s'\\), as required.\n\nA.6 Proof of Lemma A.10\n\nFirst, recall that since \\(x_t\\) satisfies Eq. (4) with the initial condition \\(x_0 \\sim \\mu_0\\), then \\(x_t \\sim \\mu_0 + \\mathcal{N}(0, \\sigma^2 t I)\\), namely,\n\\(x_t\\) is the addition of a random variable drawn from \\(\\mu_0\\) and an independent Gaussian \\(\\mathcal{N}(0, \\sigma^2 t I)\\). Therefore,\nthe density of \\(x_t\\), which we denote by \\(p(x, t)\\), equals\n\n\\[p(x, t) = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x - \\frac{2\\sigma^2}{t} a\\|^2}{2\\sigma^2 t} \\right) \\right].\\]\nUsing the equation\n\n\\[\\nabla_x \\log f(x) = \\nabla_x f(x) \\frac{1}{f(x)},\\]\nwe get that\n\n\\[s(x, a) = \\nabla_x \\log p(x, a) = \\nabla_x p(x, a) \\frac{1}{p(x, a)} = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x-a\\|^2}{2\\sigma^2} \\right) \\right]. \\quad (25)\\]\nBy using the fact that the Taylor formula for \\(e^x\\) equals\n\n\\[e^x = \\sum_{i=0}^{\\infty} \\frac{x^i}{i!},\\]\nwe obtain that the right hand side of Eq. (25) equals\n\n\\[\\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x-a\\|^2}{2\\sigma^2 t} \\right) \\sum_{i=0}^{\\infty} \\frac{(-1)^i \\|x-a\\|^2}{i! 2\\sigma^2 t} \\right] = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\sum_{i=0}^{\\infty} \\frac{(-1)^i \\|x-a\\|^2}{i! 2\\sigma^2 t} \\right]. \\quad (26)\\]\nWe will use the following property of analytic functions: if \\(f\\) and \\(g\\) are analytic functions over \\(\\mathbb{R}^d\\) and \\(g(x) \\neq 0\\)\nfor all \\(x\\) then \\(f/g\\) is analytic over \\(\\mathbb{R}^d\\). Since the denominator at the right hand side of Eq. (26) is nonzero, it\nsuffices to prove that the numerator and the denominator are analytic. We will prove for the denominator and\nthe proof for the numerator is nearly identical. By assumption of this lemma, the support of \\(\\mu_0\\) is bounded,\nhence there is some \\(M > 0\\) such that \\(\\|x\\| \\leq M\\) for any \\(x\\) in the support. Then,\n\n\\[\\frac{(-1)^i \\|x - a\\|^2}{i! 2\\sigma^2} \\leq \\frac{1}{x^2 + a^2} \\leq \\frac{M^{2i}}{\\sigma^2 (\\sigma^2)^i t^i i!}.\\]\nThis bound is independent on \\(a\\), and summing these absolute values of coefficients for \\(i \\in \\mathbb{N}\\), one obtains a\nconvergent series. Hence we can replace the summation and the expectation in the denominator at the right\nhand side of Eq. (26) to get that it equals\n\n\\[\\sum_{i=0}^{\\infty} (-1)^i \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\|x - a\\|^2 \\right]. \\quad (27)\\]", "md": "where the last equation holds since the integral of a Laplacian of probability density integrates to 0. It follows\nthat \\(c(t) = 0\\) which implies that\n\n\\[\\frac{\\partial p}{\\partial t} = g(t)^2 \\Delta p, \\quad (24)\\]\nand the same PDE holds where \\(p'\\) replaces \\(p\\), and this follows without loss of generality. Further, since \\(\\log p\\)\nand \\(\\log p'\\) are differentiable, it holds that \\(p(\\cdot, t)\\) and \\(p'(\\cdot, t)\\) are continuous for all fixed \\(t\\). This implies that \\(p\\)\nand \\(p'\\) are continuous as functions of \\(x\\) and \\(t\\) since they both satisfy the heat equation Eq. (13). Consequently,\nLemma A.6 implies that \\(p = p'\\) on \\(\\mathbb{R}^d \\times [t_0, 1]\\). Finally, \\(s = \\nabla \\log p = \\nabla \\log p' = s'\\), as required.\n\nA.6 Proof of Lemma A.10\n\nFirst, recall that since \\(x_t\\) satisfies Eq. (4) with the initial condition \\(x_0 \\sim \\mu_0\\), then \\(x_t \\sim \\mu_0 + \\mathcal{N}(0, \\sigma^2 t I)\\), namely,\n\\(x_t\\) is the addition of a random variable drawn from \\(\\mu_0\\) and an independent Gaussian \\(\\mathcal{N}(0, \\sigma^2 t I)\\). Therefore,\nthe density of \\(x_t\\), which we denote by \\(p(x, t)\\), equals\n\n\\[p(x, t) = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x - \\frac{2\\sigma^2}{t} a\\|^2}{2\\sigma^2 t} \\right) \\right].\\]\nUsing the equation\n\n\\[\\nabla_x \\log f(x) = \\nabla_x f(x) \\frac{1}{f(x)},\\]\nwe get that\n\n\\[s(x, a) = \\nabla_x \\log p(x, a) = \\nabla_x p(x, a) \\frac{1}{p(x, a)} = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x-a\\|^2}{2\\sigma^2} \\right) \\right]. \\quad (25)\\]\nBy using the fact that the Taylor formula for \\(e^x\\) equals\n\n\\[e^x = \\sum_{i=0}^{\\infty} \\frac{x^i}{i!},\\]\nwe obtain that the right hand side of Eq. (25) equals\n\n\\[\\mathbb{E}_{a \\sim \\mu_0} \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma t}} \\exp \\left( -\\frac{\\|x-a\\|^2}{2\\sigma^2 t} \\right) \\sum_{i=0}^{\\infty} \\frac{(-1)^i \\|x-a\\|^2}{i! 2\\sigma^2 t} \\right] = \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\sum_{i=0}^{\\infty} \\frac{(-1)^i \\|x-a\\|^2}{i! 2\\sigma^2 t} \\right]. \\quad (26)\\]\nWe will use the following property of analytic functions: if \\(f\\) and \\(g\\) are analytic functions over \\(\\mathbb{R}^d\\) and \\(g(x) \\neq 0\\)\nfor all \\(x\\) then \\(f/g\\) is analytic over \\(\\mathbb{R}^d\\). Since the denominator at the right hand side of Eq. (26) is nonzero, it\nsuffices to prove that the numerator and the denominator are analytic. We will prove for the denominator and\nthe proof for the numerator is nearly identical. By assumption of this lemma, the support of \\(\\mu_0\\) is bounded,\nhence there is some \\(M > 0\\) such that \\(\\|x\\| \\leq M\\) for any \\(x\\) in the support. Then,\n\n\\[\\frac{(-1)^i \\|x - a\\|^2}{i! 2\\sigma^2} \\leq \\frac{1}{x^2 + a^2} \\leq \\frac{M^{2i}}{\\sigma^2 (\\sigma^2)^i t^i i!}.\\]\nThis bound is independent on \\(a\\), and summing these absolute values of coefficients for \\(i \\in \\mathbb{N}\\), one obtains a\nconvergent series. Hence we can replace the summation and the expectation in the denominator at the right\nhand side of Eq. (26) to get that it equals\n\n\\[\\sum_{i=0}^{\\infty} (-1)^i \\mathbb{E}_{a \\sim \\mu_0} \\left[ \\|x - a\\|^2 \\right]. \\quad (27)\\]"}]}, {"page": 23, "text": "This is the Taylor series around 0 of the above-described denominator it converges to the value of the\n denominator at any x. While this Taylor series is taken around 0, we note the Taylor series around any\n other point x0 \u2208           Rn converges as well. This can be shown by shifting the coordinate system by a constant\nvector such that x0 shifts to 0 and applying the same proof. One deduces that the Taylor series for the\n denominator around any point x0 converges on all Rd, which implies that the denominator in the right hand\n side of Eq. (26) is analytic. The numerator is analytic as well by the same argument. Therefore the ratio,\nwhich equals s(x, t), is analytic as well as required.\n A.7         Proof of Lemma A.11\n Let t > 0, denote by \u00b5t and \u00b5\u2032                  t the distributions of xt and x\u2032                 t, respectively, and by p(x, t) and p\u2032(x, t) the\n densities of these variables. Then, \u00b5t = \u00b50 + N(0, \u03c32                              t I), namely, \u00b5t is obtained by adding an independent\n sample from \u00b50 with an independent N(0, \u03c32                             t I) variables, and similarly for \u00b5\u2032                 t. Hence, the density p(x, t)\n is the convolution of the densities p(x, 0) with the density of a Gaussian N(0, \u03c32                                              t I). Denote by \u02c6        p(y, t) the\n Fourier transform of the density p(x, t) with respect to x (while keeping t fixed) and similarly define \u02c6                                                   p\u2032 as the\n Fourier transform of p\u2032. Denote by g and by \u02c6                        g the density of N(0, \u03c32           t I) and its Fourier transform, respectively.\n Denote the convolution of two functions by the operator \u2217. Then,\n                                               p(x, t) = p(x, 0) \u2217         g(x),      p\u2032(x, t) = p\u2032(x, 0) \u2217         g(x).\n Since the Fourier transform turns convolutions into multiplications, one obtains that\n                                                   \u02c6\n                                                   p(y, t) = \u02c6   p(y, 0)\u02c6  g(y),       \u02c6\n                                                                                      p\u2032(y, t) = \u02c6    p\u2032(y, 0)\u02c6  g(y).\n Since p(x, t) = p\u2032(x, t) we obtain that \u02c6                   p(y, t) = \u02c6   p\u2032(y, t). Consequently,\n                                                                  \u02c6\n                                                                  p(y, 0)\u02c6  g(y) = \u02c6   p\u2032(y, 0)\u02c6  g(y)\n Since the Fourier transform of a Gaussian is nonzero, we can divide by \u02c6                                         g(y) to get that\n                                                                        \u02c6\n                                                                        p(y, 0) = \u02c6    p\u2032(y, 0).\nThis implies that the Fourier transform of p(x, 0) equals that of p\u2032(x, 0) hence p(x, 0) = p\u2032(x, 0) for all x, as\n required.\n B         Other proofs\n B.1         Differentiating the loss function\n Denote our parameter space as \u0398 \u2286                         Rm. In order to differentiate L1                 t,t\u2032,x(\u03b8) with respect to \u03b8 \u2208               \u0398, we make\n the following calculations below, and we notice that E\u03b8 is used to denote an expectation with respect to the\n distribution of x         [t\u2032,t] according to Eq. (7) with s = s\u03b8 and the initial condition xt = x. In other words, the\n expectation is over x           [t\u2032,t] that is taken with respect to the sampler that is parameterized by \u03b8 with the initial\n condition xt = x. We denote by p\u03b8(x[t\u2032,t] | xt = x) the corresponding density of x[t\u2032,t]. For any function\n f = (f1, . . . , fn): \u0398 \u2192           Rn, denote by \u2207\u03b8f the Jacobian matrix of f, where\n                                                                         (\u2207\u03b8f)i,j = \u2202fi    \u2202\u03b8j   .\n                                                                                    23", "md": "This is the Taylor series around 0 of the above-described denominator it converges to the value of the denominator at any x. While this Taylor series is taken around 0, we note the Taylor series around any other point \\( x_0 \\in \\mathbb{R}^n \\) converges as well. This can be shown by shifting the coordinate system by a constant vector such that \\( x_0 \\) shifts to 0 and applying the same proof. One deduces that the Taylor series for the denominator around any point \\( x_0 \\) converges on all \\( \\mathbb{R}^d \\), which implies that the denominator in the right-hand side of Eq. (26) is analytic. The numerator is analytic as well by the same argument. Therefore the ratio, which equals \\( s(x, t) \\), is analytic as well as required.\n\n### Proof of Lemma A.11\nLet \\( t > 0 \\), denote by \\( \\mu_t \\) and \\( \\mu'_t \\) the distributions of \\( x_t \\) and \\( x'_t \\), respectively, and by \\( p(x, t) \\) and \\( p'(x, t) \\) the densities of these variables. Then, \\( \\mu_t = \\mu_0 + \\mathcal{N}(0, \\sigma^2_t I) \\), namely, \\( \\mu_t \\) is obtained by adding an independent sample from \\( \\mu_0 \\) with an independent \\( \\mathcal{N}(0, \\sigma^2_t I) \\) variables, and similarly for \\( \\mu'_t \\). Hence, the density \\( p(x, t) \\) is the convolution of the densities \\( p(x, 0) \\) with the density of a Gaussian \\( \\mathcal{N}(0, \\sigma^2_t I) \\). Denote by \\( \\hat{p}(y, t) \\) the Fourier transform of the density \\( p(x, t) \\) with respect to \\( x \\) (while keeping \\( t \\) fixed) and similarly define \\( \\hat{p}' \\) as the Fourier transform of \\( p' \\). Denote by \\( g \\) and by \\( \\hat{g} \\) the density of \\( \\mathcal{N}(0, \\sigma^2_t I) \\) and its Fourier transform, respectively. Denote the convolution of two functions by the operator \\( * \\). Then,\n\\[ p(x, t) = p(x, 0) * g(x), \\quad p'(x, t) = p'(x, 0) * g(x). \\]\nSince the Fourier transform turns convolutions into multiplications, one obtains that\n\\[ \\hat{p}(y, t) = \\hat{p}(y, 0) \\hat{g}(y), \\quad \\hat{p}'(y, t) = \\hat{p}'(y, 0) \\hat{g}(y). \\]\nSince \\( p(x, t) = p'(x, t) \\) we obtain that \\( \\hat{p}(y, t) = \\hat{p}'(y, t) \\). Consequently,\n\\[ \\hat{p}(y, 0) \\hat{g}(y) = \\hat{p}'(y, 0) \\hat{g}(y) \\]\nSince the Fourier transform of a Gaussian is nonzero, we can divide by \\( \\hat{g}(y) \\) to get that\n\\[ \\hat{p}(y, 0) = \\hat{p}'(y, 0). \\]\nThis implies that the Fourier transform of \\( p(x, 0) \\) equals that of \\( p'(x, 0) \\) hence \\( p(x, 0) = p'(x, 0) \\) for all \\( x \\), as required.\n\n### Other proofs\n\n#### Differentiating the loss function\nDenote our parameter space as \\( \\Theta \\subseteq \\mathbb{R}^m \\). In order to differentiate \\( L_{1,t,t'},x(\\theta) \\) with respect to \\( \\theta \\in \\Theta \\), we make the following calculations below, and we notice that \\( E_{\\theta} \\) is used to denote an expectation with respect to the distribution of \\( x_{[t',t]} \\) according to Eq. (7) with \\( s = s_{\\theta} \\) and the initial condition \\( x_t = x \\). In other words, the expectation is over \\( x_{[t',t]} \\) that is taken with respect to the sampler that is parameterized by \\( \\theta \\) with the initial condition \\( x_t = x \\). We denote by \\( p_{\\theta}(x[t',t] | x_t = x) \\) the corresponding density of \\( x_{[t',t]} \\). For any function \\( f = (f_1, \\ldots, f_n) : \\Theta \\rightarrow \\mathbb{R}^n \\), denote by \\( \\nabla_{\\theta} f \\) the Jacobian matrix of \\( f \\), where\n\\[ (\\nabla_{\\theta} f)_{i,j} = \\frac{\\partial f_i}{\\partial \\theta_j}. \\]", "images": [], "items": [{"type": "text", "value": "This is the Taylor series around 0 of the above-described denominator it converges to the value of the denominator at any x. While this Taylor series is taken around 0, we note the Taylor series around any other point \\( x_0 \\in \\mathbb{R}^n \\) converges as well. This can be shown by shifting the coordinate system by a constant vector such that \\( x_0 \\) shifts to 0 and applying the same proof. One deduces that the Taylor series for the denominator around any point \\( x_0 \\) converges on all \\( \\mathbb{R}^d \\), which implies that the denominator in the right-hand side of Eq. (26) is analytic. The numerator is analytic as well by the same argument. Therefore the ratio, which equals \\( s(x, t) \\), is analytic as well as required.", "md": "This is the Taylor series around 0 of the above-described denominator it converges to the value of the denominator at any x. While this Taylor series is taken around 0, we note the Taylor series around any other point \\( x_0 \\in \\mathbb{R}^n \\) converges as well. This can be shown by shifting the coordinate system by a constant vector such that \\( x_0 \\) shifts to 0 and applying the same proof. One deduces that the Taylor series for the denominator around any point \\( x_0 \\) converges on all \\( \\mathbb{R}^d \\), which implies that the denominator in the right-hand side of Eq. (26) is analytic. The numerator is analytic as well by the same argument. Therefore the ratio, which equals \\( s(x, t) \\), is analytic as well as required."}, {"type": "heading", "lvl": 3, "value": "Proof of Lemma A.11", "md": "### Proof of Lemma A.11"}, {"type": "text", "value": "Let \\( t > 0 \\), denote by \\( \\mu_t \\) and \\( \\mu'_t \\) the distributions of \\( x_t \\) and \\( x'_t \\), respectively, and by \\( p(x, t) \\) and \\( p'(x, t) \\) the densities of these variables. Then, \\( \\mu_t = \\mu_0 + \\mathcal{N}(0, \\sigma^2_t I) \\), namely, \\( \\mu_t \\) is obtained by adding an independent sample from \\( \\mu_0 \\) with an independent \\( \\mathcal{N}(0, \\sigma^2_t I) \\) variables, and similarly for \\( \\mu'_t \\). Hence, the density \\( p(x, t) \\) is the convolution of the densities \\( p(x, 0) \\) with the density of a Gaussian \\( \\mathcal{N}(0, \\sigma^2_t I) \\). Denote by \\( \\hat{p}(y, t) \\) the Fourier transform of the density \\( p(x, t) \\) with respect to \\( x \\) (while keeping \\( t \\) fixed) and similarly define \\( \\hat{p}' \\) as the Fourier transform of \\( p' \\). Denote by \\( g \\) and by \\( \\hat{g} \\) the density of \\( \\mathcal{N}(0, \\sigma^2_t I) \\) and its Fourier transform, respectively. Denote the convolution of two functions by the operator \\( * \\). Then,\n\\[ p(x, t) = p(x, 0) * g(x), \\quad p'(x, t) = p'(x, 0) * g(x). \\]\nSince the Fourier transform turns convolutions into multiplications, one obtains that\n\\[ \\hat{p}(y, t) = \\hat{p}(y, 0) \\hat{g}(y), \\quad \\hat{p}'(y, t) = \\hat{p}'(y, 0) \\hat{g}(y). \\]\nSince \\( p(x, t) = p'(x, t) \\) we obtain that \\( \\hat{p}(y, t) = \\hat{p}'(y, t) \\). Consequently,\n\\[ \\hat{p}(y, 0) \\hat{g}(y) = \\hat{p}'(y, 0) \\hat{g}(y) \\]\nSince the Fourier transform of a Gaussian is nonzero, we can divide by \\( \\hat{g}(y) \\) to get that\n\\[ \\hat{p}(y, 0) = \\hat{p}'(y, 0). \\]\nThis implies that the Fourier transform of \\( p(x, 0) \\) equals that of \\( p'(x, 0) \\) hence \\( p(x, 0) = p'(x, 0) \\) for all \\( x \\), as required.", "md": "Let \\( t > 0 \\), denote by \\( \\mu_t \\) and \\( \\mu'_t \\) the distributions of \\( x_t \\) and \\( x'_t \\), respectively, and by \\( p(x, t) \\) and \\( p'(x, t) \\) the densities of these variables. Then, \\( \\mu_t = \\mu_0 + \\mathcal{N}(0, \\sigma^2_t I) \\), namely, \\( \\mu_t \\) is obtained by adding an independent sample from \\( \\mu_0 \\) with an independent \\( \\mathcal{N}(0, \\sigma^2_t I) \\) variables, and similarly for \\( \\mu'_t \\). Hence, the density \\( p(x, t) \\) is the convolution of the densities \\( p(x, 0) \\) with the density of a Gaussian \\( \\mathcal{N}(0, \\sigma^2_t I) \\). Denote by \\( \\hat{p}(y, t) \\) the Fourier transform of the density \\( p(x, t) \\) with respect to \\( x \\) (while keeping \\( t \\) fixed) and similarly define \\( \\hat{p}' \\) as the Fourier transform of \\( p' \\). Denote by \\( g \\) and by \\( \\hat{g} \\) the density of \\( \\mathcal{N}(0, \\sigma^2_t I) \\) and its Fourier transform, respectively. Denote the convolution of two functions by the operator \\( * \\). Then,\n\\[ p(x, t) = p(x, 0) * g(x), \\quad p'(x, t) = p'(x, 0) * g(x). \\]\nSince the Fourier transform turns convolutions into multiplications, one obtains that\n\\[ \\hat{p}(y, t) = \\hat{p}(y, 0) \\hat{g}(y), \\quad \\hat{p}'(y, t) = \\hat{p}'(y, 0) \\hat{g}(y). \\]\nSince \\( p(x, t) = p'(x, t) \\) we obtain that \\( \\hat{p}(y, t) = \\hat{p}'(y, t) \\). Consequently,\n\\[ \\hat{p}(y, 0) \\hat{g}(y) = \\hat{p}'(y, 0) \\hat{g}(y) \\]\nSince the Fourier transform of a Gaussian is nonzero, we can divide by \\( \\hat{g}(y) \\) to get that\n\\[ \\hat{p}(y, 0) = \\hat{p}'(y, 0). \\]\nThis implies that the Fourier transform of \\( p(x, 0) \\) equals that of \\( p'(x, 0) \\) hence \\( p(x, 0) = p'(x, 0) \\) for all \\( x \\), as required."}, {"type": "heading", "lvl": 3, "value": "Other proofs", "md": "### Other proofs"}, {"type": "heading", "lvl": 4, "value": "Differentiating the loss function", "md": "#### Differentiating the loss function"}, {"type": "text", "value": "Denote our parameter space as \\( \\Theta \\subseteq \\mathbb{R}^m \\). In order to differentiate \\( L_{1,t,t'},x(\\theta) \\) with respect to \\( \\theta \\in \\Theta \\), we make the following calculations below, and we notice that \\( E_{\\theta} \\) is used to denote an expectation with respect to the distribution of \\( x_{[t',t]} \\) according to Eq. (7) with \\( s = s_{\\theta} \\) and the initial condition \\( x_t = x \\). In other words, the expectation is over \\( x_{[t',t]} \\) that is taken with respect to the sampler that is parameterized by \\( \\theta \\) with the initial condition \\( x_t = x \\). We denote by \\( p_{\\theta}(x[t',t] | x_t = x) \\) the corresponding density of \\( x_{[t',t]} \\). For any function \\( f = (f_1, \\ldots, f_n) : \\Theta \\rightarrow \\mathbb{R}^n \\), denote by \\( \\nabla_{\\theta} f \\) the Jacobian matrix of \\( f \\), where\n\\[ (\\nabla_{\\theta} f)_{i,j} = \\frac{\\partial f_i}{\\partial \\theta_j}. \\]", "md": "Denote our parameter space as \\( \\Theta \\subseteq \\mathbb{R}^m \\). In order to differentiate \\( L_{1,t,t'},x(\\theta) \\) with respect to \\( \\theta \\in \\Theta \\), we make the following calculations below, and we notice that \\( E_{\\theta} \\) is used to denote an expectation with respect to the distribution of \\( x_{[t',t]} \\) according to Eq. (7) with \\( s = s_{\\theta} \\) and the initial condition \\( x_t = x \\). In other words, the expectation is over \\( x_{[t',t]} \\) that is taken with respect to the sampler that is parameterized by \\( \\theta \\) with the initial condition \\( x_t = x \\). We denote by \\( p_{\\theta}(x[t',t] | x_t = x) \\) the corresponding density of \\( x_{[t',t]} \\). For any function \\( f = (f_1, \\ldots, f_n) : \\Theta \\rightarrow \\mathbb{R}^n \\), denote by \\( \\nabla_{\\theta} f \\) the Jacobian matrix of \\( f \\), where\n\\[ (\\nabla_{\\theta} f)_{i,j} = \\frac{\\partial f_i}{\\partial \\theta_j}. \\]"}]}, {"page": 24, "text": "For notational consistency, if f is a single-valued function, namely, if n = 1, then \u2207\u03b8f is a column vector.\nWe begin with the following:\n         \u2207\u03b8E\u03b8 [h\u03b8(xt\u2032, t\u2032)] = \u2207\u03b8              Rd h\u03b8(xt\u2032, t\u2032)p\u03b8(x[t\u2032,t] | xt = x)dxt\u2032\n                                   =     Rd \u2207\u03b8h\u03b8(xt\u2032, t\u2032)p\u03b8(x[t\u2032,t] | xt = x)dxt\u2032 +                    Rd h\u03b8(xt\u2032, t\u2032)\u2207\u03b8p\u03b8(x[t\u2032,t]|xt = x)dxt\u2032\n                                   = E\u03b8 [\u2207\u03b8h\u03b8(xt\u2032, t\u2032)] + E\u03b8               h\u03b8(xt\u2032, t\u2032)\u2207\u03b8p\u03b8(x[t\u2032,t]|xt = x)\n                                                                                             p\u03b8(x[t\u2032,t]|xt = x)\n                                   = E\u03b8 [\u2207\u03b8h\u03b8(xt\u2032, t\u2032)] + E\u03b8               h\u03b8(xt\u2032, t\u2032)\u2207\u03b8 log         p\u03b8(x[t\u2032,t] | xt = x)\nDifferentiating the whole loss, we get the following:\n                     \u2207\u03b8L1   t,t\u2032,x(\u03b8) = 1   2\u2207\u03b8 (E\u03b8[h\u03b8(xt\u2032, t\u2032)] \u2212            h\u03b8(x, t))2\n                                        = (E\u03b8[h\u03b8(xt\u2032, t\u2032)] \u2212          h\u03b8(x, t))\u22a4     (\u2207\u03b8E[h\u03b8(xt\u2032, t\u2032)] \u2212          \u2207\u03b8h\u03b8(x, t))\n                                        = E\u03b8 [h\u03b8(xt\u2032, t\u2032) \u2212         h\u03b8(x, t)]\u22a4     E\u03b8 [\u2207\u03b8h\u03b8(xt\u2032, t\u2032) \u2212          \u2207\u03b8h\u03b8(x, t)]\n                                        + E\u03b8 [h\u03b8(xt\u2032, t\u2032) \u2212         h\u03b8(x, t)]\u22a4     E\u03b8    h\u03b8(xt\u2032, t\u2032)\u2207\u03b8 log         p\u03b8(x[t\u2032,t] | xt = x)\n     Let us compute the gradient of the log density. We use the discrete process, and let us assume that\nt = t0 > t1 > \u00b7 \u00b7 \u00b7 > tk = t\u2032 are the sampling times. Then,                            k\nWe assume that                                        p\u03b8(x[t\u2032,t] | xt = x) =         i=1  p\u03b8(xti | xti\u22121).\nThen,                                                        p\u03b8(xt   i | ti\u22121) = N      (\u00b5\u03b8,i, giId).\n                                        p\u03b8(x[t\u2032,t] | xt = x) \u221d           k   exp      \u2212\u2225\u00b5\u03b8,i \u2212       (xti \u2212    xti\u22121)\u22252\n                                                                        i=1                            2g2 i\nTherefore                                                                              k   \u2225\u00b5\u03b8,i \u2212     (xti \u2212    xti\u22121)\u22252\n                                         log p\u03b8(x[t\u2032,t] | xt = x) = C +              i=1                 2g2 i\nwhere C corresponds to the normalizing factor that is independent of \u03b8. Differentiating, we get that\n                                                                                k     \u00b5\u03b8,i \u2212    (xti \u2212    xti\u22121)    \u22a4  \u2207\u03b8\u00b5\u03b8,i\n                                    \u2207\u03b8 log p\u03b8(x[t\u2032,t] | xt = x) =             i=1                        g2i\nB.2         Proof of Lemma 3.1\nIn what appears below, the expectation E[\u00b7 | xt = x] is taken with respect to the distribution obtained by\nEq. (7), namely, the backward SDE that corresponds to the function s, with the initial condition xt = x.\nSimilarly, E[\u00b7 | xt\u2032] is taken with the initial condition at xt\u2032. To prove the first direction in the equivalence,\nassume that Property 1 holds and our goal is to prove the two consequences as described in the lemma. To\nprove the first consequence, by the law of total expectation and by the fact that xt \u2212                                                xt\u2032 \u2212    x0 is a Markov\nchain, namely, x0 and xt are independent conditioned on xt\u2032, we obtain that\n                             h(x, t) = E[x0 | xt = x] = E[E[x0 | xt\u2032] | xt = x] = E[h(xt\u2032, t\u2032) | xt = x].\n                                                                                 24", "md": "# Math Equations\n\nFor notational consistency, if f is a single-valued function, namely, if n = 1, then $$\\nabla_{\\theta}f$$ is a column vector.\n\nWe begin with the following:\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}E_{\\theta}[h_{\\theta}(x_{t'}, t')] & = \\nabla_{\\theta} \\int_{\\mathbb{R}^d} h_{\\theta}(x_{t'}, t') p_{\\theta}(x[t',t] | x_t = x) dx_{t'} \\\\\n& = \\int_{\\mathbb{R}^d} \\nabla_{\\theta}h_{\\theta}(x_{t'}, t') p_{\\theta}(x[t',t] | x_t = x) dx_{t'} + \\int_{\\mathbb{R}^d} h_{\\theta}(x_{t'}, t') \\nabla_{\\theta}p_{\\theta}(x[t',t] | x_t = x) dx_{t'} \\\\\n& = E_{\\theta}[\\nabla_{\\theta}h_{\\theta}(x_{t'}, t')] + E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') \\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x) \\right]\n\\end{align*}\n$$\nDifferentiating the whole loss, we get the following:\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}L_{1}^{t,t',x}(\\theta) & = \\frac{1}{2} \\nabla_{\\theta} \\left( E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t) \\right)^2 \\\\\n& = \\left( E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t) \\right)^{\\top} \\left( \\nabla_{\\theta}E_{\\theta}[h_{\\theta}(x_{t'}, t')] - \\nabla_{\\theta}h_{\\theta}(x, t) \\right) \\\\\n& = E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t) \\right]^{\\top} E_{\\theta} \\left[ \\nabla_{\\theta}h_{\\theta}(x_{t'}, t') - \\nabla_{\\theta}h_{\\theta}(x, t) \\right] \\\\\n& + E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t) \\right]^{\\top} E_{\\theta} h_{\\theta}(x_{t'}, t') \\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x)\n\\end{align*}\n$$\nLet us compute the gradient of the log density. We use the discrete process, and let us assume that $$t = t_0 > t_1 > \\ldots > t_k = t'$$ are the sampling times. Then,\n\nWe assume that $$p_{\\theta}(x[t',t] | x_t = x) = \\prod_{i=1}^{k} p_{\\theta}(x_{t_i} | x_{t_{i-1}})$$.\n\nThen,\n\n$$\n\\begin{align*}\np_{\\theta}(x_{t_i} | t_{i-1}) & = \\mathcal{N}(\\mu_{\\theta,i}, g_iI_d) \\\\\np_{\\theta}(x[t',t] | x_t = x) & \\propto \\prod_{i=1}^{k} \\exp \\left( -\\frac{\\| \\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}}) \\|^{2}}{2g_i} \\right)\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\log p_{\\theta}(x[t',t] | x_t = x) = C + \\sum_{i=1}^{k} \\frac{\\| \\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}}) \\|^{2}}{2g_i}\n$$\nwhere C corresponds to the normalizing factor that is independent of $$\\theta$$. Differentiating, we get that\n\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x) = \\sum_{i=1}^{k} \\frac{\\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}})^{\\top} \\nabla_{\\theta}\\mu_{\\theta,i}}{g_i}\n$$\nProof of Lemma 3.1\n\nIn what appears below, the expectation $$E[\\cdot | x_t = x]$$ is taken with respect to the distribution obtained by Eq. (7), namely, the backward SDE that corresponds to the function s, with the initial condition $$x_t = x$$. Similarly, $$E[\\cdot | x_{t'}]$$ is taken with the initial condition at $$x_{t'}$$. To prove the first direction in the equivalence, assume that Property 1 holds and our goal is to prove the two consequences as described in the lemma. To prove the first consequence, by the law of total expectation and by the fact that $$x_t - x_{t'} - x_0$$ is a Markov chain, namely, $$x_0$$ and $$x_t$$ are independent conditioned on $$x_{t'}$$, we obtain that\n\n$$\nh(x, t) = E[x_0 | x_t = x] = E[E[x_0 | x_{t'}] | x_t = x] = E[h(x_{t'}, t') | x_t = x].\n$$\n24", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "For notational consistency, if f is a single-valued function, namely, if n = 1, then $$\\nabla_{\\theta}f$$ is a column vector.\n\nWe begin with the following:\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}E_{\\theta}[h_{\\theta}(x_{t'}, t')] & = \\nabla_{\\theta} \\int_{\\mathbb{R}^d} h_{\\theta}(x_{t'}, t') p_{\\theta}(x[t',t] | x_t = x) dx_{t'} \\\\\n& = \\int_{\\mathbb{R}^d} \\nabla_{\\theta}h_{\\theta}(x_{t'}, t') p_{\\theta}(x[t',t] | x_t = x) dx_{t'} + \\int_{\\mathbb{R}^d} h_{\\theta}(x_{t'}, t') \\nabla_{\\theta}p_{\\theta}(x[t',t] | x_t = x) dx_{t'} \\\\\n& = E_{\\theta}[\\nabla_{\\theta}h_{\\theta}(x_{t'}, t')] + E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') \\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x) \\right]\n\\end{align*}\n$$\nDifferentiating the whole loss, we get the following:\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}L_{1}^{t,t',x}(\\theta) & = \\frac{1}{2} \\nabla_{\\theta} \\left( E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t) \\right)^2 \\\\\n& = \\left( E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t) \\right)^{\\top} \\left( \\nabla_{\\theta}E_{\\theta}[h_{\\theta}(x_{t'}, t')] - \\nabla_{\\theta}h_{\\theta}(x, t) \\right) \\\\\n& = E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t) \\right]^{\\top} E_{\\theta} \\left[ \\nabla_{\\theta}h_{\\theta}(x_{t'}, t') - \\nabla_{\\theta}h_{\\theta}(x, t) \\right] \\\\\n& + E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t) \\right]^{\\top} E_{\\theta} h_{\\theta}(x_{t'}, t') \\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x)\n\\end{align*}\n$$\nLet us compute the gradient of the log density. We use the discrete process, and let us assume that $$t = t_0 > t_1 > \\ldots > t_k = t'$$ are the sampling times. Then,\n\nWe assume that $$p_{\\theta}(x[t',t] | x_t = x) = \\prod_{i=1}^{k} p_{\\theta}(x_{t_i} | x_{t_{i-1}})$$.\n\nThen,\n\n$$\n\\begin{align*}\np_{\\theta}(x_{t_i} | t_{i-1}) & = \\mathcal{N}(\\mu_{\\theta,i}, g_iI_d) \\\\\np_{\\theta}(x[t',t] | x_t = x) & \\propto \\prod_{i=1}^{k} \\exp \\left( -\\frac{\\| \\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}}) \\|^{2}}{2g_i} \\right)\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\log p_{\\theta}(x[t',t] | x_t = x) = C + \\sum_{i=1}^{k} \\frac{\\| \\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}}) \\|^{2}}{2g_i}\n$$\nwhere C corresponds to the normalizing factor that is independent of $$\\theta$$. Differentiating, we get that\n\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x) = \\sum_{i=1}^{k} \\frac{\\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}})^{\\top} \\nabla_{\\theta}\\mu_{\\theta,i}}{g_i}\n$$\nProof of Lemma 3.1\n\nIn what appears below, the expectation $$E[\\cdot | x_t = x]$$ is taken with respect to the distribution obtained by Eq. (7), namely, the backward SDE that corresponds to the function s, with the initial condition $$x_t = x$$. Similarly, $$E[\\cdot | x_{t'}]$$ is taken with the initial condition at $$x_{t'}$$. To prove the first direction in the equivalence, assume that Property 1 holds and our goal is to prove the two consequences as described in the lemma. To prove the first consequence, by the law of total expectation and by the fact that $$x_t - x_{t'} - x_0$$ is a Markov chain, namely, $$x_0$$ and $$x_t$$ are independent conditioned on $$x_{t'}$$, we obtain that\n\n$$\nh(x, t) = E[x_0 | x_t = x] = E[E[x_0 | x_{t'}] | x_t = x] = E[h(x_{t'}, t') | x_t = x].\n$$\n24", "md": "For notational consistency, if f is a single-valued function, namely, if n = 1, then $$\\nabla_{\\theta}f$$ is a column vector.\n\nWe begin with the following:\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}E_{\\theta}[h_{\\theta}(x_{t'}, t')] & = \\nabla_{\\theta} \\int_{\\mathbb{R}^d} h_{\\theta}(x_{t'}, t') p_{\\theta}(x[t',t] | x_t = x) dx_{t'} \\\\\n& = \\int_{\\mathbb{R}^d} \\nabla_{\\theta}h_{\\theta}(x_{t'}, t') p_{\\theta}(x[t',t] | x_t = x) dx_{t'} + \\int_{\\mathbb{R}^d} h_{\\theta}(x_{t'}, t') \\nabla_{\\theta}p_{\\theta}(x[t',t] | x_t = x) dx_{t'} \\\\\n& = E_{\\theta}[\\nabla_{\\theta}h_{\\theta}(x_{t'}, t')] + E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') \\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x) \\right]\n\\end{align*}\n$$\nDifferentiating the whole loss, we get the following:\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}L_{1}^{t,t',x}(\\theta) & = \\frac{1}{2} \\nabla_{\\theta} \\left( E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t) \\right)^2 \\\\\n& = \\left( E_{\\theta}[h_{\\theta}(x_{t'}, t')] - h_{\\theta}(x, t) \\right)^{\\top} \\left( \\nabla_{\\theta}E_{\\theta}[h_{\\theta}(x_{t'}, t')] - \\nabla_{\\theta}h_{\\theta}(x, t) \\right) \\\\\n& = E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t) \\right]^{\\top} E_{\\theta} \\left[ \\nabla_{\\theta}h_{\\theta}(x_{t'}, t') - \\nabla_{\\theta}h_{\\theta}(x, t) \\right] \\\\\n& + E_{\\theta} \\left[ h_{\\theta}(x_{t'}, t') - h_{\\theta}(x, t) \\right]^{\\top} E_{\\theta} h_{\\theta}(x_{t'}, t') \\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x)\n\\end{align*}\n$$\nLet us compute the gradient of the log density. We use the discrete process, and let us assume that $$t = t_0 > t_1 > \\ldots > t_k = t'$$ are the sampling times. Then,\n\nWe assume that $$p_{\\theta}(x[t',t] | x_t = x) = \\prod_{i=1}^{k} p_{\\theta}(x_{t_i} | x_{t_{i-1}})$$.\n\nThen,\n\n$$\n\\begin{align*}\np_{\\theta}(x_{t_i} | t_{i-1}) & = \\mathcal{N}(\\mu_{\\theta,i}, g_iI_d) \\\\\np_{\\theta}(x[t',t] | x_t = x) & \\propto \\prod_{i=1}^{k} \\exp \\left( -\\frac{\\| \\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}}) \\|^{2}}{2g_i} \\right)\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\log p_{\\theta}(x[t',t] | x_t = x) = C + \\sum_{i=1}^{k} \\frac{\\| \\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}}) \\|^{2}}{2g_i}\n$$\nwhere C corresponds to the normalizing factor that is independent of $$\\theta$$. Differentiating, we get that\n\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(x[t',t] | x_t = x) = \\sum_{i=1}^{k} \\frac{\\mu_{\\theta,i} - (x_{t_i} - x_{t_{i-1}})^{\\top} \\nabla_{\\theta}\\mu_{\\theta,i}}{g_i}\n$$\nProof of Lemma 3.1\n\nIn what appears below, the expectation $$E[\\cdot | x_t = x]$$ is taken with respect to the distribution obtained by Eq. (7), namely, the backward SDE that corresponds to the function s, with the initial condition $$x_t = x$$. Similarly, $$E[\\cdot | x_{t'}]$$ is taken with the initial condition at $$x_{t'}$$. To prove the first direction in the equivalence, assume that Property 1 holds and our goal is to prove the two consequences as described in the lemma. To prove the first consequence, by the law of total expectation and by the fact that $$x_t - x_{t'} - x_0$$ is a Markov chain, namely, $$x_0$$ and $$x_t$$ are independent conditioned on $$x_{t'}$$, we obtain that\n\n$$\nh(x, t) = E[x_0 | x_t = x] = E[E[x_0 | x_{t'}] | x_t = x] = E[h(x_{t'}, t') | x_t = x].\n$$\n24"}]}, {"page": 25, "text": "To prove the second consequence, by Property 1\nThis concludes the first direction in the equivalence.       h(x, 0) = E[x0 | x0 = x] = x0.\n     To prove the second direction, assume that h(x, t) = E[h(xt\u2032, t\u2032) | xt = x] and that h(x, 0) = x and notice\nthat by substituting t\u2032 = 0 we derive the following:\nas required.                                    h(x, t) = E[h(x0, 0) | xt = x] = E[x0 | xt = x],\nC         Additional Results\nC.1         Property Testing\n                                                           Consistency Property Testing (CIFAR10)\n                                                                 Backward Sampler (300) steps\n                               0.0025          Baseline\n                                               Ours\n                               0.0020\n                            h (xt, t)||2\n                               0.0015\n                             p (x0|xt)[x0]\n                               0.0010\n                              x0\n                             ||0.0005\n                               0.0000\n                                           0         10        20        30         40        50        60        70  80\n                                                                                     t\nFigure 4: Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, L2                      t,t\u2032,xt,\nbehaves for t\u2032 = 0, as t changes.\n                                                                                  25", "md": "To prove the second consequence, by Property 1\nThis concludes the first direction in the equivalence.       $$h(x, 0) = E[x0 | x0 = x] = x0.$$\nTo prove the second direction, assume that $$h(x, t) = E[h(xt', t') | xt = x]$$ and that $$h(x, 0) = x$$ and notice\nthat by substituting $$t' = 0$$ we derive the following:\nas required.                                    $$h(x, t) = E[h(x0, 0) | xt = x] = E[x0 | xt = x],$$\n\nAdditional Results\n\nProperty Testing\n\nConsistency Property Testing (CIFAR10)\n\n| Backward Sampler (300) steps | 0.0025 | Baseline | Ours |\n|------------------------------|--------|----------|------|\n| $$||h(xt, t)||_2$$           | 0.0015 |          |      |\n| $$p(x0|xt)[x0]$$             | 0.0010 |          |      |\n| $$x0$$                       | 0.0005 |          |      |\n|                              | 0.0000 |          |      |\n|                              | 0      | 10       | 20   |\n|                              | 30     | 40       | 50   |\n|                              | 60     | 70       | 80   |\n\nFigure 4: Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, $$L_2^{t,t',xt}$$,\nbehaves for $$t' = 0$$, as $$t$$ changes.", "images": [], "items": [{"type": "text", "value": "To prove the second consequence, by Property 1\nThis concludes the first direction in the equivalence.       $$h(x, 0) = E[x0 | x0 = x] = x0.$$\nTo prove the second direction, assume that $$h(x, t) = E[h(xt', t') | xt = x]$$ and that $$h(x, 0) = x$$ and notice\nthat by substituting $$t' = 0$$ we derive the following:\nas required.                                    $$h(x, t) = E[h(x0, 0) | xt = x] = E[x0 | xt = x],$$\n\nAdditional Results\n\nProperty Testing\n\nConsistency Property Testing (CIFAR10)", "md": "To prove the second consequence, by Property 1\nThis concludes the first direction in the equivalence.       $$h(x, 0) = E[x0 | x0 = x] = x0.$$\nTo prove the second direction, assume that $$h(x, t) = E[h(xt', t') | xt = x]$$ and that $$h(x, 0) = x$$ and notice\nthat by substituting $$t' = 0$$ we derive the following:\nas required.                                    $$h(x, t) = E[h(x0, 0) | xt = x] = E[x0 | xt = x],$$\n\nAdditional Results\n\nProperty Testing\n\nConsistency Property Testing (CIFAR10)"}, {"type": "table", "rows": [["Backward Sampler (300) steps", "0.0025", "Baseline", "Ours"], ["$$", "", "h(xt, t)", "", "_2$$", "0.0015", "", ""], ["$$p(x0", "xt)[x0]$$", "0.0010", "", ""], ["$$x0$$", "0.0005", "", ""], ["", "0.0000", "", ""], ["", "0", "10", "20"], ["", "30", "40", "50"], ["", "60", "70", "80"]], "md": "| Backward Sampler (300) steps | 0.0025 | Baseline | Ours |\n|------------------------------|--------|----------|------|\n| $$||h(xt, t)||_2$$           | 0.0015 |          |      |\n| $$p(x0|xt)[x0]$$             | 0.0010 |          |      |\n| $$x0$$                       | 0.0005 |          |      |\n|                              | 0.0000 |          |      |\n|                              | 0      | 10       | 20   |\n|                              | 30     | 40       | 50   |\n|                              | 60     | 70       | 80   |", "isPerfectTable": false, "csv": "\"Backward Sampler (300) steps\",\"0.0025\",\"Baseline\",\"Ours\"\n\"$$\",\"\",\"h(xt, t)\",\"\",\"_2$$\",\"0.0015\",\"\",\"\"\n\"$$p(x0\",\"xt)[x0]$$\",\"0.0010\",\"\",\"\"\n\"$$x0$$\",\"0.0005\",\"\",\"\"\n\"\",\"0.0000\",\"\",\"\"\n\"\",\"0\",\"10\",\"20\"\n\"\",\"30\",\"40\",\"50\"\n\"\",\"60\",\"70\",\"80\""}, {"type": "text", "value": "Figure 4: Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, $$L_2^{t,t',xt}$$,\nbehaves for $$t' = 0$$, as $$t$$ changes.", "md": "Figure 4: Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, $$L_2^{t,t',xt}$$,\nbehaves for $$t' = 0$$, as $$t$$ changes."}]}, {"page": 26, "text": "                                                           Consistency Property Testing (CIFAR10)\n                                                                 Backward Sampler (300) steps\n                               0.0025                                                                         Baseline\n                                                                                                              Ours\n                            h (x1, 1.0)||2\n                               0.0020\n                               0.0015\n                             p (xt|x1)[h (xt, t)]\n                               0.0010\n                               0.0005\n                              xt\n                             ||\n                               0.0000\n                                           0         10        20        30         40        50      60  70       80\n                                                                                     t\nFigure 5: Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, L2                    t,t\u2032,xt,\nbehaves for t = 0, as t\u2032 changes.\n                                                                                  26", "md": "# Consistency Property Testing (CIFAR10)\n\n## Consistency Property Testing (CIFAR10)\n\nBackward Sampler (300) steps\n\n| |Baseline|Ours|\n|---|---|---|\n|h (x1, 1.0)||2|0.0025| |\n| |0.0020| |\n|p (xt|x1)[h (xt, t)]|0.0010| |\n| |0.0005| |\n|xt| | |\n||||0.0000| |\n\nFigure 5: Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, $$L_2^{t,t',xt}$$, behaves for t = 0, as t' changes.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Consistency Property Testing (CIFAR10)", "md": "# Consistency Property Testing (CIFAR10)"}, {"type": "heading", "lvl": 2, "value": "Consistency Property Testing (CIFAR10)", "md": "## Consistency Property Testing (CIFAR10)"}, {"type": "text", "value": "Backward Sampler (300) steps", "md": "Backward Sampler (300) steps"}, {"type": "table", "rows": [["", "Baseline", "Ours"], ["h (x1, 1.0)", "", "2", "0.0025", ""], ["", "0.0020", ""], ["p (xt", "x1)[h (xt, t)]", "0.0010", ""], ["", "0.0005", ""], ["xt", "", ""], ["", "", "", "0.0000", ""]], "md": "| |Baseline|Ours|\n|---|---|---|\n|h (x1, 1.0)||2|0.0025| |\n| |0.0020| |\n|p (xt|x1)[h (xt, t)]|0.0010| |\n| |0.0005| |\n|xt| | |\n||||0.0000| |", "isPerfectTable": false, "csv": "\"\",\"Baseline\",\"Ours\"\n\"h (x1, 1.0)\",\"\",\"2\",\"0.0025\",\"\"\n\"\",\"0.0020\",\"\"\n\"p (xt\",\"x1)[h (xt, t)]\",\"0.0010\",\"\"\n\"\",\"0.0005\",\"\"\n\"xt\",\"\",\"\"\n\"\",\"\",\"\",\"0.0000\",\"\""}, {"type": "text", "value": "Figure 5: Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, $$L_2^{t,t',xt}$$, behaves for t = 0, as t' changes.", "md": "Figure 5: Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, $$L_2^{t,t',xt}$$, behaves for t = 0, as t' changes."}]}, {"page": 27, "text": "                                                             Consistency Property Testing (FFHQ)\n                                                                 Backward Sampler (1000) steps\n                                0.005          Baseline\n                                               Ours\n                                0.004\n                             h (xt, t)||2\n                                0.003\n                              p (x0|xt)[x0]\n                                0.002\n                               x0\n                              ||0.001\n                                0.000\n                                           0         10        20        30         40        50   60  70  80\n                                                                                     t\nFigure 6: Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, L2              t,t\u2032,xt,\nbehaves for t\u2032 = 0, as t changes.\n                                                                                  27", "md": "# Consistency Property Testing (FFHQ)\n\n## Consistency Property Testing (FFHQ)\n\nBackward Sampler (1000) steps\n\n| |0.005|Baseline|Ours|\n|---|---|---|---|\n|h (xt, t)||2|0.004| | |\n|p (x0|xt)[x0]|0.003| | |\n| |x0|0.002| |\n| |||0.001|0.000| |\n\nFigure 6: Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',x_t}$$, behaves for $$t' = 0$$, as $$t$$ changes.\n\n27", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Consistency Property Testing (FFHQ)", "md": "# Consistency Property Testing (FFHQ)"}, {"type": "heading", "lvl": 2, "value": "Consistency Property Testing (FFHQ)", "md": "## Consistency Property Testing (FFHQ)"}, {"type": "text", "value": "Backward Sampler (1000) steps", "md": "Backward Sampler (1000) steps"}, {"type": "table", "rows": [["", "0.005", "Baseline", "Ours"], ["h (xt, t)", "", "2", "0.004", "", ""], ["p (x0", "xt)[x0]", "0.003", "", ""], ["", "x0", "0.002", ""], ["", "", "", "0.001", "0.000", ""]], "md": "| |0.005|Baseline|Ours|\n|---|---|---|---|\n|h (xt, t)||2|0.004| | |\n|p (x0|xt)[x0]|0.003| | |\n| |x0|0.002| |\n| |||0.001|0.000| |", "isPerfectTable": false, "csv": "\"\",\"0.005\",\"Baseline\",\"Ours\"\n\"h (xt, t)\",\"\",\"2\",\"0.004\",\"\",\"\"\n\"p (x0\",\"xt)[x0]\",\"0.003\",\"\",\"\"\n\"\",\"x0\",\"0.002\",\"\"\n\"\",\"\",\"\",\"0.001\",\"0.000\",\"\""}, {"type": "text", "value": "Figure 6: Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',x_t}$$, behaves for $$t' = 0$$, as $$t$$ changes.\n\n27", "md": "Figure 6: Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',x_t}$$, behaves for $$t' = 0$$, as $$t$$ changes.\n\n27"}]}, {"page": 28, "text": "                                                             Consistency Property Testing (FFHQ)\n                                                                 Backward Sampler (1000) steps\n                                0.005                                                                      Baseline\n                                                                                                           Ours\n                             h (x1, 1.0)||2\n                                0.004\n                                0.003\n                              p (xt|x1)[h (xt, t)]\n                                0.002\n                                0.001\n                               xt\n                              ||\n                                0.000\n                                           0         10        20        30         40        50   60  70       80\n                                                                                     t\nFigure 7: Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, L2                    t,t\u2032,xt,\nbehaves for t = 0, as t\u2032 changes.\n                                                                                  28", "md": "# Consistency Property Testing (FFHQ)\n\n## Consistency Property Testing (FFHQ)\n\nBackward Sampler (1000) steps\n\n| |0.005|Baseline|Ours|\n|---|---|---|---|\n|h (x1, 1.0)||2|0.004| | |\n| |0.003| | |\n|p (xt|x1)[h (xt, t)]|0.002| | |\n| |0.001| | |\n|xt| | | |\n| ||||0.000| |\n\n0 10 20 30 40 50 60 70 80 t\n\nFigure 7: Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t = 0, as t' changes.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Consistency Property Testing (FFHQ)", "md": "# Consistency Property Testing (FFHQ)"}, {"type": "heading", "lvl": 2, "value": "Consistency Property Testing (FFHQ)", "md": "## Consistency Property Testing (FFHQ)"}, {"type": "text", "value": "Backward Sampler (1000) steps", "md": "Backward Sampler (1000) steps"}, {"type": "table", "rows": [["", "0.005", "Baseline", "Ours"], ["h (x1, 1.0)", "", "2", "0.004", "", ""], ["", "0.003", "", ""], ["p (xt", "x1)[h (xt, t)]", "0.002", "", ""], ["", "0.001", "", ""], ["xt", "", "", ""], ["", "", "", "", "0.000", ""]], "md": "| |0.005|Baseline|Ours|\n|---|---|---|---|\n|h (x1, 1.0)||2|0.004| | |\n| |0.003| | |\n|p (xt|x1)[h (xt, t)]|0.002| | |\n| |0.001| | |\n|xt| | | |\n| ||||0.000| |", "isPerfectTable": false, "csv": "\"\",\"0.005\",\"Baseline\",\"Ours\"\n\"h (x1, 1.0)\",\"\",\"2\",\"0.004\",\"\",\"\"\n\"\",\"0.003\",\"\",\"\"\n\"p (xt\",\"x1)[h (xt, t)]\",\"0.002\",\"\",\"\"\n\"\",\"0.001\",\"\",\"\"\n\"xt\",\"\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"0.000\",\"\""}, {"type": "text", "value": "0 10 20 30 40 50 60 70 80 t\n\nFigure 7: Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t = 0, as t' changes.", "md": "0 10 20 30 40 50 60 70 80 t\n\nFigure 7: Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, $$L2_{t,t',xt}$$, behaves for t = 0, as t' changes."}]}, {"page": 29, "text": "C.2    Uncurated Samples\n     Figure 8: Uncurated generated images by our fine-tuned model on FFHQ. FID: 2.61, NFEs: 79.\n                                                 29", "md": "C.2    Uncurated Samples\n\nFigure 8: Uncurated generated images by our fine-tuned model on FFHQ. FID: 2.61, NFEs: 79.\n\n29", "images": [{"name": "page-29-0.jpg", "height": 281, "width": 281, "x": 165, "y": 102}], "items": [{"type": "text", "value": "C.2    Uncurated Samples\n\nFigure 8: Uncurated generated images by our fine-tuned model on FFHQ. FID: 2.61, NFEs: 79.\n\n29", "md": "C.2    Uncurated Samples\n\nFigure 8: Uncurated generated images by our fine-tuned model on FFHQ. FID: 2.61, NFEs: 79.\n\n29"}]}], "job_id": "532436bb-e318-4297-85a4-ba8f18a7dbf1", "file_path": "./corpus/2302.09057.pdf"}