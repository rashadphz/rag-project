{"pages": [{"page": 1, "text": "                            LIBERO: Benchmarking Knowledge Transfer for\n                                                         Lifelong Robot Learning\n                                                  \u2020Bo Liu,  \u2217\u2020 Yifeng Zhu     \u2217 ,\u2021Chongkai Gao        \u2217, \u2020Yihao Feng\n                                                            \u2020Qiang Liu,     \u2020 Yuke Zhu,     \u2020,\u00a7 Peter Stone\n                                           \u2020The University of Texas at Austin,          \u00a7Sony AI,    \u2021Tsinghua University\n                                              {bliu,yifengz,lqiang,yukez,pstone}@cs.utexas.edu\n                                               yihao.ac@gmail.com, gck20@mails.tsinghua.edu.cn\n                                                                              Abstract\n                                  Lifelong learning offers a promising paradigm of building a generalist agent that\n                                  learns and adapts over its lifespan. Unlike traditional lifelong learning problems in\n                                  image and text domains, which primarily involve the transfer of declarative knowl-\n                                  edge of entities and concepts, lifelong learning in decision-making (LLDM) also\n                                  necessitates the transfer of procedural knowledge, such as actions and behaviors.\n                                  To advance research in LLDM, we introduce LIBERO, a novel benchmark of\n                                  lifelong learning for robot manipulation. Specifically, LIBERO highlights five key\n                                  research topics in LLDM:            1) how to efficiently transfer declarative knowledge,\n                                  procedural knowledge, or the mixture of both;               2)  how to design effective policy\n                                  architectures and     3) effective algorithms for LLDM;           4) the robustness of a lifelong\n                                  learner with respect to task ordering; and             5) the effect of model pretraining for\n                                   LLDM. We develop an extendible                procedural generation         pipeline that can in\n                                  principle generate infinitely many tasks. For benchmarking purpose, we create\n                                  four task suites (130 tasks in total) that we use to investigate the above-mentioned\n                                  research topics. To support sample-efficient learning, we provide high-quality\n                                  human-teleoperated demonstration data for all tasks. Our extensive experiments\n                                  present several insightful or even          unexpected     discoveries: sequential finetuning\n                                  outperforms existing lifelong learning methods in forward transfer, no single visual\n                                  encoder architecture excels at all types of knowledge transfer, and naive supervised\n                                  pretraining can hinder agents\u2019 performance in the subsequent LLDM.                      2\n                       1     Introduction\narXiv:2306.03310v2 [cs.AI] 14 Oct 2023A longstanding goal in machine learning is to develop a generalist agent that can perform a wide\n                       range of tasks. While multitask learning [10] is one approach, it is computationally demanding and\n                       not adaptable to ongoing changes. Lifelong learning [65], however, offers a practical solution by\n                       amortizing the learning process over the agent\u2019s lifespan. Its goal is to leverage prior knowledge to\n                       facilitate learning new tasks (forward transfer) and use the newly acquired knowledge to enhance\n                       performance on prior tasks (backward transfer).\n                       The main body of the lifelong learning literature has focused on how agents transfer                          declarative\n                       knowledge in visual or language tasks, which pertains to                 declarative knowledge        about entities and\n                       concepts [7,     40]. Yet it is understudied how agents transfer knowledge in decision-making tasks,\n                       which involves a mixture of both          declarative   and  procedural    knowledge (knowledge about how to              do\n                       something). Consider a scenario where a robot, initially trained to retrieve juice from a fridge, fails\n                           \u2217Equal contribution.\n                           2Check the website athttps://libero-project.github.io                     for the code and the datasets.\n                       37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "md": "# LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning\n\n## LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning\n\nAuthors: Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone\n\nAffiliations: The University of Texas at Austin, Sony AI, Tsinghua University\n\nContact: {bliu,yifengz,lqiang,yukez,pstone}@cs.utexas.edu, yihao.ac@gmail.com, gck20@mails.tsinghua.edu.cn\n\n### Abstract\n\nLifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors.\n\nTo advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM:\n\n1. How to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both\n2. How to design effective policy architectures\n3. Effective algorithms for LLDM\n4. The robustness of a lifelong learner with respect to task ordering\n5. The effect of model pretraining for LLDM\n\nWe develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents\u2019 performance in the subsequent LLDM.\n\n### Introduction\n\nA longstanding goal in machine learning is to develop a generalist agent that can perform a wide range of tasks. While multitask learning is one approach, it is computationally demanding and not adaptable to ongoing changes. Lifelong learning, however, offers a practical solution by amortizing the learning process over the agent\u2019s lifespan. Its goal is to leverage prior knowledge to facilitate learning new tasks (forward transfer) and use the newly acquired knowledge to enhance performance on prior tasks (backward transfer).\n\nThe main body of the lifelong learning literature has focused on how agents transfer declarative knowledge in visual or language tasks, which pertains to declarative knowledge about entities and concepts. Yet it is understudied how agents transfer knowledge in decision-making tasks, which involves a mixture of both declarative and procedural knowledge (knowledge about how to do something). Consider a scenario where a robot, initially trained to retrieve juice from a fridge, fails.\n\nNote: Check the website at https://libero-project.github.io for the code and the datasets.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning", "md": "# LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning"}, {"type": "heading", "lvl": 2, "value": "LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning", "md": "## LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning"}, {"type": "text", "value": "Authors: Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone\n\nAffiliations: The University of Texas at Austin, Sony AI, Tsinghua University\n\nContact: {bliu,yifengz,lqiang,yukez,pstone}@cs.utexas.edu, yihao.ac@gmail.com, gck20@mails.tsinghua.edu.cn", "md": "Authors: Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone\n\nAffiliations: The University of Texas at Austin, Sony AI, Tsinghua University\n\nContact: {bliu,yifengz,lqiang,yukez,pstone}@cs.utexas.edu, yihao.ac@gmail.com, gck20@mails.tsinghua.edu.cn"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors.\n\nTo advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM:\n\n1. How to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both\n2. How to design effective policy architectures\n3. Effective algorithms for LLDM\n4. The robustness of a lifelong learner with respect to task ordering\n5. The effect of model pretraining for LLDM\n\nWe develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents\u2019 performance in the subsequent LLDM.", "md": "Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors.\n\nTo advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM:\n\n1. How to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both\n2. How to design effective policy architectures\n3. Effective algorithms for LLDM\n4. The robustness of a lifelong learner with respect to task ordering\n5. The effect of model pretraining for LLDM\n\nWe develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents\u2019 performance in the subsequent LLDM."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "A longstanding goal in machine learning is to develop a generalist agent that can perform a wide range of tasks. While multitask learning is one approach, it is computationally demanding and not adaptable to ongoing changes. Lifelong learning, however, offers a practical solution by amortizing the learning process over the agent\u2019s lifespan. Its goal is to leverage prior knowledge to facilitate learning new tasks (forward transfer) and use the newly acquired knowledge to enhance performance on prior tasks (backward transfer).\n\nThe main body of the lifelong learning literature has focused on how agents transfer declarative knowledge in visual or language tasks, which pertains to declarative knowledge about entities and concepts. Yet it is understudied how agents transfer knowledge in decision-making tasks, which involves a mixture of both declarative and procedural knowledge (knowledge about how to do something). Consider a scenario where a robot, initially trained to retrieve juice from a fridge, fails.\n\nNote: Check the website at https://libero-project.github.io for the code and the datasets.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "md": "A longstanding goal in machine learning is to develop a generalist agent that can perform a wide range of tasks. While multitask learning is one approach, it is computationally demanding and not adaptable to ongoing changes. Lifelong learning, however, offers a practical solution by amortizing the learning process over the agent\u2019s lifespan. Its goal is to leverage prior knowledge to facilitate learning new tasks (forward transfer) and use the newly acquired knowledge to enhance performance on prior tasks (backward transfer).\n\nThe main body of the lifelong learning literature has focused on how agents transfer declarative knowledge in visual or language tasks, which pertains to declarative knowledge about entities and concepts. Yet it is understudied how agents transfer knowledge in decision-making tasks, which involves a mixture of both declarative and procedural knowledge (knowledge about how to do something). Consider a scenario where a robot, initially trained to retrieve juice from a fridge, fails.\n\nNote: Check the website at https://libero-project.github.io for the code and the datasets.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks."}]}, {"page": 2, "text": "                                                               Procedural          InvolvedeclarativeknowledgeInvolveproceduralknowledge\n      Differentlayouts,same objects       LIBERO-Object       Generation      LIBERO-Goal     Differentgoals,same  objects&layout\n     Differentobjects,same layout        LIBERO-Spatial                        LIBERO-100       Diverse objects,layouts,backgrounds\n     Distribution  Shifts    Algorithmic   Designs     Neural   Architectures       Task  Orderings         Pretraining  Effects\n Figure 1:   Top: LIBERO has four procedurally-generated task suites: LIBERO-SPATIAL, LIBERO-\n OBJECT, and LIBERO-GOAL                  have 10 tasks each and require transferring knowledge about spatial\n relationships, objects, and task goals; LIBERO-100 has 100 tasks and requires the transfer of\n entangled knowledge.          Bottom: we investigate five key research topics in LLDM on LIBERO.\n after learning new tasks. This could be due to forgetting the juice or fridge\u2019s location (declarative\n knowledge) or how to open the fridge or grasp the juice (procedural knowledge). So far, we lack\n methods to systematically and quantitatively analyze this complex knowledge transfer.\nTo bridge this research gap, this paper introduces a new simulation benchmark, LIfelong learning\n BEchmark on RObot manipulation tasks, LIBERO, to facilitate the systematic study of lifelong\n learning in decision making (LLDM). An ideal LLDM testbed should enable continuous learning\n across an expanding set of diverse tasks that share concepts and actions. LIBERO supports this\n through a procedural generation pipeline for endless task creation, based on robot manipulation tasks\nwith shared visual concepts (declarative knowledge) and interactions (procedural knowledge).\n For benchmarking purpose, LIBERO generates 130 language-conditioned robot manipulation tasks\n inspired by human activities [22] and, grouped into four suites. The four task suites are designed\n to examine distribution shifts in the object types, the spatial arrangement of objects, the task goals,\n or the mixture of the previous three (top row of Figure 1). LIBERO is scalable, extendable, and\n designed explicitly for studying lifelong learning in robot manipulation. To support efficient learning,\nwe provide high-quality, human-teleoperated demonstration data for all 130 tasks.\nWe present an initial study using LIBERO to investigate five major research topics in LLDM\n(Figure 1):     1)  knowledge transfer with different types of distribution shift;                      2) neural architecture\n design;   3)  lifelong learning algorithm design;             4)  robustness of the learner to task ordering; and                5)\n how to leverage pre-trained models in LLDM (bottom row of Figure 1). We perform extensive\n experiments across different policy architectures and different lifelong learning algorithms. Based on\n our experiments, we make several insightful or even                   unexpected       observations:\n        1.  Policy architecture design is as crucial as lifelong learning algorithms. The transformer\n            architecture is better at abstracting temporal information than a recurrent neural network.\n            Vision transformers work well on tasks with rich visual information (e.g., a variety of\n            objects). Convolution networks work well when tasks primarily need procedural knowledge.\n        2.  While the lifelong learning algorithms we evaluated are effective at preventing forgetting,\n            they generally perform          worse    than sequential finetuning in terms of forward transfer.\n        3.  Our experiment shows that using pretrained language embeddings of semantically-rich task\n            descriptions yields performance             no better    than using those of the task IDs.\n                                                                  2", "md": "# LIBERO Research Paper\n\n## Procedural vs Declarative Knowledge in Lifelong Learning\n\nDifferent layouts, same objects: LIBERO-Object Generation, LIBERO-Goal, Different goals, same objects & layout\n\nDifferent objects, same layout: LIBERO-Spatial, LIBERO-100, Diverse objects, layouts, backgrounds\n\nDistribution Shifts, Algorithmic Designs, Neural Architectures, Task Orderings, Pretraining Effects\n\nFigure 1:\n\nTop: LIBERO has four procedurally-generated task suites: LIBERO-SPATIAL, LIBERO-OBJECT, and LIBERO-GOAL have 10 tasks each and require transferring knowledge about spatial relationships, objects, and task goals; LIBERO-100 has 100 tasks and requires the transfer of entangled knowledge.\n\nBottom: we investigate five key research topics in LLDM on LIBERO after learning new tasks. This could be due to forgetting the juice or fridge\u2019s location (declarative knowledge) or how to open the fridge or grasp the juice (procedural knowledge). So far, we lack methods to systematically and quantitatively analyze this complex knowledge transfer.\n\nTo bridge this research gap, this paper introduces a new simulation benchmark, LIfelong learning BEchmark on RObot manipulation tasks, LIBERO, to facilitate the systematic study of lifelong learning in decision making (LLDM). An ideal LLDM testbed should enable continuous learning across an expanding set of diverse tasks that share concepts and actions. LIBERO supports this through a procedural generation pipeline for endless task creation, based on robot manipulation tasks with shared visual concepts (declarative knowledge) and interactions (procedural knowledge).\n\nFor benchmarking purpose, LIBERO generates 130 language-conditioned robot manipulation tasks inspired by human activities [22] and, grouped into four suites. The four task suites are designed to examine distribution shifts in the object types, the spatial arrangement of objects, the task goals, or the mixture of the previous three (top row of Figure 1). LIBERO is scalable, extendable, and designed explicitly for studying lifelong learning in robot manipulation. To support efficient learning, we provide high-quality, human-teleoperated demonstration data for all 130 tasks.\n\nWe present an initial study using LIBERO to investigate five major research topics in LLDM (Figure 1):\n\n1. Knowledge transfer with different types of distribution shift\n2. Neural architecture design\n3. Lifelong learning algorithm design\n4. Robustness of the learner to task ordering\n5. How to leverage pre-trained models in LLDM\n\nWe perform extensive experiments across different policy architectures and different lifelong learning algorithms. Based on our experiments, we make several insightful or even unexpected observations:\n\n1. Policy architecture design is as crucial as lifelong learning algorithms. The transformer architecture is better at abstracting temporal information than a recurrent neural network. Vision transformers work well on tasks with rich visual information (e.g., a variety of objects). Convolution networks work well when tasks primarily need procedural knowledge.\n2. While the lifelong learning algorithms we evaluated are effective at preventing forgetting, they generally perform worse than sequential finetuning in terms of forward transfer.\n3. Our experiment shows that using pretrained language embeddings of semantically-rich task descriptions yields performance no better than using those of the task IDs.", "images": [{"name": "img_p1_1", "height": 64, "width": 97}, {"name": "img_p1_2", "height": 116, "width": 116}, {"name": "img_p1_3", "height": 91, "width": 225}, {"name": "img_p1_4", "height": 64, "width": 64}, {"name": "img_p1_5", "height": 36, "width": 36}, {"name": "img_p1_6", "height": 32, "width": 32}, {"name": "img_p1_6", "height": 37, "width": 37}, {"name": "img_p1_6", "height": 37, "width": 37}, {"name": "img_p1_5", "height": 36, "width": 36}, {"name": "img_p1_6", "height": 37, "width": 37}, {"name": "img_p1_5", "height": 36, "width": 36}, {"name": "img_p1_7", "height": 142, "width": 324}, {"name": "img_p1_8", "height": 104, "width": 104}, {"name": "img_p1_9", "height": 104, "width": 104}, {"name": "img_p1_10", "height": 139, "width": 324}, {"name": "img_p1_11", "height": 104, "width": 104}, {"name": "img_p1_12", "height": 104, "width": 104}, {"name": "img_p1_13", "height": 135, "width": 354}, {"name": "img_p1_14", "height": 98, "width": 98}, {"name": "img_p1_15", "height": 98, "width": 98}, {"name": "img_p1_16", "height": 195, "width": 342}, {"name": "img_p1_17", "height": 87, "width": 87}, {"name": "img_p1_18", "height": 87, "width": 87}, {"name": "img_p1_19", "height": 87, "width": 87}, {"name": "img_p1_20", "height": 87, "width": 87}, {"name": "img_p1_21", "height": 87, "width": 87}, {"name": "img_p1_1", "height": 97, "width": 97}, {"name": "img_p1_22", "height": 78, "width": 78}, {"name": "img_p1_23", "height": 64, "width": 64}, {"name": "img_p1_24", "height": 192, "width": 238}, {"name": "img_p1_25", "height": 62, "width": 238}, {"name": "img_p1_26", "height": 192, "width": 246}, {"name": "img_p1_27", "height": 62, "width": 246}, {"name": "img_p1_28", "height": 34, "width": 34}, {"name": "img_p1_29", "height": 52, "width": 52}, {"name": "img_p1_30", "height": 70, "width": 70}, {"name": "img_p1_31", "height": 56, "width": 56}, {"name": "img_p1_32", "height": 192, "width": 238}, {"name": "img_p1_33", "height": 62, "width": 238}, {"name": "img_p1_34", "height": 50, "width": 50}, {"name": "img_p1_35", "height": 45, "width": 45}, {"name": "img_p1_36", "height": 40, "width": 40}, {"name": "img_p1_35", "height": 45, "width": 45}, {"name": "img_p1_34", "height": 50, "width": 50}, {"name": "img_p1_37", "height": 192, "width": 238}, {"name": "img_p1_38", "height": 62, "width": 238}, {"name": "img_p1_39", "height": 59, "width": 59}, {"name": "img_p1_39", "height": 59, "width": 59}, {"name": "img_p1_40", "height": 29, "width": 29}, {"name": "img_p1_41", "height": 38, "width": 38}], "items": [{"type": "heading", "lvl": 1, "value": "LIBERO Research Paper", "md": "# LIBERO Research Paper"}, {"type": "heading", "lvl": 2, "value": "Procedural vs Declarative Knowledge in Lifelong Learning", "md": "## Procedural vs Declarative Knowledge in Lifelong Learning"}, {"type": "text", "value": "Different layouts, same objects: LIBERO-Object Generation, LIBERO-Goal, Different goals, same objects & layout\n\nDifferent objects, same layout: LIBERO-Spatial, LIBERO-100, Diverse objects, layouts, backgrounds\n\nDistribution Shifts, Algorithmic Designs, Neural Architectures, Task Orderings, Pretraining Effects\n\nFigure 1:\n\nTop: LIBERO has four procedurally-generated task suites: LIBERO-SPATIAL, LIBERO-OBJECT, and LIBERO-GOAL have 10 tasks each and require transferring knowledge about spatial relationships, objects, and task goals; LIBERO-100 has 100 tasks and requires the transfer of entangled knowledge.\n\nBottom: we investigate five key research topics in LLDM on LIBERO after learning new tasks. This could be due to forgetting the juice or fridge\u2019s location (declarative knowledge) or how to open the fridge or grasp the juice (procedural knowledge). So far, we lack methods to systematically and quantitatively analyze this complex knowledge transfer.\n\nTo bridge this research gap, this paper introduces a new simulation benchmark, LIfelong learning BEchmark on RObot manipulation tasks, LIBERO, to facilitate the systematic study of lifelong learning in decision making (LLDM). An ideal LLDM testbed should enable continuous learning across an expanding set of diverse tasks that share concepts and actions. LIBERO supports this through a procedural generation pipeline for endless task creation, based on robot manipulation tasks with shared visual concepts (declarative knowledge) and interactions (procedural knowledge).\n\nFor benchmarking purpose, LIBERO generates 130 language-conditioned robot manipulation tasks inspired by human activities [22] and, grouped into four suites. The four task suites are designed to examine distribution shifts in the object types, the spatial arrangement of objects, the task goals, or the mixture of the previous three (top row of Figure 1). LIBERO is scalable, extendable, and designed explicitly for studying lifelong learning in robot manipulation. To support efficient learning, we provide high-quality, human-teleoperated demonstration data for all 130 tasks.\n\nWe present an initial study using LIBERO to investigate five major research topics in LLDM (Figure 1):\n\n1. Knowledge transfer with different types of distribution shift\n2. Neural architecture design\n3. Lifelong learning algorithm design\n4. Robustness of the learner to task ordering\n5. How to leverage pre-trained models in LLDM\n\nWe perform extensive experiments across different policy architectures and different lifelong learning algorithms. Based on our experiments, we make several insightful or even unexpected observations:\n\n1. Policy architecture design is as crucial as lifelong learning algorithms. The transformer architecture is better at abstracting temporal information than a recurrent neural network. Vision transformers work well on tasks with rich visual information (e.g., a variety of objects). Convolution networks work well when tasks primarily need procedural knowledge.\n2. While the lifelong learning algorithms we evaluated are effective at preventing forgetting, they generally perform worse than sequential finetuning in terms of forward transfer.\n3. Our experiment shows that using pretrained language embeddings of semantically-rich task descriptions yields performance no better than using those of the task IDs.", "md": "Different layouts, same objects: LIBERO-Object Generation, LIBERO-Goal, Different goals, same objects & layout\n\nDifferent objects, same layout: LIBERO-Spatial, LIBERO-100, Diverse objects, layouts, backgrounds\n\nDistribution Shifts, Algorithmic Designs, Neural Architectures, Task Orderings, Pretraining Effects\n\nFigure 1:\n\nTop: LIBERO has four procedurally-generated task suites: LIBERO-SPATIAL, LIBERO-OBJECT, and LIBERO-GOAL have 10 tasks each and require transferring knowledge about spatial relationships, objects, and task goals; LIBERO-100 has 100 tasks and requires the transfer of entangled knowledge.\n\nBottom: we investigate five key research topics in LLDM on LIBERO after learning new tasks. This could be due to forgetting the juice or fridge\u2019s location (declarative knowledge) or how to open the fridge or grasp the juice (procedural knowledge). So far, we lack methods to systematically and quantitatively analyze this complex knowledge transfer.\n\nTo bridge this research gap, this paper introduces a new simulation benchmark, LIfelong learning BEchmark on RObot manipulation tasks, LIBERO, to facilitate the systematic study of lifelong learning in decision making (LLDM). An ideal LLDM testbed should enable continuous learning across an expanding set of diverse tasks that share concepts and actions. LIBERO supports this through a procedural generation pipeline for endless task creation, based on robot manipulation tasks with shared visual concepts (declarative knowledge) and interactions (procedural knowledge).\n\nFor benchmarking purpose, LIBERO generates 130 language-conditioned robot manipulation tasks inspired by human activities [22] and, grouped into four suites. The four task suites are designed to examine distribution shifts in the object types, the spatial arrangement of objects, the task goals, or the mixture of the previous three (top row of Figure 1). LIBERO is scalable, extendable, and designed explicitly for studying lifelong learning in robot manipulation. To support efficient learning, we provide high-quality, human-teleoperated demonstration data for all 130 tasks.\n\nWe present an initial study using LIBERO to investigate five major research topics in LLDM (Figure 1):\n\n1. Knowledge transfer with different types of distribution shift\n2. Neural architecture design\n3. Lifelong learning algorithm design\n4. Robustness of the learner to task ordering\n5. How to leverage pre-trained models in LLDM\n\nWe perform extensive experiments across different policy architectures and different lifelong learning algorithms. Based on our experiments, we make several insightful or even unexpected observations:\n\n1. Policy architecture design is as crucial as lifelong learning algorithms. The transformer architecture is better at abstracting temporal information than a recurrent neural network. Vision transformers work well on tasks with rich visual information (e.g., a variety of objects). Convolution networks work well when tasks primarily need procedural knowledge.\n2. While the lifelong learning algorithms we evaluated are effective at preventing forgetting, they generally perform worse than sequential finetuning in terms of forward transfer.\n3. Our experiment shows that using pretrained language embeddings of semantically-rich task descriptions yields performance no better than using those of the task IDs."}]}, {"page": 3, "text": "        4.  Basic supervised pretraining on a large-scale offline dataset can have a                    negative   impact on\n            the learner\u2019s downstream performance in LLDM.\n2     Background\nThis section introduces the problem formulation and defines key terms used throughout the paper.\n2.1    Markov Decision Process for Robot Learning\nA robot learning problem can be formulated as a finite-horizon Markov Decision Process:                                 M    =\n(S,A,    T , H, \u03bc   0, R).  Here,   S   and  A   are the state and action spaces of the robot.              \u03bc 0 is the initial\nstate distribution,     R   : S \u00d7 A \u2192       R  is the reward function, and          T   : S \u00d7 A \u2192 S        is the transition\nfunction. In this work, we assume a sparse-reward setting and replace                           R   with a goal predicate\ng  : S \u2192 {0,      1}. The robot\u2019s objective is to learn a policy              \u03c0  that maximizes the expected return:\nmax   \u03c0 J(\u03c0) =     E s t,at\u223c\u03c0,\u03bc  0[PH t=1   g(s t)].\n2.2    Lifelong Robot Learning Problem                                                                  1          K\nIn a  lifelong robot learning problem, a robot sequentially learns over                  K   tasks  {T   , . . . , T }  with a\nsingle policy\u03c0. We assume          \u03c0  is conditioned on the task, i.e.,     \u03c0(\u00b7 |  s;T). For each task,      T  k \u2261   (\u03bc k, gk )\n                                                                                                                         0\nis defined by the initial state distribution         \u03bc k and the goal predicate        gk .3 We assume       S, A,T    , H  are\nthe same for all tasks. Up to the         k-th task   T0k , the robot aims to optimize\n                                                  1 X k  \u0014                       \u0014 XL     p   p  \u0015\u0015\n                           max    J LRL (\u03c0) =                p  p    E         p        g   (st )   .                       (1)\n                             \u03c0                   k  p=1     st,at\u223c\u03c0(\u00b7;T   p), \u03bc0   t=1\nAn important feature of the lifelong setting is that the agent loses access to the previous                      k  \u2212  1  tasks\nwhen it learns on task       T k .\nLifelong Imitation Learning                Due to the challenge of sparse-reward reinforcement learning, we\nconsider a practical alternative setting where a user would provide a small demonstration dataset\nfor each task in the sequence. Denote                D k  =   {\u03c4  k} N     as N   demonstrations for task          T k . Each\n                                                                 i   i=1\n\u03c4 k  = (o  0 , a0, o1 , a1, . . . ,lok)where    lk  \u2264   H. Here,     o t is the robot\u2019s sensory input, including the\n  i\nperceptual observation and the information about the robot\u2019s joints and gripper. In practice, the\nobservation    o t is often non-Markovian. Therefore, following works in partially observable MDPs [25],\nwe represent      s  by the aggregated history of observations, i.e.               s   \u2261   o    \u225c   (o  , o  , . . . , o). This\n                   t                                                                 t      \u2264t         0   1         t\nresults in the    lifelong imitation learning problem            with the same objective as in Eq.          (1). But during\ntraining, we perform behavioral cloning [4] with the following surrogate objective function:\n                          min    J    (\u03c0) =    1  X k              \u0014 Xlp  L \u25af \u03c0(o     ;T  p), ap \u0001\u0015  ,                      (2)\n                            \u03c0      BC          k       o ,a E\u223cD  p                 \u2264t          t\n                                                  p=1   t   t        t=0\nwhere    L  is a supervised learning loss, e.g., the negative log-likelihood loss, and                     \u03c0  is a Gaussian\nmixture model. Similarly, we assume              {D  p  :p < k}     are not fully available when learning          T  k.\n3     Research Topics in LLDM\nWe outline five major research topics in LLDM that motivate the design of LIBERO and our study.\n(T1) Transfer of Different Types of Knowledge                        In order to accomplish a task such as             put the\nketchup next to the plate in the basket, a robot must understand the concept                         ketchup, the location\nof the   plate/basket, and how to         put  the ketchup in the basket. Indeed, robot manipulation tasks in\ngeneral necessitate different types of knowledge, making it hard to determine the cause of failure.\nWe present four task suites in Section 4.2: three task suites for studying the transfer of knowledge\nabout spatial relationships, object concepts, and task goals in a disentangled manner, and one suite\nfor studying the transfer of mixed types of knowledge.\n    3Throughout the paper, a superscript/subscript is used to index the task/time step.\n                                                               3", "md": "# Document\n\n## Background\n\nThis section introduces the problem formulation and defines key terms used throughout the paper.\n\n### Markov Decision Process for Robot Learning\n\nA robot learning problem can be formulated as a finite-horizon Markov Decision Process: $$M = (S, A, T, H, \\mu_0, R)$$. Here, $$S$$ and $$A$$ are the state and action spaces of the robot. $$\\mu_0$$ is the initial state distribution, $$R : S \\times A \\rightarrow \\mathbb{R}$$ is the reward function, and $$T : S \\times A \\rightarrow S$$ is the transition function. In this work, we assume a sparse-reward setting and replace $$R$$ with a goal predicate $$g : S \\rightarrow \\{0, 1\\}$$. The robot's objective is to learn a policy $$\\pi$$ that maximizes the expected return: $$\\max_{\\pi} J(\\pi) = \\mathbb{E}_{s_t, a_t \\sim \\pi, \\mu_0}[\\sum_{t=1}^{H} g(s_t)]$$.\n\n### Lifelong Robot Learning Problem\n\nIn a lifelong robot learning problem, a robot sequentially learns over $$K$$ tasks $$\\{T_1, ..., T_K\\}$$ with a single policy $$\\pi$$. We assume $$\\pi$$ is conditioned on the task, i.e., $$\\pi(\\cdot | s; T)$$. For each task, $$T_k \\equiv (\\mu_k, g_k)$$ is defined by the initial state distribution $$\\mu_k$$ and the goal predicate $$g_k$$. We assume $$S, A, T, H$$ are the same for all tasks. Up to the $$k$$-th task $$T_{0k}$$, the robot aims to optimize: $$\\max_{\\pi} J_{LRL}(\\pi) = \\sum_{k=1}^{K} \\mathbb{E}_{s_t, a_t \\sim \\pi(\\cdot; T_p), \\mu_0} g(st)$$. An important feature of the lifelong setting is that the agent loses access to the previous $$k - 1$$ tasks when it learns on task $$T_k$$.\n\n### Lifelong Imitation Learning\n\nDue to the challenge of sparse-reward reinforcement learning, we consider a practical alternative setting where a user would provide a small demonstration dataset for each task in the sequence. Denote $$D_k = \\{\\tau_k\\}_N$$ as $$N$$ demonstrations for task $$T_k$$. Each $$\\tau_k = (o_0, a_0, o_1, a_1, ..., l_k)$$ where $$l_k \\leq H$$. Here, $$o_t$$ is the robot's sensory input, including the perceptual observation and the information about the robot's joints and gripper. In practice, the observation $$o_t$$ is often non-Markovian. Therefore, following works in partially observable MDPs, we represent $$s$$ by the aggregated history of observations, i.e. $$s \\equiv o \\triangleq (o_0, o_1, ..., o_t)$$. This results in the lifelong imitation learning problem with the same objective as in Eq. (1). But during training, we perform behavioral cloning with the following surrogate objective function: $$\\min_{\\pi} J_{BC}(\\pi) = \\sum_{k} \\sum_{t} L \\left( \\pi(o_t; T_p), a_t \\right)$$, where $$L$$ is a supervised learning loss, e.g., the negative log-likelihood loss, and $$\\pi$$ is a Gaussian mixture model. Similarly, we assume $$\\{D_p : p < k\\}$$ are not fully available when learning $$T_k$$.\n\n## Research Topics in LLDM\n\nWe outline five major research topics in LLDM that motivate the design of LIBERO and our study.\n\n1. (T1) Transfer of Different Types of Knowledge\n\nIn order to accomplish a task such as putting the ketchup next to the plate in the basket, a robot must understand the concept of ketchup, the location of the plate/basket, and how to put the ketchup in the basket. Indeed, robot manipulation tasks in general necessitate different types of knowledge, making it hard to determine the cause of failure. We present four task suites in Section 4.2: three task suites for studying the transfer of knowledge about spatial relationships, object concepts, and task goals in a disentangled manner, and one suite for studying the transfer of mixed types of knowledge.\n\nThroughout the paper, a superscript/subscript is used to index the task/time step.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Background", "md": "## Background"}, {"type": "text", "value": "This section introduces the problem formulation and defines key terms used throughout the paper.", "md": "This section introduces the problem formulation and defines key terms used throughout the paper."}, {"type": "heading", "lvl": 3, "value": "Markov Decision Process for Robot Learning", "md": "### Markov Decision Process for Robot Learning"}, {"type": "text", "value": "A robot learning problem can be formulated as a finite-horizon Markov Decision Process: $$M = (S, A, T, H, \\mu_0, R)$$. Here, $$S$$ and $$A$$ are the state and action spaces of the robot. $$\\mu_0$$ is the initial state distribution, $$R : S \\times A \\rightarrow \\mathbb{R}$$ is the reward function, and $$T : S \\times A \\rightarrow S$$ is the transition function. In this work, we assume a sparse-reward setting and replace $$R$$ with a goal predicate $$g : S \\rightarrow \\{0, 1\\}$$. The robot's objective is to learn a policy $$\\pi$$ that maximizes the expected return: $$\\max_{\\pi} J(\\pi) = \\mathbb{E}_{s_t, a_t \\sim \\pi, \\mu_0}[\\sum_{t=1}^{H} g(s_t)]$$.", "md": "A robot learning problem can be formulated as a finite-horizon Markov Decision Process: $$M = (S, A, T, H, \\mu_0, R)$$. Here, $$S$$ and $$A$$ are the state and action spaces of the robot. $$\\mu_0$$ is the initial state distribution, $$R : S \\times A \\rightarrow \\mathbb{R}$$ is the reward function, and $$T : S \\times A \\rightarrow S$$ is the transition function. In this work, we assume a sparse-reward setting and replace $$R$$ with a goal predicate $$g : S \\rightarrow \\{0, 1\\}$$. The robot's objective is to learn a policy $$\\pi$$ that maximizes the expected return: $$\\max_{\\pi} J(\\pi) = \\mathbb{E}_{s_t, a_t \\sim \\pi, \\mu_0}[\\sum_{t=1}^{H} g(s_t)]$$."}, {"type": "heading", "lvl": 3, "value": "Lifelong Robot Learning Problem", "md": "### Lifelong Robot Learning Problem"}, {"type": "text", "value": "In a lifelong robot learning problem, a robot sequentially learns over $$K$$ tasks $$\\{T_1, ..., T_K\\}$$ with a single policy $$\\pi$$. We assume $$\\pi$$ is conditioned on the task, i.e., $$\\pi(\\cdot | s; T)$$. For each task, $$T_k \\equiv (\\mu_k, g_k)$$ is defined by the initial state distribution $$\\mu_k$$ and the goal predicate $$g_k$$. We assume $$S, A, T, H$$ are the same for all tasks. Up to the $$k$$-th task $$T_{0k}$$, the robot aims to optimize: $$\\max_{\\pi} J_{LRL}(\\pi) = \\sum_{k=1}^{K} \\mathbb{E}_{s_t, a_t \\sim \\pi(\\cdot; T_p), \\mu_0} g(st)$$. An important feature of the lifelong setting is that the agent loses access to the previous $$k - 1$$ tasks when it learns on task $$T_k$$.", "md": "In a lifelong robot learning problem, a robot sequentially learns over $$K$$ tasks $$\\{T_1, ..., T_K\\}$$ with a single policy $$\\pi$$. We assume $$\\pi$$ is conditioned on the task, i.e., $$\\pi(\\cdot | s; T)$$. For each task, $$T_k \\equiv (\\mu_k, g_k)$$ is defined by the initial state distribution $$\\mu_k$$ and the goal predicate $$g_k$$. We assume $$S, A, T, H$$ are the same for all tasks. Up to the $$k$$-th task $$T_{0k}$$, the robot aims to optimize: $$\\max_{\\pi} J_{LRL}(\\pi) = \\sum_{k=1}^{K} \\mathbb{E}_{s_t, a_t \\sim \\pi(\\cdot; T_p), \\mu_0} g(st)$$. An important feature of the lifelong setting is that the agent loses access to the previous $$k - 1$$ tasks when it learns on task $$T_k$$."}, {"type": "heading", "lvl": 3, "value": "Lifelong Imitation Learning", "md": "### Lifelong Imitation Learning"}, {"type": "text", "value": "Due to the challenge of sparse-reward reinforcement learning, we consider a practical alternative setting where a user would provide a small demonstration dataset for each task in the sequence. Denote $$D_k = \\{\\tau_k\\}_N$$ as $$N$$ demonstrations for task $$T_k$$. Each $$\\tau_k = (o_0, a_0, o_1, a_1, ..., l_k)$$ where $$l_k \\leq H$$. Here, $$o_t$$ is the robot's sensory input, including the perceptual observation and the information about the robot's joints and gripper. In practice, the observation $$o_t$$ is often non-Markovian. Therefore, following works in partially observable MDPs, we represent $$s$$ by the aggregated history of observations, i.e. $$s \\equiv o \\triangleq (o_0, o_1, ..., o_t)$$. This results in the lifelong imitation learning problem with the same objective as in Eq. (1). But during training, we perform behavioral cloning with the following surrogate objective function: $$\\min_{\\pi} J_{BC}(\\pi) = \\sum_{k} \\sum_{t} L \\left( \\pi(o_t; T_p), a_t \\right)$$, where $$L$$ is a supervised learning loss, e.g., the negative log-likelihood loss, and $$\\pi$$ is a Gaussian mixture model. Similarly, we assume $$\\{D_p : p < k\\}$$ are not fully available when learning $$T_k$$.", "md": "Due to the challenge of sparse-reward reinforcement learning, we consider a practical alternative setting where a user would provide a small demonstration dataset for each task in the sequence. Denote $$D_k = \\{\\tau_k\\}_N$$ as $$N$$ demonstrations for task $$T_k$$. Each $$\\tau_k = (o_0, a_0, o_1, a_1, ..., l_k)$$ where $$l_k \\leq H$$. Here, $$o_t$$ is the robot's sensory input, including the perceptual observation and the information about the robot's joints and gripper. In practice, the observation $$o_t$$ is often non-Markovian. Therefore, following works in partially observable MDPs, we represent $$s$$ by the aggregated history of observations, i.e. $$s \\equiv o \\triangleq (o_0, o_1, ..., o_t)$$. This results in the lifelong imitation learning problem with the same objective as in Eq. (1). But during training, we perform behavioral cloning with the following surrogate objective function: $$\\min_{\\pi} J_{BC}(\\pi) = \\sum_{k} \\sum_{t} L \\left( \\pi(o_t; T_p), a_t \\right)$$, where $$L$$ is a supervised learning loss, e.g., the negative log-likelihood loss, and $$\\pi$$ is a Gaussian mixture model. Similarly, we assume $$\\{D_p : p < k\\}$$ are not fully available when learning $$T_k$$."}, {"type": "heading", "lvl": 2, "value": "Research Topics in LLDM", "md": "## Research Topics in LLDM"}, {"type": "text", "value": "We outline five major research topics in LLDM that motivate the design of LIBERO and our study.\n\n1. (T1) Transfer of Different Types of Knowledge\n\nIn order to accomplish a task such as putting the ketchup next to the plate in the basket, a robot must understand the concept of ketchup, the location of the plate/basket, and how to put the ketchup in the basket. Indeed, robot manipulation tasks in general necessitate different types of knowledge, making it hard to determine the cause of failure. We present four task suites in Section 4.2: three task suites for studying the transfer of knowledge about spatial relationships, object concepts, and task goals in a disentangled manner, and one suite for studying the transfer of mixed types of knowledge.\n\nThroughout the paper, a superscript/subscript is used to index the task/time step.", "md": "We outline five major research topics in LLDM that motivate the design of LIBERO and our study.\n\n1. (T1) Transfer of Different Types of Knowledge\n\nIn order to accomplish a task such as putting the ketchup next to the plate in the basket, a robot must understand the concept of ketchup, the location of the plate/basket, and how to put the ketchup in the basket. Indeed, robot manipulation tasks in general necessitate different types of knowledge, making it hard to determine the cause of failure. We present four task suites in Section 4.2: three task suites for studying the transfer of knowledge about spatial relationships, object concepts, and task goals in a disentangled manner, and one suite for studying the transfer of mixed types of knowledge.\n\nThroughout the paper, a superscript/subscript is used to index the task/time step."}]}, {"page": 4, "text": "      Scenes            Human activity dataset    Specify Objects And Layouts\n                                                     object\n                                                     black_bowl_1black_bowl\n                                                     plate   plate\n                              (1)                    fixture\n                                                     kitchentable kitchentable\n                              open                   cabinet  woodencabinet\n                              put  in :\n                              turn on              (:regions\n                              (2)                     bowl_region                   Init Configuration         Goal States\n                                                        target kitchen_table)\n                          \"Open the top drawer of the (-0.02 0.02 0.020.02)\n                        cabinet and put the bowl in it\n                              (3)                             (A)                                 Specify Task Goals\n                                                   Specify Init Configuration               (:goal (and\n                                                 (:init                                       (Open cabinettop_drawer)\n                                                   (Oncabinet_1 cabinet_region)               (Inbowl cabinet top_drawer)\n                                                   (On plate plateinit_region)\n                                                   (On bowl   bowlinit_region)                           (C)\n                                :pddl                         (B)\nFigure 2: LIBERO\u2019s procedural generation pipeline: Extracting behavioral templates from a large-\nscale human activity dataset           (1), Ego4D, for generating task instructions                 (2); Based on the task\ndescription, selecting the scene and generating the PDDL description file                     (3) that specifies the objects\nand layouts     (A), the initial object configurations         (B), and the task goal       (C).\n(T2) Neural Architecture Design                   An important research question in LLDM is how to design\neffective neural architectures to abstract the multi-modal observations (images, language descriptions,\nand robot states) and transfer only relevant knowledge when learning new tasks.\n(T3) Lifelong Learning Algorithm Design                      Given a policy architecture, it is crucial to determine\nwhat learning algorithms to apply for LLDM. Specifically, the sequential nature of LLDM suggests\nthat even minor forgetting over successive steps can potentially lead to a total failure in execution. As\nsuch, we consider the design of lifelong learning algorithms to be an open area of research in LLDM.\n(T4) Robustness to Task Ordering                       It is well-known that task curriculum influences policy\nlearning [6,    48]. A robot in the real world, however, often cannot choose which task to encounter\nfirst. Therefore, a good lifelong learning algorithm should be robust to different task orderings.\n(T5) Usage of Pretrained Models                     In practice, robots will be most likely pretrained on large\ndatasets in factories before deployment [28]. However, it is not well-understood whether or how\npretraining could benefit subsequent LLDM.\n4    LIBERO\nThis section introduces the components in LIBERO: the procedural generation pipeline that allows\nthe never-ending creation of tasks (Section 4.1), the four task suites we generate for benchmarking\n(Section 4.2), five algorithms (Section 4.3), and three neural architectures (Section 4.4).\n4.1    Procedural Generation of Tasks\nResearch in LLDM requires a systematic way to create new tasks while maintaining task diversity\nand relevance to existing tasks. LIBERO procedurally generates new tasks in three steps:                             1)  extract\nbehavioral templates from language annotations of human activities and generate sampled tasks\ndescribed in natural language based on such templates;                 2) specify an initial object distribution given a\ntask description; and      3) specify task goals using a propositional formula that aligns with the language\ninstructions. Our generation pipeline is built on top of                  Robosuite       [76], a modular manipulation\nsimulator that offers seamless integration. Figure 2 illustrates an example of task creation using this\npipeline, and each component is expanded upon below.\nBehavioral Templates and Instruction Generation                         Human activities serve as a fertile source of\ntasks that can inspire and generate a vast number of manipulation tasks. We choose a large-scale\nactivity dataset, Ego4D [22], which includes a large variety of everyday activities with language\nannotations. We pre-process the dataset by extracting the language descriptions and then summarize\nthem into a large set of commonly used language templates. After this pre-processing step, we use\nthe templates and select objects available in the simulator to generate a set of task descriptions in the\n                                                                4", "md": "# Document\n\n## Scenes Human activity dataset Specify Objects And Layouts\n\n|object|black_bowl_1|black_bowl|plate|\n|---|---|---|---|\n|fixture|kitchen_table|wooden_cabinet| |\n\n(1) open cabinet\n\nput in: turn on\n\n(2) \"Open the top drawer of the cabinet and put the bowl in it\"\n\n(3) (A) Specify Task Goals\n\n(B) Specify Init Configuration\n\n(C) Goal States\n\n:regions (bowl_region target kitchen_table) (-0.02 0.02 0.020.02)\n\n$$\n\\text{Init Configuration:} \\\\\n\\text{(:init} \\\\\n\\text{(On cabinet_1 cabinet_region)} \\\\\n\\text{(In bowl cabinet top_drawer)} \\\\\n\\text{(On plate plate_init_region)} \\\\\n\\text{(On bowl bowl_init_region)} \\\\\n\\text{)} \\\\\n$$\n### Figure 2: LIBERO\u2019s procedural generation pipeline\n\nExtracting behavioral templates from a large-scale human activity dataset (1), Ego4D, for generating task instructions (2); Based on the task description, selecting the scene and generating the PDDL description file (3) that specifies the objects and layouts (A), the initial object configurations (B), and the task goal (C).\n\n### (T2) Neural Architecture Design\n\nAn important research question in LLDM is how to design effective neural architectures to abstract the multi-modal observations (images, language descriptions, and robot states) and transfer only relevant knowledge when learning new tasks.\n\n### (T3) Lifelong Learning Algorithm Design\n\nGiven a policy architecture, it is crucial to determine what learning algorithms to apply for LLDM. Specifically, the sequential nature of LLDM suggests that even minor forgetting over successive steps can potentially lead to a total failure in execution. As such, we consider the design of lifelong learning algorithms to be an open area of research in LLDM.\n\n### (T4) Robustness to Task Ordering\n\nIt is well-known that task curriculum influences policy learning [6, 48]. A robot in the real world, however, often cannot choose which task to encounter first. Therefore, a good lifelong learning algorithm should be robust to different task orderings.\n\n### (T5) Usage of Pretrained Models\n\nIn practice, robots will be most likely pretrained on large datasets in factories before deployment [28]. However, it is not well-understood whether or how pretraining could benefit subsequent LLDM.\n\n## LIBERO\n\nThis section introduces the components in LIBERO: the procedural generation pipeline that allows the never-ending creation of tasks (Section 4.1), the four task suites we generate for benchmarking (Section 4.2), five algorithms (Section 4.3), and three neural architectures (Section 4.4).\n\n### 4.1 Procedural Generation of Tasks\n\nResearch in LLDM requires a systematic way to create new tasks while maintaining task diversity and relevance to existing tasks. LIBERO procedurally generates new tasks in three steps:\n\n1. extract behavioral templates from language annotations of human activities and generate sampled tasks described in natural language based on such templates;\n2. specify an initial object distribution given a task description; and\n3. specify task goals using a propositional formula that aligns with the language instructions.\n\nOur generation pipeline is built on top of Robosuite [76], a modular manipulation simulator that offers seamless integration. Figure 2 illustrates an example of task creation using this pipeline, and each component is expanded upon below.\n\n### Behavioral Templates and Instruction Generation\n\nHuman activities serve as a fertile source of tasks that can inspire and generate a vast number of manipulation tasks. We choose a large-scale activity dataset, Ego4D [22], which includes a large variety of everyday activities with language annotations. We pre-process the dataset by extracting the language descriptions and then summarize them into a large set of commonly used language templates. After this pre-processing step, we use the templates and select objects available in the simulator to generate a set of task descriptions.", "images": [{"name": "img_p3_1", "height": 457, "width": 1188}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Scenes Human activity dataset Specify Objects And Layouts", "md": "## Scenes Human activity dataset Specify Objects And Layouts"}, {"type": "table", "rows": [["object", "black_bowl_1", "black_bowl", "plate"], ["fixture", "kitchen_table", "wooden_cabinet", ""]], "md": "|object|black_bowl_1|black_bowl|plate|\n|---|---|---|---|\n|fixture|kitchen_table|wooden_cabinet| |", "isPerfectTable": true, "csv": "\"object\",\"black_bowl_1\",\"black_bowl\",\"plate\"\n\"fixture\",\"kitchen_table\",\"wooden_cabinet\",\"\""}, {"type": "text", "value": "(1) open cabinet\n\nput in: turn on\n\n(2) \"Open the top drawer of the cabinet and put the bowl in it\"\n\n(3) (A) Specify Task Goals\n\n(B) Specify Init Configuration\n\n(C) Goal States\n\n:regions (bowl_region target kitchen_table) (-0.02 0.02 0.020.02)\n\n$$\n\\text{Init Configuration:} \\\\\n\\text{(:init} \\\\\n\\text{(On cabinet_1 cabinet_region)} \\\\\n\\text{(In bowl cabinet top_drawer)} \\\\\n\\text{(On plate plate_init_region)} \\\\\n\\text{(On bowl bowl_init_region)} \\\\\n\\text{)} \\\\\n$$", "md": "(1) open cabinet\n\nput in: turn on\n\n(2) \"Open the top drawer of the cabinet and put the bowl in it\"\n\n(3) (A) Specify Task Goals\n\n(B) Specify Init Configuration\n\n(C) Goal States\n\n:regions (bowl_region target kitchen_table) (-0.02 0.02 0.020.02)\n\n$$\n\\text{Init Configuration:} \\\\\n\\text{(:init} \\\\\n\\text{(On cabinet_1 cabinet_region)} \\\\\n\\text{(In bowl cabinet top_drawer)} \\\\\n\\text{(On plate plate_init_region)} \\\\\n\\text{(On bowl bowl_init_region)} \\\\\n\\text{)} \\\\\n$$"}, {"type": "heading", "lvl": 3, "value": "Figure 2: LIBERO\u2019s procedural generation pipeline", "md": "### Figure 2: LIBERO\u2019s procedural generation pipeline"}, {"type": "text", "value": "Extracting behavioral templates from a large-scale human activity dataset (1), Ego4D, for generating task instructions (2); Based on the task description, selecting the scene and generating the PDDL description file (3) that specifies the objects and layouts (A), the initial object configurations (B), and the task goal (C).", "md": "Extracting behavioral templates from a large-scale human activity dataset (1), Ego4D, for generating task instructions (2); Based on the task description, selecting the scene and generating the PDDL description file (3) that specifies the objects and layouts (A), the initial object configurations (B), and the task goal (C)."}, {"type": "heading", "lvl": 3, "value": "(T2) Neural Architecture Design", "md": "### (T2) Neural Architecture Design"}, {"type": "text", "value": "An important research question in LLDM is how to design effective neural architectures to abstract the multi-modal observations (images, language descriptions, and robot states) and transfer only relevant knowledge when learning new tasks.", "md": "An important research question in LLDM is how to design effective neural architectures to abstract the multi-modal observations (images, language descriptions, and robot states) and transfer only relevant knowledge when learning new tasks."}, {"type": "heading", "lvl": 3, "value": "(T3) Lifelong Learning Algorithm Design", "md": "### (T3) Lifelong Learning Algorithm Design"}, {"type": "text", "value": "Given a policy architecture, it is crucial to determine what learning algorithms to apply for LLDM. Specifically, the sequential nature of LLDM suggests that even minor forgetting over successive steps can potentially lead to a total failure in execution. As such, we consider the design of lifelong learning algorithms to be an open area of research in LLDM.", "md": "Given a policy architecture, it is crucial to determine what learning algorithms to apply for LLDM. Specifically, the sequential nature of LLDM suggests that even minor forgetting over successive steps can potentially lead to a total failure in execution. As such, we consider the design of lifelong learning algorithms to be an open area of research in LLDM."}, {"type": "heading", "lvl": 3, "value": "(T4) Robustness to Task Ordering", "md": "### (T4) Robustness to Task Ordering"}, {"type": "text", "value": "It is well-known that task curriculum influences policy learning [6, 48]. A robot in the real world, however, often cannot choose which task to encounter first. Therefore, a good lifelong learning algorithm should be robust to different task orderings.", "md": "It is well-known that task curriculum influences policy learning [6, 48]. A robot in the real world, however, often cannot choose which task to encounter first. Therefore, a good lifelong learning algorithm should be robust to different task orderings."}, {"type": "heading", "lvl": 3, "value": "(T5) Usage of Pretrained Models", "md": "### (T5) Usage of Pretrained Models"}, {"type": "text", "value": "In practice, robots will be most likely pretrained on large datasets in factories before deployment [28]. However, it is not well-understood whether or how pretraining could benefit subsequent LLDM.", "md": "In practice, robots will be most likely pretrained on large datasets in factories before deployment [28]. However, it is not well-understood whether or how pretraining could benefit subsequent LLDM."}, {"type": "heading", "lvl": 2, "value": "LIBERO", "md": "## LIBERO"}, {"type": "text", "value": "This section introduces the components in LIBERO: the procedural generation pipeline that allows the never-ending creation of tasks (Section 4.1), the four task suites we generate for benchmarking (Section 4.2), five algorithms (Section 4.3), and three neural architectures (Section 4.4).", "md": "This section introduces the components in LIBERO: the procedural generation pipeline that allows the never-ending creation of tasks (Section 4.1), the four task suites we generate for benchmarking (Section 4.2), five algorithms (Section 4.3), and three neural architectures (Section 4.4)."}, {"type": "heading", "lvl": 3, "value": "4.1 Procedural Generation of Tasks", "md": "### 4.1 Procedural Generation of Tasks"}, {"type": "text", "value": "Research in LLDM requires a systematic way to create new tasks while maintaining task diversity and relevance to existing tasks. LIBERO procedurally generates new tasks in three steps:\n\n1. extract behavioral templates from language annotations of human activities and generate sampled tasks described in natural language based on such templates;\n2. specify an initial object distribution given a task description; and\n3. specify task goals using a propositional formula that aligns with the language instructions.\n\nOur generation pipeline is built on top of Robosuite [76], a modular manipulation simulator that offers seamless integration. Figure 2 illustrates an example of task creation using this pipeline, and each component is expanded upon below.", "md": "Research in LLDM requires a systematic way to create new tasks while maintaining task diversity and relevance to existing tasks. LIBERO procedurally generates new tasks in three steps:\n\n1. extract behavioral templates from language annotations of human activities and generate sampled tasks described in natural language based on such templates;\n2. specify an initial object distribution given a task description; and\n3. specify task goals using a propositional formula that aligns with the language instructions.\n\nOur generation pipeline is built on top of Robosuite [76], a modular manipulation simulator that offers seamless integration. Figure 2 illustrates an example of task creation using this pipeline, and each component is expanded upon below."}, {"type": "heading", "lvl": 3, "value": "Behavioral Templates and Instruction Generation", "md": "### Behavioral Templates and Instruction Generation"}, {"type": "text", "value": "Human activities serve as a fertile source of tasks that can inspire and generate a vast number of manipulation tasks. We choose a large-scale activity dataset, Ego4D [22], which includes a large variety of everyday activities with language annotations. We pre-process the dataset by extracting the language descriptions and then summarize them into a large set of commonly used language templates. After this pre-processing step, we use the templates and select objects available in the simulator to generate a set of task descriptions.", "md": "Human activities serve as a fertile source of tasks that can inspire and generate a vast number of manipulation tasks. We choose a large-scale activity dataset, Ego4D [22], which includes a large variety of everyday activities with language annotations. We pre-process the dataset by extracting the language descriptions and then summarize them into a large set of commonly used language templates. After this pre-processing step, we use the templates and select objects available in the simulator to generate a set of task descriptions."}]}, {"page": 5, "text": "form of language instructions. For example, we can generate an instruction \u201cOpen the drawer of the\ncabinet\u201d from the template \u201cOpen ...\u201d.\nInitial State Distribution (\u03bc          0)    To specify     \u03bc 0 , we first sample a scene layout that matches the\nobjects/behaviors in a provided instruction. For instance, a kitchen scene is selected for an instruction\nOpen the top drawer of the cabinet and put the bowl in it. Then, the details about                      \u03bc 0 are generated in\nthe PDDL language [43,           63]. Concretely,     \u03bc 0  contains information about object categories and their\nplacement (Figure 2-(A)), and their initial status (Figure 2-(B)).\nGoal Specifications        (g)     Based on    \u03bc 0 and the language instruction, we specify the task goal using\na conjunction of predicates. Predicates include               unary predicates       that describe the properties of an\nobject, such as    Open(X) or      TurnOff(X), and        binary predicates     that describe spatial relations between\nobjects, such as     On(A, B) or     In(A, B). An example of the goal specification using PDDL language\ncan be found in Figure 2-(C). The simulation terminates when all predicates are verified true.\n4.2    Task Suites\nWhile the pipeline in Section 4.1 supports the generation of an unlimited number of tasks, we offer\nfixed sets of tasks for benchmarking purposes. LIBERO has four task suites: LIBERO-SPATIAL,\nLIBERO-OBJECT, LIBERO-GOAL, and LIBERO-100. The first three task suites are curated to\ndisentangle the transfer of         declarative     and   procedural     knowledge (as mentioned in (T1)), while\nLIBERO-100 is a suite of 100 tasks with entangled knowledge transfer.\nLIBERO-X              LIBERO-SPATIAL, LIBERO-OBJECT, and LIBERO-GOAL                                     all have 10 tasks     4\nand are designed to investigate the controlled transfer of knowledge about spatial information\n(declarative), objects (declarative), and task goals (procedural). Specifically, all tasks in LIBERO-\nSPATIAL      request the robot to place a bowl, among the same set of objects, on a plate. But there are\ntwo identical bowls that differ only in their location or spatial relationship to other objects. Hence, to\nsuccessfully complete LIBERO-SPATIAL, the robot needs to continually learn and memorize new\nspatial relationships. All tasks in LIBERO-OBJECT                    request the robot to pick-place a unique object.\nHence, to accomplish LIBERO-OBJECT, the robot needs to continually learn and memorize new\nobject types. All tasks in LIBERO-GOAL                 share the same objects with fixed spatial relationships but\ndiffer only in the task goal. Hence, to accomplish LIBERO-GOAL, the robot needs to continually\nlearn new knowledge about motions and behaviors. More details are in Appendix C.\nLIBERO-100             LIBERO-100 contains 100 tasks that entail diverse object interactions and versatile\nmotor skills. In this paper, we split LIBERO-100 into 90 short-horizon tasks (LIBERO-90) and 10\nlong-horizon tasks (LIBERO-LONG). LIBERO-90 serves as the data source for pretraining                                     (T5)\nand LIBERO-LONG             for downstream evaluation of lifelong learning algorithms.\n4.3    Lifelong Learning Algorithms\nWe implement three representative lifelong learning algorithms to facilitate research in algorith-\nmic design for LLDM. Specifically, we implement Experience Replay (ER) [13], Elastic Weight\nConsolidation (EWC) [33], and PACKNET                    [41]. We pick ER, EWC, and PACKNET                    because they\ncorrespond to the memory-based, regularization-based, and dynamic-architecture-based methods for\nlifelong learning. In addition, prior research [69] has discovered that they are state-of-the-art methods.\nBesides these three methods, we also implement sequential finetuning (SEQL) and multitask learning\n(MTL), which serve as a lower bound and upper bound for lifelong learning algorithms, respectively.\nMore details about the algorithms are in Appendix B.1.\n4.4    Neural Network Architectures\nWe implement three vision-language policy networks, RESNET-RNN, RESNET-T, and VIT-T, that\nintegrate visual, temporal, and linguistic information for LLDM. Language instructions of tasks\nare encoded using pretrained BERT embeddings [19]. The RESNET-RNN [42] uses a ResNet as the\nvisual backbone that encodes per-step visual observations and an LSTM as the temporal backbone to\nprocess a sequence of encoded visual information. The language instruction is incorporated into the\nResNet features using the FiLM method [50] and added to the LSTM inputs, respectively. RESNET-T\n    4A suite of 10 tasks is enough to observe catastrophic forgetting while maintaining computation efficiency.\n                                                               5", "md": "# Document\n\nForm of language instructions. For example, we can generate an instruction \"Open the drawer of the cabinet\" from the template \"Open ...\".\n\nInitial State Distribution ($$\\mu_0$$) To specify $$\\mu_0$$, we first sample a scene layout that matches the objects/behaviors in a provided instruction. For instance, a kitchen scene is selected for an instruction Open the top drawer of the cabinet and put the bowl in it. Then, the details about $$\\mu_0$$ are generated in the PDDL language [43, 63]. Concretely, $$\\mu_0$$ contains information about object categories and their placement (Figure 2-(A)), and their initial status (Figure 2-(B)).\n\nGoal Specifications ($$g$$) Based on $$\\mu_0$$ and the language instruction, we specify the task goal using a conjunction of predicates. Predicates include unary predicates that describe the properties of an object, such as Open(X) or TurnOff(X), and binary predicates that describe spatial relations between objects, such as On(A, B) or In(A, B). An example of the goal specification using PDDL language can be found in Figure 2-(C). The simulation terminates when all predicates are verified true.\n\n#### Task Suites\n\nWhile the pipeline in Section 4.1 supports the generation of an unlimited number of tasks, we offer fixed sets of tasks for benchmarking purposes. LIBERO has four task suites: LIBERO-SPATIAL, LIBERO-OBJECT, LIBERO-GOAL, and LIBERO-100. The first three task suites are curated to disentangle the transfer of declarative and procedural knowledge (as mentioned in (T1)), while LIBERO-100 is a suite of 100 tasks with entangled knowledge transfer.\n\nLIBERO-X LIBERO-SPATIAL, LIBERO-OBJECT, and LIBERO-GOAL all have 10 tasks and are designed to investigate the controlled transfer of knowledge about spatial information (declarative), objects (declarative), and task goals (procedural). Specifically, all tasks in LIBERO-SPATIAL request the robot to place a bowl, among the same set of objects, on a plate. But there are two identical bowls that differ only in their location or spatial relationship to other objects. Hence, to successfully complete LIBERO-SPATIAL, the robot needs to continually learn and memorize new spatial relationships. All tasks in LIBERO-OBJECT request the robot to pick-place a unique object. Hence, to accomplish LIBERO-OBJECT, the robot needs to continually learn and memorize new object types. All tasks in LIBERO-GOAL share the same objects with fixed spatial relationships but differ only in the task goal. Hence, to accomplish LIBERO-GOAL, the robot needs to continually learn new knowledge about motions and behaviors. More details are in Appendix C.\n\nLIBERO-100 LIBERO-100 contains 100 tasks that entail diverse object interactions and versatile motor skills. In this paper, we split LIBERO-100 into 90 short-horizon tasks (LIBERO-90) and 10 long-horizon tasks (LIBERO-LONG). LIBERO-90 serves as the data source for pretraining (T5) and LIBERO-LONG for downstream evaluation of lifelong learning algorithms.\n\n#### Lifelong Learning Algorithms\n\nWe implement three representative lifelong learning algorithms to facilitate research in algorithmic design for LLDM. Specifically, we implement Experience Replay (ER) [13], Elastic Weight Consolidation (EWC) [33], and PACKNET [41]. We pick ER, EWC, and PACKNET because they correspond to the memory-based, regularization-based, and dynamic-architecture-based methods for lifelong learning. In addition, prior research [69] has discovered that they are state-of-the-art methods. Besides these three methods, we also implement sequential finetuning (SEQL) and multitask learning (MTL), which serve as a lower bound and upper bound for lifelong learning algorithms, respectively. More details about the algorithms are in Appendix B.1.\n\n#### Neural Network Architectures\n\nWe implement three vision-language policy networks, RESNET-RNN, RESNET-T, and VIT-T, that integrate visual, temporal, and linguistic information for LLDM. Language instructions of tasks are encoded using pretrained BERT embeddings [19]. The RESNET-RNN [42] uses a ResNet as the visual backbone that encodes per-step visual observations and an LSTM as the temporal backbone to process a sequence of encoded visual information. The language instruction is incorporated into the ResNet features using the FiLM method [50] and added to the LSTM inputs, respectively. RESNET-T 4A suite of 10 tasks is enough to observe catastrophic forgetting while maintaining computation efficiency.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Form of language instructions. For example, we can generate an instruction \"Open the drawer of the cabinet\" from the template \"Open ...\".\n\nInitial State Distribution ($$\\mu_0$$) To specify $$\\mu_0$$, we first sample a scene layout that matches the objects/behaviors in a provided instruction. For instance, a kitchen scene is selected for an instruction Open the top drawer of the cabinet and put the bowl in it. Then, the details about $$\\mu_0$$ are generated in the PDDL language [43, 63]. Concretely, $$\\mu_0$$ contains information about object categories and their placement (Figure 2-(A)), and their initial status (Figure 2-(B)).\n\nGoal Specifications ($$g$$) Based on $$\\mu_0$$ and the language instruction, we specify the task goal using a conjunction of predicates. Predicates include unary predicates that describe the properties of an object, such as Open(X) or TurnOff(X), and binary predicates that describe spatial relations between objects, such as On(A, B) or In(A, B). An example of the goal specification using PDDL language can be found in Figure 2-(C). The simulation terminates when all predicates are verified true.", "md": "Form of language instructions. For example, we can generate an instruction \"Open the drawer of the cabinet\" from the template \"Open ...\".\n\nInitial State Distribution ($$\\mu_0$$) To specify $$\\mu_0$$, we first sample a scene layout that matches the objects/behaviors in a provided instruction. For instance, a kitchen scene is selected for an instruction Open the top drawer of the cabinet and put the bowl in it. Then, the details about $$\\mu_0$$ are generated in the PDDL language [43, 63]. Concretely, $$\\mu_0$$ contains information about object categories and their placement (Figure 2-(A)), and their initial status (Figure 2-(B)).\n\nGoal Specifications ($$g$$) Based on $$\\mu_0$$ and the language instruction, we specify the task goal using a conjunction of predicates. Predicates include unary predicates that describe the properties of an object, such as Open(X) or TurnOff(X), and binary predicates that describe spatial relations between objects, such as On(A, B) or In(A, B). An example of the goal specification using PDDL language can be found in Figure 2-(C). The simulation terminates when all predicates are verified true."}, {"type": "heading", "lvl": 4, "value": "Task Suites", "md": "#### Task Suites"}, {"type": "text", "value": "While the pipeline in Section 4.1 supports the generation of an unlimited number of tasks, we offer fixed sets of tasks for benchmarking purposes. LIBERO has four task suites: LIBERO-SPATIAL, LIBERO-OBJECT, LIBERO-GOAL, and LIBERO-100. The first three task suites are curated to disentangle the transfer of declarative and procedural knowledge (as mentioned in (T1)), while LIBERO-100 is a suite of 100 tasks with entangled knowledge transfer.\n\nLIBERO-X LIBERO-SPATIAL, LIBERO-OBJECT, and LIBERO-GOAL all have 10 tasks and are designed to investigate the controlled transfer of knowledge about spatial information (declarative), objects (declarative), and task goals (procedural). Specifically, all tasks in LIBERO-SPATIAL request the robot to place a bowl, among the same set of objects, on a plate. But there are two identical bowls that differ only in their location or spatial relationship to other objects. Hence, to successfully complete LIBERO-SPATIAL, the robot needs to continually learn and memorize new spatial relationships. All tasks in LIBERO-OBJECT request the robot to pick-place a unique object. Hence, to accomplish LIBERO-OBJECT, the robot needs to continually learn and memorize new object types. All tasks in LIBERO-GOAL share the same objects with fixed spatial relationships but differ only in the task goal. Hence, to accomplish LIBERO-GOAL, the robot needs to continually learn new knowledge about motions and behaviors. More details are in Appendix C.\n\nLIBERO-100 LIBERO-100 contains 100 tasks that entail diverse object interactions and versatile motor skills. In this paper, we split LIBERO-100 into 90 short-horizon tasks (LIBERO-90) and 10 long-horizon tasks (LIBERO-LONG). LIBERO-90 serves as the data source for pretraining (T5) and LIBERO-LONG for downstream evaluation of lifelong learning algorithms.", "md": "While the pipeline in Section 4.1 supports the generation of an unlimited number of tasks, we offer fixed sets of tasks for benchmarking purposes. LIBERO has four task suites: LIBERO-SPATIAL, LIBERO-OBJECT, LIBERO-GOAL, and LIBERO-100. The first three task suites are curated to disentangle the transfer of declarative and procedural knowledge (as mentioned in (T1)), while LIBERO-100 is a suite of 100 tasks with entangled knowledge transfer.\n\nLIBERO-X LIBERO-SPATIAL, LIBERO-OBJECT, and LIBERO-GOAL all have 10 tasks and are designed to investigate the controlled transfer of knowledge about spatial information (declarative), objects (declarative), and task goals (procedural). Specifically, all tasks in LIBERO-SPATIAL request the robot to place a bowl, among the same set of objects, on a plate. But there are two identical bowls that differ only in their location or spatial relationship to other objects. Hence, to successfully complete LIBERO-SPATIAL, the robot needs to continually learn and memorize new spatial relationships. All tasks in LIBERO-OBJECT request the robot to pick-place a unique object. Hence, to accomplish LIBERO-OBJECT, the robot needs to continually learn and memorize new object types. All tasks in LIBERO-GOAL share the same objects with fixed spatial relationships but differ only in the task goal. Hence, to accomplish LIBERO-GOAL, the robot needs to continually learn new knowledge about motions and behaviors. More details are in Appendix C.\n\nLIBERO-100 LIBERO-100 contains 100 tasks that entail diverse object interactions and versatile motor skills. In this paper, we split LIBERO-100 into 90 short-horizon tasks (LIBERO-90) and 10 long-horizon tasks (LIBERO-LONG). LIBERO-90 serves as the data source for pretraining (T5) and LIBERO-LONG for downstream evaluation of lifelong learning algorithms."}, {"type": "heading", "lvl": 4, "value": "Lifelong Learning Algorithms", "md": "#### Lifelong Learning Algorithms"}, {"type": "text", "value": "We implement three representative lifelong learning algorithms to facilitate research in algorithmic design for LLDM. Specifically, we implement Experience Replay (ER) [13], Elastic Weight Consolidation (EWC) [33], and PACKNET [41]. We pick ER, EWC, and PACKNET because they correspond to the memory-based, regularization-based, and dynamic-architecture-based methods for lifelong learning. In addition, prior research [69] has discovered that they are state-of-the-art methods. Besides these three methods, we also implement sequential finetuning (SEQL) and multitask learning (MTL), which serve as a lower bound and upper bound for lifelong learning algorithms, respectively. More details about the algorithms are in Appendix B.1.", "md": "We implement three representative lifelong learning algorithms to facilitate research in algorithmic design for LLDM. Specifically, we implement Experience Replay (ER) [13], Elastic Weight Consolidation (EWC) [33], and PACKNET [41]. We pick ER, EWC, and PACKNET because they correspond to the memory-based, regularization-based, and dynamic-architecture-based methods for lifelong learning. In addition, prior research [69] has discovered that they are state-of-the-art methods. Besides these three methods, we also implement sequential finetuning (SEQL) and multitask learning (MTL), which serve as a lower bound and upper bound for lifelong learning algorithms, respectively. More details about the algorithms are in Appendix B.1."}, {"type": "heading", "lvl": 4, "value": "Neural Network Architectures", "md": "#### Neural Network Architectures"}, {"type": "text", "value": "We implement three vision-language policy networks, RESNET-RNN, RESNET-T, and VIT-T, that integrate visual, temporal, and linguistic information for LLDM. Language instructions of tasks are encoded using pretrained BERT embeddings [19]. The RESNET-RNN [42] uses a ResNet as the visual backbone that encodes per-step visual observations and an LSTM as the temporal backbone to process a sequence of encoded visual information. The language instruction is incorporated into the ResNet features using the FiLM method [50] and added to the LSTM inputs, respectively. RESNET-T 4A suite of 10 tasks is enough to observe catastrophic forgetting while maintaining computation efficiency.", "md": "We implement three vision-language policy networks, RESNET-RNN, RESNET-T, and VIT-T, that integrate visual, temporal, and linguistic information for LLDM. Language instructions of tasks are encoded using pretrained BERT embeddings [19]. The RESNET-RNN [42] uses a ResNet as the visual backbone that encodes per-step visual observations and an LSTM as the temporal backbone to process a sequence of encoded visual information. The language instruction is incorporated into the ResNet features using the FiLM method [50] and added to the LSTM inputs, respectively. RESNET-T 4A suite of 10 tasks is enough to observe catastrophic forgetting while maintaining computation efficiency."}]}, {"page": 6, "text": " architecture [75] uses a similar ResNet-based visual backbone, but a transformer decoder [66] as\n the temporal backbone to process outputs from ResNet, which are a temporal sequence of visual\n tokens. The language embedding is treated as a separate token in inputs to the transformer alongside\n the visual tokens. The VIT-T architecture [31], which is widely used in visual-language tasks, uses a\nVision Transformer (ViT) as the visual backbone and a transformer decoder as the temporal backbone.\nThe language embedding is treated as a separate token in inputs of both ViT and the transformer\n decoder. All the temporal backbones output a latent vector for every decision-making step. We\n compute the multi-modal distribution over manipulation actions using a Gaussian-Mixture-Model\n(GMM) based output head [8,             42,  68]. In the end, a robot executes a policy by sampling a continuous\nvalue for end-effector action from the output distribution. Figure 6 visualizes the three architectures.\n For all the lifelong learning algorithms and neural architectures, we use behavioral cloning (BC) [4]\n to train policies for individual tasks (See           (2)). BC allows for efficient policy learning such that we\n can study lifelong learning algorithms with limited computational resources. To train BC, we provide\n 50 trajectories of high-quality demonstrations for every single task in the generated task suites. The\n demonstrations are collected by human experts through teleoperation with 3Dconnexion Spacemouse.\n 5    Experiments\n Experiments are conducted as an initial study for the five research topics mentioned in Section 3. We\n first introduce the evaluation metric used in experiments, and present analysis of empirical results in\n LIBERO. The detailed experimental setup is in Appendix D. Our experiments focus on addressing\n the following research questions:\n Q1: How do different architectures/LL algorithms perform under specific distribution shifts?\n Q2: To what extent does neural architecture impact knowledge transfer in LLDM, and are there any\n discernible patterns in the specialized capabilities of each architecture?\n Q3: How do existing algorithms from lifelong supervised learning perform on LLDM tasks?\n Q4: To what extent does language embedding affect knowledge transfer in LLDM?\n Q5: How robust are different LL algorithms to task ordering in LLDM?\n Q6: Can supervised pretraining improve downstream lifelong learning performance in LLDM?\n 5.1    Evaluation Metrics\nWe report three metrics: FWT (forward transfer) [20], NBT (negative backward transfer), and\nAUC (area under the success rate curve). All metrics are computed in terms of success rate, as\n previous literature has shown that the success rate is a more reliable metric than training loss for\n manipulation policies [42] (Detailed explanation in Appendix E.2). Lower NBT means a policy\n has better performance in the previously seen tasks, higher FWT means a policy learns faster on a\n new task, and higher AUC means an overall better performance considering both NBT and FWT.\n Specifically, denote     c i,j,eas the agent\u2019s success rate on task         j when it learned over      i\u22121    previous tasks\n and has just learned      e epochs (e     \u2208 {0,5, . . . ,50}) on task      i. Let  c i,ibe the best success rate over all\n evaluated epochs      e  for the current task     i (i.e., c   = max       c     ). Then, we find the earliest epoch          e\u2217\n                                                             i,i          e  i,i,e                                              i\n in which the agent achieves the best performance on task                    i (i.e., e \u2217 = arg min       c        =  c   ), and\n                                                                                                            i,i,e       i,i\n                                                                                        i               e       i\n assume for all     e \u2265   e\u2217 ,c       =  c   .5 Given a different task        j \u0338=  i, we define    c    =   c     \u2217. Then the\n three metrics are defined:i    i,i,e      i,i                                                       i,j      i,j,ei\n                     FWT     =   X      FWT    k ,   FWT    k =    1      X        ck,k,e\n                                k\u2208[K]      K                      11  e\u2208{0...50}\n                     NBT     =   X      NBT   k ,    NBT    k =      1       XK     \u25af ck,k  \u2212  c\u03c4,k \u0001                         (3)\n                                k\u2208[K]     K                       K   \u2212k    \u03c4=k+1\n                                 X      AUC   k                         1       \u25af               XK          \u0001\n                     AUC     =  k\u2208[K]      K     ,  AUC    k  =  K   \u2212   k + 1    FWT    k +  \u03c4=k+1    c\u03c4,k\n     5In practice, it\u2019s possible that the agent\u2019s performance on task         i is not monotonically increasing due to the\nvariance of learning. But we keep the best checkpoint among those saved at epochs                    {e}  as if the agent stops\n learning after  e\u2217 .\n                  i                                             6", "md": "Architecture [75] uses a similar ResNet-based visual backbone, but a transformer decoder [66] as the temporal backbone to process outputs from ResNet, which are a temporal sequence of visual tokens. The language embedding is treated as a separate token in inputs to the transformer alongside the visual tokens. The VIT-T architecture [31], which is widely used in visual-language tasks, uses a Vision Transformer (ViT) as the visual backbone and a transformer decoder as the temporal backbone. The language embedding is treated as a separate token in inputs of both ViT and the transformer decoder. All the temporal backbones output a latent vector for every decision-making step. We compute the multi-modal distribution over manipulation actions using a Gaussian-Mixture-Model (GMM) based output head [8, 42, 68]. In the end, a robot executes a policy by sampling a continuous value for end-effector action from the output distribution. Figure 6 visualizes the three architectures. For all the lifelong learning algorithms and neural architectures, we use behavioral cloning (BC) [4] to train policies for individual tasks (See (2)). BC allows for efficient policy learning such that we can study lifelong learning algorithms with limited computational resources. To train BC, we provide 50 trajectories of high-quality demonstrations for every single task in the generated task suites. The demonstrations are collected by human experts through teleoperation with 3Dconnexion Spacemouse.\n\n## 5 Experiments\n\nExperiments are conducted as an initial study for the five research topics mentioned in Section 3. We first introduce the evaluation metric used in experiments, and present analysis of empirical results in LIBERO. The detailed experimental setup is in Appendix D. Our experiments focus on addressing the following research questions:\n\n1. Q1: How do different architectures/LL algorithms perform under specific distribution shifts?\n2. Q2: To what extent does neural architecture impact knowledge transfer in LLDM, and are there any discernible patterns in the specialized capabilities of each architecture?\n3. Q3: How do existing algorithms from lifelong supervised learning perform on LLDM tasks?\n4. Q4: To what extent does language embedding affect knowledge transfer in LLDM?\n5. Q5: How robust are different LL algorithms to task ordering in LLDM?\n6. Q6: Can supervised pretraining improve downstream lifelong learning performance in LLDM?\n\n### 5.1 Evaluation Metrics\n\nWe report three metrics: FWT (forward transfer) [20], NBT (negative backward transfer), and AUC (area under the success rate curve). All metrics are computed in terms of success rate, as previous literature has shown that the success rate is a more reliable metric than training loss for manipulation policies [42] (Detailed explanation in Appendix E.2). Lower NBT means a policy has better performance in the previously seen tasks, higher FWT means a policy learns faster on a new task, and higher AUC means an overall better performance considering both NBT and FWT. Specifically, denote $$c_{i,j,e}$$ as the agent\u2019s success rate on task $$j$$ when it learned over $$i-1$$ previous tasks and has just learned $$e$$ epochs ($$e \\in \\{0,5, ..., 50\\}$$) on task $$i$$. Let $$c_{i,i}^{*}$$ be the best success rate over all evaluated epochs $$e$$ for the current task $$i$$ (i.e., $$c^{*} = \\max c$$). Then, we find the earliest epoch $$e^{*}$$ in which the agent achieves the best performance on task $$i$$ (i.e., $$e^{*} = \\arg \\min c = c$$), and assume for all $$e \\geq e^{*}, c = c^{*}$$. Given a different task $$j \\neq i$$, we define $$c^{*} = c_{i,j}^{*}$$. Then the three metrics are defined:\n\n$$FWT = \\sum_{k \\in [K]} FWT_{k}, \\quad FWT_{k} = \\frac{1}{11} \\sum_{e \\in \\{0...50\\}} c_{k,k,e}$$\n\n$$NBT = \\sum_{k \\in [K]} NBT_{k}, \\quad NBT_{k} = \\frac{1}{K} \\sum_{\\tau=k+1}^{K} |c_{k,k} - c_{\\tau,k}|$$\n\n$$AUC = \\sum_{k \\in [K]} AUC_{k}, \\quad AUC_{k} = K - k + 1 \\cdot FWT_{k} + \\sum_{\\tau=k+1}^{K} c_{\\tau,k}$$\n\nIn practice, it\u2019s possible that the agent\u2019s performance on task $$i$$ is not monotonically increasing due to the variance of learning. But we keep the best checkpoint among those saved at epochs $$\\{e\\}$$ as if the agent stops learning after $$e^{*}$$.", "images": [], "items": [{"type": "text", "value": "Architecture [75] uses a similar ResNet-based visual backbone, but a transformer decoder [66] as the temporal backbone to process outputs from ResNet, which are a temporal sequence of visual tokens. The language embedding is treated as a separate token in inputs to the transformer alongside the visual tokens. The VIT-T architecture [31], which is widely used in visual-language tasks, uses a Vision Transformer (ViT) as the visual backbone and a transformer decoder as the temporal backbone. The language embedding is treated as a separate token in inputs of both ViT and the transformer decoder. All the temporal backbones output a latent vector for every decision-making step. We compute the multi-modal distribution over manipulation actions using a Gaussian-Mixture-Model (GMM) based output head [8, 42, 68]. In the end, a robot executes a policy by sampling a continuous value for end-effector action from the output distribution. Figure 6 visualizes the three architectures. For all the lifelong learning algorithms and neural architectures, we use behavioral cloning (BC) [4] to train policies for individual tasks (See (2)). BC allows for efficient policy learning such that we can study lifelong learning algorithms with limited computational resources. To train BC, we provide 50 trajectories of high-quality demonstrations for every single task in the generated task suites. The demonstrations are collected by human experts through teleoperation with 3Dconnexion Spacemouse.", "md": "Architecture [75] uses a similar ResNet-based visual backbone, but a transformer decoder [66] as the temporal backbone to process outputs from ResNet, which are a temporal sequence of visual tokens. The language embedding is treated as a separate token in inputs to the transformer alongside the visual tokens. The VIT-T architecture [31], which is widely used in visual-language tasks, uses a Vision Transformer (ViT) as the visual backbone and a transformer decoder as the temporal backbone. The language embedding is treated as a separate token in inputs of both ViT and the transformer decoder. All the temporal backbones output a latent vector for every decision-making step. We compute the multi-modal distribution over manipulation actions using a Gaussian-Mixture-Model (GMM) based output head [8, 42, 68]. In the end, a robot executes a policy by sampling a continuous value for end-effector action from the output distribution. Figure 6 visualizes the three architectures. For all the lifelong learning algorithms and neural architectures, we use behavioral cloning (BC) [4] to train policies for individual tasks (See (2)). BC allows for efficient policy learning such that we can study lifelong learning algorithms with limited computational resources. To train BC, we provide 50 trajectories of high-quality demonstrations for every single task in the generated task suites. The demonstrations are collected by human experts through teleoperation with 3Dconnexion Spacemouse."}, {"type": "heading", "lvl": 2, "value": "5 Experiments", "md": "## 5 Experiments"}, {"type": "text", "value": "Experiments are conducted as an initial study for the five research topics mentioned in Section 3. We first introduce the evaluation metric used in experiments, and present analysis of empirical results in LIBERO. The detailed experimental setup is in Appendix D. Our experiments focus on addressing the following research questions:\n\n1. Q1: How do different architectures/LL algorithms perform under specific distribution shifts?\n2. Q2: To what extent does neural architecture impact knowledge transfer in LLDM, and are there any discernible patterns in the specialized capabilities of each architecture?\n3. Q3: How do existing algorithms from lifelong supervised learning perform on LLDM tasks?\n4. Q4: To what extent does language embedding affect knowledge transfer in LLDM?\n5. Q5: How robust are different LL algorithms to task ordering in LLDM?\n6. Q6: Can supervised pretraining improve downstream lifelong learning performance in LLDM?", "md": "Experiments are conducted as an initial study for the five research topics mentioned in Section 3. We first introduce the evaluation metric used in experiments, and present analysis of empirical results in LIBERO. The detailed experimental setup is in Appendix D. Our experiments focus on addressing the following research questions:\n\n1. Q1: How do different architectures/LL algorithms perform under specific distribution shifts?\n2. Q2: To what extent does neural architecture impact knowledge transfer in LLDM, and are there any discernible patterns in the specialized capabilities of each architecture?\n3. Q3: How do existing algorithms from lifelong supervised learning perform on LLDM tasks?\n4. Q4: To what extent does language embedding affect knowledge transfer in LLDM?\n5. Q5: How robust are different LL algorithms to task ordering in LLDM?\n6. Q6: Can supervised pretraining improve downstream lifelong learning performance in LLDM?"}, {"type": "heading", "lvl": 3, "value": "5.1 Evaluation Metrics", "md": "### 5.1 Evaluation Metrics"}, {"type": "text", "value": "We report three metrics: FWT (forward transfer) [20], NBT (negative backward transfer), and AUC (area under the success rate curve). All metrics are computed in terms of success rate, as previous literature has shown that the success rate is a more reliable metric than training loss for manipulation policies [42] (Detailed explanation in Appendix E.2). Lower NBT means a policy has better performance in the previously seen tasks, higher FWT means a policy learns faster on a new task, and higher AUC means an overall better performance considering both NBT and FWT. Specifically, denote $$c_{i,j,e}$$ as the agent\u2019s success rate on task $$j$$ when it learned over $$i-1$$ previous tasks and has just learned $$e$$ epochs ($$e \\in \\{0,5, ..., 50\\}$$) on task $$i$$. Let $$c_{i,i}^{*}$$ be the best success rate over all evaluated epochs $$e$$ for the current task $$i$$ (i.e., $$c^{*} = \\max c$$). Then, we find the earliest epoch $$e^{*}$$ in which the agent achieves the best performance on task $$i$$ (i.e., $$e^{*} = \\arg \\min c = c$$), and assume for all $$e \\geq e^{*}, c = c^{*}$$. Given a different task $$j \\neq i$$, we define $$c^{*} = c_{i,j}^{*}$$. Then the three metrics are defined:\n\n$$FWT = \\sum_{k \\in [K]} FWT_{k}, \\quad FWT_{k} = \\frac{1}{11} \\sum_{e \\in \\{0...50\\}} c_{k,k,e}$$\n\n$$NBT = \\sum_{k \\in [K]} NBT_{k}, \\quad NBT_{k} = \\frac{1}{K} \\sum_{\\tau=k+1}^{K} |c_{k,k} - c_{\\tau,k}|$$\n\n$$AUC = \\sum_{k \\in [K]} AUC_{k}, \\quad AUC_{k} = K - k + 1 \\cdot FWT_{k} + \\sum_{\\tau=k+1}^{K} c_{\\tau,k}$$\n\nIn practice, it\u2019s possible that the agent\u2019s performance on task $$i$$ is not monotonically increasing due to the variance of learning. But we keep the best checkpoint among those saved at epochs $$\\{e\\}$$ as if the agent stops learning after $$e^{*}$$.", "md": "We report three metrics: FWT (forward transfer) [20], NBT (negative backward transfer), and AUC (area under the success rate curve). All metrics are computed in terms of success rate, as previous literature has shown that the success rate is a more reliable metric than training loss for manipulation policies [42] (Detailed explanation in Appendix E.2). Lower NBT means a policy has better performance in the previously seen tasks, higher FWT means a policy learns faster on a new task, and higher AUC means an overall better performance considering both NBT and FWT. Specifically, denote $$c_{i,j,e}$$ as the agent\u2019s success rate on task $$j$$ when it learned over $$i-1$$ previous tasks and has just learned $$e$$ epochs ($$e \\in \\{0,5, ..., 50\\}$$) on task $$i$$. Let $$c_{i,i}^{*}$$ be the best success rate over all evaluated epochs $$e$$ for the current task $$i$$ (i.e., $$c^{*} = \\max c$$). Then, we find the earliest epoch $$e^{*}$$ in which the agent achieves the best performance on task $$i$$ (i.e., $$e^{*} = \\arg \\min c = c$$), and assume for all $$e \\geq e^{*}, c = c^{*}$$. Given a different task $$j \\neq i$$, we define $$c^{*} = c_{i,j}^{*}$$. Then the three metrics are defined:\n\n$$FWT = \\sum_{k \\in [K]} FWT_{k}, \\quad FWT_{k} = \\frac{1}{11} \\sum_{e \\in \\{0...50\\}} c_{k,k,e}$$\n\n$$NBT = \\sum_{k \\in [K]} NBT_{k}, \\quad NBT_{k} = \\frac{1}{K} \\sum_{\\tau=k+1}^{K} |c_{k,k} - c_{\\tau,k}|$$\n\n$$AUC = \\sum_{k \\in [K]} AUC_{k}, \\quad AUC_{k} = K - k + 1 \\cdot FWT_{k} + \\sum_{\\tau=k+1}^{K} c_{\\tau,k}$$\n\nIn practice, it\u2019s possible that the agent\u2019s performance on task $$i$$ is not monotonically increasing due to the variance of learning. But we keep the best checkpoint among those saved at epochs $$\\{e\\}$$ as if the agent stops learning after $$e^{*}$$."}]}, {"page": 7, "text": "A visualization of these metrics is provided in Figure 3.\n               2      FWTk=                   NBTk-K-k                AUCk-K-k+1\n               J        k-1              k                                               K  Task\n                                        Figure 3: Metrics for LLDM.\n5.2   Experimental Results\nWe present empirical results to address the research questions. Please refer to Appendix E.1 for the\nfull results across all algorithms, policy architectures, and task suites.\nStudy on the Policy\u2019s Neural Architectures (Q1, Q2)              Table 1 reports the agent\u2019s lifelong learning\nperformance using the three different neural architectures on the four task suites. Results are reported\nwhen ER and PACKNET          are used as they demonstrate the best lifelong learning performance across\nall task suites.\n Policy Arch.                            ER                                          PACKNET\n                       FWT(\u2191)          NBT(\u2193)          AUC(\u2191)          FWT(\u2191)          NBT(\u2193)         AUC(\u2191)\n                                                          LIBERO-LONG\n RESNET-RNN          0.16 \u00b1  0.02    0.16 \u00b10.02      0.08 \u00b1 0.01     0.13 \u00b1 0.00    0.21  \u00b1 0.01    0.03  \u00b10.00\n RESNET-T            0.48 \u00b1  0.02    0.32 \u00b10.04      0.32 \u00b1 0.01     0.22 \u00b1 0.01    0.08  \u00b1 0.01    0.25  \u00b10.00\n VIT-T               0.38 \u00b1  0.05    0.29 \u00b10.06      0.25 \u00b1 0.02     0.36 \u00b1 0.01    0.14  \u00b1 0.01    0.34  \u00b10.01\n                                                        LIBERO-SPATIAL\n RESNET-RNN          0.40 \u00b1  0.02    0.29 \u00b10.02      0.29 \u00b1 0.01     0.27 \u00b1 0.03    0.38  \u00b1 0.03    0.06  \u00b10.01\n RESNET-T            0.65 \u00b1  0.03    0.27 \u00b10.03      0.56 \u00b1 0.01     0.55 \u00b1 0.01    0.07  \u00b1 0.02    0.63  \u00b10.00\n VIT-T               0.63 \u00b1  0.01    0.29 \u00b10.02      0.50 \u00b1 0.02     0.57 \u00b1 0.04    0.15  \u00b1 0.00    0.59  \u00b10.03\n                                                        LIBERO-OBJECT\n RESNET-RNN          0.30 \u00b1  0.01    0.27 \u00b10.05      0.17 \u00b1 0.05     0.29 \u00b1 0.02    0.35  \u00b1 0.02    0.13  \u00b10.01\n RESNET-T            0.67 \u00b1  0.07    0.43 \u00b10.04      0.44 \u00b1 0.06     0.60 \u00b1 0.07    0.17  \u00b1 0.05    0.60  \u00b10.05\n VIT-T               0.70 \u00b1  0.02    0.28 \u00b10.01      0.57 \u00b1 0.01     0.58 \u00b1 0.03    0.18  \u00b1 0.02    0.56  \u00b10.04\n                                                          LIBERO-GOAL\n RESNET-RNN          0.41 \u00b1  0.00    0.35 \u00b10.01      0.26 \u00b1 0.01     0.32 \u00b1 0.03    0.37  \u00b1 0.04    0.11  \u00b10.01\n RESNET-T            0.64 \u00b1  0.01    0.34 \u00b10.02      0.49 \u00b1 0.02     0.63 \u00b1 0.02    0.06  \u00b1 0.01    0.75  \u00b10.01\n VIT-T               0.57 \u00b1  0.00    0.40 \u00b10.02      0.38 \u00b1 0.01     0.69 \u00b1 0.02    0.08  \u00b1 0.01    0.76  \u00b10.02\nTable 1: Performance of the three neural architectures using ER and PACKNET                     on the four task\nsuites. Results are averaged over three seeds and we report the mean and standard error. The best\nperformance is    bolded, and colored in       purple   if the improvement is statistically significant over\nother neural architectures, when a two-tailed, Student\u2019s t-test under equal sample sizes and unequal\nvariance is applied with a    p-value of 0.05.\nFindings:   First, we observe that RESNET-T and VIT-T work much better than RESNET-RNN on\naverage, indicating that using a transformer on the \u201ctemporal\" level could be a better option than\nusing an RNN model. Second, the performance difference among different architectures depends\non the underlying lifelong learning algorithm. If PACKNET              (a dynamic architecture approach) is\n                                                        7", "md": "# Document\n\nA visualization of these metrics is provided in Figure 3.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& FWT_k & NBT_k-K-k & AUC_{k-K-k+1} \\\\\n\\hline\nJ & k-1 & k & K & Task \\\\\n\\hline\n\\end{array}\n$$\nFigure 3: Metrics for LLDM.\n\n## 5.2 Experimental Results\n\nWe present empirical results to address the research questions. Please refer to Appendix E.1 for the full results across all algorithms, policy architectures, and task suites.\n\nStudy on the Policy\u2019s Neural Architectures (Q1, Q2)\n\n|Policy Arch.|ER|PACKNET|\n|---|---|---|\n| |FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|\n|RESNET-RNN|0.16 \u00b1 0.02|0.16 \u00b1 0.02|0.08 \u00b1 0.01|0.13 \u00b1 0.00|0.21 \u00b1 0.01|0.03 \u00b1 0.00|\n|RESNET-T|0.48 \u00b1 0.02|0.32 \u00b1 0.04|0.32 \u00b1 0.01|0.22 \u00b1 0.01|0.08 \u00b1 0.01|0.25 \u00b1 0.00|\n|VIT-T|0.38 \u00b1 0.05|0.29 \u00b1 0.06|0.25 \u00b1 0.02|0.36 \u00b1 0.01|0.14 \u00b1 0.01|0.34 \u00b1 0.01|\n|...| | |\n\nTable 1: Performance of the three neural architectures using ER and PACKNET on the four task suites. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded, and colored in purple if the improvement is statistically significant over other neural architectures, when a two-tailed, Student\u2019s t-test under equal sample sizes and unequal variance is applied with a p-value of 0.05.\n\nFindings: First, we observe that RESNET-T and VIT-T work much better than RESNET-RNN on average, indicating that using a transformer on the \u201ctemporal\" level could be a better option than using an RNN model. Second, the performance difference among different architectures depends on the underlying lifelong learning algorithm. If PACKNET (a dynamic architecture approach) is...", "images": [{"name": "img_p6_1", "height": 351, "width": 892}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "A visualization of these metrics is provided in Figure 3.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& FWT_k & NBT_k-K-k & AUC_{k-K-k+1} \\\\\n\\hline\nJ & k-1 & k & K & Task \\\\\n\\hline\n\\end{array}\n$$\nFigure 3: Metrics for LLDM.", "md": "A visualization of these metrics is provided in Figure 3.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& FWT_k & NBT_k-K-k & AUC_{k-K-k+1} \\\\\n\\hline\nJ & k-1 & k & K & Task \\\\\n\\hline\n\\end{array}\n$$\nFigure 3: Metrics for LLDM."}, {"type": "heading", "lvl": 2, "value": "5.2 Experimental Results", "md": "## 5.2 Experimental Results"}, {"type": "text", "value": "We present empirical results to address the research questions. Please refer to Appendix E.1 for the full results across all algorithms, policy architectures, and task suites.\n\nStudy on the Policy\u2019s Neural Architectures (Q1, Q2)", "md": "We present empirical results to address the research questions. Please refer to Appendix E.1 for the full results across all algorithms, policy architectures, and task suites.\n\nStudy on the Policy\u2019s Neural Architectures (Q1, Q2)"}, {"type": "table", "rows": [["Policy Arch.", "ER", "PACKNET"], ["", "FWT(\u2191)", "NBT(\u2193)", "AUC(\u2191)", "FWT(\u2191)", "NBT(\u2193)", "AUC(\u2191)"], ["RESNET-RNN", "0.16 \u00b1 0.02", "0.16 \u00b1 0.02", "0.08 \u00b1 0.01", "0.13 \u00b1 0.00", "0.21 \u00b1 0.01", "0.03 \u00b1 0.00"], ["RESNET-T", "0.48 \u00b1 0.02", "0.32 \u00b1 0.04", "0.32 \u00b1 0.01", "0.22 \u00b1 0.01", "0.08 \u00b1 0.01", "0.25 \u00b1 0.00"], ["VIT-T", "0.38 \u00b1 0.05", "0.29 \u00b1 0.06", "0.25 \u00b1 0.02", "0.36 \u00b1 0.01", "0.14 \u00b1 0.01", "0.34 \u00b1 0.01"], ["...", "", ""]], "md": "|Policy Arch.|ER|PACKNET|\n|---|---|---|\n| |FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|\n|RESNET-RNN|0.16 \u00b1 0.02|0.16 \u00b1 0.02|0.08 \u00b1 0.01|0.13 \u00b1 0.00|0.21 \u00b1 0.01|0.03 \u00b1 0.00|\n|RESNET-T|0.48 \u00b1 0.02|0.32 \u00b1 0.04|0.32 \u00b1 0.01|0.22 \u00b1 0.01|0.08 \u00b1 0.01|0.25 \u00b1 0.00|\n|VIT-T|0.38 \u00b1 0.05|0.29 \u00b1 0.06|0.25 \u00b1 0.02|0.36 \u00b1 0.01|0.14 \u00b1 0.01|0.34 \u00b1 0.01|\n|...| | |", "isPerfectTable": false, "csv": "\"Policy Arch.\",\"ER\",\"PACKNET\"\n\"\",\"FWT(\u2191)\",\"NBT(\u2193)\",\"AUC(\u2191)\",\"FWT(\u2191)\",\"NBT(\u2193)\",\"AUC(\u2191)\"\n\"RESNET-RNN\",\"0.16 \u00b1 0.02\",\"0.16 \u00b1 0.02\",\"0.08 \u00b1 0.01\",\"0.13 \u00b1 0.00\",\"0.21 \u00b1 0.01\",\"0.03 \u00b1 0.00\"\n\"RESNET-T\",\"0.48 \u00b1 0.02\",\"0.32 \u00b1 0.04\",\"0.32 \u00b1 0.01\",\"0.22 \u00b1 0.01\",\"0.08 \u00b1 0.01\",\"0.25 \u00b1 0.00\"\n\"VIT-T\",\"0.38 \u00b1 0.05\",\"0.29 \u00b1 0.06\",\"0.25 \u00b1 0.02\",\"0.36 \u00b1 0.01\",\"0.14 \u00b1 0.01\",\"0.34 \u00b1 0.01\"\n\"...\",\"\",\"\""}, {"type": "text", "value": "Table 1: Performance of the three neural architectures using ER and PACKNET on the four task suites. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded, and colored in purple if the improvement is statistically significant over other neural architectures, when a two-tailed, Student\u2019s t-test under equal sample sizes and unequal variance is applied with a p-value of 0.05.\n\nFindings: First, we observe that RESNET-T and VIT-T work much better than RESNET-RNN on average, indicating that using a transformer on the \u201ctemporal\" level could be a better option than using an RNN model. Second, the performance difference among different architectures depends on the underlying lifelong learning algorithm. If PACKNET (a dynamic architecture approach) is...", "md": "Table 1: Performance of the three neural architectures using ER and PACKNET on the four task suites. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded, and colored in purple if the improvement is statistically significant over other neural architectures, when a two-tailed, Student\u2019s t-test under equal sample sizes and unequal variance is applied with a p-value of 0.05.\n\nFindings: First, we observe that RESNET-T and VIT-T work much better than RESNET-RNN on average, indicating that using a transformer on the \u201ctemporal\" level could be a better option than using an RNN model. Second, the performance difference among different architectures depends on the underlying lifelong learning algorithm. If PACKNET (a dynamic architecture approach) is..."}]}, {"page": 8, "text": "used, we observe no significant performance difference between RESNET-T and VIT-T except on\nthe LIBERO-LONG           task suite where VIT-T performs much better than RESNET-T. In contrast,\nif ER is used, we observe that RESNET-T performs better than VIT-T on all task suites except\nLIBERO-OBJECT. This potentially indicates that the ViT architecture is better at processing visual\ninformation with more object varieties than the ResNet architecture when the network capacity is\nsufficiently large (See the MTL results in Table 8 on LIBERO-OBJECT                 as the supporting evidence).\nThe above findings shed light on how one can improve architecture design for better processing of\nspatial and temporal information in LLDM.\nStudy on Lifelong Learning Algorithms (Q1, Q3)                     Table 2 reports the lifelong learning per-\nformance of the three lifelong learning algorithms, together with the SEQL and MTL baselines.\nAll experiments use the same RESNET-T architecture as it performs the best across all policy\narchitectures.\n  Lifelong Algo.       FWT(\u2191)          NBT(\u2193)          AUC(\u2191)          FWT(\u2191)          NBT(\u2193)          AUC(\u2191)\n                                  LIBERO-LONG                                    LIBERO-SPATIAL\n  SEQL               0.54 \u00b1  0.01    0.63 \u00b10.01      0.15 \u00b1  0.00    0.72 \u00b1  0.01    0.81 \u00b10.01      0.20 \u00b1  0.01\n  ER                 0.48 \u00b1  0.02    0.32 \u00b10.04      0.32 \u00b1  0.01    0.65 \u00b1  0.03    0.27 \u00b10.03      0.56 \u00b1  0.01\n  EWC                0.13 \u00b1  0.02    0.22 \u00b10.03      0.02 \u00b1  0.00    0.23 \u00b1  0.01    0.33 \u00b10.01      0.06 \u00b1  0.01\n  PACKNET            0.22 \u00b1  0.01    0.08 \u00b10.01      0.25 \u00b1  0.00    0.55 \u00b1  0.01    0.07 \u00b10.02      0.63 \u00b1  0.00\n  MTL                            LIBERO-OBJECT       0.48 \u00b1  0.01                 LIBERO-GOAL        0.83 \u00b1  0.00\n  SEQL               0.78 \u00b1  0.04    0.76 \u00b10.04      0.26 \u00b1  0.02    0.77 \u00b1  0.01    0.82 \u00b10.01      0.22 \u00b1  0.00\n  ER                 0.67 \u00b1  0.07    0.43 \u00b10.04      0.44 \u00b1  0.06    0.64 \u00b1  0.01    0.34 \u00b10.02      0.49 \u00b1  0.02\n  EWC                0.56 \u00b1  0.03    0.69 \u00b10.02      0.16 \u00b1  0.02    0.32 \u00b1  0.02    0.48 \u00b10.03      0.06 \u00b1  0.00\n  PACKNET            0.60 \u00b1  0.07    0.17 \u00b10.05      0.60 \u00b1  0.05    0.63 \u00b1  0.02    0.06 \u00b10.01      0.75 \u00b1  0.01\n  MTL                                                0.54 \u00b1  0.02                                    0.80 \u00b1  0.01\nTable 2: Performance of three lifelong algorithms and the SEQL and MTL baselines on the four\ntask suites, where the policy is fixed to be RESNET-T. Results are averaged over three seeds and\nwe report the mean and standard error. The best performance is               bolded, and colored in      purple   if\nthe improvement is statistically significant over other algorithms, when a two-tailed, Student\u2019s t-test\nunder equal sample sizes and unequal variance is applied with ap-value of 0.05.\nFindings:   We observed a series of interesting findings that could potentially benefit future research\non algorithm design for LLDM:1)          SEQL shows the best FWT over all task suites. This is surprising\nsince it indicates all lifelong learning algorithms we consider actually hurt forward transfer;                   2)\nPACKNET      outperforms other lifelong learning algorithms on LIBERO-X but is outperformed by\nER significantly on LIBERO-LONG, mainly because of low forward transfer. This confirms that\nthe dynamic architecture approach is good at preventing forgetting. But since PACKNET                    splits the\nnetwork into different sub-networks, the essential capacity of the network for learning any individual\ntask is smaller. Therefore, we conjecture that PACKNET              is not rich enough to learn on LIBERO-\nLONG;    3) EWC works worse than SEQL, showing that the regularization on the loss term can actually\nimpede the agent\u2019s performance on LLDM problems (See Appendix E.2); and4)                      ER, the rehearsal\nmethod, is robust across all task suites.\nStudy on Language Embeddings as the Task Identifier (Q4)                         To investigate to what extent\nlanguage embedding play a role in LLDM, we compare the performance of the same lifelong learner\nusing four different pretrained language embeddings. Namely, we choose BERT [19], CLIP [52],\nGPT-2 [53] and the Task-ID embedding. Task-ID embeddings are produced by feeding a string such\nas \u201cTask 5\u201d into a pretrained BERT model.\nFindings:   From Table 3, we observe       no  statistically significant difference among various language\nembeddings, including the Task-ID embedding. This, we believe, is due to sentence embeddings\nfunctioning as bag-of-words that differentiates different tasks. This insight calls for better language\nencoding to harness the semantic information in task descriptions. Despite the similar performance,\nwe opt for BERT embeddings as our default task embedding.\n                                                         8", "md": "used, we observe no significant performance difference between RESNET-T and VIT-T except on the LIBERO-LONG task suite where VIT-T performs much better than RESNET-T. In contrast, if ER is used, we observe that RESNET-T performs better than VIT-T on all task suites except LIBERO-OBJECT. This potentially indicates that the ViT architecture is better at processing visual information with more object varieties than the ResNet architecture when the network capacity is sufficiently large (See the MTL results in Table 8 on LIBERO-OBJECT as the supporting evidence).\n\nThe above findings shed light on how one can improve architecture design for better processing of spatial and temporal information in LLDM.\n\nStudy on Lifelong Learning Algorithms (Q1, Q3)\n\nTable 2 reports the lifelong learning performance of the three lifelong learning algorithms, together with the SEQL and MTL baselines. All experiments use the same RESNET-T architecture as it performs the best across all policy architectures.\n\n| Lifelong Algo. | FWT(\u2191) | NBT(\u2193) | AUC(\u2191) | FWT(\u2191) | NBT(\u2193) | AUC(\u2191) |\n| --- | --- | --- | --- | --- | --- | --- |\n| SEQL | 0.54 \u00b1 0.01 | 0.63 \u00b1 0.01 | 0.15 \u00b1 0.00 | 0.72 \u00b1 0.01 | 0.81 \u00b1 0.01 | 0.20 \u00b1 0.01 |\n| ER | 0.48 \u00b1 0.02 | 0.32 \u00b1 0.04 | 0.32 \u00b1 0.01 | 0.65 \u00b1 0.03 | 0.27 \u00b1 0.03 | 0.56 \u00b1 0.01 |\n| EWC | 0.13 \u00b1 0.02 | 0.22 \u00b1 0.03 | 0.02 \u00b1 0.00 | 0.23 \u00b1 0.01 | 0.33 \u00b1 0.01 | 0.06 \u00b1 0.01 |\n| PACKNET | 0.22 \u00b1 0.01 | 0.08 \u00b1 0.01 | 0.25 \u00b1 0.00 | 0.55 \u00b1 0.01 | 0.07 \u00b1 0.02 | 0.63 \u00b1 0.00 |\n| MTL | LIBERO-OBJECT | 0.48 \u00b1 0.01 | LIBERO-GOAL | 0.83 \u00b1 0.00 |\n| SEQL | 0.78 \u00b1 0.04 | 0.76 \u00b1 0.04 | 0.26 \u00b1 0.02 | 0.77 \u00b1 0.01 | 0.82 \u00b1 0.01 | 0.22 \u00b1 0.00 |\n| ER | 0.67 \u00b1 0.07 | 0.43 \u00b1 0.04 | 0.44 \u00b1 0.06 | 0.64 \u00b1 0.01 | 0.34 \u00b1 0.02 | 0.49 \u00b1 0.02 |\n| EWC | 0.56 \u00b1 0.03 | 0.69 \u00b1 0.02 | 0.16 \u00b1 0.02 | 0.32 \u00b1 0.02 | 0.48 \u00b1 0.03 | 0.06 \u00b1 0.00 |\n| PACKNET | 0.60 \u00b1 0.07 | 0.17 \u00b1 0.05 | 0.60 \u00b1 0.05 | 0.63 \u00b1 0.02 | 0.06 \u00b1 0.01 | 0.75 \u00b1 0.01 |\n| MTL | | 0.54 \u00b1 0.02 | | 0.80 \u00b1 0.01 |\n\nTable 2: Performance of three lifelong algorithms and the SEQL and MTL baselines on the four task suites, where the policy is fixed to be RESNET-T. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded, and colored in purple if the improvement is statistically significant over other algorithms, when a two-tailed, Student\u2019s t-test under equal sample sizes and unequal variance is applied with a p-value of 0.05.\n\nFindings:\n\n1. SEQL shows the best FWT over all task suites. This is surprising since it indicates all lifelong learning algorithms we consider actually hurt forward transfer;\n2. PACKNET outperforms other lifelong learning algorithms on LIBERO-X but is outperformed by ER significantly on LIBERO-LONG, mainly because of low forward transfer. This confirms that the dynamic architecture approach is good at preventing forgetting. But since PACKNET splits the network into different sub-networks, the essential capacity of the network for learning any individual task is smaller. Therefore, we conjecture that PACKNET is not rich enough to learn on LIBERO-LONG;\n3. EWC works worse than SEQL, showing that the regularization on the loss term can actually impede the agent\u2019s performance on LLDM problems (See Appendix E.2); and\n4. ER, the rehearsal method, is robust across all task suites.\n\nStudy on Language Embeddings as the Task Identifier (Q4)\n\nTo investigate to what extent language embedding play a role in LLDM, we compare the performance of the same lifelong learner using four different pretrained language embeddings. Namely, we choose BERT [19], CLIP [52], GPT-2 [53] and the Task-ID embedding. Task-ID embeddings are produced by feeding a string such as \u201cTask 5\u201d into a pretrained BERT model.\n\nFindings:\n\nFrom Table 3, we observe no statistically significant difference among various language embeddings, including the Task-ID embedding. This, we believe, is due to sentence embeddings functioning as bag-of-words that differentiates different tasks. This insight calls for better language encoding to harness the semantic information in task descriptions. Despite the similar performance, we opt for BERT embeddings as our default task embedding.", "images": [], "items": [{"type": "text", "value": "used, we observe no significant performance difference between RESNET-T and VIT-T except on the LIBERO-LONG task suite where VIT-T performs much better than RESNET-T. In contrast, if ER is used, we observe that RESNET-T performs better than VIT-T on all task suites except LIBERO-OBJECT. This potentially indicates that the ViT architecture is better at processing visual information with more object varieties than the ResNet architecture when the network capacity is sufficiently large (See the MTL results in Table 8 on LIBERO-OBJECT as the supporting evidence).\n\nThe above findings shed light on how one can improve architecture design for better processing of spatial and temporal information in LLDM.\n\nStudy on Lifelong Learning Algorithms (Q1, Q3)\n\nTable 2 reports the lifelong learning performance of the three lifelong learning algorithms, together with the SEQL and MTL baselines. All experiments use the same RESNET-T architecture as it performs the best across all policy architectures.", "md": "used, we observe no significant performance difference between RESNET-T and VIT-T except on the LIBERO-LONG task suite where VIT-T performs much better than RESNET-T. In contrast, if ER is used, we observe that RESNET-T performs better than VIT-T on all task suites except LIBERO-OBJECT. This potentially indicates that the ViT architecture is better at processing visual information with more object varieties than the ResNet architecture when the network capacity is sufficiently large (See the MTL results in Table 8 on LIBERO-OBJECT as the supporting evidence).\n\nThe above findings shed light on how one can improve architecture design for better processing of spatial and temporal information in LLDM.\n\nStudy on Lifelong Learning Algorithms (Q1, Q3)\n\nTable 2 reports the lifelong learning performance of the three lifelong learning algorithms, together with the SEQL and MTL baselines. All experiments use the same RESNET-T architecture as it performs the best across all policy architectures."}, {"type": "table", "rows": [["Lifelong Algo.", "FWT(\u2191)", "NBT(\u2193)", "AUC(\u2191)", "FWT(\u2191)", "NBT(\u2193)", "AUC(\u2191)"], ["SEQL", "0.54 \u00b1 0.01", "0.63 \u00b1 0.01", "0.15 \u00b1 0.00", "0.72 \u00b1 0.01", "0.81 \u00b1 0.01", "0.20 \u00b1 0.01"], ["ER", "0.48 \u00b1 0.02", "0.32 \u00b1 0.04", "0.32 \u00b1 0.01", "0.65 \u00b1 0.03", "0.27 \u00b1 0.03", "0.56 \u00b1 0.01"], ["EWC", "0.13 \u00b1 0.02", "0.22 \u00b1 0.03", "0.02 \u00b1 0.00", "0.23 \u00b1 0.01", "0.33 \u00b1 0.01", "0.06 \u00b1 0.01"], ["PACKNET", "0.22 \u00b1 0.01", "0.08 \u00b1 0.01", "0.25 \u00b1 0.00", "0.55 \u00b1 0.01", "0.07 \u00b1 0.02", "0.63 \u00b1 0.00"], ["MTL", "LIBERO-OBJECT", "0.48 \u00b1 0.01", "LIBERO-GOAL", "0.83 \u00b1 0.00"], ["SEQL", "0.78 \u00b1 0.04", "0.76 \u00b1 0.04", "0.26 \u00b1 0.02", "0.77 \u00b1 0.01", "0.82 \u00b1 0.01", "0.22 \u00b1 0.00"], ["ER", "0.67 \u00b1 0.07", "0.43 \u00b1 0.04", "0.44 \u00b1 0.06", "0.64 \u00b1 0.01", "0.34 \u00b1 0.02", "0.49 \u00b1 0.02"], ["EWC", "0.56 \u00b1 0.03", "0.69 \u00b1 0.02", "0.16 \u00b1 0.02", "0.32 \u00b1 0.02", "0.48 \u00b1 0.03", "0.06 \u00b1 0.00"], ["PACKNET", "0.60 \u00b1 0.07", "0.17 \u00b1 0.05", "0.60 \u00b1 0.05", "0.63 \u00b1 0.02", "0.06 \u00b1 0.01", "0.75 \u00b1 0.01"], ["MTL", "", "0.54 \u00b1 0.02", "", "0.80 \u00b1 0.01"]], "md": "| Lifelong Algo. | FWT(\u2191) | NBT(\u2193) | AUC(\u2191) | FWT(\u2191) | NBT(\u2193) | AUC(\u2191) |\n| --- | --- | --- | --- | --- | --- | --- |\n| SEQL | 0.54 \u00b1 0.01 | 0.63 \u00b1 0.01 | 0.15 \u00b1 0.00 | 0.72 \u00b1 0.01 | 0.81 \u00b1 0.01 | 0.20 \u00b1 0.01 |\n| ER | 0.48 \u00b1 0.02 | 0.32 \u00b1 0.04 | 0.32 \u00b1 0.01 | 0.65 \u00b1 0.03 | 0.27 \u00b1 0.03 | 0.56 \u00b1 0.01 |\n| EWC | 0.13 \u00b1 0.02 | 0.22 \u00b1 0.03 | 0.02 \u00b1 0.00 | 0.23 \u00b1 0.01 | 0.33 \u00b1 0.01 | 0.06 \u00b1 0.01 |\n| PACKNET | 0.22 \u00b1 0.01 | 0.08 \u00b1 0.01 | 0.25 \u00b1 0.00 | 0.55 \u00b1 0.01 | 0.07 \u00b1 0.02 | 0.63 \u00b1 0.00 |\n| MTL | LIBERO-OBJECT | 0.48 \u00b1 0.01 | LIBERO-GOAL | 0.83 \u00b1 0.00 |\n| SEQL | 0.78 \u00b1 0.04 | 0.76 \u00b1 0.04 | 0.26 \u00b1 0.02 | 0.77 \u00b1 0.01 | 0.82 \u00b1 0.01 | 0.22 \u00b1 0.00 |\n| ER | 0.67 \u00b1 0.07 | 0.43 \u00b1 0.04 | 0.44 \u00b1 0.06 | 0.64 \u00b1 0.01 | 0.34 \u00b1 0.02 | 0.49 \u00b1 0.02 |\n| EWC | 0.56 \u00b1 0.03 | 0.69 \u00b1 0.02 | 0.16 \u00b1 0.02 | 0.32 \u00b1 0.02 | 0.48 \u00b1 0.03 | 0.06 \u00b1 0.00 |\n| PACKNET | 0.60 \u00b1 0.07 | 0.17 \u00b1 0.05 | 0.60 \u00b1 0.05 | 0.63 \u00b1 0.02 | 0.06 \u00b1 0.01 | 0.75 \u00b1 0.01 |\n| MTL | | 0.54 \u00b1 0.02 | | 0.80 \u00b1 0.01 |", "isPerfectTable": false, "csv": "\"Lifelong Algo.\",\"FWT(\u2191)\",\"NBT(\u2193)\",\"AUC(\u2191)\",\"FWT(\u2191)\",\"NBT(\u2193)\",\"AUC(\u2191)\"\n\"SEQL\",\"0.54 \u00b1 0.01\",\"0.63 \u00b1 0.01\",\"0.15 \u00b1 0.00\",\"0.72 \u00b1 0.01\",\"0.81 \u00b1 0.01\",\"0.20 \u00b1 0.01\"\n\"ER\",\"0.48 \u00b1 0.02\",\"0.32 \u00b1 0.04\",\"0.32 \u00b1 0.01\",\"0.65 \u00b1 0.03\",\"0.27 \u00b1 0.03\",\"0.56 \u00b1 0.01\"\n\"EWC\",\"0.13 \u00b1 0.02\",\"0.22 \u00b1 0.03\",\"0.02 \u00b1 0.00\",\"0.23 \u00b1 0.01\",\"0.33 \u00b1 0.01\",\"0.06 \u00b1 0.01\"\n\"PACKNET\",\"0.22 \u00b1 0.01\",\"0.08 \u00b1 0.01\",\"0.25 \u00b1 0.00\",\"0.55 \u00b1 0.01\",\"0.07 \u00b1 0.02\",\"0.63 \u00b1 0.00\"\n\"MTL\",\"LIBERO-OBJECT\",\"0.48 \u00b1 0.01\",\"LIBERO-GOAL\",\"0.83 \u00b1 0.00\"\n\"SEQL\",\"0.78 \u00b1 0.04\",\"0.76 \u00b1 0.04\",\"0.26 \u00b1 0.02\",\"0.77 \u00b1 0.01\",\"0.82 \u00b1 0.01\",\"0.22 \u00b1 0.00\"\n\"ER\",\"0.67 \u00b1 0.07\",\"0.43 \u00b1 0.04\",\"0.44 \u00b1 0.06\",\"0.64 \u00b1 0.01\",\"0.34 \u00b1 0.02\",\"0.49 \u00b1 0.02\"\n\"EWC\",\"0.56 \u00b1 0.03\",\"0.69 \u00b1 0.02\",\"0.16 \u00b1 0.02\",\"0.32 \u00b1 0.02\",\"0.48 \u00b1 0.03\",\"0.06 \u00b1 0.00\"\n\"PACKNET\",\"0.60 \u00b1 0.07\",\"0.17 \u00b1 0.05\",\"0.60 \u00b1 0.05\",\"0.63 \u00b1 0.02\",\"0.06 \u00b1 0.01\",\"0.75 \u00b1 0.01\"\n\"MTL\",\"\",\"0.54 \u00b1 0.02\",\"\",\"0.80 \u00b1 0.01\""}, {"type": "text", "value": "Table 2: Performance of three lifelong algorithms and the SEQL and MTL baselines on the four task suites, where the policy is fixed to be RESNET-T. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded, and colored in purple if the improvement is statistically significant over other algorithms, when a two-tailed, Student\u2019s t-test under equal sample sizes and unequal variance is applied with a p-value of 0.05.\n\nFindings:\n\n1. SEQL shows the best FWT over all task suites. This is surprising since it indicates all lifelong learning algorithms we consider actually hurt forward transfer;\n2. PACKNET outperforms other lifelong learning algorithms on LIBERO-X but is outperformed by ER significantly on LIBERO-LONG, mainly because of low forward transfer. This confirms that the dynamic architecture approach is good at preventing forgetting. But since PACKNET splits the network into different sub-networks, the essential capacity of the network for learning any individual task is smaller. Therefore, we conjecture that PACKNET is not rich enough to learn on LIBERO-LONG;\n3. EWC works worse than SEQL, showing that the regularization on the loss term can actually impede the agent\u2019s performance on LLDM problems (See Appendix E.2); and\n4. ER, the rehearsal method, is robust across all task suites.\n\nStudy on Language Embeddings as the Task Identifier (Q4)\n\nTo investigate to what extent language embedding play a role in LLDM, we compare the performance of the same lifelong learner using four different pretrained language embeddings. Namely, we choose BERT [19], CLIP [52], GPT-2 [53] and the Task-ID embedding. Task-ID embeddings are produced by feeding a string such as \u201cTask 5\u201d into a pretrained BERT model.\n\nFindings:\n\nFrom Table 3, we observe no statistically significant difference among various language embeddings, including the Task-ID embedding. This, we believe, is due to sentence embeddings functioning as bag-of-words that differentiates different tasks. This insight calls for better language encoding to harness the semantic information in task descriptions. Despite the similar performance, we opt for BERT embeddings as our default task embedding.", "md": "Table 2: Performance of three lifelong algorithms and the SEQL and MTL baselines on the four task suites, where the policy is fixed to be RESNET-T. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded, and colored in purple if the improvement is statistically significant over other algorithms, when a two-tailed, Student\u2019s t-test under equal sample sizes and unequal variance is applied with a p-value of 0.05.\n\nFindings:\n\n1. SEQL shows the best FWT over all task suites. This is surprising since it indicates all lifelong learning algorithms we consider actually hurt forward transfer;\n2. PACKNET outperforms other lifelong learning algorithms on LIBERO-X but is outperformed by ER significantly on LIBERO-LONG, mainly because of low forward transfer. This confirms that the dynamic architecture approach is good at preventing forgetting. But since PACKNET splits the network into different sub-networks, the essential capacity of the network for learning any individual task is smaller. Therefore, we conjecture that PACKNET is not rich enough to learn on LIBERO-LONG;\n3. EWC works worse than SEQL, showing that the regularization on the loss term can actually impede the agent\u2019s performance on LLDM problems (See Appendix E.2); and\n4. ER, the rehearsal method, is robust across all task suites.\n\nStudy on Language Embeddings as the Task Identifier (Q4)\n\nTo investigate to what extent language embedding play a role in LLDM, we compare the performance of the same lifelong learner using four different pretrained language embeddings. Namely, we choose BERT [19], CLIP [52], GPT-2 [53] and the Task-ID embedding. Task-ID embeddings are produced by feeding a string such as \u201cTask 5\u201d into a pretrained BERT model.\n\nFindings:\n\nFrom Table 3, we observe no statistically significant difference among various language embeddings, including the Task-ID embedding. This, we believe, is due to sentence embeddings functioning as bag-of-words that differentiates different tasks. This insight calls for better language encoding to harness the semantic information in task descriptions. Despite the similar performance, we opt for BERT embeddings as our default task embedding."}]}, {"page": 9, "text": "               Embedding Type           Dimension          FWT(\u2191)            NBT(\u2193)            AUC(\u2191)\n               BERT                         768          0.48  \u00b1  0.02     0.32  \u00b1  0.04     0.32  \u00b1  0.01\n               CLIP                         512          0.52  \u00b1  0.00     0.34  \u00b1  0.01     0.35  \u00b1  0.01\n               GPT-2                        768          0.46  \u00b1  0.01     0.34  \u00b1  0.02     0.30  \u00b1  0.01\n               Task-ID                      768          0.50  \u00b1  0.01     0.37  \u00b1  0.01     0.33  \u00b1  0.01\nTable 3: Performance of a lifelong learner using four different language embeddings on LIBERO-\n LONG, where we fix the policy architecture to RESNET-T and the lifelong learning algorithm to ER.\nThe Task-ID embeddings are retrieved by feeding \u201cTask + ID\" into a pretrained BERT model. Results\n are averaged over three seeds and we report the mean and standard error. The best performance is\n bolded. No statistically significant difference is observed among the different language embeddings.\n Study on task ordering (Q5)            Figure 4 shows the result of the study on          Q4. For all experiments in\n this study, we used RESNET-T as the neural architecture and evaluated both ER and PACKNET. As\n the figure illustrates, the performance of both algorithms varies across different task orderings. This\n finding highlights an important direction for future research: developing algorithms or architectures\n that are robust to varying task orderings.\n                               Success Rate\n                       0.4       0.32  0.26       0.30  0,30         0.25 0.28  0.31 0.27  0.33\n                       0.2          AUC (5 orderings)                   AUC (5 orderings)\n                                      Er    ResNet-T                  PackNet + ResNet-T\n Figure 4: Performance of ER and PACKNET                 using RESNET-T on five different task orderings. An\n error bar shows the performance standard deviation for a fixed ordering.\nFindings:    From Figure 4, we observe that indeed different task ordering could result in very different\n performances for the same algorithm. Specifically, such difference is statistically significant for\n PACKNET.\n Study on How Pretraining Affects Downstream LLDM (Q6)                                  Fig 5 reports the results on\n LIBERO-LONG          of five combinations of algorithms and policy architectures, when the underlying\n model is pretrained on the 90 short-horizion tasks in LIBERO-100 or learned from scratch. For\n pretraining, we apply behavioral cloning on the 90 tasks using the three policy architectures for 50\n epochs. We save a checkpoint every 5 epochs of training and then pick the checkpoint for each\n architecture that has the best performance as the pretrained model for downstream LLDM.\n 0.6  Success Rate                      wl pretraining                      wlo pretraining                     multitask\n  0.4                                  0.32  0.48                   0.46                   0.48                    0.48\n 0.2      0.04  0.08   20        0.22                   0.10   .25              0.01  0.02             0.17  0.25\n 0.0            ER                      ER                     ER                    EWC                  PackNet\n           ResNet-RNN               ResNet-T                  ViT-T                ResNet-T               ResNet-T\n Figure 5: Performance of different combinations of algorithms and architectures without pretraining\n or with pretraining. The multi-task learning performance is also included for reference.\nFindings:    We observe that the basic supervised pretraining can            hurt  the model\u2019s downstream lifelong\n learning performance. This, together with the results seen in Table 2 (e.g., naive sequential fine-tuning\n                                                             9", "md": "|Embedding Type|Dimension|FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|\n|---|---|---|---|---|\n|BERT|768|0.48 \u00b1 0.02|0.32 \u00b1 0.04|0.32 \u00b1 0.01|\n|CLIP|512|0.52 \u00b1 0.00|0.34 \u00b1 0.01|0.35 \u00b1 0.01|\n|GPT-2|768|0.46 \u00b1 0.01|0.34 \u00b1 0.02|0.30 \u00b1 0.01|\n|Task-ID|768|0.50 \u00b1 0.01|0.37 \u00b1 0.01|0.33 \u00b1 0.01|\n\nTable 3: Performance of a lifelong learner using four different language embeddings on LIBERO-LONG, where we fix the policy architecture to RESNET-T and the lifelong learning algorithm to ER. The Task-ID embeddings are retrieved by feeding \"Task + ID\" into a pretrained BERT model. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded. No statistically significant difference is observed among the different language embeddings.\n\nStudy on task ordering (Q5) Figure 4 shows the result of the study on Q4. For all experiments in this study, we used RESNET-T as the neural architecture and evaluated both ER and PACKNET. As the figure illustrates, the performance of both algorithms varies across different task orderings. This finding highlights an important direction for future research: developing algorithms or architectures that are robust to varying task orderings.\n\n|Success Rate|0.4|0.32|0.26|0.30|0.30|0.25|0.28|0.31|0.27|0.33|\n|---|---|---|---|---|---|---|---|---|---|---|\n|0.2|AUC (5 orderings)|ER|ResNet-T|PackNet + ResNet-T| | | | | | |\n\nFigure 4: Performance of ER and PACKNET using RESNET-T on five different task orderings. An error bar shows the performance standard deviation for a fixed ordering.\n\nFindings: From Figure 4, we observe that indeed different task ordering could result in very different performances for the same algorithm. Specifically, such difference is statistically significant for PACKNET.\n\nStudy on How Pretraining Affects Downstream LLDM (Q6) Fig 5 reports the results on LIBERO-LONG of five combinations of algorithms and policy architectures, when the underlying model is pretrained on the 90 short-horizon tasks in LIBERO-100 or learned from scratch. For pretraining, we apply behavioral cloning on the 90 tasks using the three policy architectures for 50 epochs. We save a checkpoint every 5 epochs of training and then pick the checkpoint for each architecture that has the best performance as the pretrained model for downstream LLDM.\n\n|Success Rate|wl pretraining|wlo pretraining|multitask|\n|---|---|---|---|\n|0.6|0.32|0.48|0.48|\n|0.4|0.04|0.08|0.22|0.10|0.25|0.01|0.02|0.17|0.25|\n|0.0|ER|ER|ER|EWC|PackNet|\n|ResNet-RNN|ResNet-T|ViT-T|ResNet-T|ResNet-T|\n\nFigure 5: Performance of different combinations of algorithms and architectures without pretraining or with pretraining. The multi-task learning performance is also included for reference.\n\nFindings: We observe that the basic supervised pretraining can hurt the model\u2019s downstream lifelong learning performance. This, together with the results seen in Table 2 (e.g., naive sequential fine-tuning).", "images": [{"name": "img_p8_1", "height": 356, "width": 832}, {"name": "img_p8_2", "height": 258, "width": 1188}], "items": [{"type": "table", "rows": [["Embedding Type", "Dimension", "FWT(\u2191)", "NBT(\u2193)", "AUC(\u2191)"], ["BERT", "768", "0.48 \u00b1 0.02", "0.32 \u00b1 0.04", "0.32 \u00b1 0.01"], ["CLIP", "512", "0.52 \u00b1 0.00", "0.34 \u00b1 0.01", "0.35 \u00b1 0.01"], ["GPT-2", "768", "0.46 \u00b1 0.01", "0.34 \u00b1 0.02", "0.30 \u00b1 0.01"], ["Task-ID", "768", "0.50 \u00b1 0.01", "0.37 \u00b1 0.01", "0.33 \u00b1 0.01"]], "md": "|Embedding Type|Dimension|FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|\n|---|---|---|---|---|\n|BERT|768|0.48 \u00b1 0.02|0.32 \u00b1 0.04|0.32 \u00b1 0.01|\n|CLIP|512|0.52 \u00b1 0.00|0.34 \u00b1 0.01|0.35 \u00b1 0.01|\n|GPT-2|768|0.46 \u00b1 0.01|0.34 \u00b1 0.02|0.30 \u00b1 0.01|\n|Task-ID|768|0.50 \u00b1 0.01|0.37 \u00b1 0.01|0.33 \u00b1 0.01|", "isPerfectTable": true, "csv": "\"Embedding Type\",\"Dimension\",\"FWT(\u2191)\",\"NBT(\u2193)\",\"AUC(\u2191)\"\n\"BERT\",\"768\",\"0.48 \u00b1 0.02\",\"0.32 \u00b1 0.04\",\"0.32 \u00b1 0.01\"\n\"CLIP\",\"512\",\"0.52 \u00b1 0.00\",\"0.34 \u00b1 0.01\",\"0.35 \u00b1 0.01\"\n\"GPT-2\",\"768\",\"0.46 \u00b1 0.01\",\"0.34 \u00b1 0.02\",\"0.30 \u00b1 0.01\"\n\"Task-ID\",\"768\",\"0.50 \u00b1 0.01\",\"0.37 \u00b1 0.01\",\"0.33 \u00b1 0.01\""}, {"type": "text", "value": "Table 3: Performance of a lifelong learner using four different language embeddings on LIBERO-LONG, where we fix the policy architecture to RESNET-T and the lifelong learning algorithm to ER. The Task-ID embeddings are retrieved by feeding \"Task + ID\" into a pretrained BERT model. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded. No statistically significant difference is observed among the different language embeddings.\n\nStudy on task ordering (Q5) Figure 4 shows the result of the study on Q4. For all experiments in this study, we used RESNET-T as the neural architecture and evaluated both ER and PACKNET. As the figure illustrates, the performance of both algorithms varies across different task orderings. This finding highlights an important direction for future research: developing algorithms or architectures that are robust to varying task orderings.", "md": "Table 3: Performance of a lifelong learner using four different language embeddings on LIBERO-LONG, where we fix the policy architecture to RESNET-T and the lifelong learning algorithm to ER. The Task-ID embeddings are retrieved by feeding \"Task + ID\" into a pretrained BERT model. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded. No statistically significant difference is observed among the different language embeddings.\n\nStudy on task ordering (Q5) Figure 4 shows the result of the study on Q4. For all experiments in this study, we used RESNET-T as the neural architecture and evaluated both ER and PACKNET. As the figure illustrates, the performance of both algorithms varies across different task orderings. This finding highlights an important direction for future research: developing algorithms or architectures that are robust to varying task orderings."}, {"type": "table", "rows": [["Success Rate", "0.4", "0.32", "0.26", "0.30", "0.30", "0.25", "0.28", "0.31", "0.27", "0.33"], ["0.2", "AUC (5 orderings)", "ER", "ResNet-T", "PackNet + ResNet-T", "", "", "", "", "", ""]], "md": "|Success Rate|0.4|0.32|0.26|0.30|0.30|0.25|0.28|0.31|0.27|0.33|\n|---|---|---|---|---|---|---|---|---|---|---|\n|0.2|AUC (5 orderings)|ER|ResNet-T|PackNet + ResNet-T| | | | | | |", "isPerfectTable": true, "csv": "\"Success Rate\",\"0.4\",\"0.32\",\"0.26\",\"0.30\",\"0.30\",\"0.25\",\"0.28\",\"0.31\",\"0.27\",\"0.33\"\n\"0.2\",\"AUC (5 orderings)\",\"ER\",\"ResNet-T\",\"PackNet + ResNet-T\",\"\",\"\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 4: Performance of ER and PACKNET using RESNET-T on five different task orderings. An error bar shows the performance standard deviation for a fixed ordering.\n\nFindings: From Figure 4, we observe that indeed different task ordering could result in very different performances for the same algorithm. Specifically, such difference is statistically significant for PACKNET.\n\nStudy on How Pretraining Affects Downstream LLDM (Q6) Fig 5 reports the results on LIBERO-LONG of five combinations of algorithms and policy architectures, when the underlying model is pretrained on the 90 short-horizon tasks in LIBERO-100 or learned from scratch. For pretraining, we apply behavioral cloning on the 90 tasks using the three policy architectures for 50 epochs. We save a checkpoint every 5 epochs of training and then pick the checkpoint for each architecture that has the best performance as the pretrained model for downstream LLDM.", "md": "Figure 4: Performance of ER and PACKNET using RESNET-T on five different task orderings. An error bar shows the performance standard deviation for a fixed ordering.\n\nFindings: From Figure 4, we observe that indeed different task ordering could result in very different performances for the same algorithm. Specifically, such difference is statistically significant for PACKNET.\n\nStudy on How Pretraining Affects Downstream LLDM (Q6) Fig 5 reports the results on LIBERO-LONG of five combinations of algorithms and policy architectures, when the underlying model is pretrained on the 90 short-horizon tasks in LIBERO-100 or learned from scratch. For pretraining, we apply behavioral cloning on the 90 tasks using the three policy architectures for 50 epochs. We save a checkpoint every 5 epochs of training and then pick the checkpoint for each architecture that has the best performance as the pretrained model for downstream LLDM."}, {"type": "table", "rows": [["Success Rate", "wl pretraining", "wlo pretraining", "multitask"], ["0.6", "0.32", "0.48", "0.48"], ["0.4", "0.04", "0.08", "0.22", "0.10", "0.25", "0.01", "0.02", "0.17", "0.25"], ["0.0", "ER", "ER", "ER", "EWC", "PackNet"], ["ResNet-RNN", "ResNet-T", "ViT-T", "ResNet-T", "ResNet-T"]], "md": "|Success Rate|wl pretraining|wlo pretraining|multitask|\n|---|---|---|---|\n|0.6|0.32|0.48|0.48|\n|0.4|0.04|0.08|0.22|0.10|0.25|0.01|0.02|0.17|0.25|\n|0.0|ER|ER|ER|EWC|PackNet|\n|ResNet-RNN|ResNet-T|ViT-T|ResNet-T|ResNet-T|", "isPerfectTable": false, "csv": "\"Success Rate\",\"wl pretraining\",\"wlo pretraining\",\"multitask\"\n\"0.6\",\"0.32\",\"0.48\",\"0.48\"\n\"0.4\",\"0.04\",\"0.08\",\"0.22\",\"0.10\",\"0.25\",\"0.01\",\"0.02\",\"0.17\",\"0.25\"\n\"0.0\",\"ER\",\"ER\",\"ER\",\"EWC\",\"PackNet\"\n\"ResNet-RNN\",\"ResNet-T\",\"ViT-T\",\"ResNet-T\",\"ResNet-T\""}, {"type": "text", "value": "Figure 5: Performance of different combinations of algorithms and architectures without pretraining or with pretraining. The multi-task learning performance is also included for reference.\n\nFindings: We observe that the basic supervised pretraining can hurt the model\u2019s downstream lifelong learning performance. This, together with the results seen in Table 2 (e.g., naive sequential fine-tuning).", "md": "Figure 5: Performance of different combinations of algorithms and architectures without pretraining or with pretraining. The multi-task learning performance is also included for reference.\n\nFindings: We observe that the basic supervised pretraining can hurt the model\u2019s downstream lifelong learning performance. This, together with the results seen in Table 2 (e.g., naive sequential fine-tuning)."}]}, {"page": 10, "text": "has better forward transfer than when lifelong learning algorithms are applied), indicates that better\npretraining techniques are needed.\nAttention Visualization:           To better understand what type of knowledge the agent forgets during the\nlifelong learning process, we visualize the agent\u2019s attention map on each observed image input. The\nvisualized saliency maps and the discussion can be found in Appendix E.4.\n6     Related Work\nThis section provides an overview of existing benchmarks for lifelong learning and robot learning.\nWe refer the reader to Appendix B.1 for a detailed review of lifelong learning algorithms.\nLifelong Learning Benchmarks                      Pioneering work has adapted standard vision or language\ndatasets for studying LL. This line of work includes image classification datasets like MNIST [18],\nCIFAR [34], and ImageNet [17]; segmentation datasets like Core50 [38]; and natural language\nunderstanding datasets like GLUE [67] and SuperGLUE [59]. Besides supervised learning datasets,\nvideo game benchmarks (e.g., Atari [46], XLand [64], and VisDoom [30]) in reinforcement learning\n(RL) have also been used for studying LL. However, LL in standard supervised learning does not\ninvolve procedural knowledge transfer, while RL problems in games do not represent human activities.\nContinualWorld [69] modifies the 50 manipulation tasks in MetaWorld for LL. CORA [51] builds\nfour lifelong RL benchmarks based on Atari, Procgen [15], MiniHack [58], and ALFRED [62].\nF-SIOL-310 [3] and OpenLORIS [61] are challenging real-world lifelong object learning datasets\nthat are captured from robotic vision systems. Prior works have also analyzed different components\nin a LL agent [45, 70, 21], but they do not focus on robot manipulation problems.\nRobot Learning Benchmarks                     A variety of robot learning benchmarks have been proposed\nto address challenges in meta learning (MetaWorld [73]), causality learning (CausalWorld [1]),\nmulti-task learning [27,        35], policy generalization to unseen objects [47,                24], and compositional\nlearning [44]. Compared to existing benchmarks in lifelong learning and robot learning, the task\nsuites in LIBERO are curated to address the research topics of LLDM. The benchmark includes a\nlarge number of tasks based on everyday human activities that feature rich interactive behaviors with\na diverse range of objects. Additionally, the tasks in LIBERO are procedurally generated, making\nthe benchmark scalable and adaptable. Moreover, the provided high-quality human demonstration\ndataset in LIBERO supports and encourages learning efficiency.\n7     Conclusion and Limitations\nThis paper introduces LIBERO, a new benchmark in the robot manipulation domain for supporting\nresearch in LLDM. LIBERO includes a procedural generation pipeline that can create an infinite\nnumber of manipulation tasks in the simulator. We use this pipeline to create 130 standardized tasks\nand conduct a comprehensive set of experiments on policy and algorithm designs. The empirical\nresults suggest several future research directions: 1) how to design a better neural architecture to better\nprocess spatial information or temporal information; 2) how to design a better algorithm to improve\nforward transfer ability; and 3) how to use pretraining to help improve lifelong learning performance.\nIn the short term, we do not envision any negative societal impacts triggered by LIBERO. But\nas the lifelong learner mainly learns from humans, studying how to preserve user privacy within\nLLDM [36] is crucial in the long run.\nReferences\n [1]   Ossama Ahmed, Frederik Tr\u00e4uble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bern-\n       hard Sch\u00f6lkopf, Manuel W\u00fcthrich, and Stefan Bauer. Causalworld: A robotic manipulation\n       benchmark for causal structure and transfer learning.                arXiv preprint arXiv:2010.04296, 2020.\n [2]   Ali Ayub and Carter Fendley. Few-shot continual active learning by a robot.                          arXiv preprint\n       arXiv:2210.04137, 2022.\n [3]   Ali Ayub and Alan R Wagner. F-siol-310: A robotic dataset and benchmark for few-shot incre-\n       mental object learning. In        2021 IEEE International Conference on Robotics and Automation\n       (ICRA), pages 13496\u201313502. IEEE, 2021.\n                                                              10", "md": "# Document\n\nhas better forward transfer than when lifelong learning algorithms are applied), indicates that better\npretraining techniques are needed.\n\nAttention Visualization: To better understand what type of knowledge the agent forgets during the\nlifelong learning process, we visualize the agent\u2019s attention map on each observed image input. The\nvisualized saliency maps and the discussion can be found in Appendix E.4.\n\n## Related Work\n\nThis section provides an overview of existing benchmarks for lifelong learning and robot learning.\nWe refer the reader to Appendix B.1 for a detailed review of lifelong learning algorithms.\n\n### Lifelong Learning Benchmarks\n\nPioneering work has adapted standard vision or language datasets for studying LL. This line of work includes image classification datasets like MNIST [18],\nCIFAR [34], and ImageNet [17]; segmentation datasets like Core50 [38]; and natural language\nunderstanding datasets like GLUE [67] and SuperGLUE [59]. Besides supervised learning datasets,\nvideo game benchmarks (e.g., Atari [46], XLand [64], and VisDoom [30]) in reinforcement learning\n(RL) have also been used for studying LL. However, LL in standard supervised learning does not\ninvolve procedural knowledge transfer, while RL problems in games do not represent human activities.\nContinualWorld [69] modifies the 50 manipulation tasks in MetaWorld for LL. CORA [51] builds\nfour lifelong RL benchmarks based on Atari, Procgen [15], MiniHack [58], and ALFRED [62].\nF-SIOL-310 [3] and OpenLORIS [61] are challenging real-world lifelong object learning datasets\nthat are captured from robotic vision systems. Prior works have also analyzed different components\nin a LL agent [45, 70, 21], but they do not focus on robot manipulation problems.\n\n### Robot Learning Benchmarks\n\nA variety of robot learning benchmarks have been proposed\nto address challenges in meta learning (MetaWorld [73]), causality learning (CausalWorld [1]),\nmulti-task learning [27, 35], policy generalization to unseen objects [47, 24], and compositional\nlearning [44]. Compared to existing benchmarks in lifelong learning and robot learning, the task\nsuites in LIBERO are curated to address the research topics of LLDM. The benchmark includes a\nlarge number of tasks based on everyday human activities that feature rich interactive behaviors with\na diverse range of objects. Additionally, the tasks in LIBERO are procedurally generated, making\nthe benchmark scalable and adaptable. Moreover, the provided high-quality human demonstration\ndataset in LIBERO supports and encourages learning efficiency.\n\n## Conclusion and Limitations\n\nThis paper introduces LIBERO, a new benchmark in the robot manipulation domain for supporting\nresearch in LLDM. LIBERO includes a procedural generation pipeline that can create an infinite\nnumber of manipulation tasks in the simulator. We use this pipeline to create 130 standardized tasks\nand conduct a comprehensive set of experiments on policy and algorithm designs. The empirical\nresults suggest several future research directions: 1) how to design a better neural architecture to better\nprocess spatial information or temporal information; 2) how to design a better algorithm to improve\nforward transfer ability; and 3) how to use pretraining to help improve lifelong learning performance.\nIn the short term, we do not envision any negative societal impacts triggered by LIBERO. But\nas the lifelong learner mainly learns from humans, studying how to preserve user privacy within\nLLDM [36] is crucial in the long run.\n\n## References\n\n[1] Ossama Ahmed, Frederik Tr\u00e4uble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Sch\u00f6lkopf, Manuel W\u00fcthrich, and Stefan Bauer. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprint arXiv:2010.04296, 2020.\n\n[2] Ali Ayub and Carter Fendley. Few-shot continual active learning by a robot. arXiv preprint arXiv:2210.04137, 2022.\n\n[3] Ali Ayub and Alan R Wagner. F-siol-310: A robotic dataset and benchmark for few-shot incremental object learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13496\u201313502. IEEE, 2021.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "has better forward transfer than when lifelong learning algorithms are applied), indicates that better\npretraining techniques are needed.\n\nAttention Visualization: To better understand what type of knowledge the agent forgets during the\nlifelong learning process, we visualize the agent\u2019s attention map on each observed image input. The\nvisualized saliency maps and the discussion can be found in Appendix E.4.", "md": "has better forward transfer than when lifelong learning algorithms are applied), indicates that better\npretraining techniques are needed.\n\nAttention Visualization: To better understand what type of knowledge the agent forgets during the\nlifelong learning process, we visualize the agent\u2019s attention map on each observed image input. The\nvisualized saliency maps and the discussion can be found in Appendix E.4."}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "This section provides an overview of existing benchmarks for lifelong learning and robot learning.\nWe refer the reader to Appendix B.1 for a detailed review of lifelong learning algorithms.", "md": "This section provides an overview of existing benchmarks for lifelong learning and robot learning.\nWe refer the reader to Appendix B.1 for a detailed review of lifelong learning algorithms."}, {"type": "heading", "lvl": 3, "value": "Lifelong Learning Benchmarks", "md": "### Lifelong Learning Benchmarks"}, {"type": "text", "value": "Pioneering work has adapted standard vision or language datasets for studying LL. This line of work includes image classification datasets like MNIST [18],\nCIFAR [34], and ImageNet [17]; segmentation datasets like Core50 [38]; and natural language\nunderstanding datasets like GLUE [67] and SuperGLUE [59]. Besides supervised learning datasets,\nvideo game benchmarks (e.g., Atari [46], XLand [64], and VisDoom [30]) in reinforcement learning\n(RL) have also been used for studying LL. However, LL in standard supervised learning does not\ninvolve procedural knowledge transfer, while RL problems in games do not represent human activities.\nContinualWorld [69] modifies the 50 manipulation tasks in MetaWorld for LL. CORA [51] builds\nfour lifelong RL benchmarks based on Atari, Procgen [15], MiniHack [58], and ALFRED [62].\nF-SIOL-310 [3] and OpenLORIS [61] are challenging real-world lifelong object learning datasets\nthat are captured from robotic vision systems. Prior works have also analyzed different components\nin a LL agent [45, 70, 21], but they do not focus on robot manipulation problems.", "md": "Pioneering work has adapted standard vision or language datasets for studying LL. This line of work includes image classification datasets like MNIST [18],\nCIFAR [34], and ImageNet [17]; segmentation datasets like Core50 [38]; and natural language\nunderstanding datasets like GLUE [67] and SuperGLUE [59]. Besides supervised learning datasets,\nvideo game benchmarks (e.g., Atari [46], XLand [64], and VisDoom [30]) in reinforcement learning\n(RL) have also been used for studying LL. However, LL in standard supervised learning does not\ninvolve procedural knowledge transfer, while RL problems in games do not represent human activities.\nContinualWorld [69] modifies the 50 manipulation tasks in MetaWorld for LL. CORA [51] builds\nfour lifelong RL benchmarks based on Atari, Procgen [15], MiniHack [58], and ALFRED [62].\nF-SIOL-310 [3] and OpenLORIS [61] are challenging real-world lifelong object learning datasets\nthat are captured from robotic vision systems. Prior works have also analyzed different components\nin a LL agent [45, 70, 21], but they do not focus on robot manipulation problems."}, {"type": "heading", "lvl": 3, "value": "Robot Learning Benchmarks", "md": "### Robot Learning Benchmarks"}, {"type": "text", "value": "A variety of robot learning benchmarks have been proposed\nto address challenges in meta learning (MetaWorld [73]), causality learning (CausalWorld [1]),\nmulti-task learning [27, 35], policy generalization to unseen objects [47, 24], and compositional\nlearning [44]. Compared to existing benchmarks in lifelong learning and robot learning, the task\nsuites in LIBERO are curated to address the research topics of LLDM. The benchmark includes a\nlarge number of tasks based on everyday human activities that feature rich interactive behaviors with\na diverse range of objects. Additionally, the tasks in LIBERO are procedurally generated, making\nthe benchmark scalable and adaptable. Moreover, the provided high-quality human demonstration\ndataset in LIBERO supports and encourages learning efficiency.", "md": "A variety of robot learning benchmarks have been proposed\nto address challenges in meta learning (MetaWorld [73]), causality learning (CausalWorld [1]),\nmulti-task learning [27, 35], policy generalization to unseen objects [47, 24], and compositional\nlearning [44]. Compared to existing benchmarks in lifelong learning and robot learning, the task\nsuites in LIBERO are curated to address the research topics of LLDM. The benchmark includes a\nlarge number of tasks based on everyday human activities that feature rich interactive behaviors with\na diverse range of objects. Additionally, the tasks in LIBERO are procedurally generated, making\nthe benchmark scalable and adaptable. Moreover, the provided high-quality human demonstration\ndataset in LIBERO supports and encourages learning efficiency."}, {"type": "heading", "lvl": 2, "value": "Conclusion and Limitations", "md": "## Conclusion and Limitations"}, {"type": "text", "value": "This paper introduces LIBERO, a new benchmark in the robot manipulation domain for supporting\nresearch in LLDM. LIBERO includes a procedural generation pipeline that can create an infinite\nnumber of manipulation tasks in the simulator. We use this pipeline to create 130 standardized tasks\nand conduct a comprehensive set of experiments on policy and algorithm designs. The empirical\nresults suggest several future research directions: 1) how to design a better neural architecture to better\nprocess spatial information or temporal information; 2) how to design a better algorithm to improve\nforward transfer ability; and 3) how to use pretraining to help improve lifelong learning performance.\nIn the short term, we do not envision any negative societal impacts triggered by LIBERO. But\nas the lifelong learner mainly learns from humans, studying how to preserve user privacy within\nLLDM [36] is crucial in the long run.", "md": "This paper introduces LIBERO, a new benchmark in the robot manipulation domain for supporting\nresearch in LLDM. LIBERO includes a procedural generation pipeline that can create an infinite\nnumber of manipulation tasks in the simulator. We use this pipeline to create 130 standardized tasks\nand conduct a comprehensive set of experiments on policy and algorithm designs. The empirical\nresults suggest several future research directions: 1) how to design a better neural architecture to better\nprocess spatial information or temporal information; 2) how to design a better algorithm to improve\nforward transfer ability; and 3) how to use pretraining to help improve lifelong learning performance.\nIn the short term, we do not envision any negative societal impacts triggered by LIBERO. But\nas the lifelong learner mainly learns from humans, studying how to preserve user privacy within\nLLDM [36] is crucial in the long run."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "[1] Ossama Ahmed, Frederik Tr\u00e4uble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Sch\u00f6lkopf, Manuel W\u00fcthrich, and Stefan Bauer. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprint arXiv:2010.04296, 2020.\n\n[2] Ali Ayub and Carter Fendley. Few-shot continual active learning by a robot. arXiv preprint arXiv:2210.04137, 2022.\n\n[3] Ali Ayub and Alan R Wagner. F-siol-310: A robotic dataset and benchmark for few-shot incremental object learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13496\u201313502. IEEE, 2021.", "md": "[1] Ossama Ahmed, Frederik Tr\u00e4uble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Sch\u00f6lkopf, Manuel W\u00fcthrich, and Stefan Bauer. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprint arXiv:2010.04296, 2020.\n\n[2] Ali Ayub and Carter Fendley. Few-shot continual active learning by a robot. arXiv preprint arXiv:2210.04137, 2022.\n\n[3] Ali Ayub and Alan R Wagner. F-siol-310: A robotic dataset and benchmark for few-shot incremental object learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13496\u201313502. IEEE, 2021."}]}, {"page": 11, "text": "  [4]  Michael Bain and Claude Sammut. A framework for behavioural cloning. In                            Machine Intelli-\n       gence 15, pages 103\u2013129, 1995.\n  [5]  Eseoghene Ben-Iwhiwhu, Saptarshi Nath, Praveen K Pilly, Soheil Kolouri, and Andrea Soltog-\n       gio. Lifelong reinforcement learning with modulating masks.                   arXiv preprint arXiv:2212.11110,\n       2022.\n  [6]  Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.\n       In  Proceedings of the 26th annual international conference on machine learning, pages 41\u201348,\n       2009.\n  [7]  Magdalena Biesialska, Katarzyna Biesialska, and Marta R Costa-Jussa. Continual lifelong\n       learning in natural language processing: A survey.               arXiv preprint arXiv:2012.09823, 2020.\n  [8] Christopher M Bishop. Mixture density networks. 1994.\n  [9]  Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark\n       experience for general continual learning: a strong, simple baseline.                        Advances in neural\n       information processing systems, 33:15920\u201315930, 2020.\n [10] Rich Caruana. Multitask learning.             Machine learning, 28(1):41\u201375, 1997.\n [11]  Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian\n       walk for incremental learning: Understanding forgetting and intransigence. In                       Proceedings of\n       the European Conference on Computer Vision (ECCV), pages 532\u2013547, 2018.\n [12]  Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient\n       lifelong learning with a-gem.         arXiv preprint arXiv:1812.00420, 2018.\n [13]  Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K\n       Dokania, Philip HS Torr, and Marc\u2019Aurelio Ranzato. On tiny episodic memories in continual\n       learning.    arXiv preprint arXiv:1902.10486, 2019.\n[14]   Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Su-\n       perposition of many models into one.             Advances in neural information processing systems, 32,\n       2019.\n [15]  Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation\n       to benchmark reinforcement learning. In             International conference on machine learning, pages\n       2048\u20132056. PMLR, 2020.\n [16]  Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis,\n       Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in\n       classification tasks.    IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u2013\n       3385, 2021.\n [17]  Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\n       scale hierarchical image database. In            2009 IEEE conference on computer vision and pattern\n       recognition, pages 248\u2013255. Ieee, 2009.\n [18]  Li Deng. The mnist database of handwritten digit images for machine learning research.                          IEEE\n       Signal Processing Magazine, 29(6):141\u2013142, 2012.\n [19]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\n       deep bidirectional transformers for language understanding.                  arXiv preprint arXiv:1810.04805,\n       2018.\n [20]  Natalia D\u00edaz-Rodr\u00edguez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni.                                   Don\u2019t\n       forget, there is more than forgetting: new metrics for continual learning.                          arXiv preprint\n       arXiv:1810.13166, 2018.\n [21] Beyza Ermis, Giovanni Zappella, Martin Wistuba, and C\u00e9dric Archambeau. Memory efficient\n       continual learning with transformers. 2022.            11", "md": "- Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence 15, pages 103\u2013129, 1995.\n- Eseoghene Ben-Iwhiwhu, Saptarshi Nath, Praveen K Pilly, Soheil Kolouri, and Andrea Soltoggio. Lifelong reinforcement learning with modulating masks. arXiv preprint arXiv:2212.11110, 2022.\n- Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348, 2009.\n- Magdalena Biesialska, Katarzyna Biesialska, and Marta R Costa-Jussa. Continual lifelong learning in natural language processing: A survey. arXiv preprint arXiv:2012.09823, 2020.\n- Christopher M Bishop. Mixture density networks. 1994.\n- Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020.\n- Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\n- Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European Conference on Computer Vision (ECCV), pages 532\u2013547, 2018.\n- Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.\n- Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc\u2019Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.\n- Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Superposition of many models into one. Advances in neural information processing systems, 32, 2019.\n- Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pages 2048\u20132056. PMLR, 2020.\n- Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u20133385, 2021.\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\n- Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n- Natalia D\u00edaz-Rodr\u00edguez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni. Don\u2019t forget, there is more than forgetting: new metrics for continual learning. arXiv preprint arXiv:1810.13166, 2018.\n- Beyza Ermis, Giovanni Zappella, Martin Wistuba, and C\u00e9dric Archambeau. Memory efficient continual learning with transformers. 2022.", "images": [], "items": [{"type": "text", "value": "- Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence 15, pages 103\u2013129, 1995.\n- Eseoghene Ben-Iwhiwhu, Saptarshi Nath, Praveen K Pilly, Soheil Kolouri, and Andrea Soltoggio. Lifelong reinforcement learning with modulating masks. arXiv preprint arXiv:2212.11110, 2022.\n- Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348, 2009.\n- Magdalena Biesialska, Katarzyna Biesialska, and Marta R Costa-Jussa. Continual lifelong learning in natural language processing: A survey. arXiv preprint arXiv:2012.09823, 2020.\n- Christopher M Bishop. Mixture density networks. 1994.\n- Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020.\n- Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\n- Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European Conference on Computer Vision (ECCV), pages 532\u2013547, 2018.\n- Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.\n- Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc\u2019Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.\n- Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Superposition of many models into one. Advances in neural information processing systems, 32, 2019.\n- Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pages 2048\u20132056. PMLR, 2020.\n- Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u20133385, 2021.\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\n- Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n- Natalia D\u00edaz-Rodr\u00edguez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni. Don\u2019t forget, there is more than forgetting: new metrics for continual learning. arXiv preprint arXiv:1810.13166, 2018.\n- Beyza Ermis, Giovanni Zappella, Martin Wistuba, and C\u00e9dric Archambeau. Memory efficient continual learning with transformers. 2022.", "md": "- Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence 15, pages 103\u2013129, 1995.\n- Eseoghene Ben-Iwhiwhu, Saptarshi Nath, Praveen K Pilly, Soheil Kolouri, and Andrea Soltoggio. Lifelong reinforcement learning with modulating masks. arXiv preprint arXiv:2212.11110, 2022.\n- Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348, 2009.\n- Magdalena Biesialska, Katarzyna Biesialska, and Marta R Costa-Jussa. Continual lifelong learning in natural language processing: A survey. arXiv preprint arXiv:2012.09823, 2020.\n- Christopher M Bishop. Mixture density networks. 1994.\n- Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances in neural information processing systems, 33:15920\u201315930, 2020.\n- Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\n- Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European Conference on Computer Vision (ECCV), pages 532\u2013547, 2018.\n- Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.\n- Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc\u2019Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.\n- Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Superposition of many models into one. Advances in neural information processing systems, 32, 2019.\n- Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pages 2048\u20132056. PMLR, 2020.\n- Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u20133385, 2021.\n- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\n- Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n- Natalia D\u00edaz-Rodr\u00edguez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni. Don\u2019t forget, there is more than forgetting: new metrics for continual learning. arXiv preprint arXiv:1810.13166, 2018.\n- Beyza Ermis, Giovanni Zappella, Martin Wistuba, and C\u00e9dric Archambeau. Memory efficient continual learning with transformers. 2022."}]}, {"page": 12, "text": "[22]  Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit\n      Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world\n      in 3,000 hours of egocentric video. In          Proceedings of the IEEE/CVF Conference on Computer\n      Vision and Pattern Recognition, pages 18995\u201319012, 2022.\n[23]  Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding\n      atari agents.   ArXiv, abs/1711.00138, 2017.\n[24]  Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang,\n      Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified benchmark for generalizable\n      manipulation skills.      arXiv preprint arXiv:2302.04659, 2023.\n[25]  Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps.\n      In  2015 aaai fall symposium series, 2015.\n[26]  Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-\n      Song Chen. Compacting, picking and growing for unforgetting continual learning.                        Advances in\n      Neural Information Processing Systems, 32, 2019.\n[27]  Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison.                                Rlbench: The\n      robot learning benchmark & learning environment.                   IEEE Robotics and Automation Letters,\n      5(2):3019\u20133026, 2020.\n[28]  Leslie Pack Kaelbling. The foundation of efficient robot learning.                Science, 369(6506):915\u2013916,\n      2020.\n[29]  Minsoo Kang, Jaeyoo Park, and Bohyung Han.                      Class-incremental learning by knowledge\n      distillation with adaptive feature consolidation. In           Proceedings of the IEEE/CVF conference on\n      computer vision and pattern recognition, pages 16071\u201316080, 2022.\n[30]  Micha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja\u00b4skowski.\n      Vizdoom: A doom-based ai research platform for visual reinforcement learning. In                        2016 IEEE\n      conference on computational intelligence and games (CIG), pages 1\u20138. IEEE, 2016.\n[31]  Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without\n      convolution or region supervision. In           International Conference on Machine Learning, pages\n      5583\u20135594. PMLR, 2021.\n[32]  Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.                         arXiv preprint\n      arXiv:1412.6980, 2014.\n[33]  James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\n      Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\n      Overcoming catastrophic forgetting in neural networks.               Proceedings of the national academy of\n      sciences, 114(13):3521\u20133526, 2017.\n[34]  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n      2009.\n[35]  Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00edn-\n      Mart\u00edn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k:\n      A benchmark for embodied ai with 1,000 everyday activities and realistic simulation.                               In\n      Conference on Robot Learning, pages 80\u201393. PMLR, 2023.\n[36]  B. Liu, Qian Liu, and Peter Stone. Continual learning and private unlearning. In                    CoLLAs, 2022.\n[37]  Hao Liu and Huaping Liu. Continual learning with recursive gradient optimization.                              arXiv\n      preprint arXiv:2201.12522, 2022.\n[38]  Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous\n      object recognition. In      Conference on Robot Learning, pages 17\u201326. PMLR, 2017.\n[39]  David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning.\n      Advances in neural information processing systems, 30, 2017.\n                                                            12", "md": "# References\n\n## List of References\n\n1. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\n2. Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari agents. ArXiv, abs/1711.00138, 2017.\n3. Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified benchmark for generalizable manipulation skills. arXiv preprint arXiv:2302.04659, 2023.\n4. Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In 2015 aaai fall symposium series, 2015.\n5. Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting, picking and growing for unforgetting continual learning. Advances in Neural Information Processing Systems, 32, 2019.\n6. Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020.\n7. Leslie Pack Kaelbling. The foundation of efficient robot learning. Science, 369(6506):915\u2013916, 2020.\n8. Minsoo Kang, Jaeyoo Park, and Bohyung Han. Class-incremental learning by knowledge distillation with adaptive feature consolidation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16071\u201316080, 2022.\n9. Micha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja\u00b4skowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE conference on computational intelligence and games (CIG), pages 1\u20138. IEEE, 2016.\n10. Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.\n11. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n12. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\n13. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n14. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00edn-Mart\u00edn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 80\u201393. PMLR, 2023.\n15. B. Liu, Qian Liu, and Peter Stone. Continual learning and private unlearning. In CoLLAs, 2022.\n16. Hao Liu and Huaping Liu. Continual learning with recursive gradient optimization. arXiv preprint arXiv:2201.12522, 2022.\n17. Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pages 17\u201326. PMLR, 2017.\n18. David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "List of References", "md": "## List of References"}, {"type": "text", "value": "1. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\n2. Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari agents. ArXiv, abs/1711.00138, 2017.\n3. Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified benchmark for generalizable manipulation skills. arXiv preprint arXiv:2302.04659, 2023.\n4. Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In 2015 aaai fall symposium series, 2015.\n5. Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting, picking and growing for unforgetting continual learning. Advances in Neural Information Processing Systems, 32, 2019.\n6. Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020.\n7. Leslie Pack Kaelbling. The foundation of efficient robot learning. Science, 369(6506):915\u2013916, 2020.\n8. Minsoo Kang, Jaeyoo Park, and Bohyung Han. Class-incremental learning by knowledge distillation with adaptive feature consolidation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16071\u201316080, 2022.\n9. Micha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja\u00b4skowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE conference on computational intelligence and games (CIG), pages 1\u20138. IEEE, 2016.\n10. Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.\n11. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n12. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\n13. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n14. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00edn-Mart\u00edn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 80\u201393. PMLR, 2023.\n15. B. Liu, Qian Liu, and Peter Stone. Continual learning and private unlearning. In CoLLAs, 2022.\n16. Hao Liu and Huaping Liu. Continual learning with recursive gradient optimization. arXiv preprint arXiv:2201.12522, 2022.\n17. Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pages 17\u201326. PMLR, 2017.\n18. David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.", "md": "1. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\n2. Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari agents. ArXiv, abs/1711.00138, 2017.\n3. Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified benchmark for generalizable manipulation skills. arXiv preprint arXiv:2302.04659, 2023.\n4. Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In 2015 aaai fall symposium series, 2015.\n5. Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting, picking and growing for unforgetting continual learning. Advances in Neural Information Processing Systems, 32, 2019.\n6. Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020.\n7. Leslie Pack Kaelbling. The foundation of efficient robot learning. Science, 369(6506):915\u2013916, 2020.\n8. Minsoo Kang, Jaeyoo Park, and Bohyung Han. Class-incremental learning by knowledge distillation with adaptive feature consolidation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16071\u201316080, 2022.\n9. Micha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja\u00b4skowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE conference on computational intelligence and games (CIG), pages 1\u20138. IEEE, 2016.\n10. Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.\n11. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n12. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\n13. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n14. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00edn-Mart\u00edn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 80\u201393. PMLR, 2023.\n15. B. Liu, Qian Liu, and Peter Stone. Continual learning and private unlearning. In CoLLAs, 2022.\n16. Hao Liu and Huaping Liu. Continual learning with recursive gradient optimization. arXiv preprint arXiv:2201.12522, 2022.\n17. Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pages 17\u201326. PMLR, 2017.\n18. David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017."}]}, {"page": 13, "text": "[40]  Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online\n      continual learning in image classification: An empirical survey.                  Neurocomputing, 469:28\u201351,\n      2022.\n[41]  Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by\n      iterative pruning. In       Proceedings of the IEEE conference on Computer Vision and Pattern\n      Recognition, pages 7765\u20137773, 2018.\n[42]  Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni,\n      Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Mart\u00edn-Mart\u00edn. What matters in learning\n      from offline human demonstrations for robot manipulation.                    arXiv preprint arXiv:2108.03298,\n      2021.\n[43]  Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso,\n      Daniel Weld, and David Wilkins. Pddl-the planning domain definition language. 1998.\n[44]  Jorge A Mendez, Marcel Hussing, Meghna Gummadi, and Eric Eaton. Composuite: A compo-\n      sitional reinforcement learning benchmark.              arXiv preprint arXiv:2207.04136, 2022.\n[45]  Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan\n      Gorur, and Mehrdad Farajtabar. Architecture matters in continual learning.                           arXiv preprint\n      arXiv:2202.00275, 2022.\n[46]  Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\n      Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning.                      arXiv preprint\n      arXiv:1312.5602, 2013.\n[47]  Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang,\n      Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale\n      demonstrations.       arXiv preprint arXiv:2107.14483, 2021.\n[48]  Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone.\n      Curriculum learning for reinforcement learning domains: A framework and survey.                                 arXiv\n      preprint arXiv:2003.04960, 2020.\n[49]  German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual\n      lifelong learning with neural networks: A review.               Neural Networks, 113:54\u201371, 2019.\n[50]  Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film:\n      Visual reasoning with a general conditioning layer. In               Proceedings of the AAAI Conference on\n      Artificial Intelligence, volume 32, 2018.\n[51]  Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. Cora: Benchmarks,\n      baselines, and metrics as a platform for continual reinforcement learning agents.                     arXiv preprint\n      arXiv:2110.10067, 2021.\n[52]  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n      Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n      models from natural language supervision. In                International conference on machine learning,\n      pages 8748\u20138763. PMLR, 2021.\n[53]  Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\n      Language models are unsupervised multitask learners.                  OpenAI blog, 1(8):9, 2019.\n[54]  Amanda Rios and Laurent Itti. Lifelong learning without a task oracle. In                        2020 IEEE 32nd\n      International Conference on Tools with Artificial Intelligence (ICTAI), pages 255\u2013263. IEEE,\n      2020.\n[55]  St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and\n      structured prediction to no-regret online learning. In                Proceedings of the fourteenth interna-\n      tional conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and\n      Conference Proceedings, 2011.\n                                                             13", "md": "# References\n\n# References\n\n[40] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. Neurocomputing, 469:28\u201351, 2022.\n\n[41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 7765\u20137773, 2018.\n\n[42] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Mart\u00edn-Mart\u00edn. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298, 2021.\n\n[43] Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. Pddl-the planning domain definition language. 1998.\n\n[44] Jorge A Mendez, Marcel Hussing, Meghna Gummadi, and Eric Eaton. Composuite: A compositional reinforcement learning benchmark. arXiv preprint arXiv:2207.04136, 2022.\n\n[45] Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan Gorur, and Mehrdad Farajtabar. Architecture matters in continual learning. arXiv preprint arXiv:2202.00275, 2022.\n\n[46] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\n[47] Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. arXiv preprint arXiv:2107.14483, 2021.\n\n[48] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. arXiv preprint arXiv:2003.04960, 2020.\n\n[49] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019.\n\n[50] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\n[51] Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. arXiv preprint arXiv:2110.10067, 2021.\n\n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n\n[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\n[54] Amanda Rios and Laurent Itti. Lifelong learning without a task oracle. In 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI), pages 255\u2013263. IEEE, 2020.\n\n[55] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "[40] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. Neurocomputing, 469:28\u201351, 2022.\n\n[41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 7765\u20137773, 2018.\n\n[42] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Mart\u00edn-Mart\u00edn. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298, 2021.\n\n[43] Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. Pddl-the planning domain definition language. 1998.\n\n[44] Jorge A Mendez, Marcel Hussing, Meghna Gummadi, and Eric Eaton. Composuite: A compositional reinforcement learning benchmark. arXiv preprint arXiv:2207.04136, 2022.\n\n[45] Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan Gorur, and Mehrdad Farajtabar. Architecture matters in continual learning. arXiv preprint arXiv:2202.00275, 2022.\n\n[46] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\n[47] Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. arXiv preprint arXiv:2107.14483, 2021.\n\n[48] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. arXiv preprint arXiv:2003.04960, 2020.\n\n[49] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019.\n\n[50] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\n[51] Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. arXiv preprint arXiv:2110.10067, 2021.\n\n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n\n[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\n[54] Amanda Rios and Laurent Itti. Lifelong learning without a task oracle. In 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI), pages 255\u2013263. IEEE, 2020.\n\n[55] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011.", "md": "[40] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. Neurocomputing, 469:28\u201351, 2022.\n\n[41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 7765\u20137773, 2018.\n\n[42] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Mart\u00edn-Mart\u00edn. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298, 2021.\n\n[43] Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. Pddl-the planning domain definition language. 1998.\n\n[44] Jorge A Mendez, Marcel Hussing, Meghna Gummadi, and Eric Eaton. Composuite: A compositional reinforcement learning benchmark. arXiv preprint arXiv:2207.04136, 2022.\n\n[45] Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan Gorur, and Mehrdad Farajtabar. Architecture matters in continual learning. arXiv preprint arXiv:2202.00275, 2022.\n\n[46] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\n[47] Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. arXiv preprint arXiv:2107.14483, 2021.\n\n[48] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. arXiv preprint arXiv:2003.04960, 2020.\n\n[49] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54\u201371, 2019.\n\n[50] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\n[51] Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. arXiv preprint arXiv:2110.10067, 2021.\n\n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n\n[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n\n[54] Amanda Rios and Laurent Itti. Lifelong learning without a task oracle. In 2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI), pages 255\u2013263. IEEE, 2020.\n\n[55] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011."}]}, {"page": 14, "text": " [56]  Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,\n       Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks.                              arXiv\n       preprint arXiv:1606.04671, 2016.\n [57]  Gobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy. Space: Structured compression and\n       sharing of representational space for continual learning.             IEEE Access, 9:150480\u2013150494, 2021.\n [58]  Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro,\n       Fabio Petroni, Heinrich K\u00fcttler, Edward Grefenstette, and Tim Rockt\u00e4schel. Minihack the planet:\n       A sandbox for open-ended reinforcement learning research.                   arXiv preprint arXiv:2109.13202,\n       2021.\n [59]  Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue:\n       Learning feature matching with graph neural networks.                      In  Proceedings of the IEEE/CVF\n       conference on computer vision and pattern recognition, pages 4938\u20134947, 2020.\n [60]  Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska,\n       Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework\n       for continual learning. In       International Conference on Machine Learning, pages 4528\u20134537.\n       PMLR, 2018.\n [61]  Qi She, Fan Feng, Xinyue Hao, Qihan Yang, Chuanlin Lan, Vincenzo Lomonaco, Xuesong Shi,\n       Zhengwei Wang, Yao Guo, Yimin Zhang, et al. Openloris-object: A robotic vision dataset and\n       benchmark for lifelong deep learning. In            2020 IEEE international conference on robotics and\n       automation (ICRA), pages 4767\u20134773. IEEE, 2020.\n [62]  Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-\n       taghi, Luke Zettlemoyer, and Dieter Fox.                Alfred: A benchmark for interpreting grounded\n       instructions for everyday tasks. In        Proceedings of the IEEE/CVF conference on computer vision\n       and pattern recognition, pages 10740\u201310749, 2020.\n [63]  Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent El-\n       liott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark\n       for everyday household activities in virtual, interactive, and ecological environments.                            In\n       Conference on Robot Learning, pages 477\u2013490. PMLR, 2022.\n [64]  Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck,\n       Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. Open-\n       ended learning leads to generally capable agents.              arXiv preprint arXiv:2107.12808, 2021.\n [65]  Sebastian Thrun and Tom M Mitchell. Lifelong robot learning.                         Robotics and autonomous\n       systems, 15(1-2):25\u201346, 1995.\n [66]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n       \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.                Advances in neural information\n       processing systems, 30, 2017.\n[67]   Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\n       Glue: A multi-task benchmark and analysis platform for natural language understanding.                         arXiv\n       preprint arXiv:1804.07461, 2018.\n [68]  Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and\n       Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play.\n       arXiv preprint arXiv:2302.12422, 2023.\n [69]  Maciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u2019nski, and Piotr Milo\u2019s. Continual\n       world: A robotic benchmark for continual reinforcement learning.                         In  Neural Information\n       Processing Systems, 2021.\n [70]  Maciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u2019nski, and Piotr Milo\u2019s. Disen-\n       tangling transfer in continual reinforcement learning.              ArXiv, abs/2209.13900, 2022.\n                                                             14", "md": "- Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. *Progressive neural networks.* arXiv preprint arXiv:1606.04671, 2016.\n- Gobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy. *Space: Structured compression and sharing of representational space for continual learning.* IEEE Access, 9:150480\u2013150494, 2021.\n- Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro, Fabio Petroni, Heinrich K\u00fcttler, Edward Grefenstette, and Tim Rockt\u00e4schel. *Minihack the planet: A sandbox for open-ended reinforcement learning research.* arXiv preprint arXiv:2109.13202, 2021.\n- Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. *Superglue: Learning feature matching with graph neural networks.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938\u20134947, 2020.\n- Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. *Progress & compress: A scalable framework for continual learning.* In International Conference on Machine Learning, pages 4528\u20134537. PMLR, 2018.\n- Qi She, Fan Feng, Xinyue Hao, Qihan Yang, Chuanlin Lan, Vincenzo Lomonaco, Xuesong Shi, Zhengwei Wang, Yao Guo, Yimin Zhang, et al. *Openloris-object: A robotic vision dataset and benchmark for lifelong deep learning.* In 2020 IEEE international conference on robotics and automation (ICRA), pages 4767\u20134773. IEEE, 2020.\n- Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. *Alfred: A benchmark for interpreting grounded instructions for everyday tasks.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740\u201310749, 2020.\n- Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. *Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments.* In Conference on Robot Learning, pages 477\u2013490. PMLR, 2022.\n- Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. *Open-ended learning leads to generally capable agents.* arXiv preprint arXiv:2107.12808, 2021.\n- Sebastian Thrun and Tom M Mitchell. *Lifelong robot learning.* Robotics and autonomous systems, 15(1-2):25\u201346, 1995.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. *Attention is all you need.* Advances in neural information processing systems, 30, 2017.\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. *Glue: A multi-task benchmark and analysis platform for natural language understanding.* arXiv preprint arXiv:1804.07461, 2018.\n- Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. *Mimicplay: Long-horizon imitation learning by watching human play.* arXiv preprint arXiv:2302.12422, 2023.\n- Maciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u2019nski, and Piotr Milo\u2019s. *Continual world: A robotic benchmark for continual reinforcement learning.* In Neural Information Processing Systems, 2021.\n- Maciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u2019nski, and Piotr Milo\u2019s. *Disentangling transfer in continual reinforcement learning.* ArXiv, abs/2209.13900, 2022.", "images": [], "items": [{"type": "text", "value": "- Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. *Progressive neural networks.* arXiv preprint arXiv:1606.04671, 2016.\n- Gobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy. *Space: Structured compression and sharing of representational space for continual learning.* IEEE Access, 9:150480\u2013150494, 2021.\n- Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro, Fabio Petroni, Heinrich K\u00fcttler, Edward Grefenstette, and Tim Rockt\u00e4schel. *Minihack the planet: A sandbox for open-ended reinforcement learning research.* arXiv preprint arXiv:2109.13202, 2021.\n- Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. *Superglue: Learning feature matching with graph neural networks.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938\u20134947, 2020.\n- Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. *Progress & compress: A scalable framework for continual learning.* In International Conference on Machine Learning, pages 4528\u20134537. PMLR, 2018.\n- Qi She, Fan Feng, Xinyue Hao, Qihan Yang, Chuanlin Lan, Vincenzo Lomonaco, Xuesong Shi, Zhengwei Wang, Yao Guo, Yimin Zhang, et al. *Openloris-object: A robotic vision dataset and benchmark for lifelong deep learning.* In 2020 IEEE international conference on robotics and automation (ICRA), pages 4767\u20134773. IEEE, 2020.\n- Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. *Alfred: A benchmark for interpreting grounded instructions for everyday tasks.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740\u201310749, 2020.\n- Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. *Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments.* In Conference on Robot Learning, pages 477\u2013490. PMLR, 2022.\n- Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. *Open-ended learning leads to generally capable agents.* arXiv preprint arXiv:2107.12808, 2021.\n- Sebastian Thrun and Tom M Mitchell. *Lifelong robot learning.* Robotics and autonomous systems, 15(1-2):25\u201346, 1995.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. *Attention is all you need.* Advances in neural information processing systems, 30, 2017.\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. *Glue: A multi-task benchmark and analysis platform for natural language understanding.* arXiv preprint arXiv:1804.07461, 2018.\n- Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. *Mimicplay: Long-horizon imitation learning by watching human play.* arXiv preprint arXiv:2302.12422, 2023.\n- Maciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u2019nski, and Piotr Milo\u2019s. *Continual world: A robotic benchmark for continual reinforcement learning.* In Neural Information Processing Systems, 2021.\n- Maciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u2019nski, and Piotr Milo\u2019s. *Disentangling transfer in continual reinforcement learning.* ArXiv, abs/2209.13900, 2022.", "md": "- Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. *Progressive neural networks.* arXiv preprint arXiv:1606.04671, 2016.\n- Gobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy. *Space: Structured compression and sharing of representational space for continual learning.* IEEE Access, 9:150480\u2013150494, 2021.\n- Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro, Fabio Petroni, Heinrich K\u00fcttler, Edward Grefenstette, and Tim Rockt\u00e4schel. *Minihack the planet: A sandbox for open-ended reinforcement learning research.* arXiv preprint arXiv:2109.13202, 2021.\n- Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. *Superglue: Learning feature matching with graph neural networks.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938\u20134947, 2020.\n- Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. *Progress & compress: A scalable framework for continual learning.* In International Conference on Machine Learning, pages 4528\u20134537. PMLR, 2018.\n- Qi She, Fan Feng, Xinyue Hao, Qihan Yang, Chuanlin Lan, Vincenzo Lomonaco, Xuesong Shi, Zhengwei Wang, Yao Guo, Yimin Zhang, et al. *Openloris-object: A robotic vision dataset and benchmark for lifelong deep learning.* In 2020 IEEE international conference on robotics and automation (ICRA), pages 4767\u20134773. IEEE, 2020.\n- Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. *Alfred: A benchmark for interpreting grounded instructions for everyday tasks.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740\u201310749, 2020.\n- Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. *Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments.* In Conference on Robot Learning, pages 477\u2013490. PMLR, 2022.\n- Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. *Open-ended learning leads to generally capable agents.* arXiv preprint arXiv:2107.12808, 2021.\n- Sebastian Thrun and Tom M Mitchell. *Lifelong robot learning.* Robotics and autonomous systems, 15(1-2):25\u201346, 1995.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. *Attention is all you need.* Advances in neural information processing systems, 30, 2017.\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. *Glue: A multi-task benchmark and analysis platform for natural language understanding.* arXiv preprint arXiv:1804.07461, 2018.\n- Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. *Mimicplay: Long-horizon imitation learning by watching human play.* arXiv preprint arXiv:2302.12422, 2023.\n- Maciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u2019nski, and Piotr Milo\u2019s. *Continual world: A robotic benchmark for continual reinforcement learning.* In Neural Information Processing Systems, 2021.\n- Maciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u2019nski, and Piotr Milo\u2019s. *Disentangling transfer in continual reinforcement learning.* ArXiv, abs/2209.13900, 2022."}]}, {"page": 15, "text": "[71]  Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general\n      approach for growing neural networks.             Advances in Neural Information Processing Systems,\n      33:22373\u201322383, 2020.\n[72]  Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang.                              Lifelong learning with\n      dynamically expandable networks.            arXiv preprint arXiv:1708.01547, 2017.\n[73]  Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and\n      Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement\n      learning. In    Conference on robot learning, pages 1094\u20131100. PMLR, 2020.\n[74]  Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan Zhan. Forward\n      compatible few-shot class-incremental learning. In              Proceedings of the IEEE/CVF Conference\n      on Computer Vision and Pattern Recognition, pages 9046\u20139056, 2022.\n[75]  Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. Viola: Imitation learning for vision-\n      based manipulation with object proposal priors.             arXiv preprint arXiv:2210.11339, 2022.\n[76]  Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular\n      simulation framework and benchmark for robot learning.                    arXiv preprint arXiv:2009.12293,\n      2020.\n                                                            15", "md": "# References\n\n## References\n\n[71] Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general approach for growing neural networks. Advances in Neural Information Processing Systems, 33:22373\u201322383, 2020.\n\n[72] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.\n\n[73] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020.\n\n[74] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan Zhan. Forward compatible few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9046\u20139056, 2022.\n\n[75] Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. Viola: Imitation learning for vision-based manipulation with object proposal priors. arXiv preprint arXiv:2210.11339, 2022.\n\n[76] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "[71] Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general approach for growing neural networks. Advances in Neural Information Processing Systems, 33:22373\u201322383, 2020.\n\n[72] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.\n\n[73] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020.\n\n[74] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan Zhan. Forward compatible few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9046\u20139056, 2022.\n\n[75] Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. Viola: Imitation learning for vision-based manipulation with object proposal priors. arXiv preprint arXiv:2210.11339, 2022.\n\n[76] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.", "md": "[71] Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general approach for growing neural networks. Advances in Neural Information Processing Systems, 33:22373\u201322383, 2020.\n\n[72] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.\n\n[73] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020.\n\n[74] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan Zhan. Forward compatible few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9046\u20139056, 2022.\n\n[75] Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. Viola: Imitation learning for vision-based manipulation with object proposal priors. arXiv preprint arXiv:2210.11339, 2022.\n\n[76] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020."}]}, {"page": 16, "text": "Checklist\n        1. For all authors...\n             (a)   Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s\n                   contributions and scope? [Yes]\n             (b) Did you describe the limitations of your work? [Yes]\n             (c) Did you discuss any potential negative societal impacts of your work? [Yes]\n             (d)   Have you read the ethics review guidelines and ensured that your paper conforms to\n                   them? [Yes]\n        2. If you are including theoretical results...\n             (a) Did you state the full set of assumptions of all theoretical results? [N/A]\n             (b) Did you include complete proofs of all theoretical results? [N/A]\n        3. If you ran experiments (e.g. for benchmarks)...\n             (a)   Did you include the code, data, and instructions needed to reproduce the main experi-\n                   mental results (either in the supplemental material or as a URL)? [Yes]\n             (b)   Did you specify all the training details (e.g., data splits, hyperparameters, how they\n                   were chosen)? [Yes]\n             (c)   Did you report error bars (e.g., with respect to the random seed after running experi-\n                   ments multiple times)? [Yes]\n             (d)   Did you include the total amount of compute and the type of resources used (e.g., type\n                   of GPUs, internal cluster, or cloud provider)? [Yes]\n        4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n             (a) If your work uses existing assets, did you cite the creators? [Yes]\n             (b) Did you mention the license of the assets? [N/A]\n             (c)   Did you include any new assets either in the supplemental material or as a URL? [Yes]\n             (d)   Did you discuss whether and how consent was obtained from people whose data you\u2019re\n                   using/curating? [N/A]\n             (e)   Did you discuss whether the data you are using/curating contains personally identifiable\n                   information or offensive content? [N/A]\n        5. If you used crowdsourcing or conducted research with human subjects...\n             (a)   Did you include the full text of instructions given to participants and screenshots, if\n                   applicable? [N/A]\n             (b)   Did you describe any potential participant risks, with links to Institutional Review\n                   Board (IRB) approvals, if applicable? [N/A]\n             (c)   Did you include the estimated hourly wage paid to participants and the total amount\n                   spent on participant compensation? [N/A]\n                                                                 16", "md": "# Checklist\n\n## Checklist\n\n1. For all authors...\n- (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\n- (b) Did you describe the limitations of your work? [Yes]\n- (c) Did you discuss any potential negative societal impacts of your work? [Yes]\n- (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n2. If you are including theoretical results...\n- (a) Did you state the full set of assumptions of all theoretical results? [N/A]\n- (b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n- (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\n- (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\n- (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]\n- (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n- (a) If your work uses existing assets, did you cite the creators? [Yes]\n- (b) Did you mention the license of the assets? [N/A]\n- (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\n- (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [N/A]\n- (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n- (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\n- (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n- (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Checklist", "md": "# Checklist"}, {"type": "heading", "lvl": 2, "value": "Checklist", "md": "## Checklist"}, {"type": "text", "value": "1. For all authors...\n- (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\n- (b) Did you describe the limitations of your work? [Yes]\n- (c) Did you discuss any potential negative societal impacts of your work? [Yes]\n- (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n2. If you are including theoretical results...\n- (a) Did you state the full set of assumptions of all theoretical results? [N/A]\n- (b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n- (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\n- (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\n- (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]\n- (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n- (a) If your work uses existing assets, did you cite the creators? [Yes]\n- (b) Did you mention the license of the assets? [N/A]\n- (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\n- (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [N/A]\n- (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n- (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\n- (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n- (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]", "md": "1. For all authors...\n- (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\n- (b) Did you describe the limitations of your work? [Yes]\n- (c) Did you discuss any potential negative societal impacts of your work? [Yes]\n- (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n2. If you are including theoretical results...\n- (a) Did you state the full set of assumptions of all theoretical results? [N/A]\n- (b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n- (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\n- (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\n- (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]\n- (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n- (a) If your work uses existing assets, did you cite the creators? [Yes]\n- (b) Did you mention the license of the assets? [N/A]\n- (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\n- (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [N/A]\n- (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n- (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\n- (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n- (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]"}]}, {"page": 17, "text": "A      Implemented Neural Architectures and Lifelong Learning Algorithms\n                                                                    RESNET-RNN\n                                   Neural Policy Arch.              RESNET-T\n                                                                    VIT-T\n                                                                    SEQL\n                                                                    EWC [33]\n                                   Lifelong Learning Algo.          ER [13]\n                                                                    PACKNET       [41]\n                                                                    MTL\nTable 4:     The implemented neural policy architectures and the lifelong learning algorithms in\nLIBERO.\nA.1     Neural Architectures\nIn Section 4.4, we outlined the neural network architectures utilized in our experiments, namely\nRESNET-RNN, RESNET-T, and VIT-T. The specifics of each architecture are illustrated in Figure 6.\nFurthermore, Table 5, 6, and 7 display the hyperparameters for the architectures used throughout all\nof our experiments.\n           BC-ResNet-RNN                        BC-ResNet-Transformer                       BC-Transformer2\n             GMM Policy Head                          GMM Policy Head                          GMM Policy Head\n                  LSTM                               Temporal Transformer                    Temporal Transformer\n                                              Token    Token    Token    Token    1    Token    Token    Token   Token\n                                         8                            JointGripper                            Joint/Gripper\n   Language   ResNet   ResNetJointIGripper  Language   ResNet  ResNet                Language    Vit      Vit\n   Embedding Encoder  Encoder               Embedding Encoder  Encoder              Embedding  Encoder  Encoder\n             AgentViewEye-in-Hand                    AgentView Eye-in-Hand                    AgentView Eye-in-Hand\nFigure 6: We provide visualizations of the architectures for RESNET-RNN, RESNET-T, and VIT-T,\nrespectively. It is worth noting that each model architecture incorporates language embedding in\ndistinct ways.                       ResNet Encoder                  ViT Encoder\n                                            Linear                        Linear\n                                       Spatial SoftMax                Self Attention\n                                       1 +\n                                       Residual Block\n                                   Language\n                                  Embedding                           Self Attention\n                                       1 +y)                          Self Attention\n                                       Residual Block\n                                         +                             Convolution\n                                       Residual Block          Language Image Patch Tokens\n                                         Convolution             Token\n  Figure 7: The image encoders: ResNet-based encoder and the vision transformer-based encoder.\n                                                           17", "md": "A Implemented Neural Architectures and Lifelong Learning Algorithms\n\nRESNET-RNN\n\nNeural Policy Arch.\n\nRESNET-T\n\nVIT-T\n\nSEQL\n\nEWC [33]\n\nLifelong Learning Algo.\n\nER [13]\n\nPACKNET [41]\n\nMTL\n\n**Table 4: The implemented neural policy architectures and the lifelong learning algorithms in LIBERO.**\n|Neural Architectures|\n|---|\n|In Section 4.4, we outlined the neural network architectures utilized in our experiments, namely RESNET-RNN, RESNET-T, and VIT-T. The specifics of each architecture are illustrated in Figure 6. Furthermore, Table 5, 6, and 7 display the hyperparameters for the architectures used throughout all of our experiments.|\n|<br/>BC-ResNet-RNN|BC-ResNet-Transformer|BC-Transformer2|\n|GMM Policy Head LSTM|GMM Policy Head Temporal Transformer|GMM Policy Head Token Token Token Token 1 Token Token Token Token Token|\n|Language ResNet ResNetJointIGripper|Language ResNet ResNet|Language Vit Vit|\n|Embedding Encoder Encoder|Embedding Encoder Encoder|Embedding Encoder Encoder|\n|AgentViewEye-in-Hand|AgentView Eye-in-Hand|AgentView Eye-in-Hand|\n\nFigure 6: We provide visualizations of the architectures for RESNET-RNN, RESNET-T, and VIT-T, respectively. It is worth noting that each model architecture incorporates language embedding in distinct ways. ResNet Encoder ViT Encoder Linear Linear Spatial SoftMax Self Attention 1 + Residual Block Language Embedding Self Attention 1 +y) Self Attention Residual Block + Residual Block Convolution Residual Block Language Image Patch Tokens Token\n\nFigure 7: The image encoders: ResNet-based encoder and pe vision transformer-based encoder.\nResNet Encoder Linear Spatial SoftMax 1 + Residual Block Language Embedding Self Attention 1 + Residual Block + Convolution Residual Block Language Image Patch Tokens Token", "images": [{"name": "img_p16_1", "height": 306, "width": 1188}, {"name": "img_p16_2", "height": 565, "width": 594}], "items": [{"type": "text", "value": "A Implemented Neural Architectures and Lifelong Learning Algorithms\n\nRESNET-RNN\n\nNeural Policy Arch.\n\nRESNET-T\n\nVIT-T\n\nSEQL\n\nEWC [33]\n\nLifelong Learning Algo.\n\nER [13]\n\nPACKNET [41]\n\nMTL\n\n**Table 4: The implemented neural policy architectures and the lifelong learning algorithms in LIBERO.**", "md": "A Implemented Neural Architectures and Lifelong Learning Algorithms\n\nRESNET-RNN\n\nNeural Policy Arch.\n\nRESNET-T\n\nVIT-T\n\nSEQL\n\nEWC [33]\n\nLifelong Learning Algo.\n\nER [13]\n\nPACKNET [41]\n\nMTL\n\n**Table 4: The implemented neural policy architectures and the lifelong learning algorithms in LIBERO.**"}, {"type": "table", "rows": [["Neural Architectures"], ["In Section 4.4, we outlined the neural network architectures utilized in our experiments, namely RESNET-RNN, RESNET-T, and VIT-T. The specifics of each architecture are illustrated in Figure 6. Furthermore, Table 5, 6, and 7 display the hyperparameters for the architectures used throughout all of our experiments."], ["<br/>BC-ResNet-RNN", "BC-ResNet-Transformer", "BC-Transformer2"], ["GMM Policy Head LSTM", "GMM Policy Head Temporal Transformer", "GMM Policy Head Token Token Token Token 1 Token Token Token Token Token"], ["Language ResNet ResNetJointIGripper", "Language ResNet ResNet", "Language Vit Vit"], ["Embedding Encoder Encoder", "Embedding Encoder Encoder", "Embedding Encoder Encoder"], ["AgentViewEye-in-Hand", "AgentView Eye-in-Hand", "AgentView Eye-in-Hand"]], "md": "|Neural Architectures|\n|---|\n|In Section 4.4, we outlined the neural network architectures utilized in our experiments, namely RESNET-RNN, RESNET-T, and VIT-T. The specifics of each architecture are illustrated in Figure 6. Furthermore, Table 5, 6, and 7 display the hyperparameters for the architectures used throughout all of our experiments.|\n|<br/>BC-ResNet-RNN|BC-ResNet-Transformer|BC-Transformer2|\n|GMM Policy Head LSTM|GMM Policy Head Temporal Transformer|GMM Policy Head Token Token Token Token 1 Token Token Token Token Token|\n|Language ResNet ResNetJointIGripper|Language ResNet ResNet|Language Vit Vit|\n|Embedding Encoder Encoder|Embedding Encoder Encoder|Embedding Encoder Encoder|\n|AgentViewEye-in-Hand|AgentView Eye-in-Hand|AgentView Eye-in-Hand|", "isPerfectTable": false, "csv": "\"Neural Architectures\"\n\"In Section 4.4, we outlined the neural network architectures utilized in our experiments, namely RESNET-RNN, RESNET-T, and VIT-T. The specifics of each architecture are illustrated in Figure 6. Furthermore, Table 5, 6, and 7 display the hyperparameters for the architectures used throughout all of our experiments.\"\n\"<br/>BC-ResNet-RNN\",\"BC-ResNet-Transformer\",\"BC-Transformer2\"\n\"GMM Policy Head LSTM\",\"GMM Policy Head Temporal Transformer\",\"GMM Policy Head Token Token Token Token 1 Token Token Token Token Token\"\n\"Language ResNet ResNetJointIGripper\",\"Language ResNet ResNet\",\"Language Vit Vit\"\n\"Embedding Encoder Encoder\",\"Embedding Encoder Encoder\",\"Embedding Encoder Encoder\"\n\"AgentViewEye-in-Hand\",\"AgentView Eye-in-Hand\",\"AgentView Eye-in-Hand\""}, {"type": "text", "value": "Figure 6: We provide visualizations of the architectures for RESNET-RNN, RESNET-T, and VIT-T, respectively. It is worth noting that each model architecture incorporates language embedding in distinct ways. ResNet Encoder ViT Encoder Linear Linear Spatial SoftMax Self Attention 1 + Residual Block Language Embedding Self Attention 1 +y) Self Attention Residual Block + Residual Block Convolution Residual Block Language Image Patch Tokens Token\n\nFigure 7: The image encoders: ResNet-based encoder and pe vision transformer-based encoder.\nResNet Encoder Linear Spatial SoftMax 1 + Residual Block Language Embedding Self Attention 1 + Residual Block + Convolution Residual Block Language Image Patch Tokens Token", "md": "Figure 6: We provide visualizations of the architectures for RESNET-RNN, RESNET-T, and VIT-T, respectively. It is worth noting that each model architecture incorporates language embedding in distinct ways. ResNet Encoder ViT Encoder Linear Linear Spatial SoftMax Self Attention 1 + Residual Block Language Embedding Self Attention 1 +y) Self Attention Residual Block + Residual Block Convolution Residual Block Language Image Patch Tokens Token\n\nFigure 7: The image encoders: ResNet-based encoder and pe vision transformer-based encoder.\nResNet Encoder Linear Spatial SoftMax 1 + Residual Block Language Embedding Self Attention 1 + Residual Block + Convolution Residual Block Language Image Patch Tokens Token"}]}, {"page": 18, "text": " B    Computation\n For all experiments, we use a single Nvidia A100 GPU or a single Nvidia A40 GPU (CUDA 11.7)\nwith 8 16 CPUs for training and evaluation.               18", "md": "Computation\n\nFor all experiments, we use a single Nvidia A100 GPU or a single Nvidia A40 GPU (CUDA 11.7) with 8 16 CPUs for training and evaluation. 18", "images": [], "items": [{"type": "text", "value": "Computation\n\nFor all experiments, we use a single Nvidia A100 GPU or a single Nvidia A40 GPU (CUDA 11.7) with 8 16 CPUs for training and evaluation. 18", "md": "Computation\n\nFor all experiments, we use a single Nvidia A100 GPU or a single Nvidia A40 GPU (CUDA 11.7) with 8 16 CPUs for training and evaluation. 18"}]}, {"page": 19, "text": "                   Variable                Value                            Variable                  Value\n                                                                   extra_info_hidden_size              128\n        resnet_image_embed_size              64                        img_embed_size                   64\n              text_embed_size                32                   transformer_num_layers                 4\n              rnn_hidden_size              1024                   transformer_num_heads                  6\n               rnn_layer_num                 2                 transformer_head_output_size             64\n                rnn_dropout                 0.0                transformer_mlp_hidden_size             256\n                                                                     transformer_dropout                0.1\n Table 5: Hyper parameters of RESNET-RNN.                        transformer_max_seq_len                10\n                                                             Table 6: Hyper parameters of RESNET-T.\n                                                 Variable                         Value\n                                        extra_info_hidden_size                     128\n                                            img_embed_size                         128\n                                   spatial_transformer_num_layers                   7\n                                   spatial_transformer_num_heads                    8\n                               spatial_transformer_head_output_size                120\n                                spatial_transformer_mlp_hidden_size                256\n                                     spatial_transformer_dropout                   0.1\n                                  spatial_down_sample_embed_size                   64\n                                  temporal_transformer_input_size                  null\n                                  temporal_transformer_num_layers                   4\n                                  temporal_transformer_num_heads                    6\n                              temporal_transformer_head_output_size                64\n                              temporal_transformer_mlp_hidden_size                 256\n                                    temporal_transformer_dropout                   0.1\n                                 temporal_transformer_max_seq_len                  10\n                                     Table 7: Hyper parameters of VIT-T.\n B.1    Lifelong Learning Algorithms\n Lifelong learning (LL) is a field of study that aims to understand how an agent can continually\n acquire and retain knowledge over an infinite sequence of tasks without catastrophically forgetting\n previous knowledge. Recent literature proposes three main approaches to address the problem of\n catastrophic forgetting in deep learning: Dynamic Architecture approaches, Regularization-Based\n approaches, and Rehearsal approaches. Although some recent works explore the combination of\n different approaches [2,     29,  54] or new strategies [74,     57,  14], our benchmark aims to provide an\n in-depth analysis of these three basic lifelong learning directions to reveal their pros and cons on\n robot learning tasks.\nThe dynamic architecture approach gradually expands the learning model to incorporate new knowl-\n edge [56,  72,  41, 26,  71, 5]. Regularization-based methods, on the other hand, regularize the learner\n to a previous checkpoint when it learns a new task [33,         11,60,  37]. Rehearsal methods save exemplar\n data from prior tasks and replay them with new data to consolidate the agent\u2019s memory [13,                39, 12,  9].\n For a comprehensive review of LL methods, we refer readers to surveys [16, 49].\nThe following paragraphs provide details on the three lifelong learning algorithms that we have\n implemented.\n ER     Experience Replay (ER) [13] is a        rehearsal-based      approach that maintains a memory buffer\n of samples from previous tasks and leverages it to learn new tasks. After the completion of policy\n learning for a task, ER stores a portion of the data into a storage memory. When training a new\n task, ER samples data from the memory and combines it with the training data from the current task\n so that the training data approximately represents the empirical distribution of all-task data. In our\n implementation, we use a replay buffer to store a portion of the training data (up to 1000 trajectories)\n after training each task. For every training iteration during the training of a new task, we uniformly\n                                                          19", "md": "# Document\n\n## Hyper parameters of RESNET-RNN\n\n|Variable|Value|\n|---|---|\n|extra_info_hidden_size|128|\n|resnet_image_embed_size|64|\n|img_embed_size|64|\n|text_embed_size|32|\n|rnn_hidden_size|1024|\n|rnn_layer_num|2|\n|rnn_dropout|0.0|\n|transformer_num_layers|4|\n|transformer_num_heads|6|\n|transformer_head_output_size|64|\n|transformer_mlp_hidden_size|256|\n|transformer_dropout|0.1|\n|transformer_max_seq_len|10|\n\n## Hyper parameters of RESNET-T\n\n|Variable|Value|\n|---|---|\n|extra_info_hidden_size|128|\n|img_embed_size|128|\n|spatial_transformer_num_layers|7|\n|spatial_transformer_num_heads|8|\n|spatial_transformer_head_output_size|120|\n|spatial_transformer_mlp_hidden_size|256|\n|spatial_transformer_dropout|0.1|\n|spatial_down_sample_embed_size|64|\n|temporal_transformer_input_size|null|\n|temporal_transformer_num_layers|4|\n|temporal_transformer_num_heads|6|\n|temporal_transformer_head_output_size|64|\n|temporal_transformer_mlp_hidden_size|256|\n|temporal_transformer_dropout|0.1|\n|temporal_transformer_max_seq_len|10|\n\n## Lifelong Learning Algorithms\n\nLifelong learning (LL) is a field of study that aims to understand how an agent can continually acquire and retain knowledge over an infinite sequence of tasks without catastrophically forgetting previous knowledge. Recent literature proposes three main approaches to address the problem of catastrophic forgetting in deep learning: Dynamic Architecture approaches, Regularization-Based approaches, and Rehearsal approaches. Although some recent works explore the combination of different approaches [2, 29, 54] or new strategies [74, 57, 14], our benchmark aims to provide an in-depth analysis of these three basic lifelong learning directions to reveal their pros and cons on robot learning tasks.\n\nThe dynamic architecture approach gradually expands the learning model to incorporate new knowledge [56, 72, 41, 26, 71, 5]. Regularization-based methods, on the other hand, regularize the learner to a previous checkpoint when it learns a new task [33, 11, 60, 37]. Rehearsal methods save exemplar data from prior tasks and replay them with new data to consolidate the agent\u2019s memory [13, 39, 12, 9]. For a comprehensive review of LL methods, we refer readers to surveys [16, 49].\n\nThe following paragraphs provide details on the three lifelong learning algorithms that we have implemented.\n\n### Experience Replay (ER)\n\nExperience Replay (ER) [13] is a rehearsal-based approach that maintains a memory buffer of samples from previous tasks and leverages it to learn new tasks. After the completion of policy learning for a task, ER stores a portion of the data into a storage memory. When training a new task, ER samples data from the memory and combines it with the training data from the current task so that the training data approximately represents the empirical distribution of all-task data. In our implementation, we use a replay buffer to store a portion of the training data (up to 1000 trajectories) after training each task. For every training iteration during the training of a new task, we uniformly", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Hyper parameters of RESNET-RNN", "md": "## Hyper parameters of RESNET-RNN"}, {"type": "table", "rows": [["Variable", "Value"], ["extra_info_hidden_size", "128"], ["resnet_image_embed_size", "64"], ["img_embed_size", "64"], ["text_embed_size", "32"], ["rnn_hidden_size", "1024"], ["rnn_layer_num", "2"], ["rnn_dropout", "0.0"], ["transformer_num_layers", "4"], ["transformer_num_heads", "6"], ["transformer_head_output_size", "64"], ["transformer_mlp_hidden_size", "256"], ["transformer_dropout", "0.1"], ["transformer_max_seq_len", "10"]], "md": "|Variable|Value|\n|---|---|\n|extra_info_hidden_size|128|\n|resnet_image_embed_size|64|\n|img_embed_size|64|\n|text_embed_size|32|\n|rnn_hidden_size|1024|\n|rnn_layer_num|2|\n|rnn_dropout|0.0|\n|transformer_num_layers|4|\n|transformer_num_heads|6|\n|transformer_head_output_size|64|\n|transformer_mlp_hidden_size|256|\n|transformer_dropout|0.1|\n|transformer_max_seq_len|10|", "isPerfectTable": true, "csv": "\"Variable\",\"Value\"\n\"extra_info_hidden_size\",\"128\"\n\"resnet_image_embed_size\",\"64\"\n\"img_embed_size\",\"64\"\n\"text_embed_size\",\"32\"\n\"rnn_hidden_size\",\"1024\"\n\"rnn_layer_num\",\"2\"\n\"rnn_dropout\",\"0.0\"\n\"transformer_num_layers\",\"4\"\n\"transformer_num_heads\",\"6\"\n\"transformer_head_output_size\",\"64\"\n\"transformer_mlp_hidden_size\",\"256\"\n\"transformer_dropout\",\"0.1\"\n\"transformer_max_seq_len\",\"10\""}, {"type": "heading", "lvl": 2, "value": "Hyper parameters of RESNET-T", "md": "## Hyper parameters of RESNET-T"}, {"type": "table", "rows": [["Variable", "Value"], ["extra_info_hidden_size", "128"], ["img_embed_size", "128"], ["spatial_transformer_num_layers", "7"], ["spatial_transformer_num_heads", "8"], ["spatial_transformer_head_output_size", "120"], ["spatial_transformer_mlp_hidden_size", "256"], ["spatial_transformer_dropout", "0.1"], ["spatial_down_sample_embed_size", "64"], ["temporal_transformer_input_size", "null"], ["temporal_transformer_num_layers", "4"], ["temporal_transformer_num_heads", "6"], ["temporal_transformer_head_output_size", "64"], ["temporal_transformer_mlp_hidden_size", "256"], ["temporal_transformer_dropout", "0.1"], ["temporal_transformer_max_seq_len", "10"]], "md": "|Variable|Value|\n|---|---|\n|extra_info_hidden_size|128|\n|img_embed_size|128|\n|spatial_transformer_num_layers|7|\n|spatial_transformer_num_heads|8|\n|spatial_transformer_head_output_size|120|\n|spatial_transformer_mlp_hidden_size|256|\n|spatial_transformer_dropout|0.1|\n|spatial_down_sample_embed_size|64|\n|temporal_transformer_input_size|null|\n|temporal_transformer_num_layers|4|\n|temporal_transformer_num_heads|6|\n|temporal_transformer_head_output_size|64|\n|temporal_transformer_mlp_hidden_size|256|\n|temporal_transformer_dropout|0.1|\n|temporal_transformer_max_seq_len|10|", "isPerfectTable": true, "csv": "\"Variable\",\"Value\"\n\"extra_info_hidden_size\",\"128\"\n\"img_embed_size\",\"128\"\n\"spatial_transformer_num_layers\",\"7\"\n\"spatial_transformer_num_heads\",\"8\"\n\"spatial_transformer_head_output_size\",\"120\"\n\"spatial_transformer_mlp_hidden_size\",\"256\"\n\"spatial_transformer_dropout\",\"0.1\"\n\"spatial_down_sample_embed_size\",\"64\"\n\"temporal_transformer_input_size\",\"null\"\n\"temporal_transformer_num_layers\",\"4\"\n\"temporal_transformer_num_heads\",\"6\"\n\"temporal_transformer_head_output_size\",\"64\"\n\"temporal_transformer_mlp_hidden_size\",\"256\"\n\"temporal_transformer_dropout\",\"0.1\"\n\"temporal_transformer_max_seq_len\",\"10\""}, {"type": "heading", "lvl": 2, "value": "Lifelong Learning Algorithms", "md": "## Lifelong Learning Algorithms"}, {"type": "text", "value": "Lifelong learning (LL) is a field of study that aims to understand how an agent can continually acquire and retain knowledge over an infinite sequence of tasks without catastrophically forgetting previous knowledge. Recent literature proposes three main approaches to address the problem of catastrophic forgetting in deep learning: Dynamic Architecture approaches, Regularization-Based approaches, and Rehearsal approaches. Although some recent works explore the combination of different approaches [2, 29, 54] or new strategies [74, 57, 14], our benchmark aims to provide an in-depth analysis of these three basic lifelong learning directions to reveal their pros and cons on robot learning tasks.\n\nThe dynamic architecture approach gradually expands the learning model to incorporate new knowledge [56, 72, 41, 26, 71, 5]. Regularization-based methods, on the other hand, regularize the learner to a previous checkpoint when it learns a new task [33, 11, 60, 37]. Rehearsal methods save exemplar data from prior tasks and replay them with new data to consolidate the agent\u2019s memory [13, 39, 12, 9]. For a comprehensive review of LL methods, we refer readers to surveys [16, 49].\n\nThe following paragraphs provide details on the three lifelong learning algorithms that we have implemented.", "md": "Lifelong learning (LL) is a field of study that aims to understand how an agent can continually acquire and retain knowledge over an infinite sequence of tasks without catastrophically forgetting previous knowledge. Recent literature proposes three main approaches to address the problem of catastrophic forgetting in deep learning: Dynamic Architecture approaches, Regularization-Based approaches, and Rehearsal approaches. Although some recent works explore the combination of different approaches [2, 29, 54] or new strategies [74, 57, 14], our benchmark aims to provide an in-depth analysis of these three basic lifelong learning directions to reveal their pros and cons on robot learning tasks.\n\nThe dynamic architecture approach gradually expands the learning model to incorporate new knowledge [56, 72, 41, 26, 71, 5]. Regularization-based methods, on the other hand, regularize the learner to a previous checkpoint when it learns a new task [33, 11, 60, 37]. Rehearsal methods save exemplar data from prior tasks and replay them with new data to consolidate the agent\u2019s memory [13, 39, 12, 9]. For a comprehensive review of LL methods, we refer readers to surveys [16, 49].\n\nThe following paragraphs provide details on the three lifelong learning algorithms that we have implemented."}, {"type": "heading", "lvl": 3, "value": "Experience Replay (ER)", "md": "### Experience Replay (ER)"}, {"type": "text", "value": "Experience Replay (ER) [13] is a rehearsal-based approach that maintains a memory buffer of samples from previous tasks and leverages it to learn new tasks. After the completion of policy learning for a task, ER stores a portion of the data into a storage memory. When training a new task, ER samples data from the memory and combines it with the training data from the current task so that the training data approximately represents the empirical distribution of all-task data. In our implementation, we use a replay buffer to store a portion of the training data (up to 1000 trajectories) after training each task. For every training iteration during the training of a new task, we uniformly", "md": "Experience Replay (ER) [13] is a rehearsal-based approach that maintains a memory buffer of samples from previous tasks and leverages it to learn new tasks. After the completion of policy learning for a task, ER stores a portion of the data into a storage memory. When training a new task, ER samples data from the memory and combines it with the training data from the current task so that the training data approximately represents the empirical distribution of all-task data. In our implementation, we use a replay buffer to store a portion of the training data (up to 1000 trajectories) after training each task. For every training iteration during the training of a new task, we uniformly"}]}, {"page": 20, "text": "sample a fixed number of replay data from the memory (32 trajectories) along with each batch of\ntraining data from the new task.\nEWC        Elastic Weight Consolidation(EWC) [33] is a                   regularization-based          approach that add a\nregularization term that constraints neural network update to the original single-task learning objective.\nSpecifically, EWC uses the Fisher information matrix that quantify the importance of every neural\nnetwrk parameter. The loss function for task              k  is:\n                                L EW C   (\u03b8) =    L BC  (\u03b8) +    X    \u03bb F i \u25af \u03b8i \u2212  \u03b8 \u2217     \u0001 2 ,\n                                  k                 K             i   2               k\u22121,i\nwhere   \u03bb  is a penalty hyperparameter, and the coefficient       2      F i is the diagonal of the Fisher information\nmatrix:   F k  =  E s\u223cD   kE a\u223cp  \u03b8(\u00b7|s) (\u2207  \u03b8k  logp  \u03b8k (a|s))   . In this work, we use the online update version\nof EWC that updates the Fisher information matrix using exponential moving average along the\nlifelong learning process, and use the empirical estimation of above Fisher information matrix to\nstabilize the estimation. Formally, the actually used estimation of Fisher Information Matrix is\nF\u02dck =   \u03b3F  k\u22121  + (1   \u2212\u03b3)F    k , where   F k =   E (s,a)\u223cD  k  (\u2207  \u03b8k log  p\u03b8 k(a|s))  2 and   k is the task number. We\nset \u03b3  = 0.9   and   \u03bb  = 5\u00b7   10 4 .\nPACKNET          PACKNET       [41] is a  dynamic architecture-based             approach that aims to prevent changes\nto parameters that are important for previous tasks in lifelong learning. To achieve this, PACKNET\niteratively trains, prunes, fine-tunes, and freezes parts of the network. The method theoretically\ncompletely avoids catastrophic forgetting, but for each new task, the number of available parameters\nshrinks. The pruning process in PACKNET               involves two stages. First, the network is trained, and at the\nend of the training, a fixed proportion of the most important parameters (25% in our implementation)\nare chosen, and the rest are pruned. Second, the selected part of the network is fine-tuned and then\nfrozen. In our implementation, we follow the original paper [41] and do not train all biases and\nnormalization layers. We perform the same number of fine-tuning epochs as for training (50 epochs\nin our implementation). Note that all evaluation metrics are calculated                     before   the fine-tuning stage.\n                                                               20", "md": "```markdown\nsample a fixed number of replay data from the memory (32 trajectories) along with each batch of\ntraining data from the new task.\nEWC        Elastic Weight Consolidation(EWC) [33] is a regularization-based approach that adds a\nregularization term that constraints neural network update to the original single-task learning objective.\nSpecifically, EWC uses the Fisher information matrix that quantifies the importance of every neural\nnetwork parameter. The loss function for task $k$ is:\n$$\nL_{EWC}(\\theta) = L_{BC}(\\theta) + \\sum_{i} \\lambda F_i (\\theta_i - \\theta^*)^2,\n$$\nwhere $\\lambda$ is a penalty hyperparameter, and the coefficient $F_i$ is the diagonal of the Fisher information\nmatrix: $F_k = E_{s \\sim D_k} E_{a \\sim p_{\\theta(\\cdot|s)}} (\\nabla_{\\theta_k} \\log p_{\\theta_k}(a|s))^2$. In this work, we use the online update version\nof EWC that updates the Fisher information matrix using exponential moving average along the\nlifelong learning process, and use the empirical estimation of the above Fisher information matrix to\nstabilize the estimation. Formally, the actually used estimation of the Fisher Information Matrix is\n$\\tilde{F}_k = \\gamma F_{k-1} + (1 - \\gamma)F_k$, where $F_k = E_{(s,a) \\sim D_k} (\\nabla_{\\theta_k} \\log p_{\\theta_k}(a|s))^2$ and $k$ is the task number. We\nset $\\gamma = 0.9$ and $\\lambda = 5 \\times 10^4$.\nPACKNET          PACKNET       [41] is a dynamic architecture-based approach that aims to prevent changes\nto parameters that are important for previous tasks in lifelong learning. To achieve this, PACKNET\niteratively trains, prunes, fine-tunes, and freezes parts of the network. The method theoretically\ncompletely avoids catastrophic forgetting, but for each new task, the number of available parameters\nshrinks. The pruning process in PACKNET involves two stages. First, the network is trained, and at the\nend of the training, a fixed proportion of the most important parameters (25% in our implementation)\nare chosen, and the rest are pruned. Second, the selected part of the network is fine-tuned and then\nfrozen. In our implementation, we follow the original paper [41] and do not train all biases and\nnormalization layers. We perform the same number of fine-tuning epochs as for training (50 epochs\nin our implementation). Note that all evaluation metrics are calculated before the fine-tuning stage.\n20\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nsample a fixed number of replay data from the memory (32 trajectories) along with each batch of\ntraining data from the new task.\nEWC        Elastic Weight Consolidation(EWC) [33] is a regularization-based approach that adds a\nregularization term that constraints neural network update to the original single-task learning objective.\nSpecifically, EWC uses the Fisher information matrix that quantifies the importance of every neural\nnetwork parameter. The loss function for task $k$ is:\n$$\nL_{EWC}(\\theta) = L_{BC}(\\theta) + \\sum_{i} \\lambda F_i (\\theta_i - \\theta^*)^2,\n$$\nwhere $\\lambda$ is a penalty hyperparameter, and the coefficient $F_i$ is the diagonal of the Fisher information\nmatrix: $F_k = E_{s \\sim D_k} E_{a \\sim p_{\\theta(\\cdot|s)}} (\\nabla_{\\theta_k} \\log p_{\\theta_k}(a|s))^2$. In this work, we use the online update version\nof EWC that updates the Fisher information matrix using exponential moving average along the\nlifelong learning process, and use the empirical estimation of the above Fisher information matrix to\nstabilize the estimation. Formally, the actually used estimation of the Fisher Information Matrix is\n$\\tilde{F}_k = \\gamma F_{k-1} + (1 - \\gamma)F_k$, where $F_k = E_{(s,a) \\sim D_k} (\\nabla_{\\theta_k} \\log p_{\\theta_k}(a|s))^2$ and $k$ is the task number. We\nset $\\gamma = 0.9$ and $\\lambda = 5 \\times 10^4$.\nPACKNET          PACKNET       [41] is a dynamic architecture-based approach that aims to prevent changes\nto parameters that are important for previous tasks in lifelong learning. To achieve this, PACKNET\niteratively trains, prunes, fine-tunes, and freezes parts of the network. The method theoretically\ncompletely avoids catastrophic forgetting, but for each new task, the number of available parameters\nshrinks. The pruning process in PACKNET involves two stages. First, the network is trained, and at the\nend of the training, a fixed proportion of the most important parameters (25% in our implementation)\nare chosen, and the rest are pruned. Second, the selected part of the network is fine-tuned and then\nfrozen. In our implementation, we follow the original paper [41] and do not train all biases and\nnormalization layers. We perform the same number of fine-tuning epochs as for training (50 epochs\nin our implementation). Note that all evaluation metrics are calculated before the fine-tuning stage.\n20\n```", "md": "```markdown\nsample a fixed number of replay data from the memory (32 trajectories) along with each batch of\ntraining data from the new task.\nEWC        Elastic Weight Consolidation(EWC) [33] is a regularization-based approach that adds a\nregularization term that constraints neural network update to the original single-task learning objective.\nSpecifically, EWC uses the Fisher information matrix that quantifies the importance of every neural\nnetwork parameter. The loss function for task $k$ is:\n$$\nL_{EWC}(\\theta) = L_{BC}(\\theta) + \\sum_{i} \\lambda F_i (\\theta_i - \\theta^*)^2,\n$$\nwhere $\\lambda$ is a penalty hyperparameter, and the coefficient $F_i$ is the diagonal of the Fisher information\nmatrix: $F_k = E_{s \\sim D_k} E_{a \\sim p_{\\theta(\\cdot|s)}} (\\nabla_{\\theta_k} \\log p_{\\theta_k}(a|s))^2$. In this work, we use the online update version\nof EWC that updates the Fisher information matrix using exponential moving average along the\nlifelong learning process, and use the empirical estimation of the above Fisher information matrix to\nstabilize the estimation. Formally, the actually used estimation of the Fisher Information Matrix is\n$\\tilde{F}_k = \\gamma F_{k-1} + (1 - \\gamma)F_k$, where $F_k = E_{(s,a) \\sim D_k} (\\nabla_{\\theta_k} \\log p_{\\theta_k}(a|s))^2$ and $k$ is the task number. We\nset $\\gamma = 0.9$ and $\\lambda = 5 \\times 10^4$.\nPACKNET          PACKNET       [41] is a dynamic architecture-based approach that aims to prevent changes\nto parameters that are important for previous tasks in lifelong learning. To achieve this, PACKNET\niteratively trains, prunes, fine-tunes, and freezes parts of the network. The method theoretically\ncompletely avoids catastrophic forgetting, but for each new task, the number of available parameters\nshrinks. The pruning process in PACKNET involves two stages. First, the network is trained, and at the\nend of the training, a fixed proportion of the most important parameters (25% in our implementation)\nare chosen, and the rest are pruned. Second, the selected part of the network is fine-tuned and then\nfrozen. In our implementation, we follow the original paper [41] and do not train all biases and\nnormalization layers. We perform the same number of fine-tuning epochs as for training (50 epochs\nin our implementation). Note that all evaluation metrics are calculated before the fine-tuning stage.\n20\n```"}]}, {"page": 21, "text": " C     LIBERO Task Suite Designs\n C.1    Task Suites\nWe visualize all the tasks from the four task suites in Figure 8- 11. Figure 8 visualizes the initial\n states since the task goals are always the same. All the figures visualize the goal states of tasks except\n for Figure 8, which visualizes the initial states since the task goals are always the same.\n                                             Figure 8: LIBERO-SPATIAL\n                                             Figure 9: LIBERO-OBJECT\n                                              Figure 10: LIBERO-GOAL\n                                                              21", "md": "C     LIBERO Task Suite Designs\n\nC.1    Task Suites\n\nWe visualize all the tasks from the four task suites in Figure 8- 11. Figure 8 visualizes the initial states since the task goals are always the same. All the figures visualize the goal states of tasks except for Figure 8, which visualizes the initial states since the task goals are always the same.\n\nFigure 8: LIBERO-SPATIAL\n\nFigure 9: LIBERO-OBJECT\n\nFigure 10: LIBERO-GOAL\n\n21", "images": [{"name": "img_p20_1", "height": 381, "width": 951}, {"name": "img_p20_2", "height": 381, "width": 951}, {"name": "img_p20_3", "height": 381, "width": 951}], "items": [{"type": "text", "value": "C     LIBERO Task Suite Designs\n\nC.1    Task Suites\n\nWe visualize all the tasks from the four task suites in Figure 8- 11. Figure 8 visualizes the initial states since the task goals are always the same. All the figures visualize the goal states of tasks except for Figure 8, which visualizes the initial states since the task goals are always the same.\n\nFigure 8: LIBERO-SPATIAL\n\nFigure 9: LIBERO-OBJECT\n\nFigure 10: LIBERO-GOAL\n\n21", "md": "C     LIBERO Task Suite Designs\n\nC.1    Task Suites\n\nWe visualize all the tasks from the four task suites in Figure 8- 11. Figure 8 visualizes the initial states since the task goals are always the same. All the figures visualize the goal states of tasks except for Figure 8, which visualizes the initial states since the task goals are always the same.\n\nFigure 8: LIBERO-SPATIAL\n\nFigure 9: LIBERO-OBJECT\n\nFigure 10: LIBERO-GOAL\n\n21"}]}, {"page": 22, "text": "Figure 11: LIBERO-100\n            22", "md": "Figure 11: LIBERO-100\n\n22", "images": [{"name": "img_p21_1", "height": 1070, "width": 1070}], "items": [{"type": "text", "value": "Figure 11: LIBERO-100\n\n22", "md": "Figure 11: LIBERO-100\n\n22"}]}, {"page": 23, "text": "C.2      PDDL-based Scene Description File\nHere we visualize the whole content of an example scene description file based on PDDL. This file\ncorresponds to the task shown in Figure 2.\n                 Example task:                  Open the top drawer of the cabinet and put the bowl in it.\n( d e f i n e      ( p r o b l e m      L I B E R O _ K i t c h e n _ T a b l e t o p _ M a n i p u l a t i o n )\n     ( : domain           r o b o s u i t e )\n     ( : l(a:nrgeugaigoen s   open        t h e    t o p     d r a w e r     o f    t h e     c a b i n e t     and  p u t  t h e  bowl  i n  i t )\n              ( w o o d e n _ c a b i n e t _ i n i t _ r e g i o n\n                        ( : t a r g e t      k i t c h e n _ t a b l e )\n                        ( : r a n(g\u2212e0s. 0 1(    \u22120.31         0 . 0 1     \u2212 0 . 2 9 )\n                        )   )\n                        ( : y a w(_3r.o1t4a1t5i9o2n6 5 3 5(8 9 7 9 3         3 . 1 4 1 5 9 2 6 5 3 5 8 9 7 9 3 )\n              )         )   )\n              ( a k i t a _ b l a c k _ b o w l _ i n i t _ r e g i o n\n                        ( : t a r g e t      k i t c h e n _ t a b l e )\n                        ( : r a n(g\u2212e0s. 0 2(5      \u22120.025          0 . 0 2 5     0 . 0 2 5 )\n                        )   )\n                        ( : y a w(_0r.o0t a t0i.o0n)      (\n              )         )   )\n              ( p l a t e _ i n i t _ r e g i o n\n                        ( : t a r g e t      k i t c h e n _ t a b l e )\n                        ( : r a n(g\u2212e0s. 0 2(5     0 . 2 2 5     0 . 0 2 5      0 . 2 7 5 )\n                        )   )\n                        ( : y a w(_0r.o0t a t0i.o0n)      (\n              )         )   )\n              ( t o p _ s i d e\n              )         ( : t a r g e t     w o o d e n _ c a b i n e t _ 1 )\n              ( t o p _ r e g i o n\n              )         ( : t a r g e t     w o o d e n _ c a b i n e t _ 1 )\n              ( m i d d l e _ r e g i o n\n              )         ( : t a r g e t     w o o d e n _ c a b i n e t _ 1 )\n              ( b o t t o m _ r e g i o n\n          )   )         ( : t a r g e t     w o o d e n _ c a b i n e t _ 1 )\n     ( : fkiixttcuhreens_ t a b l e       \u2212    k i t c h e n _ t a b l e\n                                                                           23", "md": "## PDDL-based Scene Description File\n\nHere we visualize the whole content of an example scene description file based on PDDL. This file corresponds to the task shown in Figure 2.\n\nExample task: Open the top drawer of the cabinet and put the bowl in it.\n\n$$\n\\begin{align*}\n&\\text{(define (problem LIBERO\\_Kitchen\\_Tabletop\\_Manipulation)} \\\\\n&\\text{(:domain robosuite)} \\\\\n&\\text{(:init} \\\\\n&\\quad \\text{(wooden\\_cabinet\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges -0.31 0.01 -0.29)} \\\\\n&\\quad \\quad \\text{(:yaw-rotation 3.141592653589793)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(akita\\_black\\_bowl\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges -0.025 0.025 0.025)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(plate\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges 0.225 0.025 0.275)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(top\\_side} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(top\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(middle\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(bottom\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\text{(:features table - kitchen\\_table)} \\\\\n&\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "PDDL-based Scene Description File", "md": "## PDDL-based Scene Description File"}, {"type": "text", "value": "Here we visualize the whole content of an example scene description file based on PDDL. This file corresponds to the task shown in Figure 2.\n\nExample task: Open the top drawer of the cabinet and put the bowl in it.\n\n$$\n\\begin{align*}\n&\\text{(define (problem LIBERO\\_Kitchen\\_Tabletop\\_Manipulation)} \\\\\n&\\text{(:domain robosuite)} \\\\\n&\\text{(:init} \\\\\n&\\quad \\text{(wooden\\_cabinet\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges -0.31 0.01 -0.29)} \\\\\n&\\quad \\quad \\text{(:yaw-rotation 3.141592653589793)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(akita\\_black\\_bowl\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges -0.025 0.025 0.025)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(plate\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges 0.225 0.025 0.275)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(top\\_side} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(top\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(middle\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(bottom\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\text{(:features table - kitchen\\_table)} \\\\\n&\\end{align*}\n$$", "md": "Here we visualize the whole content of an example scene description file based on PDDL. This file corresponds to the task shown in Figure 2.\n\nExample task: Open the top drawer of the cabinet and put the bowl in it.\n\n$$\n\\begin{align*}\n&\\text{(define (problem LIBERO\\_Kitchen\\_Tabletop\\_Manipulation)} \\\\\n&\\text{(:domain robosuite)} \\\\\n&\\text{(:init} \\\\\n&\\quad \\text{(wooden\\_cabinet\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges -0.31 0.01 -0.29)} \\\\\n&\\quad \\quad \\text{(:yaw-rotation 3.141592653589793)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(akita\\_black\\_bowl\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges -0.025 0.025 0.025)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(plate\\_init\\_region} \\\\\n&\\quad \\quad \\text{(:target kitchen\\_table)} \\\\\n&\\quad \\quad \\text{(:ranges 0.225 0.025 0.275)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(top\\_side} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(top\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(middle\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\quad \\text{(bottom\\_region} \\\\\n&\\quad \\quad \\text{(:target wooden\\_cabinet\\_1)} \\\\\n&\\quad ) \\\\\n&\\text{(:features table - kitchen\\_table)} \\\\\n&\\end{align*}\n$$"}]}, {"page": 24, "text": "   )    w o o d e n _ c a b i n e t _ 1           \u2212     w o o d e n _ c a b i n e t\n   ( : oabkjietcat_sb l a c k _ b o w l _ 1            \u2212     a k i t a _ b l a c k _ b o w l\n   )    p l a t e _ 1      \u2212     p l a t e\n   ( : owbojo_doefn__icnatbeirneestt_ 1\n   )    a k i t a _ b l a c k _ b o w l _ 1\n   ( : i n i t\n        ( On      a k i t a _ b l a c k _ b o w l _ 1             k i t c h e n _ t a b l e _ a k i t a _ b l a c k _ b o w l _ i n i t _ r e g i o n )\n        ( On      p l a t e _ 1       k i t c h e n _ t a b l e _ p l a t e _ i n i t _ r e g i o n )\n   )    ( On      w o o d e n _ c a b i n e t _ 1            k i t c h e n _ t a b l e _ w o o d e n _ c a b i n e t _ i n i t _ r e g i o n )\n   ( : g o a l\n        ( And        ( Open         w o o d e n _ c a b i n e t _ 1 _ t o p _ r e g i o n )\n)  )    )            ( I n     a k i t a _ b l a c k _ b o w l _ 1             w o o d e n _ c a b i n e t _ 1 _ t o p _ r e g i o n )\n                                                                               24", "md": "$$wooden\\_cabinet\\_1 - wooden\\_cabinet$$\n\n$$:oabkjietcat\\_sb\\ black\\_bowl\\_1 - akita\\_black\\_bowl$$\n\n$$plate\\_1 - plate$$\n\n$$:owbojo\\_doefn\\_\\_icnatbeirneestt\\_1$$\n\n$$akita\\_black\\_bowl\\_1$$\n\n$$:init$$\n- $On\\ akita\\_black\\_bowl\\_1\\ kitchen\\_table\\_akita\\_black\\_bowl\\_init\\_region$\n- $On\\ plate\\_1\\ kitchen\\_table\\_plate\\_init\\_region$\n\n$$On\\ wooden\\_cabinet\\_1\\ kitchen\\_table\\_wooden\\_cabinet\\_init\\_region$$\n\n$$:goal$$\n- $And\\ Open\\ wooden\\_cabinet\\_1\\_top\\_region$\n- $In\\ akita\\_black\\_bowl\\_1\\ wooden\\_cabinet\\_1\\_top\\_region$\n\n$$24$$", "images": [], "items": [{"type": "text", "value": "$$wooden\\_cabinet\\_1 - wooden\\_cabinet$$\n\n$$:oabkjietcat\\_sb\\ black\\_bowl\\_1 - akita\\_black\\_bowl$$\n\n$$plate\\_1 - plate$$\n\n$$:owbojo\\_doefn\\_\\_icnatbeirneestt\\_1$$\n\n$$akita\\_black\\_bowl\\_1$$\n\n$$:init$$\n- $On\\ akita\\_black\\_bowl\\_1\\ kitchen\\_table\\_akita\\_black\\_bowl\\_init\\_region$\n- $On\\ plate\\_1\\ kitchen\\_table\\_plate\\_init\\_region$\n\n$$On\\ wooden\\_cabinet\\_1\\ kitchen\\_table\\_wooden\\_cabinet\\_init\\_region$$\n\n$$:goal$$\n- $And\\ Open\\ wooden\\_cabinet\\_1\\_top\\_region$\n- $In\\ akita\\_black\\_bowl\\_1\\ wooden\\_cabinet\\_1\\_top\\_region$\n\n$$24$$", "md": "$$wooden\\_cabinet\\_1 - wooden\\_cabinet$$\n\n$$:oabkjietcat\\_sb\\ black\\_bowl\\_1 - akita\\_black\\_bowl$$\n\n$$plate\\_1 - plate$$\n\n$$:owbojo\\_doefn\\_\\_icnatbeirneestt\\_1$$\n\n$$akita\\_black\\_bowl\\_1$$\n\n$$:init$$\n- $On\\ akita\\_black\\_bowl\\_1\\ kitchen\\_table\\_akita\\_black\\_bowl\\_init\\_region$\n- $On\\ plate\\_1\\ kitchen\\_table\\_plate\\_init\\_region$\n\n$$On\\ wooden\\_cabinet\\_1\\ kitchen\\_table\\_wooden\\_cabinet\\_init\\_region$$\n\n$$:goal$$\n- $And\\ Open\\ wooden\\_cabinet\\_1\\_top\\_region$\n- $In\\ akita\\_black\\_bowl\\_1\\ wooden\\_cabinet\\_1\\_top\\_region$\n\n$$24$$"}]}, {"page": 25, "text": "D      Experimental Setup\nWe consider five lifelong learning algorithms: SEQL the sequential learning baseline where the\nagent learns each task in the sequence directly without any further consideration, MTL the multitask\nlearning baseline where the agent learns all tasks in the sequence simultaneously, the regularization-\nbased method EWC [33], the replay-based method ER [13], and the dynamic architecture-based\nmethod PACKNET            [41]. SEQL and MTL can be seen as approximations of the lower and upper\nbounds respectively for any lifelong learning algorithm. The other three methods represent the\nthree primary categories of lifelong learning algorithms. For the neural architectures, we consider\nthree vision-language policy architectures: RESNET-RNN, RESNET-T, VIT-T, which differ in how\nspatial or temporal information is aggregated (See Appendix A.1 for more details). For each task,\nthe agent is trained over 50 epochs on the 50 demonstration trajectories. We evaluate the agent\u2019s\naverage success rate over 20 test rollout trajectories of a maximum length of 600 every 5 epochs.\nWe use Adam optimizer [32] with a batch size of                    32, and a cosine scheduled learning rate from\n0.0001    to 0.00001     for each task. Following the convention of              Robomimic       [42], we pick the model\ncheckpoint that achieves the best success rate as the final policy for a given task. After 50 epochs\nof training, the agent with the best checkpoint is then evaluated on all previously learned tasks,\nwith 20 test rollout trajectories for each task. All policy networks are matched in Floating Point\nOperations Per Second (FLOPS): all policy architectures have                   \u223c13.5G FLOPS. For each combination\nof algorithm, policy architecture, and task suite, we run the lifelong learning method 3 times with\nrandom seeds      {100,200,300}         (180 experiments in total). See Table 4 for the implemented algorithms\nand architectures.\nE      Additional Experiment Results\nE.1     Full Results\nWe provide the full results across three different lifelong learning algorithms (e.g., EWC, ER,\nPACKNET) and three different policy architectures (e.g., RESNET-RNN, RESNET-T, VIT-T) on the\nfour task suites in Table 8.\nTo better illustrate the performance of each lifelong learning agent throughout the learning process,\nwe present plots that show how the agent\u2019s performance evolves over the stream of tasks. Firstly, we\nprovide plots that compare the performance of the agent using different lifelong learning algorithms\nwhile fixing the policy architecture (refer to Figure 12,13, and 14). Next, we provide plots that\ncompare the performance of the agent using different policy architectures while fixing the lifelong\nlearning algorithm (refer to Figure15, 16, and 17)             25", "md": "# Experimental Setup\n\n## Experimental Setup\n\nWe consider five lifelong learning algorithms: SEQL the sequential learning baseline where the agent learns each task in the sequence directly without any further consideration, MTL the multitask learning baseline where the agent learns all tasks in the sequence simultaneously, the regularization-based method EWC [33], the replay-based method ER [13], and the dynamic architecture-based method PACKNET [41]. SEQL and MTL can be seen as approximations of the lower and upper bounds respectively for any lifelong learning algorithm. The other three methods represent the three primary categories of lifelong learning algorithms. For the neural architectures, we consider three vision-language policy architectures: RESNET-RNN, RESNET-T, VIT-T, which differ in how spatial or temporal information is aggregated (See Appendix A.1 for more details). For each task, the agent is trained over 50 epochs on the 50 demonstration trajectories. We evaluate the agent\u2019s average success rate over 20 test rollout trajectories of a maximum length of 600 every 5 epochs. We use Adam optimizer [32] with a batch size of 32, and a cosine scheduled learning rate from $$0.0001$$ to $$0.00001$$ for each task. Following the convention of Robomimic [42], we pick the model checkpoint that achieves the best success rate as the final policy for a given task. After 50 epochs of training, the agent with the best checkpoint is then evaluated on all previously learned tasks, with 20 test rollout trajectories for each task. All policy networks are matched in Floating Point Operations Per Second (FLOPS): all policy architectures have $$\\sim13.5G$$ FLOPS. For each combination of algorithm, policy architecture, and task suite, we run the lifelong learning method 3 times with random seeds {100,200,300} (180 experiments in total). See Table 4 for the implemented algorithms and architectures.\n\n## Additional Experiment Results\n\n### Full Results\n\nWe provide the full results across three different lifelong learning algorithms (e.g., EWC, ER, PACKNET) and three different policy architectures (e.g., RESNET-RNN, RESNET-T, VIT-T) on the four task suites in Table 8.\n\nTo better illustrate the performance of each lifelong learning agent throughout the learning process, we present plots that show how the agent\u2019s performance evolves over the stream of tasks. Firstly, we provide plots that compare the performance of the agent using different lifelong learning algorithms while fixing the policy architecture (refer to Figure 12, 13, and 14). Next, we provide plots that compare the performance of the agent using different policy architectures while fixing the lifelong learning algorithm (refer to Figure 15, 16, and 17).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Experimental Setup", "md": "# Experimental Setup"}, {"type": "heading", "lvl": 2, "value": "Experimental Setup", "md": "## Experimental Setup"}, {"type": "text", "value": "We consider five lifelong learning algorithms: SEQL the sequential learning baseline where the agent learns each task in the sequence directly without any further consideration, MTL the multitask learning baseline where the agent learns all tasks in the sequence simultaneously, the regularization-based method EWC [33], the replay-based method ER [13], and the dynamic architecture-based method PACKNET [41]. SEQL and MTL can be seen as approximations of the lower and upper bounds respectively for any lifelong learning algorithm. The other three methods represent the three primary categories of lifelong learning algorithms. For the neural architectures, we consider three vision-language policy architectures: RESNET-RNN, RESNET-T, VIT-T, which differ in how spatial or temporal information is aggregated (See Appendix A.1 for more details). For each task, the agent is trained over 50 epochs on the 50 demonstration trajectories. We evaluate the agent\u2019s average success rate over 20 test rollout trajectories of a maximum length of 600 every 5 epochs. We use Adam optimizer [32] with a batch size of 32, and a cosine scheduled learning rate from $$0.0001$$ to $$0.00001$$ for each task. Following the convention of Robomimic [42], we pick the model checkpoint that achieves the best success rate as the final policy for a given task. After 50 epochs of training, the agent with the best checkpoint is then evaluated on all previously learned tasks, with 20 test rollout trajectories for each task. All policy networks are matched in Floating Point Operations Per Second (FLOPS): all policy architectures have $$\\sim13.5G$$ FLOPS. For each combination of algorithm, policy architecture, and task suite, we run the lifelong learning method 3 times with random seeds {100,200,300} (180 experiments in total). See Table 4 for the implemented algorithms and architectures.", "md": "We consider five lifelong learning algorithms: SEQL the sequential learning baseline where the agent learns each task in the sequence directly without any further consideration, MTL the multitask learning baseline where the agent learns all tasks in the sequence simultaneously, the regularization-based method EWC [33], the replay-based method ER [13], and the dynamic architecture-based method PACKNET [41]. SEQL and MTL can be seen as approximations of the lower and upper bounds respectively for any lifelong learning algorithm. The other three methods represent the three primary categories of lifelong learning algorithms. For the neural architectures, we consider three vision-language policy architectures: RESNET-RNN, RESNET-T, VIT-T, which differ in how spatial or temporal information is aggregated (See Appendix A.1 for more details). For each task, the agent is trained over 50 epochs on the 50 demonstration trajectories. We evaluate the agent\u2019s average success rate over 20 test rollout trajectories of a maximum length of 600 every 5 epochs. We use Adam optimizer [32] with a batch size of 32, and a cosine scheduled learning rate from $$0.0001$$ to $$0.00001$$ for each task. Following the convention of Robomimic [42], we pick the model checkpoint that achieves the best success rate as the final policy for a given task. After 50 epochs of training, the agent with the best checkpoint is then evaluated on all previously learned tasks, with 20 test rollout trajectories for each task. All policy networks are matched in Floating Point Operations Per Second (FLOPS): all policy architectures have $$\\sim13.5G$$ FLOPS. For each combination of algorithm, policy architecture, and task suite, we run the lifelong learning method 3 times with random seeds {100,200,300} (180 experiments in total). See Table 4 for the implemented algorithms and architectures."}, {"type": "heading", "lvl": 2, "value": "Additional Experiment Results", "md": "## Additional Experiment Results"}, {"type": "heading", "lvl": 3, "value": "Full Results", "md": "### Full Results"}, {"type": "text", "value": "We provide the full results across three different lifelong learning algorithms (e.g., EWC, ER, PACKNET) and three different policy architectures (e.g., RESNET-RNN, RESNET-T, VIT-T) on the four task suites in Table 8.\n\nTo better illustrate the performance of each lifelong learning agent throughout the learning process, we present plots that show how the agent\u2019s performance evolves over the stream of tasks. Firstly, we provide plots that compare the performance of the agent using different lifelong learning algorithms while fixing the policy architecture (refer to Figure 12, 13, and 14). Next, we provide plots that compare the performance of the agent using different policy architectures while fixing the lifelong learning algorithm (refer to Figure 15, 16, and 17).", "md": "We provide the full results across three different lifelong learning algorithms (e.g., EWC, ER, PACKNET) and three different policy architectures (e.g., RESNET-RNN, RESNET-T, VIT-T) on the four task suites in Table 8.\n\nTo better illustrate the performance of each lifelong learning agent throughout the learning process, we present plots that show how the agent\u2019s performance evolves over the stream of tasks. Firstly, we provide plots that compare the performance of the agent using different lifelong learning algorithms while fixing the policy architecture (refer to Figure 12, 13, and 14). Next, we provide plots that compare the performance of the agent using different policy architectures while fixing the lifelong learning algorithm (refer to Figure 15, 16, and 17)."}]}, {"page": 26, "text": "  Algo.       Policy Arch.    FWT(\u2191)       NBT(\u2193)       AUC(\u2191)      FWT(\u2191)       NBT(\u2193)      AUC(\u2191)\n                                        LIBERO-LONG                         LIBERO-SPATIAL\n              RESNET-RNN     0.24\u00b10.02    0.28\u00b1 0.01  0.07\u00b1 0.01   0.50\u00b1 0.01  0.61\u00b1 0.01   0.14\u00b1 0.01\n  SEQL        RESNET-T       0.54\u00b10.01    0.63\u00b1 0.01  0.15\u00b1 0.00   0.72\u00b1 0.01  0.81\u00b1 0.01   0.20\u00b1 0.01\n              VIT-T          0.44\u00b10.04    0.50\u00b1 0.05  0.13\u00b1 0.01   0.63\u00b1 0.02  0.76\u00b1 0.01   0.16\u00b1 0.01\n              RESNET-RNN     0.16\u00b10.02    0.16\u00b1 0.02  0.08\u00b1 0.01   0.40\u00b1 0.02  0.29\u00b1 0.02   0.29\u00b1 0.01\n  ER          RESNET-T       0.48\u00b10.02    0.32\u00b1 0.04  0.32\u00b1 0.01   0.65\u00b1 0.03  0.27\u00b1 0.03   0.56\u00b1 0.01\n              VIT-T          0.38\u00b10.05    0.29\u00b1 0.06  0.25\u00b1 0.02   0.63\u00b1 0.01  0.29\u00b1 0.02   0.50\u00b1 0.02\n              RESNET-RNN     0.02\u00b10.00    0.04\u00b1 0.01  0.00\u00b1 0.00   0.14\u00b1 0.02  0.23\u00b1 0.02   0.03\u00b1 0.00\n  EWC         RESNET-T       0.13\u00b10.02    0.22\u00b1 0.03  0.02\u00b1 0.00   0.23\u00b1 0.01  0.33\u00b1 0.01   0.06\u00b1 0.01\n              VIT-T          0.05\u00b10.02    0.09\u00b1 0.03  0.01\u00b1 0.00   0.32\u00b1 0.03  0.48\u00b1 0.03   0.06\u00b1 0.01\n              RESNET-RNN     0.13\u00b10.00    0.21\u00b1 0.01  0.03\u00b1 0.00   0.27\u00b1 0.03  0.38\u00b1 0.03   0.06\u00b1 0.01\n  PACKNET     RESNET-T       0.22\u00b10.01    0.08\u00b1 0.01  0.25\u00b1 0.00   0.55\u00b1 0.01  0.07\u00b1 0.02   0.63\u00b1 0.00\n              VIT-T          0.36\u00b10.01    0.14\u00b1 0.01  0.34\u00b1 0.01   0.57\u00b1 0.04  0.15\u00b1 0.00   0.59\u00b1 0.03\n              RESNET-RNN                              0.20\u00b1 0.01                            0.61\u00b1 0.00\n  MTL         RESNET-T                                0.48\u00b1 0.01                            0.83\u00b1 0.00\n              VIT-T                    LIBERO-OBJECT  0.46\u00b1 0.00             LIBERO-GOAL    0.79\u00b1 0.01\n              RESNET-RNN     0.48\u00b10.03    0.53\u00b1 0.04  0.15\u00b1 0.01   0.61\u00b1 0.01  0.73\u00b1 0.01   0.16\u00b1 0.00\n  SEQL        RESNET-T       0.78\u00b10.04    0.76\u00b1 0.04  0.26\u00b1 0.02   0.77\u00b1 0.01  0.82\u00b1 0.01   0.22\u00b1 0.00\n              VIT-T          0.76\u00b10.03    0.73\u00b1 0.03  0.27\u00b1 0.02   0.75\u00b1 0.01  0.85\u00b1 0.01   0.20\u00b1 0.01\n              RESNET-RNN     0.30\u00b10.01    0.27\u00b1 0.05  0.17\u00b1 0.05   0.41\u00b1 0.00  0.35\u00b1 0.01   0.26\u00b1 0.01\n  ER          RESNET-T       0.67\u00b10.07    0.43\u00b1 0.04  0.44\u00b1 0.06   0.64\u00b1 0.01  0.34\u00b1 0.02   0.49\u00b1 0.02\n              VIT-T          0.70\u00b10.02    0.28\u00b1 0.01  0.57\u00b1 0.01   0.57\u00b1 0.00  0.40\u00b1 0.02   0.38\u00b1 0.01\n              RESNET-RNN     0.17\u00b10.04    0.23\u00b1 0.04  0.06\u00b1 0.01   0.16\u00b1 0.01  0.22\u00b1 0.01   0.06\u00b1 0.01\n  EWC         RESNET-T       0.56\u00b10.03    0.69\u00b1 0.02  0.16\u00b1 0.02   0.32\u00b1 0.02  0.48\u00b1 0.03   0.06\u00b1 0.00\n              VIT-T          0.57\u00b10.03    0.64\u00b1 0.03  0.23\u00b1 0.00   0.32\u00b1 0.04  0.45\u00b1 0.04   0.07\u00b1 0.01\n              RESNET-RNN     0.29\u00b10.02    0.35\u00b1 0.02  0.13\u00b1 0.01   0.32\u00b1 0.03  0.37\u00b1 0.04   0.11\u00b1 0.01\n  PACKNET     RESNET-T       0.60\u00b10.07    0.17\u00b1 0.05  0.60\u00b1 0.05   0.63\u00b1 0.02  0.06\u00b1 0.01   0.75\u00b1 0.01\n              VIT-T          0.58\u00b10.03    0.18\u00b1 0.02  0.56\u00b1 0.04   0.69\u00b1 0.02  0.08\u00b1 0.01   0.76\u00b1 0.02\n              RESNET-RNN                              0.10\u00b1 0.03                            0.59\u00b1 0.00\n  MTL         RESNET-T                                0.54\u00b1 0.02                            0.80\u00b1 0.01\n              VIT-T                                   0.78\u00b1 0.02                            0.82\u00b1 0.01\nTable 8: We present the full results of all networks and algorithms on all four task suites. For each task\n suite, we highlight the top three AUC scores among the combinations of the three lifelong learning\n algorithms and the three neural architectures. The best three results are highlighted inmagenta   (the\n best),light magenta   (the second best), and super light magenta   (the third best), respectively.\n                                                   26", "md": "|Algo.|Policy Arch.|FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|\n|---|---|---|---|---|---|---|---|\n| |LIBERO-LONG| | | |LIBERO-SPATIAL| | |\n|RESNET-RNN| |0.24\u00b10.02|0.28\u00b10.01|0.07\u00b10.01|0.50\u00b10.01|0.61\u00b10.01|0.14\u00b10.01|\n|SEQL|RESNET-T|0.54\u00b10.01|0.63\u00b10.01|0.15\u00b10.00|0.72\u00b10.01|0.81\u00b10.01|0.20\u00b10.01|\n| |VIT-T|0.44\u00b10.04|0.50\u00b10.05|0.13\u00b10.01|0.63\u00b10.02|0.76\u00b10.01|0.16\u00b10.01|\n| |RESNET-RNN|0.16\u00b10.02|0.16\u00b10.02|0.08\u00b10.01|0.40\u00b10.02|0.29\u00b10.02|0.29\u00b10.01|\n|ER|RESNET-T|0.48\u00b10.02|0.32\u00b10.04|0.32\u00b10.01|0.65\u00b10.03|0.27\u00b10.03|0.56\u00b10.01|\n| |VIT-T|0.38\u00b10.05|0.29\u00b10.06|0.25\u00b10.02|0.63\u00b10.01|0.29\u00b10.02|0.50\u00b10.02|\n| |RESNET-RNN|0.02\u00b10.00|0.04\u00b10.01|0.00\u00b10.00|0.14\u00b10.02|0.23\u00b10.02|0.03\u00b10.00|\n|EWC|RESNET-T|0.13\u00b10.02|0.22\u00b10.03|0.02\u00b10.00|0.23\u00b10.01|0.33\u00b10.01|0.06\u00b10.01|\n| |VIT-T|0.05\u00b10.02|0.09\u00b10.03|0.01\u00b10.00|0.32\u00b10.03|0.48\u00b10.03|0.06\u00b10.01|\n| |RESNET-RNN|0.13\u00b10.00|0.21\u00b10.01|0.03\u00b10.00|0.27\u00b10.03|0.38\u00b10.03|0.06\u00b10.01|\n|PACKNET|RESNET-T|0.22\u00b10.01|0.08\u00b10.01|0.25\u00b10.00|0.55\u00b10.01|0.07\u00b10.02|0.63\u00b10.00|\n| |VIT-T|0.36\u00b10.01|0.14\u00b10.01|0.34\u00b10.01|0.57\u00b10.04|0.15\u00b10.00|0.59\u00b10.03|\n| |RESNET-RNN| | |0.20\u00b10.01| | |0.61\u00b10.00|\n|MTL|RESNET-T| | |0.48\u00b10.01| | |0.83\u00b10.00|\n| |VIT-T| |LIBERO-OBJECT|0.46\u00b10.00| |LIBERO-GOAL|0.79\u00b10.01|\n| |RESNET-RNN|0.48\u00b10.03|0.53\u00b10.04|0.15\u00b10.01|0.61\u00b10.01|0.73\u00b10.01|0.16\u00b10.00|\n|SEQL|RESNET-T|0.78\u00b10.04|0.76\u00b10.04|0.26\u00b10.02|0.77\u00b10.01|0.82\u00b10.01|0.22\u00b10.00|\n| |VIT-T|0.76\u00b10.03|0.73\u00b10.03|0.27\u00b10.02|0.75\u00b10.01|0.85\u00b10.01|0.20\u00b10.01|\n| |RESNET-RNN|0.30\u00b10.01|0.27\u00b10.05|0.17\u00b10.05|0.41\u00b10.00|0.35\u00b10.01|0.26\u00b10.01|\n|ER|RESNET-T|0.67\u00b10.07|0.43\u00b10.04|0.44\u00b10.06|0.64\u00b10.01|0.34\u00b10.02|0.49\u00b10.02|\n| |VIT-T|0.70\u00b10.02|0.28\u00b10.01|0.57\u00b10.01|0.57\u00b10.00|0.40\u00b10.02|0.38\u00b10.01|\n| |RESNET-RNN|0.17\u00b10.04|0.23\u00b10.04|0.06\u00b10.01|0.16\u00b10.01|0.22\u00b10.01|0.06\u00b10.01|\n|EWC|RESNET-T|0.56\u00b10.03|0.69\u00b10.02|0.16\u00b10.02|0.32\u00b10.02|0.48\u00b10.03|0.06\u00b10.00|\n| |VIT-T|0.57\u00b10.03|0.64\u00b10.03|0.23\u00b10.00|0.32\u00b10.04|0.45\u00b10.04|0.07\u00b10.01|\n| |RESNET-RNN|0.29\u00b10.02|0.35\u00b10.02|0.13\u00b10.01|0.32\u00b10.03|0.37\u00b10.04|0.11\u00b10.01|\n|PACKNET|RESNET-T|0.60\u00b10.07|0.17\u00b10.05|0.60\u00b10.05|0.63\u00b10.02|0.06\u00b10.01|0.75\u00b10.01|\n| |VIT-T|0.58\u00b10.03|0.18\u00b10.02|0.56\u00b10.04|0.69\u00b10.02|0.08\u00b10.01|0.76\u00b10.02|\n| |RESNET-RNN| | |0.10\u00b10.03| | |0.59\u00b10.00|\n|MTL|RESNET-T| | |0.54\u00b10.02| | |0.80\u00b10.01|\n| |VIT-T| | |0.78\u00b10.02| | |0.82\u00b10.01|", "images": [], "items": [{"type": "table", "rows": [["Algo.", "Policy Arch.", "FWT(\u2191)", "NBT(\u2193)", "AUC(\u2191)", "FWT(\u2191)", "NBT(\u2193)", "AUC(\u2191)"], ["", "LIBERO-LONG", "", "", "", "LIBERO-SPATIAL", "", ""], ["RESNET-RNN", "", "0.24\u00b10.02", "0.28\u00b10.01", "0.07\u00b10.01", "0.50\u00b10.01", "0.61\u00b10.01", "0.14\u00b10.01"], ["SEQL", "RESNET-T", "0.54\u00b10.01", "0.63\u00b10.01", "0.15\u00b10.00", "0.72\u00b10.01", "0.81\u00b10.01", "0.20\u00b10.01"], ["", "VIT-T", "0.44\u00b10.04", "0.50\u00b10.05", "0.13\u00b10.01", "0.63\u00b10.02", "0.76\u00b10.01", "0.16\u00b10.01"], ["", "RESNET-RNN", "0.16\u00b10.02", "0.16\u00b10.02", "0.08\u00b10.01", "0.40\u00b10.02", "0.29\u00b10.02", "0.29\u00b10.01"], ["ER", "RESNET-T", "0.48\u00b10.02", "0.32\u00b10.04", "0.32\u00b10.01", "0.65\u00b10.03", "0.27\u00b10.03", "0.56\u00b10.01"], ["", "VIT-T", "0.38\u00b10.05", "0.29\u00b10.06", "0.25\u00b10.02", "0.63\u00b10.01", "0.29\u00b10.02", "0.50\u00b10.02"], ["", "RESNET-RNN", "0.02\u00b10.00", "0.04\u00b10.01", "0.00\u00b10.00", "0.14\u00b10.02", "0.23\u00b10.02", "0.03\u00b10.00"], ["EWC", "RESNET-T", "0.13\u00b10.02", "0.22\u00b10.03", "0.02\u00b10.00", "0.23\u00b10.01", "0.33\u00b10.01", "0.06\u00b10.01"], ["", "VIT-T", "0.05\u00b10.02", "0.09\u00b10.03", "0.01\u00b10.00", "0.32\u00b10.03", "0.48\u00b10.03", "0.06\u00b10.01"], ["", "RESNET-RNN", "0.13\u00b10.00", "0.21\u00b10.01", "0.03\u00b10.00", "0.27\u00b10.03", "0.38\u00b10.03", "0.06\u00b10.01"], ["PACKNET", "RESNET-T", "0.22\u00b10.01", "0.08\u00b10.01", "0.25\u00b10.00", "0.55\u00b10.01", "0.07\u00b10.02", "0.63\u00b10.00"], ["", "VIT-T", "0.36\u00b10.01", "0.14\u00b10.01", "0.34\u00b10.01", "0.57\u00b10.04", "0.15\u00b10.00", "0.59\u00b10.03"], ["", "RESNET-RNN", "", "", "0.20\u00b10.01", "", "", "0.61\u00b10.00"], ["MTL", "RESNET-T", "", "", "0.48\u00b10.01", "", "", "0.83\u00b10.00"], ["", "VIT-T", "", "LIBERO-OBJECT", "0.46\u00b10.00", "", "LIBERO-GOAL", "0.79\u00b10.01"], ["", "RESNET-RNN", "0.48\u00b10.03", "0.53\u00b10.04", "0.15\u00b10.01", "0.61\u00b10.01", "0.73\u00b10.01", "0.16\u00b10.00"], ["SEQL", "RESNET-T", "0.78\u00b10.04", "0.76\u00b10.04", "0.26\u00b10.02", "0.77\u00b10.01", "0.82\u00b10.01", "0.22\u00b10.00"], ["", "VIT-T", "0.76\u00b10.03", "0.73\u00b10.03", "0.27\u00b10.02", "0.75\u00b10.01", "0.85\u00b10.01", "0.20\u00b10.01"], ["", "RESNET-RNN", "0.30\u00b10.01", "0.27\u00b10.05", "0.17\u00b10.05", "0.41\u00b10.00", "0.35\u00b10.01", "0.26\u00b10.01"], ["ER", "RESNET-T", "0.67\u00b10.07", "0.43\u00b10.04", "0.44\u00b10.06", "0.64\u00b10.01", "0.34\u00b10.02", "0.49\u00b10.02"], ["", "VIT-T", "0.70\u00b10.02", "0.28\u00b10.01", "0.57\u00b10.01", "0.57\u00b10.00", "0.40\u00b10.02", "0.38\u00b10.01"], ["", "RESNET-RNN", "0.17\u00b10.04", "0.23\u00b10.04", "0.06\u00b10.01", "0.16\u00b10.01", "0.22\u00b10.01", "0.06\u00b10.01"], ["EWC", "RESNET-T", "0.56\u00b10.03", "0.69\u00b10.02", "0.16\u00b10.02", "0.32\u00b10.02", "0.48\u00b10.03", "0.06\u00b10.00"], ["", "VIT-T", "0.57\u00b10.03", "0.64\u00b10.03", "0.23\u00b10.00", "0.32\u00b10.04", "0.45\u00b10.04", "0.07\u00b10.01"], ["", "RESNET-RNN", "0.29\u00b10.02", "0.35\u00b10.02", "0.13\u00b10.01", "0.32\u00b10.03", "0.37\u00b10.04", "0.11\u00b10.01"], ["PACKNET", "RESNET-T", "0.60\u00b10.07", "0.17\u00b10.05", "0.60\u00b10.05", "0.63\u00b10.02", "0.06\u00b10.01", "0.75\u00b10.01"], ["", "VIT-T", "0.58\u00b10.03", "0.18\u00b10.02", "0.56\u00b10.04", "0.69\u00b10.02", "0.08\u00b10.01", "0.76\u00b10.02"], ["", "RESNET-RNN", "", "", "0.10\u00b10.03", "", "", "0.59\u00b10.00"], ["MTL", "RESNET-T", "", "", "0.54\u00b10.02", "", "", "0.80\u00b10.01"], ["", "VIT-T", "", "", "0.78\u00b10.02", "", "", "0.82\u00b10.01"]], "md": "|Algo.|Policy Arch.|FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|FWT(\u2191)|NBT(\u2193)|AUC(\u2191)|\n|---|---|---|---|---|---|---|---|\n| |LIBERO-LONG| | | |LIBERO-SPATIAL| | |\n|RESNET-RNN| |0.24\u00b10.02|0.28\u00b10.01|0.07\u00b10.01|0.50\u00b10.01|0.61\u00b10.01|0.14\u00b10.01|\n|SEQL|RESNET-T|0.54\u00b10.01|0.63\u00b10.01|0.15\u00b10.00|0.72\u00b10.01|0.81\u00b10.01|0.20\u00b10.01|\n| |VIT-T|0.44\u00b10.04|0.50\u00b10.05|0.13\u00b10.01|0.63\u00b10.02|0.76\u00b10.01|0.16\u00b10.01|\n| |RESNET-RNN|0.16\u00b10.02|0.16\u00b10.02|0.08\u00b10.01|0.40\u00b10.02|0.29\u00b10.02|0.29\u00b10.01|\n|ER|RESNET-T|0.48\u00b10.02|0.32\u00b10.04|0.32\u00b10.01|0.65\u00b10.03|0.27\u00b10.03|0.56\u00b10.01|\n| |VIT-T|0.38\u00b10.05|0.29\u00b10.06|0.25\u00b10.02|0.63\u00b10.01|0.29\u00b10.02|0.50\u00b10.02|\n| |RESNET-RNN|0.02\u00b10.00|0.04\u00b10.01|0.00\u00b10.00|0.14\u00b10.02|0.23\u00b10.02|0.03\u00b10.00|\n|EWC|RESNET-T|0.13\u00b10.02|0.22\u00b10.03|0.02\u00b10.00|0.23\u00b10.01|0.33\u00b10.01|0.06\u00b10.01|\n| |VIT-T|0.05\u00b10.02|0.09\u00b10.03|0.01\u00b10.00|0.32\u00b10.03|0.48\u00b10.03|0.06\u00b10.01|\n| |RESNET-RNN|0.13\u00b10.00|0.21\u00b10.01|0.03\u00b10.00|0.27\u00b10.03|0.38\u00b10.03|0.06\u00b10.01|\n|PACKNET|RESNET-T|0.22\u00b10.01|0.08\u00b10.01|0.25\u00b10.00|0.55\u00b10.01|0.07\u00b10.02|0.63\u00b10.00|\n| |VIT-T|0.36\u00b10.01|0.14\u00b10.01|0.34\u00b10.01|0.57\u00b10.04|0.15\u00b10.00|0.59\u00b10.03|\n| |RESNET-RNN| | |0.20\u00b10.01| | |0.61\u00b10.00|\n|MTL|RESNET-T| | |0.48\u00b10.01| | |0.83\u00b10.00|\n| |VIT-T| |LIBERO-OBJECT|0.46\u00b10.00| |LIBERO-GOAL|0.79\u00b10.01|\n| |RESNET-RNN|0.48\u00b10.03|0.53\u00b10.04|0.15\u00b10.01|0.61\u00b10.01|0.73\u00b10.01|0.16\u00b10.00|\n|SEQL|RESNET-T|0.78\u00b10.04|0.76\u00b10.04|0.26\u00b10.02|0.77\u00b10.01|0.82\u00b10.01|0.22\u00b10.00|\n| |VIT-T|0.76\u00b10.03|0.73\u00b10.03|0.27\u00b10.02|0.75\u00b10.01|0.85\u00b10.01|0.20\u00b10.01|\n| |RESNET-RNN|0.30\u00b10.01|0.27\u00b10.05|0.17\u00b10.05|0.41\u00b10.00|0.35\u00b10.01|0.26\u00b10.01|\n|ER|RESNET-T|0.67\u00b10.07|0.43\u00b10.04|0.44\u00b10.06|0.64\u00b10.01|0.34\u00b10.02|0.49\u00b10.02|\n| |VIT-T|0.70\u00b10.02|0.28\u00b10.01|0.57\u00b10.01|0.57\u00b10.00|0.40\u00b10.02|0.38\u00b10.01|\n| |RESNET-RNN|0.17\u00b10.04|0.23\u00b10.04|0.06\u00b10.01|0.16\u00b10.01|0.22\u00b10.01|0.06\u00b10.01|\n|EWC|RESNET-T|0.56\u00b10.03|0.69\u00b10.02|0.16\u00b10.02|0.32\u00b10.02|0.48\u00b10.03|0.06\u00b10.00|\n| |VIT-T|0.57\u00b10.03|0.64\u00b10.03|0.23\u00b10.00|0.32\u00b10.04|0.45\u00b10.04|0.07\u00b10.01|\n| |RESNET-RNN|0.29\u00b10.02|0.35\u00b10.02|0.13\u00b10.01|0.32\u00b10.03|0.37\u00b10.04|0.11\u00b10.01|\n|PACKNET|RESNET-T|0.60\u00b10.07|0.17\u00b10.05|0.60\u00b10.05|0.63\u00b10.02|0.06\u00b10.01|0.75\u00b10.01|\n| |VIT-T|0.58\u00b10.03|0.18\u00b10.02|0.56\u00b10.04|0.69\u00b10.02|0.08\u00b10.01|0.76\u00b10.02|\n| |RESNET-RNN| | |0.10\u00b10.03| | |0.59\u00b10.00|\n|MTL|RESNET-T| | |0.54\u00b10.02| | |0.80\u00b10.01|\n| |VIT-T| | |0.78\u00b10.02| | |0.82\u00b10.01|", "isPerfectTable": true, "csv": "\"Algo.\",\"Policy Arch.\",\"FWT(\u2191)\",\"NBT(\u2193)\",\"AUC(\u2191)\",\"FWT(\u2191)\",\"NBT(\u2193)\",\"AUC(\u2191)\"\n\"\",\"LIBERO-LONG\",\"\",\"\",\"\",\"LIBERO-SPATIAL\",\"\",\"\"\n\"RESNET-RNN\",\"\",\"0.24\u00b10.02\",\"0.28\u00b10.01\",\"0.07\u00b10.01\",\"0.50\u00b10.01\",\"0.61\u00b10.01\",\"0.14\u00b10.01\"\n\"SEQL\",\"RESNET-T\",\"0.54\u00b10.01\",\"0.63\u00b10.01\",\"0.15\u00b10.00\",\"0.72\u00b10.01\",\"0.81\u00b10.01\",\"0.20\u00b10.01\"\n\"\",\"VIT-T\",\"0.44\u00b10.04\",\"0.50\u00b10.05\",\"0.13\u00b10.01\",\"0.63\u00b10.02\",\"0.76\u00b10.01\",\"0.16\u00b10.01\"\n\"\",\"RESNET-RNN\",\"0.16\u00b10.02\",\"0.16\u00b10.02\",\"0.08\u00b10.01\",\"0.40\u00b10.02\",\"0.29\u00b10.02\",\"0.29\u00b10.01\"\n\"ER\",\"RESNET-T\",\"0.48\u00b10.02\",\"0.32\u00b10.04\",\"0.32\u00b10.01\",\"0.65\u00b10.03\",\"0.27\u00b10.03\",\"0.56\u00b10.01\"\n\"\",\"VIT-T\",\"0.38\u00b10.05\",\"0.29\u00b10.06\",\"0.25\u00b10.02\",\"0.63\u00b10.01\",\"0.29\u00b10.02\",\"0.50\u00b10.02\"\n\"\",\"RESNET-RNN\",\"0.02\u00b10.00\",\"0.04\u00b10.01\",\"0.00\u00b10.00\",\"0.14\u00b10.02\",\"0.23\u00b10.02\",\"0.03\u00b10.00\"\n\"EWC\",\"RESNET-T\",\"0.13\u00b10.02\",\"0.22\u00b10.03\",\"0.02\u00b10.00\",\"0.23\u00b10.01\",\"0.33\u00b10.01\",\"0.06\u00b10.01\"\n\"\",\"VIT-T\",\"0.05\u00b10.02\",\"0.09\u00b10.03\",\"0.01\u00b10.00\",\"0.32\u00b10.03\",\"0.48\u00b10.03\",\"0.06\u00b10.01\"\n\"\",\"RESNET-RNN\",\"0.13\u00b10.00\",\"0.21\u00b10.01\",\"0.03\u00b10.00\",\"0.27\u00b10.03\",\"0.38\u00b10.03\",\"0.06\u00b10.01\"\n\"PACKNET\",\"RESNET-T\",\"0.22\u00b10.01\",\"0.08\u00b10.01\",\"0.25\u00b10.00\",\"0.55\u00b10.01\",\"0.07\u00b10.02\",\"0.63\u00b10.00\"\n\"\",\"VIT-T\",\"0.36\u00b10.01\",\"0.14\u00b10.01\",\"0.34\u00b10.01\",\"0.57\u00b10.04\",\"0.15\u00b10.00\",\"0.59\u00b10.03\"\n\"\",\"RESNET-RNN\",\"\",\"\",\"0.20\u00b10.01\",\"\",\"\",\"0.61\u00b10.00\"\n\"MTL\",\"RESNET-T\",\"\",\"\",\"0.48\u00b10.01\",\"\",\"\",\"0.83\u00b10.00\"\n\"\",\"VIT-T\",\"\",\"LIBERO-OBJECT\",\"0.46\u00b10.00\",\"\",\"LIBERO-GOAL\",\"0.79\u00b10.01\"\n\"\",\"RESNET-RNN\",\"0.48\u00b10.03\",\"0.53\u00b10.04\",\"0.15\u00b10.01\",\"0.61\u00b10.01\",\"0.73\u00b10.01\",\"0.16\u00b10.00\"\n\"SEQL\",\"RESNET-T\",\"0.78\u00b10.04\",\"0.76\u00b10.04\",\"0.26\u00b10.02\",\"0.77\u00b10.01\",\"0.82\u00b10.01\",\"0.22\u00b10.00\"\n\"\",\"VIT-T\",\"0.76\u00b10.03\",\"0.73\u00b10.03\",\"0.27\u00b10.02\",\"0.75\u00b10.01\",\"0.85\u00b10.01\",\"0.20\u00b10.01\"\n\"\",\"RESNET-RNN\",\"0.30\u00b10.01\",\"0.27\u00b10.05\",\"0.17\u00b10.05\",\"0.41\u00b10.00\",\"0.35\u00b10.01\",\"0.26\u00b10.01\"\n\"ER\",\"RESNET-T\",\"0.67\u00b10.07\",\"0.43\u00b10.04\",\"0.44\u00b10.06\",\"0.64\u00b10.01\",\"0.34\u00b10.02\",\"0.49\u00b10.02\"\n\"\",\"VIT-T\",\"0.70\u00b10.02\",\"0.28\u00b10.01\",\"0.57\u00b10.01\",\"0.57\u00b10.00\",\"0.40\u00b10.02\",\"0.38\u00b10.01\"\n\"\",\"RESNET-RNN\",\"0.17\u00b10.04\",\"0.23\u00b10.04\",\"0.06\u00b10.01\",\"0.16\u00b10.01\",\"0.22\u00b10.01\",\"0.06\u00b10.01\"\n\"EWC\",\"RESNET-T\",\"0.56\u00b10.03\",\"0.69\u00b10.02\",\"0.16\u00b10.02\",\"0.32\u00b10.02\",\"0.48\u00b10.03\",\"0.06\u00b10.00\"\n\"\",\"VIT-T\",\"0.57\u00b10.03\",\"0.64\u00b10.03\",\"0.23\u00b10.00\",\"0.32\u00b10.04\",\"0.45\u00b10.04\",\"0.07\u00b10.01\"\n\"\",\"RESNET-RNN\",\"0.29\u00b10.02\",\"0.35\u00b10.02\",\"0.13\u00b10.01\",\"0.32\u00b10.03\",\"0.37\u00b10.04\",\"0.11\u00b10.01\"\n\"PACKNET\",\"RESNET-T\",\"0.60\u00b10.07\",\"0.17\u00b10.05\",\"0.60\u00b10.05\",\"0.63\u00b10.02\",\"0.06\u00b10.01\",\"0.75\u00b10.01\"\n\"\",\"VIT-T\",\"0.58\u00b10.03\",\"0.18\u00b10.02\",\"0.56\u00b10.04\",\"0.69\u00b10.02\",\"0.08\u00b10.01\",\"0.76\u00b10.02\"\n\"\",\"RESNET-RNN\",\"\",\"\",\"0.10\u00b10.03\",\"\",\"\",\"0.59\u00b10.00\"\n\"MTL\",\"RESNET-T\",\"\",\"\",\"0.54\u00b10.02\",\"\",\"\",\"0.80\u00b10.01\"\n\"\",\"VIT-T\",\"\",\"\",\"0.78\u00b10.02\",\"\",\"\",\"0.82\u00b10.01\""}]}, {"page": 27, "text": "                            Multitask         Sequential             ER              EWC                PackNet           ResNet-RNN\n 9                                                            LIBERO-LONG\n 7                                                           LIBERO-SPATIAL\n 19\n 1                                                           LIBERO-OBJECT\n 1                                                            LIBERO-GOAL\n        Task         Task 2      Task 3       Task 4       Task 5      Task 6       Task        Task 8       Task 9      Task 10\nFigure 12: We compare the performance of different algorithms using the RESNET-RNN policy\narchitecture in Figure 12. The            y-axis represents the success rate, and the               x-axis shows the agent\u2019s\nperformance on each of the 10 tasks in a specific task suite over the course of learning. For example,\nthe upper-left plot in the figure displays the agent\u2019s performance on the first task as it learns the 10\ntasks sequentially.\n                            Multitask         Sequential             ER              EWC                PackNet             ResNet-T\n 9                                                            LIBERO-LONG\n 7                                                           LIBERO-SPATIAL\n 19\n 9                                                           LIBERO-OBJECT\n 1                                                            LIBERO-GOAL\n        Task         Task 2      Task 3       Task 4       Task 5      Task 6       Task        Task 8       Task 9      Task 10\nFigure 13: Comparison of different algorithms using the RESNET-T policy architecture. The                                    y-axis\nrepresents the success rate, while the           x-axis shows the agent\u2019s performance on each of the 10 tasks in\na given task suite during the course of learning. For example, the plot in the upper-left corner depicts\nthe agent\u2019s performance on the first task as it learns the 10 tasks sequentially.\n                                                                 27", "md": "# OCR Text\n\nMultitask         Sequential             ER              EWC                PackNet           ResNet-RNN\n\n9                                                            LIBERO-LONG\n\n7                                                           LIBERO-SPATIAL\n\n19\n\n1                                                           LIBERO-OBJECT\n\n1                                                            LIBERO-GOAL\n\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 12: We compare the performance of different algorithms using the RESNET-RNN policy\narchitecture in Figure 12. The $$y-axis$$ represents the success rate, and the $$x-axis$$ shows the agent\u2019s\nperformance on each of the 10 tasks in a specific task suite over the course of learning. For example,\nthe upper-left plot in the figure displays the agent\u2019s performance on the first task as it learns the 10\ntasks sequentially.\n\nMultitask         Sequential             ER              EWC                PackNet             ResNet-T\n\n9                                                            LIBERO-LONG\n\n7                                                           LIBERO-SPATIAL\n\n19\n\n9                                                           LIBERO-OBJECT\n\n1                                                            LIBERO-GOAL\n\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 13: Comparison of different algorithms using the RESNET-T policy architecture. The $$y-axis$$\nrepresents the success rate, while the $$x-axis$$ shows the agent\u2019s performance on each of the 10 tasks in\na given task suite during the course of learning. For example, the plot in the upper-left corner depicts\nthe agent\u2019s performance on the first task as it learns the 10 tasks sequentially.\n\n27", "images": [{"name": "img_p26_1", "height": 581, "width": 1188}, {"name": "img_p26_2", "height": 581, "width": 1188}], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "text", "value": "Multitask         Sequential             ER              EWC                PackNet           ResNet-RNN\n\n9                                                            LIBERO-LONG\n\n7                                                           LIBERO-SPATIAL\n\n19\n\n1                                                           LIBERO-OBJECT\n\n1                                                            LIBERO-GOAL\n\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 12: We compare the performance of different algorithms using the RESNET-RNN policy\narchitecture in Figure 12. The $$y-axis$$ represents the success rate, and the $$x-axis$$ shows the agent\u2019s\nperformance on each of the 10 tasks in a specific task suite over the course of learning. For example,\nthe upper-left plot in the figure displays the agent\u2019s performance on the first task as it learns the 10\ntasks sequentially.\n\nMultitask         Sequential             ER              EWC                PackNet             ResNet-T\n\n9                                                            LIBERO-LONG\n\n7                                                           LIBERO-SPATIAL\n\n19\n\n9                                                           LIBERO-OBJECT\n\n1                                                            LIBERO-GOAL\n\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 13: Comparison of different algorithms using the RESNET-T policy architecture. The $$y-axis$$\nrepresents the success rate, while the $$x-axis$$ shows the agent\u2019s performance on each of the 10 tasks in\na given task suite during the course of learning. For example, the plot in the upper-left corner depicts\nthe agent\u2019s performance on the first task as it learns the 10 tasks sequentially.\n\n27", "md": "Multitask         Sequential             ER              EWC                PackNet           ResNet-RNN\n\n9                                                            LIBERO-LONG\n\n7                                                           LIBERO-SPATIAL\n\n19\n\n1                                                           LIBERO-OBJECT\n\n1                                                            LIBERO-GOAL\n\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 12: We compare the performance of different algorithms using the RESNET-RNN policy\narchitecture in Figure 12. The $$y-axis$$ represents the success rate, and the $$x-axis$$ shows the agent\u2019s\nperformance on each of the 10 tasks in a specific task suite over the course of learning. For example,\nthe upper-left plot in the figure displays the agent\u2019s performance on the first task as it learns the 10\ntasks sequentially.\n\nMultitask         Sequential             ER              EWC                PackNet             ResNet-T\n\n9                                                            LIBERO-LONG\n\n7                                                           LIBERO-SPATIAL\n\n19\n\n9                                                           LIBERO-OBJECT\n\n1                                                            LIBERO-GOAL\n\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 13: Comparison of different algorithms using the RESNET-T policy architecture. The $$y-axis$$\nrepresents the success rate, while the $$x-axis$$ shows the agent\u2019s performance on each of the 10 tasks in\na given task suite during the course of learning. For example, the plot in the upper-left corner depicts\nthe agent\u2019s performance on the first task as it learns the 10 tasks sequentially.\n\n27"}]}, {"page": 28, "text": "                            Multitask         Sequential             ER              EWC                PackNet                 Vit-T\n 9                                                            LIBERO-LONG\n 7                                                           LIBERO-SPATIAL\n 19\n 9                                                           LIBERO-OBJECT\n 1                                                            LIBERO-GOAL\n        Task         Task 2      Task 3       Task 4       Task 5      Task 6       Task        Task 8       Task 9      Task 10\nFigure 14: Comparison of different algorithms using the VIT-T policy architecture. The success rate\nis represented on the        y-axis, while the       x-axis shows the agent\u2019s performance on the 10 tasks in a\ngiven task suite over the course of learning. For instance, the plot in the upper-left corner illustrates\nthe agent\u2019s performance on the first task when learning the 10 tasks sequentially.\n                            Multitask           ResNet-Transformer          ViT-Transformer          ResNet-RNN                 EWC\n 9                                                            LIBERO-LONG\n 1                                                           LIBERO-SPATIAL\n 9                                                           LIBERO-OBJECT\n 1                                                            LIBERO-GOAL\n 7      Task         Task 2      Task 3       Task 4       Task 5      Task 6       Task 7      Task 8       Task 9      Task 10\nFigure 15: Comparison of different architectures with the EWC algorithm. The                              y-axis is the success\nrate, while the     x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite over the\ncourse of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task\nwhen learning the 10 tasks sequentially.\n                                                                 28", "md": "| |Multitask|Sequential|ER|EWC|PackNet|Vit-T|\n|---|---|---|---|---|---|---|\n|9| | |LIBERO-LONG| | | |\n|7| | |LIBERO-SPATIAL| | | |\n|19| | | | | | |\n|9| | |LIBERO-OBJECT| | | |\n|1| | |LIBERO-GOAL| | | |\n\nTask\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 14: Comparison of different algorithms using the VIT-T policy architecture. The success rate is represented on the $$y-axis$$, while the $$x-axis$$ shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the plot in the upper-left corner illustrates the agent\u2019s performance on the first task when learning the 10 tasks sequentially.\n\n| |Multitask|ResNet-Transformer|ViT-Transformer|ResNet-RNN|EWC|\n|---|---|---|---|---|---|\n|9| |LIBERO-LONG| | | |\n|1| |LIBERO-SPATIAL| | | |\n|9| |LIBERO-OBJECT| | | |\n|1| |LIBERO-GOAL| | | |\n|7| | | | | |\n\nTask\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 15: Comparison of different architectures with the EWC algorithm. The $$y-axis$$ is the success rate, while the $$x-axis$$ shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially.\n\n28", "images": [{"name": "img_p27_1", "height": 581, "width": 1188}, {"name": "img_p27_2", "height": 583, "width": 1188}], "items": [{"type": "table", "rows": [["", "Multitask", "Sequential", "ER", "EWC", "PackNet", "Vit-T"], ["9", "", "", "LIBERO-LONG", "", "", ""], ["7", "", "", "LIBERO-SPATIAL", "", "", ""], ["19", "", "", "", "", "", ""], ["9", "", "", "LIBERO-OBJECT", "", "", ""], ["1", "", "", "LIBERO-GOAL", "", "", ""]], "md": "| |Multitask|Sequential|ER|EWC|PackNet|Vit-T|\n|---|---|---|---|---|---|---|\n|9| | |LIBERO-LONG| | | |\n|7| | |LIBERO-SPATIAL| | | |\n|19| | | | | | |\n|9| | |LIBERO-OBJECT| | | |\n|1| | |LIBERO-GOAL| | | |", "isPerfectTable": true, "csv": "\"\",\"Multitask\",\"Sequential\",\"ER\",\"EWC\",\"PackNet\",\"Vit-T\"\n\"9\",\"\",\"\",\"LIBERO-LONG\",\"\",\"\",\"\"\n\"7\",\"\",\"\",\"LIBERO-SPATIAL\",\"\",\"\",\"\"\n\"19\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"9\",\"\",\"\",\"LIBERO-OBJECT\",\"\",\"\",\"\"\n\"1\",\"\",\"\",\"LIBERO-GOAL\",\"\",\"\",\"\""}, {"type": "text", "value": "Task\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 14: Comparison of different algorithms using the VIT-T policy architecture. The success rate is represented on the $$y-axis$$, while the $$x-axis$$ shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the plot in the upper-left corner illustrates the agent\u2019s performance on the first task when learning the 10 tasks sequentially.", "md": "Task\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 14: Comparison of different algorithms using the VIT-T policy architecture. The success rate is represented on the $$y-axis$$, while the $$x-axis$$ shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the plot in the upper-left corner illustrates the agent\u2019s performance on the first task when learning the 10 tasks sequentially."}, {"type": "table", "rows": [["", "Multitask", "ResNet-Transformer", "ViT-Transformer", "ResNet-RNN", "EWC"], ["9", "", "LIBERO-LONG", "", "", ""], ["1", "", "LIBERO-SPATIAL", "", "", ""], ["9", "", "LIBERO-OBJECT", "", "", ""], ["1", "", "LIBERO-GOAL", "", "", ""], ["7", "", "", "", "", ""]], "md": "| |Multitask|ResNet-Transformer|ViT-Transformer|ResNet-RNN|EWC|\n|---|---|---|---|---|---|\n|9| |LIBERO-LONG| | | |\n|1| |LIBERO-SPATIAL| | | |\n|9| |LIBERO-OBJECT| | | |\n|1| |LIBERO-GOAL| | | |\n|7| | | | | |", "isPerfectTable": true, "csv": "\"\",\"Multitask\",\"ResNet-Transformer\",\"ViT-Transformer\",\"ResNet-RNN\",\"EWC\"\n\"9\",\"\",\"LIBERO-LONG\",\"\",\"\",\"\"\n\"1\",\"\",\"LIBERO-SPATIAL\",\"\",\"\",\"\"\n\"9\",\"\",\"LIBERO-OBJECT\",\"\",\"\",\"\"\n\"1\",\"\",\"LIBERO-GOAL\",\"\",\"\",\"\"\n\"7\",\"\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "Task\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 15: Comparison of different architectures with the EWC algorithm. The $$y-axis$$ is the success rate, while the $$x-axis$$ shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially.\n\n28", "md": "Task\nTask\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nTask 7\nTask 8\nTask 9\nTask 10\n\nFigure 15: Comparison of different architectures with the EWC algorithm. The $$y-axis$$ is the success rate, while the $$x-axis$$ shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially.\n\n28"}]}, {"page": 29, "text": "                            Multitask          ResNet-Transformer          ViT-Transformer          ResNet-RNN\n 9                                                           LIBERO-LONG\n                                                            LIBERO-SPATIAL\n 1                                                          F\n 9                                                          LIBERO-OBJECT\n 1                                                           LIBERO-GOAL\n 7      Task         Task 2      Task 3       Task 4      Task 5       Task 6      Task        Task 8      Task 9      Task 10\nFigure 16: Comparison of different architectures with the ER algorithm. The                             y-axis is the success\nrate, while the     x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite ver the\ncourse of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task\nwhen learning the 10 tasks sequentially.\n                            Multitask          ResNet-Transformer          ViT-Transformer          ResNet-RNN              PackNet\n 9                                                           LIBERO-LONG\n 1                                                          LIBERO-SPATIAL\n 9                                                          LIBERO-OBJECT\n 1                                                           LIBERO-GOAL\n 7      Task         Task 2      Task 3       Task 4      Task 5       Task 6      Task        Task 8      Task 9      Task 10\nFigure 17: Comparison of different architectures with the PACKNET                             algorithm. The       y-axis is the\nsuccess rate, while the        x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite\nover the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the\nfirst task when learning the 10 tasks sequentially.\n                                                                29", "md": "# Comparison of Different Architectures\n\n## Comparison of different architectures with the ER algorithm\n\nFigure 16: Comparison of different architectures with the ER algorithm. The y-axis is the success rate, while the x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially.\n\n| |Multitask|ResNet-Transformer|ViT-Transformer|ResNet-RNN|\n|---|---|---|---|---|\n|LIBERO-LONG|9| | | |\n|LIBERO-SPATIAL| |1| | |\n|F| | | | |\n|LIBERO-OBJECT|9| | | |\n|LIBERO-GOAL|1| | | |\n|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task 7|Task 8|Task 9|Task 10|\n\n## Comparison of different architectures with the PACKNET algorithm\n\nFigure 17: Comparison of different architectures with the PACKNET algorithm. The y-axis is the success rate, while the x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially.\n\n| |Multitask|ResNet-Transformer|ViT-Transformer|ResNet-RNN|PackNet|\n|---|---|---|---|---|---|\n|LIBERO-LONG|9| | | | |\n|LIBERO-SPATIAL| |1| | | |\n|LIBERO-OBJECT|9| | | | |\n|LIBERO-GOAL|1| | | | |\n|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task 7|Task 8|Task 9|Task 10|", "images": [{"name": "img_p28_1", "height": 583, "width": 1188}, {"name": "img_p28_2", "height": 583, "width": 1188}], "items": [{"type": "heading", "lvl": 1, "value": "Comparison of Different Architectures", "md": "# Comparison of Different Architectures"}, {"type": "heading", "lvl": 2, "value": "Comparison of different architectures with the ER algorithm", "md": "## Comparison of different architectures with the ER algorithm"}, {"type": "text", "value": "Figure 16: Comparison of different architectures with the ER algorithm. The y-axis is the success rate, while the x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially.", "md": "Figure 16: Comparison of different architectures with the ER algorithm. The y-axis is the success rate, while the x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially."}, {"type": "table", "rows": [["", "Multitask", "ResNet-Transformer", "ViT-Transformer", "ResNet-RNN"], ["LIBERO-LONG", "9", "", "", ""], ["LIBERO-SPATIAL", "", "1", "", ""], ["F", "", "", "", ""], ["LIBERO-OBJECT", "9", "", "", ""], ["LIBERO-GOAL", "1", "", "", ""], ["Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task 7", "Task 8", "Task 9", "Task 10"]], "md": "| |Multitask|ResNet-Transformer|ViT-Transformer|ResNet-RNN|\n|---|---|---|---|---|\n|LIBERO-LONG|9| | | |\n|LIBERO-SPATIAL| |1| | |\n|F| | | | |\n|LIBERO-OBJECT|9| | | |\n|LIBERO-GOAL|1| | | |\n|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task 7|Task 8|Task 9|Task 10|", "isPerfectTable": false, "csv": "\"\",\"Multitask\",\"ResNet-Transformer\",\"ViT-Transformer\",\"ResNet-RNN\"\n\"LIBERO-LONG\",\"9\",\"\",\"\",\"\"\n\"LIBERO-SPATIAL\",\"\",\"1\",\"\",\"\"\n\"F\",\"\",\"\",\"\",\"\"\n\"LIBERO-OBJECT\",\"9\",\"\",\"\",\"\"\n\"LIBERO-GOAL\",\"1\",\"\",\"\",\"\"\n\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task 7\",\"Task 8\",\"Task 9\",\"Task 10\""}, {"type": "heading", "lvl": 2, "value": "Comparison of different architectures with the PACKNET algorithm", "md": "## Comparison of different architectures with the PACKNET algorithm"}, {"type": "text", "value": "Figure 17: Comparison of different architectures with the PACKNET algorithm. The y-axis is the success rate, while the x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially.", "md": "Figure 17: Comparison of different architectures with the PACKNET algorithm. The y-axis is the success rate, while the x-axis shows the agent\u2019s performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent\u2019s performance on the first task when learning the 10 tasks sequentially."}, {"type": "table", "rows": [["", "Multitask", "ResNet-Transformer", "ViT-Transformer", "ResNet-RNN", "PackNet"], ["LIBERO-LONG", "9", "", "", "", ""], ["LIBERO-SPATIAL", "", "1", "", "", ""], ["LIBERO-OBJECT", "9", "", "", "", ""], ["LIBERO-GOAL", "1", "", "", "", ""], ["Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task 7", "Task 8", "Task 9", "Task 10"]], "md": "| |Multitask|ResNet-Transformer|ViT-Transformer|ResNet-RNN|PackNet|\n|---|---|---|---|---|---|\n|LIBERO-LONG|9| | | | |\n|LIBERO-SPATIAL| |1| | | |\n|LIBERO-OBJECT|9| | | | |\n|LIBERO-GOAL|1| | | | |\n|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task 7|Task 8|Task 9|Task 10|", "isPerfectTable": false, "csv": "\"\",\"Multitask\",\"ResNet-Transformer\",\"ViT-Transformer\",\"ResNet-RNN\",\"PackNet\"\n\"LIBERO-LONG\",\"9\",\"\",\"\",\"\",\"\"\n\"LIBERO-SPATIAL\",\"\",\"1\",\"\",\"\",\"\"\n\"LIBERO-OBJECT\",\"9\",\"\",\"\",\"\",\"\"\n\"LIBERO-GOAL\",\"1\",\"\",\"\",\"\",\"\"\n\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task 7\",\"Task 8\",\"Task 9\",\"Task 10\""}]}, {"page": 30, "text": " E.2    Loss v.s. Success Rates\nWe demonstrate that behavioral cloning loss can be a misleading indicator of task success rate\n in this section. In supervised learning tasks like image classifications, lower loss often indicates\n better prediction accuracy. However, this is not, in general, true for decision-making tasks. This is\n because errors can compound until failures during executing a robot [55]. Figure 18, 13 and 14 plots\n the training loss and success rates of three lifelong learning methods (ER, EWC, and PACKNET)\n for comparison. We evaluate the three algorithms on four task suites using three different neural\n architectures.\nFindings:     We observe that though sometimes EWC has the                        lowest   loss, it did not achieve good\n success rate. ER, on the other hand, can have the highest loss but perform better than EWC. In\n conclusion, success rates, instead of behavioral cloning loss, should be the right metric to evaluate\nwhether a model checkpoint is good or not.\n                                                               30", "md": "# Loss v.s. Success Rates\n\n## E.2 Loss v.s. Success Rates\n\nWe demonstrate that behavioral cloning loss can be a misleading indicator of task success rate in this section. In supervised learning tasks like image classifications, lower loss often indicates better prediction accuracy. However, this is not, in general, true for decision-making tasks. This is because errors can compound until failures during executing a robot [55].\n\nFigure 18, 13 and 14 plots the training loss and success rates of three lifelong learning methods (ER, EWC, and PACKNET) for comparison. We evaluate the three algorithms on four task suites using three different neural architectures.\n\n### Findings:\n\nWe observe that though sometimes EWC has the lowest loss, it did not achieve good success rate. ER, on the other hand, can have the highest loss but perform better than EWC. In conclusion, success rates, instead of behavioral cloning loss, should be the right metric to evaluate whether a model checkpoint is good or not.\n\n$$30$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Loss v.s. Success Rates", "md": "# Loss v.s. Success Rates"}, {"type": "heading", "lvl": 2, "value": "E.2 Loss v.s. Success Rates", "md": "## E.2 Loss v.s. Success Rates"}, {"type": "text", "value": "We demonstrate that behavioral cloning loss can be a misleading indicator of task success rate in this section. In supervised learning tasks like image classifications, lower loss often indicates better prediction accuracy. However, this is not, in general, true for decision-making tasks. This is because errors can compound until failures during executing a robot [55].\n\nFigure 18, 13 and 14 plots the training loss and success rates of three lifelong learning methods (ER, EWC, and PACKNET) for comparison. We evaluate the three algorithms on four task suites using three different neural architectures.", "md": "We demonstrate that behavioral cloning loss can be a misleading indicator of task success rate in this section. In supervised learning tasks like image classifications, lower loss often indicates better prediction accuracy. However, this is not, in general, true for decision-making tasks. This is because errors can compound until failures during executing a robot [55].\n\nFigure 18, 13 and 14 plots the training loss and success rates of three lifelong learning methods (ER, EWC, and PACKNET) for comparison. We evaluate the three algorithms on four task suites using three different neural architectures."}, {"type": "heading", "lvl": 3, "value": "Findings:", "md": "### Findings:"}, {"type": "text", "value": "We observe that though sometimes EWC has the lowest loss, it did not achieve good success rate. ER, on the other hand, can have the highest loss but perform better than EWC. In conclusion, success rates, instead of behavioral cloning loss, should be the right metric to evaluate whether a model checkpoint is good or not.\n\n$$30$$", "md": "We observe that though sometimes EWC has the lowest loss, it did not achieve good success rate. ER, on the other hand, can have the highest loss but perform better than EWC. In conclusion, success rates, instead of behavioral cloning loss, should be the right metric to evaluate whether a model checkpoint is good or not.\n\n$$30$$"}]}, {"page": 31, "text": "  3  ER             EWC               PackNet              LIBERO-LONG                                          ResNet-RNN\n  1\n  7      Task        Task 2      Task 3      Task 4     Task 5      Task 6      Task        Task 8   Task 9     Task 10\n  3                                                       LIBERO-SPATIAL\n  1\n  J      Task        Task 2     Task 3      Task        Task 5      Task 6      Task       Task 8   Task 9      Task 10\n  3                                                       LIBERO-OBJECT\n  9\n        Task        Task 2      Task 3      Task 4      Task 5      Task 6     Task        Task 8   Task 9     Task 10\n 3                                                        LIBERO-GOAL\n 1\n        Task        Task 2     Task 3      Task 4      Task 5      Task 6      Task        Task 8   Task 9     Task 10\nFigure 18: Losses and success rates of ER (violet), EWC (grey), and PACKNET                         (blue) on four task\nsuites with RESNET-RNN policy. The first (second) row shows the loss (success rate) of the agent\non taski   throughout the LLDM procedure.\n                                                              31", "md": "| |Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | |LIBERO-SPATIAL| | | | | |\n| |Task|Task 2|Task 3|Task|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n| | | | | |LIBERO-OBJECT| | | | | |\n| |Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n| | | | | |LIBERO-GOAL| | | | | |\n\nFigure 18: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with RESNET-RNN policy. The first (second) row shows the loss (success rate) of the agent on taski throughout the LLDM procedure.\n\n31", "images": [{"name": "img_p30_1", "height": 1252, "width": 1188}], "items": [{"type": "table", "rows": [["", "Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["", "", "", "", "", "LIBERO-SPATIAL", "", "", "", "", ""], ["", "Task", "Task 2", "Task 3", "Task", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["", "", "", "", "", "LIBERO-OBJECT", "", "", "", "", ""], ["", "Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["", "", "", "", "", "LIBERO-GOAL", "", "", "", "", ""]], "md": "| |Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | |LIBERO-SPATIAL| | | | | |\n| |Task|Task 2|Task 3|Task|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n| | | | | |LIBERO-OBJECT| | | | | |\n| |Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n| | | | | |LIBERO-GOAL| | | | | |", "isPerfectTable": true, "csv": "\"\",\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"\",\"\",\"\",\"\",\"\",\"LIBERO-SPATIAL\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"Task\",\"Task 2\",\"Task 3\",\"Task\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"\",\"\",\"\",\"\",\"\",\"LIBERO-OBJECT\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"\",\"\",\"\",\"\",\"\",\"LIBERO-GOAL\",\"\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 18: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with RESNET-RNN policy. The first (second) row shows the loss (success rate) of the agent on taski throughout the LLDM procedure.\n\n31", "md": "Figure 18: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with RESNET-RNN policy. The first (second) row shows the loss (success rate) of the agent on taski throughout the LLDM procedure.\n\n31"}]}, {"page": 32, "text": "   3 ER              EWC              PackNet               LIBERO-LONG                                              ResNet-T\n  1\n  7      Task        Task 2      Task 3      Task 4      Task 5      Task 6      Task        Task 8   Task 9      Task 10\n  3                                                        LIBERO-SPATIAL\n  1\n  J      Task        Task 2      Task 3      Task       Task 5      Task 6      Task        Task 8    Task 9     Task 10\n  3                                                       LIBERO-OBJECT\n  9\n        Task        Task 2      Task 3      Task 4      Task 5      Task 6      Task        Task 8    Task 9     Task 10\n 3                                                         LIBERO-GOAL\n 1\n        Task        Task 2      Task 3      Task 4      Task 5      Task 6      Task 7     Task 8    Task 9      Task 10\nFigure 19: Losses and success rates of ER (violet), EWC (grey), and PACKNET                          (blue) on four task\nsuites with RESNET-T policy. The first (second) row shows the loss (success rate) of the agent on\ntask  i throughout the LLDM procedure.\n                                                              32", "md": "|3|ER|EWC|PackNet|LIBERO-LONG|ResNet-T|\n|---|---|---|---|---|---|\n| | | | | |1|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|3| | |LIBERO-SPATIAL| | |\n| | | | | |1|J|Task|Task 2|Task 3|Task|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|3| | |LIBERO-OBJECT| | |\n| | | | | |9| |Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|3| | |LIBERO-GOAL| | |\n| | | | | |1| |Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task 7|Task 8|Task 9|Task 10|\n\nFigure 19: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with RESNET-T policy. The first (second) row shows the loss (success rate) of the agent on task $$i$$ throughout the LLDM procedure.", "images": [{"name": "img_p31_1", "height": 1252, "width": 1188}], "items": [{"type": "table", "rows": [["3", "ER", "EWC", "PackNet", "LIBERO-LONG", "ResNet-T"], ["", "", "", "", "", "1", "Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["3", "", "", "LIBERO-SPATIAL", "", ""], ["", "", "", "", "", "1", "J", "Task", "Task 2", "Task 3", "Task", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["3", "", "", "LIBERO-OBJECT", "", ""], ["", "", "", "", "", "9", "", "Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["3", "", "", "LIBERO-GOAL", "", ""], ["", "", "", "", "", "1", "", "Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task 7", "Task 8", "Task 9", "Task 10"]], "md": "|3|ER|EWC|PackNet|LIBERO-LONG|ResNet-T|\n|---|---|---|---|---|---|\n| | | | | |1|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|3| | |LIBERO-SPATIAL| | |\n| | | | | |1|J|Task|Task 2|Task 3|Task|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|3| | |LIBERO-OBJECT| | |\n| | | | | |9| |Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|3| | |LIBERO-GOAL| | |\n| | | | | |1| |Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task 7|Task 8|Task 9|Task 10|", "isPerfectTable": false, "csv": "\"3\",\"ER\",\"EWC\",\"PackNet\",\"LIBERO-LONG\",\"ResNet-T\"\n\"\",\"\",\"\",\"\",\"\",\"1\",\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"3\",\"\",\"\",\"LIBERO-SPATIAL\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"1\",\"J\",\"Task\",\"Task 2\",\"Task 3\",\"Task\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"3\",\"\",\"\",\"LIBERO-OBJECT\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"9\",\"\",\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"3\",\"\",\"\",\"LIBERO-GOAL\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"1\",\"\",\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task 7\",\"Task 8\",\"Task 9\",\"Task 10\""}, {"type": "text", "value": "Figure 19: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with RESNET-T policy. The first (second) row shows the loss (success rate) of the agent on task $$i$$ throughout the LLDM procedure.", "md": "Figure 19: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with RESNET-T policy. The first (second) row shows the loss (success rate) of the agent on task $$i$$ throughout the LLDM procedure."}]}, {"page": 33, "text": "  3  ER             EWC               PackNet               LIBERO-LONG                                                 Vit-T\n  1\n  7      Task        Task 2      Task 3      Task 4      Task 5      Task 6      Task        Task 8   Task 9      Task 10\n 13                                                        LIBERO-SPATIAL\n J      Task        Task 2       Task 3      Task        Task 5     Task 6       Task        Task 8   Task 9     Task 10\n 3                                                        LIBERO-OBJECT\n 9\n        Task        Task 2      Task 3      Task 4      Task 5      Task 6      Task        Task 8   Task 9      Task 10\n 9                                                         LIBERO-GOAL\n 1\n       Task         Task 2      Task 3      Task 4      Task 5      Task 6      Task 7      Task 8   Task 9     Task 10\nFigure 20: Losses and success rates of ER (violet), EWC (grey), and PACKNET                          (blue) on four task\nsuites with VIT-T policy. The first (second) row shows the loss (success rate) of the agent on task                         i\nthroughout the LLDM procedure.                                33", "md": "|3|ER|EWC|PackNet|LIBERO-LONG|Vit-T|\n|---|---|---|---|---|---|\n| | | | | |1|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|13| | |LIBERO-SPATIAL| | |\n|J|Task|Task 2|Task 3|Task|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|3| | |LIBERO-OBJECT| | |\n| | | | | |9|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|9| | |LIBERO-GOAL| | |\n| | | | | |1|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task 7|Task 8|Task 9|Task 10|\n\nFigure 20: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with VIT-T policy. The first (second) row shows the loss (success rate) of the agent on task i throughout the LLDM procedure.", "images": [{"name": "img_p32_1", "height": 1252, "width": 1188}], "items": [{"type": "table", "rows": [["3", "ER", "EWC", "PackNet", "LIBERO-LONG", "Vit-T"], ["", "", "", "", "", "1", "Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["13", "", "", "LIBERO-SPATIAL", "", ""], ["J", "Task", "Task 2", "Task 3", "Task", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["3", "", "", "LIBERO-OBJECT", "", ""], ["", "", "", "", "", "9", "Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task", "Task 8", "Task 9", "Task 10"], ["9", "", "", "LIBERO-GOAL", "", ""], ["", "", "", "", "", "1", "Task", "Task 2", "Task 3", "Task 4", "Task 5", "Task 6", "Task 7", "Task 8", "Task 9", "Task 10"]], "md": "|3|ER|EWC|PackNet|LIBERO-LONG|Vit-T|\n|---|---|---|---|---|---|\n| | | | | |1|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|13| | |LIBERO-SPATIAL| | |\n|J|Task|Task 2|Task 3|Task|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|3| | |LIBERO-OBJECT| | |\n| | | | | |9|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task|Task 8|Task 9|Task 10|\n|9| | |LIBERO-GOAL| | |\n| | | | | |1|Task|Task 2|Task 3|Task 4|Task 5|Task 6|Task 7|Task 8|Task 9|Task 10|", "isPerfectTable": false, "csv": "\"3\",\"ER\",\"EWC\",\"PackNet\",\"LIBERO-LONG\",\"Vit-T\"\n\"\",\"\",\"\",\"\",\"\",\"1\",\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"13\",\"\",\"\",\"LIBERO-SPATIAL\",\"\",\"\"\n\"J\",\"Task\",\"Task 2\",\"Task 3\",\"Task\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"3\",\"\",\"\",\"LIBERO-OBJECT\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"9\",\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task\",\"Task 8\",\"Task 9\",\"Task 10\"\n\"9\",\"\",\"\",\"LIBERO-GOAL\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"1\",\"Task\",\"Task 2\",\"Task 3\",\"Task 4\",\"Task 5\",\"Task 6\",\"Task 7\",\"Task 8\",\"Task 9\",\"Task 10\""}, {"type": "text", "value": "Figure 20: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with VIT-T policy. The first (second) row shows the loss (success rate) of the agent on task i throughout the LLDM procedure.", "md": "Figure 20: Losses and success rates of ER (violet), EWC (grey), and PACKNET (blue) on four task suites with VIT-T policy. The first (second) row shows the loss (success rate) of the agent on task i throughout the LLDM procedure."}]}, {"page": 34, "text": "E.3  Multitask Success Rate\n Env Name                                                 RESNET-RNN RESNET-T VIT-T\n KITCHEN SCENE10 close the top drawer of the cabinet         0.45      0.45    0.6\n KITCHEN SCENE10 close the top drawer of the cabinet and put the black bowl on top of it0.10.10.0\n KITCHEN SCENE10 put the black bowl in the top drawer of the cabinet0.00.25   0.05\n KITCHEN SCENE10 put the butter at the back in the top drawer of the cabinet and close it0.150.00.0\n KITCHEN SCENE10 put the butter at the front in the top drawer of the cabinet and close it0.20.00.0\n KITCHEN SCENE10 put the chocolate pudding in the top drawer of the cabinet and close it0.250.00.0\n KITCHEN SCENE1 open the bottom drawer of the cabinet        0.05       0.0   0.15\n KITCHEN SCENE1 open the top drawer of the cabinet            0.3      0.45   0.25\n KITCHEN SCENE1 open the top drawer of the cabinet and put the bowl in it0.050.00.0\n KITCHEN SCENE1 put the black bowl on the plate              0.35       0.3    0.0\n KITCHEN SCENE1 put the black bowl on top of the cabinet      0.2       0.7   0.15\n KITCHEN SCENE2 open the top drawer of the cabinet           0.45       0.4   0.65\n KITCHEN SCENE2 put the black bowl at the back on the plate   0.2      0.05   0.35\n KITCHEN SCENE2 put the black bowl at the front on the plate  0.1      0.35    0.0\n KITCHEN SCENE2 put the middle black bowl on the plate       0.35       0.1    0.0\n KITCHEN SCENE2 put the middle black bowl on top of the cabinet0.5      0.1    0.6\n KITCHEN SCENE2 stack the black bowl at the front on the black bowl in the middle0.250.050.25\n KITCHEN SCENE2 stack the middle black bowl on the back black bowl0.05 0.05    0.0\n KITCHEN SCENE3 put the frying pan on the stove               0.4       0.3   0.35\n KITCHEN SCENE3 put the moka pot on the stove                0.15       0.0    0.4\n KITCHEN SCENE3 turn on the stove                             0.6       0.7    1.0\n KITCHEN SCENE3 turn on the stove and put the frying pan on it0.15      0.0   0.35\n KITCHEN SCENE4 close the bottom drawer of the cabinet       0.75       0.4   0.55\n KITCHEN SCENE4 close the bottom drawer of the cabinet and open the top drawer0.20.00.05\n KITCHEN SCENE4 put the black bowl in the bottom drawer of the cabinet0.250.3 0.25\n KITCHEN SCENE4 put the black bowl on top of the cabinet      0.8       0.6   0.85\n KITCHEN SCENE4 put the wine bottle in the bottom drawer of the cabinet0.00.0 0.35\n KITCHEN SCENE4 put the wine bottle on the wine rack         0.05       0.0    0.2\n KITCHEN SCENE5 close the top drawer of the cabinet          0.05       0.8    0.9\n KITCHEN SCENE5 put the black bowl in the top drawer of the cabinet0.2  0.1   0.15\n KITCHEN SCENE5 put the black bowl on the plate              0.05      0.05    0.1\n KITCHEN SCENE5 put the black bowl on top of the cabinet      0.5      0.25    0.2\n KITCHEN SCENE5 put the ketchup in the top drawer of the cabinet0.0     0.0   0.05\n KITCHEN SCENE6 close the microwave                           0.1       0.1    0.2\n KITCHEN SCENE6 put the yellow and white mug to the front of the white mug0.20.050.1\n KITCHEN SCENE7 open the microwave                            0.7       0.1   0.45\n KITCHEN SCENE7 put the white bowl on the plate              0.05       0.0   0.05\n KITCHEN SCENE7 put the white bowl to the right of the plate 0.05      0.05   0.15\n KITCHEN SCENE8 put the right moka pot on the stove          0.15       0.0    0.1\n KITCHEN SCENE8 turn off the stove                            0.2      0.25   0.75\n KITCHEN SCENE9 put the frying pan on the cabinet shelf      0.45      0.15   0.05\n KITCHEN SCENE9 put the frying pan on top of the cabinet     0.25       0.4    0.3\n KITCHEN SCENE9 put the frying pan under the cabinet shelf   0.15      0.45   0.15\n KITCHEN SCENE9 put the white bowl on top of the cabinet      0.1       0.1   0.15\n KITCHEN SCENE9 turn on the stove                             0.5       0.4   0.95\n KITCHEN SCENE9 turn on the stove and put the frying pan on it0.0       0.0    0.0\n                                         34", "md": "|Env Name|RESNET-RNN|RESNET-T|VIT-T|\n|---|---|---|---|\n|KITCHEN SCENE10 close the top drawer of the cabinet|0.45|0.45|0.6|\n|KITCHEN SCENE10 close the top drawer of the cabinet and put the black bowl on top of it|0.1|0.1|0.0|\n|KITCHEN SCENE10 put the black bowl in the top drawer of the cabinet|0.0|0.25|0.05|\n|KITCHEN SCENE10 put the butter at the back in the top drawer of the cabinet and close it|0.15|0.0|0.0|\n|KITCHEN SCENE10 put the butter at the front in the top drawer of the cabinet and close it|0.2|0.0|0.0|\n|KITCHEN SCENE10 put the chocolate pudding in the top drawer of the cabinet and close it|0.25|0.0|0.0|\n|KITCHEN SCENE1 open the bottom drawer of the cabinet|0.05|0.0|0.15|\n|KITCHEN SCENE1 open the top drawer of the cabinet|0.3|0.45|0.25|\n|KITCHEN SCENE1 open the top drawer of the cabinet and put the bowl in it|0.05|0.0|0.0|\n|KITCHEN SCENE1 put the black bowl on the plate|0.35|0.3|0.0|\n|KITCHEN SCENE1 put the black bowl on top of the cabinet|0.2|0.7|0.15|\n|KITCHEN SCENE2 open the top drawer of the cabinet|0.45|0.4|0.65|\n|KITCHEN SCENE2 put the black bowl at the back on the plate|0.2|0.05|0.35|\n|KITCHEN SCENE2 put the black bowl at the front on the plate|0.1|0.35|0.0|\n|KITCHEN SCENE2 put the middle black bowl on the plate|0.35|0.1|0.0|\n|KITCHEN SCENE2 put the middle black bowl on top of the cabinet|0.5|0.1|0.6|\n|KITCHEN SCENE2 stack the black bowl at the front on the black bowl in the middle|0.25|0.05|0.25|\n|KITCHEN SCENE2 stack the middle black bowl on the back black bowl|0.05|0.05|0.0|\n|KITCHEN SCENE3 put the frying pan on the stove|0.4|0.3|0.35|\n|KITCHEN SCENE3 put the moka pot on the stove|0.15|0.0|0.4|\n|KITCHEN SCENE3 turn on the stove|0.6|0.7|1.0|\n|KITCHEN SCENE3 turn on the stove and put the frying pan on it|0.15|0.0|0.35|\n|KITCHEN SCENE4 close the bottom drawer of the cabinet|0.75|0.4|0.55|\n|KITCHEN SCENE4 close the bottom drawer of the cabinet and open the top drawer|0.2|0.0|0.05|\n|KITCHEN SCENE4 put the black bowl in the bottom drawer of the cabinet|0.25|0.3|0.25|\n|KITCHEN SCENE4 put the black bowl on top of the cabinet|0.8|0.6|0.85|\n|KITCHEN SCENE4 put the wine bottle in the bottom drawer of the cabinet|0.0|0.0|0.35|\n|KITCHEN SCENE4 put the wine bottle on the wine rack|0.05|0.0|0.2|\n|KITCHEN SCENE5 close the top drawer of the cabinet|0.05|0.8|0.9|\n|KITCHEN SCENE5 put the black bowl in the top drawer of the cabinet|0.2|0.1|0.15|\n|KITCHEN SCENE5 put the black bowl on the plate|0.05|0.05|0.1|\n|KITCHEN SCENE5 put the black bowl on top of the cabinet|0.5|0.25|0.2|\n|KITCHEN SCENE5 put the ketchup in the top drawer of the cabinet|0.0|0.0|0.05|\n|KITCHEN SCENE6 close the microwave|0.1|0.1|0.2|\n|KITCHEN SCENE6 put the yellow and white mug to the front of the white mug|0.2|0.05|0.1|\n|KITCHEN SCENE7 open the microwave|0.7|0.1|0.45|\n|KITCHEN SCENE7 put the white bowl on the plate|0.05|0.0|0.05|\n|KITCHEN SCENE7 put the white bowl to the right of the plate|0.05|0.05|0.15|\n|KITCHEN SCENE8 put the right moka pot on the stove|0.15|0.0|0.1|\n|KITCHEN SCENE8 turn off the stove|0.2|0.25|0.75|\n|KITCHEN SCENE9 put the frying pan on the cabinet shelf|0.45|0.15|0.05|\n|KITCHEN SCENE9 put the frying pan on top of the cabinet|0.25|0.4|0.3|\n|KITCHEN SCENE9 put the frying pan under the cabinet shelf|0.15|0.45|0.15|\n|KITCHEN SCENE9 put the white bowl on top of the cabinet|0.1|0.1|0.15|\n|KITCHEN SCENE9 turn on the stove|0.5|0.4|0.95|\n|KITCHEN SCENE9 turn on the stove and put the frying pan on it|0.0|0.0|0.0|", "images": [], "items": [{"type": "table", "rows": [["Env Name", "RESNET-RNN", "RESNET-T", "VIT-T"], ["KITCHEN SCENE10 close the top drawer of the cabinet", "0.45", "0.45", "0.6"], ["KITCHEN SCENE10 close the top drawer of the cabinet and put the black bowl on top of it", "0.1", "0.1", "0.0"], ["KITCHEN SCENE10 put the black bowl in the top drawer of the cabinet", "0.0", "0.25", "0.05"], ["KITCHEN SCENE10 put the butter at the back in the top drawer of the cabinet and close it", "0.15", "0.0", "0.0"], ["KITCHEN SCENE10 put the butter at the front in the top drawer of the cabinet and close it", "0.2", "0.0", "0.0"], ["KITCHEN SCENE10 put the chocolate pudding in the top drawer of the cabinet and close it", "0.25", "0.0", "0.0"], ["KITCHEN SCENE1 open the bottom drawer of the cabinet", "0.05", "0.0", "0.15"], ["KITCHEN SCENE1 open the top drawer of the cabinet", "0.3", "0.45", "0.25"], ["KITCHEN SCENE1 open the top drawer of the cabinet and put the bowl in it", "0.05", "0.0", "0.0"], ["KITCHEN SCENE1 put the black bowl on the plate", "0.35", "0.3", "0.0"], ["KITCHEN SCENE1 put the black bowl on top of the cabinet", "0.2", "0.7", "0.15"], ["KITCHEN SCENE2 open the top drawer of the cabinet", "0.45", "0.4", "0.65"], ["KITCHEN SCENE2 put the black bowl at the back on the plate", "0.2", "0.05", "0.35"], ["KITCHEN SCENE2 put the black bowl at the front on the plate", "0.1", "0.35", "0.0"], ["KITCHEN SCENE2 put the middle black bowl on the plate", "0.35", "0.1", "0.0"], ["KITCHEN SCENE2 put the middle black bowl on top of the cabinet", "0.5", "0.1", "0.6"], ["KITCHEN SCENE2 stack the black bowl at the front on the black bowl in the middle", "0.25", "0.05", "0.25"], ["KITCHEN SCENE2 stack the middle black bowl on the back black bowl", "0.05", "0.05", "0.0"], ["KITCHEN SCENE3 put the frying pan on the stove", "0.4", "0.3", "0.35"], ["KITCHEN SCENE3 put the moka pot on the stove", "0.15", "0.0", "0.4"], ["KITCHEN SCENE3 turn on the stove", "0.6", "0.7", "1.0"], ["KITCHEN SCENE3 turn on the stove and put the frying pan on it", "0.15", "0.0", "0.35"], ["KITCHEN SCENE4 close the bottom drawer of the cabinet", "0.75", "0.4", "0.55"], ["KITCHEN SCENE4 close the bottom drawer of the cabinet and open the top drawer", "0.2", "0.0", "0.05"], ["KITCHEN SCENE4 put the black bowl in the bottom drawer of the cabinet", "0.25", "0.3", "0.25"], ["KITCHEN SCENE4 put the black bowl on top of the cabinet", "0.8", "0.6", "0.85"], ["KITCHEN SCENE4 put the wine bottle in the bottom drawer of the cabinet", "0.0", "0.0", "0.35"], ["KITCHEN SCENE4 put the wine bottle on the wine rack", "0.05", "0.0", "0.2"], ["KITCHEN SCENE5 close the top drawer of the cabinet", "0.05", "0.8", "0.9"], ["KITCHEN SCENE5 put the black bowl in the top drawer of the cabinet", "0.2", "0.1", "0.15"], ["KITCHEN SCENE5 put the black bowl on the plate", "0.05", "0.05", "0.1"], ["KITCHEN SCENE5 put the black bowl on top of the cabinet", "0.5", "0.25", "0.2"], ["KITCHEN SCENE5 put the ketchup in the top drawer of the cabinet", "0.0", "0.0", "0.05"], ["KITCHEN SCENE6 close the microwave", "0.1", "0.1", "0.2"], ["KITCHEN SCENE6 put the yellow and white mug to the front of the white mug", "0.2", "0.05", "0.1"], ["KITCHEN SCENE7 open the microwave", "0.7", "0.1", "0.45"], ["KITCHEN SCENE7 put the white bowl on the plate", "0.05", "0.0", "0.05"], ["KITCHEN SCENE7 put the white bowl to the right of the plate", "0.05", "0.05", "0.15"], ["KITCHEN SCENE8 put the right moka pot on the stove", "0.15", "0.0", "0.1"], ["KITCHEN SCENE8 turn off the stove", "0.2", "0.25", "0.75"], ["KITCHEN SCENE9 put the frying pan on the cabinet shelf", "0.45", "0.15", "0.05"], ["KITCHEN SCENE9 put the frying pan on top of the cabinet", "0.25", "0.4", "0.3"], ["KITCHEN SCENE9 put the frying pan under the cabinet shelf", "0.15", "0.45", "0.15"], ["KITCHEN SCENE9 put the white bowl on top of the cabinet", "0.1", "0.1", "0.15"], ["KITCHEN SCENE9 turn on the stove", "0.5", "0.4", "0.95"], ["KITCHEN SCENE9 turn on the stove and put the frying pan on it", "0.0", "0.0", "0.0"]], "md": "|Env Name|RESNET-RNN|RESNET-T|VIT-T|\n|---|---|---|---|\n|KITCHEN SCENE10 close the top drawer of the cabinet|0.45|0.45|0.6|\n|KITCHEN SCENE10 close the top drawer of the cabinet and put the black bowl on top of it|0.1|0.1|0.0|\n|KITCHEN SCENE10 put the black bowl in the top drawer of the cabinet|0.0|0.25|0.05|\n|KITCHEN SCENE10 put the butter at the back in the top drawer of the cabinet and close it|0.15|0.0|0.0|\n|KITCHEN SCENE10 put the butter at the front in the top drawer of the cabinet and close it|0.2|0.0|0.0|\n|KITCHEN SCENE10 put the chocolate pudding in the top drawer of the cabinet and close it|0.25|0.0|0.0|\n|KITCHEN SCENE1 open the bottom drawer of the cabinet|0.05|0.0|0.15|\n|KITCHEN SCENE1 open the top drawer of the cabinet|0.3|0.45|0.25|\n|KITCHEN SCENE1 open the top drawer of the cabinet and put the bowl in it|0.05|0.0|0.0|\n|KITCHEN SCENE1 put the black bowl on the plate|0.35|0.3|0.0|\n|KITCHEN SCENE1 put the black bowl on top of the cabinet|0.2|0.7|0.15|\n|KITCHEN SCENE2 open the top drawer of the cabinet|0.45|0.4|0.65|\n|KITCHEN SCENE2 put the black bowl at the back on the plate|0.2|0.05|0.35|\n|KITCHEN SCENE2 put the black bowl at the front on the plate|0.1|0.35|0.0|\n|KITCHEN SCENE2 put the middle black bowl on the plate|0.35|0.1|0.0|\n|KITCHEN SCENE2 put the middle black bowl on top of the cabinet|0.5|0.1|0.6|\n|KITCHEN SCENE2 stack the black bowl at the front on the black bowl in the middle|0.25|0.05|0.25|\n|KITCHEN SCENE2 stack the middle black bowl on the back black bowl|0.05|0.05|0.0|\n|KITCHEN SCENE3 put the frying pan on the stove|0.4|0.3|0.35|\n|KITCHEN SCENE3 put the moka pot on the stove|0.15|0.0|0.4|\n|KITCHEN SCENE3 turn on the stove|0.6|0.7|1.0|\n|KITCHEN SCENE3 turn on the stove and put the frying pan on it|0.15|0.0|0.35|\n|KITCHEN SCENE4 close the bottom drawer of the cabinet|0.75|0.4|0.55|\n|KITCHEN SCENE4 close the bottom drawer of the cabinet and open the top drawer|0.2|0.0|0.05|\n|KITCHEN SCENE4 put the black bowl in the bottom drawer of the cabinet|0.25|0.3|0.25|\n|KITCHEN SCENE4 put the black bowl on top of the cabinet|0.8|0.6|0.85|\n|KITCHEN SCENE4 put the wine bottle in the bottom drawer of the cabinet|0.0|0.0|0.35|\n|KITCHEN SCENE4 put the wine bottle on the wine rack|0.05|0.0|0.2|\n|KITCHEN SCENE5 close the top drawer of the cabinet|0.05|0.8|0.9|\n|KITCHEN SCENE5 put the black bowl in the top drawer of the cabinet|0.2|0.1|0.15|\n|KITCHEN SCENE5 put the black bowl on the plate|0.05|0.05|0.1|\n|KITCHEN SCENE5 put the black bowl on top of the cabinet|0.5|0.25|0.2|\n|KITCHEN SCENE5 put the ketchup in the top drawer of the cabinet|0.0|0.0|0.05|\n|KITCHEN SCENE6 close the microwave|0.1|0.1|0.2|\n|KITCHEN SCENE6 put the yellow and white mug to the front of the white mug|0.2|0.05|0.1|\n|KITCHEN SCENE7 open the microwave|0.7|0.1|0.45|\n|KITCHEN SCENE7 put the white bowl on the plate|0.05|0.0|0.05|\n|KITCHEN SCENE7 put the white bowl to the right of the plate|0.05|0.05|0.15|\n|KITCHEN SCENE8 put the right moka pot on the stove|0.15|0.0|0.1|\n|KITCHEN SCENE8 turn off the stove|0.2|0.25|0.75|\n|KITCHEN SCENE9 put the frying pan on the cabinet shelf|0.45|0.15|0.05|\n|KITCHEN SCENE9 put the frying pan on top of the cabinet|0.25|0.4|0.3|\n|KITCHEN SCENE9 put the frying pan under the cabinet shelf|0.15|0.45|0.15|\n|KITCHEN SCENE9 put the white bowl on top of the cabinet|0.1|0.1|0.15|\n|KITCHEN SCENE9 turn on the stove|0.5|0.4|0.95|\n|KITCHEN SCENE9 turn on the stove and put the frying pan on it|0.0|0.0|0.0|", "isPerfectTable": true, "csv": "\"Env Name\",\"RESNET-RNN\",\"RESNET-T\",\"VIT-T\"\n\"KITCHEN SCENE10 close the top drawer of the cabinet\",\"0.45\",\"0.45\",\"0.6\"\n\"KITCHEN SCENE10 close the top drawer of the cabinet and put the black bowl on top of it\",\"0.1\",\"0.1\",\"0.0\"\n\"KITCHEN SCENE10 put the black bowl in the top drawer of the cabinet\",\"0.0\",\"0.25\",\"0.05\"\n\"KITCHEN SCENE10 put the butter at the back in the top drawer of the cabinet and close it\",\"0.15\",\"0.0\",\"0.0\"\n\"KITCHEN SCENE10 put the butter at the front in the top drawer of the cabinet and close it\",\"0.2\",\"0.0\",\"0.0\"\n\"KITCHEN SCENE10 put the chocolate pudding in the top drawer of the cabinet and close it\",\"0.25\",\"0.0\",\"0.0\"\n\"KITCHEN SCENE1 open the bottom drawer of the cabinet\",\"0.05\",\"0.0\",\"0.15\"\n\"KITCHEN SCENE1 open the top drawer of the cabinet\",\"0.3\",\"0.45\",\"0.25\"\n\"KITCHEN SCENE1 open the top drawer of the cabinet and put the bowl in it\",\"0.05\",\"0.0\",\"0.0\"\n\"KITCHEN SCENE1 put the black bowl on the plate\",\"0.35\",\"0.3\",\"0.0\"\n\"KITCHEN SCENE1 put the black bowl on top of the cabinet\",\"0.2\",\"0.7\",\"0.15\"\n\"KITCHEN SCENE2 open the top drawer of the cabinet\",\"0.45\",\"0.4\",\"0.65\"\n\"KITCHEN SCENE2 put the black bowl at the back on the plate\",\"0.2\",\"0.05\",\"0.35\"\n\"KITCHEN SCENE2 put the black bowl at the front on the plate\",\"0.1\",\"0.35\",\"0.0\"\n\"KITCHEN SCENE2 put the middle black bowl on the plate\",\"0.35\",\"0.1\",\"0.0\"\n\"KITCHEN SCENE2 put the middle black bowl on top of the cabinet\",\"0.5\",\"0.1\",\"0.6\"\n\"KITCHEN SCENE2 stack the black bowl at the front on the black bowl in the middle\",\"0.25\",\"0.05\",\"0.25\"\n\"KITCHEN SCENE2 stack the middle black bowl on the back black bowl\",\"0.05\",\"0.05\",\"0.0\"\n\"KITCHEN SCENE3 put the frying pan on the stove\",\"0.4\",\"0.3\",\"0.35\"\n\"KITCHEN SCENE3 put the moka pot on the stove\",\"0.15\",\"0.0\",\"0.4\"\n\"KITCHEN SCENE3 turn on the stove\",\"0.6\",\"0.7\",\"1.0\"\n\"KITCHEN SCENE3 turn on the stove and put the frying pan on it\",\"0.15\",\"0.0\",\"0.35\"\n\"KITCHEN SCENE4 close the bottom drawer of the cabinet\",\"0.75\",\"0.4\",\"0.55\"\n\"KITCHEN SCENE4 close the bottom drawer of the cabinet and open the top drawer\",\"0.2\",\"0.0\",\"0.05\"\n\"KITCHEN SCENE4 put the black bowl in the bottom drawer of the cabinet\",\"0.25\",\"0.3\",\"0.25\"\n\"KITCHEN SCENE4 put the black bowl on top of the cabinet\",\"0.8\",\"0.6\",\"0.85\"\n\"KITCHEN SCENE4 put the wine bottle in the bottom drawer of the cabinet\",\"0.0\",\"0.0\",\"0.35\"\n\"KITCHEN SCENE4 put the wine bottle on the wine rack\",\"0.05\",\"0.0\",\"0.2\"\n\"KITCHEN SCENE5 close the top drawer of the cabinet\",\"0.05\",\"0.8\",\"0.9\"\n\"KITCHEN SCENE5 put the black bowl in the top drawer of the cabinet\",\"0.2\",\"0.1\",\"0.15\"\n\"KITCHEN SCENE5 put the black bowl on the plate\",\"0.05\",\"0.05\",\"0.1\"\n\"KITCHEN SCENE5 put the black bowl on top of the cabinet\",\"0.5\",\"0.25\",\"0.2\"\n\"KITCHEN SCENE5 put the ketchup in the top drawer of the cabinet\",\"0.0\",\"0.0\",\"0.05\"\n\"KITCHEN SCENE6 close the microwave\",\"0.1\",\"0.1\",\"0.2\"\n\"KITCHEN SCENE6 put the yellow and white mug to the front of the white mug\",\"0.2\",\"0.05\",\"0.1\"\n\"KITCHEN SCENE7 open the microwave\",\"0.7\",\"0.1\",\"0.45\"\n\"KITCHEN SCENE7 put the white bowl on the plate\",\"0.05\",\"0.0\",\"0.05\"\n\"KITCHEN SCENE7 put the white bowl to the right of the plate\",\"0.05\",\"0.05\",\"0.15\"\n\"KITCHEN SCENE8 put the right moka pot on the stove\",\"0.15\",\"0.0\",\"0.1\"\n\"KITCHEN SCENE8 turn off the stove\",\"0.2\",\"0.25\",\"0.75\"\n\"KITCHEN SCENE9 put the frying pan on the cabinet shelf\",\"0.45\",\"0.15\",\"0.05\"\n\"KITCHEN SCENE9 put the frying pan on top of the cabinet\",\"0.25\",\"0.4\",\"0.3\"\n\"KITCHEN SCENE9 put the frying pan under the cabinet shelf\",\"0.15\",\"0.45\",\"0.15\"\n\"KITCHEN SCENE9 put the white bowl on top of the cabinet\",\"0.1\",\"0.1\",\"0.15\"\n\"KITCHEN SCENE9 turn on the stove\",\"0.5\",\"0.4\",\"0.95\"\n\"KITCHEN SCENE9 turn on the stove and put the frying pan on it\",\"0.0\",\"0.0\",\"0.0\""}]}, {"page": 35, "text": "Env Name                                                            RESNET-RNN    RESNET-T  VIT-T\nLIVING ROOM SCENE1 pick up the alphabet soup and put it in the basket    0.0         0.0     0.0\nLIVING ROOM SCENE1 pick up the cream cheese box and put it in the basket 0.0         0.0     0.0\nLIVING ROOM SCENE1 pick up the ketchup and put it in the basket          0.0         0.0     0.0\nLIVING ROOM SCENE1 pick up the tomato sauce and put it in the basket     0.0         0.0     0.0\nLIVING ROOM SCENE2 pick up the alphabet soup and put it in the basket    0.0         0.0     0.0\nLIVING ROOM SCENE2 pick up the butter and put it in the basket           0.0         0.0     0.0\nLIVING ROOM SCENE2 pick up the milk and put it in the basket             0.0         0.05    0.0\nLIVING ROOM SCENE2 pick up the orange juice and put it in the basket     0.0         0.0     0.0\nLIVING ROOM SCENE2 pick up the tomato sauce and put it in the basket     0.0         0.05    0.0\nLIVING ROOM SCENE3 pick up the alphabet soup and put it in the tray      0.0         0.05    0.0\nLIVING ROOM SCENE3 pick up the butter and put it in the tray             0.0         0.3     0.0\nLIVING ROOM SCENE3 pick up the cream cheese and put it in the tray       0.0         0.25    0.0\nLIVING ROOM SCENE3 pick up the ketchup and put it in the tray            0.0         0.0     0.0\nLIVING ROOM SCENE3 pick up the tomato sauce and put it in the tray       0.0         0.25    0.0\nLIVING ROOM SCENE4 pick up the black bowl on the left and put it in the tray0.0      0.4     0.2\nLIVING ROOM SCENE4 pick up the chocolate pudding and put it in the tray  0.0         0.2     0.25\nLIVING ROOM SCENE4 pick up the salad dressing and put it in the tray     0.0         0.0     0.1\nLIVING ROOM SCENE4 stack the left bowl on the right bowl and place them in the tray0.00.0    0.0\nLIVING ROOM SCENE4 stack the right bowl on the left bowl and place them in the tray0.00.0    0.05\nLIVING ROOM SCENE5 put the red mug on the left plate                     0.0         0.0     0.05\nLIVING ROOM SCENE5 put the red mug on the right plate                    0.15        0.0     0.0\nLIVING ROOM SCENE5 put the white mug on the left plate                   0.1         0.15    0.05\nLIVING ROOM SCENE5 put the yellow and white mug on the right plate       0.35        0.05    0.05\nLIVING ROOM SCENE6 put the chocolate pudding to the left of the plate    0.1         0.65    0.0\nLIVING ROOM SCENE6 put the chocolate pudding to the right of the plate   0.05        0.55    0.0\nLIVING ROOM SCENE6 put the red mug on the plate                          0.0         0.2     0.0\nLIVING ROOM SCENE6 put the white mug on the plate                        0.0         0.2     0.0\nSTUDY SCENE1 pick up the book and place it in the front compartment of the caddy0.0  0.0     0.05\nSTUDY SCENE1 pick up the book and place it in the left compartment of the caddy0.2   0.05    0.0\nSTUDY SCENE1 pick up the book and place it in the right compartment of the caddy0.0  0.1     0.0\nSTUDY SCENE1 pick up the yellow and white mug and place it to the right of the caddy0.00.3   0.35\nSTUDY SCENE2 pick up the book and place it in the back compartment of the caddy0.35  0.3     0.0\nSTUDY SCENE2 pick up the book and place it in the front compartment of the caddy0.25 0.1     0.0\nSTUDY SCENE2 pick up the book and place it in the left compartment of the caddy0.25  0.45    0.05\nSTUDY SCENE2 pick up the book and place it in the right compartment of the caddy0.0  0.2     0.0\nSTUDY SCENE3 pick up the book and place it in the front compartment of the caddy0.2  0.0     0.0\nSTUDY SCENE3 pick up the book and place it in the left compartment of the caddy0.4   0.45    0.15\nSTUDY SCENE3 pick up the book and place it in the right compartment of the caddy0.0  0.05    0.05\nSTUDY SCENE3 pick up the red mug and place it to the right of the caddy  0.0         0.2     0.05\nSTUDY SCENE3 pick up the white mug and place it to the right of the caddy0.0         0.0     0.05\nSTUDY SCENE4 pick up the book in the middle and place it on the cabinet shelf0.05    0.2     0.05\nSTUDY SCENE4 pick up the book on the left and place it on top of the shelf0.2        0.1     0.15\nSTUDY SCENE4 pick up the book on the right and place it on the cabinet shelf0.0      0.25    0.05\nSTUDY SCENE4 pick up the book on the right and place it under the cabinet shelf0.15  0.1     0.05\n                                               35", "md": "|Env Name|RESNET-RNN|RESNET-T|VIT-T|\n|---|---|---|---|\n|LIVING ROOM SCENE1 pick up the alphabet soup and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE1 pick up the cream cheese box and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE1 pick up the ketchup and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE1 pick up the tomato sauce and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE2 pick up the alphabet soup and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE2 pick up the butter and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE2 pick up the milk and put it in the basket|$0.0$|$0.05$|$0.0$|\n|LIVING ROOM SCENE2 pick up the orange juice and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE2 pick up the tomato sauce and put it in the basket|$0.0$|$0.05$|$0.0$|\n|LIVING ROOM SCENE3 pick up the alphabet soup and put it in the tray|$0.0$|$0.05$|$0.0$|\n|LIVING ROOM SCENE3 pick up the butter and put it in the tray|$0.0$|$0.3$|$0.0$|\n|LIVING ROOM SCENE3 pick up the cream cheese and put it in the tray|$0.0$|$0.25$|$0.0$|\n|LIVING ROOM SCENE3 pick up the ketchup and put it in the tray|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE3 pick up the tomato sauce and put it in the tray|$0.0$|$0.25$|$0.0$|\n|LIVING ROOM SCENE4 pick up the black bowl on the left and put it in the tray|$0.0$|$0.4$|$0.2$|\n|LIVING ROOM SCENE4 pick up the chocolate pudding and put it in the tray|$0.0$|$0.2$|$0.25$|\n|LIVING ROOM SCENE4 pick up the salad dressing and put it in the tray|$0.0$|$0.0$|$0.1$|\n|LIVING ROOM SCENE4 stack the left bowl on the right bowl and place them in the tray|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE4 stack the right bowl on the left bowl and place them in the tray|$0.0$|$0.0$|$0.05$|\n|LIVING ROOM SCENE5 put the red mug on the left plate|$0.0$|$0.0$|$0.05$|\n|LIVING ROOM SCENE5 put the red mug on the right plate|$0.15$|$0.0$|$0.0$|\n|LIVING ROOM SCENE5 put the white mug on the left plate|$0.1$|$0.15$|$0.05$|\n|LIVING ROOM SCENE5 put the yellow and white mug on the right plate|$0.35$|$0.05$|$0.05$|\n|LIVING ROOM SCENE6 put the chocolate pudding to the left of the plate|$0.1$|$0.65$|$0.0$|\n|LIVING ROOM SCENE6 put the chocolate pudding to the right of the plate|$0.05$|$0.55$|$0.0$|\n|LIVING ROOM SCENE6 put the red mug on the plate|$0.0$|$0.2$|$0.0$|\n|LIVING ROOM SCENE6 put the white mug on the plate|$0.0$|$0.2$|$0.0$|\n|STUDY SCENE1 pick up the book and place it in the front compartment of the caddy|$0.0$|$0.0$|$0.05$|\n|STUDY SCENE1 pick up the book and place it in the left compartment of the caddy|$0.2$|$0.05$|$0.0$|\n|STUDY SCENE1 pick up the book and place it in the right compartment of the caddy|$0.0$|$0.1$|$0.0$|\n|STUDY SCENE1 pick up the yellow and white mug and place it to the right of the caddy|$0.0$|$0.3$|$0.35$|\n|STUDY SCENE2 pick up the book and place it in the back compartment of the caddy|$0.35$|$0.3$|$0.0$|\n|STUDY SCENE2 pick up the book and place it in the front compartment of the caddy|$0.25$|$0.1$|$0.0$|\n|STUDY SCENE2 pick up the book and place it in the left compartment of the caddy|$0.25$|$0.45$|$0.05$|\n|STUDY SCENE2 pick up the book and place it in the right compartment of the caddy|$0.0$|$0.2$|$0.0$|\n|STUDY SCENE3 pick up the book and place it in the front compartment of the caddy|$0.2$|$0.0$|$0.0$|\n|STUDY SCENE3 pick up the book and place it in the left compartment of the caddy|$0.4$|$0.45$|$0.15$|\n|STUDY SCENE3 pick up the book and place it in the right compartment of the caddy|$0.0$|$0.05$|$0.05$|\n|STUDY SCENE3 pick up the red mug and place it to the right of the caddy|$0.0$|$0.2$|$0.05$|\n|STUDY SCENE3 pick up the white mug and place it to the right of the caddy|$0.0$|$0.0$|$0.05$|\n|STUDY SCENE4 pick up the book in the middle and place it on the cabinet shelf|$0.05$|$0.2$|$0.05$|\n|STUDY SCENE4 pick up the book on the left and place it on top of the shelf|$0.2$|$0.1$|$0.15$|\n|STUDY SCENE4 pick up the book on the right and place it on the cabinet shelf|$0.0$|$0.25$|$0.05$|\n|STUDY SCENE4 pick up the book on the right and place it under the cabinet shelf|$0.15$|$0.1$|$0.05$|", "images": [], "items": [{"type": "table", "rows": [["Env Name", "RESNET-RNN", "RESNET-T", "VIT-T"], ["LIVING ROOM SCENE1 pick up the alphabet soup and put it in the basket", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE1 pick up the cream cheese box and put it in the basket", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE1 pick up the ketchup and put it in the basket", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE1 pick up the tomato sauce and put it in the basket", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE2 pick up the alphabet soup and put it in the basket", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE2 pick up the butter and put it in the basket", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE2 pick up the milk and put it in the basket", "$0.0$", "$0.05$", "$0.0$"], ["LIVING ROOM SCENE2 pick up the orange juice and put it in the basket", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE2 pick up the tomato sauce and put it in the basket", "$0.0$", "$0.05$", "$0.0$"], ["LIVING ROOM SCENE3 pick up the alphabet soup and put it in the tray", "$0.0$", "$0.05$", "$0.0$"], ["LIVING ROOM SCENE3 pick up the butter and put it in the tray", "$0.0$", "$0.3$", "$0.0$"], ["LIVING ROOM SCENE3 pick up the cream cheese and put it in the tray", "$0.0$", "$0.25$", "$0.0$"], ["LIVING ROOM SCENE3 pick up the ketchup and put it in the tray", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE3 pick up the tomato sauce and put it in the tray", "$0.0$", "$0.25$", "$0.0$"], ["LIVING ROOM SCENE4 pick up the black bowl on the left and put it in the tray", "$0.0$", "$0.4$", "$0.2$"], ["LIVING ROOM SCENE4 pick up the chocolate pudding and put it in the tray", "$0.0$", "$0.2$", "$0.25$"], ["LIVING ROOM SCENE4 pick up the salad dressing and put it in the tray", "$0.0$", "$0.0$", "$0.1$"], ["LIVING ROOM SCENE4 stack the left bowl on the right bowl and place them in the tray", "$0.0$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE4 stack the right bowl on the left bowl and place them in the tray", "$0.0$", "$0.0$", "$0.05$"], ["LIVING ROOM SCENE5 put the red mug on the left plate", "$0.0$", "$0.0$", "$0.05$"], ["LIVING ROOM SCENE5 put the red mug on the right plate", "$0.15$", "$0.0$", "$0.0$"], ["LIVING ROOM SCENE5 put the white mug on the left plate", "$0.1$", "$0.15$", "$0.05$"], ["LIVING ROOM SCENE5 put the yellow and white mug on the right plate", "$0.35$", "$0.05$", "$0.05$"], ["LIVING ROOM SCENE6 put the chocolate pudding to the left of the plate", "$0.1$", "$0.65$", "$0.0$"], ["LIVING ROOM SCENE6 put the chocolate pudding to the right of the plate", "$0.05$", "$0.55$", "$0.0$"], ["LIVING ROOM SCENE6 put the red mug on the plate", "$0.0$", "$0.2$", "$0.0$"], ["LIVING ROOM SCENE6 put the white mug on the plate", "$0.0$", "$0.2$", "$0.0$"], ["STUDY SCENE1 pick up the book and place it in the front compartment of the caddy", "$0.0$", "$0.0$", "$0.05$"], ["STUDY SCENE1 pick up the book and place it in the left compartment of the caddy", "$0.2$", "$0.05$", "$0.0$"], ["STUDY SCENE1 pick up the book and place it in the right compartment of the caddy", "$0.0$", "$0.1$", "$0.0$"], ["STUDY SCENE1 pick up the yellow and white mug and place it to the right of the caddy", "$0.0$", "$0.3$", "$0.35$"], ["STUDY SCENE2 pick up the book and place it in the back compartment of the caddy", "$0.35$", "$0.3$", "$0.0$"], ["STUDY SCENE2 pick up the book and place it in the front compartment of the caddy", "$0.25$", "$0.1$", "$0.0$"], ["STUDY SCENE2 pick up the book and place it in the left compartment of the caddy", "$0.25$", "$0.45$", "$0.05$"], ["STUDY SCENE2 pick up the book and place it in the right compartment of the caddy", "$0.0$", "$0.2$", "$0.0$"], ["STUDY SCENE3 pick up the book and place it in the front compartment of the caddy", "$0.2$", "$0.0$", "$0.0$"], ["STUDY SCENE3 pick up the book and place it in the left compartment of the caddy", "$0.4$", "$0.45$", "$0.15$"], ["STUDY SCENE3 pick up the book and place it in the right compartment of the caddy", "$0.0$", "$0.05$", "$0.05$"], ["STUDY SCENE3 pick up the red mug and place it to the right of the caddy", "$0.0$", "$0.2$", "$0.05$"], ["STUDY SCENE3 pick up the white mug and place it to the right of the caddy", "$0.0$", "$0.0$", "$0.05$"], ["STUDY SCENE4 pick up the book in the middle and place it on the cabinet shelf", "$0.05$", "$0.2$", "$0.05$"], ["STUDY SCENE4 pick up the book on the left and place it on top of the shelf", "$0.2$", "$0.1$", "$0.15$"], ["STUDY SCENE4 pick up the book on the right and place it on the cabinet shelf", "$0.0$", "$0.25$", "$0.05$"], ["STUDY SCENE4 pick up the book on the right and place it under the cabinet shelf", "$0.15$", "$0.1$", "$0.05$"]], "md": "|Env Name|RESNET-RNN|RESNET-T|VIT-T|\n|---|---|---|---|\n|LIVING ROOM SCENE1 pick up the alphabet soup and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE1 pick up the cream cheese box and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE1 pick up the ketchup and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE1 pick up the tomato sauce and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE2 pick up the alphabet soup and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE2 pick up the butter and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE2 pick up the milk and put it in the basket|$0.0$|$0.05$|$0.0$|\n|LIVING ROOM SCENE2 pick up the orange juice and put it in the basket|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE2 pick up the tomato sauce and put it in the basket|$0.0$|$0.05$|$0.0$|\n|LIVING ROOM SCENE3 pick up the alphabet soup and put it in the tray|$0.0$|$0.05$|$0.0$|\n|LIVING ROOM SCENE3 pick up the butter and put it in the tray|$0.0$|$0.3$|$0.0$|\n|LIVING ROOM SCENE3 pick up the cream cheese and put it in the tray|$0.0$|$0.25$|$0.0$|\n|LIVING ROOM SCENE3 pick up the ketchup and put it in the tray|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE3 pick up the tomato sauce and put it in the tray|$0.0$|$0.25$|$0.0$|\n|LIVING ROOM SCENE4 pick up the black bowl on the left and put it in the tray|$0.0$|$0.4$|$0.2$|\n|LIVING ROOM SCENE4 pick up the chocolate pudding and put it in the tray|$0.0$|$0.2$|$0.25$|\n|LIVING ROOM SCENE4 pick up the salad dressing and put it in the tray|$0.0$|$0.0$|$0.1$|\n|LIVING ROOM SCENE4 stack the left bowl on the right bowl and place them in the tray|$0.0$|$0.0$|$0.0$|\n|LIVING ROOM SCENE4 stack the right bowl on the left bowl and place them in the tray|$0.0$|$0.0$|$0.05$|\n|LIVING ROOM SCENE5 put the red mug on the left plate|$0.0$|$0.0$|$0.05$|\n|LIVING ROOM SCENE5 put the red mug on the right plate|$0.15$|$0.0$|$0.0$|\n|LIVING ROOM SCENE5 put the white mug on the left plate|$0.1$|$0.15$|$0.05$|\n|LIVING ROOM SCENE5 put the yellow and white mug on the right plate|$0.35$|$0.05$|$0.05$|\n|LIVING ROOM SCENE6 put the chocolate pudding to the left of the plate|$0.1$|$0.65$|$0.0$|\n|LIVING ROOM SCENE6 put the chocolate pudding to the right of the plate|$0.05$|$0.55$|$0.0$|\n|LIVING ROOM SCENE6 put the red mug on the plate|$0.0$|$0.2$|$0.0$|\n|LIVING ROOM SCENE6 put the white mug on the plate|$0.0$|$0.2$|$0.0$|\n|STUDY SCENE1 pick up the book and place it in the front compartment of the caddy|$0.0$|$0.0$|$0.05$|\n|STUDY SCENE1 pick up the book and place it in the left compartment of the caddy|$0.2$|$0.05$|$0.0$|\n|STUDY SCENE1 pick up the book and place it in the right compartment of the caddy|$0.0$|$0.1$|$0.0$|\n|STUDY SCENE1 pick up the yellow and white mug and place it to the right of the caddy|$0.0$|$0.3$|$0.35$|\n|STUDY SCENE2 pick up the book and place it in the back compartment of the caddy|$0.35$|$0.3$|$0.0$|\n|STUDY SCENE2 pick up the book and place it in the front compartment of the caddy|$0.25$|$0.1$|$0.0$|\n|STUDY SCENE2 pick up the book and place it in the left compartment of the caddy|$0.25$|$0.45$|$0.05$|\n|STUDY SCENE2 pick up the book and place it in the right compartment of the caddy|$0.0$|$0.2$|$0.0$|\n|STUDY SCENE3 pick up the book and place it in the front compartment of the caddy|$0.2$|$0.0$|$0.0$|\n|STUDY SCENE3 pick up the book and place it in the left compartment of the caddy|$0.4$|$0.45$|$0.15$|\n|STUDY SCENE3 pick up the book and place it in the right compartment of the caddy|$0.0$|$0.05$|$0.05$|\n|STUDY SCENE3 pick up the red mug and place it to the right of the caddy|$0.0$|$0.2$|$0.05$|\n|STUDY SCENE3 pick up the white mug and place it to the right of the caddy|$0.0$|$0.0$|$0.05$|\n|STUDY SCENE4 pick up the book in the middle and place it on the cabinet shelf|$0.05$|$0.2$|$0.05$|\n|STUDY SCENE4 pick up the book on the left and place it on top of the shelf|$0.2$|$0.1$|$0.15$|\n|STUDY SCENE4 pick up the book on the right and place it on the cabinet shelf|$0.0$|$0.25$|$0.05$|\n|STUDY SCENE4 pick up the book on the right and place it under the cabinet shelf|$0.15$|$0.1$|$0.05$|", "isPerfectTable": true, "csv": "\"Env Name\",\"RESNET-RNN\",\"RESNET-T\",\"VIT-T\"\n\"LIVING ROOM SCENE1 pick up the alphabet soup and put it in the basket\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE1 pick up the cream cheese box and put it in the basket\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE1 pick up the ketchup and put it in the basket\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE1 pick up the tomato sauce and put it in the basket\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE2 pick up the alphabet soup and put it in the basket\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE2 pick up the butter and put it in the basket\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE2 pick up the milk and put it in the basket\",\"$0.0$\",\"$0.05$\",\"$0.0$\"\n\"LIVING ROOM SCENE2 pick up the orange juice and put it in the basket\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE2 pick up the tomato sauce and put it in the basket\",\"$0.0$\",\"$0.05$\",\"$0.0$\"\n\"LIVING ROOM SCENE3 pick up the alphabet soup and put it in the tray\",\"$0.0$\",\"$0.05$\",\"$0.0$\"\n\"LIVING ROOM SCENE3 pick up the butter and put it in the tray\",\"$0.0$\",\"$0.3$\",\"$0.0$\"\n\"LIVING ROOM SCENE3 pick up the cream cheese and put it in the tray\",\"$0.0$\",\"$0.25$\",\"$0.0$\"\n\"LIVING ROOM SCENE3 pick up the ketchup and put it in the tray\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE3 pick up the tomato sauce and put it in the tray\",\"$0.0$\",\"$0.25$\",\"$0.0$\"\n\"LIVING ROOM SCENE4 pick up the black bowl on the left and put it in the tray\",\"$0.0$\",\"$0.4$\",\"$0.2$\"\n\"LIVING ROOM SCENE4 pick up the chocolate pudding and put it in the tray\",\"$0.0$\",\"$0.2$\",\"$0.25$\"\n\"LIVING ROOM SCENE4 pick up the salad dressing and put it in the tray\",\"$0.0$\",\"$0.0$\",\"$0.1$\"\n\"LIVING ROOM SCENE4 stack the left bowl on the right bowl and place them in the tray\",\"$0.0$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE4 stack the right bowl on the left bowl and place them in the tray\",\"$0.0$\",\"$0.0$\",\"$0.05$\"\n\"LIVING ROOM SCENE5 put the red mug on the left plate\",\"$0.0$\",\"$0.0$\",\"$0.05$\"\n\"LIVING ROOM SCENE5 put the red mug on the right plate\",\"$0.15$\",\"$0.0$\",\"$0.0$\"\n\"LIVING ROOM SCENE5 put the white mug on the left plate\",\"$0.1$\",\"$0.15$\",\"$0.05$\"\n\"LIVING ROOM SCENE5 put the yellow and white mug on the right plate\",\"$0.35$\",\"$0.05$\",\"$0.05$\"\n\"LIVING ROOM SCENE6 put the chocolate pudding to the left of the plate\",\"$0.1$\",\"$0.65$\",\"$0.0$\"\n\"LIVING ROOM SCENE6 put the chocolate pudding to the right of the plate\",\"$0.05$\",\"$0.55$\",\"$0.0$\"\n\"LIVING ROOM SCENE6 put the red mug on the plate\",\"$0.0$\",\"$0.2$\",\"$0.0$\"\n\"LIVING ROOM SCENE6 put the white mug on the plate\",\"$0.0$\",\"$0.2$\",\"$0.0$\"\n\"STUDY SCENE1 pick up the book and place it in the front compartment of the caddy\",\"$0.0$\",\"$0.0$\",\"$0.05$\"\n\"STUDY SCENE1 pick up the book and place it in the left compartment of the caddy\",\"$0.2$\",\"$0.05$\",\"$0.0$\"\n\"STUDY SCENE1 pick up the book and place it in the right compartment of the caddy\",\"$0.0$\",\"$0.1$\",\"$0.0$\"\n\"STUDY SCENE1 pick up the yellow and white mug and place it to the right of the caddy\",\"$0.0$\",\"$0.3$\",\"$0.35$\"\n\"STUDY SCENE2 pick up the book and place it in the back compartment of the caddy\",\"$0.35$\",\"$0.3$\",\"$0.0$\"\n\"STUDY SCENE2 pick up the book and place it in the front compartment of the caddy\",\"$0.25$\",\"$0.1$\",\"$0.0$\"\n\"STUDY SCENE2 pick up the book and place it in the left compartment of the caddy\",\"$0.25$\",\"$0.45$\",\"$0.05$\"\n\"STUDY SCENE2 pick up the book and place it in the right compartment of the caddy\",\"$0.0$\",\"$0.2$\",\"$0.0$\"\n\"STUDY SCENE3 pick up the book and place it in the front compartment of the caddy\",\"$0.2$\",\"$0.0$\",\"$0.0$\"\n\"STUDY SCENE3 pick up the book and place it in the left compartment of the caddy\",\"$0.4$\",\"$0.45$\",\"$0.15$\"\n\"STUDY SCENE3 pick up the book and place it in the right compartment of the caddy\",\"$0.0$\",\"$0.05$\",\"$0.05$\"\n\"STUDY SCENE3 pick up the red mug and place it to the right of the caddy\",\"$0.0$\",\"$0.2$\",\"$0.05$\"\n\"STUDY SCENE3 pick up the white mug and place it to the right of the caddy\",\"$0.0$\",\"$0.0$\",\"$0.05$\"\n\"STUDY SCENE4 pick up the book in the middle and place it on the cabinet shelf\",\"$0.05$\",\"$0.2$\",\"$0.05$\"\n\"STUDY SCENE4 pick up the book on the left and place it on top of the shelf\",\"$0.2$\",\"$0.1$\",\"$0.15$\"\n\"STUDY SCENE4 pick up the book on the right and place it on the cabinet shelf\",\"$0.0$\",\"$0.25$\",\"$0.05$\"\n\"STUDY SCENE4 pick up the book on the right and place it under the cabinet shelf\",\"$0.15$\",\"$0.1$\",\"$0.05$\""}]}, {"page": 36, "text": "E.4     Attention Visualization\nIt is also important to visualize the behavior of the robot and its attention maps during the completion\nof tasks in the lifelong learning process to give us intuition and qualitative feedback on the perfor-\nmance of different algorithms and architectures. We visualize the attention maps of learned policies\nwith   (author?)     [23]  and compare them in different studies as in 5.2 to see if the robot correctly pays\nattention to the right regions of interest in each task.\nPerturbation-based attention visualization:                     We use a perturbation-based method [23] to extract\nattention maps from agents. Given an input imageI                    , the method applies a Gaussian filter to a pixel\nlocation    (i, j) to blur the image partially, and produces the perturbed image                      \u03a6(I, i, j). Denote the\nlearned policy as      \u03c0  and the inputs to the spatial module (e.g., the last latent representation of resnet\nor ViT encoder)        \u03c0 u(I)   for image     I . Then we define the saliency score as the Euclidean distance\nbetween the latent representations of the original and the blurred images:                 2\n                                      S \u03c0 (i, j) =    1   \u03c0 u(I)  \u2212   \u03c0 u(\u03a6(I, i, j))       .                                   (4)\n                                                      2\nIntuitively,   S \u03c0 (i, j) describes    how much removing information from the region around location                         (i, j)\nchanges the policy. In other words, a large              S \u03c0(i, j)  indicates that the information around pixel              (i, j)\nis important for the learning agent\u2019s decision-making. Instead of calculating the score for every\npixel, [23] found that computing a saliency score for pixel                      i mod 5 and       j mod 5 produced good\nsaliency maps at lower computational costs for Atari games. The final saliency map                            P  is normalized\nas  P (i, j) =    P  S\u03c0 (i,j)   .\n                    i,j S\u03c0 (i,j)\nWe provide the visualization and our analysis on the following pages.\n                                                                36", "md": "# Attention Visualization\n\n## Attention Visualization\n\nIt is also important to visualize the behavior of the robot and its attention maps during the completion\nof tasks in the lifelong learning process to give us intuition and qualitative feedback on the performance of different algorithms and architectures. We visualize the attention maps of learned policies with [23] and compare them in different studies as in 5.2 to see if the robot correctly pays attention to the right regions of interest in each task.\n\nPerturbation-based attention visualization: We use a perturbation-based method [23] to extract attention maps from agents. Given an input image $$I$$, the method applies a Gaussian filter to a pixel location $$(i, j)$$ to blur the image partially, and produces the perturbed image $$\\Phi(I, i, j)$$. Denote the learned policy as $$\\pi$$ and the inputs to the spatial module (e.g., the last latent representation of resnet or ViT encoder) $$\\pi_u(I)$$ for image $$I$$. Then we define the saliency score as the Euclidean distance between the latent representations of the original and the blurred images:\n\n$$\nS_{\\pi}(i, j) = \\frac{1}{2} ||\\pi_u(I) - \\pi_u(\\Phi(I, i, j))||^2 \\quad (4)\n$$\n\nIntuitively, $$S_{\\pi}(i, j)$$ describes how much removing information from the region around location $$(i, j)$$ changes the policy. In other words, a large $$S_{\\pi}(i, j)$$ indicates that the information around pixel $$(i, j)$$ is important for the learning agent\u2019s decision-making. Instead of calculating the score for every pixel, [23] found that computing a saliency score for pixel $$i \\mod 5$$ and $$j \\mod 5$$ produced good saliency maps at lower computational costs for Atari games. The final saliency map $$P$$ is normalized as\n\n$$\nP(i, j) = \\frac{\\sum_{i,j} S_{\\pi}(i,j)}{P S_{\\pi}(i,j)}\n$$\n\nWe provide the visualization and our analysis on the following pages.\n\nReference: [23] - Include the reference details here.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Attention Visualization", "md": "# Attention Visualization"}, {"type": "heading", "lvl": 2, "value": "Attention Visualization", "md": "## Attention Visualization"}, {"type": "text", "value": "It is also important to visualize the behavior of the robot and its attention maps during the completion\nof tasks in the lifelong learning process to give us intuition and qualitative feedback on the performance of different algorithms and architectures. We visualize the attention maps of learned policies with [23] and compare them in different studies as in 5.2 to see if the robot correctly pays attention to the right regions of interest in each task.\n\nPerturbation-based attention visualization: We use a perturbation-based method [23] to extract attention maps from agents. Given an input image $$I$$, the method applies a Gaussian filter to a pixel location $$(i, j)$$ to blur the image partially, and produces the perturbed image $$\\Phi(I, i, j)$$. Denote the learned policy as $$\\pi$$ and the inputs to the spatial module (e.g., the last latent representation of resnet or ViT encoder) $$\\pi_u(I)$$ for image $$I$$. Then we define the saliency score as the Euclidean distance between the latent representations of the original and the blurred images:\n\n$$\nS_{\\pi}(i, j) = \\frac{1}{2} ||\\pi_u(I) - \\pi_u(\\Phi(I, i, j))||^2 \\quad (4)\n$$\n\nIntuitively, $$S_{\\pi}(i, j)$$ describes how much removing information from the region around location $$(i, j)$$ changes the policy. In other words, a large $$S_{\\pi}(i, j)$$ indicates that the information around pixel $$(i, j)$$ is important for the learning agent\u2019s decision-making. Instead of calculating the score for every pixel, [23] found that computing a saliency score for pixel $$i \\mod 5$$ and $$j \\mod 5$$ produced good saliency maps at lower computational costs for Atari games. The final saliency map $$P$$ is normalized as\n\n$$\nP(i, j) = \\frac{\\sum_{i,j} S_{\\pi}(i,j)}{P S_{\\pi}(i,j)}\n$$\n\nWe provide the visualization and our analysis on the following pages.\n\nReference: [23] - Include the reference details here.", "md": "It is also important to visualize the behavior of the robot and its attention maps during the completion\nof tasks in the lifelong learning process to give us intuition and qualitative feedback on the performance of different algorithms and architectures. We visualize the attention maps of learned policies with [23] and compare them in different studies as in 5.2 to see if the robot correctly pays attention to the right regions of interest in each task.\n\nPerturbation-based attention visualization: We use a perturbation-based method [23] to extract attention maps from agents. Given an input image $$I$$, the method applies a Gaussian filter to a pixel location $$(i, j)$$ to blur the image partially, and produces the perturbed image $$\\Phi(I, i, j)$$. Denote the learned policy as $$\\pi$$ and the inputs to the spatial module (e.g., the last latent representation of resnet or ViT encoder) $$\\pi_u(I)$$ for image $$I$$. Then we define the saliency score as the Euclidean distance between the latent representations of the original and the blurred images:\n\n$$\nS_{\\pi}(i, j) = \\frac{1}{2} ||\\pi_u(I) - \\pi_u(\\Phi(I, i, j))||^2 \\quad (4)\n$$\n\nIntuitively, $$S_{\\pi}(i, j)$$ describes how much removing information from the region around location $$(i, j)$$ changes the policy. In other words, a large $$S_{\\pi}(i, j)$$ indicates that the information around pixel $$(i, j)$$ is important for the learning agent\u2019s decision-making. Instead of calculating the score for every pixel, [23] found that computing a saliency score for pixel $$i \\mod 5$$ and $$j \\mod 5$$ produced good saliency maps at lower computational costs for Atari games. The final saliency map $$P$$ is normalized as\n\n$$\nP(i, j) = \\frac{\\sum_{i,j} S_{\\pi}(i,j)}{P S_{\\pi}(i,j)}\n$$\n\nWe provide the visualization and our analysis on the following pages.\n\nReference: [23] - Include the reference details here."}]}, {"page": 37, "text": "Different Task Suites\n          LIBERO-SPATIAL\n                                        pick  up the black           pickup the    black          pick up  the black\n                                        bowl  on  the wooden         bowl  in the  top            bowl  between  the\n                                        cabinet  and place it        drawer of the                plate and the\n                                        on the plate                 cabinetand place it          ramekin and place it\n                                                                     on the plate                 on the plate\n          LIBERO-OBJECT\n                                        pick up  the orange          pick up the   ketchup        pick up  the alphabet\n                                        juice and place    it        and place   it in the        soup and place   it  in\n                                        in the basket                basket                       the basket\n          LIBERO-GOAL\n                                        put the wine bottle          put  the bowl   on top       open the middle\n                                        on the rack                  of the cabinet               drawer of the\n                                                                                                  cabinet\n          LIBERO-LONG\n                                        put the white mug    on      put  both the                put the yellow and\n                                        the left plate and           alphabet   soup and          white mug in the\n                                        put  the yellow and          the  tomato sauce   in       microwave and close\n                                        white mug   on  the          the basket                   it\n                                        right plate\nFigure 21: Attention map comparison among different task suites with ER and RESNET-T. Each row\ncorresponds to a task suite.\nFindings:     Figure 21 shows attention visualization for 12 tasks across 4 task suites (e.g., 3 tasks per\nsuite). We observe that:\n        1.  policies pay more attention to the robot arm and the target placement area than the target\n            object.\n        2.  sometimes the policy pays attention to task-irrelevant areas, such as the blank area on the\n            table.\n                                                                37", "md": "# Task Suites\n\n## Different Task Suites\n\n### LIBERO-SPATIAL\n\npick up pe black bowl on pe wooden cabinet and place it on pe plate\npickup pe black bowl in pe top drawer of pe cabinet and place it on pe plate\npick up pe black bowl between pe plate and pe ramekin and place it on pe plate\n\n### LIBERO-OBJECT\n\npick up pe orange juice and place it in pe basket\npick up pe ketchup and place it in pe basket\npick up pe alphabet soup and place it in pe basket\n\n### LIBERO-GOAL\n\nput pe wine bottle on pe rack\nput pe bowl on top of pe cabinet\nopen pe middle drawer of pe cabinet\n\n### LIBERO-LONG\n\nput pe white mug on pe left plate\nput bop pe alphabet soup and pe tomato sauce in pe basket\nput pe yellow and white mug in pe microwave and close it\n\nFigure 21: Attention map comparison among different task suites with ER and RESNET-T. Each row corresponds to a task suite.\n\nFindings:\n\n1. policies pay more attention to the robot arm and the target placement area than the target object.\n2. sometimes the policy pays attention to task-irrelevant areas, such as the blank area on the table.", "images": [{"name": "img_p36_1", "height": 1444, "width": 1070}], "items": [{"type": "heading", "lvl": 1, "value": "Task Suites", "md": "# Task Suites"}, {"type": "heading", "lvl": 2, "value": "Different Task Suites", "md": "## Different Task Suites"}, {"type": "heading", "lvl": 3, "value": "LIBERO-SPATIAL", "md": "### LIBERO-SPATIAL"}, {"type": "text", "value": "pick up pe black bowl on pe wooden cabinet and place it on pe plate\npickup pe black bowl in pe top drawer of pe cabinet and place it on pe plate\npick up pe black bowl between pe plate and pe ramekin and place it on pe plate", "md": "pick up pe black bowl on pe wooden cabinet and place it on pe plate\npickup pe black bowl in pe top drawer of pe cabinet and place it on pe plate\npick up pe black bowl between pe plate and pe ramekin and place it on pe plate"}, {"type": "heading", "lvl": 3, "value": "LIBERO-OBJECT", "md": "### LIBERO-OBJECT"}, {"type": "text", "value": "pick up pe orange juice and place it in pe basket\npick up pe ketchup and place it in pe basket\npick up pe alphabet soup and place it in pe basket", "md": "pick up pe orange juice and place it in pe basket\npick up pe ketchup and place it in pe basket\npick up pe alphabet soup and place it in pe basket"}, {"type": "heading", "lvl": 3, "value": "LIBERO-GOAL", "md": "### LIBERO-GOAL"}, {"type": "text", "value": "put pe wine bottle on pe rack\nput pe bowl on top of pe cabinet\nopen pe middle drawer of pe cabinet", "md": "put pe wine bottle on pe rack\nput pe bowl on top of pe cabinet\nopen pe middle drawer of pe cabinet"}, {"type": "heading", "lvl": 3, "value": "LIBERO-LONG", "md": "### LIBERO-LONG"}, {"type": "text", "value": "put pe white mug on pe left plate\nput bop pe alphabet soup and pe tomato sauce in pe basket\nput pe yellow and white mug in pe microwave and close it\n\nFigure 21: Attention map comparison among different task suites with ER and RESNET-T. Each row corresponds to a task suite.\n\nFindings:\n\n1. policies pay more attention to the robot arm and the target placement area than the target object.\n2. sometimes the policy pays attention to task-irrelevant areas, such as the blank area on the table.", "md": "put pe white mug on pe left plate\nput bop pe alphabet soup and pe tomato sauce in pe basket\nput pe yellow and white mug in pe microwave and close it\n\nFigure 21: Attention map comparison among different task suites with ER and RESNET-T. Each row corresponds to a task suite.\n\nFindings:\n\n1. policies pay more attention to the robot arm and the target placement area than the target object.\n2. sometimes the policy pays attention to task-irrelevant areas, such as the blank area on the table."}]}, {"page": 38, "text": "These observations demonstrate that the learned policy use perceptual data for decision-making\n in a very different way from how humans do.                     The robot policies tends to spuriously correlate\n task-irrelevant features with actions, a major reason why the policies overfit to the tasks and do not\n generalize well across tasks.\n                                                               38", "md": "# Observations on Learned Policy\n\nThese observations demonstrate that the learned policy uses perceptual data for decision-making in a very different way from how humans do. The robot policies tend to spuriously correlate task-irrelevant features with actions, a major reason why the policies overfit to the tasks and do not generalize well across tasks.\n\n$$38$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Observations on Learned Policy", "md": "# Observations on Learned Policy"}, {"type": "text", "value": "These observations demonstrate that the learned policy uses perceptual data for decision-making in a very different way from how humans do. The robot policies tend to spuriously correlate task-irrelevant features with actions, a major reason why the policies overfit to the tasks and do not generalize well across tasks.\n\n$$38$$", "md": "These observations demonstrate that the learned policy uses perceptual data for decision-making in a very different way from how humans do. The robot policies tend to spuriously correlate task-irrelevant features with actions, a major reason why the policies overfit to the tasks and do not generalize well across tasks.\n\n$$38$$"}]}, {"page": 39, "text": "The Same Task over the Course of Lifelong Learning\n                                      put  both the alphabet soup     and the tomato sauce in the basket\n           ER\n           ResNet-T\n           ER\n           ViT-T\n           PackNet\n           ResNet-T\n           PackNet\n           ViT-T\n                               Initial  Policy        After  3rd  Task       After Sth Task         After 1Oth Task\n Figure 22: Attention map of the same state of the task                  put both the alphabet soup and the tomato\n sauce in the basket       from LIBERO-LONG               during lifelong learning. Each row visualizes how the\n attention maps change on the first task with one of the LL algorithms (ER and PACKNET) and one of\n the neural architectures (RESNET-T and VIT-T). Initial policy is the policy that is trained on the first\n task. And all the following attention maps correspond to policies after training on the third, fifth, and\n the tenth tasks.\nFindings:     Figure 22 shows attention visualizations from policies trained with ER and PACKNET\n using the architectures RESNET-T and VIT-T respectively. We observe that:\n        1.  The ViT visual encoder\u2019s attention is more consistent over time, while the ResNet encoder\u2019s\n            attention map gradually dilutes.\n        2.  PackNet, as it splits the model capacity for different tasks, shows a more consistent attention\n            map over the course of learning.\n                                                                39", "md": "# The Same Task over the Course of Lifelong Learning\n\n## The Same Task over the Course of Lifelong Learning\n\nPut both the alphabet soup and the tomato sauce in the basket\n\n|ER|ResNet-T|\n|---|---|\n|ViT-T|PackNet|\n|ResNet-T|PackNet|\n|ViT-T| |\n\nInitial Policy | After 3rd Task | After 5th Task | After 10th Task\n\nFigure 22: Attention map of the same state of the task put both the alphabet soup and the tomato sauce in the basket from LIBERO-LONG during lifelong learning. Each row visualizes how the attention maps change on the first task with one of the LL algorithms (ER and PACKNET) and one of the neural architectures (RESNET-T and VIT-T). Initial policy is the policy that is trained on the first task. And all the following attention maps correspond to policies after training on the third, fifth, and the tenth tasks.\n\n### Findings:\n\nFigure 22 shows attention visualizations from policies trained with ER and PACKNET using the architectures RESNET-T and VIT-T respectively. We observe that:\n\n1. The ViT visual encoder\u2019s attention is more consistent over time, while the ResNet encoder\u2019s attention map gradually dilutes.\n2. PackNet, as it splits the model capacity for different tasks, shows a more consistent attention map over the course of learning.", "images": [{"name": "img_p38_1", "height": 963, "width": 1070}], "items": [{"type": "heading", "lvl": 1, "value": "The Same Task over the Course of Lifelong Learning", "md": "# The Same Task over the Course of Lifelong Learning"}, {"type": "heading", "lvl": 2, "value": "The Same Task over the Course of Lifelong Learning", "md": "## The Same Task over the Course of Lifelong Learning"}, {"type": "text", "value": "Put both the alphabet soup and the tomato sauce in the basket", "md": "Put both the alphabet soup and the tomato sauce in the basket"}, {"type": "table", "rows": [["ER", "ResNet-T"], ["ViT-T", "PackNet"], ["ResNet-T", "PackNet"], ["ViT-T", ""]], "md": "|ER|ResNet-T|\n|---|---|\n|ViT-T|PackNet|\n|ResNet-T|PackNet|\n|ViT-T| |", "isPerfectTable": true, "csv": "\"ER\",\"ResNet-T\"\n\"ViT-T\",\"PackNet\"\n\"ResNet-T\",\"PackNet\"\n\"ViT-T\",\"\""}, {"type": "text", "value": "Initial Policy | After 3rd Task | After 5th Task | After 10th Task\n\nFigure 22: Attention map of the same state of the task put both the alphabet soup and the tomato sauce in the basket from LIBERO-LONG during lifelong learning. Each row visualizes how the attention maps change on the first task with one of the LL algorithms (ER and PACKNET) and one of the neural architectures (RESNET-T and VIT-T). Initial policy is the policy that is trained on the first task. And all the following attention maps correspond to policies after training on the third, fifth, and the tenth tasks.", "md": "Initial Policy | After 3rd Task | After 5th Task | After 10th Task\n\nFigure 22: Attention map of the same state of the task put both the alphabet soup and the tomato sauce in the basket from LIBERO-LONG during lifelong learning. Each row visualizes how the attention maps change on the first task with one of the LL algorithms (ER and PACKNET) and one of the neural architectures (RESNET-T and VIT-T). Initial policy is the policy that is trained on the first task. And all the following attention maps correspond to policies after training on the third, fifth, and the tenth tasks."}, {"type": "heading", "lvl": 3, "value": "Findings:", "md": "### Findings:"}, {"type": "text", "value": "Figure 22 shows attention visualizations from policies trained with ER and PACKNET using the architectures RESNET-T and VIT-T respectively. We observe that:\n\n1. The ViT visual encoder\u2019s attention is more consistent over time, while the ResNet encoder\u2019s attention map gradually dilutes.\n2. PackNet, as it splits the model capacity for different tasks, shows a more consistent attention map over the course of learning.", "md": "Figure 22 shows attention visualizations from policies trained with ER and PACKNET using the architectures RESNET-T and VIT-T respectively. We observe that:\n\n1. The ViT visual encoder\u2019s attention is more consistent over time, while the ResNet encoder\u2019s attention map gradually dilutes.\n2. PackNet, as it splits the model capacity for different tasks, shows a more consistent attention map over the course of learning."}]}, {"page": 40, "text": "Different Lifelong Learning Algorithms\n              Policy after\n             training task 5\n              on task 5\n              Policyafter\n             training task 10\n              on task 5\n              Policy after\n             training task 10\n              on task 10\n                                                ER                    PackNet                     EWC\nFigure 23: Comparison of attention maps of different lifelong learning algorithms with RESNET-T\non LIBERO-LONG. Each row shows the same state of a task with different neural architectures.\n\u201cTask 5\u201d refers to the task      put the white mug on the left plate and put the yellow and white mug on the\nright plate. \u201cTask 10\u201d refers to the task          put the yellow and white mug in the microwave and close it.\nThe second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the\nattention map differences in backward transfer.\nFindings:     Figure 23 shows the attention visualization of three lifelong learning algorithms on\nLIBERO-LONG           with RESNET-T on two tasks (task 5 and task 10). The first and third rows show the\nattention of the policy on the same task it has just learned. While the second row shows the attention\nof the policy on the task it learned in the past. We observe that:\n        1.  PACKNET       shows more concentrated attention compared against ER and EWC (usually just\n            a single mode).\n        2.  ER shares similar attention map with EWC, but EWC performs much worse than ER.\n            Therefore, attention can only assist the analysis but cannot be treated as a criterion for\n            performance prediction.\n                                                               40", "md": "# Lifelong Learning Algorithms\n\n## Different Lifelong Learning Algorithms\n\nPolicy after training task 5 on task 5\n\nPolicy after training task 10 on task 5\n\nPolicy after training task 10 on task 10\n\nFigure 23: Comparison of attention maps of different lifelong learning algorithms with RESNET-T on LIBERO-LONG. Each row shows the same state of a task with different neural architectures.\n\n\"Task 5\" refers to the task: put the white mug on the left plate and put the yellow and white mug on the right plate.\n\n\"Task 10\" refers to the task: put the yellow and white mug in the microwave and close it.\n\nThe second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention map differences in backward transfer.\n\n### Findings:\n\nFigure 23 shows the attention visualization of three lifelong learning algorithms on LIBERO-LONG with RESNET-T on two tasks (task 5 and task 10). The first and third rows show the attention of the policy on the same task it has just learned. While the second row shows the attention of the policy on the task it learned in the past. We observe that:\n\n1. PACKNET shows more concentrated attention compared against ER and EWC (usually just a single mode).\n2. ER shares similar attention map with EWC, but EWC performs much worse than ER. Therefore, attention can only assist the analysis but cannot be treated as a criterion for performance prediction.\n\n40", "images": [{"name": "img_p39_1", "height": 808, "width": 951}], "items": [{"type": "heading", "lvl": 1, "value": "Lifelong Learning Algorithms", "md": "# Lifelong Learning Algorithms"}, {"type": "heading", "lvl": 2, "value": "Different Lifelong Learning Algorithms", "md": "## Different Lifelong Learning Algorithms"}, {"type": "text", "value": "Policy after training task 5 on task 5\n\nPolicy after training task 10 on task 5\n\nPolicy after training task 10 on task 10\n\nFigure 23: Comparison of attention maps of different lifelong learning algorithms with RESNET-T on LIBERO-LONG. Each row shows the same state of a task with different neural architectures.\n\n\"Task 5\" refers to the task: put the white mug on the left plate and put the yellow and white mug on the right plate.\n\n\"Task 10\" refers to the task: put the yellow and white mug in the microwave and close it.\n\nThe second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention map differences in backward transfer.", "md": "Policy after training task 5 on task 5\n\nPolicy after training task 10 on task 5\n\nPolicy after training task 10 on task 10\n\nFigure 23: Comparison of attention maps of different lifelong learning algorithms with RESNET-T on LIBERO-LONG. Each row shows the same state of a task with different neural architectures.\n\n\"Task 5\" refers to the task: put the white mug on the left plate and put the yellow and white mug on the right plate.\n\n\"Task 10\" refers to the task: put the yellow and white mug in the microwave and close it.\n\nThe second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention map differences in backward transfer."}, {"type": "heading", "lvl": 3, "value": "Findings:", "md": "### Findings:"}, {"type": "text", "value": "Figure 23 shows the attention visualization of three lifelong learning algorithms on LIBERO-LONG with RESNET-T on two tasks (task 5 and task 10). The first and third rows show the attention of the policy on the same task it has just learned. While the second row shows the attention of the policy on the task it learned in the past. We observe that:\n\n1. PACKNET shows more concentrated attention compared against ER and EWC (usually just a single mode).\n2. ER shares similar attention map with EWC, but EWC performs much worse than ER. Therefore, attention can only assist the analysis but cannot be treated as a criterion for performance prediction.\n\n40", "md": "Figure 23 shows the attention visualization of three lifelong learning algorithms on LIBERO-LONG with RESNET-T on two tasks (task 5 and task 10). The first and third rows show the attention of the policy on the same task it has just learned. While the second row shows the attention of the policy on the task it learned in the past. We observe that:\n\n1. PACKNET shows more concentrated attention compared against ER and EWC (usually just a single mode).\n2. ER shares similar attention map with EWC, but EWC performs much worse than ER. Therefore, attention can only assist the analysis but cannot be treated as a criterion for performance prediction.\n\n40"}]}, {"page": 41, "text": " Different Neural Architectures\n              Policy after\n              training task 5\n               on task 5\n               Policyafter\n              training task 10\n               on task 5\n              Policy after\n              training task 10\n               on task 10\n                                           ResNet-RNN                 ResNet-T                      Vit-T\n Figure 24: Comparison of attention maps of different neural architectures with ER on LIBERO-\n LONG. Each row shows the same state of a task with different neural architectures. \u201cTask 5\u201d refers\n to the task   put the white mug on the left plate and put the yellow and white mug on the right plate.\n\u201cTask 10\u201d refers to the task        put the yellow and white mug in the microwave and close it. The second\n row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention\n map differences in backward transfer.\nFindings:     Figure 24 shows attention map comparisons of the three neural architectures on LIBERO-\n LONG     with ER on two tasks (task 5 and task 10). We observe that:\n        1. ViT has more concentrated attention than policies using ResNet.\n        2.  When ResNet forgets, the attention is changing smoothly (more diluted). But for ViT, when\n            it forgets, the attention can completely shift to a different location.\n        3. When ResNet is combined with LSTM or a temporal transformer, the attention hints at the\n            \"course of future trajectory\". But we do not observe that when ViT is used as the encoder.\n                                                                41", "md": "# Neural Architectures Comparison\n\n## Different Neural Architectures\n\nPolicy after training task 5 on task 5\nPolicy after training task 10 on task 5\nPolicy after training task 10 on task 10\n\n### Comparison of attention maps of different neural architectures\n\nFigure 24: Comparison of attention maps of different neural architectures with ER on LIBERO-LONG. Each row shows the same state of a task with different neural architectures. \"Task 5\" refers to the task of putting the white mug on the left plate and putting the yellow and white mug on the right plate. \"Task 10\" refers to the task of putting the yellow and white mug in the microwave and closing it. The second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention map differences in backward transfer.\n\n### Findings:\n\n1. ViT has more concentrated attention than policies using ResNet.\n2. When ResNet forgets, the attention is changing smoothly (more diluted). But for ViT, when it forgets, the attention can completely shift to a different location.\n3. When ResNet is combined with LSTM or a temporal transformer, the attention hints at the \"course of future trajectory\". But we do not observe that when ViT is used as the encoder.\n\nFigure 24 shows attention map comparisons of the three neural architectures on LIBERO-LONG with ER on two tasks (task 5 and task 10).\n\n### Neural Architectures:\n\n- ResNet-RNN\n- ResNet-T\n- Vit-T", "images": [{"name": "img_p40_1", "height": 808, "width": 951}], "items": [{"type": "heading", "lvl": 1, "value": "Neural Architectures Comparison", "md": "# Neural Architectures Comparison"}, {"type": "heading", "lvl": 2, "value": "Different Neural Architectures", "md": "## Different Neural Architectures"}, {"type": "text", "value": "Policy after training task 5 on task 5\nPolicy after training task 10 on task 5\nPolicy after training task 10 on task 10", "md": "Policy after training task 5 on task 5\nPolicy after training task 10 on task 5\nPolicy after training task 10 on task 10"}, {"type": "heading", "lvl": 3, "value": "Comparison of attention maps of different neural architectures", "md": "### Comparison of attention maps of different neural architectures"}, {"type": "text", "value": "Figure 24: Comparison of attention maps of different neural architectures with ER on LIBERO-LONG. Each row shows the same state of a task with different neural architectures. \"Task 5\" refers to the task of putting the white mug on the left plate and putting the yellow and white mug on the right plate. \"Task 10\" refers to the task of putting the yellow and white mug in the microwave and closing it. The second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention map differences in backward transfer.", "md": "Figure 24: Comparison of attention maps of different neural architectures with ER on LIBERO-LONG. Each row shows the same state of a task with different neural architectures. \"Task 5\" refers to the task of putting the white mug on the left plate and putting the yellow and white mug on the right plate. \"Task 10\" refers to the task of putting the yellow and white mug in the microwave and closing it. The second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention map differences in backward transfer."}, {"type": "heading", "lvl": 3, "value": "Findings:", "md": "### Findings:"}, {"type": "text", "value": "1. ViT has more concentrated attention than policies using ResNet.\n2. When ResNet forgets, the attention is changing smoothly (more diluted). But for ViT, when it forgets, the attention can completely shift to a different location.\n3. When ResNet is combined with LSTM or a temporal transformer, the attention hints at the \"course of future trajectory\". But we do not observe that when ViT is used as the encoder.\n\nFigure 24 shows attention map comparisons of the three neural architectures on LIBERO-LONG with ER on two tasks (task 5 and task 10).", "md": "1. ViT has more concentrated attention than policies using ResNet.\n2. When ResNet forgets, the attention is changing smoothly (more diluted). But for ViT, when it forgets, the attention can completely shift to a different location.\n3. When ResNet is combined with LSTM or a temporal transformer, the attention hints at the \"course of future trajectory\". But we do not observe that when ViT is used as the encoder.\n\nFigure 24 shows attention map comparisons of the three neural architectures on LIBERO-LONG with ER on two tasks (task 5 and task 10)."}, {"type": "heading", "lvl": 3, "value": "Neural Architectures:", "md": "### Neural Architectures:"}, {"type": "text", "value": "- ResNet-RNN\n- ResNet-T\n- Vit-T", "md": "- ResNet-RNN\n- ResNet-T\n- Vit-T"}]}, {"page": 42, "text": " Different Task Ordering\n              Canonical task order\n                                                    1st                      Sth                      1Oth\n              Task order\n                                                   8th                       1st                       9th\n              Task order 3\n                                                   4th                       2nd                       9th\n Figure 25: Attention map comparison among different orderings with ER and RESNET-T on three\n selected tasks from LIBERO-LONG:                put both the alphabet soup and the tomato sauce in the basket,\n put the white mug on the left plate and put the yellow and white mug on the right plate, and                    put the\n yellow and white mug in the microwave and close it. Each row corresponds to a specific sequence of\n task ordering, and the caption of each attention map indicates the order of the task in that sequence.\nFindings:    Figure 25 shows attention map comparisons of three different task orderings. We show two\n immediately learned tasks from LIBERO-LONG                      trained with ER and RESNET-T. We observe that:\n        1.  As expected, learning the same task at different positions in the task stream results in\n            different attention visualization.\n        2.  There seems to be a trend that the policy has a more spread-out attention when it learns on\n            tasks that are later in the sequence.\n                                                               42", "md": "Different Task Ordering\n\n|Canonical task order|1st|5th|10th|\n|---|---|---|---|\n|Task order|8th|1st|9th|\n|Task order 3|4th|2nd|9th|\n\nFigure 25: Attention map comparison among different orderings with ER and RESNET-T on three selected tasks from LIBERO-LONG:\n\n$$\\text{put both the alphabet soup and the tomato sauce in the basket,}$$\n\n$$\\text{put the white mug on the left plate and put the yellow and white mug on the right plate, and}$$\n\n$$\\text{put the yellow and white mug in the microwave and close it.}$$\n\nEach row corresponds to a specific sequence of task ordering, and the caption of each attention map indicates the order of the task in that sequence.\n\nFindings: Figure 25 shows attention map comparisons of three different task orderings. We show two immediately learned tasks from LIBERO-LONG trained with ER and RESNET-T. We observe that:\n\n1. As expected, learning the same task at different positions in the task stream results in different attention visualization.\n2. There seems to be a trend that the policy has a more spread-out attention when it learns on tasks that are later in the sequence.\n\n42", "images": [{"name": "img_p41_1", "height": 808, "width": 951}], "items": [{"type": "text", "value": "Different Task Ordering", "md": "Different Task Ordering"}, {"type": "table", "rows": [["Canonical task order", "1st", "5th", "10th"], ["Task order", "8th", "1st", "9th"], ["Task order 3", "4th", "2nd", "9th"]], "md": "|Canonical task order|1st|5th|10th|\n|---|---|---|---|\n|Task order|8th|1st|9th|\n|Task order 3|4th|2nd|9th|", "isPerfectTable": true, "csv": "\"Canonical task order\",\"1st\",\"5th\",\"10th\"\n\"Task order\",\"8th\",\"1st\",\"9th\"\n\"Task order 3\",\"4th\",\"2nd\",\"9th\""}, {"type": "text", "value": "Figure 25: Attention map comparison among different orderings with ER and RESNET-T on three selected tasks from LIBERO-LONG:\n\n$$\\text{put both the alphabet soup and the tomato sauce in the basket,}$$\n\n$$\\text{put the white mug on the left plate and put the yellow and white mug on the right plate, and}$$\n\n$$\\text{put the yellow and white mug in the microwave and close it.}$$\n\nEach row corresponds to a specific sequence of task ordering, and the caption of each attention map indicates the order of the task in that sequence.\n\nFindings: Figure 25 shows attention map comparisons of three different task orderings. We show two immediately learned tasks from LIBERO-LONG trained with ER and RESNET-T. We observe that:\n\n1. As expected, learning the same task at different positions in the task stream results in different attention visualization.\n2. There seems to be a trend that the policy has a more spread-out attention when it learns on tasks that are later in the sequence.\n\n42", "md": "Figure 25: Attention map comparison among different orderings with ER and RESNET-T on three selected tasks from LIBERO-LONG:\n\n$$\\text{put both the alphabet soup and the tomato sauce in the basket,}$$\n\n$$\\text{put the white mug on the left plate and put the yellow and white mug on the right plate, and}$$\n\n$$\\text{put the yellow and white mug in the microwave and close it.}$$\n\nEach row corresponds to a specific sequence of task ordering, and the caption of each attention map indicates the order of the task in that sequence.\n\nFindings: Figure 25 shows attention map comparisons of three different task orderings. We show two immediately learned tasks from LIBERO-LONG trained with ER and RESNET-T. We observe that:\n\n1. As expected, learning the same task at different positions in the task stream results in different attention visualization.\n2. There seems to be a trend that the policy has a more spread-out attention when it learns on tasks that are later in the sequence.\n\n42"}]}, {"page": 43, "text": "With or Without Pretraining\n            wlo pretrained\n            ER\n            w/  pretrained\n            ER\n            w/o pretrained\n            PackNet\n            w/ pretrained\n            PackNet\n            wlo pretrained\n            EWC\n            w/  pretrained\n            EWC                        put both the           put  the white mug on   put the yellow and\n                                       alphabet  soup and     the left plate  and     white mug in the\n                                       the tomato sauce in     put theyellow and      microwave and close\n                                       the basket              white mug on the       it\n                                                               right plate\n Figure 26: Attention map comparison between models without/with pretrained models using RESNET-\nT and different lifelong learning algorithms on three selected tasks from LIBERO-LONG.\n                                                            43", "md": "# With or Without Pretraining\n\n## With or Without Pretraining\n\n| |w/o pretrained|w/ pretrained|\n|---|---|---|\n|ER|PackNet|PackNet|\n|EWC|put both the alphabet soup and the tomato sauce in the basket|put the yellow and white mug on the left plate and put the yellow and white mug on the right plate|\n\nFigure 26: Attention map comparison between models without/with pretrained models using RESNET-T and different lifelong learning algorithms on three selected tasks from LIBERO-LONG.", "images": [{"name": "img_p42_1", "height": 1465, "width": 1010}], "items": [{"type": "heading", "lvl": 1, "value": "With or Without Pretraining", "md": "# With or Without Pretraining"}, {"type": "heading", "lvl": 2, "value": "With or Without Pretraining", "md": "## With or Without Pretraining"}, {"type": "table", "rows": [["", "w/o pretrained", "w/ pretrained"], ["ER", "PackNet", "PackNet"], ["EWC", "put both the alphabet soup and the tomato sauce in the basket", "put the yellow and white mug on the left plate and put the yellow and white mug on the right plate"]], "md": "| |w/o pretrained|w/ pretrained|\n|---|---|---|\n|ER|PackNet|PackNet|\n|EWC|put both the alphabet soup and the tomato sauce in the basket|put the yellow and white mug on the left plate and put the yellow and white mug on the right plate|", "isPerfectTable": true, "csv": "\"\",\"w/o pretrained\",\"w/ pretrained\"\n\"ER\",\"PackNet\",\"PackNet\"\n\"EWC\",\"put both the alphabet soup and the tomato sauce in the basket\",\"put the yellow and white mug on the left plate and put the yellow and white mug on the right plate\""}, {"type": "text", "value": "Figure 26: Attention map comparison between models without/with pretrained models using RESNET-T and different lifelong learning algorithms on three selected tasks from LIBERO-LONG.", "md": "Figure 26: Attention map comparison between models without/with pretrained models using RESNET-T and different lifelong learning algorithms on three selected tasks from LIBERO-LONG."}]}, {"page": 44, "text": "Findings:      Figure 26 shows attention map comparisons between models with/without pretrained\n models on LIBERO-LONG                with RESNET-T and all three LL algorithms. We observe that:\n        1.  With pretraining, the policies attend to task-irrelevant regions more easily than those without\n            pretraining.\n        2.  Some of the policies with pretraining have better attention to the task-relevant features than\n            their counterparts without pertaining, but their performance remains lower (the last in the\n            second row and the second in the fourth row). This observation, again, shows that there is\n            no positive correlation between semantically meaningful attention maps and the policy\u2019s\n            performance.\n                                                                44", "md": "# Findings\n\nFigure 26 shows attention map comparisons between models with/without pretrained models on LIBERO-LONG with RESNET-T and all three LL algorithms. We observe that:\n\n1. With pretraining, the policies attend to task-irrelevant regions more easily than those without pretraining.\n2. Some of the policies with pretraining have better attention to the task-relevant features than their counterparts without pretraining, but their performance remains lower (the last in the second row and the second in the fourth row). This observation, again, shows that there is no positive correlation between semantically meaningful attention maps and the policy\u2019s performance.\n\n44", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Findings", "md": "# Findings"}, {"type": "text", "value": "Figure 26 shows attention map comparisons between models with/without pretrained models on LIBERO-LONG with RESNET-T and all three LL algorithms. We observe that:\n\n1. With pretraining, the policies attend to task-irrelevant regions more easily than those without pretraining.\n2. Some of the policies with pretraining have better attention to the task-relevant features than their counterparts without pretraining, but their performance remains lower (the last in the second row and the second in the fourth row). This observation, again, shows that there is no positive correlation between semantically meaningful attention maps and the policy\u2019s performance.\n\n44", "md": "Figure 26 shows attention map comparisons between models with/without pretrained models on LIBERO-LONG with RESNET-T and all three LL algorithms. We observe that:\n\n1. With pretraining, the policies attend to task-irrelevant regions more easily than those without pretraining.\n2. Some of the policies with pretraining have better attention to the task-relevant features than their counterparts without pretraining, but their performance remains lower (the last in the second row and the second in the fourth row). This observation, again, shows that there is no positive correlation between semantically meaningful attention maps and the policy\u2019s performance.\n\n44"}]}], "job_id": "30f965a5-9946-439a-a585-a5a37d06d8a3", "file_path": "./corpus/2306.03310.pdf"}