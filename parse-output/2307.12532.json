{"pages": [{"page": 1, "text": "                On the Connection between Pre-training Data Diversity and\n                                                     Fine-tuning Robustness\n                                               Vivek Ramanujan           \u2217\u2020          Thao Nguyen        \u2217\u2020\n                                    Sewoong Oh        \u2020\u2021         Ludwig Schmidt         \u2020\u00a7\u00b6          Ali Farhadi     \u2020\n                                                                      Abstract\n                       Pre-training has been widely adopted in deep learning to improve model performance,\n                  especially when the training data for a target task is limited. In our work, we seek to understand\n                  the implications of this training strategy on the generalization properties of downstream models.\n                  More specifically, we ask the following question: how do properties of the pre-training distribution\n                  affect the robustness of a fine-tuned model? The properties we explore include the label space,\n                  label semantics, image diversity, data domains, and data quantity of the pre-training distribution.\n                  We find that the primary factor influencing downstream effective robustness [43] is data quantity,\n                  while other factors have limited significance. For example, reducing the number of ImageNet\n                  pre-training classes by 4\u00d7      while increasing the number of images per class by 4\u00d7            (that is, keeping\n                  total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate\n                  our findings on pre-training distributions drawn from various natural and synthetic data sources,\n                  primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.\n           1     Introduction\n          Transfer learning is a popular technique to deal with data scarcity, improve training speed, or transfer\n           useful inductive biases that can benefit downstream tasks [7,                       9,  27]. In the domain of computer\n          vision, pre-training on ImageNet in particular has been the de-facto standard for obtaining features\n           to solve a wide range of vision tasks, such as object detection [8,                    17,  33], segmentation [5,       15], and\n           action recognition [41]. While there exists previous work that seeks to pinpoint specific properties\n           of ImageNet-trained features that benefit downstream performance [18,                            22,  23,  37], the analysis is\n           often done with respect to model accuracy. Our work instead examines the robustness of fine-tuned\n           models to natural distribution shifts. Instead of looking at architecture variations and pre-training\narXiv:2307.12532v1 [cs.CV] 24 Jul 2023algorithms as done in prior work [13,37,48], we focus on the role of the pre-training data. This\n           data-centric approach has been validated by past work [12,                   26,  28], which show that the training data\n           distribution plays a larger role than training method or architecture in influencing model robustness.\n           Robustness under distribution shifts is a fundamental concern for producing reliable machine learning\n           systems: a model can perform in unexpected and undesirable ways when there is a mismatch between\n               \u2217Equal Contribution\n               \u2020University of Washington\n               \u2021Google Research\n               \u00a7Allen Institute for Artificial Intelligence\n               \u00b6LAION                                                       1", "md": "# On the Connection between Pre-training Data Diversity and Fine-tuning Robustness\n\n# On the Connection between Pre-training Data Diversity and Fine-tuning Robustness\n\nVivek Ramanujan *\u2020 Thao Nguyen *\u2020\n\nSewoong Oh \u2020\u2021 Ludwig Schmidt \u2020\u00a7\u00b6 Ali Farhadi \u2020\n\n## Abstract\n\nPre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.\n\n### Introduction\n\nTransfer learning is a popular technique to deal with data scarcity, improve training speed, or transfer useful inductive biases that can benefit downstream tasks. In the domain of computer vision, pre-training on ImageNet in particular has been the de-facto standard for obtaining features to solve a wide range of vision tasks, such as object detection, segmentation, and action recognition. While there exists previous work that seeks to pinpoint specific properties of ImageNet-trained features that benefit downstream performance, the analysis is often done with respect to model accuracy. Our work instead examines the robustness of fine-tuned models to natural distribution shifts. Instead of looking at architecture variations and pre-training algorithms as done in prior work, we focus on the role of the pre-training data. This data-centric approach has been validated by past work, which show that the training data distribution plays a larger role than training method or architecture in influencing model robustness. Robustness under distribution shifts is a fundamental concern for producing reliable machine learning systems: a model can perform in unexpected and undesirable ways when there is a mismatch between.\n\n*Equal Contribution\n\n\u2020University of Washington\n\n\u2021Google Research\n\n\u00a7Allen Institute for Artificial Intelligence\n\n\u00b6LAION", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "On the Connection between Pre-training Data Diversity and Fine-tuning Robustness", "md": "# On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"}, {"type": "heading", "lvl": 1, "value": "On the Connection between Pre-training Data Diversity and Fine-tuning Robustness", "md": "# On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"}, {"type": "text", "value": "Vivek Ramanujan *\u2020 Thao Nguyen *\u2020\n\nSewoong Oh \u2020\u2021 Ludwig Schmidt \u2020\u00a7\u00b6 Ali Farhadi \u2020", "md": "Vivek Ramanujan *\u2020 Thao Nguyen *\u2020\n\nSewoong Oh \u2020\u2021 Ludwig Schmidt \u2020\u00a7\u00b6 Ali Farhadi \u2020"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.", "md": "Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "Transfer learning is a popular technique to deal with data scarcity, improve training speed, or transfer useful inductive biases that can benefit downstream tasks. In the domain of computer vision, pre-training on ImageNet in particular has been the de-facto standard for obtaining features to solve a wide range of vision tasks, such as object detection, segmentation, and action recognition. While there exists previous work that seeks to pinpoint specific properties of ImageNet-trained features that benefit downstream performance, the analysis is often done with respect to model accuracy. Our work instead examines the robustness of fine-tuned models to natural distribution shifts. Instead of looking at architecture variations and pre-training algorithms as done in prior work, we focus on the role of the pre-training data. This data-centric approach has been validated by past work, which show that the training data distribution plays a larger role than training method or architecture in influencing model robustness. Robustness under distribution shifts is a fundamental concern for producing reliable machine learning systems: a model can perform in unexpected and undesirable ways when there is a mismatch between.\n\n*Equal Contribution\n\n\u2020University of Washington\n\n\u2021Google Research\n\n\u00a7Allen Institute for Artificial Intelligence\n\n\u00b6LAION", "md": "Transfer learning is a popular technique to deal with data scarcity, improve training speed, or transfer useful inductive biases that can benefit downstream tasks. In the domain of computer vision, pre-training on ImageNet in particular has been the de-facto standard for obtaining features to solve a wide range of vision tasks, such as object detection, segmentation, and action recognition. While there exists previous work that seeks to pinpoint specific properties of ImageNet-trained features that benefit downstream performance, the analysis is often done with respect to model accuracy. Our work instead examines the robustness of fine-tuned models to natural distribution shifts. Instead of looking at architecture variations and pre-training algorithms as done in prior work, we focus on the role of the pre-training data. This data-centric approach has been validated by past work, which show that the training data distribution plays a larger role than training method or architecture in influencing model robustness. Robustness under distribution shifts is a fundamental concern for producing reliable machine learning systems: a model can perform in unexpected and undesirable ways when there is a mismatch between.\n\n*Equal Contribution\n\n\u2020University of Washington\n\n\u2021Google Research\n\n\u00a7Allen Institute for Artificial Intelligence\n\n\u00b6LAION"}]}, {"page": 2, "text": "       Pretraining                       Finetuning                                    Evaluation\n    Pretraining Datasets\n       ImageNet  Diffusion                 Data distribution                  Test Sets\n     FractalDB iNaturalist\n                                                                       In Distribution Out of\n           \u2026                            Downstream Task                             Distribution\n           Model                             Model                               Model\n Figure 1: A summary of our experimental pipeline. We pre-train a model on a variety of different\n data distributions and evaluate its effective robustness after fine-tuning on a downstream task (i.e.,\n iWildCam). By examining many models in this manner, we can determine empirical properties of\n the pre-training distribution that are important for fine-tuning robustness.\n the data distribution encountered in deployment and the one on which the model is trained [21,                                  35].\n For example, a self-driving car should be able to generalize to a wide variety of weather scenarios to\n be considered safe, some of which it may not have seen during training. In our work, we focus on\n these forms of     natural    distribution shifts [21,      32]\u2014named so because they are induced by real-world\n processes\u2014and study what aspects of the source dataset could help fine-tuned models become more\n robust to these shifts. We tackle this question along five different ablation axes:                         (i)  Data quantity,\n(ii)  Label granularity,      (iii)  Label semantics,       (iv)  Image diversity, and        (v)  Data sources. Most of our\n experiments revolve around the supervised learning setting, which makes controlled experiments on\n the pre-training distribution more tractable and systematic. Through a better understanding of the\n interplay between various properties of the pre-training distribution and downstream robustness, we\n seek to establish guidelines for constructing better pre-training datasets for fine-tuning.\n Previous work by Miller et al.            [26]  experimented with a wide range of natural distribution shifts\n and found that pre-training on ImageNet yields the biggest improvement in robustness for the task\n of iWildCam-WILDS classification [2,                21]. Consequently, we use iWildCam-WILDS as a probe to\n evaluate how our interventions with the ImageNet pre-training distribution would alter the robustness\n trend uncovered in this previous work. We also analyze the use of other pre-training data sources that\n may differ significantly from ImageNet in both semantic content and data collection methodology.\n Our main findings can be summarized as follows:\n (i)     Data quantity.          Pre-training with more data helps boost robustness. However, we do not\n need a lot of pre-training data to see significant robustness gains: using 25K images subsampled\n from either ImageNet or iNaturalist, which is 6\u00d7                    smaller than the size of the fine-tuning dataset,\n already offers noticable robustness improvements.\n (ii)    Label granularity.          Making pre-training labels more coarse-grained lowers transfer robustness.\n The effect is less significant than altering data quantity: extreme reduction in label granularity (e.g.,\n                                                                  2", "md": "# Experimental Pipeline Summary\n\n## Experimental Pipeline Summary\n\nWe pre-train a model on a variety of different data distributions and evaluate its effective robustness after fine-tuning on a downstream task (i.e., iWildCam). By examining many models in this manner, we can determine empirical properties of the pre-training distribution that are important for fine-tuning robustness.\n\nThe data distribution encountered in deployment and the one on which the model is trained are crucial. For example, a self-driving car should be able to generalize to a wide variety of weather scenarios to be considered safe, some of which it may not have seen during training. In our work, we focus on natural distribution shifts\u2014named so because they are induced by real-world processes\u2014and study what aspects of the source dataset could help fine-tuned models become more robust to these shifts. We tackle this question along five different ablation axes: Data quantity, Label granularity, Label semantics, Image diversity, and Data sources. Most of our experiments revolve around the supervised learning setting, which makes controlled experiments on the pre-training distribution more tractable and systematic. Through a better understanding of the interplay between various properties of the pre-training distribution and downstream robustness, we seek to establish guidelines for constructing better pre-training datasets for fine-tuning.\n\nPrevious work by Miller et al. experimented with a wide range of natural distribution shifts and found that pre-training on ImageNet yields the biggest improvement in robustness for the task of iWildCam-WILDS classification. Consequently, we use iWildCam-WILDS as a probe to evaluate how our interventions with the ImageNet pre-training distribution would alter the robustness trend uncovered in this previous work. We also analyze the use of other pre-training data sources that may differ significantly from ImageNet in both semantic content and data collection methodology.\n\n### Main Findings:\n\n1. Data quantity: Pre-training with more data helps boost robustness. However, we do not need a lot of pre-training data to see significant robustness gains: using 25K images subsampled from either ImageNet or iNaturalist, which is 6 times smaller than the size of the fine-tuning dataset, already offers noticeable robustness improvements.\n2. Label granularity: Making pre-training labels more coarse-grained lowers transfer robustness. The effect is less significant than altering data quantity: extreme reduction in label granularity (e.g., ...)", "images": [{"name": "img_p1_1", "height": 191, "width": 191}, {"name": "img_p1_2", "height": 96, "width": 96}, {"name": "img_p1_3", "height": 96, "width": 96}, {"name": "img_p1_4", "height": 96, "width": 96}, {"name": "img_p1_5", "height": 96, "width": 96}, {"name": "img_p1_6", "height": 120, "width": 115}, {"name": "img_p1_7", "height": 120, "width": 115}], "items": [{"type": "heading", "lvl": 1, "value": "Experimental Pipeline Summary", "md": "# Experimental Pipeline Summary"}, {"type": "heading", "lvl": 2, "value": "Experimental Pipeline Summary", "md": "## Experimental Pipeline Summary"}, {"type": "text", "value": "We pre-train a model on a variety of different data distributions and evaluate its effective robustness after fine-tuning on a downstream task (i.e., iWildCam). By examining many models in this manner, we can determine empirical properties of the pre-training distribution that are important for fine-tuning robustness.\n\nThe data distribution encountered in deployment and the one on which the model is trained are crucial. For example, a self-driving car should be able to generalize to a wide variety of weather scenarios to be considered safe, some of which it may not have seen during training. In our work, we focus on natural distribution shifts\u2014named so because they are induced by real-world processes\u2014and study what aspects of the source dataset could help fine-tuned models become more robust to these shifts. We tackle this question along five different ablation axes: Data quantity, Label granularity, Label semantics, Image diversity, and Data sources. Most of our experiments revolve around the supervised learning setting, which makes controlled experiments on the pre-training distribution more tractable and systematic. Through a better understanding of the interplay between various properties of the pre-training distribution and downstream robustness, we seek to establish guidelines for constructing better pre-training datasets for fine-tuning.\n\nPrevious work by Miller et al. experimented with a wide range of natural distribution shifts and found that pre-training on ImageNet yields the biggest improvement in robustness for the task of iWildCam-WILDS classification. Consequently, we use iWildCam-WILDS as a probe to evaluate how our interventions with the ImageNet pre-training distribution would alter the robustness trend uncovered in this previous work. We also analyze the use of other pre-training data sources that may differ significantly from ImageNet in both semantic content and data collection methodology.", "md": "We pre-train a model on a variety of different data distributions and evaluate its effective robustness after fine-tuning on a downstream task (i.e., iWildCam). By examining many models in this manner, we can determine empirical properties of the pre-training distribution that are important for fine-tuning robustness.\n\nThe data distribution encountered in deployment and the one on which the model is trained are crucial. For example, a self-driving car should be able to generalize to a wide variety of weather scenarios to be considered safe, some of which it may not have seen during training. In our work, we focus on natural distribution shifts\u2014named so because they are induced by real-world processes\u2014and study what aspects of the source dataset could help fine-tuned models become more robust to these shifts. We tackle this question along five different ablation axes: Data quantity, Label granularity, Label semantics, Image diversity, and Data sources. Most of our experiments revolve around the supervised learning setting, which makes controlled experiments on the pre-training distribution more tractable and systematic. Through a better understanding of the interplay between various properties of the pre-training distribution and downstream robustness, we seek to establish guidelines for constructing better pre-training datasets for fine-tuning.\n\nPrevious work by Miller et al. experimented with a wide range of natural distribution shifts and found that pre-training on ImageNet yields the biggest improvement in robustness for the task of iWildCam-WILDS classification. Consequently, we use iWildCam-WILDS as a probe to evaluate how our interventions with the ImageNet pre-training distribution would alter the robustness trend uncovered in this previous work. We also analyze the use of other pre-training data sources that may differ significantly from ImageNet in both semantic content and data collection methodology."}, {"type": "heading", "lvl": 3, "value": "Main Findings:", "md": "### Main Findings:"}, {"type": "text", "value": "1. Data quantity: Pre-training with more data helps boost robustness. However, we do not need a lot of pre-training data to see significant robustness gains: using 25K images subsampled from either ImageNet or iNaturalist, which is 6 times smaller than the size of the fine-tuning dataset, already offers noticeable robustness improvements.\n2. Label granularity: Making pre-training labels more coarse-grained lowers transfer robustness. The effect is less significant than altering data quantity: extreme reduction in label granularity (e.g., ...)", "md": "1. Data quantity: Pre-training with more data helps boost robustness. However, we do not need a lot of pre-training data to see significant robustness gains: using 25K images subsampled from either ImageNet or iNaturalist, which is 6 times smaller than the size of the fine-tuning dataset, already offers noticeable robustness improvements.\n2. Label granularity: Making pre-training labels more coarse-grained lowers transfer robustness. The effect is less significant than altering data quantity: extreme reduction in label granularity (e.g., ...)"}]}, {"page": 3, "text": " using 5 coarse classes instead of 1000 fine-grained classes) still preserves some of the robustness gains\n compared to training from scratch.\n (iii)  Label semantics.           Given enough data and labels, pre-training on more semantically similar\n classes does not have a notable impact on the robustness of fine-tuned models. In particular, we\n find that pre-training on the 600 inanimate object categories in ImageNet yields the same effective\n robustness as pre-training on the 400 animal categories, despite the fact that the downstream task\n consists of only animal categories.\n (iv)   Image diversity.         Given the same pre-training label set and data quantity, increasing per-class\n diversity (e.g., by including more subclasses) has no effect on transfer robustness. In addition, the\n trade-off between having more classes and more images per class is negligible if the total number of\n samples is kept constant.\n (v)    Data sources.         We find that natural data sources (i.e., ImageNet, iNaturalist) yield similar\n downstream robustness when controlling for data quantity. Pre-training with synthetic fractal data\n is less effective at the same data quantity regime but still has some robustness gain to offer compared\n to training from scratch. Synthetic natural-looking data (e.g., generated by Stable Diffusion [34])\n can help close this gap between using natural data and synthetic fractal data.\n Overall we find that increasing pre-training data quantity and label granularity makes fine-tuned\n models more robust to distribution shifts. However, not all additional data is equally helpful. For\n instance, in the context of iWildCam-WILDS task, pre-training with natural-looking data offers\n much more robustness than using 10\u00d7                more synthetic fractal data.\n2      Background\nThe main motivation for our paper comes from the work by Huh et al.                        [18], which investigates various\n factors in ImageNet training that affect the quality of the features used subsequently for transfer\n learning. For our investigation, we shift the focus from accuracy to robustness against distribution\n shift, which has been a long-standing issue in machine learning [3,                         4, 30,  42].   In particular, we\n analyze the robustness of pre-trained features to natural distribution shifts observed in the real\nworld through the iWildCam-WILDS benchmark [21]. Furthermore, in contrast to Huh et al.                                       [18],\nwe experiment with a greater variety of more recent neural network architectures, in addition to\n exploring the use of synthetic pre-training data.\nA key goal in robustness is to reduce the impact of distribution shifts on the performance of a model.\n If model performances on in- and out-of-distribution test sets are plotted along the                        x and   y-axes of a\n scatter plot respectively, then a more robust model would lie closer to the diagonal                         y =  x  line. This\n notion of robustness was captured by Taori et al.                  [43]  under the term       effective robustness, which\n measures the difference between a model\u2019s actual OOD performance and what could be predicted\n from its ID performance (Figure 2). Miller et al.                 [26] adopted this effective robustness framework\n and evaluated hundreds of models on various distribution shift settings. The authors observed that\n ID performance is highly correlated with OOD performance. This linear trend mapping ID to OOD\n performance, and how close it is to the            y =  x  line, is what we use in our work to compare the quality\n of the pre-trained features.\n More notably, Miller et al.          [26]  discovered that on the iWildCam dataset, models trained from\n                                                                 3", "md": "Using 5 coarse classes instead of 1000 fine-grained classes) still preserves some of the robustness gains compared to training from scratch.\n\n(iii) Label semantics. Given enough data and labels, pre-training on more semantically similar classes does not have a notable impact on the robustness of fine-tuned models. In particular, we find that pre-training on the 600 inanimate object categories in ImageNet yields the same effective robustness as pre-training on the 400 animal categories, despite the fact that the downstream task consists of only animal categories.\n\n(iv) Image diversity. Given the same pre-training label set and data quantity, increasing per-class diversity (e.g., by including more subclasses) has no effect on transfer robustness. In addition, the trade-off between having more classes and more images per class is negligible if the total number of samples is kept constant.\n\n(v) Data sources. We find that natural data sources (i.e., ImageNet, iNaturalist) yield similar downstream robustness when controlling for data quantity. Pre-training with synthetic fractal data is less effective at the same data quantity regime but still has some robustness gain to offer compared to training from scratch. Synthetic natural-looking data (e.g., generated by Stable Diffusion [34]) can help close this gap between using natural data and synthetic fractal data.\n\nOverall we find that increasing pre-training data quantity and label granularity makes fine-tuned models more robust to distribution shifts. However, not all additional data is equally helpful. For instance, in the context of iWildCam-WILDS task, pre-training with natural-looking data offers much more robustness than using 10x more synthetic fractal data.\n\n## Background\n\nThe main motivation for our paper comes from the work by Huh et al. [18], which investigates various factors in ImageNet training that affect the quality of the features used subsequently for transfer learning. For our investigation, we shift the focus from accuracy to robustness against distribution shift, which has been a long-standing issue in machine learning [3, 4, 30, 42]. In particular, we analyze the robustness of pre-trained features to natural distribution shifts observed in the real world through the iWildCam-WILDS benchmark [21]. Furthermore, in contrast to Huh et al. [18], we experiment with a greater variety of more recent neural network architectures, in addition to exploring the use of synthetic pre-training data.\n\nA key goal in robustness is to reduce the impact of distribution shifts on the performance of a model. If model performances on in- and out-of-distribution test sets are plotted along the x and y-axes of a scatter plot respectively, then a more robust model would lie closer to the diagonal y = x line. This notion of robustness was captured by Taori et al. [43] under the term effective robustness, which measures the difference between a model\u2019s actual OOD performance and what could be predicted from its ID performance (Figure 2). Miller et al. [26] adopted this effective robustness framework and evaluated hundreds of models on various distribution shift settings. The authors observed that ID performance is highly correlated with OOD performance. This linear trend mapping ID to OOD performance, and how close it is to the y = x line, is what we use in our work to compare the quality of the pre-trained features.\n\nMore notably, Miller et al. [26] discovered that on the iWildCam dataset, models trained from", "images": [], "items": [{"type": "text", "value": "Using 5 coarse classes instead of 1000 fine-grained classes) still preserves some of the robustness gains compared to training from scratch.\n\n(iii) Label semantics. Given enough data and labels, pre-training on more semantically similar classes does not have a notable impact on the robustness of fine-tuned models. In particular, we find that pre-training on the 600 inanimate object categories in ImageNet yields the same effective robustness as pre-training on the 400 animal categories, despite the fact that the downstream task consists of only animal categories.\n\n(iv) Image diversity. Given the same pre-training label set and data quantity, increasing per-class diversity (e.g., by including more subclasses) has no effect on transfer robustness. In addition, the trade-off between having more classes and more images per class is negligible if the total number of samples is kept constant.\n\n(v) Data sources. We find that natural data sources (i.e., ImageNet, iNaturalist) yield similar downstream robustness when controlling for data quantity. Pre-training with synthetic fractal data is less effective at the same data quantity regime but still has some robustness gain to offer compared to training from scratch. Synthetic natural-looking data (e.g., generated by Stable Diffusion [34]) can help close this gap between using natural data and synthetic fractal data.\n\nOverall we find that increasing pre-training data quantity and label granularity makes fine-tuned models more robust to distribution shifts. However, not all additional data is equally helpful. For instance, in the context of iWildCam-WILDS task, pre-training with natural-looking data offers much more robustness than using 10x more synthetic fractal data.", "md": "Using 5 coarse classes instead of 1000 fine-grained classes) still preserves some of the robustness gains compared to training from scratch.\n\n(iii) Label semantics. Given enough data and labels, pre-training on more semantically similar classes does not have a notable impact on the robustness of fine-tuned models. In particular, we find that pre-training on the 600 inanimate object categories in ImageNet yields the same effective robustness as pre-training on the 400 animal categories, despite the fact that the downstream task consists of only animal categories.\n\n(iv) Image diversity. Given the same pre-training label set and data quantity, increasing per-class diversity (e.g., by including more subclasses) has no effect on transfer robustness. In addition, the trade-off between having more classes and more images per class is negligible if the total number of samples is kept constant.\n\n(v) Data sources. We find that natural data sources (i.e., ImageNet, iNaturalist) yield similar downstream robustness when controlling for data quantity. Pre-training with synthetic fractal data is less effective at the same data quantity regime but still has some robustness gain to offer compared to training from scratch. Synthetic natural-looking data (e.g., generated by Stable Diffusion [34]) can help close this gap between using natural data and synthetic fractal data.\n\nOverall we find that increasing pre-training data quantity and label granularity makes fine-tuned models more robust to distribution shifts. However, not all additional data is equally helpful. For instance, in the context of iWildCam-WILDS task, pre-training with natural-looking data offers much more robustness than using 10x more synthetic fractal data."}, {"type": "heading", "lvl": 2, "value": "Background", "md": "## Background"}, {"type": "text", "value": "The main motivation for our paper comes from the work by Huh et al. [18], which investigates various factors in ImageNet training that affect the quality of the features used subsequently for transfer learning. For our investigation, we shift the focus from accuracy to robustness against distribution shift, which has been a long-standing issue in machine learning [3, 4, 30, 42]. In particular, we analyze the robustness of pre-trained features to natural distribution shifts observed in the real world through the iWildCam-WILDS benchmark [21]. Furthermore, in contrast to Huh et al. [18], we experiment with a greater variety of more recent neural network architectures, in addition to exploring the use of synthetic pre-training data.\n\nA key goal in robustness is to reduce the impact of distribution shifts on the performance of a model. If model performances on in- and out-of-distribution test sets are plotted along the x and y-axes of a scatter plot respectively, then a more robust model would lie closer to the diagonal y = x line. This notion of robustness was captured by Taori et al. [43] under the term effective robustness, which measures the difference between a model\u2019s actual OOD performance and what could be predicted from its ID performance (Figure 2). Miller et al. [26] adopted this effective robustness framework and evaluated hundreds of models on various distribution shift settings. The authors observed that ID performance is highly correlated with OOD performance. This linear trend mapping ID to OOD performance, and how close it is to the y = x line, is what we use in our work to compare the quality of the pre-trained features.\n\nMore notably, Miller et al. [26] discovered that on the iWildCam dataset, models trained from", "md": "The main motivation for our paper comes from the work by Huh et al. [18], which investigates various factors in ImageNet training that affect the quality of the features used subsequently for transfer learning. For our investigation, we shift the focus from accuracy to robustness against distribution shift, which has been a long-standing issue in machine learning [3, 4, 30, 42]. In particular, we analyze the robustness of pre-trained features to natural distribution shifts observed in the real world through the iWildCam-WILDS benchmark [21]. Furthermore, in contrast to Huh et al. [18], we experiment with a greater variety of more recent neural network architectures, in addition to exploring the use of synthetic pre-training data.\n\nA key goal in robustness is to reduce the impact of distribution shifts on the performance of a model. If model performances on in- and out-of-distribution test sets are plotted along the x and y-axes of a scatter plot respectively, then a more robust model would lie closer to the diagonal y = x line. This notion of robustness was captured by Taori et al. [43] under the term effective robustness, which measures the difference between a model\u2019s actual OOD performance and what could be predicted from its ID performance (Figure 2). Miller et al. [26] adopted this effective robustness framework and evaluated hundreds of models on various distribution shift settings. The authors observed that ID performance is highly correlated with OOD performance. This linear trend mapping ID to OOD performance, and how close it is to the y = x line, is what we use in our work to compare the quality of the pre-trained features.\n\nMore notably, Miller et al. [26] discovered that on the iWildCam dataset, models trained from"}]}, {"page": 4, "text": "    50        y=x                                                 Figure 2: Effective robustness is defined as move-\n              ImageNet pretrained\n    40        Trained from scratch                                ment towards a classifier which is robust to distri-\n              ImageNet pretrained (Miller et al.)\n    30        Trained from scratch (Miller et al.)                bution shift (i.e., line     y  =  x). Using this metric,\n                                                                  Miller et al.   [26]  observes that for the iWildCam-\n    20                                                           WILDS task, models pre-trained on ImageNet\n                                                                  are much more robust than models trained from\n    10                                                            scratch. We reproduce these two trends and use\n                                          Effective               them as points of reference for our subsequent ex-\n   OOD5Test macro F1                      robustness              periments, in which we modify the pre-training\n                                                                  distribution and observe how our interventions\n                10         20     30     40   50    60     70     alter the robustness trend lines.\n                        ID Test macro F1                                Figure 3: We visualize the residuals of various\n     0.04                                                               architectures after fitting a linear trend that\n     0.02                                                               predicts OOD accuracy from ID accuracy. All\n     0.00                                                               models are pre-trained on the full ImageNet\n     0.02                                               alexnet         dataset and fine-tuned on iWildCam for 12\n                                                        densenet121     epochs. We observe that overall the residuals\n   Trend Residual0.04                                   resnet18        fluctuate around the          y  = 0    line and vary\n                                                        resnet34\n     0.06   0  1  2  3  4  5   6  7  8  9 10  11        resnet50        throughout the course of fine-tuning for most\n                           Epoch                                        architectures.\n scratch and those that have been pre-trained on ImageNet lie on distinct linear trends, with the\n latter exhibiting much more robustness. We replicate these reported trends in Figure 2. Motivated\n by this result, our work seeks to better understand what aspects of ImageNet pre-training contribute\n to the improved robustness on iWildCam, and how these aspects translate to other pre-training data\n sources.\n Previous work by Andreassen et al.           [1] has looked at effective robustness over the course of fine-tuning\n and found that pre-trained models exhibit high effective robustness in the middle of fine-tuning,\nwhich eventually decreases as the training proceeds. The paper also experimented with ImageNet as\n one of the pre-training data sources. In our investigation, as a sanity check to remove number of\n training epochs as a potential source of bias for the linear fit, we adopt the linear trend of models\n pre-trained on ImageNet and fine-tuned on iWildCam computed previously by Miller et al.                                  [26]  as\n the baseline. We then report the residuals from comparing actual OOD performance at different\n epochs to what could be predicted from the corresponding ID performance using this baseline. Refer\n to Figure 3 for more details. We find that in the context of iWildCam fine-tuning, at each epoch, the\n residuals from our architectures of choice concentrate around the                   y = 0   line and exhibit no particular\n trend. This in turn allows us to vary the number of fine-tuning epochs as a hyperparameter, and\n obtain models covering a wide range of test performances for the scatter plots.\n 3     Experimental Setup\nAs mentioned earlier, the downstream task of interest is wildlife classification with the iWildCam-\nWILDS dataset [21]: the input is a photo taken by a camera trap, and the output is one of 182\n                                                                 4", "md": "# Document\n\n50        $$y=x$$                                                 Figure 2: Effective robustness is defined as movement towards a classifier which is robust to distribution shift (i.e., line $$y=x$$). Using this metric, Miller et al. [26] observes that for the iWildCam-WILDS task, models pre-trained on ImageNet are much more robust than models trained from scratch. We reproduce these two trends and use them as points of reference for our subsequent experiments, in which we modify the pre-training distribution and observe how our interventions alter the robustness trend lines.\n\n| |10|20|30|40|50|60|70|\n|---|---|---|---|---|---|---|---|\n|OOD5Test macro F1|0.04| | | | | | |\n| |0.02| | | | | | |\n| |0.00| | | | | | |\n| |0.02| | | | | | |\n| | |alexnet| | | | | |\n| | |densenet121| | | | | |\n|Trend Residual|0.04| | | | | | |\n\n0.06   0  1  2  3  4  5   6  7  8  9 10  11        resnet50        scratch and those that have been pre-trained on ImageNet lie on distinct linear trends, with the latter exhibiting much more robustness. We replicate these reported trends in Figure 2. Motivated by this result, our work seeks to better understand what aspects of ImageNet pre-training contribute to the improved robustness on iWildCam, and how these aspects translate to other pre-training data sources.\n\nPrevious work by Andreassen et al. [1] has looked at effective robustness over the course of fine-tuning and found that pre-trained models exhibit high effective robustness in the middle of fine-tuning, which eventually decreases as the training proceeds. The paper also experimented with ImageNet as one of the pre-training data sources. In our investigation, as a sanity check to remove the number of training epochs as a potential source of bias for the linear fit, we adopt the linear trend of models pre-trained on ImageNet and fine-tuned on iWildCam computed previously by Miller et al. [26] as the baseline. We then report the residuals from comparing actual OOD performance at different epochs to what could be predicted from the corresponding ID performance using this baseline. Refer to Figure 3 for more details. We find that in the context of iWildCam fine-tuning, at each epoch, the residuals from our architectures of choice concentrate around the $$y=0$$ line and exhibit no particular trend. This in turn allows us to vary the number of fine-tuning epochs as a hyperparameter, and obtain models covering a wide range of test performances for the scatter plots.\n\n3     Experimental Setup\nAs mentioned earlier, the downstream task of interest is wildlife classification with the iWildCam-WILDS dataset [21]: the input is a photo taken by a camera trap, and the output is one of 182.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "50        $$y=x$$                                                 Figure 2: Effective robustness is defined as movement towards a classifier which is robust to distribution shift (i.e., line $$y=x$$). Using this metric, Miller et al. [26] observes that for the iWildCam-WILDS task, models pre-trained on ImageNet are much more robust than models trained from scratch. We reproduce these two trends and use them as points of reference for our subsequent experiments, in which we modify the pre-training distribution and observe how our interventions alter the robustness trend lines.", "md": "50        $$y=x$$                                                 Figure 2: Effective robustness is defined as movement towards a classifier which is robust to distribution shift (i.e., line $$y=x$$). Using this metric, Miller et al. [26] observes that for the iWildCam-WILDS task, models pre-trained on ImageNet are much more robust than models trained from scratch. We reproduce these two trends and use them as points of reference for our subsequent experiments, in which we modify the pre-training distribution and observe how our interventions alter the robustness trend lines."}, {"type": "table", "rows": [["", "10", "20", "30", "40", "50", "60", "70"], ["OOD5Test macro F1", "0.04", "", "", "", "", "", ""], ["", "0.02", "", "", "", "", "", ""], ["", "0.00", "", "", "", "", "", ""], ["", "0.02", "", "", "", "", "", ""], ["", "", "alexnet", "", "", "", "", ""], ["", "", "densenet121", "", "", "", "", ""], ["Trend Residual", "0.04", "", "", "", "", "", ""]], "md": "| |10|20|30|40|50|60|70|\n|---|---|---|---|---|---|---|---|\n|OOD5Test macro F1|0.04| | | | | | |\n| |0.02| | | | | | |\n| |0.00| | | | | | |\n| |0.02| | | | | | |\n| | |alexnet| | | | | |\n| | |densenet121| | | | | |\n|Trend Residual|0.04| | | | | | |", "isPerfectTable": true, "csv": "\"\",\"10\",\"20\",\"30\",\"40\",\"50\",\"60\",\"70\"\n\"OOD5Test macro F1\",\"0.04\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"0.02\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"0.00\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"0.02\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"\",\"alexnet\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"\",\"densenet121\",\"\",\"\",\"\",\"\",\"\"\n\"Trend Residual\",\"0.04\",\"\",\"\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "0.06   0  1  2  3  4  5   6  7  8  9 10  11        resnet50        scratch and those that have been pre-trained on ImageNet lie on distinct linear trends, with the latter exhibiting much more robustness. We replicate these reported trends in Figure 2. Motivated by this result, our work seeks to better understand what aspects of ImageNet pre-training contribute to the improved robustness on iWildCam, and how these aspects translate to other pre-training data sources.\n\nPrevious work by Andreassen et al. [1] has looked at effective robustness over the course of fine-tuning and found that pre-trained models exhibit high effective robustness in the middle of fine-tuning, which eventually decreases as the training proceeds. The paper also experimented with ImageNet as one of the pre-training data sources. In our investigation, as a sanity check to remove the number of training epochs as a potential source of bias for the linear fit, we adopt the linear trend of models pre-trained on ImageNet and fine-tuned on iWildCam computed previously by Miller et al. [26] as the baseline. We then report the residuals from comparing actual OOD performance at different epochs to what could be predicted from the corresponding ID performance using this baseline. Refer to Figure 3 for more details. We find that in the context of iWildCam fine-tuning, at each epoch, the residuals from our architectures of choice concentrate around the $$y=0$$ line and exhibit no particular trend. This in turn allows us to vary the number of fine-tuning epochs as a hyperparameter, and obtain models covering a wide range of test performances for the scatter plots.\n\n3     Experimental Setup\nAs mentioned earlier, the downstream task of interest is wildlife classification with the iWildCam-WILDS dataset [21]: the input is a photo taken by a camera trap, and the output is one of 182.", "md": "0.06   0  1  2  3  4  5   6  7  8  9 10  11        resnet50        scratch and those that have been pre-trained on ImageNet lie on distinct linear trends, with the latter exhibiting much more robustness. We replicate these reported trends in Figure 2. Motivated by this result, our work seeks to better understand what aspects of ImageNet pre-training contribute to the improved robustness on iWildCam, and how these aspects translate to other pre-training data sources.\n\nPrevious work by Andreassen et al. [1] has looked at effective robustness over the course of fine-tuning and found that pre-trained models exhibit high effective robustness in the middle of fine-tuning, which eventually decreases as the training proceeds. The paper also experimented with ImageNet as one of the pre-training data sources. In our investigation, as a sanity check to remove the number of training epochs as a potential source of bias for the linear fit, we adopt the linear trend of models pre-trained on ImageNet and fine-tuned on iWildCam computed previously by Miller et al. [26] as the baseline. We then report the residuals from comparing actual OOD performance at different epochs to what could be predicted from the corresponding ID performance using this baseline. Refer to Figure 3 for more details. We find that in the context of iWildCam fine-tuning, at each epoch, the residuals from our architectures of choice concentrate around the $$y=0$$ line and exhibit no particular trend. This in turn allows us to vary the number of fine-tuning epochs as a hyperparameter, and obtain models covering a wide range of test performances for the scatter plots.\n\n3     Experimental Setup\nAs mentioned earlier, the downstream task of interest is wildlife classification with the iWildCam-WILDS dataset [21]: the input is a photo taken by a camera trap, and the output is one of 182."}]}, {"page": 5, "text": "                 Training set size      Number of classes      Class distribution      Class hierarchy      Expert-labeled\n  ImageNet           1,281,167                 1,000             Class-balanced            WordNet                 No\n iNaturalist          579,184                  5,089               Long-tailed            Tree of life             Yes\n                   Table 1: Differences between the ImageNet and iNaturalist datasets.\ndifferent animal species. There are two test sets for evaluation: ID test data consists of images taken\nby the same camera traps as the training set, but on different days from the training and validation\n(ID) images. In contrast, OOD test data contains images taken by a disjoint set of camera traps\nfrom training and validation (ID) images. We include some examples of the geodiversity represented\nin each test split in Appendix Figure 13.                Following [21], we report the macro F1 scores of the\ntrained networks because this metric emphasizes performance on rare species, which is critical to the\nbiodiversity monitoring application that the dataset was designed for.\nPre-training datasets.            We use ImageNet [10] and iNaturalist [45] as the primary pre-training\ndistributions of interest, given their hierarchical structures, complexity, and relevance to the down-\nstream task. The two data sources also differ in many ways (Table 1), hence their pre-trained features\nmake for an informative comparison. We also include experiments with synthetic pre-training data\nby using Stable Diffusion [34] and the FractalDB-1k dataset [20].                         We will elaborate on this in\nSection 4.5.\nNetwork architectures.             To obtain data for plotting linear trends, we train a range of standard\nneural network architectures including ResNet [14], ResNext [47], DenseNet [19], AlexNet [24] and\nMobileNet-V3 [16]. In our scatter plots, besides varying the architectures, we also vary the number\nof fine-tuning epochs to obtain models with varying F1 scores. Appendix A contains further training\ndetails. While our focus is on supervised pre-training, we also report some additional results with\nCLIP [31] architecture in Section 5.\nIn the subsequent sections, we detail different interventions made to the pre-training distribution to\ndisentangle key properties of interest. We show the resulting linear trends in relation to the trends\nreplicated from previous work [26], which include models trained from scratch on iWildCam (solid\nblue line) as well as models pre-trained on ImageNet (solid cyan line). For each trend line, we show\n95% bootstrap confidence intervals for the linear fit.\n4     Experiment Results\n4.1     Effect of Data Quantity\nFirst, we experiment with reducing the pre-training set size. To remove potential confounding effects\nfrom a long-tailed data distribution, we ensure that the class distribution of our pre-training datasets\nis uniform. ImageNet is already class-balanced, but this is not the case for iNaturalist [45]. We\nexperiment with a 1000-class subset of iNaturalist using its most frequent classes. We further select\nimages within each class uniformly at random so that the number of samples is the same across all\nclasses. This results in a class-balanced training set of size 150K from iNaturalist. We repeat the\nsame procedure to obtain subsets of size 100K, 50K, 25K and 5K. A similar subsampling process is\ndone on ImageNet, using the full 1000-class dataset which already has a uniform label distribution.\nIn Figure 4, we observe that reducing the data quantity during pre-training lowers the effective\n                                                               5", "md": "# Experiment Results\n\n## Experiment Results\n\n### Effect of Data Quantity\n\nFirst, we experiment with reducing the pre-training set size. To remove potential confounding effects from a long-tailed data distribution, we ensure that the class distribution of our pre-training datasets is uniform. ImageNet is already class-balanced, but this is not the case for iNaturalist [45]. We experiment with a 1000-class subset of iNaturalist using its most frequent classes. We further select images within each class uniformly at random so that the number of samples is the same across all classes. This results in a class-balanced training set of size 150K from iNaturalist. We repeat the same procedure to obtain subsets of size 100K, 50K, 25K and 5K. A similar subsampling process is done on ImageNet, using the full 1000-class dataset which already has a uniform label distribution.\n\nIn Figure 4, we observe that reducing the data quantity during pre-training lowers the effective", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Experiment Results", "md": "# Experiment Results"}, {"type": "heading", "lvl": 2, "value": "Experiment Results", "md": "## Experiment Results"}, {"type": "heading", "lvl": 3, "value": "Effect of Data Quantity", "md": "### Effect of Data Quantity"}, {"type": "text", "value": "First, we experiment with reducing the pre-training set size. To remove potential confounding effects from a long-tailed data distribution, we ensure that the class distribution of our pre-training datasets is uniform. ImageNet is already class-balanced, but this is not the case for iNaturalist [45]. We experiment with a 1000-class subset of iNaturalist using its most frequent classes. We further select images within each class uniformly at random so that the number of samples is the same across all classes. This results in a class-balanced training set of size 150K from iNaturalist. We repeat the same procedure to obtain subsets of size 100K, 50K, 25K and 5K. A similar subsampling process is done on ImageNet, using the full 1000-class dataset which already has a uniform label distribution.\n\nIn Figure 4, we observe that reducing the data quantity during pre-training lowers the effective", "md": "First, we experiment with reducing the pre-training set size. To remove potential confounding effects from a long-tailed data distribution, we ensure that the class distribution of our pre-training datasets is uniform. ImageNet is already class-balanced, but this is not the case for iNaturalist [45]. We experiment with a 1000-class subset of iNaturalist using its most frequent classes. We further select images within each class uniformly at random so that the number of samples is the same across all classes. This results in a class-balanced training set of size 150K from iNaturalist. We repeat the same procedure to obtain subsets of size 100K, 50K, 25K and 5K. A similar subsampling process is done on ImageNet, using the full 1000-class dataset which already has a uniform label distribution.\n\nIn Figure 4, we observe that reducing the data quantity during pre-training lowers the effective"}]}, {"page": 6, "text": "                50         samples = 5k                              50         samples = 5k\n                40         samples = 25k                             40         samples = 25k\n                30         samples = 50k                             30         samples = 50k\n                           samples = 100k                                       samples = 100k\n                20         samples = 150k                            20         samples = 150k\n                10                                                   10\n               OOD Test macro F1                                   OOD Test macro F1\n                 5                                                    5\n                          10      20     30   40   50  60    70               10       20    30   40   50   60   70\n                                ID Test macro F1                                     ID Test macro F1\nFigure 4: Reducing the number of pre-training images randomly sampled from                            (left)   ImageNet and\n(right)    iNaturalist lowers the robustness linear trends of the fine-tuned models. However, using only\n25K pre-training samples (green line) still yields significant robustness improvements compared to\ntraining from scratch on 129K iWildCam images (dark blue line). We subsample iNaturalist and\nensure class balance, only including 1000 classes with the most number of samples.\nrobustness of fine-tuned models.              However, it is worth noting at 25K images, pre-training with\nsubsampled ImageNet and iNaturalist data still produces much more robust models compared to\ntraining from scratch. This is 6\u00d7            less data compared to what is used for fine-tuning. As a sanity\ncheck, we find that using only 5K samples (i.e., 5 examples per class) during pre-training yields\nroughly the same level of robustness as training from scratch on iWildCam.\n4.2      Effect of Label Granularity\nNext, we adapt a question raised previously by Huh et al.                  [18] to our investigation: how does varying\nthe number of pre-training classes affect downstream robustness?                           Following [18], we construct\nsupersets    of classes in ImageNet using the WordNet hierarchy. We use the maximum of the shortest\npath distance from the root of WordNet to a label to compute the depth of the current label set. We\nthen contract ImageNet label nodes along the shortest path to construct superclasses. Specifically, we\ninvestigate depths 2, 4, 5, 6, and 7, which result in class counts of 5, 17, 37, 85, and 232 respectively,\nin order to provide good coverage across a range of label granularities. Similarly, on iNaturalist, we\nuse the superclass information that comes with the dataset to collapse the label space from 5,089\nfine-grained classes to 13 coarse classes.\nFor ImageNet pre-training, we find that using the full 1000 classes provides the most robustness.\nHowever, when the label set size is reduced by four times (i.e., taking 232 superclasses at depth\n7), model robustness only decreases slightly. From then on, reducing the label set further to 85\nclasses (depth 6), and then 37 classes (depth 5), does not deteriorate the linear trend further. Only\nwhen we experiment with 17 classes (depth 4) do we find another noticeable reduction in effective\nrobustness. With 5 superclasses as the only pre-training labels (depth 2), pre-trained models are\nstill significantly more robust than models trained from scratch.\nOn iNaturalist, we also observe a similar downward shift in linear trend when we reduce the initial\nlabel space to its phylum. Refer to Figure 5 for more details. Overall these findings suggest that\nusing fine-grained labels during pre-training is better for learning representations that are robust to\n                                                                6", "md": "# Document\n\n## Reducing the number of pre-training images randomly sampled from ImageNet and iNaturalist\n\n(left) ImageNet and (right) iNaturalist lowers the robustness linear trends of the fine-tuned models. However, using only 25K pre-training samples (green line) still yields significant robustness improvements compared to training from scratch on 129K iWildCam images (dark blue line). We subsample iNaturalist and ensure class balance, only including 1000 classes with the most number of samples.\n\nHowever, it is worth noting at 25K images, pre-training with subsampled ImageNet and iNaturalist data still produces much more robust models compared to training from scratch. This is 6x less data compared to what is used for fine-tuning. As a sanity check, we find that using only 5K samples (i.e., 5 examples per class) during pre-training yields roughly the same level of robustness as training from scratch on iWildCam.\n\n### Effect of Label Granularity\n\nNext, we adapt a question raised previously by Huh et al. [18] to our investigation: how does varying the number of pre-training classes affect downstream robustness? Following [18], we construct supersets of classes in ImageNet using the WordNet hierarchy. We use the maximum of the shortest path distance from the root of WordNet to a label to compute the depth of the current label set. We then contract ImageNet label nodes along the shortest path to construct superclasses. Specifically, we investigate depths 2, 4, 5, 6, and 7, which result in class counts of 5, 17, 37, 85, and 232 respectively, in order to provide good coverage across a range of label granularities. Similarly, on iNaturalist, we use the superclass information that comes with the dataset to collapse the label space from 5,089 fine-grained classes to 13 coarse classes.\n\nFor ImageNet pre-training, we find that using the full 1000 classes provides the most robustness. However, when the label set size is reduced by four times (i.e., taking 232 superclasses at depth 7), model robustness only decreases slightly. From then on, reducing the label set further to 85 classes (depth 6), and then 37 classes (depth 5), does not deteriorate the linear trend further. Only when we experiment with 17 classes (depth 4) do we find another noticeable reduction in effective robustness. With 5 superclasses as the only pre-training labels (depth 2), pre-trained models are still significantly more robust than models trained from scratch.\n\nOn iNaturalist, we also observe a similar downward shift in linear trend when we reduce the initial label space to its phylum. Refer to Figure 5 for more details. Overall these findings suggest that using fine-grained labels during pre-training is better for learning representations that are robust to.\n\n#### Figure 4:\n\nReducing the number of pre-training images randomly sampled from ImageNet and iNaturalist lowers the robustness linear trends of the fine-tuned models. However, using only 25K pre-training samples (green line) still yields significant robustness improvements compared to training from scratch on 129K iWildCam images (dark blue line). We subsample iNaturalist and ensure class balance, only including 1000 classes with the most number of samples.\n\nHowever, it is worth noting at 25K images, pre-training with subsampled ImageNet and iNaturalist data still produces much more robust models compared to training from scratch. This is 6x less data compared to what is used for fine-tuning. As a sanity check, we find that using only 5K samples (i.e., 5 examples per class) during pre-training yields roughly the same level of robustness as training from scratch on iWildCam.\n\n#### Effect of Label Granularity\n\nNext, we adapt a question raised previously by Huh et al. [18] to our investigation: how does varying the number of pre-training classes affect downstream robustness? Following [18], we construct supersets of classes in ImageNet using the WordNet hierarchy. We use the maximum of the shortest path distance from the root of WordNet to a label to compute the depth of the current label set. We then contract ImageNet label nodes along the shortest path to construct superclasses. Specifically, we investigate depths 2, 4, 5, 6, and 7, which result in class counts of 5, 17, 37, 85, and 232 respectively, in order to provide good coverage across a range of label granularities. Similarly, on iNaturalist, we use the superclass information that comes with the dataset to collapse the label space from 5,089 fine-grained classes to 13 coarse classes.\n\nFor ImageNet pre-training, we find that using the full 1000 classes provides the most robustness. However, when the label set size is reduced by four times (i.e., taking 232 superclasses at depth 7), model robustness only decreases slightly. From then on, reducing the label set further to 85 classes (depth 6), and then 37 classes (depth 5), does not deteriorate the linear trend further. Only when we experiment with 17 classes (depth 4) do we find another noticeable reduction in effective robustness. With 5 superclasses as the only pre-training labels (depth 2), pre-trained models are still significantly more robust than models trained from scratch.\n\nOn iNaturalist, we also observe a similar downward shift in linear trend when we reduce the initial label space to its phylum. Refer to Figure 5 for more details. Overall these findings suggest that using fine-grained labels during pre-training is better for learning representations that are robust to.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Reducing the number of pre-training images randomly sampled from ImageNet and iNaturalist", "md": "## Reducing the number of pre-training images randomly sampled from ImageNet and iNaturalist"}, {"type": "text", "value": "(left) ImageNet and (right) iNaturalist lowers the robustness linear trends of the fine-tuned models. However, using only 25K pre-training samples (green line) still yields significant robustness improvements compared to training from scratch on 129K iWildCam images (dark blue line). We subsample iNaturalist and ensure class balance, only including 1000 classes with the most number of samples.\n\nHowever, it is worth noting at 25K images, pre-training with subsampled ImageNet and iNaturalist data still produces much more robust models compared to training from scratch. This is 6x less data compared to what is used for fine-tuning. As a sanity check, we find that using only 5K samples (i.e., 5 examples per class) during pre-training yields roughly the same level of robustness as training from scratch on iWildCam.", "md": "(left) ImageNet and (right) iNaturalist lowers the robustness linear trends of the fine-tuned models. However, using only 25K pre-training samples (green line) still yields significant robustness improvements compared to training from scratch on 129K iWildCam images (dark blue line). We subsample iNaturalist and ensure class balance, only including 1000 classes with the most number of samples.\n\nHowever, it is worth noting at 25K images, pre-training with subsampled ImageNet and iNaturalist data still produces much more robust models compared to training from scratch. This is 6x less data compared to what is used for fine-tuning. As a sanity check, we find that using only 5K samples (i.e., 5 examples per class) during pre-training yields roughly the same level of robustness as training from scratch on iWildCam."}, {"type": "heading", "lvl": 3, "value": "Effect of Label Granularity", "md": "### Effect of Label Granularity"}, {"type": "text", "value": "Next, we adapt a question raised previously by Huh et al. [18] to our investigation: how does varying the number of pre-training classes affect downstream robustness? Following [18], we construct supersets of classes in ImageNet using the WordNet hierarchy. We use the maximum of the shortest path distance from the root of WordNet to a label to compute the depth of the current label set. We then contract ImageNet label nodes along the shortest path to construct superclasses. Specifically, we investigate depths 2, 4, 5, 6, and 7, which result in class counts of 5, 17, 37, 85, and 232 respectively, in order to provide good coverage across a range of label granularities. Similarly, on iNaturalist, we use the superclass information that comes with the dataset to collapse the label space from 5,089 fine-grained classes to 13 coarse classes.\n\nFor ImageNet pre-training, we find that using the full 1000 classes provides the most robustness. However, when the label set size is reduced by four times (i.e., taking 232 superclasses at depth 7), model robustness only decreases slightly. From then on, reducing the label set further to 85 classes (depth 6), and then 37 classes (depth 5), does not deteriorate the linear trend further. Only when we experiment with 17 classes (depth 4) do we find another noticeable reduction in effective robustness. With 5 superclasses as the only pre-training labels (depth 2), pre-trained models are still significantly more robust than models trained from scratch.\n\nOn iNaturalist, we also observe a similar downward shift in linear trend when we reduce the initial label space to its phylum. Refer to Figure 5 for more details. Overall these findings suggest that using fine-grained labels during pre-training is better for learning representations that are robust to.", "md": "Next, we adapt a question raised previously by Huh et al. [18] to our investigation: how does varying the number of pre-training classes affect downstream robustness? Following [18], we construct supersets of classes in ImageNet using the WordNet hierarchy. We use the maximum of the shortest path distance from the root of WordNet to a label to compute the depth of the current label set. We then contract ImageNet label nodes along the shortest path to construct superclasses. Specifically, we investigate depths 2, 4, 5, 6, and 7, which result in class counts of 5, 17, 37, 85, and 232 respectively, in order to provide good coverage across a range of label granularities. Similarly, on iNaturalist, we use the superclass information that comes with the dataset to collapse the label space from 5,089 fine-grained classes to 13 coarse classes.\n\nFor ImageNet pre-training, we find that using the full 1000 classes provides the most robustness. However, when the label set size is reduced by four times (i.e., taking 232 superclasses at depth 7), model robustness only decreases slightly. From then on, reducing the label set further to 85 classes (depth 6), and then 37 classes (depth 5), does not deteriorate the linear trend further. Only when we experiment with 17 classes (depth 4) do we find another noticeable reduction in effective robustness. With 5 superclasses as the only pre-training labels (depth 2), pre-trained models are still significantly more robust than models trained from scratch.\n\nOn iNaturalist, we also observe a similar downward shift in linear trend when we reduce the initial label space to its phylum. Refer to Figure 5 for more details. Overall these findings suggest that using fine-grained labels during pre-training is better for learning representations that are robust to."}, {"type": "heading", "lvl": 4, "value": "Figure 4:", "md": "#### Figure 4:"}, {"type": "text", "value": "Reducing the number of pre-training images randomly sampled from ImageNet and iNaturalist lowers the robustness linear trends of the fine-tuned models. However, using only 25K pre-training samples (green line) still yields significant robustness improvements compared to training from scratch on 129K iWildCam images (dark blue line). We subsample iNaturalist and ensure class balance, only including 1000 classes with the most number of samples.\n\nHowever, it is worth noting at 25K images, pre-training with subsampled ImageNet and iNaturalist data still produces much more robust models compared to training from scratch. This is 6x less data compared to what is used for fine-tuning. As a sanity check, we find that using only 5K samples (i.e., 5 examples per class) during pre-training yields roughly the same level of robustness as training from scratch on iWildCam.", "md": "Reducing the number of pre-training images randomly sampled from ImageNet and iNaturalist lowers the robustness linear trends of the fine-tuned models. However, using only 25K pre-training samples (green line) still yields significant robustness improvements compared to training from scratch on 129K iWildCam images (dark blue line). We subsample iNaturalist and ensure class balance, only including 1000 classes with the most number of samples.\n\nHowever, it is worth noting at 25K images, pre-training with subsampled ImageNet and iNaturalist data still produces much more robust models compared to training from scratch. This is 6x less data compared to what is used for fine-tuning. As a sanity check, we find that using only 5K samples (i.e., 5 examples per class) during pre-training yields roughly the same level of robustness as training from scratch on iWildCam."}, {"type": "heading", "lvl": 4, "value": "Effect of Label Granularity", "md": "#### Effect of Label Granularity"}, {"type": "text", "value": "Next, we adapt a question raised previously by Huh et al. [18] to our investigation: how does varying the number of pre-training classes affect downstream robustness? Following [18], we construct supersets of classes in ImageNet using the WordNet hierarchy. We use the maximum of the shortest path distance from the root of WordNet to a label to compute the depth of the current label set. We then contract ImageNet label nodes along the shortest path to construct superclasses. Specifically, we investigate depths 2, 4, 5, 6, and 7, which result in class counts of 5, 17, 37, 85, and 232 respectively, in order to provide good coverage across a range of label granularities. Similarly, on iNaturalist, we use the superclass information that comes with the dataset to collapse the label space from 5,089 fine-grained classes to 13 coarse classes.\n\nFor ImageNet pre-training, we find that using the full 1000 classes provides the most robustness. However, when the label set size is reduced by four times (i.e., taking 232 superclasses at depth 7), model robustness only decreases slightly. From then on, reducing the label set further to 85 classes (depth 6), and then 37 classes (depth 5), does not deteriorate the linear trend further. Only when we experiment with 17 classes (depth 4) do we find another noticeable reduction in effective robustness. With 5 superclasses as the only pre-training labels (depth 2), pre-trained models are still significantly more robust than models trained from scratch.\n\nOn iNaturalist, we also observe a similar downward shift in linear trend when we reduce the initial label space to its phylum. Refer to Figure 5 for more details. Overall these findings suggest that using fine-grained labels during pre-training is better for learning representations that are robust to.", "md": "Next, we adapt a question raised previously by Huh et al. [18] to our investigation: how does varying the number of pre-training classes affect downstream robustness? Following [18], we construct supersets of classes in ImageNet using the WordNet hierarchy. We use the maximum of the shortest path distance from the root of WordNet to a label to compute the depth of the current label set. We then contract ImageNet label nodes along the shortest path to construct superclasses. Specifically, we investigate depths 2, 4, 5, 6, and 7, which result in class counts of 5, 17, 37, 85, and 232 respectively, in order to provide good coverage across a range of label granularities. Similarly, on iNaturalist, we use the superclass information that comes with the dataset to collapse the label space from 5,089 fine-grained classes to 13 coarse classes.\n\nFor ImageNet pre-training, we find that using the full 1000 classes provides the most robustness. However, when the label set size is reduced by four times (i.e., taking 232 superclasses at depth 7), model robustness only decreases slightly. From then on, reducing the label set further to 85 classes (depth 6), and then 37 classes (depth 5), does not deteriorate the linear trend further. Only when we experiment with 17 classes (depth 4) do we find another noticeable reduction in effective robustness. With 5 superclasses as the only pre-training labels (depth 2), pre-trained models are still significantly more robust than models trained from scratch.\n\nOn iNaturalist, we also observe a similar downward shift in linear trend when we reduce the initial label space to its phylum. Refer to Figure 5 for more details. Overall these findings suggest that using fine-grained labels during pre-training is better for learning representations that are robust to."}]}, {"page": 7, "text": "                50                                                  50\n                40                                                  40\n                30                                                  30\n                20                               5 classes          20\n                                                 17 classes\n                10                               37 classes         10\n               OOD Test macro F1                 85 classes        OOD Test macro F1        iNaturalist\n                 5                               232 classes          5                     Superclass iNaturalist\n                          10      20    30    40   50  60    70               10       20    30   40   50   60   70\n                                ID Test macro F1                                    ID Test macro F1\nFigure 5: Results of changing the label granularity of the pre-training task by combining classes\naccording to some semantics hierarchy to form supersets, for                 (left)   ImageNet and      (right)    iNaturalist.\nIn general, this intervention lowers model robustness on downstream task.                                However, extreme\nreduction of the pre-training label space, e.g. by 200\u00d7               in the case of ImageNet, still offers robustness\ngains compared to training from scratch.\ndistributions shifts in downstream tasks. But even if only coarse labels are available, pre-training\nwith enough data still has significant robustness benefits to offer. We note that when it comes to\nabsolute F1 scores, pre-training on more fine-grained labels also tends to lead to higher performance.\n4.3      Effect of Label Semantics\nThe number of pre-training classes seems to have an impact on downstream robustness, but does it\nmatter what kind of classes models are pre-trained on? We next investigate whether using classes\nwhose semantics are more aligned with the downstream task would improve robustness.\nTo do so, we separately pre-train models on ImageNet classes that are subsets of the \u201cobject\u201d and\n\u201canimal\u201d WordNet synsets. This yields 2 broad categories that are similar in total sample size, each\nhaving around 600K images. In Figure 6, we find that models trained on \u201canimal\u201d classes (yellow\nline) exhibit slightly higher F1 scores, but roughly the same effective robustness as models trained\non \"object\" classes (green line). This is surprising given that the fine-tuning distribution, iWildCam,\ncontains only images of animals in the wild, which are semantically more similar to the animal\nclasses of ImageNet. It is also worth noting that models pre-trained on \u201cobject\u201d classes are also\nmuch more robust than models trained from scratch (blue line).\nOne potential confounder to this experiment setup is that some images from \u201cobject\u201d classes also\ncontain animals (i.e., co-occurrences that are not accounted for by ImageNet labels). To estimate\nthe extent of this problem, we use TensorFlow\u2019s ImageNet2012 multilabel set [40], containing 20k\nImageNet validation data with multi-class labels reviewed by experts. We find that 1.1% of the data\nhave labels from both \u201canimal\u201d and \u201cobject\u201d classes present in the same image, suggesting that the\nlabel co-occurrence issue only impacts a small fraction of training data. Consequently, to explain\nthe result of Figure 6, we posit that training on a diverse set of classes in general helps the model\npick up on useful invariances that in turn lead to similar downstream robustness. We explore this\nhypothesis further in Section 4.5 with synthetic training data. 7", "md": "# Document\n\n## Results of changing the label granularity of the pre-training task\n\nFigure 5: Results of changing the label granularity of the pre-training task by combining classes according to some semantics hierarchy to form supersets, for (left) ImageNet and (right) iNaturalist.\n\nIn general, this intervention lowers model robustness on downstream task. However, extreme reduction of the pre-training label space, e.g. by 200\u00d7 in the case of ImageNet, still offers robustness gains compared to training from scratch.\n\nDistributions shifts in downstream tasks. But even if only coarse labels are available, pre-training with enough data still has significant robustness benefits to offer. We note that when it comes to absolute F1 scores, pre-training on more fine-grained labels also tends to lead to higher performance.\n\n### Effect of Label Semantics\n\nThe number of pre-training classes seems to have an impact on downstream robustness, but does it matter what kind of classes models are pre-trained on? We next investigate whether using classes whose semantics are more aligned with the downstream task would improve robustness.\n\nTo do so, we separately pre-train models on ImageNet classes that are subsets of the \u201cobject\u201d and \u201canimal\u201d WordNet synsets. This yields 2 broad categories that are similar in total sample size, each having around 600K images. In Figure 6, we find that models trained on \u201canimal\u201d classes (yellow line) exhibit slightly higher F1 scores, but roughly the same effective robustness as models trained on \"object\" classes (green line). This is surprising given that the fine-tuning distribution, iWildCam, contains only images of animals in the wild, which are semantically more similar to the animal classes of ImageNet. It is also worth noting that models pre-trained on \u201cobject\u201d classes are also much more robust than models trained from scratch (blue line).\n\nOne potential confounder to this experiment setup is that some images from \u201cobject\u201d classes also contain animals (i.e., co-occurrences that are not accounted for by ImageNet labels). To estimate the extent of this problem, we use TensorFlow\u2019s ImageNet2012 multilabel set [40], containing 20k ImageNet validation data with multi-class labels reviewed by experts. We find that 1.1% of the data have labels from both \u201canimal\u201d and \u201cobject\u201d classes present in the same image, suggesting that the label co-occurrence issue only impacts a small fraction of training data. Consequently, to explain the result of Figure 6, we posit that training on a diverse set of classes in general helps the model pick up on useful invariances that in turn lead to similar downstream robustness. We explore this hypothesis further in Section 4.5 with synthetic training data.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Results of changing the label granularity of the pre-training task", "md": "## Results of changing the label granularity of the pre-training task"}, {"type": "text", "value": "Figure 5: Results of changing the label granularity of the pre-training task by combining classes according to some semantics hierarchy to form supersets, for (left) ImageNet and (right) iNaturalist.\n\nIn general, this intervention lowers model robustness on downstream task. However, extreme reduction of the pre-training label space, e.g. by 200\u00d7 in the case of ImageNet, still offers robustness gains compared to training from scratch.\n\nDistributions shifts in downstream tasks. But even if only coarse labels are available, pre-training with enough data still has significant robustness benefits to offer. We note that when it comes to absolute F1 scores, pre-training on more fine-grained labels also tends to lead to higher performance.", "md": "Figure 5: Results of changing the label granularity of the pre-training task by combining classes according to some semantics hierarchy to form supersets, for (left) ImageNet and (right) iNaturalist.\n\nIn general, this intervention lowers model robustness on downstream task. However, extreme reduction of the pre-training label space, e.g. by 200\u00d7 in the case of ImageNet, still offers robustness gains compared to training from scratch.\n\nDistributions shifts in downstream tasks. But even if only coarse labels are available, pre-training with enough data still has significant robustness benefits to offer. We note that when it comes to absolute F1 scores, pre-training on more fine-grained labels also tends to lead to higher performance."}, {"type": "heading", "lvl": 3, "value": "Effect of Label Semantics", "md": "### Effect of Label Semantics"}, {"type": "text", "value": "The number of pre-training classes seems to have an impact on downstream robustness, but does it matter what kind of classes models are pre-trained on? We next investigate whether using classes whose semantics are more aligned with the downstream task would improve robustness.\n\nTo do so, we separately pre-train models on ImageNet classes that are subsets of the \u201cobject\u201d and \u201canimal\u201d WordNet synsets. This yields 2 broad categories that are similar in total sample size, each having around 600K images. In Figure 6, we find that models trained on \u201canimal\u201d classes (yellow line) exhibit slightly higher F1 scores, but roughly the same effective robustness as models trained on \"object\" classes (green line). This is surprising given that the fine-tuning distribution, iWildCam, contains only images of animals in the wild, which are semantically more similar to the animal classes of ImageNet. It is also worth noting that models pre-trained on \u201cobject\u201d classes are also much more robust than models trained from scratch (blue line).\n\nOne potential confounder to this experiment setup is that some images from \u201cobject\u201d classes also contain animals (i.e., co-occurrences that are not accounted for by ImageNet labels). To estimate the extent of this problem, we use TensorFlow\u2019s ImageNet2012 multilabel set [40], containing 20k ImageNet validation data with multi-class labels reviewed by experts. We find that 1.1% of the data have labels from both \u201canimal\u201d and \u201cobject\u201d classes present in the same image, suggesting that the label co-occurrence issue only impacts a small fraction of training data. Consequently, to explain the result of Figure 6, we posit that training on a diverse set of classes in general helps the model pick up on useful invariances that in turn lead to similar downstream robustness. We explore this hypothesis further in Section 4.5 with synthetic training data.", "md": "The number of pre-training classes seems to have an impact on downstream robustness, but does it matter what kind of classes models are pre-trained on? We next investigate whether using classes whose semantics are more aligned with the downstream task would improve robustness.\n\nTo do so, we separately pre-train models on ImageNet classes that are subsets of the \u201cobject\u201d and \u201canimal\u201d WordNet synsets. This yields 2 broad categories that are similar in total sample size, each having around 600K images. In Figure 6, we find that models trained on \u201canimal\u201d classes (yellow line) exhibit slightly higher F1 scores, but roughly the same effective robustness as models trained on \"object\" classes (green line). This is surprising given that the fine-tuning distribution, iWildCam, contains only images of animals in the wild, which are semantically more similar to the animal classes of ImageNet. It is also worth noting that models pre-trained on \u201cobject\u201d classes are also much more robust than models trained from scratch (blue line).\n\nOne potential confounder to this experiment setup is that some images from \u201cobject\u201d classes also contain animals (i.e., co-occurrences that are not accounted for by ImageNet labels). To estimate the extent of this problem, we use TensorFlow\u2019s ImageNet2012 multilabel set [40], containing 20k ImageNet validation data with multi-class labels reviewed by experts. We find that 1.1% of the data have labels from both \u201canimal\u201d and \u201cobject\u201d classes present in the same image, suggesting that the label co-occurrence issue only impacts a small fraction of training data. Consequently, to explain the result of Figure 6, we posit that training on a diverse set of classes in general helps the model pick up on useful invariances that in turn lead to similar downstream robustness. We explore this hypothesis further in Section 4.5 with synthetic training data."}]}, {"page": 8, "text": "   50           Animals                                       Figure 6: Varying the semantic category of classes\n   40           Objects\n   30                                                         included in the pre-training data yields similar ro-\n                                                              bustness linear trends, with pre-training on only\n   20                                                        \u201canimal\u201d classes exhibiting slightly higher F1 scores\n                                                              than pre-training on only \u201cobject\u201d classes. Even\n   10                                                         though the downstream task is animal classifica-\n   OOD Test macro F1                                          tion, models pre-trained only on \u201cobject\u201d classes\n     5                                                        are still much more robust than models that do\n              10        20    30    40    50   60    70       not undergo any pre-training.\n   50                ID Test macro F1\n   40                                                         Figure 7: We vary the number of classes randomly\n   30                                                         selected from the original 1000 ImageNet classes\n                                                              and adjust the number of images per class corre-\n   20                                                         spondingly, such that total image quantity is fixed\n                                                              at 60K. We observe that having 4\u00d7              more classes,\n   10                           # of classes = 50             or 4\u00d7  more images per class, induces the same level\n                                                              of robustness in fine-tuned models. Experiments\n   OOD Test macro F1            # of classes = 100           with 300K data regime can be found in Appendix\n     5                          # of classes = 200            Figure 18.\n              10        20    30    40    50   60    70\n                     ID Test macro F1\n4.4     Effect of Image Diversity\nBesides labels, another source of diversity comes from the training images themselves. We experiment\nwith two different notions of image diversity:            (i)  Number of categories collected, and           (ii) Per-class\nimage diversity.\n4.4.1     More Data Per Class vs. More Classes of Data\nA natural question arises when designing a dataset with a fixed data budget (or labeling cost):\nshould we collect more data from existing categories or more categories of data?                          To address this\nquestion, we keep the total number of images fixed while varying the number of classes of ImageNet\nused for pre-training. For example, if we have a budget of 60K images and 100 randomly selected\nImageNet classes, we sample 600 images uniformly at random from each of these classes (Figure\n7). Here, we find that in the context of the iWildCam distribution shift, there is no difference on\ndownstream robustness between having more data per class or having more classes, as long as the\ntotal number of images is constant. This observation also holds at a larger data quantity scale (300K\nimages, see Appendix Figure 18). This result demonstrates the dominant effect of data quantity\nover other aspects of the pre-training distribution (e.g., label set size).\n                                                              8", "md": "# Document\n\n## Animals\n\nFigure 6: Varying the semantic category of classes\n\n| |10|20|30|40|50|60|70|\n|---|---|---|---|---|---|---|---|\n|OOD Test macro F1|5| | | | | | |\n|ID Test macro F1| | | | |50|40|30|20|10|\n\n## Effect of Image Diversity\n\n4.4 Besides labels, another source of diversity comes from the training images themselves. We experiment with two different notions of image diversity: (i) Number of categories collected, and (ii) Per-class image diversity.\n\n### More Data Per Class vs. More Classes of Data\n\nA natural question arises when designing a dataset with a fixed data budget (or labeling cost): should we collect more data from existing categories or more categories of data? To address this question, we keep the total number of images fixed while varying the number of classes of ImageNet used for pre-training. For example, if we have a budget of 60K images and 100 randomly selected ImageNet classes, we sample 600 images uniformly at random from each of these classes (Figure 7). Here, we find that in the context of the iWildCam distribution shift, there is no difference on downstream robustness between having more data per class or having more classes, as long as the total number of images is constant. This observation also holds at a larger data quantity scale (300K images, see Appendix Figure 18). This result demonstrates the dominant effect of data quantity over other aspects of the pre-training distribution (e.g., label set size).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Animals", "md": "## Animals"}, {"type": "text", "value": "Figure 6: Varying the semantic category of classes", "md": "Figure 6: Varying the semantic category of classes"}, {"type": "table", "rows": [["", "10", "20", "30", "40", "50", "60", "70"], ["OOD Test macro F1", "5", "", "", "", "", "", ""], ["ID Test macro F1", "", "", "", "", "50", "40", "30", "20", "10"]], "md": "| |10|20|30|40|50|60|70|\n|---|---|---|---|---|---|---|---|\n|OOD Test macro F1|5| | | | | | |\n|ID Test macro F1| | | | |50|40|30|20|10|", "isPerfectTable": false, "csv": "\"\",\"10\",\"20\",\"30\",\"40\",\"50\",\"60\",\"70\"\n\"OOD Test macro F1\",\"5\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"ID Test macro F1\",\"\",\"\",\"\",\"\",\"50\",\"40\",\"30\",\"20\",\"10\""}, {"type": "heading", "lvl": 2, "value": "Effect of Image Diversity", "md": "## Effect of Image Diversity"}, {"type": "text", "value": "4.4 Besides labels, another source of diversity comes from the training images themselves. We experiment with two different notions of image diversity: (i) Number of categories collected, and (ii) Per-class image diversity.", "md": "4.4 Besides labels, another source of diversity comes from the training images themselves. We experiment with two different notions of image diversity: (i) Number of categories collected, and (ii) Per-class image diversity."}, {"type": "heading", "lvl": 3, "value": "More Data Per Class vs. More Classes of Data", "md": "### More Data Per Class vs. More Classes of Data"}, {"type": "text", "value": "A natural question arises when designing a dataset with a fixed data budget (or labeling cost): should we collect more data from existing categories or more categories of data? To address this question, we keep the total number of images fixed while varying the number of classes of ImageNet used for pre-training. For example, if we have a budget of 60K images and 100 randomly selected ImageNet classes, we sample 600 images uniformly at random from each of these classes (Figure 7). Here, we find that in the context of the iWildCam distribution shift, there is no difference on downstream robustness between having more data per class or having more classes, as long as the total number of images is constant. This observation also holds at a larger data quantity scale (300K images, see Appendix Figure 18). This result demonstrates the dominant effect of data quantity over other aspects of the pre-training distribution (e.g., label set size).", "md": "A natural question arises when designing a dataset with a fixed data budget (or labeling cost): should we collect more data from existing categories or more categories of data? To address this question, we keep the total number of images fixed while varying the number of classes of ImageNet used for pre-training. For example, if we have a budget of 60K images and 100 randomly selected ImageNet classes, we sample 600 images uniformly at random from each of these classes (Figure 7). Here, we find that in the context of the iWildCam distribution shift, there is no difference on downstream robustness between having more data per class or having more classes, as long as the total number of images is constant. This observation also holds at a larger data quantity scale (300K images, see Appendix Figure 18). This result demonstrates the dominant effect of data quantity over other aspects of the pre-training distribution (e.g., label set size)."}]}, {"page": 9, "text": "4.4.2      Image Diversity Within Each Class\nAnother way of modulating dataset diversity is by changing                      per-class   image diversity. For example,\ngiven a \u201cdog\u201d class, a dataset which only contains images of one dog breed could be seen as less\ndiverse than a dataset which has examples of several breeds. In order to construct a controlled\nexperiment, we use a quantitative heuristic for the diversity of each class: we fix certain superclasses\n(using the WordNet hierarchy) as the training labels and vary the number of corresponding subclasses\nwhere the images are taken from. For iNaturalist we can do the same with the tree of life structure.\nMore diversity means more subclasses chosen per superclass.\nFor the ImageNet distribution that is built on the WordNet hierarchy, we construct a subset following\nBREEDS [38]. The two main ways that BREEDS recalibrates the WordNet hierarchy fit our goals for\nimage diversity: (i) nodes are only included in the hierarchy if they convey some visual information,\nand (ii) nodes of similar specificity share the same distance from the root (e.g., \u201cdog\u201d and \u201ccat\u201d are\nnow both at the same depth even if the \u201cdog\u201d subtree is much larger). With this new hierarchy, we\nobtain 16 superclasses, each encompassing 12 subclasses (i.e., original ImageNet classes). The full list\ncan be found in Appendix C.1. We vary image diversity by changing the number of subclasses per\nsuperclass: 4, 8 or 12 subclasses\u2014corresponding to the diversity ratio                       p = 0.33, p   = 0.67, p    = 1.0  in\nthe left panel of Figure 8. To prevent data quantity from being a confounding variable, we subsample\nimages from each chosen subclass accordingly (e.g., if we reduce number of subclasses per superclass\nby 3\u00d7    then we sample 3\u00d7          more images from each subclass).\nFor iNaturalist, we fix the total number of images at 80K and apply the same procedure described\nabove to select a fraction of subclasses (see diversity ratios in the right panel of Figure 8), for\neach of the following superclasses: \u201cPlantae\u201d, \u201cInsecta\u201d, \u201cMammalia\u201d, \u201cFungi\u201d, \u201cAves\u201d, \u201cReptilia\u201d, and\n\u201cAmphibia.\u201d We choose this set of superclasses so we could have a uniform distribution of images\nper class while maintaining the same number of images as our ImageNet experiment. For more\ndetails, see Appendix C.1. As seen in Figure 8, for both ImageNet and iNaturalist, the resulting\nlinear trends are highly similar regardless of the diversity ratio                   p, or the number of subclasses per\nsuperclass. We conclude that in this case, per-class image diversity does not have a significant impact\non downstream robustness. Note that this does not hold in the extreme setting, e.g. repeating the\nsame image to produce a dataset.\n4.5      Pre-training with Different Data Sources\nMoving beyond interventions            within    each data distribution, we now compare fine-tuning robustness\nbehaviors     across   different pre-training data sources.\nCompared to ImageNet, iNaturalist exhibits different characteristics on multiple axes (see Table 1).\nWe expect that pre-training on the diverse, domain-specific species \u2013 which have been verified by\nnature enthusiasts \u2013 in iNaturalist will provide a boost on robustness for the downstream animal-\nin-the-wild classification task, compared to training on general web-curated classes in ImageNet.\nHowever, in Figure 10, we find that iNaturalist behaves similarly to ImageNet as a pre-training\ndata source. Even when we subsample iNaturalist to follow the ImageNet class distribution (refer\nto Section 4.1 for its construction), we observe a similar level of effective robustness compared to\nthe equal-sized 150K ImageNet subset (Figure 11). We hypothesize that when a certain level of\n\u201cdiversity\u201d is reached with the training images and labels, there is negligible robustness gain to be\nmade even if we increase the alignment between the pre-training and fine-tuning data domains.\n                                                                 9", "md": "## 4.4.2 Image Diversity Within Each Class\n\nAnother way of modulating dataset diversity is by changing per-class image diversity. For example, given a \u201cdog\u201d class, a dataset which only contains images of one dog breed could be seen as less diverse than a dataset which has examples of several breeds. In order to construct a controlled experiment, we use a quantitative heuristic for the diversity of each class: we fix certain superclasses (using the WordNet hierarchy) as the training labels and vary the number of corresponding subclasses where the images are taken from. For iNaturalist we can do the same with the tree of life structure. More diversity means more subclasses chosen per superclass.\n\nFor the ImageNet distribution that is built on the WordNet hierarchy, we construct a subset following BREEDS [38]. The two main ways that BREEDS recalibrates the WordNet hierarchy fit our goals for image diversity: (i) nodes are only included in the hierarchy if they convey some visual information, and (ii) nodes of similar specificity share the same distance from the root (e.g., \u201cdog\u201d and \u201ccat\u201d are now both at the same depth even if the \u201cdog\u201d subtree is much larger). With this new hierarchy, we obtain 16 superclasses, each encompassing 12 subclasses (i.e., original ImageNet classes). The full list can be found in Appendix C.1. We vary image diversity by changing the number of subclasses per superclass: 4, 8 or 12 subclasses\u2014corresponding to the diversity ratio $$p = 0.33, p = 0.67, p = 1.0$$ in the left panel of Figure 8. To prevent data quantity from being a confounding variable, we subsample images from each chosen subclass accordingly (e.g., if we reduce the number of subclasses per superclass by 3\u00d7 then we sample 3\u00d7 more images from each subclass).\n\nFor iNaturalist, we fix the total number of images at 80K and apply the same procedure described above to select a fraction of subclasses (see diversity ratios in the right panel of Figure 8), for each of the following superclasses: \u201cPlantae\u201d, \u201cInsecta\u201d, \u201cMammalia\u201d, \u201cFungi\u201d, \u201cAves\u201d, \u201cReptilia\u201d, and \u201cAmphibia.\u201d We choose this set of superclasses so we could have a uniform distribution of images per class while maintaining the same number of images as our ImageNet experiment. For more details, see Appendix C.1. As seen in Figure 8, for both ImageNet and iNaturalist, the resulting linear trends are highly similar regardless of the diversity ratio $$p$$, or the number of subclasses per superclass. We conclude that in this case, per-class image diversity does not have a significant impact on downstream robustness. Note that this does not hold in the extreme setting, e.g. repeating the same image to produce a dataset.\n\n## 4.5 Pre-training with Different Data Sources\n\nMoving beyond interventions within each data distribution, we now compare fine-tuning robustness behaviors across different pre-training data sources.\n\nCompared to ImageNet, iNaturalist exhibits different characteristics on multiple axes (see Table 1). We expect that pre-training on the diverse, domain-specific species \u2013 which have been verified by nature enthusiasts \u2013 in iNaturalist will provide a boost on robustness for the downstream animal-in-the-wild classification task, compared to training on general web-curated classes in ImageNet. However, in Figure 10, we find that iNaturalist behaves similarly to ImageNet as a pre-training data source. Even when we subsample iNaturalist to follow the ImageNet class distribution (refer to Section 4.1 for its construction), we observe a similar level of effective robustness compared to the equal-sized 150K ImageNet subset (Figure 11). We hypothesize that when a certain level of \u201cdiversity\u201d is reached with the training images and labels, there is negligible robustness gain to be made even if we increase the alignment between the pre-training and fine-tuning data domains.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "4.4.2 Image Diversity Within Each Class", "md": "## 4.4.2 Image Diversity Within Each Class"}, {"type": "text", "value": "Another way of modulating dataset diversity is by changing per-class image diversity. For example, given a \u201cdog\u201d class, a dataset which only contains images of one dog breed could be seen as less diverse than a dataset which has examples of several breeds. In order to construct a controlled experiment, we use a quantitative heuristic for the diversity of each class: we fix certain superclasses (using the WordNet hierarchy) as the training labels and vary the number of corresponding subclasses where the images are taken from. For iNaturalist we can do the same with the tree of life structure. More diversity means more subclasses chosen per superclass.\n\nFor the ImageNet distribution that is built on the WordNet hierarchy, we construct a subset following BREEDS [38]. The two main ways that BREEDS recalibrates the WordNet hierarchy fit our goals for image diversity: (i) nodes are only included in the hierarchy if they convey some visual information, and (ii) nodes of similar specificity share the same distance from the root (e.g., \u201cdog\u201d and \u201ccat\u201d are now both at the same depth even if the \u201cdog\u201d subtree is much larger). With this new hierarchy, we obtain 16 superclasses, each encompassing 12 subclasses (i.e., original ImageNet classes). The full list can be found in Appendix C.1. We vary image diversity by changing the number of subclasses per superclass: 4, 8 or 12 subclasses\u2014corresponding to the diversity ratio $$p = 0.33, p = 0.67, p = 1.0$$ in the left panel of Figure 8. To prevent data quantity from being a confounding variable, we subsample images from each chosen subclass accordingly (e.g., if we reduce the number of subclasses per superclass by 3\u00d7 then we sample 3\u00d7 more images from each subclass).\n\nFor iNaturalist, we fix the total number of images at 80K and apply the same procedure described above to select a fraction of subclasses (see diversity ratios in the right panel of Figure 8), for each of the following superclasses: \u201cPlantae\u201d, \u201cInsecta\u201d, \u201cMammalia\u201d, \u201cFungi\u201d, \u201cAves\u201d, \u201cReptilia\u201d, and \u201cAmphibia.\u201d We choose this set of superclasses so we could have a uniform distribution of images per class while maintaining the same number of images as our ImageNet experiment. For more details, see Appendix C.1. As seen in Figure 8, for both ImageNet and iNaturalist, the resulting linear trends are highly similar regardless of the diversity ratio $$p$$, or the number of subclasses per superclass. We conclude that in this case, per-class image diversity does not have a significant impact on downstream robustness. Note that this does not hold in the extreme setting, e.g. repeating the same image to produce a dataset.", "md": "Another way of modulating dataset diversity is by changing per-class image diversity. For example, given a \u201cdog\u201d class, a dataset which only contains images of one dog breed could be seen as less diverse than a dataset which has examples of several breeds. In order to construct a controlled experiment, we use a quantitative heuristic for the diversity of each class: we fix certain superclasses (using the WordNet hierarchy) as the training labels and vary the number of corresponding subclasses where the images are taken from. For iNaturalist we can do the same with the tree of life structure. More diversity means more subclasses chosen per superclass.\n\nFor the ImageNet distribution that is built on the WordNet hierarchy, we construct a subset following BREEDS [38]. The two main ways that BREEDS recalibrates the WordNet hierarchy fit our goals for image diversity: (i) nodes are only included in the hierarchy if they convey some visual information, and (ii) nodes of similar specificity share the same distance from the root (e.g., \u201cdog\u201d and \u201ccat\u201d are now both at the same depth even if the \u201cdog\u201d subtree is much larger). With this new hierarchy, we obtain 16 superclasses, each encompassing 12 subclasses (i.e., original ImageNet classes). The full list can be found in Appendix C.1. We vary image diversity by changing the number of subclasses per superclass: 4, 8 or 12 subclasses\u2014corresponding to the diversity ratio $$p = 0.33, p = 0.67, p = 1.0$$ in the left panel of Figure 8. To prevent data quantity from being a confounding variable, we subsample images from each chosen subclass accordingly (e.g., if we reduce the number of subclasses per superclass by 3\u00d7 then we sample 3\u00d7 more images from each subclass).\n\nFor iNaturalist, we fix the total number of images at 80K and apply the same procedure described above to select a fraction of subclasses (see diversity ratios in the right panel of Figure 8), for each of the following superclasses: \u201cPlantae\u201d, \u201cInsecta\u201d, \u201cMammalia\u201d, \u201cFungi\u201d, \u201cAves\u201d, \u201cReptilia\u201d, and \u201cAmphibia.\u201d We choose this set of superclasses so we could have a uniform distribution of images per class while maintaining the same number of images as our ImageNet experiment. For more details, see Appendix C.1. As seen in Figure 8, for both ImageNet and iNaturalist, the resulting linear trends are highly similar regardless of the diversity ratio $$p$$, or the number of subclasses per superclass. We conclude that in this case, per-class image diversity does not have a significant impact on downstream robustness. Note that this does not hold in the extreme setting, e.g. repeating the same image to produce a dataset."}, {"type": "heading", "lvl": 2, "value": "4.5 Pre-training with Different Data Sources", "md": "## 4.5 Pre-training with Different Data Sources"}, {"type": "text", "value": "Moving beyond interventions within each data distribution, we now compare fine-tuning robustness behaviors across different pre-training data sources.\n\nCompared to ImageNet, iNaturalist exhibits different characteristics on multiple axes (see Table 1). We expect that pre-training on the diverse, domain-specific species \u2013 which have been verified by nature enthusiasts \u2013 in iNaturalist will provide a boost on robustness for the downstream animal-in-the-wild classification task, compared to training on general web-curated classes in ImageNet. However, in Figure 10, we find that iNaturalist behaves similarly to ImageNet as a pre-training data source. Even when we subsample iNaturalist to follow the ImageNet class distribution (refer to Section 4.1 for its construction), we observe a similar level of effective robustness compared to the equal-sized 150K ImageNet subset (Figure 11). We hypothesize that when a certain level of \u201cdiversity\u201d is reached with the training images and labels, there is negligible robustness gain to be made even if we increase the alignment between the pre-training and fine-tuning data domains.", "md": "Moving beyond interventions within each data distribution, we now compare fine-tuning robustness behaviors across different pre-training data sources.\n\nCompared to ImageNet, iNaturalist exhibits different characteristics on multiple axes (see Table 1). We expect that pre-training on the diverse, domain-specific species \u2013 which have been verified by nature enthusiasts \u2013 in iNaturalist will provide a boost on robustness for the downstream animal-in-the-wild classification task, compared to training on general web-curated classes in ImageNet. However, in Figure 10, we find that iNaturalist behaves similarly to ImageNet as a pre-training data source. Even when we subsample iNaturalist to follow the ImageNet class distribution (refer to Section 4.1 for its construction), we observe a similar level of effective robustness compared to the equal-sized 150K ImageNet subset (Figure 11). We hypothesize that when a certain level of \u201cdiversity\u201d is reached with the training images and labels, there is negligible robustness gain to be made even if we increase the alignment between the pre-training and fine-tuning data domains."}]}, {"page": 10, "text": "                50        p = 0.33                                 50         p = 0.33\n                40        p = 0.67                                 40         p = 0.67\n                30        p = 1.0                                  30         p = 1.0\n                20                                                 20\n                10                                                 10\n              OOD Test macro F1                                   OOD Test macro F1\n                 5                                                  5\n                         10      20     30   40   50  60    70               10      20    30    40  50   60   70\n                               ID Test macro F1                                    ID Test macro F1\nFigure 8: We fix the total amount of pre-training data and the label space, while reducing the number\nof subclasses that constitute each superclass label in            (left)  ImageNet and      (right)    Naturalist. Smaller\np (diversity ratio) means proportionally fewer subclasses per superclass.                         We find that reducing\nper-class diversity by up to 3\u00d7          during pre-training has no effect on downstream robustness.\nFigure 9: Each grid in order shows random examples from the ImageNet ILSVRC 2012 challenge\ntrain set [10,    36], the iNaturalist 2017 challenge train set [45], the FractalDB-1k synthetic train\nset [20], and a 1000-class ImageNet-style synthetic dataset generated using Stable Diffusion [34].\n4.5.1     Synthetic Data Sources\nTo push our diversity hypothesis to the limit, we also pre-train the same set of architectures on\nFractalDB-1k dataset [20], which has similar class distribution to ImageNet but only contains\nsynthetic fractal images. Pre-training on FractalDB-1k has been shown to surpass the accuracy of\npre-training on ImageNet/Places [20]. For the task of iWildCam-WILDS, however, it is noticeably\nless effective at improving downstream robustness compared to natural image data (Figure 10).\nHowever, pre-training with fractal images still offers more robustness than training from scratch.\nCan we generate better synthetic data for pre-training than FractalDB-1k? We experiment with\nStable Diffusion [34], a popular diffusion model which generates high-quality images from natural\nlanguage prompts, to generate natural-looking images following the ImageNet class distribution.\nWe use 80 diverse prompts per ImageNet class from [31] to generate a 150K ImageNet-like dataset.\nExamples from this synthetic dataset can be seen in Figure 9. We find that pre-training on this dataset\nyields similar robust generalization behaviors as pre-training on the same quantity of ImageNet and\niNaturalist images (Figure 11). However, at a larger scale of 1M images, the robustness benefits of\npre-training with synthetic data begins to saturate and slightly lag behind iNaturalist and ImageNet.\n                                                              10", "md": "# Document\n\n50        $$ p = 0.33 $$                                 50         $$ p = 0.33 $$\n\n40        $$ p = 0.67 $$                                 40         $$ p = 0.67 $$\n\n30        $$ p = 1.0 $$                                  30         $$ p = 1.0 $$\n\n20                                                 20\n\n10                                                 10\n\nOOD Test macro F1                                   OOD Test macro F1\n\n5                                                  5\n\n10      20     30   40   50  60    70               10      20    30    40  50   60   70\n\nID Test macro F1\n\nFigure 8: We fix the total amount of pre-training data and the label space, while reducing the number\nof subclasses that constitute each superclass label in (left) ImageNet and (right) Naturalist. Smaller\n$$ p $$ (diversity ratio) means proportionally fewer subclasses per superclass. We find that reducing\nper-class diversity by up to 3\u00d7 during pre-training has no effect on downstream robustness.\n\nFigure 9: Each grid in order shows random examples from the ImageNet ILSVRC 2012 challenge\ntrain set [10, 36], the iNaturalist 2017 challenge train set [45], the FractalDB-1k synthetic train\nset [20], and a 1000-class ImageNet-style synthetic dataset generated using Stable Diffusion [34].\n\n#### Synthetic Data Sources\n\nTo push our diversity hypothesis to the limit, we also pre-train the same set of architectures on\nFractalDB-1k dataset [20], which has similar class distribution to ImageNet but only contains\nsynthetic fractal images. Pre-training on FractalDB-1k has been shown to surpass the accuracy of\npre-training on ImageNet/Places [20]. For the task of iWildCam-WILDS, however, it is noticeably\nless effective at improving downstream robustness compared to natural image data (Figure 10).\nHowever, pre-training with fractal images still offers more robustness than training from scratch.\nCan we generate better synthetic data for pre-training than FractalDB-1k? We experiment with\nStable Diffusion [34], a popular diffusion model which generates high-quality images from natural\nlanguage prompts, to generate natural-looking images following the ImageNet class distribution.\nWe use 80 diverse prompts per ImageNet class from [31] to generate a 150K ImageNet-like dataset.\nExamples from this synthetic dataset can be seen in Figure 9. We find that pre-training on this dataset\nyields similar robust generalization behaviors as pre-training on the same quantity of ImageNet and\niNaturalist images (Figure 11). However, at a larger scale of 1M images, the robustness benefits of\npre-training with synthetic data begins to saturate and slightly lag behind iNaturalist and ImageNet.", "images": [{"name": "img_p9_1", "height": 337, "width": 337}, {"name": "img_p9_2", "height": 337, "width": 337}, {"name": "img_p9_3", "height": 337, "width": 337}, {"name": "img_p9_4", "height": 337, "width": 337}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "50        $$ p = 0.33 $$                                 50         $$ p = 0.33 $$\n\n40        $$ p = 0.67 $$                                 40         $$ p = 0.67 $$\n\n30        $$ p = 1.0 $$                                  30         $$ p = 1.0 $$\n\n20                                                 20\n\n10                                                 10\n\nOOD Test macro F1                                   OOD Test macro F1\n\n5                                                  5\n\n10      20     30   40   50  60    70               10      20    30    40  50   60   70\n\nID Test macro F1\n\nFigure 8: We fix the total amount of pre-training data and the label space, while reducing the number\nof subclasses that constitute each superclass label in (left) ImageNet and (right) Naturalist. Smaller\n$$ p $$ (diversity ratio) means proportionally fewer subclasses per superclass. We find that reducing\nper-class diversity by up to 3\u00d7 during pre-training has no effect on downstream robustness.\n\nFigure 9: Each grid in order shows random examples from the ImageNet ILSVRC 2012 challenge\ntrain set [10, 36], the iNaturalist 2017 challenge train set [45], the FractalDB-1k synthetic train\nset [20], and a 1000-class ImageNet-style synthetic dataset generated using Stable Diffusion [34].", "md": "50        $$ p = 0.33 $$                                 50         $$ p = 0.33 $$\n\n40        $$ p = 0.67 $$                                 40         $$ p = 0.67 $$\n\n30        $$ p = 1.0 $$                                  30         $$ p = 1.0 $$\n\n20                                                 20\n\n10                                                 10\n\nOOD Test macro F1                                   OOD Test macro F1\n\n5                                                  5\n\n10      20     30   40   50  60    70               10      20    30    40  50   60   70\n\nID Test macro F1\n\nFigure 8: We fix the total amount of pre-training data and the label space, while reducing the number\nof subclasses that constitute each superclass label in (left) ImageNet and (right) Naturalist. Smaller\n$$ p $$ (diversity ratio) means proportionally fewer subclasses per superclass. We find that reducing\nper-class diversity by up to 3\u00d7 during pre-training has no effect on downstream robustness.\n\nFigure 9: Each grid in order shows random examples from the ImageNet ILSVRC 2012 challenge\ntrain set [10, 36], the iNaturalist 2017 challenge train set [45], the FractalDB-1k synthetic train\nset [20], and a 1000-class ImageNet-style synthetic dataset generated using Stable Diffusion [34]."}, {"type": "heading", "lvl": 4, "value": "Synthetic Data Sources", "md": "#### Synthetic Data Sources"}, {"type": "text", "value": "To push our diversity hypothesis to the limit, we also pre-train the same set of architectures on\nFractalDB-1k dataset [20], which has similar class distribution to ImageNet but only contains\nsynthetic fractal images. Pre-training on FractalDB-1k has been shown to surpass the accuracy of\npre-training on ImageNet/Places [20]. For the task of iWildCam-WILDS, however, it is noticeably\nless effective at improving downstream robustness compared to natural image data (Figure 10).\nHowever, pre-training with fractal images still offers more robustness than training from scratch.\nCan we generate better synthetic data for pre-training than FractalDB-1k? We experiment with\nStable Diffusion [34], a popular diffusion model which generates high-quality images from natural\nlanguage prompts, to generate natural-looking images following the ImageNet class distribution.\nWe use 80 diverse prompts per ImageNet class from [31] to generate a 150K ImageNet-like dataset.\nExamples from this synthetic dataset can be seen in Figure 9. We find that pre-training on this dataset\nyields similar robust generalization behaviors as pre-training on the same quantity of ImageNet and\niNaturalist images (Figure 11). However, at a larger scale of 1M images, the robustness benefits of\npre-training with synthetic data begins to saturate and slightly lag behind iNaturalist and ImageNet.", "md": "To push our diversity hypothesis to the limit, we also pre-train the same set of architectures on\nFractalDB-1k dataset [20], which has similar class distribution to ImageNet but only contains\nsynthetic fractal images. Pre-training on FractalDB-1k has been shown to surpass the accuracy of\npre-training on ImageNet/Places [20]. For the task of iWildCam-WILDS, however, it is noticeably\nless effective at improving downstream robustness compared to natural image data (Figure 10).\nHowever, pre-training with fractal images still offers more robustness than training from scratch.\nCan we generate better synthetic data for pre-training than FractalDB-1k? We experiment with\nStable Diffusion [34], a popular diffusion model which generates high-quality images from natural\nlanguage prompts, to generate natural-looking images following the ImageNet class distribution.\nWe use 80 diverse prompts per ImageNet class from [31] to generate a 150K ImageNet-like dataset.\nExamples from this synthetic dataset can be seen in Figure 9. We find that pre-training on this dataset\nyields similar robust generalization behaviors as pre-training on the same quantity of ImageNet and\niNaturalist images (Figure 11). However, at a larger scale of 1M images, the robustness benefits of\npre-training with synthetic data begins to saturate and slightly lag behind iNaturalist and ImageNet."}]}, {"page": 11, "text": "   50           iNaturalist                                   Figure 10: Pre-training on a noisy, long-tailed dis-\n   40           FractalDB-1k\n   30                                                         tribution of natural images like iNaturalist (red\n                                                              line) does not change the robustness on down-\n   20                                                         stream task, compared to pre-training on a clean,\n                                                              class-balanced dataset like ImageNet (cyan line).\n   10                                                         Pre-training on the same quantity of synthetic\n   OOD Test macro F1                                          fractal data (FractalDB-1k) yields much lower ro-\n     5                                                        bustness (green line), but still has some benefits\n              10        20    30    40    50   60    70       compared to training from scratch (dark blue line).\n   50                ID Test macro F1                         Figure 11:     Similar to the results in Figure 10,\n                iNaturalist 150k                              with a budget of 150K images, pre-training on\n   40           ImageNet 150k\n   30           Diffusion 150k                                FractalDB-1k is significantly less effective than\n                FractalDB-1k 150k                             iNaturalist or ImageNet. However, we find that\n   20                                                         generating natural-looking data with Stable Diffu-\n                                                              sion can close this gap. All datasets are subsam-\n   10                                                         pled to be class-balanced, each having 1000 classes.\n   OOD Test macro F1                                          We note that at larger pre-training data regime\n     5                                                        (e.g., 1M samples, see Figure 19), the effective-\n                                                              ness of Stable Diffusion data begins to lag behind\n              10        20    30    40    50   60    70       natural data from ImageNet and iNaturalist.\n                     ID Test macro F1\nSee Appendix Figure 19 for more details.\nOverall our findings demonstrate that while nuances in image semantics during pre-training are not\nimportant for fine-tuning robustness (e.g., \u201canimal\u201d versus \u201cobject\u201d classes, or iNaturalist versus\nImageNet), it is still beneficial to match the general characteristics of the downstream data (e.g.,\n\u201cnatural-looking\" images).\n5     Self-supervised Pre-training\nPrevious experiments revolve around the supervised learning settings. However, it is increasingly\ncommon to pre-train on          self-supervised     tasks using web-crawled corpora, which has been shown\nto significantly improve robustness to distribution shifts [31]. Our preliminary experiments with\npre-trained CLIP models [31] on iWildCam show that the resulting ID and OOD performances still\nlie on the ImageNet pre-training linear trend (i.e., cyan line), despite the self-supervised training\nmechanism and the much larger training dataset of CLIP. Varying CLIP\u2019s data sources only move\nthe F1 scores along the same line (Figure 12).\nWe also repeat the experiments with varying data quantity (Section 4.1) in the context of CLIP\u2019s\nimage-text pre-training data. Refer to Appendix D for more details. We leave an extensive evaluation\nof how \u201cdiversity\u201d of the pre-training distribution should be defined and measured differently on\nopen-vocabulary web datasets to future work.\n                                                             11", "md": "# Document\n\n## Figure 10:\n\nPre-training on a noisy, long-tailed distribution of natural images like iNaturalist (red line) does not change the robustness on downstream task, compared to pre-training on a clean, class-balanced dataset like ImageNet (cyan line).\n\nPre-training on the same quantity of synthetic fractal data (FractalDB-1k) yields much lower robustness (green line), but still has some benefits compared to training from scratch (dark blue line).\n\n## Figure 11:\n\nSimilar to the results in Figure 10, with a budget of 150K images, pre-training on FractalDB-1k is significantly less effective than iNaturalist or ImageNet. However, we find that generating natural-looking data with Stable Diffusion can close this gap. All datasets are subsampled to be class-balanced, each having 1000 classes.\n\nWe note that at larger pre-training data regime (e.g., 1M samples, see Figure 19), the effectiveness of Stable Diffusion data begins to lag behind natural data from ImageNet and iNaturalist.\n\nSee Appendix Figure 19 for more details.\n\nOverall our findings demonstrate that while nuances in image semantics during pre-training are not important for fine-tuning robustness (e.g., \u201canimal\u201d versus \u201cobject\u201d classes, or iNaturalist versus ImageNet), it is still beneficial to match the general characteristics of the downstream data (e.g., \u201cnatural-looking\" images).\n\n## Self-supervised Pre-training\n\nPrevious experiments revolve around the supervised learning settings. However, it is increasingly common to pre-train on self-supervised tasks using web-crawled corpora, which has been shown to significantly improve robustness to distribution shifts [31]. Our preliminary experiments with pre-trained CLIP models [31] on iWildCam show that the resulting ID and OOD performances still lie on the ImageNet pre-training linear trend (i.e., cyan line), despite the self-supervised training mechanism and the much larger training dataset of CLIP. Varying CLIP\u2019s data sources only move the F1 scores along the same line (Figure 12).\n\nWe also repeat the experiments with varying data quantity (Section 4.1) in the context of CLIP\u2019s image-text pre-training data. Refer to Appendix D for more details. We leave an extensive evaluation of how \u201cdiversity\u201d of the pre-training distribution should be defined and measured differently on open-vocabulary web datasets to future work.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Figure 10:", "md": "## Figure 10:"}, {"type": "text", "value": "Pre-training on a noisy, long-tailed distribution of natural images like iNaturalist (red line) does not change the robustness on downstream task, compared to pre-training on a clean, class-balanced dataset like ImageNet (cyan line).\n\nPre-training on the same quantity of synthetic fractal data (FractalDB-1k) yields much lower robustness (green line), but still has some benefits compared to training from scratch (dark blue line).", "md": "Pre-training on a noisy, long-tailed distribution of natural images like iNaturalist (red line) does not change the robustness on downstream task, compared to pre-training on a clean, class-balanced dataset like ImageNet (cyan line).\n\nPre-training on the same quantity of synthetic fractal data (FractalDB-1k) yields much lower robustness (green line), but still has some benefits compared to training from scratch (dark blue line)."}, {"type": "heading", "lvl": 2, "value": "Figure 11:", "md": "## Figure 11:"}, {"type": "text", "value": "Similar to the results in Figure 10, with a budget of 150K images, pre-training on FractalDB-1k is significantly less effective than iNaturalist or ImageNet. However, we find that generating natural-looking data with Stable Diffusion can close this gap. All datasets are subsampled to be class-balanced, each having 1000 classes.\n\nWe note that at larger pre-training data regime (e.g., 1M samples, see Figure 19), the effectiveness of Stable Diffusion data begins to lag behind natural data from ImageNet and iNaturalist.\n\nSee Appendix Figure 19 for more details.\n\nOverall our findings demonstrate that while nuances in image semantics during pre-training are not important for fine-tuning robustness (e.g., \u201canimal\u201d versus \u201cobject\u201d classes, or iNaturalist versus ImageNet), it is still beneficial to match the general characteristics of the downstream data (e.g., \u201cnatural-looking\" images).", "md": "Similar to the results in Figure 10, with a budget of 150K images, pre-training on FractalDB-1k is significantly less effective than iNaturalist or ImageNet. However, we find that generating natural-looking data with Stable Diffusion can close this gap. All datasets are subsampled to be class-balanced, each having 1000 classes.\n\nWe note that at larger pre-training data regime (e.g., 1M samples, see Figure 19), the effectiveness of Stable Diffusion data begins to lag behind natural data from ImageNet and iNaturalist.\n\nSee Appendix Figure 19 for more details.\n\nOverall our findings demonstrate that while nuances in image semantics during pre-training are not important for fine-tuning robustness (e.g., \u201canimal\u201d versus \u201cobject\u201d classes, or iNaturalist versus ImageNet), it is still beneficial to match the general characteristics of the downstream data (e.g., \u201cnatural-looking\" images)."}, {"type": "heading", "lvl": 2, "value": "Self-supervised Pre-training", "md": "## Self-supervised Pre-training"}, {"type": "text", "value": "Previous experiments revolve around the supervised learning settings. However, it is increasingly common to pre-train on self-supervised tasks using web-crawled corpora, which has been shown to significantly improve robustness to distribution shifts [31]. Our preliminary experiments with pre-trained CLIP models [31] on iWildCam show that the resulting ID and OOD performances still lie on the ImageNet pre-training linear trend (i.e., cyan line), despite the self-supervised training mechanism and the much larger training dataset of CLIP. Varying CLIP\u2019s data sources only move the F1 scores along the same line (Figure 12).\n\nWe also repeat the experiments with varying data quantity (Section 4.1) in the context of CLIP\u2019s image-text pre-training data. Refer to Appendix D for more details. We leave an extensive evaluation of how \u201cdiversity\u201d of the pre-training distribution should be defined and measured differently on open-vocabulary web datasets to future work.", "md": "Previous experiments revolve around the supervised learning settings. However, it is increasingly common to pre-train on self-supervised tasks using web-crawled corpora, which has been shown to significantly improve robustness to distribution shifts [31]. Our preliminary experiments with pre-trained CLIP models [31] on iWildCam show that the resulting ID and OOD performances still lie on the ImageNet pre-training linear trend (i.e., cyan line), despite the self-supervised training mechanism and the much larger training dataset of CLIP. Varying CLIP\u2019s data sources only move the F1 scores along the same line (Figure 12).\n\nWe also repeat the experiments with varying data quantity (Section 4.1) in the context of CLIP\u2019s image-text pre-training data. Refer to Appendix D for more details. We leave an extensive evaluation of how \u201cdiversity\u201d of the pre-training distribution should be defined and measured differently on open-vocabulary web datasets to future work."}]}, {"page": 12, "text": "    50                                                         Figure 12:      Results from fine-tuning pre-trained\n    40                                                         CLIP models on iWildCam-WILDS. The models\n    30                                                         we use include CLIP with ResNet-50 image encoder\n                                                               trained on YFCC-15M [44] and LAION-15M [39]\n    20                                                         separately, as well as the original CLIP models\n                                                               released by OpenAI [31], including 3 with ViT [11]\n    10                           OpenAI CLIP                   backbone trained on a dataset of size 400M. All\n                                                               of these models lie on the same linear trend as\n   OOD Test macro F1             LAION-15M (RN50)              that of ImageNet pre-training, demonstrating the\n     5                           YFCC-15M (RN50)               consistency of this trend across many pre-training\n               10       20     30    40    50   60    70       dataset size scales and training algorithms.\n                      ID Test macro F1\n6      Conclusion & Discussion\nIn this work, we find that many factors during pre-training such as label semantics and image\ndiversity, do not significantly alter the effective robustness of models fine-tuned on iWildCam-WILDS.\nThe more influential factors for downstream robust generalization are the                     quantity   of the pre-training\ndata and the      granularity     of the pre-training label set. Through experiments with Stable Diffusion,\nwe also demonstrate the potential of synthetic natural-looking images as a way to increase the\neffectiveness of the pre-training distribution along these two ablation axes.\nWe can think about pre-training dataset construction in terms of an explore vs. exploit trade-off.\nExploration, such as finding new data sources, is often time consuming, while exploiting, or collecting\nas much data as possible from an existing source, can sometimes be significantly easier.                                    Our\nexperiments suggest that a good approach to building pre-training dataset for robust generalization\nis to find a few data sources that exhibit robustness characteristics on a downstream task (e.g.,\nStable Diffusion data), and then collect as many samples from these sources as possible.\nIt is important to note that we are studying a different model behavior from Huh et al.                            [18]. Some\ninterventions can reduce average performance while maintaining effective robustness (e.g., label\ngranularity in Figure 5) and vice-versa (e.g., architecture modifications). Thus, certain choices during\npre-training dataset design depend fundamentally on the goals of the dataset. For example, whether\nwe want to achieve consistent performance across many settings (i.e., robustness) or optimize for\nvery good performance on one specific application changes the applicability of our results.\nAn important open question is determining what characteristic of the iWildCam-WILDS task leads\nto the difference in linear trend between pre-training and training from scratch. Some other datasets\n(e.g., fMoW-WILDS, see [6,             21]) do not exhibit this behavior after fine-tuning.                 Therefore, it is\nimportant to pinpoint distribution shifts where pre-training can provide a significant boost in effective\nrobustness. Finding a unifying property among such datasets would allow for better interpretation\nof our current results. As a first step in this direction, in Appendix E, we look into distribution\nshift settings constructed from the DomainNet benchmark [29], and observe that pre-training and\ntraining from scratch only produce different linear trends for certain pairs of domains and not others.\nRefer to this Appendix for further discussion.\n                                                               12", "md": "# Document\n\nFigure 12: Results from fine-tuning pre-trained CLIP models on iWildCam-WILDS. The models we use include CLIP with ResNet-50 image encoder trained on YFCC-15M [44] and LAION-15M [39] separately, as well as the original CLIP models released by OpenAI [31], including 3 with ViT [11] backbone trained on a dataset of size 400M. All of these models lie on the same linear trend as that of ImageNet pre-training, demonstrating the consistency of this trend across many pre-training dataset size scales and training algorithms.\n\n| |ID Test macro F1|\n|---|---|\n|OpenAI CLIP|LAION-15M (RN50)|\n| |YFCC-15M (RN50)|\n\nConclusion & Discussion\n\nIn this work, we find that many factors during pre-training such as label semantics and image diversity, do not significantly alter the effective robustness of models fine-tuned on iWildCam-WILDS. The more influential factors for downstream robust generalization are the quantity of the pre-training data and the granularity of the pre-training label set. Through experiments with Stable Diffusion, we also demonstrate the potential of synthetic natural-looking images as a way to increase the effectiveness of the pre-training distribution along these two ablation axes.\n\nWe can think about pre-training dataset construction in terms of an explore vs. exploit trade-off. Exploration, such as finding new data sources, is often time consuming, while exploiting, or collecting as much data as possible from an existing source, can sometimes be significantly easier. Our experiments suggest that a good approach to building pre-training dataset for robust generalization is to find a few data sources that exhibit robustness characteristics on a downstream task (e.g., Stable Diffusion data), and then collect as many samples from these sources as possible.\n\nIt is important to note that we are studying a different model behavior from Huh et al. [18]. Some interventions can reduce average performance while maintaining effective robustness (e.g., label granularity in Figure 5) and vice-versa (e.g., architecture modifications). Thus, certain choices during pre-training dataset design depend fundamentally on the goals of the dataset. For example, whether we want to achieve consistent performance across many settings (i.e., robustness) or optimize for very good performance on one specific application changes the applicability of our results.\n\nAn important open question is determining what characteristic of the iWildCam-WILDS task leads to the difference in linear trend between pre-training and training from scratch. Some other datasets (e.g., fMoW-WILDS, see [6, 21]) do not exhibit this behavior after fine-tuning. Therefore, it is important to pinpoint distribution shifts where pre-training can provide a significant boost in effective robustness. Finding a unifying property among such datasets would allow for better interpretation of our current results. As a first step in this direction, in Appendix E, we look into distribution shift settings constructed from the DomainNet benchmark [29], and observe that pre-training and training from scratch only produce different linear trends for certain pairs of domains and not others. Refer to this Appendix for further discussion.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Figure 12: Results from fine-tuning pre-trained CLIP models on iWildCam-WILDS. The models we use include CLIP with ResNet-50 image encoder trained on YFCC-15M [44] and LAION-15M [39] separately, as well as the original CLIP models released by OpenAI [31], including 3 with ViT [11] backbone trained on a dataset of size 400M. All of these models lie on the same linear trend as that of ImageNet pre-training, demonstrating the consistency of this trend across many pre-training dataset size scales and training algorithms.", "md": "Figure 12: Results from fine-tuning pre-trained CLIP models on iWildCam-WILDS. The models we use include CLIP with ResNet-50 image encoder trained on YFCC-15M [44] and LAION-15M [39] separately, as well as the original CLIP models released by OpenAI [31], including 3 with ViT [11] backbone trained on a dataset of size 400M. All of these models lie on the same linear trend as that of ImageNet pre-training, demonstrating the consistency of this trend across many pre-training dataset size scales and training algorithms."}, {"type": "table", "rows": [["", "ID Test macro F1"], ["OpenAI CLIP", "LAION-15M (RN50)"], ["", "YFCC-15M (RN50)"]], "md": "| |ID Test macro F1|\n|---|---|\n|OpenAI CLIP|LAION-15M (RN50)|\n| |YFCC-15M (RN50)|", "isPerfectTable": true, "csv": "\"\",\"ID Test macro F1\"\n\"OpenAI CLIP\",\"LAION-15M (RN50)\"\n\"\",\"YFCC-15M (RN50)\""}, {"type": "text", "value": "Conclusion & Discussion\n\nIn this work, we find that many factors during pre-training such as label semantics and image diversity, do not significantly alter the effective robustness of models fine-tuned on iWildCam-WILDS. The more influential factors for downstream robust generalization are the quantity of the pre-training data and the granularity of the pre-training label set. Through experiments with Stable Diffusion, we also demonstrate the potential of synthetic natural-looking images as a way to increase the effectiveness of the pre-training distribution along these two ablation axes.\n\nWe can think about pre-training dataset construction in terms of an explore vs. exploit trade-off. Exploration, such as finding new data sources, is often time consuming, while exploiting, or collecting as much data as possible from an existing source, can sometimes be significantly easier. Our experiments suggest that a good approach to building pre-training dataset for robust generalization is to find a few data sources that exhibit robustness characteristics on a downstream task (e.g., Stable Diffusion data), and then collect as many samples from these sources as possible.\n\nIt is important to note that we are studying a different model behavior from Huh et al. [18]. Some interventions can reduce average performance while maintaining effective robustness (e.g., label granularity in Figure 5) and vice-versa (e.g., architecture modifications). Thus, certain choices during pre-training dataset design depend fundamentally on the goals of the dataset. For example, whether we want to achieve consistent performance across many settings (i.e., robustness) or optimize for very good performance on one specific application changes the applicability of our results.\n\nAn important open question is determining what characteristic of the iWildCam-WILDS task leads to the difference in linear trend between pre-training and training from scratch. Some other datasets (e.g., fMoW-WILDS, see [6, 21]) do not exhibit this behavior after fine-tuning. Therefore, it is important to pinpoint distribution shifts where pre-training can provide a significant boost in effective robustness. Finding a unifying property among such datasets would allow for better interpretation of our current results. As a first step in this direction, in Appendix E, we look into distribution shift settings constructed from the DomainNet benchmark [29], and observe that pre-training and training from scratch only produce different linear trends for certain pairs of domains and not others. Refer to this Appendix for further discussion.", "md": "Conclusion & Discussion\n\nIn this work, we find that many factors during pre-training such as label semantics and image diversity, do not significantly alter the effective robustness of models fine-tuned on iWildCam-WILDS. The more influential factors for downstream robust generalization are the quantity of the pre-training data and the granularity of the pre-training label set. Through experiments with Stable Diffusion, we also demonstrate the potential of synthetic natural-looking images as a way to increase the effectiveness of the pre-training distribution along these two ablation axes.\n\nWe can think about pre-training dataset construction in terms of an explore vs. exploit trade-off. Exploration, such as finding new data sources, is often time consuming, while exploiting, or collecting as much data as possible from an existing source, can sometimes be significantly easier. Our experiments suggest that a good approach to building pre-training dataset for robust generalization is to find a few data sources that exhibit robustness characteristics on a downstream task (e.g., Stable Diffusion data), and then collect as many samples from these sources as possible.\n\nIt is important to note that we are studying a different model behavior from Huh et al. [18]. Some interventions can reduce average performance while maintaining effective robustness (e.g., label granularity in Figure 5) and vice-versa (e.g., architecture modifications). Thus, certain choices during pre-training dataset design depend fundamentally on the goals of the dataset. For example, whether we want to achieve consistent performance across many settings (i.e., robustness) or optimize for very good performance on one specific application changes the applicability of our results.\n\nAn important open question is determining what characteristic of the iWildCam-WILDS task leads to the difference in linear trend between pre-training and training from scratch. Some other datasets (e.g., fMoW-WILDS, see [6, 21]) do not exhibit this behavior after fine-tuning. Therefore, it is important to pinpoint distribution shifts where pre-training can provide a significant boost in effective robustness. Finding a unifying property among such datasets would allow for better interpretation of our current results. As a first step in this direction, in Appendix E, we look into distribution shift settings constructed from the DomainNet benchmark [29], and observe that pre-training and training from scratch only produce different linear trends for certain pairs of domains and not others. Refer to this Appendix for further discussion."}]}, {"page": 13, "text": "Acknowledgements\nThis work is supported in part by Open Philanthropy, the Allen Institute for AI, and NSF grants DMS-\n2134012 and CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML).\nAli Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-\n19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence,\nGoogle, and Apple.\nReferences\n  [1] A. Andreassen, Y. Bahri, B. Neyshabur, and R. Roelofs. The evolution of out-of-distribution\n      robustness throughout fine-tuning.            arXiv preprint arXiv:2106.15831, 2021.\n  [2] S. Beery, E. Cole, and A. Gjoka.               The iwildcam 2020 competition dataset.                  arXiv preprint\n      arXiv:2004.10340, 2020.\n  [3] B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning.\n      Pattern Recognition, 84:317\u2013331, 2018.\n  [4] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u0160rndi\u0107, P. Laskov, G. Giacinto, and F. Roli.\n      Evasion attacks against machine learning at test time. In                Joint European conference on machine\n      learning and knowledge discovery in databases, pages 387\u2013402. Springer, 2013.\n  [5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic\n      image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\n      IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\n  [6] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee. Functional map of the world. In                            CVPR,\n      2018.\n  [7] A. Conneau and D. Kiela. Senteval: An evaluation toolkit for universal sentence representations.\n      arXiv preprint arXiv:1803.05449, 2018.\n  [8] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional\n      networks.     Advances in neural information processing systems, 29, 2016.\n  [9] J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham,\n      X. Glorot, B. O\u2019Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis\n      and referral in retinal disease.        Nature medicine, 24(9):1342\u20131350, 2018.\n[10]  J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\n      image database. In        2009 IEEE conference on computer vision and pattern recognition, pages\n      248\u2013255. Ieee, 2009.\n[11]  A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\n      M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\n      image recognition at scale.         arXiv preprint arXiv:2010.11929, 2020.\n[12]  A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt.                                     Data\n      determines distributional robustness in contrastive language image pre-training (clip).                            arXiv\n      preprint arXiv:2205.01397, 2022.\n                                                              13", "md": "# Acknowledgements and References\n\n## Acknowledgements\n\nThis work is supported in part by Open Philanthropy, the Allen Institute for AI, and NSF grants DMS-2134012 and CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML).\n\nAli Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence, Google, and Apple.\n\n## References\n\n|[1]|A. Andreassen, Y. Bahri, B. Neyshabur, and R. Roelofs. The evolution of out-of-distribution robustness throughout fine-tuning. *arXiv preprint arXiv:2106.15831*, 2021.|\n|---|---|\n|[2]|S. Beery, E. Cole, and A. Gjoka. The iwildcam 2020 competition dataset. *arXiv preprint arXiv:2004.10340*, 2020.|\n|[3]|B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning. *Pattern Recognition, 84:317\u2013331*, 2018.|\n|[4]|B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u0160rndi\u0107, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In *Joint European conference on machine learning and knowledge discovery in databases*, pages 387\u2013402. Springer, 2013.|\n|[5]|L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. *IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848*, 2017.|\n|[6]|G. Christie, N. Fendley, J. Wilson, and R. Mukherjee. Functional map of the world. In *CVPR*, 2018.|\n|[7]|A. Conneau and D. Kiela. Senteval: An evaluation toolkit for universal sentence representations. *arXiv preprint arXiv:1803.05449*, 2018.|\n|[8]|J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. *Advances in neural information processing systems, 29*, 2016.|\n|[9]|J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham, X. Glorot, B. O\u2019Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. *Nature medicine, 24(9):1342\u20131350*, 2018.|\n|[10]|J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference on computer vision and pattern recognition*, pages 248\u2013255. Ieee, 2009.|\n|[11]|A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*, 2020.|\n|[12]|A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). *arXiv preprint arXiv:2205.01397*, 2022.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgements and References", "md": "# Acknowledgements and References"}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "This work is supported in part by Open Philanthropy, the Allen Institute for AI, and NSF grants DMS-2134012 and CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML).\n\nAli Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence, Google, and Apple.", "md": "This work is supported in part by Open Philanthropy, the Allen Institute for AI, and NSF grants DMS-2134012 and CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML).\n\nAli Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence, Google, and Apple."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "table", "rows": [["[1]", "A. Andreassen, Y. Bahri, B. Neyshabur, and R. Roelofs. The evolution of out-of-distribution robustness throughout fine-tuning. *arXiv preprint arXiv:2106.15831*, 2021."], ["[2]", "S. Beery, E. Cole, and A. Gjoka. The iwildcam 2020 competition dataset. *arXiv preprint arXiv:2004.10340*, 2020."], ["[3]", "B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning. *Pattern Recognition, 84:317\u2013331*, 2018."], ["[4]", "B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u0160rndi\u0107, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In *Joint European conference on machine learning and knowledge discovery in databases*, pages 387\u2013402. Springer, 2013."], ["[5]", "L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. *IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848*, 2017."], ["[6]", "G. Christie, N. Fendley, J. Wilson, and R. Mukherjee. Functional map of the world. In *CVPR*, 2018."], ["[7]", "A. Conneau and D. Kiela. Senteval: An evaluation toolkit for universal sentence representations. *arXiv preprint arXiv:1803.05449*, 2018."], ["[8]", "J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. *Advances in neural information processing systems, 29*, 2016."], ["[9]", "J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham, X. Glorot, B. O\u2019Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. *Nature medicine, 24(9):1342\u20131350*, 2018."], ["[10]", "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference on computer vision and pattern recognition*, pages 248\u2013255. Ieee, 2009."], ["[11]", "A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*, 2020."], ["[12]", "A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). *arXiv preprint arXiv:2205.01397*, 2022."]], "md": "|[1]|A. Andreassen, Y. Bahri, B. Neyshabur, and R. Roelofs. The evolution of out-of-distribution robustness throughout fine-tuning. *arXiv preprint arXiv:2106.15831*, 2021.|\n|---|---|\n|[2]|S. Beery, E. Cole, and A. Gjoka. The iwildcam 2020 competition dataset. *arXiv preprint arXiv:2004.10340*, 2020.|\n|[3]|B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning. *Pattern Recognition, 84:317\u2013331*, 2018.|\n|[4]|B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u0160rndi\u0107, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In *Joint European conference on machine learning and knowledge discovery in databases*, pages 387\u2013402. Springer, 2013.|\n|[5]|L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. *IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848*, 2017.|\n|[6]|G. Christie, N. Fendley, J. Wilson, and R. Mukherjee. Functional map of the world. In *CVPR*, 2018.|\n|[7]|A. Conneau and D. Kiela. Senteval: An evaluation toolkit for universal sentence representations. *arXiv preprint arXiv:1803.05449*, 2018.|\n|[8]|J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. *Advances in neural information processing systems, 29*, 2016.|\n|[9]|J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham, X. Glorot, B. O\u2019Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. *Nature medicine, 24(9):1342\u20131350*, 2018.|\n|[10]|J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference on computer vision and pattern recognition*, pages 248\u2013255. Ieee, 2009.|\n|[11]|A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*, 2020.|\n|[12]|A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). *arXiv preprint arXiv:2205.01397*, 2022.|", "isPerfectTable": true, "csv": "\"[1]\",\"A. Andreassen, Y. Bahri, B. Neyshabur, and R. Roelofs. The evolution of out-of-distribution robustness throughout fine-tuning. *arXiv preprint arXiv:2106.15831*, 2021.\"\n\"[2]\",\"S. Beery, E. Cole, and A. Gjoka. The iwildcam 2020 competition dataset. *arXiv preprint arXiv:2004.10340*, 2020.\"\n\"[3]\",\"B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning. *Pattern Recognition, 84:317\u2013331*, 2018.\"\n\"[4]\",\"B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u0160rndi\u0107, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In *Joint European conference on machine learning and knowledge discovery in databases*, pages 387\u2013402. Springer, 2013.\"\n\"[5]\",\"L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. *IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848*, 2017.\"\n\"[6]\",\"G. Christie, N. Fendley, J. Wilson, and R. Mukherjee. Functional map of the world. In *CVPR*, 2018.\"\n\"[7]\",\"A. Conneau and D. Kiela. Senteval: An evaluation toolkit for universal sentence representations. *arXiv preprint arXiv:1803.05449*, 2018.\"\n\"[8]\",\"J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. *Advances in neural information processing systems, 29*, 2016.\"\n\"[9]\",\"J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham, X. Glorot, B. O\u2019Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. *Nature medicine, 24(9):1342\u20131350*, 2018.\"\n\"[10]\",\"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference on computer vision and pattern recognition*, pages 248\u2013255. Ieee, 2009.\"\n\"[11]\",\"A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*, 2020.\"\n\"[12]\",\"A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). *arXiv preprint arXiv:2205.01397*, 2022.\""}]}, {"page": 14, "text": "[13]  Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris. Spottune: transfer learning\n      through adaptive fine-tuning. In           Proceedings of the IEEE/CVF conference on computer vision\n      and pattern recognition, pages 4805\u20134814, 2019.\n[14]  K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In                      Proceedings\n      of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n[15]  K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick.                   Mask r-cnn.        In  Proceedings of the IEEE\n      international conference on computer vision, pages 2961\u20132969, 2017.\n[16]  A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang,\n      V. Vasudevan, et al. Searching for mobilenetv3. In               Proceedings of the IEEE/CVF International\n      Conference on Computer Vision, pages 1314\u20131324, 2019.\n[17]  J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song,\n      S. Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. In\n      Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7310\u20137311,\n      2017.\n[18]  M. Huh, P. Agrawal, and A. A. Efros. What makes imagenet good for transfer learning?                              arXiv\n      preprint arXiv:1608.08614, 2016.\n[19]  F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and K. Keutzer. Densenet:\n      Implementing efficient convnet descriptor pyramids.                 arXiv preprint arXiv:1404.1869, 2014.\n[20]  H. Kataoka, K. Okayasu, A. Matsumoto, E. Yamagata, R. Yamada, N. Inoue, A. Nakamura,\n      and Y. Satoh. Pre-training without natural images.                 International Journal of Computer Vision\n      (IJCV), 2022.\n[21]  P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga,\n      R. L. Phillips, I. Gao, et al.            Wilds:    A benchmark of in-the-wild distribution shifts.                   In\n      International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021.\n[22]  A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer\n      (bit): General visual representation learning. In            European conference on computer vision, pages\n      491\u2013507. Springer, 2020.\n[23]  S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In                       Proceedings\n      of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671,\n      2019.\n[24]  A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional\n      neural networks.       Advances in neural information processing systems, 25:1097\u20131105, 2012.\n[25]  I. Loshchilov and F. Hutter.                Decoupled weight decay regularization.                   arXiv preprint\n      arXiv:1711.05101, 2017.\n[26]  J. P. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon,\n      and L. Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution\n      and in-distribution generalization. In            International Conference on Machine Learning, pages\n      7721\u20137735. PMLR, 2021.\n                                                              14", "md": "# References\n\n# References\n\n|No.|Authors|Title|Publication Details|\n|---|---|---|---|\n|13|Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris|Spottune: transfer learning through adaptive fine-tuning|Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4805\u20134814, 2019|\n|14|K. He, X. Zhang, S. Ren, and J. Sun|Deep residual learning for image recognition|Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016|\n|15|K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick|Mask r-cnn|Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017|\n|16|A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, et al.|Searching for mobilenetv3|Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1314\u20131324, 2019|\n|17|J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.|Speed/accuracy trade-offs for modern convolutional object detectors|Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7310\u20137311, 2017|\n|18|M. Huh, P. Agrawal, and A. A. Efros|What makes imagenet good for transfer learning?|arXiv preprint arXiv:1608.08614, 2016|\n|19|F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and K. Keutzer|Densenet: Implementing efficient convnet descriptor pyramids|arXiv preprint arXiv:1404.1869, 2014|\n|20|H. Kataoka, K. Okayasu, A. Matsumoto, E. Yamagata, R. Yamada, N. Inoue, A. Nakamura, and Y. Satoh|Pre-training without natural images|International Journal of Computer Vision (IJCV), 2022|\n|21|P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao, et al.|Wilds: A benchmark of in-the-wild distribution shifts|International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021|\n|22|A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby|Big transfer (bit): General visual representation learning|European conference on computer vision, pages 491\u2013507. Springer, 2020|\n|23|S. Kornblith, J. Shlens, and Q. V. Le|Do better imagenet models transfer better?|Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671, 2019|\n|24|A. Krizhevsky, I. Sutskever, and G. E. Hinton|Imagenet classification with deep convolutional neural networks|Advances in neural information processing systems, 25:1097\u20131105, 2012|\n|25|I. Loshchilov and F. Hutter|Decoupled weight decay regularization|arXiv preprint arXiv:1711.05101, 2017|\n|26|J. P. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon, and L. Schmidt|Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization|International Conference on Machine Learning, pages 7721\u20137735. PMLR, 2021|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "table", "rows": [["No.", "Authors", "Title", "Publication Details"], ["13", "Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris", "Spottune: transfer learning through adaptive fine-tuning", "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4805\u20134814, 2019"], ["14", "K. He, X. Zhang, S. Ren, and J. Sun", "Deep residual learning for image recognition", "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016"], ["15", "K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick", "Mask r-cnn", "Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017"], ["16", "A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, et al.", "Searching for mobilenetv3", "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1314\u20131324, 2019"], ["17", "J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.", "Speed/accuracy trade-offs for modern convolutional object detectors", "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7310\u20137311, 2017"], ["18", "M. Huh, P. Agrawal, and A. A. Efros", "What makes imagenet good for transfer learning?", "arXiv preprint arXiv:1608.08614, 2016"], ["19", "F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and K. Keutzer", "Densenet: Implementing efficient convnet descriptor pyramids", "arXiv preprint arXiv:1404.1869, 2014"], ["20", "H. Kataoka, K. Okayasu, A. Matsumoto, E. Yamagata, R. Yamada, N. Inoue, A. Nakamura, and Y. Satoh", "Pre-training without natural images", "International Journal of Computer Vision (IJCV), 2022"], ["21", "P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao, et al.", "Wilds: A benchmark of in-the-wild distribution shifts", "International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021"], ["22", "A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby", "Big transfer (bit): General visual representation learning", "European conference on computer vision, pages 491\u2013507. Springer, 2020"], ["23", "S. Kornblith, J. Shlens, and Q. V. Le", "Do better imagenet models transfer better?", "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671, 2019"], ["24", "A. Krizhevsky, I. Sutskever, and G. E. Hinton", "Imagenet classification with deep convolutional neural networks", "Advances in neural information processing systems, 25:1097\u20131105, 2012"], ["25", "I. Loshchilov and F. Hutter", "Decoupled weight decay regularization", "arXiv preprint arXiv:1711.05101, 2017"], ["26", "J. P. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon, and L. Schmidt", "Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization", "International Conference on Machine Learning, pages 7721\u20137735. PMLR, 2021"]], "md": "|No.|Authors|Title|Publication Details|\n|---|---|---|---|\n|13|Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris|Spottune: transfer learning through adaptive fine-tuning|Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4805\u20134814, 2019|\n|14|K. He, X. Zhang, S. Ren, and J. Sun|Deep residual learning for image recognition|Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016|\n|15|K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick|Mask r-cnn|Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017|\n|16|A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, et al.|Searching for mobilenetv3|Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1314\u20131324, 2019|\n|17|J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.|Speed/accuracy trade-offs for modern convolutional object detectors|Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7310\u20137311, 2017|\n|18|M. Huh, P. Agrawal, and A. A. Efros|What makes imagenet good for transfer learning?|arXiv preprint arXiv:1608.08614, 2016|\n|19|F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and K. Keutzer|Densenet: Implementing efficient convnet descriptor pyramids|arXiv preprint arXiv:1404.1869, 2014|\n|20|H. Kataoka, K. Okayasu, A. Matsumoto, E. Yamagata, R. Yamada, N. Inoue, A. Nakamura, and Y. Satoh|Pre-training without natural images|International Journal of Computer Vision (IJCV), 2022|\n|21|P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao, et al.|Wilds: A benchmark of in-the-wild distribution shifts|International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021|\n|22|A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby|Big transfer (bit): General visual representation learning|European conference on computer vision, pages 491\u2013507. Springer, 2020|\n|23|S. Kornblith, J. Shlens, and Q. V. Le|Do better imagenet models transfer better?|Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671, 2019|\n|24|A. Krizhevsky, I. Sutskever, and G. E. Hinton|Imagenet classification with deep convolutional neural networks|Advances in neural information processing systems, 25:1097\u20131105, 2012|\n|25|I. Loshchilov and F. Hutter|Decoupled weight decay regularization|arXiv preprint arXiv:1711.05101, 2017|\n|26|J. P. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon, and L. Schmidt|Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization|International Conference on Machine Learning, pages 7721\u20137735. PMLR, 2021|", "isPerfectTable": true, "csv": "\"No.\",\"Authors\",\"Title\",\"Publication Details\"\n\"13\",\"Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris\",\"Spottune: transfer learning through adaptive fine-tuning\",\"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4805\u20134814, 2019\"\n\"14\",\"K. He, X. Zhang, S. Ren, and J. Sun\",\"Deep residual learning for image recognition\",\"Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016\"\n\"15\",\"K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick\",\"Mask r-cnn\",\"Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017\"\n\"16\",\"A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, et al.\",\"Searching for mobilenetv3\",\"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1314\u20131324, 2019\"\n\"17\",\"J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.\",\"Speed/accuracy trade-offs for modern convolutional object detectors\",\"Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7310\u20137311, 2017\"\n\"18\",\"M. Huh, P. Agrawal, and A. A. Efros\",\"What makes imagenet good for transfer learning?\",\"arXiv preprint arXiv:1608.08614, 2016\"\n\"19\",\"F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and K. Keutzer\",\"Densenet: Implementing efficient convnet descriptor pyramids\",\"arXiv preprint arXiv:1404.1869, 2014\"\n\"20\",\"H. Kataoka, K. Okayasu, A. Matsumoto, E. Yamagata, R. Yamada, N. Inoue, A. Nakamura, and Y. Satoh\",\"Pre-training without natural images\",\"International Journal of Computer Vision (IJCV), 2022\"\n\"21\",\"P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao, et al.\",\"Wilds: A benchmark of in-the-wild distribution shifts\",\"International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021\"\n\"22\",\"A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby\",\"Big transfer (bit): General visual representation learning\",\"European conference on computer vision, pages 491\u2013507. Springer, 2020\"\n\"23\",\"S. Kornblith, J. Shlens, and Q. V. Le\",\"Do better imagenet models transfer better?\",\"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671, 2019\"\n\"24\",\"A. Krizhevsky, I. Sutskever, and G. E. Hinton\",\"Imagenet classification with deep convolutional neural networks\",\"Advances in neural information processing systems, 25:1097\u20131105, 2012\"\n\"25\",\"I. Loshchilov and F. Hutter\",\"Decoupled weight decay regularization\",\"arXiv preprint arXiv:1711.05101, 2017\"\n\"26\",\"J. P. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon, and L. Schmidt\",\"Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization\",\"International Conference on Machine Learning, pages 7721\u20137735. PMLR, 2021\""}]}, {"page": 15, "text": "[27]  R. Mormont, P. Geurts, and R. Mar\u00e9e. Comparison of deep transfer learning strategies for digital\n      pathology. In     Proceedings of the IEEE conference on computer vision and pattern recognition\n      workshops, pages 2262\u20132271, 2018.\n[28]  T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt. Quality not quantity: On the\n      interaction between dataset design and robustness of clip.                   arXiv preprint arXiv:2208.05516,\n      2022.\n[29]  X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source\n      domain adaptation. In          Proceedings of the IEEE/CVF international conference on computer\n      vision, pages 1406\u20131415, 2019.\n[30]  J. Qui\u00f1onero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence.                             Dataset shift in\n      machine learning. Mit Press, 2008.\n[31]  A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n      P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\n      In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n[32]  B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet?\n      In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\n[33]  S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with\n      region proposal networks.         Advances in neural information processing systems, 28, 2015.\n[34]  R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\n      with latent diffusion models, 2021.\n[35]  A. Rosenfeld, R. Zemel, and J. K. Tsotsos.                    The elephant in the room.              arXiv preprint\n      arXiv:1808.03305, 2018.\n[36]  O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\n      A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge.                      International\n      journal of computer vision, 115(3):211\u2013252, 2015.\n[37]  H. Salman, A. Ilyas, L. Engstrom, A. Kapoor, and A. Madry. Do adversarially robust imagenet\n      models transfer better?         Advances in Neural Information Processing Systems, 33:3533\u20133545,\n      2020.\n[38]  S. Santurkar, D. Tsipras, and A. Madry. Breeds: Benchmarks for subpopulation shift.                              arXiv\n      preprint arXiv:2008.04859, 2020.\n[39]  C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes,\n      J. Jitsev, and A. Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text\n      pairs.  arXiv preprint arXiv:2111.02114, 2021.\n[40]  V. Shankar*, R. Roelofs*, H. Mania, A. Fang, B. Recht, and L. Schmidt. Evaluating machine\n      accuracy on imagenet.        ICML, 2020.     http://proceedings.mlr.press/v119/shankar20c.html.\n[41]  K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in\n      videos.   Advances in neural information processing systems, 27, 2014.\n                                                              15", "md": "1. R. Mormont, P. Geurts, and R. Mar\u00e9e. Comparison of deep transfer learning strategies for digital pathology. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 2262\u20132271, 2018.\n2. T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. arXiv preprint arXiv:2208.05516, 2022.\n3. X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1406\u20131415, 2019.\n4. J. Qui\u00f1onero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in machine learning. Mit Press, 2008.\n5. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n6. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\n7. S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n8. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models, 2021.\n9. A. Rosenfeld, R. Zemel, and J. K. Tsotsos. The elephant in the room. arXiv preprint arXiv:1808.03305, 2018.\n10. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.\n11. H. Salman, A. Ilyas, L. Engstrom, A. Kapoor, and A. Madry. Do adversarially robust imagenet models transfer better? Advances in Neural Information Processing Systems, 33:3533\u20133545, 2020.\n12. S. Santurkar, D. Tsipras, and A. Madry. Breeds: Benchmarks for subpopulation shift. arXiv preprint arXiv:2008.04859, 2020.\n13. C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n14. V. Shankar*, R. Roelofs*, H. Mania, A. Fang, B. Recht, and L. Schmidt. Evaluating machine accuracy on imagenet. ICML, 2020. Link.\n15. K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. Advances in neural information processing systems, 27, 2014.", "images": [], "items": [{"type": "text", "value": "1. R. Mormont, P. Geurts, and R. Mar\u00e9e. Comparison of deep transfer learning strategies for digital pathology. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 2262\u20132271, 2018.\n2. T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. arXiv preprint arXiv:2208.05516, 2022.\n3. X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1406\u20131415, 2019.\n4. J. Qui\u00f1onero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in machine learning. Mit Press, 2008.\n5. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n6. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\n7. S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n8. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models, 2021.\n9. A. Rosenfeld, R. Zemel, and J. K. Tsotsos. The elephant in the room. arXiv preprint arXiv:1808.03305, 2018.\n10. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.\n11. H. Salman, A. Ilyas, L. Engstrom, A. Kapoor, and A. Madry. Do adversarially robust imagenet models transfer better? Advances in Neural Information Processing Systems, 33:3533\u20133545, 2020.\n12. S. Santurkar, D. Tsipras, and A. Madry. Breeds: Benchmarks for subpopulation shift. arXiv preprint arXiv:2008.04859, 2020.\n13. C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n14. V. Shankar*, R. Roelofs*, H. Mania, A. Fang, B. Recht, and L. Schmidt. Evaluating machine accuracy on imagenet. ICML, 2020. Link.\n15. K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. Advances in neural information processing systems, 27, 2014.", "md": "1. R. Mormont, P. Geurts, and R. Mar\u00e9e. Comparison of deep transfer learning strategies for digital pathology. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 2262\u20132271, 2018.\n2. T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. arXiv preprint arXiv:2208.05516, 2022.\n3. X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1406\u20131415, 2019.\n4. J. Qui\u00f1onero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset shift in machine learning. Mit Press, 2008.\n5. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n6. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\n7. S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n8. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models, 2021.\n9. A. Rosenfeld, R. Zemel, and J. K. Tsotsos. The elephant in the room. arXiv preprint arXiv:1808.03305, 2018.\n10. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.\n11. H. Salman, A. Ilyas, L. Engstrom, A. Kapoor, and A. Madry. Do adversarially robust imagenet models transfer better? Advances in Neural Information Processing Systems, 33:3533\u20133545, 2020.\n12. S. Santurkar, D. Tsipras, and A. Madry. Breeds: Benchmarks for subpopulation shift. arXiv preprint arXiv:2008.04859, 2020.\n13. C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n14. V. Shankar*, R. Roelofs*, H. Mania, A. Fang, B. Recht, and L. Schmidt. Evaluating machine accuracy on imagenet. ICML, 2020. Link.\n15. K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. Advances in neural information processing systems, 27, 2014."}]}, {"page": 16, "text": "[42]  C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.\n      Intriguing properties of neural networks.             arXiv preprint arXiv:1312.6199, 2013.\n[43]  R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to\n      natural distribution shifts in image classification.             Advances in Neural Information Processing\n      Systems, 33:18583\u201318599, 2020.\n[44]  B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li.\n      Yfcc100m: The new data in multimedia research.                    Communications of the ACM, 59(2):64\u201373,\n      2016.\n[45]  G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and\n      S. Belongie. The inaturalist species classification and detection dataset. In                     Proceedings of the\n      IEEE conference on computer vision and pattern recognition, pages 8769\u20138778, 2018.\n[46]  H. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing\n      local predictive power. In         Advances in Neural Information Processing Systems, pages 10506\u2013\n      10518, 2019.\n[47]  S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He.                   Aggregated residual transformations for\n      deep neural networks. In          Proceedings of the IEEE conference on computer vision and pattern\n      recognition, pages 1492\u20131500, 2017.\n[48]  K. You, Z. Kou, M. Long, and J. Wang. Co-tuning for transfer learning.                          Advances in Neural\n      Information Processing Systems, 33:17236\u201317246, 2020.\n                                                              16", "md": "[42] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.\nIntriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\n[43] R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to\nnatural distribution shifts in image classification. Advances in Neural Information Processing\nSystems, 33:18583\u201318599, 2020.\n\n[44] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li.\nYfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64\u201373,\n2016.\n\n[45] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and\nS. Belongie. The inaturalist species classification and detection dataset. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 8769\u20138778, 2018.\n\n[46] H. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing\nlocal predictive power. In Advances in Neural Information Processing Systems, pages 10506\u2013\n10518, 2019.\n\n[47] S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for\ndeep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492\u20131500, 2017.\n\n[48] K. You, Z. Kou, M. Long, and J. Wang. Co-tuning for transfer learning. Advances in Neural\nInformation Processing Systems, 33:17236\u201317246, 2020.\n\n16", "images": [], "items": [{"type": "text", "value": "[42] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.\nIntriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\n[43] R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to\nnatural distribution shifts in image classification. Advances in Neural Information Processing\nSystems, 33:18583\u201318599, 2020.\n\n[44] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li.\nYfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64\u201373,\n2016.\n\n[45] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and\nS. Belongie. The inaturalist species classification and detection dataset. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 8769\u20138778, 2018.\n\n[46] H. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing\nlocal predictive power. In Advances in Neural Information Processing Systems, pages 10506\u2013\n10518, 2019.\n\n[47] S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for\ndeep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492\u20131500, 2017.\n\n[48] K. You, Z. Kou, M. Long, and J. Wang. Co-tuning for transfer learning. Advances in Neural\nInformation Processing Systems, 33:17236\u201317246, 2020.\n\n16", "md": "[42] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.\nIntriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\n[43] R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to\nnatural distribution shifts in image classification. Advances in Neural Information Processing\nSystems, 33:18583\u201318599, 2020.\n\n[44] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li.\nYfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64\u201373,\n2016.\n\n[45] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and\nS. Belongie. The inaturalist species classification and detection dataset. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 8769\u20138778, 2018.\n\n[46] H. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing\nlocal predictive power. In Advances in Neural Information Processing Systems, pages 10506\u2013\n10518, 2019.\n\n[47] S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for\ndeep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492\u20131500, 2017.\n\n[48] K. You, Z. Kou, M. Long, and J. Wang. Co-tuning for transfer learning. Advances in Neural\nInformation Processing Systems, 33:17236\u201317246, 2020.\n\n16"}]}, {"page": 17, "text": "Appendix\nA      Training Details\nFor the standard deep neural network architectures used in our investigation, their implementation\ncomes from PyTorch\u2019s         torchvision.models       package. We use standard hyperparameters found in the\nWILDS and PyTorch\u2019s official GitHub repositories (for training models on iWildCam-WILDS and\nImageNet respectively), which match the hyperparameters described in [14] and [21]. Pre-training\non different data sources all uses 90 epochs in total, and then models are fine-tuned on iWildCam\nfor 12 epochs.\nB      Dataset Details\niWildCam-WILDS                  [3] The iWildCam dataset consists of images of 182 animal species, which\nare captured through the use of camera traps. We use the iWildCam version 2.0 released in 2021 as a\ncorrection to the iWildCam 2020 competition dataset [2] to prevent test set leakage. To construct a\nnatural distribution shift, we follow the split proposed by Koh et al.                 [21], which results in 2 test sets\nfor evaluation: ID test data consists of images taken by the same camera traps as the training set,\nbut on different days from the training and validation (ID) images, while OOD test data contains\nimages taken by a disjoint set of camera traps from training and validation (ID) images. The train\nset consists of 129,809 images. The distribution of animal categories over these images is long-tailed\n(see [21]). The ID test set consists of 8,154 images and the OOD test set consists of 42,791 images,\nalso not class balanced. Examples of train set images can be seen in Figure 14.\nImageNet          [36] We use ImageNet-1k from the ILSVRC 2012 challenge. It contains 1,000 diverse\ncategories of animals and objects, with            \u223c1.2 million training images. The train set is roughly class\nbalanced with      \u223c1.2 thousand images per category. The validation set contains 50,000 images and is\nexactly class balanced, with 50 images per class. Figure 15 shows examples of train set images.\nFigure 13:    (left)  Random images from the in-distribution test set of iWildCam-WILDS [21].                         (right)\nRandom examples from the out-of-distribution test set. This split is based on the geolocations of\nthe camera traps.                                             17", "md": "## Appendix\n\n### A Training Details\n\nFor the standard deep neural network architectures used in our investigation, their implementation comes from PyTorch\u2019s torchvision.models package. We use standard hyperparameters found in the WILDS and PyTorch\u2019s official GitHub repositories (for training models on iWildCam-WILDS and ImageNet respectively), which match the hyperparameters described in [14] and [21]. Pre-training on different data sources all uses 90 epochs in total, and then models are fine-tuned on iWildCam for 12 epochs.\n\n### B Dataset Details\n\niWildCam-WILDS [3] The iWildCam dataset consists of images of 182 animal species, which are captured through the use of camera traps. We use the iWildCam version 2.0 released in 2021 as a correction to the iWildCam 2020 competition dataset [2] to prevent test set leakage. To construct a natural distribution shift, we follow the split proposed by Koh et al. [21], which results in 2 test sets for evaluation: ID test data consists of images taken by the same camera traps as the training set, but on different days from the training and validation (ID) images, while OOD test data contains images taken by a disjoint set of camera traps from training and validation (ID) images. The train set consists of 129,809 images. The distribution of animal categories over these images is long-tailed (see [21]). The ID test set consists of 8,154 images and the OOD test set consists of 42,791 images, also not class balanced. Examples of train set images can be seen in Figure 14.\n\nImageNet [36] We use ImageNet-1k from the ILSVRC 2012 challenge. It contains 1,000 diverse categories of animals and objects, with $$\\sim$$1.2 million training images. The train set is roughly class balanced with $$\\sim$$1.2 thousand images per category. The validation set contains 50,000 images and is exactly class balanced, with 50 images per class. Figure 15 shows examples of train set images.\n\nFigure 13: (left) Random images from the in-distribution test set of iWildCam-WILDS [21]. (right) Random examples from the out-of-distribution test set. This split is based on the geolocations of the camera traps.", "images": [{"name": "img_p16_1", "height": 449, "width": 449}, {"name": "img_p16_2", "height": 449, "width": 449}], "items": [{"type": "heading", "lvl": 2, "value": "Appendix", "md": "## Appendix"}, {"type": "heading", "lvl": 3, "value": "A Training Details", "md": "### A Training Details"}, {"type": "text", "value": "For the standard deep neural network architectures used in our investigation, their implementation comes from PyTorch\u2019s torchvision.models package. We use standard hyperparameters found in the WILDS and PyTorch\u2019s official GitHub repositories (for training models on iWildCam-WILDS and ImageNet respectively), which match the hyperparameters described in [14] and [21]. Pre-training on different data sources all uses 90 epochs in total, and then models are fine-tuned on iWildCam for 12 epochs.", "md": "For the standard deep neural network architectures used in our investigation, their implementation comes from PyTorch\u2019s torchvision.models package. We use standard hyperparameters found in the WILDS and PyTorch\u2019s official GitHub repositories (for training models on iWildCam-WILDS and ImageNet respectively), which match the hyperparameters described in [14] and [21]. Pre-training on different data sources all uses 90 epochs in total, and then models are fine-tuned on iWildCam for 12 epochs."}, {"type": "heading", "lvl": 3, "value": "B Dataset Details", "md": "### B Dataset Details"}, {"type": "text", "value": "iWildCam-WILDS [3] The iWildCam dataset consists of images of 182 animal species, which are captured through the use of camera traps. We use the iWildCam version 2.0 released in 2021 as a correction to the iWildCam 2020 competition dataset [2] to prevent test set leakage. To construct a natural distribution shift, we follow the split proposed by Koh et al. [21], which results in 2 test sets for evaluation: ID test data consists of images taken by the same camera traps as the training set, but on different days from the training and validation (ID) images, while OOD test data contains images taken by a disjoint set of camera traps from training and validation (ID) images. The train set consists of 129,809 images. The distribution of animal categories over these images is long-tailed (see [21]). The ID test set consists of 8,154 images and the OOD test set consists of 42,791 images, also not class balanced. Examples of train set images can be seen in Figure 14.\n\nImageNet [36] We use ImageNet-1k from the ILSVRC 2012 challenge. It contains 1,000 diverse categories of animals and objects, with $$\\sim$$1.2 million training images. The train set is roughly class balanced with $$\\sim$$1.2 thousand images per category. The validation set contains 50,000 images and is exactly class balanced, with 50 images per class. Figure 15 shows examples of train set images.\n\nFigure 13: (left) Random images from the in-distribution test set of iWildCam-WILDS [21]. (right) Random examples from the out-of-distribution test set. This split is based on the geolocations of the camera traps.", "md": "iWildCam-WILDS [3] The iWildCam dataset consists of images of 182 animal species, which are captured through the use of camera traps. We use the iWildCam version 2.0 released in 2021 as a correction to the iWildCam 2020 competition dataset [2] to prevent test set leakage. To construct a natural distribution shift, we follow the split proposed by Koh et al. [21], which results in 2 test sets for evaluation: ID test data consists of images taken by the same camera traps as the training set, but on different days from the training and validation (ID) images, while OOD test data contains images taken by a disjoint set of camera traps from training and validation (ID) images. The train set consists of 129,809 images. The distribution of animal categories over these images is long-tailed (see [21]). The ID test set consists of 8,154 images and the OOD test set consists of 42,791 images, also not class balanced. Examples of train set images can be seen in Figure 14.\n\nImageNet [36] We use ImageNet-1k from the ILSVRC 2012 challenge. It contains 1,000 diverse categories of animals and objects, with $$\\sim$$1.2 million training images. The train set is roughly class balanced with $$\\sim$$1.2 thousand images per category. The validation set contains 50,000 images and is exactly class balanced, with 50 images per class. Figure 15 shows examples of train set images.\n\nFigure 13: (left) Random images from the in-distribution test set of iWildCam-WILDS [21]. (right) Random examples from the out-of-distribution test set. This split is based on the geolocations of the camera traps."}]}, {"page": 18, "text": "           Figure 14: Random examples from the iWildCam-WILDS train set [21].\nFigure 15: Random examples from the ImageNet ILSVRC 2012 challenge train set [10, 36].\n                                                       18", "md": "Figure 14: Random examples from the iWildCam-WILDS train set [21].\n\nFigure 15: Random examples from the ImageNet ILSVRC 2012 challenge train set [10, 36].\n\n18", "images": [{"name": "img_p17_1", "height": 773, "width": 773}, {"name": "g_d382_img_p17_2"}], "items": [{"type": "text", "value": "Figure 14: Random examples from the iWildCam-WILDS train set [21].\n\nFigure 15: Random examples from the ImageNet ILSVRC 2012 challenge train set [10, 36].\n\n18", "md": "Figure 14: Random examples from the iWildCam-WILDS train set [21].\n\nFigure 15: Random examples from the ImageNet ILSVRC 2012 challenge train set [10, 36].\n\n18"}]}, {"page": 19, "text": "iNaturalist        [45] We use the version of iNaturalist from the 2017 challenge, with 579,194 training\nimages across 5,089 diverse natural organism categories. The full training set is notably not class\nbalanced, exhibiting a long-tailed distribution (see Figure 16). The validation set contains 95,986\nimages and is also not class balanced, with between 4 and 44 images per category. Refer to Figure 17\nfor examples of iNaturalist data.\n                             10  3\n                             10  2\n                             10  1\n                            Number of Classes\n                             10  0    0         1000          2000         3000         4000\n                                         Number of Examples per Class\n               Figure 16: Histogram of class size distribution for the iNaturalist dataset.\n            Figure 17: Random examples from the iNaturalist 2017 challenge train set [45].\n                                                           19", "md": "iNaturalist [45] We use the version of iNaturalist from the 2017 challenge, with 579,194 training\nimages across 5,089 diverse natural organism categories. The full training set is notably not class\nbalanced, exhibiting a long-tailed distribution (see Figure 16). The validation set contains 95,986\nimages and is also not class balanced, with between 4 and 44 images per category. Refer to Figure 17\nfor examples of iNaturalist data.\n\n| |10|3|\n|---|---|---|\n| |10|2|\n| |10|1|\n|Number of Classes| |10|0|0|1000|2000|3000|4000|\n| | |Number of Examples per Class|\n\nFigure 16: Histogram of class size distribution for the iNaturalist dataset.\n\nFigure 17: Random examples from the iNaturalist 2017 challenge train set [45].\n\n19", "images": [{"name": "g_d382_img_p18_1"}], "items": [{"type": "text", "value": "iNaturalist [45] We use the version of iNaturalist from the 2017 challenge, with 579,194 training\nimages across 5,089 diverse natural organism categories. The full training set is notably not class\nbalanced, exhibiting a long-tailed distribution (see Figure 16). The validation set contains 95,986\nimages and is also not class balanced, with between 4 and 44 images per category. Refer to Figure 17\nfor examples of iNaturalist data.", "md": "iNaturalist [45] We use the version of iNaturalist from the 2017 challenge, with 579,194 training\nimages across 5,089 diverse natural organism categories. The full training set is notably not class\nbalanced, exhibiting a long-tailed distribution (see Figure 16). The validation set contains 95,986\nimages and is also not class balanced, with between 4 and 44 images per category. Refer to Figure 17\nfor examples of iNaturalist data."}, {"type": "table", "rows": [["", "10", "3"], ["", "10", "2"], ["", "10", "1"], ["Number of Classes", "", "10", "0", "0", "1000", "2000", "3000", "4000"], ["", "", "Number of Examples per Class"]], "md": "| |10|3|\n|---|---|---|\n| |10|2|\n| |10|1|\n|Number of Classes| |10|0|0|1000|2000|3000|4000|\n| | |Number of Examples per Class|", "isPerfectTable": false, "csv": "\"\",\"10\",\"3\"\n\"\",\"10\",\"2\"\n\"\",\"10\",\"1\"\n\"Number of Classes\",\"\",\"10\",\"0\",\"0\",\"1000\",\"2000\",\"3000\",\"4000\"\n\"\",\"\",\"Number of Examples per Class\""}, {"type": "text", "value": "Figure 16: Histogram of class size distribution for the iNaturalist dataset.\n\nFigure 17: Random examples from the iNaturalist 2017 challenge train set [45].\n\n19", "md": "Figure 16: Histogram of class size distribution for the iNaturalist dataset.\n\nFigure 17: Random examples from the iNaturalist 2017 challenge train set [45].\n\n19"}]}, {"page": 20, "text": " C      Additional Experiment Results\n C.1     Image Diversity\n More diverse data or more data per class?                         As described earlier in Section 4.4.1, we vary\n image diversity by randomly selecting pre-training images from a subset of classes in ImageNet,\nwhile keeping the total data budget fixed at 300K. Similar to the findings reported in Section 4.4.1,\n in Figure 18, we find that varying the number of classes used does not have an effect on the effective\n robustness of fine-tuned models.   50\n                                    40\n                                    30\n                                    20\n                                    10                            # of classes = 250\n                                   OOD Test macro F1              # of classes = 500\n                                     5                            # of classes = 1000\n                                                10        20     30     40    50    60    70\n                                                       ID Test macro F1\n Figure 18: We repeat the experiment in Figure 7 with a higher data regime. With a total data budget\n of 300K images, we vary the number of classes randomly selected from the original 1000 ImageNet\n classes and adjust the number of images sampled from each class correspondingly.                             We observe\n that having 4\u00d7      more classes, or 4\u00d7      more images per class, induces the same level of robustness in\n fine-tuned models.\n Per-class diversity.          For experiments with the BREEDS [38] subset of ImageNet, we select 16\n superclasses, each having 12 subclasses. The mapping can be found in Table 3. We choose overlapping\n subsets when modifying our diversity ratio (e.g., the 4 subclasses per superclass chosen when                     p = 0.33\n are also part of the 8 subclasses per superclass chosen when                p = 0.66).\n For iNaturalist experiments, as mentioned in Section 4.4.2, we choose the 7 largest classes so that\nwe could select a uniform number of images per superclass while keeping the total data budget at\n 80K, matching the setup for ImageNet. When modulating our diversity ratio, we randomly select\n a subset of subclasses from each superclass and uniformly sample images                        with replacement       from\n these subclasses. As in the ImageNet experiment, subsets of subclasses chosen per superclass are\n overlapping at increasing diversity ratios.\n C.2     CLIP Fine-tuning Details\nWe investigate how the effective robustness obtained from ImageNet pre-training would change at a\n much larger pre-training data regime. To do so, we fine-tune CLIP [31] models that have been trained\n on different data distributions on iWildCam-WILDS. We use the same finetuning hyperparameters\n                                                             20", "md": "# Additional Experiment Results\n\n## C.1 Image Diversity\n\nMore diverse data or more data per class? As described earlier in Section 4.4.1, we vary image diversity by randomly selecting pre-training images from a subset of classes in ImageNet, while keeping the total data budget fixed at 300K. Similar to the findings reported in Section 4.4.1, in Figure 18, we find that varying the number of classes used does not have an effect on the effective robustness of fine-tuned models.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{# of classes} & \\text{OOD Test macro F1} \\\\\n\\hline\n250 & 50 \\\\\n500 & 40 \\\\\n1000 & 30 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 18: We repeat the experiment in Figure 7 with a higher data regime. With a total data budget of 300K images, we vary the number of classes randomly selected from the original 1000 ImageNet classes and adjust the number of images sampled from each class correspondingly. We observe that having 4x more classes, or 4x more images per class, induces the same level of robustness in fine-tuned models.\n\nPer-class diversity. For experiments with the BREEDS [38] subset of ImageNet, we select 16 superclasses, each having 12 subclasses. The mapping can be found in Table 3. We choose overlapping subsets when modifying our diversity ratio (e.g., the 4 subclasses per superclass chosen when \\( p = 0.33 \\) are also part of the 8 subclasses per superclass chosen when \\( p = 0.66 \\)).\n\nFor iNaturalist experiments, as mentioned in Section 4.4.2, we choose the 7 largest classes so that we could select a uniform number of images per superclass while keeping the total data budget at 80K, matching the setup for ImageNet. When modulating our diversity ratio, we randomly select a subset of subclasses from each superclass and uniformly sample images with replacement from these subclasses. As in the ImageNet experiment, subsets of subclasses chosen per superclass are overlapping at increasing diversity ratios.\n\n## C.2 CLIP Fine-tuning Details\n\nWe investigate how the effective robustness obtained from ImageNet pre-training would change at a much larger pre-training data regime. To do so, we fine-tune CLIP [31] models that have been trained on different data distributions on iWildCam-WILDS. We use the same finetuning hyperparameters.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Experiment Results", "md": "# Additional Experiment Results"}, {"type": "heading", "lvl": 2, "value": "C.1 Image Diversity", "md": "## C.1 Image Diversity"}, {"type": "text", "value": "More diverse data or more data per class? As described earlier in Section 4.4.1, we vary image diversity by randomly selecting pre-training images from a subset of classes in ImageNet, while keeping the total data budget fixed at 300K. Similar to the findings reported in Section 4.4.1, in Figure 18, we find that varying the number of classes used does not have an effect on the effective robustness of fine-tuned models.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{# of classes} & \\text{OOD Test macro F1} \\\\\n\\hline\n250 & 50 \\\\\n500 & 40 \\\\\n1000 & 30 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 18: We repeat the experiment in Figure 7 with a higher data regime. With a total data budget of 300K images, we vary the number of classes randomly selected from the original 1000 ImageNet classes and adjust the number of images sampled from each class correspondingly. We observe that having 4x more classes, or 4x more images per class, induces the same level of robustness in fine-tuned models.\n\nPer-class diversity. For experiments with the BREEDS [38] subset of ImageNet, we select 16 superclasses, each having 12 subclasses. The mapping can be found in Table 3. We choose overlapping subsets when modifying our diversity ratio (e.g., the 4 subclasses per superclass chosen when \\( p = 0.33 \\) are also part of the 8 subclasses per superclass chosen when \\( p = 0.66 \\)).\n\nFor iNaturalist experiments, as mentioned in Section 4.4.2, we choose the 7 largest classes so that we could select a uniform number of images per superclass while keeping the total data budget at 80K, matching the setup for ImageNet. When modulating our diversity ratio, we randomly select a subset of subclasses from each superclass and uniformly sample images with replacement from these subclasses. As in the ImageNet experiment, subsets of subclasses chosen per superclass are overlapping at increasing diversity ratios.", "md": "More diverse data or more data per class? As described earlier in Section 4.4.1, we vary image diversity by randomly selecting pre-training images from a subset of classes in ImageNet, while keeping the total data budget fixed at 300K. Similar to the findings reported in Section 4.4.1, in Figure 18, we find that varying the number of classes used does not have an effect on the effective robustness of fine-tuned models.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{# of classes} & \\text{OOD Test macro F1} \\\\\n\\hline\n250 & 50 \\\\\n500 & 40 \\\\\n1000 & 30 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 18: We repeat the experiment in Figure 7 with a higher data regime. With a total data budget of 300K images, we vary the number of classes randomly selected from the original 1000 ImageNet classes and adjust the number of images sampled from each class correspondingly. We observe that having 4x more classes, or 4x more images per class, induces the same level of robustness in fine-tuned models.\n\nPer-class diversity. For experiments with the BREEDS [38] subset of ImageNet, we select 16 superclasses, each having 12 subclasses. The mapping can be found in Table 3. We choose overlapping subsets when modifying our diversity ratio (e.g., the 4 subclasses per superclass chosen when \\( p = 0.33 \\) are also part of the 8 subclasses per superclass chosen when \\( p = 0.66 \\)).\n\nFor iNaturalist experiments, as mentioned in Section 4.4.2, we choose the 7 largest classes so that we could select a uniform number of images per superclass while keeping the total data budget at 80K, matching the setup for ImageNet. When modulating our diversity ratio, we randomly select a subset of subclasses from each superclass and uniformly sample images with replacement from these subclasses. As in the ImageNet experiment, subsets of subclasses chosen per superclass are overlapping at increasing diversity ratios."}, {"type": "heading", "lvl": 2, "value": "C.2 CLIP Fine-tuning Details", "md": "## C.2 CLIP Fine-tuning Details"}, {"type": "text", "value": "We investigate how the effective robustness obtained from ImageNet pre-training would change at a much larger pre-training data regime. To do so, we fine-tune CLIP [31] models that have been trained on different data distributions on iWildCam-WILDS. We use the same finetuning hyperparameters.", "md": "We investigate how the effective robustness obtained from ImageNet pre-training would change at a much larger pre-training data regime. To do so, we fine-tune CLIP [31] models that have been trained on different data distributions on iWildCam-WILDS. We use the same finetuning hyperparameters."}]}, {"page": 21, "text": "                                          Superclass Category         Counts\n                                          Protozoa                        308\n                                          Chromista                       398\n                                          Actinopterygii                 1982\n                                          Arachnida                      4873\n                                          Animalia                       5228\n                                          Fungi                          5826\n                                          Mollusca                       7536\n                                          Amphibia                     15318\n                                          Mammalia                     29333\n                                          Reptilia                     35201\n                                          Insecta                     100479\n                                          Plantae                     158407\n                                          Aves                        214295\nTable 2: Superclass data counts for the iNaturalist [45] train set. We use the 7 largest classes for\nour iNaturalist experiment in Section 4.4.2 so that we could select a uniform number of images per\nsuperclass while still having 80K images in total, to match the corresponding ImageNet experiment.\nNote that some superclass categories are in fact semantic superclasses of other categories (e.g. Aves,\nReptilia, and Mammalia are all subclasses of Animalia) but are labeled disjointly. Given that we\nonly use the last 7 categories, our superclass selection avoids this problem.\nfrom [21] with the AdamW optimizer [25]. The training datasets for CLIP are often large corpora of\nimage-text pairs scraped from the internet. Models from the original CLIP paper [31] are trained\nwith 400M data points, and we analyze specifically those that use ViT [11] as the image encoder\narchitecture. We also include the results from CLIP ResNet50 models trained on YFCC-15M [44]\nand LAION-15M [39] separately, to show the effects of pre-training with different data scales and\ndata sources. The results of these experiments can be found in Figure 12. We find that the linear\ntrend of ImageNet pre-trained models is still predictive of the effective robustness obtained from\nthese CLIP models. Note that along with utilizing significantly more data, CLIP also undergoes a\ndifferent pre-training mechanism (i.e., contrastive versus supervised learning). This demonstrates the\nwidespread applicability of existing effective robustness trends in analyzing generalization properties\nof pre-trained models.\nC.3      Pre-training on ImageNet-scale Diffusion Data\nAs described in Section 4.5, we explore the effect of pre-training on images from different data\ndistributions, both synthetic and natural. At 150K data regime (Figure 11), our earlier findings\ndemonstrate that the transfer robustness obtained from pre-training on natural-looking synthetic data\ncould rival that of pre-training on ImageNet or iNaturalist. Now, we increase the number of synthetic\nimages generated using Stable Diffusion to 1.2M, matching the full ImageNet class distribution. At\nthis data regime, we find that natural-looking synthetic data (i.e., Diffusion 1.2M) is slightly less\neffective than natural data (i.e., iNaturalist and ImageNet) at improving the effective robustness\nof fine-tuned models (Figure 19). However, Diffusion 1.2M still offers much more robustness gain\ncompared to pre-training on synthetic fractal data (i.e., FractalDB-1k).\n                                                           21", "md": "# Superclass Data Counts\n\n## Superclass Data Counts\n\n|Superclass Category|Counts|\n|---|---|\n|Protozoa|308|\n|Chromista|398|\n|Actinopterygii|1982|\n|Arachnida|4873|\n|Animalia|5228|\n|Fungi|5826|\n|Mollusca|7536|\n|Amphibia|15318|\n|Mammalia|29333|\n|Reptilia|35201|\n|Insecta|100479|\n|Plantae|158407|\n|Aves|214295|\n\nTable 2: Superclass data counts for the iNaturalist [45] train set. We use the 7 largest classes for\nour iNaturalist experiment in Section 4.4.2 so that we could select a uniform number of images per\nsuperclass while still having 80K images in total, to match the corresponding ImageNet experiment.\nNote that some superclass categories are in fact semantic superclasses of other categories (e.g. Aves,\nReptilia, and Mammalia are all subclasses of Animalia) but are labeled disjointly. Given that we\nonly use the last 7 categories, our superclass selection avoids this problem.\n\nfrom [21] with the AdamW optimizer [25]. The training datasets for CLIP are often large corpora of\nimage-text pairs scraped from the internet. Models from the original CLIP paper [31] are trained\nwith 400M data points, and we analyze specifically those that use ViT [11] as the image encoder\narchitecture. We also include the results from CLIP ResNet50 models trained on YFCC-15M [44]\nand LAION-15M [39] separately, to show the effects of pre-training with different data scales and\ndata sources. The results of these experiments can be found in Figure 12. We find that the linear\ntrend of ImageNet pre-trained models is still predictive of the effective robustness obtained from\nthese CLIP models. Note that along with utilizing significantly more data, CLIP also undergoes a\ndifferent pre-training mechanism (i.e., contrastive versus supervised learning). This demonstrates the\nwidespread applicability of existing effective robustness trends in analyzing generalization properties\nof pre-trained models.\n\n### Pre-training on ImageNet-scale Diffusion Data\n\nAs described in Section 4.5, we explore the effect of pre-training on images from different data\ndistributions, both synthetic and natural. At 150K data regime (Figure 11), our earlier findings\ndemonstrate that the transfer robustness obtained from pre-training on natural-looking synthetic data\ncould rival that of pre-training on ImageNet or iNaturalist. Now, we increase the number of synthetic\nimages generated using Stable Diffusion to 1.2M, matching the full ImageNet class distribution. At\nthis data regime, we find that natural-looking synthetic data (i.e., Diffusion 1.2M) is slightly less\neffective than natural data (i.e., iNaturalist and ImageNet) at improving the effective robustness\nof fine-tuned models (Figure 19). However, Diffusion 1.2M still offers much more robustness gain\ncompared to pre-training on synthetic fractal data (i.e., FractalDB-1k).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Superclass Data Counts", "md": "# Superclass Data Counts"}, {"type": "heading", "lvl": 2, "value": "Superclass Data Counts", "md": "## Superclass Data Counts"}, {"type": "table", "rows": [["Superclass Category", "Counts"], ["Protozoa", "308"], ["Chromista", "398"], ["Actinopterygii", "1982"], ["Arachnida", "4873"], ["Animalia", "5228"], ["Fungi", "5826"], ["Mollusca", "7536"], ["Amphibia", "15318"], ["Mammalia", "29333"], ["Reptilia", "35201"], ["Insecta", "100479"], ["Plantae", "158407"], ["Aves", "214295"]], "md": "|Superclass Category|Counts|\n|---|---|\n|Protozoa|308|\n|Chromista|398|\n|Actinopterygii|1982|\n|Arachnida|4873|\n|Animalia|5228|\n|Fungi|5826|\n|Mollusca|7536|\n|Amphibia|15318|\n|Mammalia|29333|\n|Reptilia|35201|\n|Insecta|100479|\n|Plantae|158407|\n|Aves|214295|", "isPerfectTable": true, "csv": "\"Superclass Category\",\"Counts\"\n\"Protozoa\",\"308\"\n\"Chromista\",\"398\"\n\"Actinopterygii\",\"1982\"\n\"Arachnida\",\"4873\"\n\"Animalia\",\"5228\"\n\"Fungi\",\"5826\"\n\"Mollusca\",\"7536\"\n\"Amphibia\",\"15318\"\n\"Mammalia\",\"29333\"\n\"Reptilia\",\"35201\"\n\"Insecta\",\"100479\"\n\"Plantae\",\"158407\"\n\"Aves\",\"214295\""}, {"type": "text", "value": "Table 2: Superclass data counts for the iNaturalist [45] train set. We use the 7 largest classes for\nour iNaturalist experiment in Section 4.4.2 so that we could select a uniform number of images per\nsuperclass while still having 80K images in total, to match the corresponding ImageNet experiment.\nNote that some superclass categories are in fact semantic superclasses of other categories (e.g. Aves,\nReptilia, and Mammalia are all subclasses of Animalia) but are labeled disjointly. Given that we\nonly use the last 7 categories, our superclass selection avoids this problem.\n\nfrom [21] with the AdamW optimizer [25]. The training datasets for CLIP are often large corpora of\nimage-text pairs scraped from the internet. Models from the original CLIP paper [31] are trained\nwith 400M data points, and we analyze specifically those that use ViT [11] as the image encoder\narchitecture. We also include the results from CLIP ResNet50 models trained on YFCC-15M [44]\nand LAION-15M [39] separately, to show the effects of pre-training with different data scales and\ndata sources. The results of these experiments can be found in Figure 12. We find that the linear\ntrend of ImageNet pre-trained models is still predictive of the effective robustness obtained from\nthese CLIP models. Note that along with utilizing significantly more data, CLIP also undergoes a\ndifferent pre-training mechanism (i.e., contrastive versus supervised learning). This demonstrates the\nwidespread applicability of existing effective robustness trends in analyzing generalization properties\nof pre-trained models.", "md": "Table 2: Superclass data counts for the iNaturalist [45] train set. We use the 7 largest classes for\nour iNaturalist experiment in Section 4.4.2 so that we could select a uniform number of images per\nsuperclass while still having 80K images in total, to match the corresponding ImageNet experiment.\nNote that some superclass categories are in fact semantic superclasses of other categories (e.g. Aves,\nReptilia, and Mammalia are all subclasses of Animalia) but are labeled disjointly. Given that we\nonly use the last 7 categories, our superclass selection avoids this problem.\n\nfrom [21] with the AdamW optimizer [25]. The training datasets for CLIP are often large corpora of\nimage-text pairs scraped from the internet. Models from the original CLIP paper [31] are trained\nwith 400M data points, and we analyze specifically those that use ViT [11] as the image encoder\narchitecture. We also include the results from CLIP ResNet50 models trained on YFCC-15M [44]\nand LAION-15M [39] separately, to show the effects of pre-training with different data scales and\ndata sources. The results of these experiments can be found in Figure 12. We find that the linear\ntrend of ImageNet pre-trained models is still predictive of the effective robustness obtained from\nthese CLIP models. Note that along with utilizing significantly more data, CLIP also undergoes a\ndifferent pre-training mechanism (i.e., contrastive versus supervised learning). This demonstrates the\nwidespread applicability of existing effective robustness trends in analyzing generalization properties\nof pre-trained models."}, {"type": "heading", "lvl": 3, "value": "Pre-training on ImageNet-scale Diffusion Data", "md": "### Pre-training on ImageNet-scale Diffusion Data"}, {"type": "text", "value": "As described in Section 4.5, we explore the effect of pre-training on images from different data\ndistributions, both synthetic and natural. At 150K data regime (Figure 11), our earlier findings\ndemonstrate that the transfer robustness obtained from pre-training on natural-looking synthetic data\ncould rival that of pre-training on ImageNet or iNaturalist. Now, we increase the number of synthetic\nimages generated using Stable Diffusion to 1.2M, matching the full ImageNet class distribution. At\nthis data regime, we find that natural-looking synthetic data (i.e., Diffusion 1.2M) is slightly less\neffective than natural data (i.e., iNaturalist and ImageNet) at improving the effective robustness\nof fine-tuned models (Figure 19). However, Diffusion 1.2M still offers much more robustness gain\ncompared to pre-training on synthetic fractal data (i.e., FractalDB-1k).", "md": "As described in Section 4.5, we explore the effect of pre-training on images from different data\ndistributions, both synthetic and natural. At 150K data regime (Figure 11), our earlier findings\ndemonstrate that the transfer robustness obtained from pre-training on natural-looking synthetic data\ncould rival that of pre-training on ImageNet or iNaturalist. Now, we increase the number of synthetic\nimages generated using Stable Diffusion to 1.2M, matching the full ImageNet class distribution. At\nthis data regime, we find that natural-looking synthetic data (i.e., Diffusion 1.2M) is slightly less\neffective than natural data (i.e., iNaturalist and ImageNet) at improving the effective robustness\nof fine-tuned models (Figure 19). However, Diffusion 1.2M still offers much more robustness gain\ncompared to pre-training on synthetic fractal data (i.e., FractalDB-1k)."}]}, {"page": 22, "text": "                                    50            iNaturalist\n                                    40            ImageNet\n                                    30            Diffusion 1.2M\n                                    20            FractalDB-1k\n                                    10\n                                   OOD Test macro F1\n                                     5\n                                                10         20     30     40    50    60    70\n                                                        ID Test macro F1\nFigure 19: We experiment with pre-training on an ImageNet-scale synthetic dataset (i.e., 1.2M\nimages) generated using Stable Diffusion, and find that it yields less robustness after fine-tuning\ncompared to pre-training on natural data of similar quantities (i.e., ImageNet, iNaturalist). However,\nthe natural-looking synthetic data in Diffusion 1.2M is still much more effective as a pre-training\nsource compared to the synthetic fractal data in FractalDB-1k.\nWe hypothesize that the additional image diversity obtained from using more synthetic data saturates\nafter certain data quantity, as the objects generated by Stable Diffusion appear to be relatively more\ncentered and less cluttered, compared to ImageNet data (Figure 9). Further study into alternative\nprompting templates/ techniques is needed to increase the variety of the images obtained from Stable\nDiffusion.\nD      Experiments with Contrastive Pre-training\nWe attempt to apply our interventions to the constrastive learning regime, done on image-text pairs\nscraped from the web. In this set of experiments, we construct different pre-training distributions from\nLAION-5B [39], and evaluate the resulting models                 zero-shot   on ImageNet and two ImageNet-derived\ndistribution shifts: ImageNet-V2 [32] and ImageNet-Sketch [46]. We experiment with pre-training\nCLIP ViT-B/32 on 10M, 50M and 100M samples separately, for 128M steps in total with batch\nsize 4096 per GPU, learning rate 0.0005 and 500 warmup steps. Varying pre-training set size by\n10\u00d7   from 10M to 100M doesn\u2019t significantly alter the robustness linear trends on either distribution\nshift, despite offering substantial boost in robustness compared to training models from scratch on\nImageNet (yellow line, obtained from [26]). We hypothesize that much larger scale differences are\nneeded to test the effect of data quantity in this case. This additional investigation is not the focus\nof this work, however we believe adapting our interventions to massive pre-training webdatasets to\nbe an interesting direction for future exploration.\n                                                              22", "md": "# Document\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{50} & \\text{iNaturalist} \\\\\n\\text{40} & \\text{ImageNet} \\\\\n\\text{30} & \\text{Diffusion 1.2M} \\\\\n\\text{20} & \\text{FractalDB-1k} \\\\\n\\text{10} &  \\\\\n\\text{OOD Test macro F1} &  \\\\\n\\text{5} &  \\\\\n\\text{10} & \\text{20} & \\text{30} & \\text{40} & \\text{50} & \\text{60} & \\text{70} \\\\\n\\text{ID Test macro F1} &  \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 19: We experiment with pre-training on an ImageNet-scale synthetic dataset (i.e., 1.2M images) generated using Stable Diffusion, and find that it yields less robustness after fine-tuning compared to pre-training on natural data of similar quantities (i.e., ImageNet, iNaturalist). However, the natural-looking synthetic data in Diffusion 1.2M is still much more effective as a pre-training source compared to the synthetic fractal data in FractalDB-1k. We hypothesize that the additional image diversity obtained from using more synthetic data saturates after certain data quantity, as the objects generated by Stable Diffusion appear to be relatively more centered and less cluttered, compared to ImageNet data (Figure 9). Further study into alternative prompting templates/ techniques is needed to increase the variety of the images obtained from Stable Diffusion.\n\nExperiments with Contrastive Pre-training\n\nWe attempt to apply our interventions to the constrastive learning regime, done on image-text pairs scraped from the web. In this set of experiments, we construct different pre-training distributions from LAION-5B [39], and evaluate the resulting models zero-shot on ImageNet and two ImageNet-derived distribution shifts: ImageNet-V2 [32] and ImageNet-Sketch [46]. We experiment with pre-training CLIP ViT-B/32 on 10M, 50M and 100M samples separately, for 128M steps in total with batch size 4096 per GPU, learning rate 0.0005 and 500 warmup steps. Varying pre-training set size by 10\u00d7 from 10M to 100M doesn\u2019t significantly alter the robustness linear trends on either distribution shift, despite offering substantial boost in robustness compared to training models from scratch on ImageNet (yellow line, obtained from [26]). We hypothesize that much larger scale differences are needed to test the effect of data quantity in this case. This additional investigation is not the focus of this work, however we believe adapting our interventions to massive pre-training webdatasets to be an interesting direction for future exploration.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{50} & \\text{iNaturalist} \\\\\n\\text{40} & \\text{ImageNet} \\\\\n\\text{30} & \\text{Diffusion 1.2M} \\\\\n\\text{20} & \\text{FractalDB-1k} \\\\\n\\text{10} &  \\\\\n\\text{OOD Test macro F1} &  \\\\\n\\text{5} &  \\\\\n\\text{10} & \\text{20} & \\text{30} & \\text{40} & \\text{50} & \\text{60} & \\text{70} \\\\\n\\text{ID Test macro F1} &  \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 19: We experiment with pre-training on an ImageNet-scale synthetic dataset (i.e., 1.2M images) generated using Stable Diffusion, and find that it yields less robustness after fine-tuning compared to pre-training on natural data of similar quantities (i.e., ImageNet, iNaturalist). However, the natural-looking synthetic data in Diffusion 1.2M is still much more effective as a pre-training source compared to the synthetic fractal data in FractalDB-1k. We hypothesize that the additional image diversity obtained from using more synthetic data saturates after certain data quantity, as the objects generated by Stable Diffusion appear to be relatively more centered and less cluttered, compared to ImageNet data (Figure 9). Further study into alternative prompting templates/ techniques is needed to increase the variety of the images obtained from Stable Diffusion.\n\nExperiments with Contrastive Pre-training\n\nWe attempt to apply our interventions to the constrastive learning regime, done on image-text pairs scraped from the web. In this set of experiments, we construct different pre-training distributions from LAION-5B [39], and evaluate the resulting models zero-shot on ImageNet and two ImageNet-derived distribution shifts: ImageNet-V2 [32] and ImageNet-Sketch [46]. We experiment with pre-training CLIP ViT-B/32 on 10M, 50M and 100M samples separately, for 128M steps in total with batch size 4096 per GPU, learning rate 0.0005 and 500 warmup steps. Varying pre-training set size by 10\u00d7 from 10M to 100M doesn\u2019t significantly alter the robustness linear trends on either distribution shift, despite offering substantial boost in robustness compared to training models from scratch on ImageNet (yellow line, obtained from [26]). We hypothesize that much larger scale differences are needed to test the effect of data quantity in this case. This additional investigation is not the focus of this work, however we believe adapting our interventions to massive pre-training webdatasets to be an interesting direction for future exploration.", "md": "$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{50} & \\text{iNaturalist} \\\\\n\\text{40} & \\text{ImageNet} \\\\\n\\text{30} & \\text{Diffusion 1.2M} \\\\\n\\text{20} & \\text{FractalDB-1k} \\\\\n\\text{10} &  \\\\\n\\text{OOD Test macro F1} &  \\\\\n\\text{5} &  \\\\\n\\text{10} & \\text{20} & \\text{30} & \\text{40} & \\text{50} & \\text{60} & \\text{70} \\\\\n\\text{ID Test macro F1} &  \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 19: We experiment with pre-training on an ImageNet-scale synthetic dataset (i.e., 1.2M images) generated using Stable Diffusion, and find that it yields less robustness after fine-tuning compared to pre-training on natural data of similar quantities (i.e., ImageNet, iNaturalist). However, the natural-looking synthetic data in Diffusion 1.2M is still much more effective as a pre-training source compared to the synthetic fractal data in FractalDB-1k. We hypothesize that the additional image diversity obtained from using more synthetic data saturates after certain data quantity, as the objects generated by Stable Diffusion appear to be relatively more centered and less cluttered, compared to ImageNet data (Figure 9). Further study into alternative prompting templates/ techniques is needed to increase the variety of the images obtained from Stable Diffusion.\n\nExperiments with Contrastive Pre-training\n\nWe attempt to apply our interventions to the constrastive learning regime, done on image-text pairs scraped from the web. In this set of experiments, we construct different pre-training distributions from LAION-5B [39], and evaluate the resulting models zero-shot on ImageNet and two ImageNet-derived distribution shifts: ImageNet-V2 [32] and ImageNet-Sketch [46]. We experiment with pre-training CLIP ViT-B/32 on 10M, 50M and 100M samples separately, for 128M steps in total with batch size 4096 per GPU, learning rate 0.0005 and 500 warmup steps. Varying pre-training set size by 10\u00d7 from 10M to 100M doesn\u2019t significantly alter the robustness linear trends on either distribution shift, despite offering substantial boost in robustness compared to training models from scratch on ImageNet (yellow line, obtained from [26]). We hypothesize that much larger scale differences are needed to test the effect of data quantity in this case. This additional investigation is not the focus of this work, however we believe adapting our interventions to massive pre-training webdatasets to be an interesting direction for future exploration."}]}, {"page": 23, "text": "   45                                            51\n   35                                            41                                          y=x\n                                                 31                                          Standard ImageNet models\n   25                                            21                                          Linear fit (standard ImageNet models)\n                                                                                             LAION-10M trained CLIP\n                                                 11                                          Linear fit (LAION-10M)\n   15                                                                                        LAION-50M trained CLIP\n                                                                                             Linear fit (LAION-50M)\n                                                                                             LAION-100M trained CLIP\n  ImageNetV2 (top-1, %)                         ImageNet Sketch (top-1, %)                   Linear fit (LAION-100M)\n   5 5            15     25   35   45             11               11   21 31 41 51\n            ImageNet (top-1, %)                            ImageNet (top-1, %)\nFigure 20: We experiment with pre-training with different data quantities sampled from LAION-5B\n[39], and find that 10M, 50M and 100M dataset sizes do not change downstream robustness when\nevaluated zero-shot on ImageNet-derived distribution shifts. However, pre-training still yields much\nmore robustness compared to training from scratch (linear trend obtained from [26]), especially on\nthe ImageNet to ImageNet-Sketch shift.\nE      DomainNet Experiments\nFor each domain provided in DomainNet [29], we compare models trained from scratch on data from\nthat domain, to models that have been pre-trained on ImageNet and then fine-tuned on the same\ndomain. We then evaluate the model performance on all other domains that have not been used for\ntraining, which are considered OOD test sets. Figure 21 shows the resulting effective robustness for\nall pairs of domains, with        x  axis and    y axis representing ID and OOD accuracies respectively. We\nfind that for most pairs, ImageNet pre-training and training from scratch do not exhibit distinctive\nlinear trends. An exception to this can be found in the case of fine-tuning on Infograph data and\nevaluating on Sketch data (middle panel of the last row). Using this setup, we proceed to evaluating\nthe impact of intervening on two different aspects of the ImageNet pre-training distribution that\nhave been found to be important in earlier experiments with iWildCam-WILDS: label granularity\nand data quantity.\nE.1      Label Granularity\nOur setup follows a similar procedure as described previously in Section 4.2. In the left plot of\nFigure 22, when we collapse the label space to 232 superclasses (depth 7), or 17 superclasses (depth\n4), the effective robustness obtained from ImageNet pre-training is reduced to about the same level\nas training from scratch. This suggests that the Infograph-Sketch distribution shift is much more\nsensitive to label granularity, compared to the iWildCam-WILDS shift.\nE.2      Data Quantity\nWe repeat the experiment with pre-training data quantity described in Section 4.1. As seen from\nthe right plot of Figure 22, using 25K, 50K, 100K and 150K subsets of ImageNet for pre-training\nresults in a similar level of effective robustness after fine-tuning, which is a lot less than using the full\nImageNet dataset, but still an improvement compared to training from scratch. As a sanity check,\nwe find that using only 5K ImageNet images for pre-training leads to no robustness gain compared\nto training from scratch on Infograph dataset itself.         23", "md": "# Document\n\n$$y=x$$\n\n|ImageNetV2 (top-1, %)|ImageNet Sketch (top-1, %)|Linear fit (LAION-100M)|\n|---|---|---|\n|5|11|11|\n|15|21|21|\n|25|31|31|\n|35|41|41|\n|45|51|51|\n\nFigure 20: We experiment with pre-training with different data quantities sampled from LAION-5B [39], and find that 10M, 50M and 100M dataset sizes do not change downstream robustness when evaluated zero-shot on ImageNet-derived distribution shifts. However, pre-training still yields much more robustness compared to training from scratch (linear trend obtained from [26]), especially on the ImageNet to ImageNet-Sketch shift.\n\nDomainNet Experiments\n\nFor each domain provided in DomainNet [29], we compare models trained from scratch on data from that domain, to models that have been pre-trained on ImageNet and then fine-tuned on the same domain. We then evaluate the model performance on all other domains that have not been used for training, which are considered OOD test sets. Figure 21 shows the resulting effective robustness for all pairs of domains, with x axis and y axis representing ID and OOD accuracies respectively. We find that for most pairs, ImageNet pre-training and training from scratch do not exhibit distinctive linear trends. An exception to this can be found in the case of fine-tuning on Infograph data and evaluating on Sketch data (middle panel of the last row). Using this setup, we proceed to evaluating the impact of intervening on two different aspects of the ImageNet pre-training distribution that have been found to be important in earlier experiments with iWildCam-WILDS: label granularity and data quantity.\n\nLabel Granularity\n\nOur setup follows a similar procedure as described previously in Section 4.2. In the left plot of Figure 22, when we collapse the label space to 232 superclasses (depth 7), or 17 superclasses (depth 4), the effective robustness obtained from ImageNet pre-training is reduced to about the same level as training from scratch. This suggests that the Infograph-Sketch distribution shift is much more sensitive to label granularity, compared to the iWildCam-WILDS shift.\n\nData Quantity\n\nWe repeat the experiment with pre-training data quantity described in Section 4.1. As seen from the right plot of Figure 22, using 25K, 50K, 100K and 150K subsets of ImageNet for pre-training results in a similar level of effective robustness after fine-tuning, which is a lot less than using the full ImageNet dataset, but still an improvement compared to training from scratch. As a sanity check, we find that using only 5K ImageNet images for pre-training leads to no robustness gain compared to training from scratch on Infograph dataset itself.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "$$y=x$$", "md": "$$y=x$$"}, {"type": "table", "rows": [["ImageNetV2 (top-1, %)", "ImageNet Sketch (top-1, %)", "Linear fit (LAION-100M)"], ["5", "11", "11"], ["15", "21", "21"], ["25", "31", "31"], ["35", "41", "41"], ["45", "51", "51"]], "md": "|ImageNetV2 (top-1, %)|ImageNet Sketch (top-1, %)|Linear fit (LAION-100M)|\n|---|---|---|\n|5|11|11|\n|15|21|21|\n|25|31|31|\n|35|41|41|\n|45|51|51|", "isPerfectTable": true, "csv": "\"ImageNetV2 (top-1, %)\",\"ImageNet Sketch (top-1, %)\",\"Linear fit (LAION-100M)\"\n\"5\",\"11\",\"11\"\n\"15\",\"21\",\"21\"\n\"25\",\"31\",\"31\"\n\"35\",\"41\",\"41\"\n\"45\",\"51\",\"51\""}, {"type": "text", "value": "Figure 20: We experiment with pre-training with different data quantities sampled from LAION-5B [39], and find that 10M, 50M and 100M dataset sizes do not change downstream robustness when evaluated zero-shot on ImageNet-derived distribution shifts. However, pre-training still yields much more robustness compared to training from scratch (linear trend obtained from [26]), especially on the ImageNet to ImageNet-Sketch shift.\n\nDomainNet Experiments\n\nFor each domain provided in DomainNet [29], we compare models trained from scratch on data from that domain, to models that have been pre-trained on ImageNet and then fine-tuned on the same domain. We then evaluate the model performance on all other domains that have not been used for training, which are considered OOD test sets. Figure 21 shows the resulting effective robustness for all pairs of domains, with x axis and y axis representing ID and OOD accuracies respectively. We find that for most pairs, ImageNet pre-training and training from scratch do not exhibit distinctive linear trends. An exception to this can be found in the case of fine-tuning on Infograph data and evaluating on Sketch data (middle panel of the last row). Using this setup, we proceed to evaluating the impact of intervening on two different aspects of the ImageNet pre-training distribution that have been found to be important in earlier experiments with iWildCam-WILDS: label granularity and data quantity.\n\nLabel Granularity\n\nOur setup follows a similar procedure as described previously in Section 4.2. In the left plot of Figure 22, when we collapse the label space to 232 superclasses (depth 7), or 17 superclasses (depth 4), the effective robustness obtained from ImageNet pre-training is reduced to about the same level as training from scratch. This suggests that the Infograph-Sketch distribution shift is much more sensitive to label granularity, compared to the iWildCam-WILDS shift.\n\nData Quantity\n\nWe repeat the experiment with pre-training data quantity described in Section 4.1. As seen from the right plot of Figure 22, using 25K, 50K, 100K and 150K subsets of ImageNet for pre-training results in a similar level of effective robustness after fine-tuning, which is a lot less than using the full ImageNet dataset, but still an improvement compared to training from scratch. As a sanity check, we find that using only 5K ImageNet images for pre-training leads to no robustness gain compared to training from scratch on Infograph dataset itself.", "md": "Figure 20: We experiment with pre-training with different data quantities sampled from LAION-5B [39], and find that 10M, 50M and 100M dataset sizes do not change downstream robustness when evaluated zero-shot on ImageNet-derived distribution shifts. However, pre-training still yields much more robustness compared to training from scratch (linear trend obtained from [26]), especially on the ImageNet to ImageNet-Sketch shift.\n\nDomainNet Experiments\n\nFor each domain provided in DomainNet [29], we compare models trained from scratch on data from that domain, to models that have been pre-trained on ImageNet and then fine-tuned on the same domain. We then evaluate the model performance on all other domains that have not been used for training, which are considered OOD test sets. Figure 21 shows the resulting effective robustness for all pairs of domains, with x axis and y axis representing ID and OOD accuracies respectively. We find that for most pairs, ImageNet pre-training and training from scratch do not exhibit distinctive linear trends. An exception to this can be found in the case of fine-tuning on Infograph data and evaluating on Sketch data (middle panel of the last row). Using this setup, we proceed to evaluating the impact of intervening on two different aspects of the ImageNet pre-training distribution that have been found to be important in earlier experiments with iWildCam-WILDS: label granularity and data quantity.\n\nLabel Granularity\n\nOur setup follows a similar procedure as described previously in Section 4.2. In the left plot of Figure 22, when we collapse the label space to 232 superclasses (depth 7), or 17 superclasses (depth 4), the effective robustness obtained from ImageNet pre-training is reduced to about the same level as training from scratch. This suggests that the Infograph-Sketch distribution shift is much more sensitive to label granularity, compared to the iWildCam-WILDS shift.\n\nData Quantity\n\nWe repeat the experiment with pre-training data quantity described in Section 4.1. As seen from the right plot of Figure 22, using 25K, 50K, 100K and 150K subsets of ImageNet for pre-training results in a similar level of effective robustness after fine-tuning, which is a lot less than using the full ImageNet dataset, but still an improvement compared to training from scratch. As a sanity check, we find that using only 5K ImageNet images for pre-training leads to no robustness gain compared to training from scratch on Infograph dataset itself."}]}, {"page": 24, "text": "    ID=ClipArtDataset OOD=ClipArtDataset    ID=ClipArtDataset OOD=PaintingDataset    ID=ClipArtDataset OOD=SketchDataset      ID=ClipArtDataset OOD=RealDataset     ID=ClipArtDataset OOD=InfographDataset\n 90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained\n 80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch\n 70                                      70                                      70                                      70                                      70\n                                         60                                      60                                      60                                      60\n 60                                      50                                      50                                      50                                      50\n 50                                      40                                      40                                      40                                      40\n 40                                      30                                      30                                      30                                      30\n 30                                      20                                      20                                                                              20\n 20                                      10                                      10                                      20                                      10\n 10       5  10  2030405060 70 80  90             5  10  20304050 6070 80  90             5  10  2030 40506070 80  90    10       5  10  20 3040506070 80  90              5 10  20 30405060 7080  90\n    ID=PaintingDataset OOD=ClipArtDataset   ID=PaintingDataset OOD=PaintingDataset  ID=PaintingDataset OOD=SketchDataset     ID=PaintingDataset OOD=RealDataset    ID=PaintingDataset OOD=InfographDataset\n 90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained\n 80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch\n 70                                                                              70                                      70                                      70\n 60                                      70                                      60                                      60                                      60\n 50                                      60                                      50                                                                              50\n 40                                      50                                      40                                      50                                      40\n 30                                      40                                      30                                      40                                      30\n 20                                      30                                      20                                      30                                      20\n 10                                      20                                      10                                      20                                      10\n          5  10  2030405060 70 80  90    10       5  10  20304050 6070 80  90             5  10  2030 40506070 80  90    10       5  10  20 3040506070 80  90              5 10  20 30405060 7080  90\n    ID=SketchDataset OOD=ClipArtDataset     ID=SketchDataset OOD=PaintingDataset     ID=SketchDataset OOD=SketchDataset       ID=SketchDataset OOD=RealDataset      ID=SketchDataset OOD=InfographDataset\n 90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained\n 80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch\n 70                                      70                                      70                                      70                                      70\n 60                                      60                                                                              60                                      60\n 50                                      50                                      60                                      50                                      50\n 40                                      40                                      50                                      40                                      40\n 30                                      30                                      40                                      30                                      30\n 20                                      20                                      30                                      20                                      20\n 10                                      10                                      20                                      10                                      10\n          5  10  2030405060 70 80  90             5  10  20304050 6070 80  90    10       5  10  2030 40506070 80  90             5  10  20 3040506070 80  90              5 10  20 30405060 7080  90\n      ID=RealDataset OOD=ClipArtDataset      ID=RealDataset OOD=PaintingDataset       ID=RealDataset OOD=SketchDataset         ID=RealDataset OOD=RealDataset        ID=RealDataset OOD=InfographDataset\n 90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained\n 80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch\n 70                                      70                                      70                                      70                                      70\n                                                                                 60                                                                              60\n 60                                      60                                      50                                      60                                      50\n 50                                      50                                      40                                      50                                      40\n 40                                      40                                      30                                      40                                      30\n 30                                      30                                      20                                      30                                      20\n 20                                      20                                      10                                      20                                      10\n 10       5  10  2030405060 70 80  90    10       5  10  20304050 6070 80  90             5  10  2030 40506070 80  90    10       5  10  20 3040506070 80  90              5 10  20 30405060 7080  90\n    ID=InfographDataset OOD=ClipArtDataset ID=InfographDataset OOD=PaintingDataset  ID=InfographDataset OOD=SketchDataset    ID=InfographDataset OOD=RealDataset   ID=InfographDataset OOD=InfographDataset\n 90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained                      90      pretrained\n 80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch                    80      from scratch\n 70                                      70                                      70                                      70                                      70\n 60                                      60                                      60                                      60                                      60\n 50                                      50                                      50                                      50                                      50\n 40                                      40                                      40\n 30                                      30                                      30                                      40                                      40\n 20                                                                              20                                      30                                      30\n 10                                      20                                      10                                      20                                      20\n          5  10  2030405060 70 80  90    10       5  10  20304050 6070 80  90             5  10  2030 40506070 80  90    10       5  10  20 3040506070 80  90    10        5 10  20 30405060 7080  90\nFigure 21: We compare effective robustness of models pre-trained on ImageNet and those trained\nfrom scratch on each domain (see each row), using the data from the same domain as ID test set,\nand data from every other domain as the OOD test set (see each column). In most cases, ImageNet\npre-training and training from scratch yield similar downstream robustness, except for the Infograph\n& Sketch pair where we observe a sufficiently large gap in effective robustness.                 24", "md": "# Comparison of Model Robustness\n\n## Comparison of Model Robustness\n\nFigure 21: We compare effective robustness of models pre-trained on ImageNet and those trained\nfrom scratch on each domain (see each row), using the data from the same domain as ID test set,\nand data from every other domain as the OOD test set (see each column). In most cases, ImageNet\npre-training and training from scratch yield similar downstream robustness, except for the Infograph\n& Sketch pair where we observe a sufficiently large gap in effective robustness.\n\n|ID=ClipArtDataset OOD=ClipArtDataset|ID=ClipArtDataset OOD=PaintingDataset|ID=ClipArtDataset OOD=SketchDataset|ID=ClipArtDataset OOD=RealDataset|ID=ClipArtDataset OOD=InfographDataset|\n|---|---|---|---|---|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|70| |70| |70|\n| |60|60|60|60|\n|60|50|50|50|50|\n|50|40|40|40|40|\n|40|30|30|30|30|\n|30|20|20| |20|\n|20|10|10|20|10|\n|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|\n\n|ID=PaintingDataset OOD=ClipArtDataset|ID=PaintingDataset OOD=PaintingDataset|ID=PaintingDataset OOD=SketchDataset|ID=PaintingDataset OOD=RealDataset|ID=PaintingDataset OOD=InfographDataset|\n|---|---|---|---|---|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|<br/><br/># Comparison of Model Robustness<br/><br/><br/>## Comparison of Model Robustness<br/>Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained<br/>from scratch on each domain (see each row), using the data from the same domain as ID test set,<br/>and data from every other domain as the OOD test set (see each column). In most cases, ImageNet<br/>pre-training and training from scratch yield similar downstream robustness, except for the Infograph<br/>& Sketch pair where we observe a sufficiently large gap in effective robustness.<br/><br/>ID=ClipArtDataset OOD=ClipArtDataset|ID=ClipArtDataset OOD=PaintingDataset|ID=ClipArtDataset OOD=SketchDataset|ID=ClipArtDataset OOD=RealDataset|ID=ClipArtDataset OOD=InfographDataset|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|70| |70| |70|\n| |60|60|60|60|\n|60|50|50|50|50|\n|50|40|40|40|40|\n|40|30|30|30|30|\n|30|20|20| |20|\n|20|10|10|20|10|\n|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|\n\n|ID=PaintingDataset OOD=ClipArtDataset|ID=PaintingDataset OOD=PaintingDataset|ID=PaintingDataset OOD=SketchDataset|ID=PaintingDataset OOD=RealDataset|ID=PaintingDataset OOD=InfographDataset|\n|---|---|---|---|---|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|<br/><br/># Comparison of Model Robustness<br/><br/><br/>## Comparison of Model Robustness<br/>Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained<br/>from scratch on each domain (see each row), using the data from the same domain as ID test set,<br/>and data from every other domain as the OOD test set (see each column). In most cases, ImageNet<br/>pre-training and training from scratch yield similar downstream robustness, except for the Infograph<br/>& Sketch pair where we observe a sufficiently large gap in effective robustness.<br/><br/>ID=ClipArtDataset OOD=ClipArtDataset|ID=ClipArtDataset OOD=PaintingDataset|ID=ClipArtDataset OOD=SketchDataset|ID=ClipArtDataset OOD=RealDataset|ID=ClipArtDataset OOD=InfographDataset|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|70| |70| |70|\n| |60|60|60|60|\n|60|50|50|50|50|\n|50|40|40|40|40|\n|40|30|30|30|30|\n|30|20|20| |20|\n|20|10|10|20|10|\n|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|\n\nID=PaintingDataset OOD=ClipArtDataset\nID=PaintingDataset OOD=PaintingDataset\nID=PaintingDataset OOD=SketchDataset\nID=PaintingDataset OOD=RealDataset\nID=PaintingDataset OOD=InfographDataset\n\n90 pretrained\n90 pretrained\n90 pretrained\n90 pretrained\n90 pretrained\n\n80 from scratch\n80 from scratch\n80 from scratch\n80 from scratch\n80 from scratch\n\nThe HTML code provided above includes the comparison of model robustness data in tabular form. The tables display the comparison of models pre-trained on ImageNet and those trained from scratch on different domains.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Comparison of Model Robustness", "md": "# Comparison of Model Robustness"}, {"type": "heading", "lvl": 2, "value": "Comparison of Model Robustness", "md": "## Comparison of Model Robustness"}, {"type": "text", "value": "Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained\nfrom scratch on each domain (see each row), using the data from the same domain as ID test set,\nand data from every other domain as the OOD test set (see each column). In most cases, ImageNet\npre-training and training from scratch yield similar downstream robustness, except for the Infograph\n& Sketch pair where we observe a sufficiently large gap in effective robustness.", "md": "Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained\nfrom scratch on each domain (see each row), using the data from the same domain as ID test set,\nand data from every other domain as the OOD test set (see each column). In most cases, ImageNet\npre-training and training from scratch yield similar downstream robustness, except for the Infograph\n& Sketch pair where we observe a sufficiently large gap in effective robustness."}, {"type": "table", "rows": [["ID=ClipArtDataset OOD=ClipArtDataset", "ID=ClipArtDataset OOD=PaintingDataset", "ID=ClipArtDataset OOD=SketchDataset", "ID=ClipArtDataset OOD=RealDataset", "ID=ClipArtDataset OOD=InfographDataset"], ["90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained"], ["80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch"], ["70", "", "70", "", "70"], ["", "60", "60", "60", "60"], ["60", "50", "50", "50", "50"], ["50", "40", "40", "40", "40"], ["40", "30", "30", "30", "30"], ["30", "20", "20", "", "20"], ["20", "10", "10", "20", "10"], ["10 5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90", "10 5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90"]], "md": "|ID=ClipArtDataset OOD=ClipArtDataset|ID=ClipArtDataset OOD=PaintingDataset|ID=ClipArtDataset OOD=SketchDataset|ID=ClipArtDataset OOD=RealDataset|ID=ClipArtDataset OOD=InfographDataset|\n|---|---|---|---|---|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|70| |70| |70|\n| |60|60|60|60|\n|60|50|50|50|50|\n|50|40|40|40|40|\n|40|30|30|30|30|\n|30|20|20| |20|\n|20|10|10|20|10|\n|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|", "isPerfectTable": true, "csv": "\"ID=ClipArtDataset OOD=ClipArtDataset\",\"ID=ClipArtDataset OOD=PaintingDataset\",\"ID=ClipArtDataset OOD=SketchDataset\",\"ID=ClipArtDataset OOD=RealDataset\",\"ID=ClipArtDataset OOD=InfographDataset\"\n\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\"\n\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\"\n\"70\",\"\",\"70\",\"\",\"70\"\n\"\",\"60\",\"60\",\"60\",\"60\"\n\"60\",\"50\",\"50\",\"50\",\"50\"\n\"50\",\"40\",\"40\",\"40\",\"40\"\n\"40\",\"30\",\"30\",\"30\",\"30\"\n\"30\",\"20\",\"20\",\"\",\"20\"\n\"20\",\"10\",\"10\",\"20\",\"10\"\n\"10 5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"10 5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\""}, {"type": "table", "rows": [["ID=PaintingDataset OOD=ClipArtDataset", "ID=PaintingDataset OOD=PaintingDataset", "ID=PaintingDataset OOD=SketchDataset", "ID=PaintingDataset OOD=RealDataset", "ID=PaintingDataset OOD=InfographDataset"], ["90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained"], ["80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch"], ["<br/><br/># Comparison of Model Robustness<br/><br/><br/>## Comparison of Model Robustness<br/>Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained<br/>from scratch on each domain (see each row), using the data from the same domain as ID test set,<br/>and data from every other domain as the OOD test set (see each column). In most cases, ImageNet<br/>pre-training and training from scratch yield similar downstream robustness, except for the Infograph<br/>& Sketch pair where we observe a sufficiently large gap in effective robustness.<br/><br/>ID=ClipArtDataset OOD=ClipArtDataset", "ID=ClipArtDataset OOD=PaintingDataset", "ID=ClipArtDataset OOD=SketchDataset", "ID=ClipArtDataset OOD=RealDataset", "ID=ClipArtDataset OOD=InfographDataset"], ["90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained"], ["80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch"], ["70", "", "70", "", "70"], ["", "60", "60", "60", "60"], ["60", "50", "50", "50", "50"], ["50", "40", "40", "40", "40"], ["40", "30", "30", "30", "30"], ["30", "20", "20", "", "20"], ["20", "10", "10", "20", "10"], ["10 5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90", "10 5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90"]], "md": "|ID=PaintingDataset OOD=ClipArtDataset|ID=PaintingDataset OOD=PaintingDataset|ID=PaintingDataset OOD=SketchDataset|ID=PaintingDataset OOD=RealDataset|ID=PaintingDataset OOD=InfographDataset|\n|---|---|---|---|---|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|<br/><br/># Comparison of Model Robustness<br/><br/><br/>## Comparison of Model Robustness<br/>Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained<br/>from scratch on each domain (see each row), using the data from the same domain as ID test set,<br/>and data from every other domain as the OOD test set (see each column). In most cases, ImageNet<br/>pre-training and training from scratch yield similar downstream robustness, except for the Infograph<br/>& Sketch pair where we observe a sufficiently large gap in effective robustness.<br/><br/>ID=ClipArtDataset OOD=ClipArtDataset|ID=ClipArtDataset OOD=PaintingDataset|ID=ClipArtDataset OOD=SketchDataset|ID=ClipArtDataset OOD=RealDataset|ID=ClipArtDataset OOD=InfographDataset|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|70| |70| |70|\n| |60|60|60|60|\n|60|50|50|50|50|\n|50|40|40|40|40|\n|40|30|30|30|30|\n|30|20|20| |20|\n|20|10|10|20|10|\n|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|", "isPerfectTable": true, "csv": "\"ID=PaintingDataset OOD=ClipArtDataset\",\"ID=PaintingDataset OOD=PaintingDataset\",\"ID=PaintingDataset OOD=SketchDataset\",\"ID=PaintingDataset OOD=RealDataset\",\"ID=PaintingDataset OOD=InfographDataset\"\n\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\"\n\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\"\n\"<br/><br/># Comparison of Model Robustness<br/><br/><br/>## Comparison of Model Robustness<br/>Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained<br/>from scratch on each domain (see each row), using the data from the same domain as ID test set,<br/>and data from every other domain as the OOD test set (see each column). In most cases, ImageNet<br/>pre-training and training from scratch yield similar downstream robustness, except for the Infograph<br/>& Sketch pair where we observe a sufficiently large gap in effective robustness.<br/><br/>ID=ClipArtDataset OOD=ClipArtDataset\",\"ID=ClipArtDataset OOD=PaintingDataset\",\"ID=ClipArtDataset OOD=SketchDataset\",\"ID=ClipArtDataset OOD=RealDataset\",\"ID=ClipArtDataset OOD=InfographDataset\"\n\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\"\n\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\"\n\"70\",\"\",\"70\",\"\",\"70\"\n\"\",\"60\",\"60\",\"60\",\"60\"\n\"60\",\"50\",\"50\",\"50\",\"50\"\n\"50\",\"40\",\"40\",\"40\",\"40\"\n\"40\",\"30\",\"30\",\"30\",\"30\"\n\"30\",\"20\",\"20\",\"\",\"20\"\n\"20\",\"10\",\"10\",\"20\",\"10\"\n\"10 5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"10 5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\""}, {"type": "table", "rows": [["ID=PaintingDataset OOD=ClipArtDataset", "ID=PaintingDataset OOD=PaintingDataset", "ID=PaintingDataset OOD=SketchDataset", "ID=PaintingDataset OOD=RealDataset", "ID=PaintingDataset OOD=InfographDataset"], ["90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained"], ["80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch"], ["<br/><br/># Comparison of Model Robustness<br/><br/><br/>## Comparison of Model Robustness<br/>Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained<br/>from scratch on each domain (see each row), using the data from the same domain as ID test set,<br/>and data from every other domain as the OOD test set (see each column). In most cases, ImageNet<br/>pre-training and training from scratch yield similar downstream robustness, except for the Infograph<br/>& Sketch pair where we observe a sufficiently large gap in effective robustness.<br/><br/>ID=ClipArtDataset OOD=ClipArtDataset", "ID=ClipArtDataset OOD=PaintingDataset", "ID=ClipArtDataset OOD=SketchDataset", "ID=ClipArtDataset OOD=RealDataset", "ID=ClipArtDataset OOD=InfographDataset"], ["90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained"], ["80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch", "80 from scratch"], ["70", "", "70", "", "70"], ["", "60", "60", "60", "60"], ["60", "50", "50", "50", "50"], ["50", "40", "40", "40", "40"], ["40", "30", "30", "30", "30"], ["30", "20", "20", "", "20"], ["20", "10", "10", "20", "10"], ["10 5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90", "10 5 10 $20^{\\circ}$30 40 50 60 70 80 90", "5 10 $20^{\\circ}$30 40 50 60 70 80 90"]], "md": "|ID=PaintingDataset OOD=ClipArtDataset|ID=PaintingDataset OOD=PaintingDataset|ID=PaintingDataset OOD=SketchDataset|ID=PaintingDataset OOD=RealDataset|ID=PaintingDataset OOD=InfographDataset|\n|---|---|---|---|---|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|<br/><br/># Comparison of Model Robustness<br/><br/><br/>## Comparison of Model Robustness<br/>Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained<br/>from scratch on each domain (see each row), using the data from the same domain as ID test set,<br/>and data from every other domain as the OOD test set (see each column). In most cases, ImageNet<br/>pre-training and training from scratch yield similar downstream robustness, except for the Infograph<br/>& Sketch pair where we observe a sufficiently large gap in effective robustness.<br/><br/>ID=ClipArtDataset OOD=ClipArtDataset|ID=ClipArtDataset OOD=PaintingDataset|ID=ClipArtDataset OOD=SketchDataset|ID=ClipArtDataset OOD=RealDataset|ID=ClipArtDataset OOD=InfographDataset|\n|90 pretrained|90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n|80 from scratch|80 from scratch|80 from scratch|80 from scratch|80 from scratch|\n|70| |70| |70|\n| |60|60|60|60|\n|60|50|50|50|50|\n|50|40|40|40|40|\n|40|30|30|30|30|\n|30|20|20| |20|\n|20|10|10|20|10|\n|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|10 5 10 $20^{\\circ}$30 40 50 60 70 80 90|5 10 $20^{\\circ}$30 40 50 60 70 80 90|", "isPerfectTable": true, "csv": "\"ID=PaintingDataset OOD=ClipArtDataset\",\"ID=PaintingDataset OOD=PaintingDataset\",\"ID=PaintingDataset OOD=SketchDataset\",\"ID=PaintingDataset OOD=RealDataset\",\"ID=PaintingDataset OOD=InfographDataset\"\n\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\"\n\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\"\n\"<br/><br/># Comparison of Model Robustness<br/><br/><br/>## Comparison of Model Robustness<br/>Figure 21: We compare effective robustness of models pre-trained on ImageNet and those trained<br/>from scratch on each domain (see each row), using the data from the same domain as ID test set,<br/>and data from every other domain as the OOD test set (see each column). In most cases, ImageNet<br/>pre-training and training from scratch yield similar downstream robustness, except for the Infograph<br/>& Sketch pair where we observe a sufficiently large gap in effective robustness.<br/><br/>ID=ClipArtDataset OOD=ClipArtDataset\",\"ID=ClipArtDataset OOD=PaintingDataset\",\"ID=ClipArtDataset OOD=SketchDataset\",\"ID=ClipArtDataset OOD=RealDataset\",\"ID=ClipArtDataset OOD=InfographDataset\"\n\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\"\n\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 from scratch\"\n\"70\",\"\",\"70\",\"\",\"70\"\n\"\",\"60\",\"60\",\"60\",\"60\"\n\"60\",\"50\",\"50\",\"50\",\"50\"\n\"50\",\"40\",\"40\",\"40\",\"40\"\n\"40\",\"30\",\"30\",\"30\",\"30\"\n\"30\",\"20\",\"20\",\"\",\"20\"\n\"20\",\"10\",\"10\",\"20\",\"10\"\n\"10 5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"10 5 10 $20^{\\circ}$30 40 50 60 70 80 90\",\"5 10 $20^{\\circ}$30 40 50 60 70 80 90\""}, {"type": "text", "value": "ID=PaintingDataset OOD=ClipArtDataset\nID=PaintingDataset OOD=PaintingDataset\nID=PaintingDataset OOD=SketchDataset\nID=PaintingDataset OOD=RealDataset\nID=PaintingDataset OOD=InfographDataset\n\n90 pretrained\n90 pretrained\n90 pretrained\n90 pretrained\n90 pretrained\n\n80 from scratch\n80 from scratch\n80 from scratch\n80 from scratch\n80 from scratch\n\nThe HTML code provided above includes the comparison of model robustness data in tabular form. The tables display the comparison of models pre-trained on ImageNet and those trained from scratch on different domains.", "md": "ID=PaintingDataset OOD=ClipArtDataset\nID=PaintingDataset OOD=PaintingDataset\nID=PaintingDataset OOD=SketchDataset\nID=PaintingDataset OOD=RealDataset\nID=PaintingDataset OOD=InfographDataset\n\n90 pretrained\n90 pretrained\n90 pretrained\n90 pretrained\n90 pretrained\n\n80 from scratch\n80 from scratch\n80 from scratch\n80 from scratch\n80 from scratch\n\nThe HTML code provided above includes the comparison of model robustness data in tabular form. The tables display the comparison of models pre-trained on ImageNet and those trained from scratch on different domains."}]}, {"page": 25, "text": "pArtDataset               ID=RealDataset OOD=PaintingDataset                         ID=RealDataset OOD=SketchDataset                           ID=RealD\n=PaintingDataset               ID=RealDataset OOD=SketchDataset                           ID=RealDataset OOD=RealDataset                         ID=Real\n                   90  90      pretrained                                     90  90      pretrained                                   90  90      pretrai\n                                   pretrained                                                 pretrained                                               pre\n                               from scratch                                               from scratch                                             from sc\n                   80  80          from scratch                               80  80          from scratch                             80  80          fro\n                   70  70                                                     70  70                                                   70  70\n                       60                                                     60                                                           60\n                   60  50                                                     50  60                                                   60  50\n                   50  40                                                     40  50                                                   50  40\n                   40  30                                                     30  40                                                   40  30\n                   30  20                                                     20  30                                                   30  20\n                   20  10                                                     10  20                                                   20  10\n                   10                                                             10                                                   10\n6070 80   90                     5   10   20   30 405060   70  80    90                     5   10   20  30 405060    70  80   90                    5   1\n5060 70  80   90                     5   10    20  30 405060   70  80    90                     5   10   20  30 405060    70  80   90                    5\nClipArtDataset         ID=InfographDataset OOD=PaintingDataset                    ID=InfographDataset OOD=SketchDataset                      ID=Infograp\nD=PaintingDataset          ID=InfographDataset OOD=SketchDataset                       ID=InfographDataset OOD=RealDataset                    ID=Infogra\n                   90  90      pretrained                                     90          pretrained                                       90      pretrai\n                                   pretrained                                     90          pretrained                               90              pre\n                               from scratch                                               from scratch                                             from sc\n                   80  80          from scratch                               80              from scratch                                 80          fro\n                               ImageNet-5k                                                ImageNet-5k                                  80          ImageN\n                   70  70          17 classes                                 70  80          17 classes\n                               ImageNet-25k                                               ImageNet-25k                                 70  70      ImageN\n                   60  60          232 classes                                60  70          232 classes\n                   50  50      ImageNet-50k                                   50  60      ImageNet-50k                                 60  60      ImageN\n                   40          ImageNet-100k                                  40  50      ImageNet-100k                                50  50      ImageN\n                       40                                                     30  40                                                   40\n                   30  30      ImageNet-150k                                  20  30      ImageNet-150k                                30  40      ImageN\n                   20  20                                                         20                                                   20  30\n                   10  10                                                     10  10                                                   10  20\n                                                                                                                                           10\n6070 80   90                     5   10   20   30 405060   70  80    90                     5   10   20  30 405060    70  80   90                    5   1\n5060 70  80   90                     5   10    20  30 405060   70  80    90                     5   10   20  30 405060    70  80   90                    5\n                  Figure 22:    (left)  Effect of label granularity.       (right)    Effect of data quantity.       We perform the\n                  same interventions described earlier in Sections 4.1 and 4.2, on these two aspects of the ImageNet\n                  pre-training distribution that have been shown to be influential to downstream robustness. The ID\n                  and OOD data comes from Infograph and Sketch domains respectively. We again find that using\n                  more coarse labels and smaller data quantity during pre-training lowers the effective robustness of\n                  fine-tuned models significantly.\n                                                                            25", "md": "| |PartDataset|ID=RealDataset OOD=PaintingDataset|ID=RealDataset OOD=SketchDataset|ID=RealDataset OOD=RealDataset|\n|---|---|---|---|---|\n| |90 90 pretrained|90 90 pretrained|90 90 pretrained|90 90 pretrained|\n| |80 80 from scratch|80 80 from scratch|80 80 from scratch|80 80 from scratch|\n| |70 70|70 70|70 70|70 70|\n| |60 50|50 60|60 50| |\n| |50 40|40 50|50 40| |\n| |40 30|30 40|40 30| |\n| |30 20|20 30|30 20| |\n| |20 10|10 20|20 10| |\n| |10|10|10| |\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n& 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n\\end{array}\n$$\n\n| |ClipArtDataset|ID=InfographDataset OOD=PaintingDataset|ID=InfographDataset OOD=SketchDataset|ID=InfographDataset OOD=RealDataset|\n|---|---|---|---|---|\n| |90 90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n| |80 80 from scratch|80 from scratch|80 from scratch|80 ImageNet-5k|\n| |70 70 17 classes|70 80 17 classes|70 70 ImageNet-25k|70 70 ImageNet-25k|\n| |60 60 232 classes|60 70 232 classes|60 60 ImageNet-50k|60 60 ImageNet-50k|\n| |50 50 ImageNet-100k|50 50 ImageNet-100k|50 ImageNet-100k|50 ImageNet-100k|\n| |40 40|30 40|40| |\n| |30 30 ImageNet-150k|20 30 ImageNet-150k|30 40 ImageNet-150k| |\n| |20 20|20|20 30| |\n| |10 10|10 10|10 20| |\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n& 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 22: (left) Effect of label granularity. (right) Effect of data quantity. We perform the same interventions described earlier in Sections 4.1 and 4.2, on these two aspects of the ImageNet pre-training distribution that have been shown to be influential to downstream robustness. The ID and OOD data comes from Infograph and Sketch domains respectively. We again find that using more coarse labels and smaller data quantity during pre-training lowers the effective robustness of fine-tuned models significantly.", "images": [], "items": [{"type": "table", "rows": [["", "PartDataset", "ID=RealDataset OOD=PaintingDataset", "ID=RealDataset OOD=SketchDataset", "ID=RealDataset OOD=RealDataset"], ["", "90 90 pretrained", "90 90 pretrained", "90 90 pretrained", "90 90 pretrained"], ["", "80 80 from scratch", "80 80 from scratch", "80 80 from scratch", "80 80 from scratch"], ["", "70 70", "70 70", "70 70", "70 70"], ["", "60 50", "50 60", "60 50", ""], ["", "50 40", "40 50", "50 40", ""], ["", "40 30", "30 40", "40 30", ""], ["", "30 20", "20 30", "30 20", ""], ["", "20 10", "10 20", "20 10", ""], ["", "10", "10", "10", ""]], "md": "| |PartDataset|ID=RealDataset OOD=PaintingDataset|ID=RealDataset OOD=SketchDataset|ID=RealDataset OOD=RealDataset|\n|---|---|---|---|---|\n| |90 90 pretrained|90 90 pretrained|90 90 pretrained|90 90 pretrained|\n| |80 80 from scratch|80 80 from scratch|80 80 from scratch|80 80 from scratch|\n| |70 70|70 70|70 70|70 70|\n| |60 50|50 60|60 50| |\n| |50 40|40 50|50 40| |\n| |40 30|30 40|40 30| |\n| |30 20|20 30|30 20| |\n| |20 10|10 20|20 10| |\n| |10|10|10| |", "isPerfectTable": true, "csv": "\"\",\"PartDataset\",\"ID=RealDataset OOD=PaintingDataset\",\"ID=RealDataset OOD=SketchDataset\",\"ID=RealDataset OOD=RealDataset\"\n\"\",\"90 90 pretrained\",\"90 90 pretrained\",\"90 90 pretrained\",\"90 90 pretrained\"\n\"\",\"80 80 from scratch\",\"80 80 from scratch\",\"80 80 from scratch\",\"80 80 from scratch\"\n\"\",\"70 70\",\"70 70\",\"70 70\",\"70 70\"\n\"\",\"60 50\",\"50 60\",\"60 50\",\"\"\n\"\",\"50 40\",\"40 50\",\"50 40\",\"\"\n\"\",\"40 30\",\"30 40\",\"40 30\",\"\"\n\"\",\"30 20\",\"20 30\",\"30 20\",\"\"\n\"\",\"20 10\",\"10 20\",\"20 10\",\"\"\n\"\",\"10\",\"10\",\"10\",\"\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n& 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n\\end{array}\n$$", "md": "$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n& 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "table", "rows": [["", "ClipArtDataset", "ID=InfographDataset OOD=PaintingDataset", "ID=InfographDataset OOD=SketchDataset", "ID=InfographDataset OOD=RealDataset"], ["", "90 90 pretrained", "90 pretrained", "90 pretrained", "90 pretrained"], ["", "80 80 from scratch", "80 from scratch", "80 from scratch", "80 ImageNet-5k"], ["", "70 70 17 classes", "70 80 17 classes", "70 70 ImageNet-25k", "70 70 ImageNet-25k"], ["", "60 60 232 classes", "60 70 232 classes", "60 60 ImageNet-50k", "60 60 ImageNet-50k"], ["", "50 50 ImageNet-100k", "50 50 ImageNet-100k", "50 ImageNet-100k", "50 ImageNet-100k"], ["", "40 40", "30 40", "40", ""], ["", "30 30 ImageNet-150k", "20 30 ImageNet-150k", "30 40 ImageNet-150k", ""], ["", "20 20", "20", "20 30", ""], ["", "10 10", "10 10", "10 20", ""]], "md": "| |ClipArtDataset|ID=InfographDataset OOD=PaintingDataset|ID=InfographDataset OOD=SketchDataset|ID=InfographDataset OOD=RealDataset|\n|---|---|---|---|---|\n| |90 90 pretrained|90 pretrained|90 pretrained|90 pretrained|\n| |80 80 from scratch|80 from scratch|80 from scratch|80 ImageNet-5k|\n| |70 70 17 classes|70 80 17 classes|70 70 ImageNet-25k|70 70 ImageNet-25k|\n| |60 60 232 classes|60 70 232 classes|60 60 ImageNet-50k|60 60 ImageNet-50k|\n| |50 50 ImageNet-100k|50 50 ImageNet-100k|50 ImageNet-100k|50 ImageNet-100k|\n| |40 40|30 40|40| |\n| |30 30 ImageNet-150k|20 30 ImageNet-150k|30 40 ImageNet-150k| |\n| |20 20|20|20 30| |\n| |10 10|10 10|10 20| |", "isPerfectTable": true, "csv": "\"\",\"ClipArtDataset\",\"ID=InfographDataset OOD=PaintingDataset\",\"ID=InfographDataset OOD=SketchDataset\",\"ID=InfographDataset OOD=RealDataset\"\n\"\",\"90 90 pretrained\",\"90 pretrained\",\"90 pretrained\",\"90 pretrained\"\n\"\",\"80 80 from scratch\",\"80 from scratch\",\"80 from scratch\",\"80 ImageNet-5k\"\n\"\",\"70 70 17 classes\",\"70 80 17 classes\",\"70 70 ImageNet-25k\",\"70 70 ImageNet-25k\"\n\"\",\"60 60 232 classes\",\"60 70 232 classes\",\"60 60 ImageNet-50k\",\"60 60 ImageNet-50k\"\n\"\",\"50 50 ImageNet-100k\",\"50 50 ImageNet-100k\",\"50 ImageNet-100k\",\"50 ImageNet-100k\"\n\"\",\"40 40\",\"30 40\",\"40\",\"\"\n\"\",\"30 30 ImageNet-150k\",\"20 30 ImageNet-150k\",\"30 40 ImageNet-150k\",\"\"\n\"\",\"20 20\",\"20\",\"20 30\",\"\"\n\"\",\"10 10\",\"10 10\",\"10 20\",\"\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n& 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 22: (left) Effect of label granularity. (right) Effect of data quantity. We perform the same interventions described earlier in Sections 4.1 and 4.2, on these two aspects of the ImageNet pre-training distribution that have been shown to be influential to downstream robustness. The ID and OOD data comes from Infograph and Sketch domains respectively. We again find that using more coarse labels and smaller data quantity during pre-training lowers the effective robustness of fine-tuned models significantly.", "md": "$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n& 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 & 5 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 22: (left) Effect of label granularity. (right) Effect of data quantity. We perform the same interventions described earlier in Sections 4.1 and 4.2, on these two aspects of the ImageNet pre-training distribution that have been shown to be influential to downstream robustness. The ID and OOD data comes from Infograph and Sketch domains respectively. We again find that using more coarse labels and smaller data quantity during pre-training lowers the effective robustness of fine-tuned models significantly."}]}, {"page": 26, "text": "  Superclass                       Subclasses\n  garment                          trench coat, abaya, gown, poncho, military uniform, jersey, cloak, bikini,\n                                   miniskirt, swimming trunks, lab coat, brassiere\n  bird                             African grey, bee eater, coucal, American coot, indigo bunting, king\n                                   penguin, spoonbill, limpkin, quail, kite, prairie chicken, red-breasted\n                                   merganser\n  reptile, reptilian               Gila monster, agama, triceratops, African chameleon, thunder snake, In-\n                                   dian cobra, green snake, mud turtle, water snake, loggerhead, sidewinder,\n                                   leatherback turtle\n  arthropod                        rock crab, black and gold garden spider, tiger beetle, black widow,\n                                   barn spider, leafhopper, ground beetle, fiddler crab, bee, walking stick,\n                                   cabbage butterfly, admiral\n  mammal, mammalian                Siamese cat, ibex, tiger, hippopotamus, Norwegian elkhound, dugong,\n                                   colobus, Samoyed, Persian cat, Irish wolfhound, English setter, llama\n  fish                             sturgeon, stingray, coho, great white shark, lionfish, gar, goldfish, ham-\n                                   merhead, rock beauty, anemone fish, barracouta, electric ray\n  accessory, accoutrement,         shower cap, stole, wig, Windsor tie, bib, necklace, bow tie, gasmask,\n  accouterment                     umbrella, cowboy boot, Christmas stocking, bathing cap\n  appliance                        espresso maker, sewing machine, stove, waffle iron, rotisserie, lawn mower,\n                                   electric fan, dishwasher, iron, microwave, vacuum, space heater\n  craft                            gondola, schooner, liner, airship, speedboat, airliner, canoe, trimaran,\n                                   balloon, lifeboat, yawl, submarine\n  equipment                        photocopier, mouse, dumbbell, iPod, home theater, projector, cassette\n                                   player, hand-held computer, tennis ball, tape player, snorkel, monitor\n  furniture, piece of furni-       bookcase, throne, barber chair, four-poster, desk, medicine chest, crib,\n  ture, article of furniture       chest, sliding door, toilet seat, rocking chair, wardrobe\n  instrument                       chime, wine bottle, ladle, reel, wall clock, hammer, wok, abacus, assault\n                                   rifle, projectile, safety pin, corkscrew\n  man-made structure, con-         lumbermill, scoreboard, monastery, church, tobacco shop, drilling plat-\n  struction                        form, dam, fountain, bell cote, yurt, bookshop, prison\n  wheeled vehicle                  convertible, oxcart, electric locomotive, tricycle, fire engine, bicycle-\n                                   built-for-two, moving van, golfcart, steam locomotive, jinrikisha, tractor,\n                                   ambulance\n  cooked    food,    prepared      consomme, burrito, meat loaf, bagel, ice cream, French loaf, ice lolly, hot\n  food                             pot, cheeseburger, potpie, trifle, mashed potato\n  produce,     green    goods,     broccoli,    cauliflower,    mushroom,       artichoke,     acorn,    head   cabbage,\n  green groceries,      garden     spaghetti squash, jackfruit, cucumber, orange, hip, banana\n  truck\nTable 3: Subclass-superclass mapping for the subset of ImageNet that we use to control per-class\ndiversity, constructed based on the BREEDS hierarchy [38].\n                                                            26", "md": "|Superclass|Subclasses|\n|---|---|\n|garment|$trench\\ coat,\\ abaya,\\ gown,\\ poncho,\\ military\\ uniform,\\ jersey,\\ cloak,\\ bikini,$ $miniskirt,\\ swimming\\ trunks,\\ lab\\ coat,\\ brassiere$|\n|bird|$African\\ grey,\\ bee\\ eater,\\ coucal,\\ American\\ coot,\\ indigo\\ bunting,\\ king\\ penguin,\\ spoonbill,$ $limpkin,\\ quail,\\ kite,\\ prairie\\ chicken,\\ red-breasted\\ merganser$|\n|reptile, reptilian|$Gila\\ monster,\\ agama,\\ triceratops,\\ African\\ chameleon,\\ thunder\\ snake,\\ Indian\\ cobra,\\ green\\ snake,$ $mud\\ turtle,\\ water\\ snake,\\ loggerhead,\\ sidewinder,\\ leatherback\\ turtle$|\n|arthropod|$rock\\ crab,\\ black\\ and\\ gold\\ garden\\ spider,\\ tiger\\ beetle,\\ black\\ widow,$ $barn\\ spider,\\ leafhopper,\\ ground\\ beetle,\\ fiddler\\ crab,\\ bee,\\ walking\\ stick,$ $cabbage\\ butterfly,\\ admiral$|\n|mammal, mammalian|$Siamese\\ cat,\\ ibex,\\ tiger,\\ hippopotamus,\\ Norwegian\\ elkhound,\\ dugong,$ $colobus,\\ Samoyed,\\ Persian\\ cat,\\ Irish\\ wolfhound,\\ English\\ setter,\\ llama$|\n|fish|$sturgeon,\\ stingray,\\ coho,\\ great\\ white\\ shark,\\ lionfish,\\ gar,\\ goldfish,\\ hammerhead,$ $rock\\ beauty,\\ anemone\\ fish,\\ barracouta,\\ electric\\ ray$|\n|accessory, accoutrement, accouterment|$shower\\ cap,\\ stole,\\ wig,\\ Windsor\\ tie,\\ bib,\\ necklace,\\ bow\\ tie,\\ gasmask,$ $umbrella,\\ cowboy\\ boot,\\ Christmas\\ stocking,\\ bathing\\ cap$|\n|appliance|$espresso\\ maker,\\ sewing\\ machine,\\ stove,\\ waffle\\ iron,\\ rotisserie,\\ lawn\\ mower,$ $electric\\ fan,\\ dishwasher,\\ iron,\\ microwave,\\ vacuum,\\ space\\ heater$|\n|craft|$gondola,\\ schooner,\\ liner,\\ airship,\\ speedboat,\\ airliner,\\ canoe,\\ trimaran,$ $balloon,\\ lifeboat,\\ yawl,\\ submarine$|\n|equipment|$photocopier,\\ mouse,\\ dumbbell,\\ iPod,\\ home\\ theater,\\ projector,\\ cassette\\ player,$ $hand-held\\ computer,\\ tennis\\ ball,\\ tape\\ player,\\ snorkel,\\ monitor$|\n|furniture, piece of furniture, article of furniture|$bookcase,\\ throne,\\ barber\\ chair,\\ four-poster,\\ desk,\\ medicine\\ chest,\\ crib,$ $chest,\\ sliding\\ door,\\ toilet\\ seat,\\ rocking\\ chair,\\ wardrobe$|\n|instrument|$chime,\\ wine\\ bottle,\\ ladle,\\ reel,\\ wall\\ clock,\\ hammer,\\ wok,\\ abacus,\\ assault$ $rifle,\\ projectile,\\ safety\\ pin,\\ corkscrew$|\n|man-made structure, construction|$lumbermill,\\ scoreboard,\\ monastery,\\ church,\\ tobacco\\ shop,\\ drilling\\ platform,$ $dam,\\ fountain,\\ bell\\ cote,\\ yurt,\\ bookshop,\\ prison$|\n|wheeled vehicle|$convertible,\\ oxcart,\\ electric\\ locomotive,\\ tricycle,\\ fire\\ engine,\\ bicycle-built-for-two,$ $moving\\ van,\\ golfcart,\\ steam\\ locomotive,\\ jinrikisha,\\ tractor,\\ ambulance$|\n|cooked food, prepared food|$consomme,\\ burrito,\\ meat\\ loaf,\\ bagel,\\ ice\\ cream,\\ French\\ loaf,\\ ice\\ lolly,\\ hot$ $pot,\\ cheeseburger,\\ potpie,\\ trifle,\\ mashed\\ potato$|\n|produce, green goods, green groceries, garden truck|$broccoli,\\ cauliflower,\\ mushroom,\\ artichoke,\\ acorn,\\ head\\ cabbage,$ $spaghetti\\ squash,\\ jackfruit,\\ cucumber,\\ orange,\\ hip,\\ banana$|", "images": [], "items": [{"type": "table", "rows": [["Superclass", "Subclasses"], ["garment", "$trench\\ coat,\\ abaya,\\ gown,\\ poncho,\\ military\\ uniform,\\ jersey,\\ cloak,\\ bikini,$ $miniskirt,\\ swimming\\ trunks,\\ lab\\ coat,\\ brassiere$"], ["bird", "$African\\ grey,\\ bee\\ eater,\\ coucal,\\ American\\ coot,\\ indigo\\ bunting,\\ king\\ penguin,\\ spoonbill,$ $limpkin,\\ quail,\\ kite,\\ prairie\\ chicken,\\ red-breasted\\ merganser$"], ["reptile, reptilian", "$Gila\\ monster,\\ agama,\\ triceratops,\\ African\\ chameleon,\\ thunder\\ snake,\\ Indian\\ cobra,\\ green\\ snake,$ $mud\\ turtle,\\ water\\ snake,\\ loggerhead,\\ sidewinder,\\ leatherback\\ turtle$"], ["arthropod", "$rock\\ crab,\\ black\\ and\\ gold\\ garden\\ spider,\\ tiger\\ beetle,\\ black\\ widow,$ $barn\\ spider,\\ leafhopper,\\ ground\\ beetle,\\ fiddler\\ crab,\\ bee,\\ walking\\ stick,$ $cabbage\\ butterfly,\\ admiral$"], ["mammal, mammalian", "$Siamese\\ cat,\\ ibex,\\ tiger,\\ hippopotamus,\\ Norwegian\\ elkhound,\\ dugong,$ $colobus,\\ Samoyed,\\ Persian\\ cat,\\ Irish\\ wolfhound,\\ English\\ setter,\\ llama$"], ["fish", "$sturgeon,\\ stingray,\\ coho,\\ great\\ white\\ shark,\\ lionfish,\\ gar,\\ goldfish,\\ hammerhead,$ $rock\\ beauty,\\ anemone\\ fish,\\ barracouta,\\ electric\\ ray$"], ["accessory, accoutrement, accouterment", "$shower\\ cap,\\ stole,\\ wig,\\ Windsor\\ tie,\\ bib,\\ necklace,\\ bow\\ tie,\\ gasmask,$ $umbrella,\\ cowboy\\ boot,\\ Christmas\\ stocking,\\ bathing\\ cap$"], ["appliance", "$espresso\\ maker,\\ sewing\\ machine,\\ stove,\\ waffle\\ iron,\\ rotisserie,\\ lawn\\ mower,$ $electric\\ fan,\\ dishwasher,\\ iron,\\ microwave,\\ vacuum,\\ space\\ heater$"], ["craft", "$gondola,\\ schooner,\\ liner,\\ airship,\\ speedboat,\\ airliner,\\ canoe,\\ trimaran,$ $balloon,\\ lifeboat,\\ yawl,\\ submarine$"], ["equipment", "$photocopier,\\ mouse,\\ dumbbell,\\ iPod,\\ home\\ theater,\\ projector,\\ cassette\\ player,$ $hand-held\\ computer,\\ tennis\\ ball,\\ tape\\ player,\\ snorkel,\\ monitor$"], ["furniture, piece of furniture, article of furniture", "$bookcase,\\ throne,\\ barber\\ chair,\\ four-poster,\\ desk,\\ medicine\\ chest,\\ crib,$ $chest,\\ sliding\\ door,\\ toilet\\ seat,\\ rocking\\ chair,\\ wardrobe$"], ["instrument", "$chime,\\ wine\\ bottle,\\ ladle,\\ reel,\\ wall\\ clock,\\ hammer,\\ wok,\\ abacus,\\ assault$ $rifle,\\ projectile,\\ safety\\ pin,\\ corkscrew$"], ["man-made structure, construction", "$lumbermill,\\ scoreboard,\\ monastery,\\ church,\\ tobacco\\ shop,\\ drilling\\ platform,$ $dam,\\ fountain,\\ bell\\ cote,\\ yurt,\\ bookshop,\\ prison$"], ["wheeled vehicle", "$convertible,\\ oxcart,\\ electric\\ locomotive,\\ tricycle,\\ fire\\ engine,\\ bicycle-built-for-two,$ $moving\\ van,\\ golfcart,\\ steam\\ locomotive,\\ jinrikisha,\\ tractor,\\ ambulance$"], ["cooked food, prepared food", "$consomme,\\ burrito,\\ meat\\ loaf,\\ bagel,\\ ice\\ cream,\\ French\\ loaf,\\ ice\\ lolly,\\ hot$ $pot,\\ cheeseburger,\\ potpie,\\ trifle,\\ mashed\\ potato$"], ["produce, green goods, green groceries, garden truck", "$broccoli,\\ cauliflower,\\ mushroom,\\ artichoke,\\ acorn,\\ head\\ cabbage,$ $spaghetti\\ squash,\\ jackfruit,\\ cucumber,\\ orange,\\ hip,\\ banana$"]], "md": "|Superclass|Subclasses|\n|---|---|\n|garment|$trench\\ coat,\\ abaya,\\ gown,\\ poncho,\\ military\\ uniform,\\ jersey,\\ cloak,\\ bikini,$ $miniskirt,\\ swimming\\ trunks,\\ lab\\ coat,\\ brassiere$|\n|bird|$African\\ grey,\\ bee\\ eater,\\ coucal,\\ American\\ coot,\\ indigo\\ bunting,\\ king\\ penguin,\\ spoonbill,$ $limpkin,\\ quail,\\ kite,\\ prairie\\ chicken,\\ red-breasted\\ merganser$|\n|reptile, reptilian|$Gila\\ monster,\\ agama,\\ triceratops,\\ African\\ chameleon,\\ thunder\\ snake,\\ Indian\\ cobra,\\ green\\ snake,$ $mud\\ turtle,\\ water\\ snake,\\ loggerhead,\\ sidewinder,\\ leatherback\\ turtle$|\n|arthropod|$rock\\ crab,\\ black\\ and\\ gold\\ garden\\ spider,\\ tiger\\ beetle,\\ black\\ widow,$ $barn\\ spider,\\ leafhopper,\\ ground\\ beetle,\\ fiddler\\ crab,\\ bee,\\ walking\\ stick,$ $cabbage\\ butterfly,\\ admiral$|\n|mammal, mammalian|$Siamese\\ cat,\\ ibex,\\ tiger,\\ hippopotamus,\\ Norwegian\\ elkhound,\\ dugong,$ $colobus,\\ Samoyed,\\ Persian\\ cat,\\ Irish\\ wolfhound,\\ English\\ setter,\\ llama$|\n|fish|$sturgeon,\\ stingray,\\ coho,\\ great\\ white\\ shark,\\ lionfish,\\ gar,\\ goldfish,\\ hammerhead,$ $rock\\ beauty,\\ anemone\\ fish,\\ barracouta,\\ electric\\ ray$|\n|accessory, accoutrement, accouterment|$shower\\ cap,\\ stole,\\ wig,\\ Windsor\\ tie,\\ bib,\\ necklace,\\ bow\\ tie,\\ gasmask,$ $umbrella,\\ cowboy\\ boot,\\ Christmas\\ stocking,\\ bathing\\ cap$|\n|appliance|$espresso\\ maker,\\ sewing\\ machine,\\ stove,\\ waffle\\ iron,\\ rotisserie,\\ lawn\\ mower,$ $electric\\ fan,\\ dishwasher,\\ iron,\\ microwave,\\ vacuum,\\ space\\ heater$|\n|craft|$gondola,\\ schooner,\\ liner,\\ airship,\\ speedboat,\\ airliner,\\ canoe,\\ trimaran,$ $balloon,\\ lifeboat,\\ yawl,\\ submarine$|\n|equipment|$photocopier,\\ mouse,\\ dumbbell,\\ iPod,\\ home\\ theater,\\ projector,\\ cassette\\ player,$ $hand-held\\ computer,\\ tennis\\ ball,\\ tape\\ player,\\ snorkel,\\ monitor$|\n|furniture, piece of furniture, article of furniture|$bookcase,\\ throne,\\ barber\\ chair,\\ four-poster,\\ desk,\\ medicine\\ chest,\\ crib,$ $chest,\\ sliding\\ door,\\ toilet\\ seat,\\ rocking\\ chair,\\ wardrobe$|\n|instrument|$chime,\\ wine\\ bottle,\\ ladle,\\ reel,\\ wall\\ clock,\\ hammer,\\ wok,\\ abacus,\\ assault$ $rifle,\\ projectile,\\ safety\\ pin,\\ corkscrew$|\n|man-made structure, construction|$lumbermill,\\ scoreboard,\\ monastery,\\ church,\\ tobacco\\ shop,\\ drilling\\ platform,$ $dam,\\ fountain,\\ bell\\ cote,\\ yurt,\\ bookshop,\\ prison$|\n|wheeled vehicle|$convertible,\\ oxcart,\\ electric\\ locomotive,\\ tricycle,\\ fire\\ engine,\\ bicycle-built-for-two,$ $moving\\ van,\\ golfcart,\\ steam\\ locomotive,\\ jinrikisha,\\ tractor,\\ ambulance$|\n|cooked food, prepared food|$consomme,\\ burrito,\\ meat\\ loaf,\\ bagel,\\ ice\\ cream,\\ French\\ loaf,\\ ice\\ lolly,\\ hot$ $pot,\\ cheeseburger,\\ potpie,\\ trifle,\\ mashed\\ potato$|\n|produce, green goods, green groceries, garden truck|$broccoli,\\ cauliflower,\\ mushroom,\\ artichoke,\\ acorn,\\ head\\ cabbage,$ $spaghetti\\ squash,\\ jackfruit,\\ cucumber,\\ orange,\\ hip,\\ banana$|", "isPerfectTable": true, "csv": "\"Superclass\",\"Subclasses\"\n\"garment\",\"$trench\\ coat,\\ abaya,\\ gown,\\ poncho,\\ military\\ uniform,\\ jersey,\\ cloak,\\ bikini,$ $miniskirt,\\ swimming\\ trunks,\\ lab\\ coat,\\ brassiere$\"\n\"bird\",\"$African\\ grey,\\ bee\\ eater,\\ coucal,\\ American\\ coot,\\ indigo\\ bunting,\\ king\\ penguin,\\ spoonbill,$ $limpkin,\\ quail,\\ kite,\\ prairie\\ chicken,\\ red-breasted\\ merganser$\"\n\"reptile, reptilian\",\"$Gila\\ monster,\\ agama,\\ triceratops,\\ African\\ chameleon,\\ thunder\\ snake,\\ Indian\\ cobra,\\ green\\ snake,$ $mud\\ turtle,\\ water\\ snake,\\ loggerhead,\\ sidewinder,\\ leatherback\\ turtle$\"\n\"arthropod\",\"$rock\\ crab,\\ black\\ and\\ gold\\ garden\\ spider,\\ tiger\\ beetle,\\ black\\ widow,$ $barn\\ spider,\\ leafhopper,\\ ground\\ beetle,\\ fiddler\\ crab,\\ bee,\\ walking\\ stick,$ $cabbage\\ butterfly,\\ admiral$\"\n\"mammal, mammalian\",\"$Siamese\\ cat,\\ ibex,\\ tiger,\\ hippopotamus,\\ Norwegian\\ elkhound,\\ dugong,$ $colobus,\\ Samoyed,\\ Persian\\ cat,\\ Irish\\ wolfhound,\\ English\\ setter,\\ llama$\"\n\"fish\",\"$sturgeon,\\ stingray,\\ coho,\\ great\\ white\\ shark,\\ lionfish,\\ gar,\\ goldfish,\\ hammerhead,$ $rock\\ beauty,\\ anemone\\ fish,\\ barracouta,\\ electric\\ ray$\"\n\"accessory, accoutrement, accouterment\",\"$shower\\ cap,\\ stole,\\ wig,\\ Windsor\\ tie,\\ bib,\\ necklace,\\ bow\\ tie,\\ gasmask,$ $umbrella,\\ cowboy\\ boot,\\ Christmas\\ stocking,\\ bathing\\ cap$\"\n\"appliance\",\"$espresso\\ maker,\\ sewing\\ machine,\\ stove,\\ waffle\\ iron,\\ rotisserie,\\ lawn\\ mower,$ $electric\\ fan,\\ dishwasher,\\ iron,\\ microwave,\\ vacuum,\\ space\\ heater$\"\n\"craft\",\"$gondola,\\ schooner,\\ liner,\\ airship,\\ speedboat,\\ airliner,\\ canoe,\\ trimaran,$ $balloon,\\ lifeboat,\\ yawl,\\ submarine$\"\n\"equipment\",\"$photocopier,\\ mouse,\\ dumbbell,\\ iPod,\\ home\\ theater,\\ projector,\\ cassette\\ player,$ $hand-held\\ computer,\\ tennis\\ ball,\\ tape\\ player,\\ snorkel,\\ monitor$\"\n\"furniture, piece of furniture, article of furniture\",\"$bookcase,\\ throne,\\ barber\\ chair,\\ four-poster,\\ desk,\\ medicine\\ chest,\\ crib,$ $chest,\\ sliding\\ door,\\ toilet\\ seat,\\ rocking\\ chair,\\ wardrobe$\"\n\"instrument\",\"$chime,\\ wine\\ bottle,\\ ladle,\\ reel,\\ wall\\ clock,\\ hammer,\\ wok,\\ abacus,\\ assault$ $rifle,\\ projectile,\\ safety\\ pin,\\ corkscrew$\"\n\"man-made structure, construction\",\"$lumbermill,\\ scoreboard,\\ monastery,\\ church,\\ tobacco\\ shop,\\ drilling\\ platform,$ $dam,\\ fountain,\\ bell\\ cote,\\ yurt,\\ bookshop,\\ prison$\"\n\"wheeled vehicle\",\"$convertible,\\ oxcart,\\ electric\\ locomotive,\\ tricycle,\\ fire\\ engine,\\ bicycle-built-for-two,$ $moving\\ van,\\ golfcart,\\ steam\\ locomotive,\\ jinrikisha,\\ tractor,\\ ambulance$\"\n\"cooked food, prepared food\",\"$consomme,\\ burrito,\\ meat\\ loaf,\\ bagel,\\ ice\\ cream,\\ French\\ loaf,\\ ice\\ lolly,\\ hot$ $pot,\\ cheeseburger,\\ potpie,\\ trifle,\\ mashed\\ potato$\"\n\"produce, green goods, green groceries, garden truck\",\"$broccoli,\\ cauliflower,\\ mushroom,\\ artichoke,\\ acorn,\\ head\\ cabbage,$ $spaghetti\\ squash,\\ jackfruit,\\ cucumber,\\ orange,\\ hip,\\ banana$\""}]}], "job_id": "8bd5c226-68d3-4a11-8b5f-cac28c5d03fd", "file_path": "./corpus/2307.12532.pdf"}