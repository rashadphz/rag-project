{"pages": [{"page": 1, "text": "                      f-Policy Gradients: A General Framework for Goal\n                                     Conditioned RL using f-Divergences\n                                        Siddhant Agarwal                                Ishan Durugkar\n                                The University of Texas at Austin                            Sony AI\n                                   siddhant@cs.utexas.edu                        ishan.durugkar@sony.com\narXiv:2310.06794v1  [cs.LG]  10 Oct 2023  Peter Stone                                    Amy Zhang\n                              The University of Texas at Austin               The University of Texas at Austin\n                                            Sony AI                          amy.zhang@austin.utexas.edu\n                                  pstone@cs.utexas.edu\n                                                                   Abstract\n                              Goal-Conditioned Reinforcement Learning (RL) problems often have access to\n                              sparse rewards where the agent receives a reward signal only when it has achieved\n                              the goal, making policy optimization a difficult problem. Several works augment\n                              this sparse reward with a learned dense reward function, but this can lead to\n                              sub-optimal policies if the reward is misaligned. Moreover, recent works have\n                              demonstrated that effective shaping rewards for a particular problem can depend on\n                              the underlying learning algorithm. This paper introduces a novel way to encourage\n                              exploration called f-Policy Gradients, or f-PG. f-PG minimizes the f-divergence\n                              between the agent\u2019s state visitation distribution and the goal, which we show can\n                              lead to an optimal policy. We derive gradients for various f-divergences to opti-\n                              mize this objective. Our learning paradigm provides dense learning signals for\n                              exploration in sparse reward settings. We further introduce an entropy-regularized\n                              policy optimization objective, that we call state-MaxEnt RL (or s-MaxEnt RL) as\n                              a special case of our objective. We show that several metric-based shaping rewards\n                              like L2 can be used with s-MaxEnt RL, providing a common ground to study such\n                              metric-based shaping rewards with efficient exploration. We find that f-PG has\n                              better performance compared to standard policy gradient methods on a challeng-\n                              ing gridworld as well as the Point Maze and FetchReach environments. More\n                              information on our website https://agarwalsiddhant10.github.io/projects/fpg.html.\n                    1    Introduction\n                    Reinforcement Learning (RL) algorithms aim to identify the optimal behavior (policy) for solving\n                    a task by interacting with the environment. The field of RL has made large strides in recent years\n                    (Mnih et al., 2013; Silver et al., 2017; Haarnoja et al., 2018; Ouyang et al., 2022; Wurman et al.,\n                    2022) and has been applied to complex tasks ranging from robotics (Gupta et al., 2019), protein\n                    synthesis (Jumper et al., 2021), computer architecture (Fawzi et al., 2022) and finance (Liu et al.,\n                    2021). Goal-Conditioned RL (GCRL) is a generalized form of the standard RL paradigm for learning\n                    a policy that can solve many tasks, as long as each task can be defined by a single rewarding goal\n                    state. Common examples of goal-conditioned tasks arise in robotics where the goal states can be a\n                    target object configuration for manipulation-based tasks (Kim et al., 2022; Gupta et al., 2019; OpenAI\n                    et al., 2021) or a target location for navigation-based tasks (Shah et al., 2020; Gervet et al., 2023).\n                    In any reinforcement learning setup, the task is conveyed to the agent using rewards (Silver et al.,\n                    2021). In goal-conditioned RL settings, a common reward function used is 1 when the goal is\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences\n\n# Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences\n\nSiddhant Agarwal - The University of Texas at Austin, Email: siddhant@cs.utexas.edu\n\nIshan Durugkar - Sony AI, Email: ishan.durugkar@sony.com\n\narXiv: 2310.06794v1 [cs.LG] - 10 Oct 2023\n\nPeter Stone - The University of Texas at Austin\n\nAmy Zhang - The University of Texas at Austin, Sony AI, Email: amy.zhang@austin.utexas.edu\n\n## Abstract\n\nGoal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem. Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. This paper introduces a novel way to encourage exploration called f-Policy Gradients, or f-PG. f-PG minimizes the f-divergence between the agent\u2019s state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call state-MaxEnt RL (or s-MaxEnt RL) as a special case of our objective. We show that several metric-based shaping rewards like L2 can be used with s-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that f-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments. More information on our website here.\n\n## Introduction\n\nReinforcement Learning (RL) algorithms aim to identify the optimal behavior (policy) for solving a task by interacting with the environment. The field of RL has made large strides in recent years and has been applied to complex tasks ranging from robotics, protein synthesis, computer architecture, and finance. Goal-Conditioned RL (GCRL) is a generalized form of the standard RL paradigm for learning a policy that can solve many tasks, as long as each task can be defined by a single rewarding goal state. Common examples of goal-conditioned tasks arise in robotics where the goal states can be a target object configuration for manipulation-based tasks or a target location for navigation-based tasks. In any reinforcement learning setup, the task is conveyed to the agent using rewards. In goal-conditioned RL settings, a common reward function used is 1 when the goal is reached.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences", "md": "# Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences"}, {"type": "heading", "lvl": 1, "value": "Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences", "md": "# Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences"}, {"type": "text", "value": "Siddhant Agarwal - The University of Texas at Austin, Email: siddhant@cs.utexas.edu\n\nIshan Durugkar - Sony AI, Email: ishan.durugkar@sony.com\n\narXiv: 2310.06794v1 [cs.LG] - 10 Oct 2023\n\nPeter Stone - The University of Texas at Austin\n\nAmy Zhang - The University of Texas at Austin, Sony AI, Email: amy.zhang@austin.utexas.edu", "md": "Siddhant Agarwal - The University of Texas at Austin, Email: siddhant@cs.utexas.edu\n\nIshan Durugkar - Sony AI, Email: ishan.durugkar@sony.com\n\narXiv: 2310.06794v1 [cs.LG] - 10 Oct 2023\n\nPeter Stone - The University of Texas at Austin\n\nAmy Zhang - The University of Texas at Austin, Sony AI, Email: amy.zhang@austin.utexas.edu"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Goal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem. Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. This paper introduces a novel way to encourage exploration called f-Policy Gradients, or f-PG. f-PG minimizes the f-divergence between the agent\u2019s state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call state-MaxEnt RL (or s-MaxEnt RL) as a special case of our objective. We show that several metric-based shaping rewards like L2 can be used with s-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that f-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments. More information on our website here.", "md": "Goal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem. Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. This paper introduces a novel way to encourage exploration called f-Policy Gradients, or f-PG. f-PG minimizes the f-divergence between the agent\u2019s state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call state-MaxEnt RL (or s-MaxEnt RL) as a special case of our objective. We show that several metric-based shaping rewards like L2 can be used with s-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that f-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments. More information on our website here."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Reinforcement Learning (RL) algorithms aim to identify the optimal behavior (policy) for solving a task by interacting with the environment. The field of RL has made large strides in recent years and has been applied to complex tasks ranging from robotics, protein synthesis, computer architecture, and finance. Goal-Conditioned RL (GCRL) is a generalized form of the standard RL paradigm for learning a policy that can solve many tasks, as long as each task can be defined by a single rewarding goal state. Common examples of goal-conditioned tasks arise in robotics where the goal states can be a target object configuration for manipulation-based tasks or a target location for navigation-based tasks. In any reinforcement learning setup, the task is conveyed to the agent using rewards. In goal-conditioned RL settings, a common reward function used is 1 when the goal is reached.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Reinforcement Learning (RL) algorithms aim to identify the optimal behavior (policy) for solving a task by interacting with the environment. The field of RL has made large strides in recent years and has been applied to complex tasks ranging from robotics, protein synthesis, computer architecture, and finance. Goal-Conditioned RL (GCRL) is a generalized form of the standard RL paradigm for learning a policy that can solve many tasks, as long as each task can be defined by a single rewarding goal state. Common examples of goal-conditioned tasks arise in robotics where the goal states can be a target object configuration for manipulation-based tasks or a target location for navigation-based tasks. In any reinforcement learning setup, the task is conveyed to the agent using rewards. In goal-conditioned RL settings, a common reward function used is 1 when the goal is reached.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "achieved and 0 everywhere else. This reward function is sparse and poses a huge learning challenge\nto obtain the optimal policy without any intermediate learning signal. Prior works (Ng et al., 1999; Ni\net al., 2020; Durugkar et al., 2021; Arjona-Medina et al., 2019; Goyal et al., 2019) have augmented the\nreward function to provide some dense signal for policy optimization. A major issue with augmenting\nreward functions is that the optimal policy for the new reward function may no longer be optimal\nunder the original, true reward function (Ng et al., 1999). Moreover, it has been shown (Booth et al.,\n2023) that shaping rewards that improve learning for one learning algorithm may not be optimal for\nanother learning algorithm. Algorithms that learn reward functions (Ni et al., 2020; Durugkar et al.,\n2021; Zheng et al., 2018) are inefficient because the reward function must first be learned before it\ncan be used for policy optimization. These challenges lead to the following research question: Is\nthere another way to provide dense learning signals for policy optimization other than through dense\nshaping rewards?\nIn this work, we look at using divergence minimization between the agent\u2019s state visitation and the\ngoal distribution (we assume that each goal can be represented as a distribution, Dirac distribution\nbeing the simplest) as an objective to provide additional learning signals. Similar perspectives to\npolicy learning has been explored by prior works (Ziebart et al., 2008; Haarnoja et al., 2017, 2018;\nHo & Ermon, 2016; Ni et al., 2020; Ghasemipour et al., 2019; Fu et al., 2017), but they reduce their\nmethods into a reward-centric view. MaxEnt RL methods (Ziebart et al., 2008; Haarnoja et al., 2017,\n2018) use the distribution over trajectories rather than state visitations and still suffer from sparsity\nif the task rewards are sparse. Imitation learning works like those of Ho & Ermon (2016); Fu et al.\n(2017); Ghasemipour et al. (2019) use a variational lower bound to obtain min-max objectives that\nrequire discriminators. These objectives suffer from mathematical instabilities and often require\ncoverage assumptions i.e., abundant overlap between the agent\u2019s state visitation distribution and\ngoal distribution. Our method does not rely on discriminators nor does it assume state coverage. It\nprovides dense signals to update the policy even when the agent has not seen the goal. These signals\npush the policy towards higher entropy state visitations until the goal is discovered.\nOur method, f-PG or f-Policy Gradient, introduces a novel GCRL framework that aims to minimize\na general measure of mismatch (the f-divergence) between the agent\u2019s state visitation distribution and\nthe goal distribution. We prove that minimizing the f-divergence (for some divergences) recovers the\noptimal policy. The analytical gradient for the objective looks very similar to a policy gradient which\nallows us to leverage established methods from the policy gradient literature to come up with an\nefficient algorithm for goal-conditioned RL. We show the connection of our method to the commonly\nused metric-based shaping rewards for GCRL like L2 rewards. We show that a special case of f-PG\njointly optimizes for maximization of a reward and the entropy of the state-visitation distribution\nthus introducing state-MaxEnt RL (or s-MaxEnt RL). Using a sparse gridworld, we establish the\nbenefits of using f-PG as a dense signal to explore when the agent has not seen the goal. We also\ndemonstrate that our framework can be extended to continuous state spaces and scale to larger and\nhigher-dimensional state spaces in maze navigation and manipulation tasks.\nOur key contributions are 1) developing a novel algorithm for goal-conditioned RL that provably\nproduces the optimal policy, 2) connecting our framework to commonly known metric-based shaping\nrewards, 3) Providing a new perspective to RL (s-MaxEnt RL) that focuses on maximizing the entropy\nof the state-visitation distribution and 4) empirical evidence demonstrating its ability to provide dense\nlearning signals and scale to larger domains.\n2    Background\nThis section goes over the standard goal-conditioned reinforcement learning formulation and the\nf-divergences that will be used in the rest of the paper.\nGoal-conditioned reinforcement learning. This paper considers an agent in a goal-conditioned\nMDP (Puterman, 1990; Kaelbling, 1993).             A goal-conditioned MDP is defined as a tuple\n\u27e8S, G, A, P, r, \u03b3, \u00b50, \u03c1g\u27e9 where S is the state space, A is the action space, P : S \u00d7 A  \u2212\u2192        \u2206(S)\nis the transition probability (\u2206(\u00b7) denotes a probability distribution over a set), \u03b3 \u2208      [0, 1) is the\ndiscount factor, \u00b50 is the distribution over initial states, G \u2282   S is the set of goals, and \u03c1g : \u2206(G)\nis the distribution over goals. At the beginning of an episode, the initial state s0 and the goal g are\nsampled from the distributions \u00b50 and \u03c1g. The rewards r : S \u00d7 G  \u2212\u2192         R are based on the state the\nagent visits and conditioned on the goal specified during that episode. This work focuses on sparse\nrewards, where r(s\u2032, g) = 1 when s\u2032 = g, and is r(s\u2032, g) = 0 otherwise. In continuous domains, the\nequality is relaxed to s\u2032 \u2208 B(g, r) where B(g, r) represents a ball around the goal g with radius r.\n                                                    2", "md": "# Research Paper Summary\n\n## Research Paper Summary\n\nThe research paper discusses the challenges of sparse reward functions in reinforcement learning and proposes a novel algorithm, f-PG, for goal-conditioned RL that provides dense learning signals for policy optimization.\n\n### Research Question\n\nIs there a way to provide dense learning signals for policy optimization other than through dense shaping rewards?\n\n### Methodology\n\nThe proposed f-PG algorithm aims to minimize the f-divergence between the agent's state visitation distribution and the goal distribution. This approach does not rely on discriminators or state coverage assumptions, providing dense signals to update the policy even in the absence of the goal.\n\n### Key Contributions\n\n1. Development of a novel algorithm for goal-conditioned RL that produces the optimal policy.\n2. Connection of the framework to metric-based shaping rewards.\n3. Introduction of state-MaxEnt RL, focusing on maximizing the entropy of the state-visitation distribution.\n4. Empirical evidence demonstrating the scalability and effectiveness of the proposed method.\n\n### Background\n\nThe paper explains the standard goal-conditioned reinforcement learning formulation in a goal-conditioned MDP, where the agent interacts with the environment to achieve specified goals based on sparse rewards. The state space, action space, transition probabilities, discount factor, initial state distribution, goal set, and goal distribution are defined within the MDP framework.\n\n### Equation\n\n$$ r(s', g) = \\begin{cases} 1 & \\text{if } s' = g \\\\ 0 & \\text{otherwise} \\end{cases} $$\n\n### Table\n\n|Symbols|Definitions|\n|---|---|\n|S|State space|\n|A|Action space|\n|P|Transition probability|\n|\u03b3|Discount factor|\n|\u00b50|Initial state distribution|\n|G|Set of goals|\n|\u03c1g|Distribution over goals|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Paper Summary", "md": "# Research Paper Summary"}, {"type": "heading", "lvl": 2, "value": "Research Paper Summary", "md": "## Research Paper Summary"}, {"type": "text", "value": "The research paper discusses the challenges of sparse reward functions in reinforcement learning and proposes a novel algorithm, f-PG, for goal-conditioned RL that provides dense learning signals for policy optimization.", "md": "The research paper discusses the challenges of sparse reward functions in reinforcement learning and proposes a novel algorithm, f-PG, for goal-conditioned RL that provides dense learning signals for policy optimization."}, {"type": "heading", "lvl": 3, "value": "Research Question", "md": "### Research Question"}, {"type": "text", "value": "Is there a way to provide dense learning signals for policy optimization other than through dense shaping rewards?", "md": "Is there a way to provide dense learning signals for policy optimization other than through dense shaping rewards?"}, {"type": "heading", "lvl": 3, "value": "Methodology", "md": "### Methodology"}, {"type": "text", "value": "The proposed f-PG algorithm aims to minimize the f-divergence between the agent's state visitation distribution and the goal distribution. This approach does not rely on discriminators or state coverage assumptions, providing dense signals to update the policy even in the absence of the goal.", "md": "The proposed f-PG algorithm aims to minimize the f-divergence between the agent's state visitation distribution and the goal distribution. This approach does not rely on discriminators or state coverage assumptions, providing dense signals to update the policy even in the absence of the goal."}, {"type": "heading", "lvl": 3, "value": "Key Contributions", "md": "### Key Contributions"}, {"type": "text", "value": "1. Development of a novel algorithm for goal-conditioned RL that produces the optimal policy.\n2. Connection of the framework to metric-based shaping rewards.\n3. Introduction of state-MaxEnt RL, focusing on maximizing the entropy of the state-visitation distribution.\n4. Empirical evidence demonstrating the scalability and effectiveness of the proposed method.", "md": "1. Development of a novel algorithm for goal-conditioned RL that produces the optimal policy.\n2. Connection of the framework to metric-based shaping rewards.\n3. Introduction of state-MaxEnt RL, focusing on maximizing the entropy of the state-visitation distribution.\n4. Empirical evidence demonstrating the scalability and effectiveness of the proposed method."}, {"type": "heading", "lvl": 3, "value": "Background", "md": "### Background"}, {"type": "text", "value": "The paper explains the standard goal-conditioned reinforcement learning formulation in a goal-conditioned MDP, where the agent interacts with the environment to achieve specified goals based on sparse rewards. The state space, action space, transition probabilities, discount factor, initial state distribution, goal set, and goal distribution are defined within the MDP framework.", "md": "The paper explains the standard goal-conditioned reinforcement learning formulation in a goal-conditioned MDP, where the agent interacts with the environment to achieve specified goals based on sparse rewards. The state space, action space, transition probabilities, discount factor, initial state distribution, goal set, and goal distribution are defined within the MDP framework."}, {"type": "heading", "lvl": 3, "value": "Equation", "md": "### Equation"}, {"type": "text", "value": "$$ r(s', g) = \\begin{cases} 1 & \\text{if } s' = g \\\\ 0 & \\text{otherwise} \\end{cases} $$", "md": "$$ r(s', g) = \\begin{cases} 1 & \\text{if } s' = g \\\\ 0 & \\text{otherwise} \\end{cases} $$"}, {"type": "heading", "lvl": 3, "value": "Table", "md": "### Table"}, {"type": "table", "rows": [["Symbols", "Definitions"], ["S", "State space"], ["A", "Action space"], ["P", "Transition probability"], ["\u03b3", "Discount factor"], ["\u00b50", "Initial state distribution"], ["G", "Set of goals"], ["\u03c1g", "Distribution over goals"]], "md": "|Symbols|Definitions|\n|---|---|\n|S|State space|\n|A|Action space|\n|P|Transition probability|\n|\u03b3|Discount factor|\n|\u00b50|Initial state distribution|\n|G|Set of goals|\n|\u03c1g|Distribution over goals|", "isPerfectTable": true, "csv": "\"Symbols\",\"Definitions\"\n\"S\",\"State space\"\n\"A\",\"Action space\"\n\"P\",\"Transition probability\"\n\"\u03b3\",\"Discount factor\"\n\"\u00b50\",\"Initial state distribution\"\n\"G\",\"Set of goals\"\n\"\u03c1g\",\"Distribution over goals\""}]}, {"page": 3, "text": "A trajectory \u03c4 is defined as the sequence (s0, a0, s1, . . . , sT \u22121, aT \u22121, sT ). The return Hg(s) is\ndefined as the cumulative discounted rewards Hg(s) :=  T                                 t=0 [\u03b3tr(st+1, g)|s0 = s], where T is the\nlength of a trajectory. We will assume the trajectory ends when a maximum number of policy steps\n(T  ) have been executed. The agent aims to learn a policy \u03c0 : S \u00d7 G  \u2212\u2192                                         \u2206(A) that maximises the\nexpected return E\u03c0,s0[Hg(s0)]. The optimal policy \u03c0\u2217                                  = arg max\u03c0\u03b8\u2208\u03a0 E\u03c0,s0[Hg(s0)], where the\nspace of policies \u03a0 is defined by a set of parameters \u03b8 \u2208                              \u0398.\nDistribution matching approach to goal-conditioned RL. The distribution over goal-conditioned\ntrajectories is defi        ned as p\u03b8(\u03c4; g) = \u03a0T             t=0p(st|st\u22121, at\u22121)\u03c0\u03b8(at|st; g). The trajectory-dependent\nstate visitation distribution is defined as \u03b7\u03c4                    (s). It is the number of times the state s is visited in the\ntrajectory \u03c4. The agent\u2019s goal-conditioned state visitation can then be defined as:\n                                p\u03b8(s; g) =           p\u03b8(\u03c4; g)\u03b7\u03c4      (s)d\u03c4                                                                (1)\n                                                              Z\n                                             =            \u03a0p(st+1|st, at)\u03c0\u03b8(at|st; g)\u03b7\u03c4                 (s)                               (2)\n                                                        \u03a0p(st+1|st, at)\u03c0\u03b8(at|st; g)\u03b7\u03c4                 (s)d\u03c4dsd\u03c4.\nThe goal g defines an idealized target distribution pg : \u2206(S), considered here as a Dirac distribution\nwhich places all the probability mass at the goal state pg = \u03b4(g). Such a formulation has been\nused previously in approaches to learn goal-conditioned policies (Durugkar et al., 2021). This work\nfocuses on minimizing the mismatch of an agent\u2019s goal-conditioned state visitation distribution\np\u03b8(s; g) to this target distribution pg. In this paper, we will be using p\u03b8 and p\u03c0 interchangeably i.e.,\np\u03b8 corresponds to the visitation distribution induced by policy \u03c0 that is parameterized by \u03b8.\nTo do so, this paper considers a family of methods that compare the state-visitation distribution\ninduced by a goal-conditioned policy and the ideal target distribution for that goal g, called f-\ndivergences. f-divergences are defined as (Polyanskiy & Wu, 2022),\n                            Df(P     ||Q) =               P  (x)f     Q(x)        dx \u2212     f \u2032(\u221e)Q[P        (x) = 0]),                    (3)\n                                                   P >0                P  (x)\nwhere f is a convex function with f(1) = 0. f \u2032(\u221e) is not defined (is \u221e) for several f-divergences\nand so it is a common assumption that Q = 0 wherever P = 0. Table 1 shows a list of commonly\nused f-divergences with corresponding f and f \u2032(\u221e).\n        f-divergence                       Df(P      ||Q)                            f(u)                      f \u2032(u)             f \u2032(\u221e)\n              FKL                       P  (x) log P (x)                            u log u                 1 + log u          Undefined\n                                                       Q(x)dx\n              RKL                       Q(x) log Q(x)                              \u2212   log u                    \u2212   1                 0\n                                                       P (x)dx                                                      u\n                                     1   P(x) log      2P (x)                     u log u\u2212                          2u\n                                     2              P (x)+Q(x)+\n                JS                                  2Q(x)                                                    log                   log 2\n                                      Q(x) log   P (x)+Q(x)dx                (1 + u) log 1+u     2                 1+u\n                \u03c72               1     Q(x)( P (x)                               1                                u            Undefined\n                                 2               Q(x) \u2212      1)2dx               2(u \u2212      1)2\nTable 1: Selected list of f-divergences Df(P                        ||Q) with generator functions f and their derivatives f \u2032,\nwhere f is convex, lower-semicontinuous and f(1) = 0.\n3      Related Work\nShaping Rewards. Our work is related to a separate class of techniques that augment the sparse\nreward function with dense signals. Ng et al. (1999) proposes a way to augment reward functions\nwithout changing the optimal behavior. Intrinsic Motivation (Durugkar et al., 2021; Bellemare et al.,\n2016; Singh et al., 2010; Barto, 2013) has been an active research area for providing shaping rewards.\nSome work (Niekum, 2010; Zheng et al., 2018) learn intrinsic or alternate reward functions for\nthe underlying task that aim to improve agent learning performance while others (Durugkar et al.,\n2021; Ni et al., 2020; Goyal et al., 2019) learn augmented rewards based on distribution matching.\nAIM (Durugkar et al., 2021) learns a potential-based shaping reward to capture the time-step distance\nbut requires a restrictive assumption about state coverage, especially around the goal while we do\nnot make any such assumption. Recursive classification methods (Eysenbach et al., 2021, 2020) use\n                                                                           3", "md": "# Goal-Conditioned Reinforcement Learning\n\n## Goal-Conditioned Reinforcement Learning\n\nA trajectory $$\\tau$$ is defined as the sequence $$(s_0, a_0, s_1, ..., s_{T-1}, a_{T-1}, s_T)$$. The return $$H_g(s)$$ is defined as the cumulative discounted rewards:\n\n$$H_g(s) := \\sum_{t=0}^{T} [\\gamma r(s_{t+1}, g) | s_0 = s]$$\nwhere T is the length of a trajectory. The agent aims to learn a policy $$\\pi : S \\times G \\rightarrow \\Delta(A)$$ that maximizes the expected return:\n\n$$E_{\\pi,s_0}[H_g(s_0)]$$\nThe optimal policy $$\\pi^* = \\arg \\max_{\\pi_{\\theta} \\in \\Pi} E_{\\pi,s_0}[H_g(s_0)]$$, where the space of policies $$\\Pi$$ is defined by a set of parameters $$\\theta \\in \\Theta$$.\n\nDistribution matching approach to goal-conditioned RL. The distribution over goal-conditioned trajectories is defined as:\n\n$$p_{\\theta}(\\tau; g) = \\prod_{t=0}^{T} p(s_t|s_{t-1}, a_{t-1})\\pi_{\\theta}(a_t|s_t; g)$$\nThe trajectory-dependent state visitation distribution is defined as $$\\eta_{\\tau}(s)$$. The agent\u2019s goal-conditioned state visitation can then be defined as:\n\n$$p_{\\theta}(s; g) = \\int \\int \\prod p(s_{t+1}|s_t, a_t)\\pi_{\\theta}(a_t|s_t; g)\\eta_{\\tau}(s) ds d\\tau$$\nThe goal g defines an idealized target distribution $$p_g : \\Delta(S)$$, considered here as a Dirac distribution which places all the probability mass at the goal state $$p_g = \\delta(g)$$. This work focuses on minimizing the mismatch of an agent\u2019s goal-conditioned state visitation distribution $$p_{\\theta}(s; g)$$ to this target distribution $$p_g$$.\n\nTo do so, this paper considers a family of methods that compare the state-visitation distribution induced by a goal-conditioned policy and the ideal target distribution for that goal g, called f-divergences. f-divergences are defined as:\n\n$$D_f(P || Q) = \\int P(x) f\\left(\\frac{Q(x)}{P(x)}\\right) dx - f'(\\infty)Q[P(x) = 0]$$\nwhere f is a convex function with $$f(1) = 0$$. $$f'(\\infty)$$ is not defined (is $$\\infty$$) for several f-divergences and so it is a common assumption that Q = 0 wherever P = 0.\n\n|f-divergence|D_f(P || Q)|f(u)|f'(u)|f'($\\infty$)|\n|---|---|---|---|---|\n|FKL|$P(x) \\log P(x) - \\int Q(x) \\, dx$|$u \\log u$|$1 + \\log u$|Undefined|\n|RKL|$Q(x) \\log Q(x) - \\int P(x) \\, dx$|$-\\log u$|$-1$|$0$|\n|JS|$\\frac{1}{2} \\int P(x) \\log \\frac{2P(x)}{P(x)+Q(x)} \\, dx$|$(1 + u) \\log(1+u)$|$\\frac{\\log 2}{2}$|$\\log 2$|\n|$\\chi^2$|$\\frac{1}{2} \\int Q(x) \\left( \\frac{P(x)}{Q(x)} - 1 \\right)^2 \\, dx$|$1$|$u$|Undefined|\n\nTable 1: Selected list of f-divergences $$D_f(P || Q)$$ with generator functions f and their derivatives $$f'$$, where f is convex, lower-semicontinuous and $$f(1) = 0$$.\n\n### Related Work\n\nShaping Rewards. Our work is related to a separate class of techniques that augment the sparse reward function with dense signals. Ng et al. (1999) proposes a way to augment reward functions without changing the optimal behavior. Intrinsic Motivation (Durugkar et al., 2021; Bellemare et al., 2016; Singh et al., 2010; Barto, 2013) has been an active research area for providing shaping rewards. Some work (Niekum, 2010; Zheng et al., 2018) learn intrinsic or alternate reward functions for the underlying task that aim to improve agent learning performance while others (Durugkar et al., 2021; Ni et al., 2020; Goyal et al., 2019) learn augmented rewards based on distribution matching. AIM (Durugkar et al., 2021) learns a potential-based shaping reward to capture the time-step distance but requires a restrictive assumption about state coverage, especially around the goal while we do not make any such assumption. Recursive classification methods (Eysenbach et al., 2021, 2020) use", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Goal-Conditioned Reinforcement Learning", "md": "# Goal-Conditioned Reinforcement Learning"}, {"type": "heading", "lvl": 2, "value": "Goal-Conditioned Reinforcement Learning", "md": "## Goal-Conditioned Reinforcement Learning"}, {"type": "text", "value": "A trajectory $$\\tau$$ is defined as the sequence $$(s_0, a_0, s_1, ..., s_{T-1}, a_{T-1}, s_T)$$. The return $$H_g(s)$$ is defined as the cumulative discounted rewards:\n\n$$H_g(s) := \\sum_{t=0}^{T} [\\gamma r(s_{t+1}, g) | s_0 = s]$$\nwhere T is the length of a trajectory. The agent aims to learn a policy $$\\pi : S \\times G \\rightarrow \\Delta(A)$$ that maximizes the expected return:\n\n$$E_{\\pi,s_0}[H_g(s_0)]$$\nThe optimal policy $$\\pi^* = \\arg \\max_{\\pi_{\\theta} \\in \\Pi} E_{\\pi,s_0}[H_g(s_0)]$$, where the space of policies $$\\Pi$$ is defined by a set of parameters $$\\theta \\in \\Theta$$.\n\nDistribution matching approach to goal-conditioned RL. The distribution over goal-conditioned trajectories is defined as:\n\n$$p_{\\theta}(\\tau; g) = \\prod_{t=0}^{T} p(s_t|s_{t-1}, a_{t-1})\\pi_{\\theta}(a_t|s_t; g)$$\nThe trajectory-dependent state visitation distribution is defined as $$\\eta_{\\tau}(s)$$. The agent\u2019s goal-conditioned state visitation can then be defined as:\n\n$$p_{\\theta}(s; g) = \\int \\int \\prod p(s_{t+1}|s_t, a_t)\\pi_{\\theta}(a_t|s_t; g)\\eta_{\\tau}(s) ds d\\tau$$\nThe goal g defines an idealized target distribution $$p_g : \\Delta(S)$$, considered here as a Dirac distribution which places all the probability mass at the goal state $$p_g = \\delta(g)$$. This work focuses on minimizing the mismatch of an agent\u2019s goal-conditioned state visitation distribution $$p_{\\theta}(s; g)$$ to this target distribution $$p_g$$.\n\nTo do so, this paper considers a family of methods that compare the state-visitation distribution induced by a goal-conditioned policy and the ideal target distribution for that goal g, called f-divergences. f-divergences are defined as:\n\n$$D_f(P || Q) = \\int P(x) f\\left(\\frac{Q(x)}{P(x)}\\right) dx - f'(\\infty)Q[P(x) = 0]$$\nwhere f is a convex function with $$f(1) = 0$$. $$f'(\\infty)$$ is not defined (is $$\\infty$$) for several f-divergences and so it is a common assumption that Q = 0 wherever P = 0.", "md": "A trajectory $$\\tau$$ is defined as the sequence $$(s_0, a_0, s_1, ..., s_{T-1}, a_{T-1}, s_T)$$. The return $$H_g(s)$$ is defined as the cumulative discounted rewards:\n\n$$H_g(s) := \\sum_{t=0}^{T} [\\gamma r(s_{t+1}, g) | s_0 = s]$$\nwhere T is the length of a trajectory. The agent aims to learn a policy $$\\pi : S \\times G \\rightarrow \\Delta(A)$$ that maximizes the expected return:\n\n$$E_{\\pi,s_0}[H_g(s_0)]$$\nThe optimal policy $$\\pi^* = \\arg \\max_{\\pi_{\\theta} \\in \\Pi} E_{\\pi,s_0}[H_g(s_0)]$$, where the space of policies $$\\Pi$$ is defined by a set of parameters $$\\theta \\in \\Theta$$.\n\nDistribution matching approach to goal-conditioned RL. The distribution over goal-conditioned trajectories is defined as:\n\n$$p_{\\theta}(\\tau; g) = \\prod_{t=0}^{T} p(s_t|s_{t-1}, a_{t-1})\\pi_{\\theta}(a_t|s_t; g)$$\nThe trajectory-dependent state visitation distribution is defined as $$\\eta_{\\tau}(s)$$. The agent\u2019s goal-conditioned state visitation can then be defined as:\n\n$$p_{\\theta}(s; g) = \\int \\int \\prod p(s_{t+1}|s_t, a_t)\\pi_{\\theta}(a_t|s_t; g)\\eta_{\\tau}(s) ds d\\tau$$\nThe goal g defines an idealized target distribution $$p_g : \\Delta(S)$$, considered here as a Dirac distribution which places all the probability mass at the goal state $$p_g = \\delta(g)$$. This work focuses on minimizing the mismatch of an agent\u2019s goal-conditioned state visitation distribution $$p_{\\theta}(s; g)$$ to this target distribution $$p_g$$.\n\nTo do so, this paper considers a family of methods that compare the state-visitation distribution induced by a goal-conditioned policy and the ideal target distribution for that goal g, called f-divergences. f-divergences are defined as:\n\n$$D_f(P || Q) = \\int P(x) f\\left(\\frac{Q(x)}{P(x)}\\right) dx - f'(\\infty)Q[P(x) = 0]$$\nwhere f is a convex function with $$f(1) = 0$$. $$f'(\\infty)$$ is not defined (is $$\\infty$$) for several f-divergences and so it is a common assumption that Q = 0 wherever P = 0."}, {"type": "table", "rows": [["f-divergence", "D_f(P", "", "Q)", "f(u)", "f'(u)", "f'($\\infty$)"], ["FKL", "$P(x) \\log P(x) - \\int Q(x) \\, dx$", "$u \\log u$", "$1 + \\log u$", "Undefined"], ["RKL", "$Q(x) \\log Q(x) - \\int P(x) \\, dx$", "$-\\log u$", "$-1$", "$0$"], ["JS", "$\\frac{1}{2} \\int P(x) \\log \\frac{2P(x)}{P(x)+Q(x)} \\, dx$", "$(1 + u) \\log(1+u)$", "$\\frac{\\log 2}{2}$", "$\\log 2$"], ["$\\chi^2$", "$\\frac{1}{2} \\int Q(x) \\left( \\frac{P(x)}{Q(x)} - 1 \\right)^2 \\, dx$", "$1$", "$u$", "Undefined"]], "md": "|f-divergence|D_f(P || Q)|f(u)|f'(u)|f'($\\infty$)|\n|---|---|---|---|---|\n|FKL|$P(x) \\log P(x) - \\int Q(x) \\, dx$|$u \\log u$|$1 + \\log u$|Undefined|\n|RKL|$Q(x) \\log Q(x) - \\int P(x) \\, dx$|$-\\log u$|$-1$|$0$|\n|JS|$\\frac{1}{2} \\int P(x) \\log \\frac{2P(x)}{P(x)+Q(x)} \\, dx$|$(1 + u) \\log(1+u)$|$\\frac{\\log 2}{2}$|$\\log 2$|\n|$\\chi^2$|$\\frac{1}{2} \\int Q(x) \\left( \\frac{P(x)}{Q(x)} - 1 \\right)^2 \\, dx$|$1$|$u$|Undefined|", "isPerfectTable": false, "csv": "\"f-divergence\",\"D_f(P\",\"\",\"Q)\",\"f(u)\",\"f'(u)\",\"f'($\\infty$)\"\n\"FKL\",\"$P(x) \\log P(x) - \\int Q(x) \\, dx$\",\"$u \\log u$\",\"$1 + \\log u$\",\"Undefined\"\n\"RKL\",\"$Q(x) \\log Q(x) - \\int P(x) \\, dx$\",\"$-\\log u$\",\"$-1$\",\"$0$\"\n\"JS\",\"$\\frac{1}{2} \\int P(x) \\log \\frac{2P(x)}{P(x)+Q(x)} \\, dx$\",\"$(1 + u) \\log(1+u)$\",\"$\\frac{\\log 2}{2}$\",\"$\\log 2$\"\n\"$\\chi^2$\",\"$\\frac{1}{2} \\int Q(x) \\left( \\frac{P(x)}{Q(x)} - 1 \\right)^2 \\, dx$\",\"$1$\",\"$u$\",\"Undefined\""}, {"type": "text", "value": "Table 1: Selected list of f-divergences $$D_f(P || Q)$$ with generator functions f and their derivatives $$f'$$, where f is convex, lower-semicontinuous and $$f(1) = 0$$.", "md": "Table 1: Selected list of f-divergences $$D_f(P || Q)$$ with generator functions f and their derivatives $$f'$$, where f is convex, lower-semicontinuous and $$f(1) = 0$$."}, {"type": "heading", "lvl": 3, "value": "Related Work", "md": "### Related Work"}, {"type": "text", "value": "Shaping Rewards. Our work is related to a separate class of techniques that augment the sparse reward function with dense signals. Ng et al. (1999) proposes a way to augment reward functions without changing the optimal behavior. Intrinsic Motivation (Durugkar et al., 2021; Bellemare et al., 2016; Singh et al., 2010; Barto, 2013) has been an active research area for providing shaping rewards. Some work (Niekum, 2010; Zheng et al., 2018) learn intrinsic or alternate reward functions for the underlying task that aim to improve agent learning performance while others (Durugkar et al., 2021; Ni et al., 2020; Goyal et al., 2019) learn augmented rewards based on distribution matching. AIM (Durugkar et al., 2021) learns a potential-based shaping reward to capture the time-step distance but requires a restrictive assumption about state coverage, especially around the goal while we do not make any such assumption. Recursive classification methods (Eysenbach et al., 2021, 2020) use", "md": "Shaping Rewards. Our work is related to a separate class of techniques that augment the sparse reward function with dense signals. Ng et al. (1999) proposes a way to augment reward functions without changing the optimal behavior. Intrinsic Motivation (Durugkar et al., 2021; Bellemare et al., 2016; Singh et al., 2010; Barto, 2013) has been an active research area for providing shaping rewards. Some work (Niekum, 2010; Zheng et al., 2018) learn intrinsic or alternate reward functions for the underlying task that aim to improve agent learning performance while others (Durugkar et al., 2021; Ni et al., 2020; Goyal et al., 2019) learn augmented rewards based on distribution matching. AIM (Durugkar et al., 2021) learns a potential-based shaping reward to capture the time-step distance but requires a restrictive assumption about state coverage, especially around the goal while we do not make any such assumption. Recursive classification methods (Eysenbach et al., 2021, 2020) use"}]}, {"page": 4, "text": "future state densities as rewards. However, these methods will fail when the agent has never seen\nthe goal. Moreover, in most of these works, the reward is not stationary (is dependent on the policy)\nwhich can lead to instabilities during policy optimization. GoFAR (Ma et al., 2022) is an offline\ngoal-conditioned RL algorithm that minimizes a lower bound to the KL divergence between p\u03b8(s)\nand the pg(s). It computes rewards using a discriminator and uses the dual formulation utilized by\nthe DICE family (Nachum et al., 2019), but reduces to GAIL (Ho & Ermon, 2016) in the online\nsetting, requiring coverage assumptions. Our work also minimizes the divergence between the agent\u2019s\nvisitation distribution and the goal distribution, but we provide a new formulation for on-policy\ngoal-conditioned RL that does not require a discriminator or the same coverage assumptions.\nPolicy Learning through State Matching. We first focus on imitation learning where the expert\ndistribution pE(s, a) is directly inferred from the expert data. GAIL (Ho & Ermon, 2016) showed\nthat the inverse RL objective is the dual of state-matching. f-MAX (Ghasemipour et al., 2019) uses\nf-divergence as a metric to match the agent\u2019s state-action visitation distribution p\u03c0(s, a) and pE(s, a).\nKe et al. (2019); Ghasemipour et al. (2019) shows how several commonly used imitation learning\nmethods can be reduced to a divergence minimization. But all of these methods optimize a lower\nbound of the divergence which is essentially a min-max bilevel optimization objective. They break\nthe min-max into two parts, fitting the density model to obtain a reward that can be used for policy\noptimization. But these rewards depend on the policy, and should not be used by RL algorithms that\nassume stationary rewards. f-IRL (Ni et al., 2020) escapes the min-max objective but learns a reward\nfunction that can be used for policy optimization. We do not aim to learn a reward function but rather\ndirectly optimize for a policy using dense signals from an f-divergence objective.\nIn reinforcement learning, the connections between entropy regularized MaxEnt RL and the mini-\nmization of reverse KL between agent\u2019s trajectory distribution, p\u03c0(\u03c4), and the \u201coptimal\" trajectory\ndistribution, p\u2217(\u03c4) \u221d          er(\u03c4) has been extensively studied Ziebart (2010); Ziebart et al. (2008); Kappen\net al. (2012); Levine (2018); Haarnoja et al. (2018). MaxEnt RL optimizes for a policy with maximum\nentropy but such a policy does not guarantee maximum coverage of the state space. Hazan et al.\n(2018) discusses an objective for maximum exploration that focuses on maximizing the entropy of the\nstate-visitation distribution or KL divergence between the state-visitation distribution and a uniform\ndistribution. A few works like Durugkar et al. (2023, 2021); Ma et al. (2022), that have explored\nstate-matching for reinforcement learning, have been discussed above.\nLimitations of Markov Rewards. Our work looks beyond the maximization of a Markov reward\nfor policy optimization. The learning signals that we use are non-stationary. We thus discuss the\nlimitations of using Markov rewards for obtaining the optimal policy. There have been works (Abel\net al., 2021; Clark & Amodei, 2016; Icarte et al., 2018, 2021) that express the difficulty in using\nMarkov rewards. Abel et al. (2021) proves that there always exist environment-task pairs that cannot\nbe described using Markov rewards. Reward Machines (Icarte et al., 2018) create finite automata to\nspecify reward functions and can specify Non-Markov rewards as well but these are hand-crafted.\n4     f-Policy Gradient\nIn this paper, we derive an algorithm where the agents learn by minimizing the following f-divergence:\n                                                     J(\u03b8) = Df(p\u03b8(s)||pg(s))                                                          (4)\nIn this section, we shall derive an algorithm to minimize J(\u03b8) and analyze the objective more closely\nin the subsequent section. Unlike f-max (Ghasemipour et al., 2019), we directly optimize J(\u03b8). We\ndifferentiate J(\u03b8) with respect to \u03b8 to get this gradient.\nTheorem 4.1. The gradient of J(\u03b8) as defined in Equation 4 is given by,\n                       \u2207\u03b8J(\u03b8) = E\u03c4\u223cp\u03b8(\u03c4)                   T   \u2207\u03b8 log \u03c0\u03b8(at|st)              T   f \u2032 p\u03b8(st)            .              (5)\n                                                          t=1                               t=1        pg(st)\nThe gradient looks exactly like policy gradient with rewards \u2212f \u2032                                p\u03b8(st)     . However, this does not\n                                                                                                 pg(st)\nmean that we are maximizing JRL(\u03b8) = E                        \u03c4\u223cp\u03b8(\u03c4)       \u2212  f \u2032   p\u03b8(st)      . This is because the gradient of\n                                                                                     pg(st)\nJRL   (\u03b8) is not the same as \u2207\u03b8J(\u03b8). For Dirac goal distributions, the gradient in Equation 5 cannot be\nused (as f \u2032      p\u03b8(st)     will not be defined when pg(st) = 0). We can use the definition of f-divergence\n                  pg(st)\nin Equation 3 to derive a gradient for such distributions.\n                                                                      4", "md": "# Math Equations and Text\n\nFuture state densities as rewards. However, these methods will fail when the agent has never seen the goal. Moreover, in most of these works, the reward is not stationary (is dependent on the policy) which can lead to instabilities during policy optimization. GoFAR (Ma et al., 2022) is an offline goal-conditioned RL algorithm that minimizes a lower bound to the KL divergence between $$p_{\\theta}(s)$$ and the $$p_{g}(s)$$. It computes rewards using a discriminator and uses the dual formulation utilized by the DICE family (Nachum et al., 2019), but reduces to GAIL (Ho & Ermon, 2016) in the online setting, requiring coverage assumptions. Our work also minimizes the divergence between the agent\u2019s visitation distribution and the goal distribution, but we provide a new formulation for on-policy goal-conditioned RL that does not require a discriminator or the same coverage assumptions.\n\nPolicy Learning through State Matching. We first focus on imitation learning where the expert distribution $$p_{E}(s, a)$$ is directly inferred from the expert data. GAIL (Ho & Ermon, 2016) showed that the inverse RL objective is the dual of state-matching. f-MAX (Ghasemipour et al., 2019) uses f-divergence as a metric to match the agent\u2019s state-action visitation distribution $$p_{\\pi}(s, a)$$ and $$p_{E}(s, a)$$. Ke et al. (2019); Ghasemipour et al. (2019) shows how several commonly used imitation learning methods can be reduced to a divergence minimization. But all of these methods optimize a lower bound of the divergence which is essentially a min-max bilevel optimization objective. They break the min-max into two parts, fitting the density model to obtain a reward that can be used for policy optimization. But these rewards depend on the policy, and should not be used by RL algorithms that assume stationary rewards. f-IRL (Ni et al., 2020) escapes the min-max objective but learns a reward function that can be used for policy optimization. We do not aim to learn a reward function but rather directly optimize for a policy using dense signals from an f-divergence objective.\n\nIn reinforcement learning, the connections between entropy regularized MaxEnt RL and the minimization of reverse KL between agent\u2019s trajectory distribution, $$p_{\\pi}(\\tau)$$, and the \u201coptimal\" trajectory distribution, $$p^{*}(\\tau) \\propto e^{r(\\tau)}$$ has been extensively studied Ziebart (2010); Ziebart et al. (2008); Kappen et al. (2012); Levine (2018); Haarnoja et al. (2018). MaxEnt RL optimizes for a policy with maximum entropy but such a policy does not guarantee maximum coverage of the state space. Hazan et al. (2018) discusses an objective for maximum exploration that focuses on maximizing the entropy of the state-visitation distribution or KL divergence between the state-visitation distribution and a uniform distribution. A few works like Durugkar et al. (2023, 2021); Ma et al. (2022), that have explored state-matching for reinforcement learning, have been discussed above.\n\nLimitations of Markov Rewards. Our work looks beyond the maximization of a Markov reward for policy optimization. The learning signals that we use are non-stationary. We thus discuss the limitations of using Markov rewards for obtaining the optimal policy. There have been works (Abel et al., 2021; Clark & Amodei, 2016; Icarte et al., 2018, 2021) that express the difficulty in using Markov rewards. Abel et al. (2021) proves that there always exist environment-task pairs that cannot be described using Markov rewards. Reward Machines (Icarte et al., 2018) create finite automata to specify reward functions and can specify Non-Markov rewards as well but these are hand-crafted.\n\nf-Policy Gradient\n\nIn this paper, we derive an algorithm where the agents learn by minimizing the following f-divergence:\n\n$$J(\\theta) = D_{f}(p_{\\theta}(s)||p_{g}(s))$$\nIn this section, we shall derive an algorithm to minimize $$J(\\theta)$$ and analyze the objective more closely in the subsequent section. Unlike f-max (Ghasemipour et al., 2019), we directly optimize $$J(\\theta)$$. We differentiate $$J(\\theta)$$ with respect to $$\\theta$$ to get this gradient.\n\nTheorem 4.1. The gradient of $$J(\\theta)$$ as defined in Equation 4 is given by,\n\n$$\\nabla_{\\theta}J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) \\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$\nThe gradient looks exactly like policy gradient with rewards $$-\\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$. However, this does not mean that we are maximizing $$J_{RL}(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} - \\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$. This is because the gradient of $$J_{RL}(\\theta)$$ is not the same as $$\\nabla_{\\theta}J(\\theta)$$. For Dirac goal distributions, the gradient in Equation 5 cannot be used (as $$f'(p_{\\theta}(s_{t}))$$ will not be defined when $$p_{g}(s_{t}) = 0$$). We can use the definition of f-divergence in Equation 3 to derive a gradient for such distributions.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Future state densities as rewards. However, these methods will fail when the agent has never seen the goal. Moreover, in most of these works, the reward is not stationary (is dependent on the policy) which can lead to instabilities during policy optimization. GoFAR (Ma et al., 2022) is an offline goal-conditioned RL algorithm that minimizes a lower bound to the KL divergence between $$p_{\\theta}(s)$$ and the $$p_{g}(s)$$. It computes rewards using a discriminator and uses the dual formulation utilized by the DICE family (Nachum et al., 2019), but reduces to GAIL (Ho & Ermon, 2016) in the online setting, requiring coverage assumptions. Our work also minimizes the divergence between the agent\u2019s visitation distribution and the goal distribution, but we provide a new formulation for on-policy goal-conditioned RL that does not require a discriminator or the same coverage assumptions.\n\nPolicy Learning through State Matching. We first focus on imitation learning where the expert distribution $$p_{E}(s, a)$$ is directly inferred from the expert data. GAIL (Ho & Ermon, 2016) showed that the inverse RL objective is the dual of state-matching. f-MAX (Ghasemipour et al., 2019) uses f-divergence as a metric to match the agent\u2019s state-action visitation distribution $$p_{\\pi}(s, a)$$ and $$p_{E}(s, a)$$. Ke et al. (2019); Ghasemipour et al. (2019) shows how several commonly used imitation learning methods can be reduced to a divergence minimization. But all of these methods optimize a lower bound of the divergence which is essentially a min-max bilevel optimization objective. They break the min-max into two parts, fitting the density model to obtain a reward that can be used for policy optimization. But these rewards depend on the policy, and should not be used by RL algorithms that assume stationary rewards. f-IRL (Ni et al., 2020) escapes the min-max objective but learns a reward function that can be used for policy optimization. We do not aim to learn a reward function but rather directly optimize for a policy using dense signals from an f-divergence objective.\n\nIn reinforcement learning, the connections between entropy regularized MaxEnt RL and the minimization of reverse KL between agent\u2019s trajectory distribution, $$p_{\\pi}(\\tau)$$, and the \u201coptimal\" trajectory distribution, $$p^{*}(\\tau) \\propto e^{r(\\tau)}$$ has been extensively studied Ziebart (2010); Ziebart et al. (2008); Kappen et al. (2012); Levine (2018); Haarnoja et al. (2018). MaxEnt RL optimizes for a policy with maximum entropy but such a policy does not guarantee maximum coverage of the state space. Hazan et al. (2018) discusses an objective for maximum exploration that focuses on maximizing the entropy of the state-visitation distribution or KL divergence between the state-visitation distribution and a uniform distribution. A few works like Durugkar et al. (2023, 2021); Ma et al. (2022), that have explored state-matching for reinforcement learning, have been discussed above.\n\nLimitations of Markov Rewards. Our work looks beyond the maximization of a Markov reward for policy optimization. The learning signals that we use are non-stationary. We thus discuss the limitations of using Markov rewards for obtaining the optimal policy. There have been works (Abel et al., 2021; Clark & Amodei, 2016; Icarte et al., 2018, 2021) that express the difficulty in using Markov rewards. Abel et al. (2021) proves that there always exist environment-task pairs that cannot be described using Markov rewards. Reward Machines (Icarte et al., 2018) create finite automata to specify reward functions and can specify Non-Markov rewards as well but these are hand-crafted.\n\nf-Policy Gradient\n\nIn this paper, we derive an algorithm where the agents learn by minimizing the following f-divergence:\n\n$$J(\\theta) = D_{f}(p_{\\theta}(s)||p_{g}(s))$$\nIn this section, we shall derive an algorithm to minimize $$J(\\theta)$$ and analyze the objective more closely in the subsequent section. Unlike f-max (Ghasemipour et al., 2019), we directly optimize $$J(\\theta)$$. We differentiate $$J(\\theta)$$ with respect to $$\\theta$$ to get this gradient.\n\nTheorem 4.1. The gradient of $$J(\\theta)$$ as defined in Equation 4 is given by,\n\n$$\\nabla_{\\theta}J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) \\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$\nThe gradient looks exactly like policy gradient with rewards $$-\\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$. However, this does not mean that we are maximizing $$J_{RL}(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} - \\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$. This is because the gradient of $$J_{RL}(\\theta)$$ is not the same as $$\\nabla_{\\theta}J(\\theta)$$. For Dirac goal distributions, the gradient in Equation 5 cannot be used (as $$f'(p_{\\theta}(s_{t}))$$ will not be defined when $$p_{g}(s_{t}) = 0$$). We can use the definition of f-divergence in Equation 3 to derive a gradient for such distributions.", "md": "Future state densities as rewards. However, these methods will fail when the agent has never seen the goal. Moreover, in most of these works, the reward is not stationary (is dependent on the policy) which can lead to instabilities during policy optimization. GoFAR (Ma et al., 2022) is an offline goal-conditioned RL algorithm that minimizes a lower bound to the KL divergence between $$p_{\\theta}(s)$$ and the $$p_{g}(s)$$. It computes rewards using a discriminator and uses the dual formulation utilized by the DICE family (Nachum et al., 2019), but reduces to GAIL (Ho & Ermon, 2016) in the online setting, requiring coverage assumptions. Our work also minimizes the divergence between the agent\u2019s visitation distribution and the goal distribution, but we provide a new formulation for on-policy goal-conditioned RL that does not require a discriminator or the same coverage assumptions.\n\nPolicy Learning through State Matching. We first focus on imitation learning where the expert distribution $$p_{E}(s, a)$$ is directly inferred from the expert data. GAIL (Ho & Ermon, 2016) showed that the inverse RL objective is the dual of state-matching. f-MAX (Ghasemipour et al., 2019) uses f-divergence as a metric to match the agent\u2019s state-action visitation distribution $$p_{\\pi}(s, a)$$ and $$p_{E}(s, a)$$. Ke et al. (2019); Ghasemipour et al. (2019) shows how several commonly used imitation learning methods can be reduced to a divergence minimization. But all of these methods optimize a lower bound of the divergence which is essentially a min-max bilevel optimization objective. They break the min-max into two parts, fitting the density model to obtain a reward that can be used for policy optimization. But these rewards depend on the policy, and should not be used by RL algorithms that assume stationary rewards. f-IRL (Ni et al., 2020) escapes the min-max objective but learns a reward function that can be used for policy optimization. We do not aim to learn a reward function but rather directly optimize for a policy using dense signals from an f-divergence objective.\n\nIn reinforcement learning, the connections between entropy regularized MaxEnt RL and the minimization of reverse KL between agent\u2019s trajectory distribution, $$p_{\\pi}(\\tau)$$, and the \u201coptimal\" trajectory distribution, $$p^{*}(\\tau) \\propto e^{r(\\tau)}$$ has been extensively studied Ziebart (2010); Ziebart et al. (2008); Kappen et al. (2012); Levine (2018); Haarnoja et al. (2018). MaxEnt RL optimizes for a policy with maximum entropy but such a policy does not guarantee maximum coverage of the state space. Hazan et al. (2018) discusses an objective for maximum exploration that focuses on maximizing the entropy of the state-visitation distribution or KL divergence between the state-visitation distribution and a uniform distribution. A few works like Durugkar et al. (2023, 2021); Ma et al. (2022), that have explored state-matching for reinforcement learning, have been discussed above.\n\nLimitations of Markov Rewards. Our work looks beyond the maximization of a Markov reward for policy optimization. The learning signals that we use are non-stationary. We thus discuss the limitations of using Markov rewards for obtaining the optimal policy. There have been works (Abel et al., 2021; Clark & Amodei, 2016; Icarte et al., 2018, 2021) that express the difficulty in using Markov rewards. Abel et al. (2021) proves that there always exist environment-task pairs that cannot be described using Markov rewards. Reward Machines (Icarte et al., 2018) create finite automata to specify reward functions and can specify Non-Markov rewards as well but these are hand-crafted.\n\nf-Policy Gradient\n\nIn this paper, we derive an algorithm where the agents learn by minimizing the following f-divergence:\n\n$$J(\\theta) = D_{f}(p_{\\theta}(s)||p_{g}(s))$$\nIn this section, we shall derive an algorithm to minimize $$J(\\theta)$$ and analyze the objective more closely in the subsequent section. Unlike f-max (Ghasemipour et al., 2019), we directly optimize $$J(\\theta)$$. We differentiate $$J(\\theta)$$ with respect to $$\\theta$$ to get this gradient.\n\nTheorem 4.1. The gradient of $$J(\\theta)$$ as defined in Equation 4 is given by,\n\n$$\\nabla_{\\theta}J(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) \\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$\nThe gradient looks exactly like policy gradient with rewards $$-\\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$. However, this does not mean that we are maximizing $$J_{RL}(\\theta) = E_{\\tau \\sim p_{\\theta}(\\tau)} - \\frac{f'(p_{\\theta}(s_{t}))}{p_{g}(s_{t})}$$. This is because the gradient of $$J_{RL}(\\theta)$$ is not the same as $$\\nabla_{\\theta}J(\\theta)$$. For Dirac goal distributions, the gradient in Equation 5 cannot be used (as $$f'(p_{\\theta}(s_{t}))$$ will not be defined when $$p_{g}(s_{t}) = 0$$). We can use the definition of f-divergence in Equation 3 to derive a gradient for such distributions."}]}, {"page": 5, "text": "The gradient is obtained in terms of the state visitation frequencies \u03b7\u03c4                                    (s). Further examination of the\ngradient leads to the following theorem,\nTheorem 4.2. Updating the policy using the gradient (Equation 5) maximizes Ep                                                   \u03b8[\u03b7\u03c4   (g)].\nTheorem 4.2 provides another perspective for f-Policy Gradient \u2013 \u03b7\u03c4                                       (g) is equivalent to the expected\nreturn for a goal-based sparse reward, hence optimizing the true goal-conditioned RL objective. We\nshall prove the optimality of the policy obtained from minimizing J(\u03b8) in the next section.\nIn practice, a Dirac goal distribution can be approximated by clipping off the zero probabilities at\n\u03f5, similar to Laplace correction. Doing so, we will be able to use dense signals from the gradient\nin Equation 5 while still producing the optimal policy. This approximation is different from simply\nadding an \u03f5 reward at every state. This is because the gradients are still weighed by f \u2032                                             p\u03b8(st)  which\n                                                                                                                                          \u03f5\ndepends on p\u03b8(st).\nSimply optimizing J(\u03b8) is difficult because it faces similar issues to REINFORCE (Williams &\nPeng, 1991). A major shortcoming of the above gradient computation is that it requires completely\non-policy updates. This requirement will make learning sample inefficient, especially when dealing\nwith any complex environments. However, there have been a number of improvements to na\u00efve\npolicy gradients that can be used. One approach is to use importance sampling (Precup, 2000),\nallowing samples collected from a previous policy \u03c0\u03b8\u2032 to be used for learning. To reap the benefits\nof importance sampling, we need the previous state-visitation distributions to computef \u2032                                                     p\u03b8(s)     .\n                                                                                                                                              pg(s)\nHence, we need to ensure that the current policy does not diverge much from the previous policy. This\ncondition is ensured by constraining the KL divergence between the current policy and the previous\npolicy. We use the clipped objective similar to Proximal Policy Optimization (Schulman et al., 2017),\nwhich has been shown to work well with policy gradients. PPO has shown that the clipped loss works\nwell even without an explicit KL constraint in the objective. The gradient used in practice is,\n            \u2207\u03b8J(\u03b8) = Est,at\u223cp\u03b8\u2032(st,at)                    min(r\u03b8(st)F\u03b8\u2032(st), clip(r\u03b8(st), 1 \u2212                       \u03f5, 1 + \u03f5)F\u03b8\u2032(st))               (6)\nwhere r\u03b8(t)             =      \u03c0\u03c0\u03b8(at|st)                              =      Algorithm 1 f-PG\n T                    p\u03b8\u2032(st)    \u03b8\u2032(at|st) and F\u03b8\u2032(st)                            Let, \u03c0\u03b8 be the policy, G be the set of goals, B\n    t\u2032=t \u03b3t\u2032f \u2032       pg(st)      . The derivation for this                       be a buffer\nobjective is provided in Appendix B. \u03b3 is added\nto improve the stability of gradients and to pre-                                 for i = 1 to num_iter do\nvent the sum of f \u2032            p \u03b8\u2032(st)      from exploding.                          B \u2190      []\n                                pg(st)                                                for j = 1 to num_traj_per_iter do\nFor the purpose of this paper, we use kernel den-                                         Sample g, set pg(s)\nsity estimators to estimate the goal distribution                                         Collect goal conditioned trajectories, \u03c4 : g\n                                                                                          Fit p\u03b8(s) using KDE on \u03c4\nand the agent\u2019s state visitation distribution. We                                         Store f \u2032      p\u03b8(s)      for each s in \u03c4\nmay also use discriminators to estimate the ratio                                                        pg(s)\nof these densities like Ho & Ermon (2016); Fu                                             B \u2190      B + {\u03c4 : g}\net al. (2017); Ghasemipour et al. (2019). But                                         end for\nunlike these methods, we will not be incorrectly                                      for j = 1 to num_policy_updates do\nbreaking a minmax objective. In our case, the                                             \u03b8 \u2190     \u03b8 \u2212    \u03b1\u2207\u03b8J(\u03b8) (Equation 6)\nestimate of the gradient requires the value of the                                    end for\nratio of the two distributions and does not make                                  end for\nany assumptions about the stationarity of these\nvalues. While the adversarial methods break the minmax objective and assume the discriminator to\nbe fixed (and rewards stationary) during policy optimization.\n5      Theoretical analysis of f                        -PG\nIn this section, we will first show that minimizing the f-divergence between the agent\u2019s state visitation\ndistribution and goal distribution yields the optimal policy. We will further analyze the connections\nto metric based shaping rewards and implicit exploration boost from the learning signals. For the rest\nof the paper, we will refer to f-PG using FKL divergence as fkl-PG, f-PG using RKL divergence as\nrkl-PG and so on.\n                                                                             5", "md": "The gradient is obtained in terms of the state visitation frequencies $$\\eta_{\\tau}(s)$$. Further examination of the gradient leads to the following theorem,\n\nTheorem 4.2. Updating the policy using the gradient (Equation 5) maximizes $$E_{p_{\\theta}}[\\eta_{\\tau}(g)]$$.\n\nTheorem 4.2 provides another perspective for f-Policy Gradient \u2013 $$\\eta_{\\tau}(g)$$ is equivalent to the expected return for a goal-based sparse reward, hence optimizing the true goal-conditioned RL objective. We shall prove the optimality of the policy obtained from minimizing $$J(\\theta)$$ in the next section.\n\nIn practice, a Dirac goal distribution can be approximated by clipping off the zero probabilities at $$\\epsilon$$, similar to Laplace correction. Doing so, we will be able to use dense signals from the gradient in Equation 5 while still producing the optimal policy. This approximation is different from simply adding an $$\\epsilon$$ reward at every state. This is because the gradients are still weighed by $$f'_{p_{\\theta}}(s) \\frac{\\epsilon}{p_{\\theta}(s)}$$ which depends on $$p_{\\theta}(s)$$.\n\nSimply optimizing $$J(\\theta)$$ is difficult because it faces similar issues to REINFORCE (Williams & Peng, 1991). A major shortcoming of the above gradient computation is that it requires completely on-policy updates. This requirement will make learning sample inefficient, especially when dealing with any complex environments. However, there have been a number of improvements to na\u00efve policy gradients that can be used. One approach is to use importance sampling (Precup, 2000), allowing samples collected from a previous policy $$\\pi_{\\theta'}$$ to be used for learning. To reap the benefits of importance sampling, we need the previous state-visitation distributions to compute $$f'_{p_{\\theta}(s)} \\frac{p_{\\theta}(s)}{p_{g}(s)}$$. Hence, we need to ensure that the current policy does not diverge much from the previous policy. This condition is ensured by constraining the KL divergence between the current policy and the previous policy. We use the clipped objective similar to Proximal Policy Optimization (Schulman et al., 2017), which has been shown to work well with policy gradients. PPO has shown that the clipped loss works well even without an explicit KL constraint in the objective. The gradient used in practice is,\n\n$$\n\\nabla_{\\theta}J(\\theta) = E_{s,t,a \\sim p_{\\theta'}}[\\min(r_{\\theta}(s,t)F_{\\theta'}(s,t), \\text{clip}(r_{\\theta}(s,t), 1 - \\epsilon, 1 + \\epsilon)F_{\\theta'}(s,t))]\n$$\n\nwhere $$r_{\\theta}(t) = \\frac{\\pi_{\\theta}(a_t|s_t)}{p_{\\theta'}(s_t)} = \\text{Algorithm 1 f-PG}$$\n\nLet, $$\\pi_{\\theta}$$ be the policy, $$G$$ be the set of goals, $$B$$ be a buffer\n\nfor i = 1 to num_iter do\nB <- []\nfor j = 1 to num_traj_per_iter do\nSample g, set pg(s)\nCollect goal conditioned trajectories, $$\\tau : g$$\nFit $$p_{\\theta}(s)$$ using KDE on $$\\tau$$\nStore $$f'_{p_{\\theta}(s)}$$ for each $$s$$ in $$\\tau$$\nB <- B + {$$\\tau : g$$}\nend for\nfor j = 1 to num_policy_updates do\n$$\\theta <- \\theta - \\alpha \\nabla_{\\theta}J(\\theta)$$ (Equation 6)\nend for\nend for\n\nFor the purpose of this paper, we use kernel density estimators to estimate the goal distribution and the agent\u2019s state visitation distribution. We may also use discriminators to estimate the ratio of these densities like Ho & Ermon (2016); Fu et al. (2017); Ghasemipour et al. (2019). But unlike these methods, we will not be incorrectly breaking a minmax objective. In our case, the estimate of the gradient requires the value of the ratio of the two distributions and does not make any assumptions about the stationarity of these values. While the adversarial methods break the minmax objective and assume the discriminator to be fixed (and rewards stationary) during policy optimization.\n\n## 5 Theoretical analysis of f-PG\n\nIn this section, we will first show that minimizing the f-divergence between the agent\u2019s state visitation distribution and goal distribution yields the optimal policy. We will further analyze the connections to metric based shaping rewards and implicit exploration boost from the learning signals. For the rest of the paper, we will refer to f-PG using FKL divergence as fkl-PG, f-PG using RKL divergence as rkl-PG and so on.", "images": [], "items": [{"type": "text", "value": "The gradient is obtained in terms of the state visitation frequencies $$\\eta_{\\tau}(s)$$. Further examination of the gradient leads to the following theorem,\n\nTheorem 4.2. Updating the policy using the gradient (Equation 5) maximizes $$E_{p_{\\theta}}[\\eta_{\\tau}(g)]$$.\n\nTheorem 4.2 provides another perspective for f-Policy Gradient \u2013 $$\\eta_{\\tau}(g)$$ is equivalent to the expected return for a goal-based sparse reward, hence optimizing the true goal-conditioned RL objective. We shall prove the optimality of the policy obtained from minimizing $$J(\\theta)$$ in the next section.\n\nIn practice, a Dirac goal distribution can be approximated by clipping off the zero probabilities at $$\\epsilon$$, similar to Laplace correction. Doing so, we will be able to use dense signals from the gradient in Equation 5 while still producing the optimal policy. This approximation is different from simply adding an $$\\epsilon$$ reward at every state. This is because the gradients are still weighed by $$f'_{p_{\\theta}}(s) \\frac{\\epsilon}{p_{\\theta}(s)}$$ which depends on $$p_{\\theta}(s)$$.\n\nSimply optimizing $$J(\\theta)$$ is difficult because it faces similar issues to REINFORCE (Williams & Peng, 1991). A major shortcoming of the above gradient computation is that it requires completely on-policy updates. This requirement will make learning sample inefficient, especially when dealing with any complex environments. However, there have been a number of improvements to na\u00efve policy gradients that can be used. One approach is to use importance sampling (Precup, 2000), allowing samples collected from a previous policy $$\\pi_{\\theta'}$$ to be used for learning. To reap the benefits of importance sampling, we need the previous state-visitation distributions to compute $$f'_{p_{\\theta}(s)} \\frac{p_{\\theta}(s)}{p_{g}(s)}$$. Hence, we need to ensure that the current policy does not diverge much from the previous policy. This condition is ensured by constraining the KL divergence between the current policy and the previous policy. We use the clipped objective similar to Proximal Policy Optimization (Schulman et al., 2017), which has been shown to work well with policy gradients. PPO has shown that the clipped loss works well even without an explicit KL constraint in the objective. The gradient used in practice is,\n\n$$\n\\nabla_{\\theta}J(\\theta) = E_{s,t,a \\sim p_{\\theta'}}[\\min(r_{\\theta}(s,t)F_{\\theta'}(s,t), \\text{clip}(r_{\\theta}(s,t), 1 - \\epsilon, 1 + \\epsilon)F_{\\theta'}(s,t))]\n$$\n\nwhere $$r_{\\theta}(t) = \\frac{\\pi_{\\theta}(a_t|s_t)}{p_{\\theta'}(s_t)} = \\text{Algorithm 1 f-PG}$$\n\nLet, $$\\pi_{\\theta}$$ be the policy, $$G$$ be the set of goals, $$B$$ be a buffer\n\nfor i = 1 to num_iter do\nB <- []\nfor j = 1 to num_traj_per_iter do\nSample g, set pg(s)\nCollect goal conditioned trajectories, $$\\tau : g$$\nFit $$p_{\\theta}(s)$$ using KDE on $$\\tau$$\nStore $$f'_{p_{\\theta}(s)}$$ for each $$s$$ in $$\\tau$$\nB <- B + {$$\\tau : g$$}\nend for\nfor j = 1 to num_policy_updates do\n$$\\theta <- \\theta - \\alpha \\nabla_{\\theta}J(\\theta)$$ (Equation 6)\nend for\nend for\n\nFor the purpose of this paper, we use kernel density estimators to estimate the goal distribution and the agent\u2019s state visitation distribution. We may also use discriminators to estimate the ratio of these densities like Ho & Ermon (2016); Fu et al. (2017); Ghasemipour et al. (2019). But unlike these methods, we will not be incorrectly breaking a minmax objective. In our case, the estimate of the gradient requires the value of the ratio of the two distributions and does not make any assumptions about the stationarity of these values. While the adversarial methods break the minmax objective and assume the discriminator to be fixed (and rewards stationary) during policy optimization.", "md": "The gradient is obtained in terms of the state visitation frequencies $$\\eta_{\\tau}(s)$$. Further examination of the gradient leads to the following theorem,\n\nTheorem 4.2. Updating the policy using the gradient (Equation 5) maximizes $$E_{p_{\\theta}}[\\eta_{\\tau}(g)]$$.\n\nTheorem 4.2 provides another perspective for f-Policy Gradient \u2013 $$\\eta_{\\tau}(g)$$ is equivalent to the expected return for a goal-based sparse reward, hence optimizing the true goal-conditioned RL objective. We shall prove the optimality of the policy obtained from minimizing $$J(\\theta)$$ in the next section.\n\nIn practice, a Dirac goal distribution can be approximated by clipping off the zero probabilities at $$\\epsilon$$, similar to Laplace correction. Doing so, we will be able to use dense signals from the gradient in Equation 5 while still producing the optimal policy. This approximation is different from simply adding an $$\\epsilon$$ reward at every state. This is because the gradients are still weighed by $$f'_{p_{\\theta}}(s) \\frac{\\epsilon}{p_{\\theta}(s)}$$ which depends on $$p_{\\theta}(s)$$.\n\nSimply optimizing $$J(\\theta)$$ is difficult because it faces similar issues to REINFORCE (Williams & Peng, 1991). A major shortcoming of the above gradient computation is that it requires completely on-policy updates. This requirement will make learning sample inefficient, especially when dealing with any complex environments. However, there have been a number of improvements to na\u00efve policy gradients that can be used. One approach is to use importance sampling (Precup, 2000), allowing samples collected from a previous policy $$\\pi_{\\theta'}$$ to be used for learning. To reap the benefits of importance sampling, we need the previous state-visitation distributions to compute $$f'_{p_{\\theta}(s)} \\frac{p_{\\theta}(s)}{p_{g}(s)}$$. Hence, we need to ensure that the current policy does not diverge much from the previous policy. This condition is ensured by constraining the KL divergence between the current policy and the previous policy. We use the clipped objective similar to Proximal Policy Optimization (Schulman et al., 2017), which has been shown to work well with policy gradients. PPO has shown that the clipped loss works well even without an explicit KL constraint in the objective. The gradient used in practice is,\n\n$$\n\\nabla_{\\theta}J(\\theta) = E_{s,t,a \\sim p_{\\theta'}}[\\min(r_{\\theta}(s,t)F_{\\theta'}(s,t), \\text{clip}(r_{\\theta}(s,t), 1 - \\epsilon, 1 + \\epsilon)F_{\\theta'}(s,t))]\n$$\n\nwhere $$r_{\\theta}(t) = \\frac{\\pi_{\\theta}(a_t|s_t)}{p_{\\theta'}(s_t)} = \\text{Algorithm 1 f-PG}$$\n\nLet, $$\\pi_{\\theta}$$ be the policy, $$G$$ be the set of goals, $$B$$ be a buffer\n\nfor i = 1 to num_iter do\nB <- []\nfor j = 1 to num_traj_per_iter do\nSample g, set pg(s)\nCollect goal conditioned trajectories, $$\\tau : g$$\nFit $$p_{\\theta}(s)$$ using KDE on $$\\tau$$\nStore $$f'_{p_{\\theta}(s)}$$ for each $$s$$ in $$\\tau$$\nB <- B + {$$\\tau : g$$}\nend for\nfor j = 1 to num_policy_updates do\n$$\\theta <- \\theta - \\alpha \\nabla_{\\theta}J(\\theta)$$ (Equation 6)\nend for\nend for\n\nFor the purpose of this paper, we use kernel density estimators to estimate the goal distribution and the agent\u2019s state visitation distribution. We may also use discriminators to estimate the ratio of these densities like Ho & Ermon (2016); Fu et al. (2017); Ghasemipour et al. (2019). But unlike these methods, we will not be incorrectly breaking a minmax objective. In our case, the estimate of the gradient requires the value of the ratio of the two distributions and does not make any assumptions about the stationarity of these values. While the adversarial methods break the minmax objective and assume the discriminator to be fixed (and rewards stationary) during policy optimization."}, {"type": "heading", "lvl": 2, "value": "5 Theoretical analysis of f-PG", "md": "## 5 Theoretical analysis of f-PG"}, {"type": "text", "value": "In this section, we will first show that minimizing the f-divergence between the agent\u2019s state visitation distribution and goal distribution yields the optimal policy. We will further analyze the connections to metric based shaping rewards and implicit exploration boost from the learning signals. For the rest of the paper, we will refer to f-PG using FKL divergence as fkl-PG, f-PG using RKL divergence as rkl-PG and so on.", "md": "In this section, we will first show that minimizing the f-divergence between the agent\u2019s state visitation distribution and goal distribution yields the optimal policy. We will further analyze the connections to metric based shaping rewards and implicit exploration boost from the learning signals. For the rest of the paper, we will refer to f-PG using FKL divergence as fkl-PG, f-PG using RKL divergence as rkl-PG and so on."}]}, {"page": 6, "text": "5.1    Analysis of J(\u03b8)\nThis section shows that the policy obtained by minimizing an f-divergence between the agent\u2019s state\nvisitation distribution and the goal distribution is the optimal policy.\nTheorem 5.1. The policy that minimizes Df(p\u03c0||pg) for a convex function f with f(1) = 0 and\nf \u2032(\u221e) being defined, is the optimal policy.\nThe proof for Theorem 5.1 is provided in Appendix A. The Theorem states that the policy obtained by\nminimizing the f-divergence between the agent\u2019s state-visitation distribution and the goal distribution\nis the optimal policy for a class of convex functions defining the f-divergence with f \u2032(\u221e) defined. It\nthus makes sense to minimize the f-divergence between the agent\u2019s visitation and the goal distribution.\nIt must be noted that the objective does not involve maximizing a reward function. Note that the\ncondition that f \u2032(\u221e) is defined is not true for all f-divergences. The common f-divergences like\nRKL, TV, and JS have f \u2032(\u221e) defined rkl-PG, tv-PG, and js-PG will produce the optimal policy.\nForward KL divergence (FKL) has f = u log u and so does not have f \u2032(\u221e) defined. Does this mean\nthat the policy obtained by minimizing the FKL divergence is not optimal? Lemma 5.1 (proof in\nAppendix A) shows that the policy obtained maximizes the entropy of the agent\u2019s state-visitation\ndistribution along with maximizing a reward of log pg(s).\nLemma 5.1. fkl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of\nthe state-visitation distribution.\nA similar result can be shown for \u03c72-divergence as well. It must be understood that Lemma 5.1 does\nnot mean that fkl-PG is the same as the commonly studied MaxEnt RL.\nDifferences from MaxEnt RL: MaxEnt RL, as studied in Haarnoja et al. (2017, 2018), maximizes the\nentropy of the policy along with the task reward to achieve better exploration. However, maximizing\nthe entropy of the policy does not imply maximum exploration. Hazan et al. (2018) shows that\nmaximizing the entropy of the state-visitation distribution provably provides maximum exploration.\nLemma 5.1 shows that fkl-PG maximizes the entropy of the state-visitation distribution along with\nthe reward making it better suited for exploration. To distinguish our work, we call the MaxEnt RL,\nas discussed in works like Haarnoja et al. (2017, 2018), as \u03c0-MaxEnt RL because it only focuses on\nthe entropy of the policy. On the other hand, fkl-PG maximizes the entropy of the state-visitation\ndistribution so we call it state-MaxEnt RL or s-MaxEnt RL. Similarly, sa-MaxEnt RL can be\ndefined to maximize the entropy of the state-action visitation distribution.\n                                              (a) s-MaxEnt RL\n                                              (b) \u03c0-MaxEnt RL\nFigure 1: Comparison of the evolution state-visitation distributions with training for \u03c0-MaxEnt RL and s-\nMaxEnt RL. The darker regions imply lower visitation while the bright regions imply higher visitations.\nSince the agent\u2019s state visitation distribution depends on both the policy and the dynamics, simply\nincreasing the entropy of the policy (without considering the dynamics) will not ensure that the agent\nwill visit most of the states or will have a state-visitation distribution with high entropy. In Figure\n1, we compare the effi    ciencies of \u03c0-MaxEnt RL and s-MaxEnt RL to explore around a wall in a\ndiscrete gridworld. The initial and the goal distributions ( highlighted in green and red respectively)\nare separated by a wall. This environment is further discussed in Section 6.1 and Appendix C. Figure\n1 shows the evolution of the agent\u2019s state-visitation distribution with training for s-MaxEnt RL\n(fkl-PG) and \u03c0-MaxEnt RL (Soft Q Learning (Haarnoja et al., 2017))\n                                                      6", "md": "# Analysis of J(\u03b8)\n\n## Analysis of J(\u03b8)\n\nThis section shows that the policy obtained by minimizing an f-divergence between the agent\u2019s state visitation distribution and the goal distribution is the optimal policy.\n\nTheorem 5.1. The policy that minimizes $$D_f(p_\\pi || p_g)$$ for a convex function f with f(1) = 0 and f'(\u221e) being defined, is the optimal policy.\n\nThe proof for Theorem 5.1 is provided in Appendix A. The Theorem states that the policy obtained by minimizing the f-divergence between the agent\u2019s state-visitation distribution and the goal distribution is the optimal policy for a class of convex functions defining the f-divergence with f'(\u221e) defined. It thus makes sense to minimize the f-divergence between the agent\u2019s visitation and the goal distribution.\n\nIt must be noted that the objective does not involve maximizing a reward function. Note that the condition that f'(\u221e) is defined is not true for all f-divergences. The common f-divergences like RKL, TV, and JS have f'(\u221e) defined rkl-PG, tv-PG, and js-PG will produce the optimal policy.\n\nForward KL divergence (FKL) has f = u log u and so does not have f'(\u221e) defined. Does this mean that the policy obtained by minimizing the FKL divergence is not optimal? Lemma 5.1 (proof in Appendix A) shows that the policy obtained maximizes the entropy of the agent\u2019s state-visitation distribution along with maximizing a reward of log pg(s).\n\nLemma 5.1. fkl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of the state-visitation distribution.\n\nA similar result can be shown for \u03c72-divergence as well. It must be understood that Lemma 5.1 does not mean that fkl-PG is the same as the commonly studied MaxEnt RL.\n\nDifferences from MaxEnt RL: MaxEnt RL, as studied in Haarnoja et al. (2017, 2018), maximizes the entropy of the policy along with the task reward to achieve better exploration. However, maximizing the entropy of the policy does not imply maximum exploration. Hazan et al. (2018) shows that maximizing the entropy of the state-visitation distribution provably provides maximum exploration.\n\nLemma 5.1 shows that fkl-PG maximizes the entropy of the state-visitation distribution along with the reward making it better suited for exploration. To distinguish our work, we call the MaxEnt RL, as discussed in works like Haarnoja et al. (2017, 2018), as \u03c0-MaxEnt RL because it only focuses on the entropy of the policy. On the other hand, fkl-PG maximizes the entropy of the state-visitation distribution so we call it state-MaxEnt RL or s-MaxEnt RL. Similarly, sa-MaxEnt RL can be defined to maximize the entropy of the state-action visitation distribution.\n\n(a) s-MaxEnt RL\n\n(b) \u03c0-MaxEnt RL\n\nFigure 1: Comparison of the evolution state-visitation distributions with training for \u03c0-MaxEnt RL and s-MaxEnt RL. The darker regions imply lower visitation while the bright regions imply higher visitations.\n\nSince the agent\u2019s state visitation distribution depends on both the policy and the dynamics, simply increasing the entropy of the policy (without considering the dynamics) will not ensure that the agent will visit most of the states or will have a state-visitation distribution with high entropy. In Figure 1, we compare the efficiencies of \u03c0-MaxEnt RL and s-MaxEnt RL to explore around a wall in a discrete gridworld. The initial and the goal distributions (highlighted in green and red respectively) are separated by a wall. This environment is further discussed in Section 6.1 and Appendix C. Figure 1 shows the evolution of the agent\u2019s state-visitation distribution with training for s-MaxEnt RL (fkl-PG) and \u03c0-MaxEnt RL (Soft Q Learning (Haarnoja et al., 2017)).", "images": [{"name": "page-6-1.jpg", "height": 60, "width": 357, "x": 127, "y": 526}, {"name": "page-6-0.jpg", "height": 60, "width": 357, "x": 127, "y": 449}], "items": [{"type": "heading", "lvl": 1, "value": "Analysis of J(\u03b8)", "md": "# Analysis of J(\u03b8)"}, {"type": "heading", "lvl": 2, "value": "Analysis of J(\u03b8)", "md": "## Analysis of J(\u03b8)"}, {"type": "text", "value": "This section shows that the policy obtained by minimizing an f-divergence between the agent\u2019s state visitation distribution and the goal distribution is the optimal policy.\n\nTheorem 5.1. The policy that minimizes $$D_f(p_\\pi || p_g)$$ for a convex function f with f(1) = 0 and f'(\u221e) being defined, is the optimal policy.\n\nThe proof for Theorem 5.1 is provided in Appendix A. The Theorem states that the policy obtained by minimizing the f-divergence between the agent\u2019s state-visitation distribution and the goal distribution is the optimal policy for a class of convex functions defining the f-divergence with f'(\u221e) defined. It thus makes sense to minimize the f-divergence between the agent\u2019s visitation and the goal distribution.\n\nIt must be noted that the objective does not involve maximizing a reward function. Note that the condition that f'(\u221e) is defined is not true for all f-divergences. The common f-divergences like RKL, TV, and JS have f'(\u221e) defined rkl-PG, tv-PG, and js-PG will produce the optimal policy.\n\nForward KL divergence (FKL) has f = u log u and so does not have f'(\u221e) defined. Does this mean that the policy obtained by minimizing the FKL divergence is not optimal? Lemma 5.1 (proof in Appendix A) shows that the policy obtained maximizes the entropy of the agent\u2019s state-visitation distribution along with maximizing a reward of log pg(s).\n\nLemma 5.1. fkl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of the state-visitation distribution.\n\nA similar result can be shown for \u03c72-divergence as well. It must be understood that Lemma 5.1 does not mean that fkl-PG is the same as the commonly studied MaxEnt RL.\n\nDifferences from MaxEnt RL: MaxEnt RL, as studied in Haarnoja et al. (2017, 2018), maximizes the entropy of the policy along with the task reward to achieve better exploration. However, maximizing the entropy of the policy does not imply maximum exploration. Hazan et al. (2018) shows that maximizing the entropy of the state-visitation distribution provably provides maximum exploration.\n\nLemma 5.1 shows that fkl-PG maximizes the entropy of the state-visitation distribution along with the reward making it better suited for exploration. To distinguish our work, we call the MaxEnt RL, as discussed in works like Haarnoja et al. (2017, 2018), as \u03c0-MaxEnt RL because it only focuses on the entropy of the policy. On the other hand, fkl-PG maximizes the entropy of the state-visitation distribution so we call it state-MaxEnt RL or s-MaxEnt RL. Similarly, sa-MaxEnt RL can be defined to maximize the entropy of the state-action visitation distribution.\n\n(a) s-MaxEnt RL\n\n(b) \u03c0-MaxEnt RL\n\nFigure 1: Comparison of the evolution state-visitation distributions with training for \u03c0-MaxEnt RL and s-MaxEnt RL. The darker regions imply lower visitation while the bright regions imply higher visitations.\n\nSince the agent\u2019s state visitation distribution depends on both the policy and the dynamics, simply increasing the entropy of the policy (without considering the dynamics) will not ensure that the agent will visit most of the states or will have a state-visitation distribution with high entropy. In Figure 1, we compare the efficiencies of \u03c0-MaxEnt RL and s-MaxEnt RL to explore around a wall in a discrete gridworld. The initial and the goal distributions (highlighted in green and red respectively) are separated by a wall. This environment is further discussed in Section 6.1 and Appendix C. Figure 1 shows the evolution of the agent\u2019s state-visitation distribution with training for s-MaxEnt RL (fkl-PG) and \u03c0-MaxEnt RL (Soft Q Learning (Haarnoja et al., 2017)).", "md": "This section shows that the policy obtained by minimizing an f-divergence between the agent\u2019s state visitation distribution and the goal distribution is the optimal policy.\n\nTheorem 5.1. The policy that minimizes $$D_f(p_\\pi || p_g)$$ for a convex function f with f(1) = 0 and f'(\u221e) being defined, is the optimal policy.\n\nThe proof for Theorem 5.1 is provided in Appendix A. The Theorem states that the policy obtained by minimizing the f-divergence between the agent\u2019s state-visitation distribution and the goal distribution is the optimal policy for a class of convex functions defining the f-divergence with f'(\u221e) defined. It thus makes sense to minimize the f-divergence between the agent\u2019s visitation and the goal distribution.\n\nIt must be noted that the objective does not involve maximizing a reward function. Note that the condition that f'(\u221e) is defined is not true for all f-divergences. The common f-divergences like RKL, TV, and JS have f'(\u221e) defined rkl-PG, tv-PG, and js-PG will produce the optimal policy.\n\nForward KL divergence (FKL) has f = u log u and so does not have f'(\u221e) defined. Does this mean that the policy obtained by minimizing the FKL divergence is not optimal? Lemma 5.1 (proof in Appendix A) shows that the policy obtained maximizes the entropy of the agent\u2019s state-visitation distribution along with maximizing a reward of log pg(s).\n\nLemma 5.1. fkl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of the state-visitation distribution.\n\nA similar result can be shown for \u03c72-divergence as well. It must be understood that Lemma 5.1 does not mean that fkl-PG is the same as the commonly studied MaxEnt RL.\n\nDifferences from MaxEnt RL: MaxEnt RL, as studied in Haarnoja et al. (2017, 2018), maximizes the entropy of the policy along with the task reward to achieve better exploration. However, maximizing the entropy of the policy does not imply maximum exploration. Hazan et al. (2018) shows that maximizing the entropy of the state-visitation distribution provably provides maximum exploration.\n\nLemma 5.1 shows that fkl-PG maximizes the entropy of the state-visitation distribution along with the reward making it better suited for exploration. To distinguish our work, we call the MaxEnt RL, as discussed in works like Haarnoja et al. (2017, 2018), as \u03c0-MaxEnt RL because it only focuses on the entropy of the policy. On the other hand, fkl-PG maximizes the entropy of the state-visitation distribution so we call it state-MaxEnt RL or s-MaxEnt RL. Similarly, sa-MaxEnt RL can be defined to maximize the entropy of the state-action visitation distribution.\n\n(a) s-MaxEnt RL\n\n(b) \u03c0-MaxEnt RL\n\nFigure 1: Comparison of the evolution state-visitation distributions with training for \u03c0-MaxEnt RL and s-MaxEnt RL. The darker regions imply lower visitation while the bright regions imply higher visitations.\n\nSince the agent\u2019s state visitation distribution depends on both the policy and the dynamics, simply increasing the entropy of the policy (without considering the dynamics) will not ensure that the agent will visit most of the states or will have a state-visitation distribution with high entropy. In Figure 1, we compare the efficiencies of \u03c0-MaxEnt RL and s-MaxEnt RL to explore around a wall in a discrete gridworld. The initial and the goal distributions (highlighted in green and red respectively) are separated by a wall. This environment is further discussed in Section 6.1 and Appendix C. Figure 1 shows the evolution of the agent\u2019s state-visitation distribution with training for s-MaxEnt RL (fkl-PG) and \u03c0-MaxEnt RL (Soft Q Learning (Haarnoja et al., 2017))."}]}, {"page": 7, "text": "Figure 2: Evolution of f \u2032( p\u03b8(s)\n                              pg(s)) for f = u log u through policy learning. Top: f \u2032( p\u03b8(s)pg(s), darker blue are\nrelatively low values while red corresponds to higher values. Bottom: Corresponding state-visitation of the\npolicy.\nMetric-based Shaping Reward: A deeper look into Lemma 5.1 shows that an appropriate choice of\npg(s) can lead to entropy maximizing policy optimization with metric-based shaping rewards. Define\nthe goal distribution as pg(s) = ef(s;g) where f(s; g) captures the metric of the underlying space.\nThen the fkl-PG objective becomes,\n                          min DF KL(p\u03b8, pg) = max Ep         \u03b8[f(s; g)] \u2212   Ep\u03b8[log p\u03b8].                         (7)\nThe above objective maximizes the reward f(s; g) along with the entropy of the agent\u2019s state visitation\ndistribution. For an L2 Euclidean metric, f(s; g) will be \u2212||s \u2212          g||2\n                                                                             2 which is the L2 shaping reward,\nand the goal distribution will be Gaussian. If the goal distribution is Laplacian, the corresponding\nshaping reward will be the L1 norm.\nAIM (Durugkar et al., 2021) used a potential-based shaping reward based on a time step quasimetric.\nIf we define f(s; g) as a Lipschitz function for the time step metric maximizing at s = g, we will\nend up optimizing for the AIM reward along with maximizing the entropy of the state-visitation\ndistribution.\n5.2    Analysis of the learning signals\nf-PG involves a learning signal f \u2032( p\u03b8(s) pg(s)) to weigh the log probabilities of the policy. It is thus\nimportant to understand how f \u2032( p\u03b8(s) pg(s)) behaves for goal-conditioned RL settings. During the initial\nstages of training, the agent visits regions with very low pg. For such states, the signal has a lower\nvalue than the states that have lower p\u03b8, i.e., the unexplored states. This is because for any convex\nfunction f, f \u2032(x) is an increasing function, so minimizing f \u2032( p\u03b8(s)   pg(s)) (recall that we are minimizing\nf-divergence) will imply minimizing p\u03b8(s) for the states with low pg(s). The only way to do this\nis to increase the entropy of the state-visitation distribution, directly making the agent explore new\nstates. As long as there is no significant overlap between the two distributions, it will push p\u03b8 down\nto a flatter distribution until there is enough overlap with the goal distribution when it will pull back\nthe agent\u2019s visitation again to be closer to the goal distribution.\nThis learning signal should not be confused with reward in reinforcement learning. It is non-stationary\nand non-Markovian as it depends on the policy. More importantly, we are not maximizing this signal,\njust using it to weigh the gradients of the policy.\nIn the following example, we shall use the Reacher environment (Todorov et al., 2012) to illustrate\nhow our learning signal (f \u2032( p\u03b8(s)\n                                 pg(s))) varies as the agent learns. We will also show how this signal can\npush for exploration when the agent has not seen the goal yet. Consider the We fix the goal at (-0.21,\n0) and show how the learning signal evolves with the policy. While Figure 2 shows the evolution of\nthe learning signal for fkl-PG, the rest can be found in Appendix D.\nThe value of f \u2032( p\u03b8(s)\n                   pg(s)) is lowest where the agent\u2019s visitation is high and higher where the agent is not\nvisiting. f \u2032( p\u03b8(s)\n               pg(s)) has the highest value at the goal. As the policy converges to the optimal policy,\nthe regions where the state-visitation distribution is considerably low (towards the bottom-right in the\nfigure), the value for f \u2032( p\u03b8(s)\n                            pg(s)) increases for those states (to still push for exploration) but its value at\nthe goal is high enough for the policy to converge.\n                                                         7", "md": "# Math Equations and Text\n\n## Evolution of $$f'(p_{\\theta}(s), p_g(s))$$ for $$f = u \\log u$$ through policy learning\n\nTop: $$f'(p_{\\theta}(s), p_g(s))$$, darker blue are relatively low values while red corresponds to higher values. Bottom: Corresponding state-visitation of the policy.\n\n## Metric-based Shaping Reward\n\nA deeper look into Lemma 5.1 shows that an appropriate choice of $$p_g(s)$$ can lead to entropy maximizing policy optimization with metric-based shaping rewards. Define the goal distribution as $$p_g(s) = e^{f(s;g)}$$ where $$f(s; g)$$ captures the metric of the underlying space. Then the $$f_{KL}-PG$$ objective becomes,\n\n$$\\min D_{KL}(p_{\\theta}, p_g) = \\max \\mathbb{E}_{p_{\\theta}}[f(s; g)] - \\mathbb{E}_{p_{\\theta}}[\\log p_{\\theta}]$$\n\nThe above objective maximizes the reward $$f(s; g)$$ along with the entropy of the agent\u2019s state visitation distribution. For an L2 Euclidean metric, $$f(s; g)$$ will be $$-||s - g||_2^2$$ which is the L2 shaping reward, and the goal distribution will be Gaussian. If the goal distribution is Laplacian, the corresponding shaping reward will be the L1 norm.\n\n## AIM (Durugkar et al., 2021)\n\nused a potential-based shaping reward based on a time step quasimetric. If we define $$f(s; g)$$ as a Lipschitz function for the time step metric maximizing at $$s = g$$, we will end up optimizing for the AIM reward along with maximizing the entropy of the state-visitation distribution.\n\n## Analysis of the learning signals\n\n$$f-PG$$ involves a learning signal $$f'(p_{\\theta}(s), p_g(s))$$ to weigh the log probabilities of the policy. It is important to understand how $$f'(p_{\\theta}(s), p_g(s))$$ behaves for goal-conditioned RL settings. During the initial stages of training, the agent visits regions with very low $$p_g$$. For such states, the signal has a lower value than the states that have lower $$p_{\\theta}$$, i.e., the unexplored states. This is because for any convex function $$f$$, $$f'(x)$$ is an increasing function, so minimizing $$f'(p_{\\theta}(s), p_g(s))$$ will imply minimizing $$p_{\\theta}(s)$$ for the states with low $$p_g(s)$$. The only way to do this is to increase the entropy of the state-visitation distribution, directly making the agent explore new states. As long as there is no significant overlap between the two distributions, it will push $$p_{\\theta}$$ down to a flatter distribution until there is enough overlap with the goal distribution when it will pull back the agent\u2019s visitation again to be closer to the goal distribution.\n\nThis learning signal should not be confused with reward in reinforcement learning. It is non-stationary and non-Markovian as it depends on the policy. More importantly, we are not maximizing this signal, just using it to weigh the gradients of the policy.\n\nIn the following example, we shall use the Reacher environment (Todorov et al., 2012) to illustrate how our learning signal $$f'(p_{\\theta}(s), p_g(s))$$ varies as the agent learns. We will also show how this signal can push for exploration when the agent has not seen the goal yet. Consider the We fix the goal at (-0.21, 0) and show how the learning signal evolves with the policy. While Figure 2 shows the evolution of the learning signal for $$f_{KL}-PG$$, the rest can be found in Appendix D.\n\nThe value of $$f'(p_{\\theta}(s), p_g(s))$$ is lowest where the agent\u2019s visitation is high and higher where the agent is not visiting. $$f'(p_{\\theta}(s), p_g(s))$$ has the highest value at the goal. As the policy converges to the optimal policy, the regions where the state-visitation distribution is considerably low (towards the bottom-right in the figure), the value for $$f'(p_{\\theta}(s), p_g(s))$$ increases for those states (to still push for exploration) but its value at the goal is high enough for the policy to converge.", "images": [{"name": "page-7-0.jpg", "height": 124, "width": 317, "x": 147, "y": 72}], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Evolution of $$f'(p_{\\theta}(s), p_g(s))$$ for $$f = u \\log u$$ through policy learning", "md": "## Evolution of $$f'(p_{\\theta}(s), p_g(s))$$ for $$f = u \\log u$$ through policy learning"}, {"type": "text", "value": "Top: $$f'(p_{\\theta}(s), p_g(s))$$, darker blue are relatively low values while red corresponds to higher values. Bottom: Corresponding state-visitation of the policy.", "md": "Top: $$f'(p_{\\theta}(s), p_g(s))$$, darker blue are relatively low values while red corresponds to higher values. Bottom: Corresponding state-visitation of the policy."}, {"type": "heading", "lvl": 2, "value": "Metric-based Shaping Reward", "md": "## Metric-based Shaping Reward"}, {"type": "text", "value": "A deeper look into Lemma 5.1 shows that an appropriate choice of $$p_g(s)$$ can lead to entropy maximizing policy optimization with metric-based shaping rewards. Define the goal distribution as $$p_g(s) = e^{f(s;g)}$$ where $$f(s; g)$$ captures the metric of the underlying space. Then the $$f_{KL}-PG$$ objective becomes,\n\n$$\\min D_{KL}(p_{\\theta}, p_g) = \\max \\mathbb{E}_{p_{\\theta}}[f(s; g)] - \\mathbb{E}_{p_{\\theta}}[\\log p_{\\theta}]$$\n\nThe above objective maximizes the reward $$f(s; g)$$ along with the entropy of the agent\u2019s state visitation distribution. For an L2 Euclidean metric, $$f(s; g)$$ will be $$-||s - g||_2^2$$ which is the L2 shaping reward, and the goal distribution will be Gaussian. If the goal distribution is Laplacian, the corresponding shaping reward will be the L1 norm.", "md": "A deeper look into Lemma 5.1 shows that an appropriate choice of $$p_g(s)$$ can lead to entropy maximizing policy optimization with metric-based shaping rewards. Define the goal distribution as $$p_g(s) = e^{f(s;g)}$$ where $$f(s; g)$$ captures the metric of the underlying space. Then the $$f_{KL}-PG$$ objective becomes,\n\n$$\\min D_{KL}(p_{\\theta}, p_g) = \\max \\mathbb{E}_{p_{\\theta}}[f(s; g)] - \\mathbb{E}_{p_{\\theta}}[\\log p_{\\theta}]$$\n\nThe above objective maximizes the reward $$f(s; g)$$ along with the entropy of the agent\u2019s state visitation distribution. For an L2 Euclidean metric, $$f(s; g)$$ will be $$-||s - g||_2^2$$ which is the L2 shaping reward, and the goal distribution will be Gaussian. If the goal distribution is Laplacian, the corresponding shaping reward will be the L1 norm."}, {"type": "heading", "lvl": 2, "value": "AIM (Durugkar et al., 2021)", "md": "## AIM (Durugkar et al., 2021)"}, {"type": "text", "value": "used a potential-based shaping reward based on a time step quasimetric. If we define $$f(s; g)$$ as a Lipschitz function for the time step metric maximizing at $$s = g$$, we will end up optimizing for the AIM reward along with maximizing the entropy of the state-visitation distribution.", "md": "used a potential-based shaping reward based on a time step quasimetric. If we define $$f(s; g)$$ as a Lipschitz function for the time step metric maximizing at $$s = g$$, we will end up optimizing for the AIM reward along with maximizing the entropy of the state-visitation distribution."}, {"type": "heading", "lvl": 2, "value": "Analysis of the learning signals", "md": "## Analysis of the learning signals"}, {"type": "text", "value": "$$f-PG$$ involves a learning signal $$f'(p_{\\theta}(s), p_g(s))$$ to weigh the log probabilities of the policy. It is important to understand how $$f'(p_{\\theta}(s), p_g(s))$$ behaves for goal-conditioned RL settings. During the initial stages of training, the agent visits regions with very low $$p_g$$. For such states, the signal has a lower value than the states that have lower $$p_{\\theta}$$, i.e., the unexplored states. This is because for any convex function $$f$$, $$f'(x)$$ is an increasing function, so minimizing $$f'(p_{\\theta}(s), p_g(s))$$ will imply minimizing $$p_{\\theta}(s)$$ for the states with low $$p_g(s)$$. The only way to do this is to increase the entropy of the state-visitation distribution, directly making the agent explore new states. As long as there is no significant overlap between the two distributions, it will push $$p_{\\theta}$$ down to a flatter distribution until there is enough overlap with the goal distribution when it will pull back the agent\u2019s visitation again to be closer to the goal distribution.\n\nThis learning signal should not be confused with reward in reinforcement learning. It is non-stationary and non-Markovian as it depends on the policy. More importantly, we are not maximizing this signal, just using it to weigh the gradients of the policy.\n\nIn the following example, we shall use the Reacher environment (Todorov et al., 2012) to illustrate how our learning signal $$f'(p_{\\theta}(s), p_g(s))$$ varies as the agent learns. We will also show how this signal can push for exploration when the agent has not seen the goal yet. Consider the We fix the goal at (-0.21, 0) and show how the learning signal evolves with the policy. While Figure 2 shows the evolution of the learning signal for $$f_{KL}-PG$$, the rest can be found in Appendix D.\n\nThe value of $$f'(p_{\\theta}(s), p_g(s))$$ is lowest where the agent\u2019s visitation is high and higher where the agent is not visiting. $$f'(p_{\\theta}(s), p_g(s))$$ has the highest value at the goal. As the policy converges to the optimal policy, the regions where the state-visitation distribution is considerably low (towards the bottom-right in the figure), the value for $$f'(p_{\\theta}(s), p_g(s))$$ increases for those states (to still push for exploration) but its value at the goal is high enough for the policy to converge.", "md": "$$f-PG$$ involves a learning signal $$f'(p_{\\theta}(s), p_g(s))$$ to weigh the log probabilities of the policy. It is important to understand how $$f'(p_{\\theta}(s), p_g(s))$$ behaves for goal-conditioned RL settings. During the initial stages of training, the agent visits regions with very low $$p_g$$. For such states, the signal has a lower value than the states that have lower $$p_{\\theta}$$, i.e., the unexplored states. This is because for any convex function $$f$$, $$f'(x)$$ is an increasing function, so minimizing $$f'(p_{\\theta}(s), p_g(s))$$ will imply minimizing $$p_{\\theta}(s)$$ for the states with low $$p_g(s)$$. The only way to do this is to increase the entropy of the state-visitation distribution, directly making the agent explore new states. As long as there is no significant overlap between the two distributions, it will push $$p_{\\theta}$$ down to a flatter distribution until there is enough overlap with the goal distribution when it will pull back the agent\u2019s visitation again to be closer to the goal distribution.\n\nThis learning signal should not be confused with reward in reinforcement learning. It is non-stationary and non-Markovian as it depends on the policy. More importantly, we are not maximizing this signal, just using it to weigh the gradients of the policy.\n\nIn the following example, we shall use the Reacher environment (Todorov et al., 2012) to illustrate how our learning signal $$f'(p_{\\theta}(s), p_g(s))$$ varies as the agent learns. We will also show how this signal can push for exploration when the agent has not seen the goal yet. Consider the We fix the goal at (-0.21, 0) and show how the learning signal evolves with the policy. While Figure 2 shows the evolution of the learning signal for $$f_{KL}-PG$$, the rest can be found in Appendix D.\n\nThe value of $$f'(p_{\\theta}(s), p_g(s))$$ is lowest where the agent\u2019s visitation is high and higher where the agent is not visiting. $$f'(p_{\\theta}(s), p_g(s))$$ has the highest value at the goal. As the policy converges to the optimal policy, the regions where the state-visitation distribution is considerably low (towards the bottom-right in the figure), the value for $$f'(p_{\\theta}(s), p_g(s))$$ increases for those states (to still push for exploration) but its value at the goal is high enough for the policy to converge."}]}, {"page": 8, "text": "6    Experiments\nOur experiments evaluate our new framework (f-PG) as an alternative to conventional reward\nmaximization for goal-conditional RL. We pose the following questions:\n       1. Does f-PG provide suffi      cient signals to explore in otherwise challenging sparse reward\n          settings?\n       2. How well does our framework perform compared to discriminator-based approaches?\n       3. Can our framework scale to larger domains with continuous state spaces and randomly\n          generated goals?\n       4. How do different f-divergences affect learning?\nThe first two questions are answered using a toy gridworld environment. The gridworld has a goal\ncontained in a room which poses a significant exploration challenge. We also show how the dense\nsignal to the gradients of the policy evolves during training on a continuous domain like Reacher. To\nanswer the third question, our framework is compared with several baselines on a 2D Maze solving\ntask (Point Maze). Additionally, we scale to more complex tasks such as FetchReach Plappert et al.\n(2018) and an exploration-heavy PointMaze.\n6.1   Gridworld\nWe use a gridworld environment to compare and visualize the effects of using different shaping\nrewards for exploration. We discussed this environment briefly in Section 5.1. The task is for the\nagent to reach the goal contained in a room. The only way to reach the goal is to go around the wall.\nThe task reward is 1 when the agent reaches the room otherwise it is 0. The state space is simply the\n(x, y) coordinates of the grid and the goal is fixed. A detailed description of the task is provided in\nAppendix C. Although the environment seems simple, exploration here is very difficult as there is no\nincentive for the agent to go around the wall.\n       (a) fkl-PG                   (b) rkl-PG                    (c) AIM                      (d) GAIL\nFigure 3: Gridworld: The agent needs to move from the green circle to the red circle. The state visitations of\nthe policies (after 500 policy updates) are shown when using our framework for training (fkl, rkl) compared with\nAIM and GAIL trained on top of soft Q learning.\nOur framework is compared against AIM (Durugkar et al., 2021), which initially introduced this\nenvironment and uses a shaping reward obtained from state-matching to solve it, and GAIL (Ho &\nErmon, 2016), which uses a discriminator to learn the probability of a state being the goal state. We\nprovide a comparison to other recent methods in Appendix C. All the baselines are implemented on\ntop of Soft Q Learning (Haarnoja et al., 2017) which along with maximizing the augmented rewards,\nalso maximizes the entropy of the policy while f-PG is implemented as an on-policy algorithm\nwithout any extrinsic entropy maximization objective. It can be seen from Figure 3 that, f-PG can\nexplore enough to find the way around the room which is difficult for methods like GAIL even after\nthe entropy boost. AIM learns a potential function and can also find its way across the wall. As\nexpected, fkl-PG converges to the policy maximizing the entropy of the state visitation while rkl-PG\nproduces the optimal state visitation as expected from Theorem 5.1. This simple experiment clearly\nillustrates two things: (1) f-PG can generate dense signals to explore the state space and search for\nthe goal and (2) although discriminator-based methods like GAIL try to perform state-matching, they\nfail to explore the space well.\n                                                       8", "md": "# Experiments\n\n## Experiments\n\nOur experiments evaluate our new framework (f-PG) as an alternative to conventional reward maximization for goal-conditional RL. We pose the following questions:\n\n1. Does f-PG provide sufficient signals to explore in otherwise challenging sparse reward settings?\n2. How well does our framework perform compared to discriminator-based approaches?\n3. Can our framework scale to larger domains with continuous state spaces and randomly generated goals?\n4. How do different f-divergences affect learning?\n\nThe first two questions are answered using a toy gridworld environment. The gridworld has a goal contained in a room which poses a significant exploration challenge. We also show how the dense signal to the gradients of the policy evolves during training on a continuous domain like Reacher. To answer the third question, our framework is compared with several baselines on a 2D Maze solving task (Point Maze). Additionally, we scale to more complex tasks such as FetchReach Plappert et al. (2018) and an exploration-heavy PointMaze.\n\n### Gridworld\n\nWe use a gridworld environment to compare and visualize the effects of using different shaping rewards for exploration. We discussed this environment briefly in Section 5.1. The task is for the agent to reach the goal contained in a room. The only way to reach the goal is to go around the wall. The task reward is 1 when the agent reaches the room otherwise it is 0. The state space is simply the (x, y) coordinates of the grid and the goal is fixed. A detailed description of the task is provided in Appendix C. Although the environment seems simple, exploration here is very difficult as there is no incentive for the agent to go around the wall.\n\n(a) fkl-PG &emsp; (b) rkl-PG &emsp; (c) AIM &emsp; (d) GAIL\n\nOur framework is compared against AIM (Durugkar et al., 2021), which initially introduced this environment and uses a shaping reward obtained from state-matching to solve it, and GAIL (Ho & Ermon, 2016), which uses a discriminator to learn the probability of a state being the goal state. We provide a comparison to other recent methods in Appendix C. All the baselines are implemented on top of Soft Q Learning (Haarnoja et al., 2017) which along with maximizing the augmented rewards, also maximizes the entropy of the policy while f-PG is implemented as an on-policy algorithm without any extrinsic entropy maximization objective. It can be seen from the image that, f-PG can explore enough to find the way around the room which is difficult for methods like GAIL even after the entropy boost. AIM learns a potential function and can also find its way across the wall. As expected, fkl-PG converges to the policy maximizing the entropy of the state visitation while rkl-PG produces the optimal state visitation as expected from Theorem 5.1. This simple experiment clearly illustrates two things: (1) f-PG can generate dense signals to explore the state space and search for the goal and (2) although discriminator-based methods like GAIL try to perform state-matching, they fail to explore the space well.", "images": [{"name": "page-8-2.jpg", "height": 88, "width": 88, "x": 313, "y": 415}, {"name": "page-8-3.jpg", "height": 88, "width": 88, "x": 416, "y": 415}, {"name": "page-8-1.jpg", "height": 88, "width": 88, "x": 210, "y": 415}, {"name": "page-8-0.jpg", "height": 88, "width": 88, "x": 108, "y": 415}], "items": [{"type": "heading", "lvl": 1, "value": "Experiments", "md": "# Experiments"}, {"type": "heading", "lvl": 2, "value": "Experiments", "md": "## Experiments"}, {"type": "text", "value": "Our experiments evaluate our new framework (f-PG) as an alternative to conventional reward maximization for goal-conditional RL. We pose the following questions:\n\n1. Does f-PG provide sufficient signals to explore in otherwise challenging sparse reward settings?\n2. How well does our framework perform compared to discriminator-based approaches?\n3. Can our framework scale to larger domains with continuous state spaces and randomly generated goals?\n4. How do different f-divergences affect learning?\n\nThe first two questions are answered using a toy gridworld environment. The gridworld has a goal contained in a room which poses a significant exploration challenge. We also show how the dense signal to the gradients of the policy evolves during training on a continuous domain like Reacher. To answer the third question, our framework is compared with several baselines on a 2D Maze solving task (Point Maze). Additionally, we scale to more complex tasks such as FetchReach Plappert et al. (2018) and an exploration-heavy PointMaze.", "md": "Our experiments evaluate our new framework (f-PG) as an alternative to conventional reward maximization for goal-conditional RL. We pose the following questions:\n\n1. Does f-PG provide sufficient signals to explore in otherwise challenging sparse reward settings?\n2. How well does our framework perform compared to discriminator-based approaches?\n3. Can our framework scale to larger domains with continuous state spaces and randomly generated goals?\n4. How do different f-divergences affect learning?\n\nThe first two questions are answered using a toy gridworld environment. The gridworld has a goal contained in a room which poses a significant exploration challenge. We also show how the dense signal to the gradients of the policy evolves during training on a continuous domain like Reacher. To answer the third question, our framework is compared with several baselines on a 2D Maze solving task (Point Maze). Additionally, we scale to more complex tasks such as FetchReach Plappert et al. (2018) and an exploration-heavy PointMaze."}, {"type": "heading", "lvl": 3, "value": "Gridworld", "md": "### Gridworld"}, {"type": "text", "value": "We use a gridworld environment to compare and visualize the effects of using different shaping rewards for exploration. We discussed this environment briefly in Section 5.1. The task is for the agent to reach the goal contained in a room. The only way to reach the goal is to go around the wall. The task reward is 1 when the agent reaches the room otherwise it is 0. The state space is simply the (x, y) coordinates of the grid and the goal is fixed. A detailed description of the task is provided in Appendix C. Although the environment seems simple, exploration here is very difficult as there is no incentive for the agent to go around the wall.\n\n(a) fkl-PG &emsp; (b) rkl-PG &emsp; (c) AIM &emsp; (d) GAIL\n\nOur framework is compared against AIM (Durugkar et al., 2021), which initially introduced this environment and uses a shaping reward obtained from state-matching to solve it, and GAIL (Ho & Ermon, 2016), which uses a discriminator to learn the probability of a state being the goal state. We provide a comparison to other recent methods in Appendix C. All the baselines are implemented on top of Soft Q Learning (Haarnoja et al., 2017) which along with maximizing the augmented rewards, also maximizes the entropy of the policy while f-PG is implemented as an on-policy algorithm without any extrinsic entropy maximization objective. It can be seen from the image that, f-PG can explore enough to find the way around the room which is difficult for methods like GAIL even after the entropy boost. AIM learns a potential function and can also find its way across the wall. As expected, fkl-PG converges to the policy maximizing the entropy of the state visitation while rkl-PG produces the optimal state visitation as expected from Theorem 5.1. This simple experiment clearly illustrates two things: (1) f-PG can generate dense signals to explore the state space and search for the goal and (2) although discriminator-based methods like GAIL try to perform state-matching, they fail to explore the space well.", "md": "We use a gridworld environment to compare and visualize the effects of using different shaping rewards for exploration. We discussed this environment briefly in Section 5.1. The task is for the agent to reach the goal contained in a room. The only way to reach the goal is to go around the wall. The task reward is 1 when the agent reaches the room otherwise it is 0. The state space is simply the (x, y) coordinates of the grid and the goal is fixed. A detailed description of the task is provided in Appendix C. Although the environment seems simple, exploration here is very difficult as there is no incentive for the agent to go around the wall.\n\n(a) fkl-PG &emsp; (b) rkl-PG &emsp; (c) AIM &emsp; (d) GAIL\n\nOur framework is compared against AIM (Durugkar et al., 2021), which initially introduced this environment and uses a shaping reward obtained from state-matching to solve it, and GAIL (Ho & Ermon, 2016), which uses a discriminator to learn the probability of a state being the goal state. We provide a comparison to other recent methods in Appendix C. All the baselines are implemented on top of Soft Q Learning (Haarnoja et al., 2017) which along with maximizing the augmented rewards, also maximizes the entropy of the policy while f-PG is implemented as an on-policy algorithm without any extrinsic entropy maximization objective. It can be seen from the image that, f-PG can explore enough to find the way around the room which is difficult for methods like GAIL even after the entropy boost. AIM learns a potential function and can also find its way across the wall. As expected, fkl-PG converges to the policy maximizing the entropy of the state visitation while rkl-PG produces the optimal state visitation as expected from Theorem 5.1. This simple experiment clearly illustrates two things: (1) f-PG can generate dense signals to explore the state space and search for the goal and (2) although discriminator-based methods like GAIL try to perform state-matching, they fail to explore the space well."}]}, {"page": 9, "text": "6.2   Point Maze\nWhile the gridworld poses an exploration challenge, the environment is simple and has only one\ngoal. This experiment shows that f-PG scales to larger domains with continuous state space and a\nlarge set of goals. We use the Point Maze environments (Fu et al., 2020) which are a set of offl       ine\nRL environments, and modify it to support our online algorithms. The state space is continuous and\nconsists of the position and velocity of the agent and the goal. The action is the force applied in each\ndirection. There are three variations of the environment namely PointMazeU, PointMazeMedium,\nPointMazeLarge. For the details of the three environments, please refer to Appendix E.\nWe compare f-PG with several goal-based shaping reward, (used alongside the task reward as\ndescribed in Ng et al. (1999)) to optimize a PPO policy1. The rewards tried (along with their\nabbreviations in the plots) are AIM (Durugkar et al., 2021)(aim), GAIL (Ho & Ermon, 2016)(gail),\nAIRL (Fu et al., 2017)(airl) and F-AIRL (Ghasemipour et al., 2019)(fairl). All these methods\nemploy a state-matching objective. AIM uses Wasserstein\u2019s distance while the rest use some form of\nf-divergence. But, all of them rely on discriminators. Along with these baselines, we experiment\nusing our learning signal as a shaping reward (fkl-rew). Additionally, we also compare with PPO\nbeing optimized by only the task reward (none). For our method, we have only shown results for\nfkl-PG. For the rest of the possible f-divergences, refer to Section 6.4.\n                                            Fainth nzckrdiun                                        Ipg\n                                                                                                    Ilrer\nFigure 4: Success rates (averaged over 100 episodes and 3 seeds) of fkl-PG and all the baselines. fkl-\nPG performs well in all three environments and better than the baseline shaping rewards in the two tougher\nenvironments.\nFigure 4 (plotting mean and std-dev for 3 seeds) clearly illustrates that fkl-PG is able to perform\nwell in all three environments. In fact, it performs better than the baselines in the more difficult\nenvironments. It can also be seen that shaping rewards can often lead to suboptimal performance as\nnone is higher than a few of the shaping rewards. As expected, the curve fkl-new performs poorly. In\nthe simpler PointMazeU environment, the performance for most of the shaping rewards are similar\n(along with none) but in more complex PointMazeMedium and PointMazeLarge, a lot of these\nshaping rewards fail.\n6.3   Scaling to Complex Tasks\nWe scale our method to more complex tasks such as FetchReach (Plappert et al., 2018) and a difficult\nversion of PointMaze. In the PointMaze environments used in the previous section, distributions from\nwhich the initial state and the goal are sampled, have a significant overlap easing the exploration. We\nmodify these environments to ensure a significant distance between the sampled goal distributions\nand the agent\u2019s state-visitation distribution as shown in Figure 5 (top), making exploration highly\nchallenging. Figure 5 (bottom) shows the comparison of fkl-PG with GAIL (Ho & Ermon, 2016)\nand AIM (Durugkar et al., 2021).\nThe following can be concluded from these experiments: (1) The discriminative-based methods\nheavily depend on coverage assumptions and fail in situations where there is no significant overlap\nbetween the goal distribution and the agent\u2019s state visitation distribution. fkl-PG does not depend on\nany such assumptions. (2) f-PG is considerably more stable than these baselines (as indicated by the\nvariance of these methods).\n    1Using spinning up implementation:\nhttps://spinningup.openai.com/en/latest/_modules/spinup/algos/pytorch/ppo/ppo.html\n                                                     9", "md": "# Point Maze Experiment\n\n## 6.2 Point Maze\n\nWhile the gridworld poses an exploration challenge, the environment is simple and has only one goal. This experiment shows that f-PG scales to larger domains with continuous state space and a large set of goals. We use the Point Maze environments (Fu et al., 2020) which are a set of offline RL environments, and modify it to support our online algorithms. The state space is continuous and consists of the position and velocity of the agent and the goal. The action is the force applied in each direction. There are three variations of the environment namely PointMazeU, PointMazeMedium, PointMazeLarge. For the details of the three environments, please refer to Appendix E.\n\nWe compare f-PG with several goal-based shaping reward, (used alongside the task reward as described in Ng et al. (1999)) to optimize a PPO policy1. The rewards tried (along with their abbreviations in the plots) are AIM (Durugkar et al., 2021)(aim), GAIL (Ho & Ermon, 2016)(gail), AIRL (Fu et al., 2017)(airl) and F-AIRL (Ghasemipour et al., 2019)(fairl). All these methods employ a state-matching objective. AIM uses Wasserstein\u2019s distance while the rest use some form of f-divergence. But, all of them rely on discriminators. Along with these baselines, we experiment using our learning signal as a shaping reward (fkl-rew). Additionally, we also compare with PPO being optimized by only the task reward (none). For our method, we have only shown results for fkl-PG. For the rest of the possible f-divergences, refer to Section 6.4.\n\nFigure 4: Success rates (averaged over 100 episodes and 3 seeds) of fkl-PG and all the baselines. fkl-PG performs well in all three environments and better than the baseline shaping rewards in the two tougher environments.\n\nFigure 4 (plotting mean and std-dev for 3 seeds) clearly illustrates that fkl-PG is able to perform well in all three environments. In fact, it performs better than the baselines in the more difficult environments. It can also be seen that shaping rewards can often lead to suboptimal performance as none is higher than a few of the shaping rewards. As expected, the curve fkl-new performs poorly. In the simpler PointMazeU environment, the performance for most of the shaping rewards are similar (along with none) but in more complex PointMazeMedium and PointMazeLarge, a lot of these shaping rewards fail.\n\n## 6.3 Scaling to Complex Tasks\n\nWe scale our method to more complex tasks such as FetchReach (Plappert et al., 2018) and a difficult version of PointMaze. In the PointMaze environments used in the previous section, distributions from which the initial state and the goal are sampled, have a significant overlap easing the exploration. We modify these environments to ensure a significant distance between the sampled goal distributions and the agent\u2019s state-visitation distribution as shown in Figure 5 (top), making exploration highly challenging. Figure 5 (bottom) shows the comparison of fkl-PG with GAIL (Ho & Ermon, 2016) and AIM (Durugkar et al., 2021).\n\nThe following can be concluded from these experiments: (1) The discriminative-based methods heavily depend on coverage assumptions and fail in situations where there is no significant overlap between the goal distribution and the agent\u2019s state visitation distribution. fkl-PG does not depend on any such assumptions. (2) f-PG is considerably more stable than these baselines (as indicated by the variance of these methods).\n\n1 Using spinning up implementation: PPO Implementation", "images": [{"name": "page-9-0.jpg", "height": 100, "width": 397, "x": 108, "y": 284}], "items": [{"type": "heading", "lvl": 1, "value": "Point Maze Experiment", "md": "# Point Maze Experiment"}, {"type": "heading", "lvl": 2, "value": "6.2 Point Maze", "md": "## 6.2 Point Maze"}, {"type": "text", "value": "While the gridworld poses an exploration challenge, the environment is simple and has only one goal. This experiment shows that f-PG scales to larger domains with continuous state space and a large set of goals. We use the Point Maze environments (Fu et al., 2020) which are a set of offline RL environments, and modify it to support our online algorithms. The state space is continuous and consists of the position and velocity of the agent and the goal. The action is the force applied in each direction. There are three variations of the environment namely PointMazeU, PointMazeMedium, PointMazeLarge. For the details of the three environments, please refer to Appendix E.\n\nWe compare f-PG with several goal-based shaping reward, (used alongside the task reward as described in Ng et al. (1999)) to optimize a PPO policy1. The rewards tried (along with their abbreviations in the plots) are AIM (Durugkar et al., 2021)(aim), GAIL (Ho & Ermon, 2016)(gail), AIRL (Fu et al., 2017)(airl) and F-AIRL (Ghasemipour et al., 2019)(fairl). All these methods employ a state-matching objective. AIM uses Wasserstein\u2019s distance while the rest use some form of f-divergence. But, all of them rely on discriminators. Along with these baselines, we experiment using our learning signal as a shaping reward (fkl-rew). Additionally, we also compare with PPO being optimized by only the task reward (none). For our method, we have only shown results for fkl-PG. For the rest of the possible f-divergences, refer to Section 6.4.\n\nFigure 4: Success rates (averaged over 100 episodes and 3 seeds) of fkl-PG and all the baselines. fkl-PG performs well in all three environments and better than the baseline shaping rewards in the two tougher environments.\n\nFigure 4 (plotting mean and std-dev for 3 seeds) clearly illustrates that fkl-PG is able to perform well in all three environments. In fact, it performs better than the baselines in the more difficult environments. It can also be seen that shaping rewards can often lead to suboptimal performance as none is higher than a few of the shaping rewards. As expected, the curve fkl-new performs poorly. In the simpler PointMazeU environment, the performance for most of the shaping rewards are similar (along with none) but in more complex PointMazeMedium and PointMazeLarge, a lot of these shaping rewards fail.", "md": "While the gridworld poses an exploration challenge, the environment is simple and has only one goal. This experiment shows that f-PG scales to larger domains with continuous state space and a large set of goals. We use the Point Maze environments (Fu et al., 2020) which are a set of offline RL environments, and modify it to support our online algorithms. The state space is continuous and consists of the position and velocity of the agent and the goal. The action is the force applied in each direction. There are three variations of the environment namely PointMazeU, PointMazeMedium, PointMazeLarge. For the details of the three environments, please refer to Appendix E.\n\nWe compare f-PG with several goal-based shaping reward, (used alongside the task reward as described in Ng et al. (1999)) to optimize a PPO policy1. The rewards tried (along with their abbreviations in the plots) are AIM (Durugkar et al., 2021)(aim), GAIL (Ho & Ermon, 2016)(gail), AIRL (Fu et al., 2017)(airl) and F-AIRL (Ghasemipour et al., 2019)(fairl). All these methods employ a state-matching objective. AIM uses Wasserstein\u2019s distance while the rest use some form of f-divergence. But, all of them rely on discriminators. Along with these baselines, we experiment using our learning signal as a shaping reward (fkl-rew). Additionally, we also compare with PPO being optimized by only the task reward (none). For our method, we have only shown results for fkl-PG. For the rest of the possible f-divergences, refer to Section 6.4.\n\nFigure 4: Success rates (averaged over 100 episodes and 3 seeds) of fkl-PG and all the baselines. fkl-PG performs well in all three environments and better than the baseline shaping rewards in the two tougher environments.\n\nFigure 4 (plotting mean and std-dev for 3 seeds) clearly illustrates that fkl-PG is able to perform well in all three environments. In fact, it performs better than the baselines in the more difficult environments. It can also be seen that shaping rewards can often lead to suboptimal performance as none is higher than a few of the shaping rewards. As expected, the curve fkl-new performs poorly. In the simpler PointMazeU environment, the performance for most of the shaping rewards are similar (along with none) but in more complex PointMazeMedium and PointMazeLarge, a lot of these shaping rewards fail."}, {"type": "heading", "lvl": 2, "value": "6.3 Scaling to Complex Tasks", "md": "## 6.3 Scaling to Complex Tasks"}, {"type": "text", "value": "We scale our method to more complex tasks such as FetchReach (Plappert et al., 2018) and a difficult version of PointMaze. In the PointMaze environments used in the previous section, distributions from which the initial state and the goal are sampled, have a significant overlap easing the exploration. We modify these environments to ensure a significant distance between the sampled goal distributions and the agent\u2019s state-visitation distribution as shown in Figure 5 (top), making exploration highly challenging. Figure 5 (bottom) shows the comparison of fkl-PG with GAIL (Ho & Ermon, 2016) and AIM (Durugkar et al., 2021).\n\nThe following can be concluded from these experiments: (1) The discriminative-based methods heavily depend on coverage assumptions and fail in situations where there is no significant overlap between the goal distribution and the agent\u2019s state visitation distribution. fkl-PG does not depend on any such assumptions. (2) f-PG is considerably more stable than these baselines (as indicated by the variance of these methods).\n\n1 Using spinning up implementation: PPO Implementation", "md": "We scale our method to more complex tasks such as FetchReach (Plappert et al., 2018) and a difficult version of PointMaze. In the PointMaze environments used in the previous section, distributions from which the initial state and the goal are sampled, have a significant overlap easing the exploration. We modify these environments to ensure a significant distance between the sampled goal distributions and the agent\u2019s state-visitation distribution as shown in Figure 5 (top), making exploration highly challenging. Figure 5 (bottom) shows the comparison of fkl-PG with GAIL (Ho & Ermon, 2016) and AIM (Durugkar et al., 2021).\n\nThe following can be concluded from these experiments: (1) The discriminative-based methods heavily depend on coverage assumptions and fail in situations where there is no significant overlap between the goal distribution and the agent\u2019s state visitation distribution. fkl-PG does not depend on any such assumptions. (2) f-PG is considerably more stable than these baselines (as indicated by the variance of these methods).\n\n1 Using spinning up implementation: PPO Implementation"}]}, {"page": 10, "text": "            8\n           Pointhazelardellouar           Pointhazehediumoudn                  FetchReach vO\n                                                                                  A\n                Timcstcns                        Timcacns                     Jopnao  200370  Zinnon\nFigure 5: (top): Description of the environments. In the PointMaze environments, the green and red shades\nrepresent the distributions from which the initial state and goal states are sampled. (bottom): Success rates\n(averaged over 100 episodes and 3 seeds) of fkl-PG, GAIL and AIM. fkl-PG outperforms these baselines with\nconsiderably lower variance.\n6.4   Comparing different f-divergences\nWe perform an ablation to compare different f-divergences on their performances on the three Point\nMaze environments. Figure 6 (plotting mean and std-dev for 3 seeds) show that, empirically, fkl-PG\n                                                                                       HointMazclara\n                                                                                          Tdeacur\nFigure 6: Success rates (averaged over 100 episodes and 3 seeds) of f-PG for different f. fkl-PG performs the\nbest followed by \u03c72-PG.\nperforms the best followed by \u03c72-PG. Interestingly, both of these do not guarantee optimal policies\nbut it can be shown from Lemma 5.1 that fkl-PG converges to the policy that along with maximizing\nfor a \u201creward\", maximizes the entropy of the state-visitation. A similar result can be shown for \u03c72 as\nwell (proof in the Appendix A). This result can be explained by the need for exploration in the larger\nmazes, hence learning policies to keep the entropy of the state visitation high.\n7    Discussion\nThis paper derives a novel framework for goal-conditioned RL in the form of an on-policy algorithm\nf-policy gradients which minimizes the f-divergence between the agent\u2019s state visitation and the\ngoal distribution. It proves that for certain f-divergences, we can recover the optimal policy while for\nsome, we obtain a policy maximizing the entropy of the state-visitation. Entropy-regularized policy\noptimization (s-MaxEnt RL) for metric-based shaping rewards can be shown as a special case of\nf-PG where f is fkl. f-PG can provide an exploration bonus when the agent has yet not seen the\ngoal. We demonstrated that f-PG can scale up to complex domains.\nThrough this work, we introduce a new perspective for goal-conditioned RL. By circumventing\nrewards, f-PG can avoid issues that arise with reward misspecification (Knox et al., 2021). There are\n                                                     10", "md": "# Document\n\n## 8\n\nPointhazelardellouar Pointhazehediumoudn FetchReach vO Timcstcns Timcacns Jopnao 200370 Zinnon\n\n(top): Description of the environments. In the PointMaze environments, the green and red shades represent the distributions from which the initial state and goal states are sampled. (bottom): Success rates (averaged over 100 episodes and 3 seeds) of fkl-PG, GAIL and AIM. fkl-PG outperforms these baselines with considerably lower variance.\n\n### 6.4 Comparing different f-divergences\n\nWe perform an ablation to compare different f-divergences on their performances on the three Point Maze environments. Figure 6 (plotting mean and std-dev for 3 seeds) show that, empirically, fkl-PG HointMazclara Tdeacur\n\nSuccess rates (averaged over 100 episodes and 3 seeds) of f-PG for different f. fkl-PG performs the best followed by $$\\chi^2$$-PG.\n\nfkl-PG performs the best followed by $$\\chi^2$$-PG. Interestingly, both of these do not guarantee optimal policies but it can be shown from Lemma 5.1 that fkl-PG converges to the policy that along with maximizing for a \u201creward\", maximizes the entropy of the state-visitation. A similar result can be shown for $$\\chi^2$$ as well (proof in the Appendix A). This result can be explained by the need for exploration in the larger mazes, hence learning policies to keep the entropy of the state visitation high.\n\n### 7 Discussion\n\nThis paper derives a novel framework for goal-conditioned RL in the form of an on-policy algorithm f-policy gradients which minimizes the f-divergence between the agent\u2019s state visitation and the goal distribution. It proves that for certain f-divergences, we can recover the optimal policy while for some, we obtain a policy maximizing the entropy of the state-visitation. Entropy-regularized policy optimization (s-MaxEnt RL) for metric-based shaping rewards can be shown as a special case of f-PG where f is fkl. f-PG can provide an exploration bonus when the agent has yet not seen the goal. We demonstrated that f-PG can scale up to complex domains.\n\nThrough this work, we introduce a new perspective for goal-conditioned RL. By circumventing rewards, f-PG can avoid issues that arise with reward misspecification (Knox et al., 2021).\n\n## 10", "images": [{"name": "page-10-1.jpg", "height": 100, "width": 397, "x": 108, "y": 393}, {"name": "page-10-0.jpg", "height": 218, "width": 397, "x": 108, "y": 71}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "8", "md": "## 8"}, {"type": "text", "value": "Pointhazelardellouar Pointhazehediumoudn FetchReach vO Timcstcns Timcacns Jopnao 200370 Zinnon\n\n(top): Description of the environments. In the PointMaze environments, the green and red shades represent the distributions from which the initial state and goal states are sampled. (bottom): Success rates (averaged over 100 episodes and 3 seeds) of fkl-PG, GAIL and AIM. fkl-PG outperforms these baselines with considerably lower variance.", "md": "Pointhazelardellouar Pointhazehediumoudn FetchReach vO Timcstcns Timcacns Jopnao 200370 Zinnon\n\n(top): Description of the environments. In the PointMaze environments, the green and red shades represent the distributions from which the initial state and goal states are sampled. (bottom): Success rates (averaged over 100 episodes and 3 seeds) of fkl-PG, GAIL and AIM. fkl-PG outperforms these baselines with considerably lower variance."}, {"type": "heading", "lvl": 3, "value": "6.4 Comparing different f-divergences", "md": "### 6.4 Comparing different f-divergences"}, {"type": "text", "value": "We perform an ablation to compare different f-divergences on their performances on the three Point Maze environments. Figure 6 (plotting mean and std-dev for 3 seeds) show that, empirically, fkl-PG HointMazclara Tdeacur\n\nSuccess rates (averaged over 100 episodes and 3 seeds) of f-PG for different f. fkl-PG performs the best followed by $$\\chi^2$$-PG.\n\nfkl-PG performs the best followed by $$\\chi^2$$-PG. Interestingly, both of these do not guarantee optimal policies but it can be shown from Lemma 5.1 that fkl-PG converges to the policy that along with maximizing for a \u201creward\", maximizes the entropy of the state-visitation. A similar result can be shown for $$\\chi^2$$ as well (proof in the Appendix A). This result can be explained by the need for exploration in the larger mazes, hence learning policies to keep the entropy of the state visitation high.", "md": "We perform an ablation to compare different f-divergences on their performances on the three Point Maze environments. Figure 6 (plotting mean and std-dev for 3 seeds) show that, empirically, fkl-PG HointMazclara Tdeacur\n\nSuccess rates (averaged over 100 episodes and 3 seeds) of f-PG for different f. fkl-PG performs the best followed by $$\\chi^2$$-PG.\n\nfkl-PG performs the best followed by $$\\chi^2$$-PG. Interestingly, both of these do not guarantee optimal policies but it can be shown from Lemma 5.1 that fkl-PG converges to the policy that along with maximizing for a \u201creward\", maximizes the entropy of the state-visitation. A similar result can be shown for $$\\chi^2$$ as well (proof in the Appendix A). This result can be explained by the need for exploration in the larger mazes, hence learning policies to keep the entropy of the state visitation high."}, {"type": "heading", "lvl": 3, "value": "7 Discussion", "md": "### 7 Discussion"}, {"type": "text", "value": "This paper derives a novel framework for goal-conditioned RL in the form of an on-policy algorithm f-policy gradients which minimizes the f-divergence between the agent\u2019s state visitation and the goal distribution. It proves that for certain f-divergences, we can recover the optimal policy while for some, we obtain a policy maximizing the entropy of the state-visitation. Entropy-regularized policy optimization (s-MaxEnt RL) for metric-based shaping rewards can be shown as a special case of f-PG where f is fkl. f-PG can provide an exploration bonus when the agent has yet not seen the goal. We demonstrated that f-PG can scale up to complex domains.\n\nThrough this work, we introduce a new perspective for goal-conditioned RL. By circumventing rewards, f-PG can avoid issues that arise with reward misspecification (Knox et al., 2021).", "md": "This paper derives a novel framework for goal-conditioned RL in the form of an on-policy algorithm f-policy gradients which minimizes the f-divergence between the agent\u2019s state visitation and the goal distribution. It proves that for certain f-divergences, we can recover the optimal policy while for some, we obtain a policy maximizing the entropy of the state-visitation. Entropy-regularized policy optimization (s-MaxEnt RL) for metric-based shaping rewards can be shown as a special case of f-PG where f is fkl. f-PG can provide an exploration bonus when the agent has yet not seen the goal. We demonstrated that f-PG can scale up to complex domains.\n\nThrough this work, we introduce a new perspective for goal-conditioned RL. By circumventing rewards, f-PG can avoid issues that arise with reward misspecification (Knox et al., 2021)."}, {"type": "heading", "lvl": 2, "value": "10", "md": "## 10"}]}, {"page": 11, "text": "several avenues to focus on for future work. First, the current framework is on-policy and poses an\nexploration challenge. An avenue for future work could be to develop an off-policy way to solve the\nobjective. Second, this paper does not tackle goal distributions with several modes. Such a target\ndistribution would be interesting to tackle in future work.\n8    Acknowledgements\nThis work was in part supported by Cisco Research. Any opinions, findings and conclusions, or\nrecommendations expressed in this material are those of the authors and do not necessarily reflect the\nviews of Cisco Research.\nThis work has partially taken place in the Learning Agents Research Group (LARG) at UT Austin.\nLARG research is supported in part by NSF (FAIN-2019844, NRT-2125858), ONR (N00014-18-\n2243), ARO (E2061621), Bosch, Lockheed Martin, and UT Austin\u2019s Good Systems grand challenge.\nPeter Stone serves as the Executive Director of Sony AI America and receives financial compensation\nfor this work. The terms of this arrangement have been reviewed and approved by the University of\nTexas at Austin in accordance with its policy on objectivity in research.\nReferences\nDavid Abel, Will Dabney, Anna Harutyunyan, Mark K. Ho, Michael L. Littman, Doina Precup,\n   and Satinder Singh. On the expressivity of markov reward. CoRR, abs/2111.00876, 2021. URL\n   https://arxiv.org/abs/2111.00876.\nJose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brand-\n   stetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards, 2019.\nAndrew G. Barto. Intrinsic Motivation and Reinforcement Learning, pp. 17\u201347. Springer Berlin Hei-\n   delberg, Berlin, Heidelberg, 2013. ISBN 978-3-642-32375-1. doi: 10.1007/978-3-642-32375-1_2.\n   URL https://doi.org/10.1007/978-3-642-32375-1_2.\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi\n   Munos. Unifying count-based exploration and intrinsic motivation, 2016.\nSerena Booth, Julie Shah, Scott Niekum, Peter Stone, and Alessandro Allievi. The perils of trial-and-\n   error reward design: misdesign through overfitting and invalid task specifications. 2023.\nJack Clark and Dario Amodei. Faulty reward functions in the wild, 2016. URL https://openai.\n   com/research/faulty-reward-functions.\nIshan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation for\n   reinforcement learning. CoRR, abs/2105.13345, 2021. URL https://arxiv.org/abs/2105.\n   13345.\nIshan Durugkar et al. Estimation and control of visitation distributions for reinforcement learning.\n   PhD thesis, 2023.\nBenjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve\n   goals via recursive classification. CoRR, abs/2011.08909, 2020. URL https://arxiv.org/\n   abs/2011.08909.\nBenjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov. Replacing rewards with examples:\n   Example-based policy search via recursive classification. CoRR, abs/2103.12656, 2021. URL\n   https://arxiv.org/abs/2103.12656.\nAlhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mo-\n   hammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grze-\n   gorz Swirszcz, David Silver, Demis Hassabis, and Pushmeet Kohli. Discovering faster matrix\n   multiplication algorithms with reinforcement learning. Nature, 610(7930):47\u201353, 2022. doi:\n   10.1038/s41586-022-05172-4.\n                                                  11", "md": "# Future Work and Acknowledgements\n\n## Future Work\n\nSeveral avenues to focus on for future work. First, the current framework is on-policy and poses an exploration challenge. An avenue for future work could be to develop an off-policy way to solve the objective. Second, this paper does not tackle goal distributions with several modes. Such a target distribution would be interesting to tackle in future work.\n\n## Acknowledgements\n\nThis work was in part supported by Cisco Research. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Cisco Research.\n\nThis work has partially taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (FAIN-2019844, NRT-2125858), ONR (N00014-18-2243), ARO (E2061621), Bosch, Lockheed Martin, and UT Austin\u2019s Good Systems grand challenge.\n\nPeter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.\n\n## References\n\n- David Abel, Will Dabney, Anna Harutyunyan, Mark K. Ho, Michael L. Littman, Doina Precup, and Satinder Singh. On the expressivity of Markov reward. CoRR, abs/2111.00876, 2021. URL https://arxiv.org/abs/2111.00876.\n- Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards, 2019.\n- Andrew G. Barto. Intrinsic Motivation and Reinforcement Learning, pp. 17\u201347. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. ISBN 978-3-642-32375-1. doi: 10.1007/978-3-642-32375-1_2. URL https://doi.org/10.1007/978-3-642-32375-1_2.\n- Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation, 2016.\n- Serena Booth, Julie Shah, Scott Niekum, Peter Stone, and Alessandro Allievi. The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications, 2023.\n- Jack Clark and Dario Amodei. Faulty reward functions in the wild, 2016. URL https://openai.com/research/faulty-reward-functions.\n- Ishan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation for reinforcement learning. CoRR, abs/2105.13345, 2021. URL https://arxiv.org/abs/2105.13345.\n- Ishan Durugkar et al. Estimation and control of visitation distributions for reinforcement learning. PhD thesis, 2023.\n- Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve goals via recursive classification. CoRR, abs/2011.08909, 2020. URL https://arxiv.org/abs/2011.08909.\n- Benjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov. Replacing rewards with examples: Example-based policy search via recursive classification. CoRR, abs/2103.12656, 2021. URL https://arxiv.org/abs/2103.12656.\n- Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis, and Pushmeet Kohli. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47\u201353, 2022. doi: 10.1038/s41586-022-05172-4.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Future Work and Acknowledgements", "md": "# Future Work and Acknowledgements"}, {"type": "heading", "lvl": 2, "value": "Future Work", "md": "## Future Work"}, {"type": "text", "value": "Several avenues to focus on for future work. First, the current framework is on-policy and poses an exploration challenge. An avenue for future work could be to develop an off-policy way to solve the objective. Second, this paper does not tackle goal distributions with several modes. Such a target distribution would be interesting to tackle in future work.", "md": "Several avenues to focus on for future work. First, the current framework is on-policy and poses an exploration challenge. An avenue for future work could be to develop an off-policy way to solve the objective. Second, this paper does not tackle goal distributions with several modes. Such a target distribution would be interesting to tackle in future work."}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "This work was in part supported by Cisco Research. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Cisco Research.\n\nThis work has partially taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (FAIN-2019844, NRT-2125858), ONR (N00014-18-2243), ARO (E2061621), Bosch, Lockheed Martin, and UT Austin\u2019s Good Systems grand challenge.\n\nPeter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.", "md": "This work was in part supported by Cisco Research. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Cisco Research.\n\nThis work has partially taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (FAIN-2019844, NRT-2125858), ONR (N00014-18-2243), ARO (E2061621), Bosch, Lockheed Martin, and UT Austin\u2019s Good Systems grand challenge.\n\nPeter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- David Abel, Will Dabney, Anna Harutyunyan, Mark K. Ho, Michael L. Littman, Doina Precup, and Satinder Singh. On the expressivity of Markov reward. CoRR, abs/2111.00876, 2021. URL https://arxiv.org/abs/2111.00876.\n- Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards, 2019.\n- Andrew G. Barto. Intrinsic Motivation and Reinforcement Learning, pp. 17\u201347. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. ISBN 978-3-642-32375-1. doi: 10.1007/978-3-642-32375-1_2. URL https://doi.org/10.1007/978-3-642-32375-1_2.\n- Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation, 2016.\n- Serena Booth, Julie Shah, Scott Niekum, Peter Stone, and Alessandro Allievi. The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications, 2023.\n- Jack Clark and Dario Amodei. Faulty reward functions in the wild, 2016. URL https://openai.com/research/faulty-reward-functions.\n- Ishan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation for reinforcement learning. CoRR, abs/2105.13345, 2021. URL https://arxiv.org/abs/2105.13345.\n- Ishan Durugkar et al. Estimation and control of visitation distributions for reinforcement learning. PhD thesis, 2023.\n- Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve goals via recursive classification. CoRR, abs/2011.08909, 2020. URL https://arxiv.org/abs/2011.08909.\n- Benjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov. Replacing rewards with examples: Example-based policy search via recursive classification. CoRR, abs/2103.12656, 2021. URL https://arxiv.org/abs/2103.12656.\n- Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis, and Pushmeet Kohli. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47\u201353, 2022. doi: 10.1038/s41586-022-05172-4.", "md": "- David Abel, Will Dabney, Anna Harutyunyan, Mark K. Ho, Michael L. Littman, Doina Precup, and Satinder Singh. On the expressivity of Markov reward. CoRR, abs/2111.00876, 2021. URL https://arxiv.org/abs/2111.00876.\n- Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards, 2019.\n- Andrew G. Barto. Intrinsic Motivation and Reinforcement Learning, pp. 17\u201347. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. ISBN 978-3-642-32375-1. doi: 10.1007/978-3-642-32375-1_2. URL https://doi.org/10.1007/978-3-642-32375-1_2.\n- Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation, 2016.\n- Serena Booth, Julie Shah, Scott Niekum, Peter Stone, and Alessandro Allievi. The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications, 2023.\n- Jack Clark and Dario Amodei. Faulty reward functions in the wild, 2016. URL https://openai.com/research/faulty-reward-functions.\n- Ishan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone. Adversarial intrinsic motivation for reinforcement learning. CoRR, abs/2105.13345, 2021. URL https://arxiv.org/abs/2105.13345.\n- Ishan Durugkar et al. Estimation and control of visitation distributions for reinforcement learning. PhD thesis, 2023.\n- Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve goals via recursive classification. CoRR, abs/2011.08909, 2020. URL https://arxiv.org/abs/2011.08909.\n- Benjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov. Replacing rewards with examples: Example-based policy search via recursive classification. CoRR, abs/2103.12656, 2021. URL https://arxiv.org/abs/2103.12656.\n- Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis, and Pushmeet Kohli. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47\u201353, 2022. doi: 10.1038/s41586-022-05172-4."}]}, {"page": 12, "text": "Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-\n  ment learning. CoRR, abs/1710.11248, 2017. URL http://arxiv.org/abs/1710.11248.\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep\n  data-driven reinforcement learning. CoRR, abs/2004.07219, 2020. URL https://arxiv.org/\n  abs/2004.07219.\nTheophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, and Devendra Singh Chaplot. Nav-\n  igating to objects in the real world. Science Robotics, 8(79):eadf6991, 2023. doi: 10.1126/\n  scirobotics.adf6991.    URL https://www.science.org/doi/abs/10.1126/scirobotics.\n  adf6991.\nSeyed Kamyar Seyed Ghasemipour, Richard S. Zemel, and Shixiang Gu. A divergence minimization\n  perspective on imitation learning methods. CoRR, abs/1911.02256, 2019. URL http://arxiv.\n  org/abs/1911.02256.\nPrasoon Goyal, Scott Niekum, and Raymond J. Mooney. Using natural language for reward shaping\n  in reinforcement learning. CoRR, abs/1903.02020, 2019. URL http://arxiv.org/abs/1903.\n  02020.\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learn-\n  ing: Solving long-horizon tasks via imitation and reinforcement learning. CoRR, abs/1910.11956,\n  2019. URL http://arxiv.org/abs/1910.11956.\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with\n  deep energy-based policies. CoRR, abs/1702.08165, 2017. URL http://arxiv.org/abs/1702.\n  08165.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\n  maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290,\n  2018. URL http://arxiv.org/abs/1801.01290.\nElad Hazan, Sham M. Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum\n  entropy exploration. CoRR, abs/1812.02690, 2018. URL http://arxiv.org/abs/1812.02690.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. CoRR, abs/1606.03476,\n  2016. URL http://arxiv.org/abs/1606.03476.\nRodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines\n  for high-level task specification and decomposition in reinforcement learning. In Jennifer Dy and\n  Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,\n  volume 80 of Proceedings of Machine Learning Research, pp. 2107\u20132116. PMLR, 10\u201315 Jul 2018.\n  URL https://proceedings.mlr.press/v80/icarte18a.html.\nRodrigo Toro Icarte, Ethan Waldie, Toryn Q. Klassen, Richard Anthony Valenzano, Margarita P.\n  Castro, and Sheila A. McIlraith. Learning reward machines: A study in partially observable\n  reinforcement learning. CoRR, abs/2112.09477, 2021. URL https://arxiv.org/abs/2112.\n  09477.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\n  Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, Alex Bridgland,\n  Clemens Meyer, Simon Kohl, Andrew Ballard, Andrew Cowie, Bernardino Romera-Paredes,\n  Stanislav Nikolov, Rishub Jain, Jonas Adler, and Demis Hassabis. Highly accurate protein struc-\n  ture prediction with alphafold. Nature, 596:1\u201311, 08 2021. doi: 10.1038/s41586-021-03819-2.\nLeslie Pack Kaelbling. Learning to achieve goals. In IJCAI, pp. 1094\u20131099. Citeseer, 1993.\nHilbert J. Kappen, Vicen\u00e7 G\u00f3mez, and Manfred Opper. Optimal control as a graphical model inference\n  problem. Machine Learning, 87(2):159\u2013182, feb 2012. doi: 10.1007/s10994-012-5278-7. URL\n  https://doi.org/10.1007%2Fs10994-012-5278-7.\nLiyiming Ke, Matt Barnes, Wen Sun, Gilwoo Lee, Sanjiban Choudhury, and Siddhartha S. Srinivasa.\n  Imitation learning as f-divergence minimization. CoRR, abs/1905.12888, 2019. URL http:\n  //arxiv.org/abs/1905.12888.\n                                                12", "md": "The provided text is too lengthy to be converted into a single HTML document. Please provide a shorter text or specific sections that you would like to be converted into HTML.", "images": [], "items": [{"type": "text", "value": "The provided text is too lengthy to be converted into a single HTML document. Please provide a shorter text or specific sections that you would like to be converted into HTML.", "md": "The provided text is too lengthy to be converted into a single HTML document. Please provide a shorter text or specific sections that you would like to be converted into HTML."}]}, {"page": 13, "text": "Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Robot peels banana with goal-conditioned\n  dual-action deep imitation learning, 2022.\nW. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward\n  (mis)design for autonomous driving. CoRR, abs/2104.13906, 2021. URL https://arxiv.org/\n  abs/2104.13906.\nSergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.\n  CoRR, abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909.\nXiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. FinRL. In Proceedings\n  of the Second ACM International Conference on AI in Finance. ACM, nov 2021. doi: 10.1145/\n  3490354.3494366. URL https://doi.org/10.1145%2F3490354.3494366.\nYecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i\u2019ll go: Offl         ine\n  goal-conditioned reinforcement learning via f-advantage regression, 2022. URL https://arxiv.\n  org/abs/2206.03023.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\n  Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,\n  abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of\n  discounted stationary distribution corrections. ArXiv, abs/1906.04733, 2019.\nA. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory\n  and application to reward shaping. In International Conference on Machine Learning, 1999.\nTianwei Ni, Harshit S. Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Benjamin Eysenbach. f-irl:\n  Inverse reinforcement learning via state marginal matching. CoRR, abs/2011.04709, 2020. URL\n  https://arxiv.org/abs/2011.04709.\nScott Niekum. Evolved intrinsic reward functions for reinforcement learning. Proceedings of the AAAI\n  Conference on Artificial Intelligence, 24(1):1955\u20131956, Jul. 2010. doi: 10.1609/aaai.v24i1.7772.\n  URL https://ojs.aaai.org/index.php/AAAI/article/view/7772.\nOpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder,\n  Ruben D\u2019Sa, Arthur Petron, Henrique Pond\u00e9 de Oliveira Pinto, Alex Paino, Hyeonwoo Noh,\n  Lilian Weng, Qiming Yuan, Casey Chu, and Wojciech Zaremba.             Asymmetric self-play for\n  automatic goal discovery in robotic manipulation. CoRR, abs/2101.04882, 2021. URL https:\n  //arxiv.org/abs/2101.04882.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\n  Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\n  Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\n  Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\nMatthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,\n  Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech\n  Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for\n  research, 2018.\nYury Polyanskiy and Yihong Wu.         \"Information Theory From Coding to Learning\".          Cam-\n  bridge University Press, 2022. URL https://people.lids.mit.edu/yp/homepage/data/\n  itbook-export.pdf.\nDoina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department\n  Faculty Publication Series, pp. 80, 2000.\nMartin L Puterman. Markov decision processes. Handbooks in operations research and management\n  science, 2:331\u2013434, 1990.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n  optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.\n  06347.\n                                                 13", "md": "# References\n\n# References\n\n- Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Robot peels banana with goal-conditioned dual-action deep imitation learning, 2022.\n- W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (mis)design for autonomous driving. CoRR, abs/2104.13906, 2021. URL https://arxiv.org/abs/2104.13906.\n- Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. CoRR, abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909.\n- Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. FinRL. In Proceedings of the Second ACM International Conference on AI in Finance. ACM, nov 2021. doi: 10.1145/3490354.3494366. URL https://doi.org/10.1145%2F3490354.3494366.\n- Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i\u2019ll go: Offline goal-conditioned reinforcement learning via f-advantage regression, 2022. URL https://arxiv.org/abs/2206.03023.\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.\n- Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. ArXiv, abs/1906.04733, 2019.\n- A. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, 1999.\n- Tianwei Ni, Harshit S. Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Benjamin Eysenbach. f-irl: Inverse reinforcement learning via state marginal matching. CoRR, abs/2011.04709, 2020. URL https://arxiv.org/abs/2011.04709.\n- Scott Niekum. Evolved intrinsic reward functions for reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 24(1):1955\u20131956, Jul. 2010. doi: 10.1609/aaai.v24i1.7772. URL https://ojs.aaai.org/index.php/AAAI/article/view/7772.\n- OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D\u2019Sa, Arthur Petron, Henrique Pond\u00e9 de Oliveira Pinto, Alex Paino, Hyeonwoo Noh, Lilian Weng, Qiming Yuan, Casey Chu, and Wojciech Zaremba. Asymmetric self-play for automatic goal discovery in robotic manipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n- Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research, 2018.\n- Yury Polyanskiy and Yihong Wu. \"Information Theory From Coding to Learning\". Cambridge University Press, 2022. URL https://people.lids.mit.edu/yp/homepage/data/itbook-export.pdf.\n- Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, pp. 80, 2000.\n- Martin L Puterman. Markov decision processes. Handbooks in operations research and management science, 2:331\u2013434, 1990.\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Robot peels banana with goal-conditioned dual-action deep imitation learning, 2022.\n- W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (mis)design for autonomous driving. CoRR, abs/2104.13906, 2021. URL https://arxiv.org/abs/2104.13906.\n- Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. CoRR, abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909.\n- Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. FinRL. In Proceedings of the Second ACM International Conference on AI in Finance. ACM, nov 2021. doi: 10.1145/3490354.3494366. URL https://doi.org/10.1145%2F3490354.3494366.\n- Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i\u2019ll go: Offline goal-conditioned reinforcement learning via f-advantage regression, 2022. URL https://arxiv.org/abs/2206.03023.\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.\n- Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. ArXiv, abs/1906.04733, 2019.\n- A. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, 1999.\n- Tianwei Ni, Harshit S. Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Benjamin Eysenbach. f-irl: Inverse reinforcement learning via state marginal matching. CoRR, abs/2011.04709, 2020. URL https://arxiv.org/abs/2011.04709.\n- Scott Niekum. Evolved intrinsic reward functions for reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 24(1):1955\u20131956, Jul. 2010. doi: 10.1609/aaai.v24i1.7772. URL https://ojs.aaai.org/index.php/AAAI/article/view/7772.\n- OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D\u2019Sa, Arthur Petron, Henrique Pond\u00e9 de Oliveira Pinto, Alex Paino, Hyeonwoo Noh, Lilian Weng, Qiming Yuan, Casey Chu, and Wojciech Zaremba. Asymmetric self-play for automatic goal discovery in robotic manipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n- Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research, 2018.\n- Yury Polyanskiy and Yihong Wu. \"Information Theory From Coding to Learning\". Cambridge University Press, 2022. URL https://people.lids.mit.edu/yp/homepage/data/itbook-export.pdf.\n- Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, pp. 80, 2000.\n- Martin L Puterman. Markov decision processes. Handbooks in operations research and management science, 2:331\u2013434, 1990.\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.", "md": "- Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Robot peels banana with goal-conditioned dual-action deep imitation learning, 2022.\n- W. Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (mis)design for autonomous driving. CoRR, abs/2104.13906, 2021. URL https://arxiv.org/abs/2104.13906.\n- Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. CoRR, abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909.\n- Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. FinRL. In Proceedings of the Second ACM International Conference on AI in Finance. ACM, nov 2021. doi: 10.1145/3490354.3494366. URL https://doi.org/10.1145%2F3490354.3494366.\n- Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i\u2019ll go: Offline goal-conditioned reinforcement learning via f-advantage regression, 2022. URL https://arxiv.org/abs/2206.03023.\n- Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.\n- Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. ArXiv, abs/1906.04733, 2019.\n- A. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, 1999.\n- Tianwei Ni, Harshit S. Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Benjamin Eysenbach. f-irl: Inverse reinforcement learning via state marginal matching. CoRR, abs/2011.04709, 2020. URL https://arxiv.org/abs/2011.04709.\n- Scott Niekum. Evolved intrinsic reward functions for reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 24(1):1955\u20131956, Jul. 2010. doi: 10.1609/aaai.v24i1.7772. URL https://ojs.aaai.org/index.php/AAAI/article/view/7772.\n- OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D\u2019Sa, Arthur Petron, Henrique Pond\u00e9 de Oliveira Pinto, Alex Paino, Hyeonwoo Noh, Lilian Weng, Qiming Yuan, Casey Chu, and Wojciech Zaremba. Asymmetric self-play for automatic goal discovery in robotic manipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n- Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research, 2018.\n- Yury Polyanskiy and Yihong Wu. \"Information Theory From Coding to Learning\". Cambridge University Press, 2022. URL https://people.lids.mit.edu/yp/homepage/data/itbook-export.pdf.\n- Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, pp. 80, 2000.\n- Martin L Puterman. Markov decision processes. Handbooks in operations research and management science, 2:331\u2013434, 1990.\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347."}]}, {"page": 14, "text": "Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, and Sergey Levine. Ving:\n  Learning open-world navigation with visual goals. CoRR, abs/2012.09812, 2020. URL https:\n  //arxiv.org/abs/2012.09812.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\n  Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap,\n  Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general\n  reinforcement learning algorithm. CoRR, abs/1712.01815, 2017. URL http://arxiv.org/abs/\n  1712.01815.\nDavid Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. Artificial\n  Intelligence, 299:103535, 2021. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2021.103535.\n  URL https://www.sciencedirect.com/science/article/pii/S0004370221000862.\nSatinder Singh, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. Intrinsically motivated\n  reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental\n  Development, 2(2):70\u201382, 2010. doi: 10.1109/TAMD.2010.2051031.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\n  In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033.\n  IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\nRonald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning\n  algorithms. Connection Science, 3(3):241\u2013268, 1991.\nPeter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,\n  Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.\n  Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):\n  223\u2013228, 2022.\nZeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient\n  methods. CoRR, abs/1804.06459, 2018. URL http://arxiv.org/abs/1804.06459.\nBrian D. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal\n  entropy. 2010.\nBrian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse\n  reinforcement learning. In Dieter Fox and Carla P. Gomes (eds.), AAAI, pp. 1433\u20131438. AAAI\n  Press, 2008. ISBN 978-1-57735-368-3. URL http://dblp.uni-trier.de/db/conf/aaai/\n  aaai2008.html#ZiebartMBD08.\n                                                 14", "md": "# References\n\n# References\n\nDhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, and Sergey Levine. Ving: Learning open-world navigation with visual goals. CoRR, abs/2012.09812, 2020. URL https://arxiv.org/abs/2012.09812.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\n\nDavid Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. Artificial Intelligence, 299:103535, 2021. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2021.103535. URL https://www.sciencedirect.com/science/article/pii/S0004370221000862.\n\nSatinder Singh, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70\u201382, 2010. doi: 10.1109/TAMD.2010.2051031.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\nRonald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991.\n\nPeter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896): 223\u2013228, 2022.\n\nZeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient methods. CoRR, abs/1804.06459, 2018. URL http://arxiv.org/abs/1804.06459.\n\nBrian D. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. 2010.\n\nBrian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Dieter Fox and Carla P. Gomes (eds.), AAAI, pp. 1433\u20131438. AAAI Press, 2008. ISBN 978-1-57735-368-3. URL http://dblp.uni-trier.de/db/conf/aaai/aaai2008.html#ZiebartMBD08.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, and Sergey Levine. Ving: Learning open-world navigation with visual goals. CoRR, abs/2012.09812, 2020. URL https://arxiv.org/abs/2012.09812.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\n\nDavid Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. Artificial Intelligence, 299:103535, 2021. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2021.103535. URL https://www.sciencedirect.com/science/article/pii/S0004370221000862.\n\nSatinder Singh, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70\u201382, 2010. doi: 10.1109/TAMD.2010.2051031.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\nRonald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991.\n\nPeter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896): 223\u2013228, 2022.\n\nZeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient methods. CoRR, abs/1804.06459, 2018. URL http://arxiv.org/abs/1804.06459.\n\nBrian D. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. 2010.\n\nBrian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Dieter Fox and Carla P. Gomes (eds.), AAAI, pp. 1433\u20131438. AAAI Press, 2008. ISBN 978-1-57735-368-3. URL http://dblp.uni-trier.de/db/conf/aaai/aaai2008.html#ZiebartMBD08.", "md": "Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, and Sergey Levine. Ving: Learning open-world navigation with visual goals. CoRR, abs/2012.09812, 2020. URL https://arxiv.org/abs/2012.09812.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\n\nDavid Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. Artificial Intelligence, 299:103535, 2021. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2021.103535. URL https://www.sciencedirect.com/science/article/pii/S0004370221000862.\n\nSatinder Singh, Richard L. Lewis, Andrew G. Barto, and Jonathan Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70\u201382, 2010. doi: 10.1109/TAMD.2010.2051031.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\nRonald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241\u2013268, 1991.\n\nPeter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896): 223\u2013228, 2022.\n\nZeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient methods. CoRR, abs/1804.06459, 2018. URL http://arxiv.org/abs/1804.06459.\n\nBrian D. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. 2010.\n\nBrian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Dieter Fox and Carla P. Gomes (eds.), AAAI, pp. 1433\u20131438. AAAI Press, 2008. ISBN 978-1-57735-368-3. URL http://dblp.uni-trier.de/db/conf/aaai/aaai2008.html#ZiebartMBD08."}]}, {"page": 15, "text": "Appendix\nA Analysis of J(\u03b8)                                                                                   16\n    A.1   Proof for Theorem 5.1     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  16\n    A.2   Proof for Lemma 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      17\nB   Gradient based optimization                                                                      18\n    B.1   Derivation of gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    18\n    B.2   Practical Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    20\n    B.3   Discounted State-Visitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     20\nC Gridworld Experiments                                                                              21\n    C.1   Description of the task   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21\n    C.2   Performance of f-PG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      21\nD Visualizing the learning signals                                                                   21\n    D.1   Description of the task   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  21\n    D.2   Comparing different f-PG      . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  22\nE   PointMaze experiments                                                                            22\n                                                    15", "md": "# Appendix\n\n## Appendix\n\n### A Analysis of J(\u03b8)\n\n16\n\n- A.1 Proof for Theorem 5.1\n- A.2 Proof for Lemma 5.1\n\n### B Gradient based optimization\n\n18\n\n- B.1 Derivation of gradients\n- B.2 Practical Algorithm\n- B.3 Discounted State-Visitations\n\n### C Gridworld Experiments\n\n21\n\n- C.1 Description of the task\n- C.2 Performance of f-PG\n\n### D Visualizing the learning signals\n\n21\n\n- D.1 Description of the task\n- D.2 Comparing different f-PG\n\n### E PointMaze experiments\n\n22\n\n15", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Appendix", "md": "# Appendix"}, {"type": "heading", "lvl": 2, "value": "Appendix", "md": "## Appendix"}, {"type": "heading", "lvl": 3, "value": "A Analysis of J(\u03b8)", "md": "### A Analysis of J(\u03b8)"}, {"type": "text", "value": "16\n\n- A.1 Proof for Theorem 5.1\n- A.2 Proof for Lemma 5.1", "md": "16\n\n- A.1 Proof for Theorem 5.1\n- A.2 Proof for Lemma 5.1"}, {"type": "heading", "lvl": 3, "value": "B Gradient based optimization", "md": "### B Gradient based optimization"}, {"type": "text", "value": "18\n\n- B.1 Derivation of gradients\n- B.2 Practical Algorithm\n- B.3 Discounted State-Visitations", "md": "18\n\n- B.1 Derivation of gradients\n- B.2 Practical Algorithm\n- B.3 Discounted State-Visitations"}, {"type": "heading", "lvl": 3, "value": "C Gridworld Experiments", "md": "### C Gridworld Experiments"}, {"type": "text", "value": "21\n\n- C.1 Description of the task\n- C.2 Performance of f-PG", "md": "21\n\n- C.1 Description of the task\n- C.2 Performance of f-PG"}, {"type": "heading", "lvl": 3, "value": "D Visualizing the learning signals", "md": "### D Visualizing the learning signals"}, {"type": "text", "value": "21\n\n- D.1 Description of the task\n- D.2 Comparing different f-PG", "md": "21\n\n- D.1 Description of the task\n- D.2 Comparing different f-PG"}, {"type": "heading", "lvl": 3, "value": "E PointMaze experiments", "md": "### E PointMaze experiments"}, {"type": "text", "value": "22\n\n15", "md": "22\n\n15"}]}, {"page": 16, "text": "A     Analysis of J(\u03b8)\nIn this section, we will present the proofs for all the Lemmas and Theorems stated in Section 5.1.\nA.1    Proof for Theorem 5.1\nTo prove Theorem 5.1, we need the following Lemmas. Lemma A.1 states that among all policies,\nthe optimal policy has the highest state visitation at the goal.\nLemma A.1. Let D be the set of all possible state visitations for the agent following some policy \u03c0 \u2208\n\u03a0. Let \u03c0\u2217  be the optimal goal-conditioned policy. This optimal policy\u2019s state-visitation distribution\nwill have the most measure at the goal for all p\u03c0 \u2208    D i.e., \u03c0\u2217 =\u21d2    p\u03c0\u2217(g) \u2265   p\u03c0(g), \u2200 p\u03c0 \u2208  D.\nProof. Let \u03c0\u2217   be the optimal policy and p\u03c0\u2217   be the corresponding state visitation distribution. The\nreward for the sparse setting is designed as,\n                                        r(s) =   1    s = g,\n                                                  0   otherwise.\nHence the expected return for a policy \u03c0 is R\u03c0 is\n                                            R\u03c0 = Ep   \u03c0[r(s)]\n                                                = p\u03c0(g).\nThe return for the optimal policy is maximum among all policies so R\u03c0\u2217            \u2265  R\u03c0, \u2200\u03c0 \u2208    \u03a0. This\nimplies p\u03c0\u2217(g) \u2265   p\u03c0(g), \u2200p\u03c0 \u2208   D.\nLemma A.2 states that the f-divergence between p\u03c0(s) and pg(s) is a decreasing function with respect\nto p\u03c0(s). This means that as the objective J(\u03b8) obtains its minimum value when p\u03c0(g) is highest.\nLemma A.2. Df(p\u03c0(\u00b7)||pg(\u00b7)) is a decreasing function with respect p\u03c0(g)\u2200f if f \u2032(\u221e) is defined.\nProof. The goal distribution is assumed to be a Dirac distribution i.e., pg(s) = 1 if s = g and 0\neverywhere else. The f-divergence between the agent state-visitation distribution, p\u03c0 and the goal\ndistribution, pg can be defined as,\n                     Df(p\u03c0||pg) =           pg(s)f  p\u03c0(s)      + f \u2032(\u221e)p\u03c0[pg = 0]\n                                     pg>0            pg(s)\n                                  = f(p\u03c0(g)) + f \u2032(\u221e)(1 \u2212      p\u03c0(g)).\nLet F = Df(p\u03c0||pg). Differentiating F w.r.t. p\u03c0(g), we get F\u2032 = f \u2032(p\u03c0(g)) \u2212         f \u2032(\u221e). Since f is a\nconvex function (by the definition of f-divergence), f \u2032(x) \u2264    f \u2032(y), \u2200x \u2264  y.\nHence, if f \u2032(\u221e) is defi ned, F\u2032 \u2264   0. Hence F = Df(p\u03c0||pg) is a decreasing function with respect\np\u03c0(g).\nAdditionally, we need Lemma A.3 and Corollary 1 to complete the proof of Theorem 5.1.\nLemma A.3. If any two policies \u03c01 and \u03c02 have the same state visitation at a given goal, they have\nthe same returns for that goal.\nProof. Follows directly from the definition of returns. R\u03c0 = Ep    \u03c0[r(s)] = p\u03c0(g). Hence two policies\n\u03c01 and \u03c02 with the same state visitation at the goal will have the same returns.\nCorollary 1. Any policy that can lead to the state-visitation distribution of the optimal policy p\u03c0\u2217    is\noptimal.\nProof. Directly follows from Lemma A.3.\nTheorem 5.1. The policy that minimizes Df(p\u03c0||pg) for a convex function f with f(1) = 0 and\nf \u2032(\u221e) being defined, is the optimal policy.\n                                                   16", "md": "# Analysis of J(\u03b8)\n\n## Analysis of J(\u03b8)\n\nIn this section, we will present the proofs for all the Lemmas and Theorems stated in Section 5.1.\n\n### Proof for Theorem 5.1\n\nTo prove Theorem 5.1, we need the following Lemmas. Lemma A.1 states that among all policies, the optimal policy has the highest state visitation at the goal.\n\nLemma A.1: Let D be the set of all possible state visitations for the agent following some policy \u03c0 \u2208 \u03a0. Let \u03c0* be the optimal goal-conditioned policy. This optimal policy\u2019s state-visitation distribution will have the most measure at the goal for all p\u03c0 \u2208 D i.e., \u03c0* \u21d2 p\u03c0*(g) \u2265 p\u03c0(g), \u2200 p\u03c0 \u2208 D.\n\nProof: Let \u03c0* be the optimal policy and p\u03c0* be the corresponding state visitation distribution. The reward for the sparse setting is designed as,\n\n$$r(s) = \\begin{cases} 1 & \\text{if } s = g, \\\\ 0 & \\text{otherwise.} \\end{cases}$$\n\nHence the expected return for a policy \u03c0 is R\u03c0 is\n\n$$R\u03c0 = E_{p_\u03c0}[r(s)] = p_\u03c0(g).$$\n\nThe return for the optimal policy is maximum among all policies so R\u03c0* \u2265 R\u03c0, \u2200\u03c0 \u2208 \u03a0. This implies p\u03c0*(g) \u2265 p\u03c0(g), \u2200p\u03c0 \u2208 D.\n\nLemma A.2: Df(p\u03c0(\u00b7)||pg(\u00b7)) is a decreasing function with respect to p\u03c0(g) \u2200f if f'(\u221e) is defined.\n\nProof: The goal distribution is assumed to be a Dirac distribution i.e., pg(s) = 1 if s = g and 0 everywhere else. The f-divergence between the agent state-visitation distribution, p\u03c0 and the goal distribution, pg can be defined as,\n\n$$Df(p\u03c0||pg) = \\sum_{s} pg(s)f\\left(\\frac{p\u03c0(s)}{pg(s)}\\right) + f'(\\infty)p\u03c0[pg = 0] = f(p\u03c0(g)) + f'(\\infty)(1 - p\u03c0(g)).$$\n\nLet F = Df(p\u03c0||pg). Differentiating F w.r.t. p\u03c0(g), we get F' = f'(p\u03c0(g)) - f'(\\infty). Since f is a convex function (by the definition of f-divergence), f'(x) \u2264 f'(y), \u2200x \u2264 y. Hence, if f'(\\infty) is defined, F' \u2264 0. Hence F = Df(p\u03c0||pg) is a decreasing function with respect p\u03c0(g).\n\nAdditionally, we need Lemma A.3 and Corollary 1 to complete the proof of Theorem 5.1.\n\nLemma A.3: If any two policies \u03c01 and \u03c02 have the same state visitation at a given goal, they have the same returns for that goal.\n\nProof: Follows directly from the definition of returns. R\u03c0 = E_{p_\u03c0}[r(s)] = p\u03c0(g). Hence two policies \u03c01 and \u03c02 with the same state visitation at the goal will have the same returns.\n\nCorollary 1: Any policy that can lead to the state-visitation distribution of the optimal policy p\u03c0* is optimal.\n\nProof: Directly follows from Lemma A.3.\n\nTheorem 5.1: The policy that minimizes Df(p\u03c0||pg) for a convex function f with f(1) = 0 and f'(\\infty) being defined, is the optimal policy.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Analysis of J(\u03b8)", "md": "# Analysis of J(\u03b8)"}, {"type": "heading", "lvl": 2, "value": "Analysis of J(\u03b8)", "md": "## Analysis of J(\u03b8)"}, {"type": "text", "value": "In this section, we will present the proofs for all the Lemmas and Theorems stated in Section 5.1.", "md": "In this section, we will present the proofs for all the Lemmas and Theorems stated in Section 5.1."}, {"type": "heading", "lvl": 3, "value": "Proof for Theorem 5.1", "md": "### Proof for Theorem 5.1"}, {"type": "text", "value": "To prove Theorem 5.1, we need the following Lemmas. Lemma A.1 states that among all policies, the optimal policy has the highest state visitation at the goal.\n\nLemma A.1: Let D be the set of all possible state visitations for the agent following some policy \u03c0 \u2208 \u03a0. Let \u03c0* be the optimal goal-conditioned policy. This optimal policy\u2019s state-visitation distribution will have the most measure at the goal for all p\u03c0 \u2208 D i.e., \u03c0* \u21d2 p\u03c0*(g) \u2265 p\u03c0(g), \u2200 p\u03c0 \u2208 D.\n\nProof: Let \u03c0* be the optimal policy and p\u03c0* be the corresponding state visitation distribution. The reward for the sparse setting is designed as,\n\n$$r(s) = \\begin{cases} 1 & \\text{if } s = g, \\\\ 0 & \\text{otherwise.} \\end{cases}$$\n\nHence the expected return for a policy \u03c0 is R\u03c0 is\n\n$$R\u03c0 = E_{p_\u03c0}[r(s)] = p_\u03c0(g).$$\n\nThe return for the optimal policy is maximum among all policies so R\u03c0* \u2265 R\u03c0, \u2200\u03c0 \u2208 \u03a0. This implies p\u03c0*(g) \u2265 p\u03c0(g), \u2200p\u03c0 \u2208 D.\n\nLemma A.2: Df(p\u03c0(\u00b7)||pg(\u00b7)) is a decreasing function with respect to p\u03c0(g) \u2200f if f'(\u221e) is defined.\n\nProof: The goal distribution is assumed to be a Dirac distribution i.e., pg(s) = 1 if s = g and 0 everywhere else. The f-divergence between the agent state-visitation distribution, p\u03c0 and the goal distribution, pg can be defined as,\n\n$$Df(p\u03c0||pg) = \\sum_{s} pg(s)f\\left(\\frac{p\u03c0(s)}{pg(s)}\\right) + f'(\\infty)p\u03c0[pg = 0] = f(p\u03c0(g)) + f'(\\infty)(1 - p\u03c0(g)).$$\n\nLet F = Df(p\u03c0||pg). Differentiating F w.r.t. p\u03c0(g), we get F' = f'(p\u03c0(g)) - f'(\\infty). Since f is a convex function (by the definition of f-divergence), f'(x) \u2264 f'(y), \u2200x \u2264 y. Hence, if f'(\\infty) is defined, F' \u2264 0. Hence F = Df(p\u03c0||pg) is a decreasing function with respect p\u03c0(g).\n\nAdditionally, we need Lemma A.3 and Corollary 1 to complete the proof of Theorem 5.1.\n\nLemma A.3: If any two policies \u03c01 and \u03c02 have the same state visitation at a given goal, they have the same returns for that goal.\n\nProof: Follows directly from the definition of returns. R\u03c0 = E_{p_\u03c0}[r(s)] = p\u03c0(g). Hence two policies \u03c01 and \u03c02 with the same state visitation at the goal will have the same returns.\n\nCorollary 1: Any policy that can lead to the state-visitation distribution of the optimal policy p\u03c0* is optimal.\n\nProof: Directly follows from Lemma A.3.\n\nTheorem 5.1: The policy that minimizes Df(p\u03c0||pg) for a convex function f with f(1) = 0 and f'(\\infty) being defined, is the optimal policy.", "md": "To prove Theorem 5.1, we need the following Lemmas. Lemma A.1 states that among all policies, the optimal policy has the highest state visitation at the goal.\n\nLemma A.1: Let D be the set of all possible state visitations for the agent following some policy \u03c0 \u2208 \u03a0. Let \u03c0* be the optimal goal-conditioned policy. This optimal policy\u2019s state-visitation distribution will have the most measure at the goal for all p\u03c0 \u2208 D i.e., \u03c0* \u21d2 p\u03c0*(g) \u2265 p\u03c0(g), \u2200 p\u03c0 \u2208 D.\n\nProof: Let \u03c0* be the optimal policy and p\u03c0* be the corresponding state visitation distribution. The reward for the sparse setting is designed as,\n\n$$r(s) = \\begin{cases} 1 & \\text{if } s = g, \\\\ 0 & \\text{otherwise.} \\end{cases}$$\n\nHence the expected return for a policy \u03c0 is R\u03c0 is\n\n$$R\u03c0 = E_{p_\u03c0}[r(s)] = p_\u03c0(g).$$\n\nThe return for the optimal policy is maximum among all policies so R\u03c0* \u2265 R\u03c0, \u2200\u03c0 \u2208 \u03a0. This implies p\u03c0*(g) \u2265 p\u03c0(g), \u2200p\u03c0 \u2208 D.\n\nLemma A.2: Df(p\u03c0(\u00b7)||pg(\u00b7)) is a decreasing function with respect to p\u03c0(g) \u2200f if f'(\u221e) is defined.\n\nProof: The goal distribution is assumed to be a Dirac distribution i.e., pg(s) = 1 if s = g and 0 everywhere else. The f-divergence between the agent state-visitation distribution, p\u03c0 and the goal distribution, pg can be defined as,\n\n$$Df(p\u03c0||pg) = \\sum_{s} pg(s)f\\left(\\frac{p\u03c0(s)}{pg(s)}\\right) + f'(\\infty)p\u03c0[pg = 0] = f(p\u03c0(g)) + f'(\\infty)(1 - p\u03c0(g)).$$\n\nLet F = Df(p\u03c0||pg). Differentiating F w.r.t. p\u03c0(g), we get F' = f'(p\u03c0(g)) - f'(\\infty). Since f is a convex function (by the definition of f-divergence), f'(x) \u2264 f'(y), \u2200x \u2264 y. Hence, if f'(\\infty) is defined, F' \u2264 0. Hence F = Df(p\u03c0||pg) is a decreasing function with respect p\u03c0(g).\n\nAdditionally, we need Lemma A.3 and Corollary 1 to complete the proof of Theorem 5.1.\n\nLemma A.3: If any two policies \u03c01 and \u03c02 have the same state visitation at a given goal, they have the same returns for that goal.\n\nProof: Follows directly from the definition of returns. R\u03c0 = E_{p_\u03c0}[r(s)] = p\u03c0(g). Hence two policies \u03c01 and \u03c02 with the same state visitation at the goal will have the same returns.\n\nCorollary 1: Any policy that can lead to the state-visitation distribution of the optimal policy p\u03c0* is optimal.\n\nProof: Directly follows from Lemma A.3.\n\nTheorem 5.1: The policy that minimizes Df(p\u03c0||pg) for a convex function f with f(1) = 0 and f'(\\infty) being defined, is the optimal policy."}]}, {"page": 17, "text": "Proof. Lemma A.1 proves that the optimal policy has the maximum state-visitation probability.\nLemma A.2 proves that the f-divergence objective decreases with increasing the state-visitation\nprobability at the goal. In other words, to minimize the f-divergence, we need to maximize the state\nvisitation at goal. Corollary 1 further indicates that any policy that can lead to the state-visitation\ndistribution of the optimal policy i.e., any policy that maximizes the state-visitation distribution at the\ngoal state is an optimal policy.\nA.2    Proof for Lemma 5.1\nLemma 5.1. fkl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of\nthe state-visitation distribution.\nProof. For fkl-PG, f = u log u. Hence, J(\u03b8) = Df(p\u03c0||pg) can be written as,\n                                                   log p\u03c0\n                             Df(p\u03c0||pg) = Ep\u03c0           pg\n                                          = \u2212   E p\u03c0[log pg] \u2212  Ep\u03c0[log p\u03c0]\n                                          = \u2212   Ep\u03c0[log pg] + H(p\u03c0)\nwhere H(p\u03c0) is the entropy of the agent\u2019s state visitation distribution. Minimizing Df(p\u03c0||pg) will\ncorrespond to maximizing the reward r(s) = log pg(s) and the entropy of p\u03c0.\nA similar result could be proved for \u03c72 divergence:\nLemma A.4. If f(u) = (u\u22121)2 (\u03c72 divergence), Df(p\u03c0||pg) is the upper bound of DF KL(p\u03c0||pg)\u2212\n1. Hence minimizing D\u03c72 will also minimize DF KL recovering the entropy regularized policy.\nProof. With f = (u \u2212     1)2, Df(p\u03c0||pg) can be written as,\n                        Df(p\u03c0||pg) =        pg(s)  p\u03c0(s)        2 ds\n                                                    pg(s) \u2212  1\n                                      =     pg(s)   p\u03c0(s) 2    \u2212  2p\u03c0(s)        ds\n                                                     pg(s)          pg(s) + 1\n                                      =     p\u03c0(s)p\u03c0(s)\n                                                  pg(s) \u2212   2p\u03c0(s) + pg(s)ds\n                                      =     p\u03c0(s)p\u03c0(s)\n                                                  pg(s) ds \u2212   1\n                                      = E  p\u03c0(s) p\u03c0(s)    \u2212  1\n                                                  pg(s)\nSince, x > log x,\n                          =\u21d2    Ep\u03c0(s)[x] > Ep\u03c0(s)[log x]\n                          =\u21d2    Ep\u03c0(s)  p\u03c0(s)     > Ep\u03c0(s)   log p\u03c0(s)\n                                         pg(s)                    pg(s)\n                          =\u21d2    Ep\u03c0(s)  p\u03c0(s)    \u2212  1 > Ep\u03c0(s)    log p\u03c0(s)    \u2212 1\n                                         pg(s)                        pg(s)\nMinimizing LHS will also minimize RHS. RHS is essentially DKL(p\u03c0||pg) \u2212                1. The \u22121 will not\nhave any effect on the minimization of DKL(p\u03c0||pg).\n                                                    17", "md": "# Math Equations and Lemmas\n\n## Proof. Lemma A.1:\n\nLemma A.1 proves that the optimal policy has the maximum state-visitation probability.\n\n## Lemma A.2:\n\nLemma A.2 proves that the f-divergence objective decreases with increasing the state-visitation probability at the goal. In other words, to minimize the f-divergence, we need to maximize the state visitation at the goal. Corollary 1 further indicates that any policy that can lead to the state-visitation distribution of the optimal policy, i.e., any policy that maximizes the state-visitation distribution at the goal state, is an optimal policy.\n\n## Proof for Lemma 5.1:\n\nLemma 5.1. fkl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of the state-visitation distribution.\n\n### Proof:\n\nFor fkl-PG, f = u log u. Hence, $$J(\\theta) = D_f(p_{\\pi} || p_g)$$ can be written as,\n\n$$\n\\begin{align*}\nD_f(p_{\\pi} || p_g) & = E_{p_{\\pi}}[\\log p_g] - E_{p_{\\pi}}[\\log p_{\\pi}] \\\\\n& = -E_{p_{\\pi}}[\\log p_g] + H(p_{\\pi})\n\\end{align*}\n$$\nwhere $$H(p_{\\pi})$$ is the entropy of the agent\u2019s state visitation distribution. Minimizing $$D_f(p_{\\pi} || p_g)$$ will correspond to maximizing the reward $$r(s) = \\log p_g(s)$$ and the entropy of $$p_{\\pi}$$.\n\n## Lemma A.4:\n\nIf $$f(u) = (u-1)^2$$ (\u03c72 divergence), $$D_f(p_{\\pi} || p_g)$$ is the upper bound of $$D_{\\text{KL}}(p_{\\pi} || p_g) - 1$$. Hence minimizing $$D_{\\chi^2}$$ will also minimize $$D_{\\text{KL}}$$ recovering the entropy regularized policy.\n\n### Proof:\n\nWith $$f = (u - 1)^2$$, $$D_f(p_{\\pi} || p_g)$$ can be written as,\n\n$$\n\\begin{align*}\nD_f(p_{\\pi} || p_g) & = \\int \\frac{p_g(s)}{p_{\\pi}(s)} (2 - 2p_{\\pi}(s)) ds \\\\\n& = \\int \\frac{p_{\\pi}(s)p_{\\pi}(s)}{p_g(s) - 2p_{\\pi}(s) + p_g(s)} ds \\\\\n& = \\int \\frac{p_{\\pi}(s)p_{\\pi}(s)}{p_g(s)} ds - 1 \\\\\n& = E_{p_{\\pi}(s)}[p_{\\pi}(s)] - 1\n\\end{align*}\n$$\nSince $$x > \\log x$$,\n\n$$\n\\begin{align*}\n\\Rightarrow E_{p_{\\pi}(s)}[x] & > E_{p_{\\pi}(s)}[\\log x] \\\\\n\\Rightarrow E_{p_{\\pi}(s)}\\left(\\frac{p_{\\pi}(s)}{p_g(s)}\\right) & > E_{p_{\\pi}(s)}\\left(\\log \\frac{p_{\\pi}(s)}{p_g(s)}\\right) \\\\\n& \\Rightarrow E_{p_{\\pi}(s)}\\left(\\frac{p_{\\pi}(s)}{p_g(s)} - 1\\right) > E_{p_{\\pi}(s)}\\left(\\log \\frac{p_{\\pi}(s)}{p_g(s)} - 1\\right)\n\\end{align*}\n$$\nMinimizing LHS will also minimize RHS. RHS is essentially $$D_{\\text{KL}}(p_{\\pi} || p_g) - 1$$. The -1 will not have any effect on the minimization of $$D_{\\text{KL}}(p_{\\pi} || p_g)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Lemmas", "md": "# Math Equations and Lemmas"}, {"type": "heading", "lvl": 2, "value": "Proof. Lemma A.1:", "md": "## Proof. Lemma A.1:"}, {"type": "text", "value": "Lemma A.1 proves that the optimal policy has the maximum state-visitation probability.", "md": "Lemma A.1 proves that the optimal policy has the maximum state-visitation probability."}, {"type": "heading", "lvl": 2, "value": "Lemma A.2:", "md": "## Lemma A.2:"}, {"type": "text", "value": "Lemma A.2 proves that the f-divergence objective decreases with increasing the state-visitation probability at the goal. In other words, to minimize the f-divergence, we need to maximize the state visitation at the goal. Corollary 1 further indicates that any policy that can lead to the state-visitation distribution of the optimal policy, i.e., any policy that maximizes the state-visitation distribution at the goal state, is an optimal policy.", "md": "Lemma A.2 proves that the f-divergence objective decreases with increasing the state-visitation probability at the goal. In other words, to minimize the f-divergence, we need to maximize the state visitation at the goal. Corollary 1 further indicates that any policy that can lead to the state-visitation distribution of the optimal policy, i.e., any policy that maximizes the state-visitation distribution at the goal state, is an optimal policy."}, {"type": "heading", "lvl": 2, "value": "Proof for Lemma 5.1:", "md": "## Proof for Lemma 5.1:"}, {"type": "text", "value": "Lemma 5.1. fkl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of the state-visitation distribution.", "md": "Lemma 5.1. fkl-PG produces a policy that maximizes both the reward log pg(s) and the entropy of the state-visitation distribution."}, {"type": "heading", "lvl": 3, "value": "Proof:", "md": "### Proof:"}, {"type": "text", "value": "For fkl-PG, f = u log u. Hence, $$J(\\theta) = D_f(p_{\\pi} || p_g)$$ can be written as,\n\n$$\n\\begin{align*}\nD_f(p_{\\pi} || p_g) & = E_{p_{\\pi}}[\\log p_g] - E_{p_{\\pi}}[\\log p_{\\pi}] \\\\\n& = -E_{p_{\\pi}}[\\log p_g] + H(p_{\\pi})\n\\end{align*}\n$$\nwhere $$H(p_{\\pi})$$ is the entropy of the agent\u2019s state visitation distribution. Minimizing $$D_f(p_{\\pi} || p_g)$$ will correspond to maximizing the reward $$r(s) = \\log p_g(s)$$ and the entropy of $$p_{\\pi}$$.", "md": "For fkl-PG, f = u log u. Hence, $$J(\\theta) = D_f(p_{\\pi} || p_g)$$ can be written as,\n\n$$\n\\begin{align*}\nD_f(p_{\\pi} || p_g) & = E_{p_{\\pi}}[\\log p_g] - E_{p_{\\pi}}[\\log p_{\\pi}] \\\\\n& = -E_{p_{\\pi}}[\\log p_g] + H(p_{\\pi})\n\\end{align*}\n$$\nwhere $$H(p_{\\pi})$$ is the entropy of the agent\u2019s state visitation distribution. Minimizing $$D_f(p_{\\pi} || p_g)$$ will correspond to maximizing the reward $$r(s) = \\log p_g(s)$$ and the entropy of $$p_{\\pi}$$."}, {"type": "heading", "lvl": 2, "value": "Lemma A.4:", "md": "## Lemma A.4:"}, {"type": "text", "value": "If $$f(u) = (u-1)^2$$ (\u03c72 divergence), $$D_f(p_{\\pi} || p_g)$$ is the upper bound of $$D_{\\text{KL}}(p_{\\pi} || p_g) - 1$$. Hence minimizing $$D_{\\chi^2}$$ will also minimize $$D_{\\text{KL}}$$ recovering the entropy regularized policy.", "md": "If $$f(u) = (u-1)^2$$ (\u03c72 divergence), $$D_f(p_{\\pi} || p_g)$$ is the upper bound of $$D_{\\text{KL}}(p_{\\pi} || p_g) - 1$$. Hence minimizing $$D_{\\chi^2}$$ will also minimize $$D_{\\text{KL}}$$ recovering the entropy regularized policy."}, {"type": "heading", "lvl": 3, "value": "Proof:", "md": "### Proof:"}, {"type": "text", "value": "With $$f = (u - 1)^2$$, $$D_f(p_{\\pi} || p_g)$$ can be written as,\n\n$$\n\\begin{align*}\nD_f(p_{\\pi} || p_g) & = \\int \\frac{p_g(s)}{p_{\\pi}(s)} (2 - 2p_{\\pi}(s)) ds \\\\\n& = \\int \\frac{p_{\\pi}(s)p_{\\pi}(s)}{p_g(s) - 2p_{\\pi}(s) + p_g(s)} ds \\\\\n& = \\int \\frac{p_{\\pi}(s)p_{\\pi}(s)}{p_g(s)} ds - 1 \\\\\n& = E_{p_{\\pi}(s)}[p_{\\pi}(s)] - 1\n\\end{align*}\n$$\nSince $$x > \\log x$$,\n\n$$\n\\begin{align*}\n\\Rightarrow E_{p_{\\pi}(s)}[x] & > E_{p_{\\pi}(s)}[\\log x] \\\\\n\\Rightarrow E_{p_{\\pi}(s)}\\left(\\frac{p_{\\pi}(s)}{p_g(s)}\\right) & > E_{p_{\\pi}(s)}\\left(\\log \\frac{p_{\\pi}(s)}{p_g(s)}\\right) \\\\\n& \\Rightarrow E_{p_{\\pi}(s)}\\left(\\frac{p_{\\pi}(s)}{p_g(s)} - 1\\right) > E_{p_{\\pi}(s)}\\left(\\log \\frac{p_{\\pi}(s)}{p_g(s)} - 1\\right)\n\\end{align*}\n$$\nMinimizing LHS will also minimize RHS. RHS is essentially $$D_{\\text{KL}}(p_{\\pi} || p_g) - 1$$. The -1 will not have any effect on the minimization of $$D_{\\text{KL}}(p_{\\pi} || p_g)$$.", "md": "With $$f = (u - 1)^2$$, $$D_f(p_{\\pi} || p_g)$$ can be written as,\n\n$$\n\\begin{align*}\nD_f(p_{\\pi} || p_g) & = \\int \\frac{p_g(s)}{p_{\\pi}(s)} (2 - 2p_{\\pi}(s)) ds \\\\\n& = \\int \\frac{p_{\\pi}(s)p_{\\pi}(s)}{p_g(s) - 2p_{\\pi}(s) + p_g(s)} ds \\\\\n& = \\int \\frac{p_{\\pi}(s)p_{\\pi}(s)}{p_g(s)} ds - 1 \\\\\n& = E_{p_{\\pi}(s)}[p_{\\pi}(s)] - 1\n\\end{align*}\n$$\nSince $$x > \\log x$$,\n\n$$\n\\begin{align*}\n\\Rightarrow E_{p_{\\pi}(s)}[x] & > E_{p_{\\pi}(s)}[\\log x] \\\\\n\\Rightarrow E_{p_{\\pi}(s)}\\left(\\frac{p_{\\pi}(s)}{p_g(s)}\\right) & > E_{p_{\\pi}(s)}\\left(\\log \\frac{p_{\\pi}(s)}{p_g(s)}\\right) \\\\\n& \\Rightarrow E_{p_{\\pi}(s)}\\left(\\frac{p_{\\pi}(s)}{p_g(s)} - 1\\right) > E_{p_{\\pi}(s)}\\left(\\log \\frac{p_{\\pi}(s)}{p_g(s)} - 1\\right)\n\\end{align*}\n$$\nMinimizing LHS will also minimize RHS. RHS is essentially $$D_{\\text{KL}}(p_{\\pi} || p_g) - 1$$. The -1 will not have any effect on the minimization of $$D_{\\text{KL}}(p_{\\pi} || p_g)$$."}]}, {"page": 18, "text": "B       Gradient based optimization\nB.1       Derivation of gradients\nTheorem 4.1. The gradient of J(\u03b8) as defined in Equation 2 is given by,\n                        \u2207\u03b8J(\u03b8) = 1                                  T   \u2207\u03b8 log \u03c0\u03b8(at|st)                 T    f \u2032   p\u03b8(st)          .       (8)\n                                          T E\u03c4\u223cp\u03b8(\u03c4)              t=1                                   t=1         p g(st)\nProof. Lets start with the state-visitation distribution. In Section 3, it was shown that the state-\nvisitation distribution can be written as,          p\u03b8(s) \u221d            p(\u03c4)\u03a0T     t=1\u03c0\u03b8(st)\u03b7\u03c4         (s)d\u03c4\n                                            =\u21d2      p\u03b8(s) \u221d            p(\u03c4)e     T  t=1 log \u03c0\u03b8(st)     \u03b7\u03c4  (s)d\u03c4\n                                                                       p(\u03c4)e     T t=1 log \u03c0\u03b8(st)\u03b7\u03c4       (s)d\u03c4\n                                         =\u21d2      p\u03b8(s) =              p(\u03c4)e      T t=1 log \u03c0\u03b8(st)\u03b7\u03c4       (s)d\u03c4ds\n                                                                       p(\u03c4)e T   T  t=1 log \u03c0\u03b8(st)\u03b7\u03c4       (s)d\u03c4\n                                         =\u21d2      p\u03b8(s) =           p(\u03c4)e     T  t=1 log \u03c0\u03b8(st)         \u03b7\u03c4  (s)dsd\u03c4\n                                                                   p(\u03c4)e        t=1 log \u03c0\u03b8(st)\u03b7\u03c4       (s)d\u03c4\n                                         =\u21d2      p\u03b8(s) =           T     p(\u03c4)e      T t=1 log \u03c0\u03b8(st)d\u03c4\n                                         =\u21d2      p\u03b8(s) = f(s)     Z\nwhere f(s) =               p(\u03c4)e     T t=1 log \u03c0\u03b8(st)     \u03b7\u03c4  (s)d\u03c4 and Z = T                 p(\u03c4)e      T t=1 log \u03c0\u03b8(st)d\u03c4.\nDifferentiating w.r.t. \u03c0\u03b8(s\u2217),\n                                          df(s)                p(\u03c4)e     T t=1 log \u03c0\u03b8(st)\u03b7\u03c4       (s)\u03b7\u03c4    (s\u2217)d\u03c4\nand,                                    d\u03c0\u03b8(s\u2217) =                                   \u03c0\u03b8(s\u2217)\n                                               dZ                    p(\u03c4)e     T  t=1 log \u03c0\u03b8(st)\u03b7\u03c4       (s\u2217)d\u03c4\n                                           d\u03c0\u03b8(s\u2217) = T    = T    f(s\u2217)              \u03c0\u03b8(s\u2217)\n                                                               \u03c0\u03b8(s\u2217)\nComputing dp\u03b8(s)                          df(s)                dZ\n                   \u03c0\u03b8(s\u2217) using         d\u03c0\u03b8(s\u2217) and         d\u03c0\u03b8(s\u2217),\n                                              df(s)                    dZ\n                       dp\u03b8(s)           Z   d\u03c0\u03b8(s\u2217) \u2212       f(s)d\u03c0\u03b8(s\u2217)\n                       \u03c0\u03b8(s\u2217) =                          Z2\n                                    =       p(\u03c4)e     T t=1 log \u03c0\u03b8(st)\u03b7\u03c4       (s)\u03b7\u03c4    (s\u2217)d\u03c4       \u2212   f(s)          f(s\u2217)\n                                                               Z\u03c0\u03b8(s\u2217)                                     Z T      Z\u03c0\u03b8(s\u2217)\n                                    =       p(\u03c4)e     T t=1 log \u03c0\u03b8(st)\u03b7\u03c4       (s)\u03b7\u03c4    (s\u2217)d\u03c4       \u2212       T\n Now we can compute dp\u03b8(s),             d\u03b8                     Z\u03c0\u03b8(s\u2217)                                   \u03c0\u03b8(s\u2217)p\u03b8(s)p\u03b8(s\u2217)\ndp\u03b8(s)       =        dp\u03b8(s)      d\u03c0\u03b8(s\u2217)       ds\u2217\n    d\u03b8                \u03c0\u03b8(s\u2217)           d\u03b8\n             =              p(\u03c4)e     T t=1 log \u03c0\u03b8(st)     \u03b7\u03c4  (s)\u03b7\u03c4    (s\u2217)d\u03c4       \u2212       T                           d\u03c0\u03b8(s\u2217)       ds\u2217\n                                               Z\u03c0\u03b8(s\u2217)                                   \u03c0\u03b8(s\u2217)p\u03b8(s)p\u03b8(s\u2217)                      d\u03b8\n                                                                             18", "md": "# Gradient based optimization\n\n## Derivation of gradients\n\nTheorem 4.1. The gradient of J(\u03b8) as defined in Equation 2 is given by,\n\n$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) f'(s_{t})$$ (8)\nProof. Let's start with the state-visitation distribution. In Section 3, it was shown that the state-visitation distribution can be written as,\n\n$$p_{\\theta}(s) \\propto \\int p(\\tau) \\prod_{t=1}^{T} \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) \\propto \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau ds$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} ds d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = f(s) Z$$\n\nwhere $$f(s) = p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$ and $$Z = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t})} d\\tau$$.\n\nDifferentiating w.r.t. $$\\pi_{\\theta}(s^*)$$,\n\n$$\\frac{df(s)}{d\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau}{\\pi_{\\theta}(s^*)}$$\n\nand,\n\n$$\\frac{dZ}{d\\pi_{\\theta}(s^*)} = \\frac{T f(s^*)}{\\pi_{\\theta}(s^*)}$$\n\nComputing $$dp_{\\theta}(s)$$ using $$\\frac{df(s)}{d\\pi_{\\theta}(s^*)}$$ and $$\\frac{dZ}{d\\pi_{\\theta}(s^*)}$$,\n\n$$\\frac{dp_{\\theta}(s)}{\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau - T f(s) f(s^*)}{Z \\pi_{\\theta}(s^*)}$$\n\nNow we can compute $$dp_{\\theta}(s)$$,\n\n$$\\frac{d\\theta}{d\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau - T f(s) f(s^*)}{Z \\pi_{\\theta}(s^*) p_{\\theta}(s) p_{\\theta}(s^*)}$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Gradient based optimization", "md": "# Gradient based optimization"}, {"type": "heading", "lvl": 2, "value": "Derivation of gradients", "md": "## Derivation of gradients"}, {"type": "text", "value": "Theorem 4.1. The gradient of J(\u03b8) as defined in Equation 2 is given by,\n\n$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) f'(s_{t})$$ (8)\nProof. Let's start with the state-visitation distribution. In Section 3, it was shown that the state-visitation distribution can be written as,\n\n$$p_{\\theta}(s) \\propto \\int p(\\tau) \\prod_{t=1}^{T} \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) \\propto \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau ds$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} ds d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = f(s) Z$$\n\nwhere $$f(s) = p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$ and $$Z = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t})} d\\tau$$.\n\nDifferentiating w.r.t. $$\\pi_{\\theta}(s^*)$$,\n\n$$\\frac{df(s)}{d\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau}{\\pi_{\\theta}(s^*)}$$\n\nand,\n\n$$\\frac{dZ}{d\\pi_{\\theta}(s^*)} = \\frac{T f(s^*)}{\\pi_{\\theta}(s^*)}$$\n\nComputing $$dp_{\\theta}(s)$$ using $$\\frac{df(s)}{d\\pi_{\\theta}(s^*)}$$ and $$\\frac{dZ}{d\\pi_{\\theta}(s^*)}$$,\n\n$$\\frac{dp_{\\theta}(s)}{\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau - T f(s) f(s^*)}{Z \\pi_{\\theta}(s^*)}$$\n\nNow we can compute $$dp_{\\theta}(s)$$,\n\n$$\\frac{d\\theta}{d\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau - T f(s) f(s^*)}{Z \\pi_{\\theta}(s^*) p_{\\theta}(s) p_{\\theta}(s^*)}$$", "md": "Theorem 4.1. The gradient of J(\u03b8) as defined in Equation 2 is given by,\n\n$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) f'(s_{t})$$ (8)\nProof. Let's start with the state-visitation distribution. In Section 3, it was shown that the state-visitation distribution can be written as,\n\n$$p_{\\theta}(s) \\propto \\int p(\\tau) \\prod_{t=1}^{T} \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) \\propto \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau ds$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} ds d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$\n\n$$\\Rightarrow p_{\\theta}(s) = f(s) Z$$\n\nwhere $$f(s) = p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s)} d\\tau$$ and $$Z = \\int p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t})} d\\tau$$.\n\nDifferentiating w.r.t. $$\\pi_{\\theta}(s^*)$$,\n\n$$\\frac{df(s)}{d\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau}{\\pi_{\\theta}(s^*)}$$\n\nand,\n\n$$\\frac{dZ}{d\\pi_{\\theta}(s^*)} = \\frac{T f(s^*)}{\\pi_{\\theta}(s^*)}$$\n\nComputing $$dp_{\\theta}(s)$$ using $$\\frac{df(s)}{d\\pi_{\\theta}(s^*)}$$ and $$\\frac{dZ}{d\\pi_{\\theta}(s^*)}$$,\n\n$$\\frac{dp_{\\theta}(s)}{\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau - T f(s) f(s^*)}{Z \\pi_{\\theta}(s^*)}$$\n\nNow we can compute $$dp_{\\theta}(s)$$,\n\n$$\\frac{d\\theta}{d\\pi_{\\theta}(s^*)} = \\frac{p(\\tau) e^{\\sum_{t=1}^{T} \\log \\pi_{\\theta}(s_{t}) \\eta_{\\tau}(s) \\eta_{\\tau}(s^*)} d\\tau - T f(s) f(s^*)}{Z \\pi_{\\theta}(s^*) p_{\\theta}(s) p_{\\theta}(s^*)}$$"}]}, {"page": 19, "text": "         = 1           p(\u03c4)e   Tt=1 log \u03c0\u03b8(st)\u03b7\u03c4(s)\u03b7\u03c4(s\u2217)\u2207\u03b8\u03c0\u03b8(s\u2217)                                 p\u03b8(s\u2217)\u2207\u03b8\u03c0\u03b8(s\u2217)\n             Z                                                 \u03c0\u03b8(s\u2217) ds\u2217d\u03c4 \u2212        Tp\u03b8(s)                 \u03c0\u03b8(s\u2217) ds\u2217\n         = 1 Z      p(\u03c4)e  T t=1 log \u03c0\u03b8(st)\u03b7\u03c4(s)   T   \u2207\u03b8 log \u03c0\u03b8(st)d\u03c4 \u2212      Tp\u03b8(s)Es\u223cp\u03b8(s)[\u2207\u03b8 log \u03c0\u03b8(s)]\n                                                  t=1\n The objective L(\u03b8) = Df(p\u03b8(s)||pg(s)) =               pg(s)f    p\u03b8(s)   ds\n                                                                 pg(s)\nThe gradient for L(\u03b8) will be given by,\n           \u2207\u03b8L(\u03b8) =         pg(s)f \u2032 p\u03b8(s)      \u2207\u03b8p\u03b8(s)        ds\n                                       pg(s)        pg(s)\n                     =      \u2207p\u03b8(s)f \u2032 p\u03b8(s)        ds\n                                          pg(s)\n                     =      f \u2032 p\u03b8(s)       1     p(\u03c4)e   T t=1 log \u03c0\u03b8(st)\u03b7\u03c4(s)   T   \u2207\u03b8 log \u03c0\u03b8(st)d\u03c4\n                                 pg(s)      Z                                    t=1\n                           \u2212  Tp\u03b8(s)Es\u223cp\u03b8(s)[\u2207\u03b8 log \u03c0\u03b8(s)]           ds\n                     = 1       p(\u03c4)e   T t=1 log \u03c0\u03b8(st)  T  \u2207\u03b8 log \u03c0\u03b8(st)        \u03b7\u03c4(s)f \u2032 p\u03b8(s)      dsd\u03c4\n                         Z                              t=1                                 pg(s)\n                           \u2212      Tp\u03b8(s)f \u2032 p\u03b8(s)       Es\u223cp\u03b8(s)[\u2207\u03b8 log \u03c0\u03b8(s)]ds\n                                               pg(s)\n                     = 1       p(\u03c4)e   T t=1 log \u03c0\u03b8(st)  T  \u2207\u03b8 log \u03c0\u03b8(st)      T  f \u2032 p\u03b8(st)     d\u03c4\n                         Z                              t=1                  t=1       pg(st)\n                           \u2212  T     p\u03b8(s)f \u2032 p\u03b8(s)      Es\u223cp\u03b8(s)[\u2207\u03b8 log \u03c0\u03b8(s)]ds\n                                                pg(s)\n                     = 1       p\u03b8(\u03c4)    T  \u2207\u03b8 log \u03c0\u03b8(st)      T  f \u2032 p\u03b8(st)     d\u03c4\n                         T             t=1                   t=1      pg(st)\n                           \u2212  TEs\u223cp\u03b8(s)      f \u2032 p\u03b8(s)      Es\u223cp\u03b8(s)[\u2207\u03b8 log \u03c0\u03b8(s)]\n                                                 pg(s)\n                     = 1                  T   \u2207\u03b8 log \u03c0\u03b8(st)     T   f \u2032 p\u03b8(st)\n                         T E\u03c4\u223cp\u03b8(\u03c4)      t=1                   t=1      p g(st)\n                           \u2212  TEs\u223cp\u03b8(s)      f \u2032 p\u03b8(s)      Es\u223cp\u03b8(s)[\u2207\u03b8 log \u03c0\u03b8(s)]\n                                                 pg(s)\n                     = 1     E \u03c4\u223cp\u03b8(\u03c4)      T   \u2207\u03b8 log \u03c0\u03b8(st)     T  f \u2032 p\u03b8(st)\n                         T                 t=1                   t=1      pg(st)\n                           \u2212  Es\u223cp\u03b8(s)      T   f \u2032 p\u03b8(st)      E\u03c4\u223cp\u03b8(\u03c4)[    T   \u2207\u03b8 log \u03c0\u03b8(st)]\n                                           t=1       pg(st)                 t=1\n                     = 1                  T   \u2207\u03b8 log \u03c0\u03b8(st)     T   f \u2032 p\u03b8(st)\n                         T E\u03c4\u223cp\u03b8(\u03c4)      t=1                   t=1      p g(st)\nTheorem 4.2. Updating the policy using the gradient maximizes Ep                 \u03b8[\u03b7\u03c4(g)].\n                                                         19", "md": "# Math Equations\n\n$$\n\\begin{align*}\n&= 1 \\int p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s)\\eta_{\\tau}(s^*)\\nabla_{\\theta}\\pi_{\\theta}(s^*)}p_{\\theta}(s^*)\\nabla_{\\theta}\\pi_{\\theta}(s^*) \\, ds^*d\\tau - \\int Tp_{\\theta}(s)\\pi_{\\theta}(s^*) \\, ds^* \\\\\n&= 1 \\int p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\, d\\tau - \\int Tp_{\\theta}(s) \\mathbb{E}_{s\\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)] \\, ds \\\\\n\\end{align*}\n$$\n\n$$\n\\text{The objective } L(\\theta) = D_f(p_{\\theta}(s)||p_g(s)) = \\int \\frac{p_g(s)f(p_{\\theta}(s))}{p_g(s)} \\, ds\n$$\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}L(\\theta) &= \\int \\frac{p_g(s)f'(p_{\\theta}(s))\\nabla_{\\theta}p_{\\theta}(s)}{p_g(s)} \\, ds \\\\\n&= \\int \\nabla_{p_{\\theta}}f'(p_{\\theta}(s)) \\, ds \\\\\n&= \\int f'(p_{\\theta}(s)) \\frac{1}{p_g(s)} p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s) \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)} \\, dsd\\tau \\\\\n&\\quad - \\int Tp_{\\theta}(s)f'(p_{\\theta}(s)) \\mathbb{E}_{s\\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)] \\, ds \\\\\n&= \\int p(\\tau) \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - T \\int \\frac{p_{\\theta}(\\tau)}{T} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n\\end{align*}\n$$\n\nTheorem 4.2. Updating the policy using the gradient maximizes $E_{p_{\\theta}}[\\eta_{\\tau}(g)]$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "$$\n\\begin{align*}\n&= 1 \\int p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s)\\eta_{\\tau}(s^*)\\nabla_{\\theta}\\pi_{\\theta}(s^*)}p_{\\theta}(s^*)\\nabla_{\\theta}\\pi_{\\theta}(s^*) \\, ds^*d\\tau - \\int Tp_{\\theta}(s)\\pi_{\\theta}(s^*) \\, ds^* \\\\\n&= 1 \\int p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\, d\\tau - \\int Tp_{\\theta}(s) \\mathbb{E}_{s\\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)] \\, ds \\\\\n\\end{align*}\n$$\n\n$$\n\\text{The objective } L(\\theta) = D_f(p_{\\theta}(s)||p_g(s)) = \\int \\frac{p_g(s)f(p_{\\theta}(s))}{p_g(s)} \\, ds\n$$\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}L(\\theta) &= \\int \\frac{p_g(s)f'(p_{\\theta}(s))\\nabla_{\\theta}p_{\\theta}(s)}{p_g(s)} \\, ds \\\\\n&= \\int \\nabla_{p_{\\theta}}f'(p_{\\theta}(s)) \\, ds \\\\\n&= \\int f'(p_{\\theta}(s)) \\frac{1}{p_g(s)} p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s) \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)} \\, dsd\\tau \\\\\n&\\quad - \\int Tp_{\\theta}(s)f'(p_{\\theta}(s)) \\mathbb{E}_{s\\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)] \\, ds \\\\\n&= \\int p(\\tau) \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - T \\int \\frac{p_{\\theta}(\\tau)}{T} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n\\end{align*}\n$$\n\nTheorem 4.2. Updating the policy using the gradient maximizes $E_{p_{\\theta}}[\\eta_{\\tau}(g)]$.", "md": "$$\n\\begin{align*}\n&= 1 \\int p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s)\\eta_{\\tau}(s^*)\\nabla_{\\theta}\\pi_{\\theta}(s^*)}p_{\\theta}(s^*)\\nabla_{\\theta}\\pi_{\\theta}(s^*) \\, ds^*d\\tau - \\int Tp_{\\theta}(s)\\pi_{\\theta}(s^*) \\, ds^* \\\\\n&= 1 \\int p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\, d\\tau - \\int Tp_{\\theta}(s) \\mathbb{E}_{s\\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)] \\, ds \\\\\n\\end{align*}\n$$\n\n$$\n\\text{The objective } L(\\theta) = D_f(p_{\\theta}(s)||p_g(s)) = \\int \\frac{p_g(s)f(p_{\\theta}(s))}{p_g(s)} \\, ds\n$$\n\n$$\n\\begin{align*}\n\\nabla_{\\theta}L(\\theta) &= \\int \\frac{p_g(s)f'(p_{\\theta}(s))\\nabla_{\\theta}p_{\\theta}(s)}{p_g(s)} \\, ds \\\\\n&= \\int \\nabla_{p_{\\theta}}f'(p_{\\theta}(s)) \\, ds \\\\\n&= \\int f'(p_{\\theta}(s)) \\frac{1}{p_g(s)} p(\\tau)e^{\\sum_{t=1} \\log \\pi_{\\theta}(s_t)\\eta_{\\tau}(s) \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)} \\, dsd\\tau \\\\\n&\\quad - \\int Tp_{\\theta}(s)f'(p_{\\theta}(s)) \\mathbb{E}_{s\\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)] \\, ds \\\\\n&= \\int p(\\tau) \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - T \\int \\frac{p_{\\theta}(\\tau)}{T} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n&= \\int \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\sum_{t} f'(p_{\\theta}(s_t)) \\, d\\tau \\\\\n&\\quad - \\mathbb{E}_{s\\sim p_{\\theta}(s)} \\sum_{t} f'(p_{\\theta}(s_t)) \\mathbb{E}_{\\tau\\sim p_{\\theta}(\\tau)}[\\sum_{t} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t)] \\\\\n\\end{align*}\n$$\n\nTheorem 4.2. Updating the policy using the gradient maximizes $E_{p_{\\theta}}[\\eta_{\\tau}(g)]$."}]}, {"page": 20, "text": "Proof. In goal-based setting, pg is sparse, so we need to use the full definition of f-divergence,\nDf   (p\u03b8||pg) =             pg>0      pg(s)f( p\u03b8(s)pg(s))     +f \u2032(\u221e)p\u03b8[pg = 0] =                      pg>0      pg(s)f( p\u03b8(s)pg(s))      +f \u2032(\u221e)(1\u2212\np\u03b8(g)). Differentiating with respect to \u03b8 gives,\n\u2207\u03b8L(\u03b8) =            f \u2032(p\u03b8(g)) \u2212        f \u2032(\u221e)       \u2207\u03b8p\u03b8(s)                         T\n              =     f \u2032(p\u03b8(g)) \u2212        f \u2032(\u221e)          1        p\u03b8(\u03c4)\u03b7\u03c4      (g)         \u2207\u03b8 log \u03c0\u03b8(st)d\u03c4 \u2212              T  p\u03b8(s)Es\u223cp\u03b8(s)[\u2207\u03b8log\u03c0\u03b8(s)]\n                                                        T                           t=1\n              =     f \u2032(p\u03b8(g)) \u2212        f \u2032(\u221e)          1                   \u03b7\u03c4  (g)     T    \u2207\u03b8 log \u03c0\u03b8(st)\n                                                      T T E\u03c4\u223cp\u03b8(\u03c4)                    t=1\n                      \u2212   p\u03b8(g)E\u03c4\u223cp\u03b8(\u03c4)              t=1   \u2207\u03b8 log \u03c0\u03b8(s)     T\n              = 1  T    f \u2032(p\u03b8(g)) \u2212        f \u2032(\u221e)       E\u03c4\u223cp\u03b8(\u03c4)          t=1   \u2207\u03b8 log \u03c0\u03b8(st)\u03b7\u03c4           (g)\nThe gradient has two terms, the first term                             f \u2032(p\u03b8(g)) \u2212        f \u2032(\u221e)        weighs the gradient based on the\nvalue of p\u03b8(g) and is always negative. The second term is the gradient of Ep                                               \u03b8[\u03b7\u03c4   (g)]. Hence using\n\u2207\u03b8L(\u03b8), we minimize L(\u03b8) which would imply maximizing Ep\u03b8[\u03b7\u03c4                                                 (g)].\nB.2       Practical Algorithm\nAs mentioned in Section 4, the derived gradient is highly sample inefficient. We employ established\nmethods to improve the performance of policy gradients like importance sampling.\nThe first modification is to use importance sampling weights to allow sampling from previous policy\n\u03b8\u2032. The gradient now looks like,\n                   \u2207\u03b8J(\u03b8) = E\u03c4\u223cp\u03b8\u2032(\u03c4)                    \u03c0\u03b8(\u03c4)          T    \u2207\u03b8 log \u03c0\u03b8(at|st)                 T   f \u2032    p\u03b8(st)           .               (9)\n                                                         \u03c0\u03b8\u2032(\u03c4)        t=1                                  t=1          pg(st)\nTo reduce the variance in the gradients, the objective can be modified to use the causal connections in\nthe MDP and ensure that the action taken at step t only affects rewards at times t\u2032 \u2192      p\u03b8(st)                                    [t, T  ]. Moreover,\na discount factor \u03b3 is used to prevent the sum  T                             t\u2032=t f \u2032 pg(st)          from exploding.\nAdditionally, the expectation is modified to be over states rather than trajectories,\n                \u2207\u03b8J(\u03b8) = Est,at\u223cp\u03b8\u2032(st,at)                      \u03c0\u03b8(at|st)                                     T    \u03b3t\u2032f \u2032     p\u03b8(st)         .          (10)\n                                                                \u03c0\u03b8\u2032(at|st)\u2207\u03b8 log \u03c0\u03b8(at|st)                  t\u2032=t              pg(st)\nThis gradient computation is still inefficient, because even though the samples are from a previous\npolicy \u03c0     \u03b8\u2032 , it still needs to compute  T               t\u2032=t \u03b3t\u2032f \u2032 pg(st)p\u03b8(st)     , requiring iteration through full trajectories.\nWe can add a bias to the gradient by modifying f \u2032                                      p\u03b8(st)       to f \u2032     p\u03b8\u2032(st)       in the objective. To\n                                                                                        pg(st)                  pg(st)\nensure the bias is small, an additional constraint needs to be added to keep \u03b8\u2032 close to \u03b8. Following\nthe literature from natural gradients, the constraint we add is DKL(p\u03b8\u2032||p\u03b8). Proximal Policy\nOptimization (Schulman et al., 2017) showed that in practical scenarios, clipped objective can be\nenough to do away with the KL regularization term. The final objective that we use is,\n        \u2207\u03b8J(\u03b8) = Est,at\u223cp\u03b8\u2032(st,at)                      min(r\u03b8(st)F\u03b8\u2032(st), clip(r\u03b8(st), 1 \u2212                       \u03f5, 1 + \u03f5)F\u03b8\u2032(st))            ,        (11)\nwhere r\u03b8(st) = \u2207\u03b8\u03c0\u03b8(at|st) \u03c0                                             t\u2032=t \u03b3t\u2032f \u2032       p \u03b8\u2032(st)     .\n                             \u03b8\u2032(sat|st) and F\u03b8\u2032(st) =  T                                    pg(st)\nB.3       Discounted State-Visitations\nThe state-visitation distribution defined so far has not considered a discount factor. To include\ndiscounting, the state-visitation frequency gets modified to \u03b7\u03c4              20                       (s) =  T       t=1 \u03b3t1st=s. Throughout", "md": "# Math Equations in HTML\n\n## Proof:\n\nIn goal-based setting, \\( p_g \\) is sparse, so we need to use the full definition of f-divergence:\n\n$$\nD_f(p_{\\theta} || p_g) = \\sum_{s} p_g > 0 \\, p_g(s) f\\left(\\frac{p_{\\theta}(s)}{p_g(s)}\\right) + f'(\\infty) p_{\\theta}[p_g = 0] = \\sum_{s} p_g > 0 \\, p_g(s) f\\left(\\frac{p_{\\theta}(s)}{p_g(s)}\\right) + f'(\\infty) \\left(1 - p_{\\theta}(g)\\right)\n$$\n\nDifferentiating with respect to \\( \\theta \\) gives:\n\n$$\n\\nabla_{\\theta} L(\\theta) = f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\nabla_{\\theta} p_{\\theta}(s) = f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\frac{1}{T} \\sum_{\\tau} p_{\\theta}(\\tau) \\eta_{\\tau}(g) \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) d\\tau - \\frac{1}{T} p_{\\theta}(s) \\mathbb{E}_{s \\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)]\n$$\n\n$$\n= \\frac{1}{T} f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\eta_{\\tau}(g)\n$$\n\nThe gradient has two terms, the first term \\( f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\) weighs the gradient based on the value of \\( p_{\\theta}(g) \\) and is always negative. The second term is the gradient of \\( \\mathbb{E}_{p_{\\theta}}[\\eta_{\\tau}(g)] \\). Hence using \\( \\nabla_{\\theta} L(\\theta) \\), we minimize \\( L(\\theta) \\) which would imply maximizing \\( \\mathbb{E}_{p_{\\theta}}[\\eta_{\\tau}(g)] \\).\n\n## Practical Algorithm:\n\nAs mentioned in Section 4, the derived gradient is highly sample inefficient. We employ established methods to improve the performance of policy gradients like importance sampling.\n\nThe first modification is to use importance sampling weights to allow sampling from the previous policy \\( \\theta' \\). The gradient now looks like:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta'}} \\left[ \\frac{\\pi_{\\theta}(\\tau)}{\\pi_{\\theta'}(\\tau)} \\sum_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) f' \\frac{p_{\\theta}(s_t)}{p_g(s_t)} \\right]\n$$\n\nTo reduce the variance in the gradients, the objective can be modified to use the causal connections in the MDP and ensure that the action taken at step \\( t \\) only affects rewards at times \\( t' \\rightarrow [t, T] \\). Moreover, a discount factor \\( \\gamma \\) is used to prevent the sum \\( \\sum_{t'=t}^{T} \\gamma^{t'} f' p_g(s_t) \\) from exploding.\n\nAdditionally, the expectation is modified to be over states rather than trajectories:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s,t \\sim p_{\\theta'}} \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta'}(a_t|s_t)} \\sum_{t'=t} \\gamma^{t'} f' p_{\\theta}(s_t) \\right]\n$$\n\nThis gradient computation is still inefficient, because even though the samples are from a previous policy \\( \\pi_{\\theta'} \\), it still needs to compute \\( \\sum_{t'=t}^{T} \\gamma^{t'} f' p_g(s_t) p_{\\theta}(s_t) \\), requiring iteration through full trajectories.\n\nWe can add a bias to the gradient by modifying \\( f' \\frac{p_{\\theta}(s_t)}{p_g(s_t)} \\) to \\( f' \\frac{p_{\\theta'}(s_t)}{p_g(s_t)} \\) in the objective. To ensure the bias is small, an additional constraint needs to be added to keep \\( \\theta' \\) close to \\( \\theta \\). Following the literature from natural gradients, the constraint we add is \\( D_{KL}(p_{\\theta'} || p_{\\theta}) \\). Proximal Policy Optimization (Schulman et al., 2017) showed that in practical scenarios, a clipped objective can be enough to do away with the KL regularization term. The final objective that we use is:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s,t \\sim p_{\\theta'}} \\left[ \\min\\left(r_{\\theta}(s_t) F_{\\theta'}(s_t), \\text{clip}\\left(r_{\\theta}(s_t), 1 - \\epsilon, 1 + \\epsilon\\right) F_{\\theta'}(s_t)\\right) \\right]\n$$\n\nwhere \\( r_{\\theta}(s_t) = \\nabla_{\\theta} \\pi_{\\theta}(a_t|s_t) / \\pi_{\\theta'}(a_t|s_t) \\) and \\( F_{\\theta'}(s_t) = \\sum_{t} p_g(s_t) \\).\n\n## Discounted State-Visitations:\n\nThe state-visitation distribution defined so far has not considered a discount factor. To include discounting, the state-visitation frequency gets modified to \\( \\eta_{\\tau}^{(s)} = \\sum_{t=1}^{T} \\gamma^{t-1} 1_{s_t=s} \\). Throughout", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations in HTML", "md": "# Math Equations in HTML"}, {"type": "heading", "lvl": 2, "value": "Proof:", "md": "## Proof:"}, {"type": "text", "value": "In goal-based setting, \\( p_g \\) is sparse, so we need to use the full definition of f-divergence:\n\n$$\nD_f(p_{\\theta} || p_g) = \\sum_{s} p_g > 0 \\, p_g(s) f\\left(\\frac{p_{\\theta}(s)}{p_g(s)}\\right) + f'(\\infty) p_{\\theta}[p_g = 0] = \\sum_{s} p_g > 0 \\, p_g(s) f\\left(\\frac{p_{\\theta}(s)}{p_g(s)}\\right) + f'(\\infty) \\left(1 - p_{\\theta}(g)\\right)\n$$\n\nDifferentiating with respect to \\( \\theta \\) gives:\n\n$$\n\\nabla_{\\theta} L(\\theta) = f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\nabla_{\\theta} p_{\\theta}(s) = f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\frac{1}{T} \\sum_{\\tau} p_{\\theta}(\\tau) \\eta_{\\tau}(g) \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) d\\tau - \\frac{1}{T} p_{\\theta}(s) \\mathbb{E}_{s \\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)]\n$$\n\n$$\n= \\frac{1}{T} f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\eta_{\\tau}(g)\n$$\n\nThe gradient has two terms, the first term \\( f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\) weighs the gradient based on the value of \\( p_{\\theta}(g) \\) and is always negative. The second term is the gradient of \\( \\mathbb{E}_{p_{\\theta}}[\\eta_{\\tau}(g)] \\). Hence using \\( \\nabla_{\\theta} L(\\theta) \\), we minimize \\( L(\\theta) \\) which would imply maximizing \\( \\mathbb{E}_{p_{\\theta}}[\\eta_{\\tau}(g)] \\).", "md": "In goal-based setting, \\( p_g \\) is sparse, so we need to use the full definition of f-divergence:\n\n$$\nD_f(p_{\\theta} || p_g) = \\sum_{s} p_g > 0 \\, p_g(s) f\\left(\\frac{p_{\\theta}(s)}{p_g(s)}\\right) + f'(\\infty) p_{\\theta}[p_g = 0] = \\sum_{s} p_g > 0 \\, p_g(s) f\\left(\\frac{p_{\\theta}(s)}{p_g(s)}\\right) + f'(\\infty) \\left(1 - p_{\\theta}(g)\\right)\n$$\n\nDifferentiating with respect to \\( \\theta \\) gives:\n\n$$\n\\nabla_{\\theta} L(\\theta) = f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\nabla_{\\theta} p_{\\theta}(s) = f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\frac{1}{T} \\sum_{\\tau} p_{\\theta}(\\tau) \\eta_{\\tau}(g) \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) d\\tau - \\frac{1}{T} p_{\\theta}(s) \\mathbb{E}_{s \\sim p_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(s)]\n$$\n\n$$\n= \\frac{1}{T} f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\sum_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t) \\eta_{\\tau}(g)\n$$\n\nThe gradient has two terms, the first term \\( f'\\left(p_{\\theta}(g)\\right) - f'(\\infty) \\) weighs the gradient based on the value of \\( p_{\\theta}(g) \\) and is always negative. The second term is the gradient of \\( \\mathbb{E}_{p_{\\theta}}[\\eta_{\\tau}(g)] \\). Hence using \\( \\nabla_{\\theta} L(\\theta) \\), we minimize \\( L(\\theta) \\) which would imply maximizing \\( \\mathbb{E}_{p_{\\theta}}[\\eta_{\\tau}(g)] \\)."}, {"type": "heading", "lvl": 2, "value": "Practical Algorithm:", "md": "## Practical Algorithm:"}, {"type": "text", "value": "As mentioned in Section 4, the derived gradient is highly sample inefficient. We employ established methods to improve the performance of policy gradients like importance sampling.\n\nThe first modification is to use importance sampling weights to allow sampling from the previous policy \\( \\theta' \\). The gradient now looks like:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta'}} \\left[ \\frac{\\pi_{\\theta}(\\tau)}{\\pi_{\\theta'}(\\tau)} \\sum_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) f' \\frac{p_{\\theta}(s_t)}{p_g(s_t)} \\right]\n$$\n\nTo reduce the variance in the gradients, the objective can be modified to use the causal connections in the MDP and ensure that the action taken at step \\( t \\) only affects rewards at times \\( t' \\rightarrow [t, T] \\). Moreover, a discount factor \\( \\gamma \\) is used to prevent the sum \\( \\sum_{t'=t}^{T} \\gamma^{t'} f' p_g(s_t) \\) from exploding.\n\nAdditionally, the expectation is modified to be over states rather than trajectories:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s,t \\sim p_{\\theta'}} \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta'}(a_t|s_t)} \\sum_{t'=t} \\gamma^{t'} f' p_{\\theta}(s_t) \\right]\n$$\n\nThis gradient computation is still inefficient, because even though the samples are from a previous policy \\( \\pi_{\\theta'} \\), it still needs to compute \\( \\sum_{t'=t}^{T} \\gamma^{t'} f' p_g(s_t) p_{\\theta}(s_t) \\), requiring iteration through full trajectories.\n\nWe can add a bias to the gradient by modifying \\( f' \\frac{p_{\\theta}(s_t)}{p_g(s_t)} \\) to \\( f' \\frac{p_{\\theta'}(s_t)}{p_g(s_t)} \\) in the objective. To ensure the bias is small, an additional constraint needs to be added to keep \\( \\theta' \\) close to \\( \\theta \\). Following the literature from natural gradients, the constraint we add is \\( D_{KL}(p_{\\theta'} || p_{\\theta}) \\). Proximal Policy Optimization (Schulman et al., 2017) showed that in practical scenarios, a clipped objective can be enough to do away with the KL regularization term. The final objective that we use is:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s,t \\sim p_{\\theta'}} \\left[ \\min\\left(r_{\\theta}(s_t) F_{\\theta'}(s_t), \\text{clip}\\left(r_{\\theta}(s_t), 1 - \\epsilon, 1 + \\epsilon\\right) F_{\\theta'}(s_t)\\right) \\right]\n$$\n\nwhere \\( r_{\\theta}(s_t) = \\nabla_{\\theta} \\pi_{\\theta}(a_t|s_t) / \\pi_{\\theta'}(a_t|s_t) \\) and \\( F_{\\theta'}(s_t) = \\sum_{t} p_g(s_t) \\).", "md": "As mentioned in Section 4, the derived gradient is highly sample inefficient. We employ established methods to improve the performance of policy gradients like importance sampling.\n\nThe first modification is to use importance sampling weights to allow sampling from the previous policy \\( \\theta' \\). The gradient now looks like:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta'}} \\left[ \\frac{\\pi_{\\theta}(\\tau)}{\\pi_{\\theta'}(\\tau)} \\sum_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) f' \\frac{p_{\\theta}(s_t)}{p_g(s_t)} \\right]\n$$\n\nTo reduce the variance in the gradients, the objective can be modified to use the causal connections in the MDP and ensure that the action taken at step \\( t \\) only affects rewards at times \\( t' \\rightarrow [t, T] \\). Moreover, a discount factor \\( \\gamma \\) is used to prevent the sum \\( \\sum_{t'=t}^{T} \\gamma^{t'} f' p_g(s_t) \\) from exploding.\n\nAdditionally, the expectation is modified to be over states rather than trajectories:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s,t \\sim p_{\\theta'}} \\left[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta'}(a_t|s_t)} \\sum_{t'=t} \\gamma^{t'} f' p_{\\theta}(s_t) \\right]\n$$\n\nThis gradient computation is still inefficient, because even though the samples are from a previous policy \\( \\pi_{\\theta'} \\), it still needs to compute \\( \\sum_{t'=t}^{T} \\gamma^{t'} f' p_g(s_t) p_{\\theta}(s_t) \\), requiring iteration through full trajectories.\n\nWe can add a bias to the gradient by modifying \\( f' \\frac{p_{\\theta}(s_t)}{p_g(s_t)} \\) to \\( f' \\frac{p_{\\theta'}(s_t)}{p_g(s_t)} \\) in the objective. To ensure the bias is small, an additional constraint needs to be added to keep \\( \\theta' \\) close to \\( \\theta \\). Following the literature from natural gradients, the constraint we add is \\( D_{KL}(p_{\\theta'} || p_{\\theta}) \\). Proximal Policy Optimization (Schulman et al., 2017) showed that in practical scenarios, a clipped objective can be enough to do away with the KL regularization term. The final objective that we use is:\n\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s,t \\sim p_{\\theta'}} \\left[ \\min\\left(r_{\\theta}(s_t) F_{\\theta'}(s_t), \\text{clip}\\left(r_{\\theta}(s_t), 1 - \\epsilon, 1 + \\epsilon\\right) F_{\\theta'}(s_t)\\right) \\right]\n$$\n\nwhere \\( r_{\\theta}(s_t) = \\nabla_{\\theta} \\pi_{\\theta}(a_t|s_t) / \\pi_{\\theta'}(a_t|s_t) \\) and \\( F_{\\theta'}(s_t) = \\sum_{t} p_g(s_t) \\)."}, {"type": "heading", "lvl": 2, "value": "Discounted State-Visitations:", "md": "## Discounted State-Visitations:"}, {"type": "text", "value": "The state-visitation distribution defined so far has not considered a discount factor. To include discounting, the state-visitation frequency gets modified to \\( \\eta_{\\tau}^{(s)} = \\sum_{t=1}^{T} \\gamma^{t-1} 1_{s_t=s} \\). Throughout", "md": "The state-visitation distribution defined so far has not considered a discount factor. To include discounting, the state-visitation frequency gets modified to \\( \\eta_{\\tau}^{(s)} = \\sum_{t=1}^{T} \\gamma^{t-1} 1_{s_t=s} \\). Throughout"}]}, {"page": 21, "text": "the derivation of the gradient, we used                      \u03b7\u03c4(s)f(s)ds =  T           t=1 f(st) but this will be modified to\n   \u03b7\u03c4(s)f(s)ds =  T           t=1 \u03b3tf(st). the corresponding gradient will be,\n                  \u2207\u03b8J(\u03b8) = 1                             T    \u03b3t\u2207\u03b8 log \u03c0\u03b8(at|st)               T   \u03b3tf \u2032 p\u03b8(st)             .            (12)\n                                   T E\u03c4\u223cp\u03b8(\u03c4)           t=1                                  t=1            p g(st)\nThis gradient can be modified as before to,\n                      \u2207\u03b8J(\u03b8) = Est,at\u223cp\u03b8(st,at)                \u03b3t \u2207\u03b8 log \u03c0\u03b8(at|st)          T    \u03b3t\u2032f \u2032 p\u03b8(st)          .                (13)\n                                                                                           t\u2032=t            pg(st)\nAdding importance sampling to the gradient in Equation 13 will give a gradient very similar to\nEquation 10. In fact, Equation 10 is a biased estimate for the gradient of the f-divergence between\nthe discounted state-visitation distribution and the goal distribution. We can use either of the two\ngradients but Equation 10 will be preferred for long horizon tasks.\nC      Gridworld Experiments\nC.1      Description of the task\nThe task involves navigating a gridworld to reach the goal state which is\nenclosed in a room. The agent can move in any of the four directions and\nhas no idea where the goal is. It needs to explore the gridworld to fi                              nd\nthe path around the room to reach the goal. The task is further elaborated\nin Figure 7. The green square represents the agent position while the red\nsquare represents the goal.\nState: The state visible to the policy is simply the normalized x and y\ncoordinates.\nAction: The action is discrete categorical distribution with four categories\none for each - left, top, right and bottom.                                                                Figure 7:          Description\n                                                                                                           of the gridworld:             The\nReward: The task reward is 1 at the goal and 0 everywhere else. f-PG                                       bold lines show the walls,\ndoes not require rewards but the baselines use task rewards.                                               green square is the start\n                                                                                                           position       and      the    red\nC.2      Performance of f-PG                                                                               square is the goal.\nIn Section 5.1, we had compared fkl-PG and rkl-PG with AIM (Durugkar et al., 2021) and GAIL\n(Ho & Ermon, 2016). In Figure 8 we present additional baselines AIRL (Fu et al., 2017) and f-AIRL\n(Ghasemipour et al., 2019).\nD      Visualizing the learning signals\nD.1     Description of the task\nTo visualize the learning signals, we use the Reacher envi-\nronment (Figure 9) (Todorov et al., 2012). The task involves\nrotating a reacher arm (two joints with one end fixed). The ap-\nplied actions (torques) would rotate the arm so that the free end\nreaches the goal. The goal is fixed to be at (\u22120.21, 0) and the\ngoal distribution is a normal centred at the goal with a standard\ndeviation of 0.02.\nState: The state of the original environment contains several\nthings but here we simplify the state space to simply be the\nposition of the free end and the target or the goal position.\nActions:        The actions are two dimensional real numbers in\n[\u22121, 1] which correspond to the torques applied on the two joint                             Figure 9: The Reacher environment\nrespectively.                                                                                with fixed goal.\n                                                                      21", "md": "the derivation of the gradient, we used $$\\eta\\tau(s)f(s)ds = \\sum_{t=1}^{T} f(s_t)$$ but this will be modified to\n$$\\eta\\tau(s)f(s)ds = \\sum_{t=1}^{T} \\gamma_t f(s_t)$$. the corresponding gradient will be,\n$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\gamma_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t=1}^{T} \\gamma_t f'(s_t)$$. (12)\n\nThis gradient can be modified as before to,\n$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s_t,a_t \\sim p_{\\theta}(s_t,a_t)} [\\gamma_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t'=t} p g(s_t)]$$. (13)\n\nAdding importance sampling to the gradient in Equation 13 will give a gradient very similar to\nEquation 10. In fact, Equation 10 is a biased estimate for the gradient of the f-divergence between\nthe discounted state-visitation distribution and the goal distribution. We can use either of the two\ngradients but Equation 10 will be preferred for long horizon tasks.\n\n## Gridworld Experiments\n\n### Description of the task\n\nThe task involves navigating a gridworld to reach the goal state which is\nenclosed in a room. The agent can move in any of the four directions and\nhas no idea where the goal is. It needs to explore the gridworld to find\nthe path around the room to reach the goal. The task is further elaborated\nin Figure 7. The green square represents the agent position while the red\nsquare represents the goal.\n\nState: The state visible to the policy is simply the normalized x and y\ncoordinates.\n\nAction: The action is discrete categorical distribution with four categories\none for each - left, top, right and bottom.\n\nReward: The task reward is 1 at the goal and 0 everywhere else. f-PG\ndoes not require rewards but the baselines use task rewards.\n\n### Performance of f-PG\n\nIn Section 5.1, we had compared fkl-PG and rkl-PG with AIM (Durugkar et al., 2021) and GAIL\n(Ho & Ermon, 2016). In Figure 8 we present additional baselines AIRL (Fu et al., 2017) and f-AIRL\n(Ghasemipour et al., 2019).\n\n## Visualizing the learning signals\n\n### Description of the task\n\nTo visualize the learning signals, we use the Reacher environment (Figure 9) (Todorov et al., 2012). The task involves\nrotating a reacher arm (two joints with one end fixed). The applied actions (torques) would rotate the arm so that the free end\nreaches the goal. The goal is fixed to be at (-0.21, 0) and the\ngoal distribution is a normal centred at the goal with a standard\ndeviation of 0.02.\n\nState: The state of the original environment contains several\nthings but here we simplify the state space to simply be the\nposition of the free end and the target or the goal position.\n\nActions: The actions are two dimensional real numbers in\n[-1, 1] which correspond to the torques applied on the two joint respectively.", "images": [{"name": "page-21-1.jpg", "height": 119, "width": 119, "x": 375, "y": 573}, {"name": "page-21-0.jpg", "height": 88, "width": 88, "x": 410, "y": 311}], "items": [{"type": "text", "value": "the derivation of the gradient, we used $$\\eta\\tau(s)f(s)ds = \\sum_{t=1}^{T} f(s_t)$$ but this will be modified to\n$$\\eta\\tau(s)f(s)ds = \\sum_{t=1}^{T} \\gamma_t f(s_t)$$. the corresponding gradient will be,\n$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\gamma_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t=1}^{T} \\gamma_t f'(s_t)$$. (12)\n\nThis gradient can be modified as before to,\n$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s_t,a_t \\sim p_{\\theta}(s_t,a_t)} [\\gamma_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t'=t} p g(s_t)]$$. (13)\n\nAdding importance sampling to the gradient in Equation 13 will give a gradient very similar to\nEquation 10. In fact, Equation 10 is a biased estimate for the gradient of the f-divergence between\nthe discounted state-visitation distribution and the goal distribution. We can use either of the two\ngradients but Equation 10 will be preferred for long horizon tasks.", "md": "the derivation of the gradient, we used $$\\eta\\tau(s)f(s)ds = \\sum_{t=1}^{T} f(s_t)$$ but this will be modified to\n$$\\eta\\tau(s)f(s)ds = \\sum_{t=1}^{T} \\gamma_t f(s_t)$$. the corresponding gradient will be,\n$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\gamma_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t=1}^{T} \\gamma_t f'(s_t)$$. (12)\n\nThis gradient can be modified as before to,\n$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s_t,a_t \\sim p_{\\theta}(s_t,a_t)} [\\gamma_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t'=t} p g(s_t)]$$. (13)\n\nAdding importance sampling to the gradient in Equation 13 will give a gradient very similar to\nEquation 10. In fact, Equation 10 is a biased estimate for the gradient of the f-divergence between\nthe discounted state-visitation distribution and the goal distribution. We can use either of the two\ngradients but Equation 10 will be preferred for long horizon tasks."}, {"type": "heading", "lvl": 2, "value": "Gridworld Experiments", "md": "## Gridworld Experiments"}, {"type": "heading", "lvl": 3, "value": "Description of the task", "md": "### Description of the task"}, {"type": "text", "value": "The task involves navigating a gridworld to reach the goal state which is\nenclosed in a room. The agent can move in any of the four directions and\nhas no idea where the goal is. It needs to explore the gridworld to find\nthe path around the room to reach the goal. The task is further elaborated\nin Figure 7. The green square represents the agent position while the red\nsquare represents the goal.\n\nState: The state visible to the policy is simply the normalized x and y\ncoordinates.\n\nAction: The action is discrete categorical distribution with four categories\none for each - left, top, right and bottom.\n\nReward: The task reward is 1 at the goal and 0 everywhere else. f-PG\ndoes not require rewards but the baselines use task rewards.", "md": "The task involves navigating a gridworld to reach the goal state which is\nenclosed in a room. The agent can move in any of the four directions and\nhas no idea where the goal is. It needs to explore the gridworld to find\nthe path around the room to reach the goal. The task is further elaborated\nin Figure 7. The green square represents the agent position while the red\nsquare represents the goal.\n\nState: The state visible to the policy is simply the normalized x and y\ncoordinates.\n\nAction: The action is discrete categorical distribution with four categories\none for each - left, top, right and bottom.\n\nReward: The task reward is 1 at the goal and 0 everywhere else. f-PG\ndoes not require rewards but the baselines use task rewards."}, {"type": "heading", "lvl": 3, "value": "Performance of f-PG", "md": "### Performance of f-PG"}, {"type": "text", "value": "In Section 5.1, we had compared fkl-PG and rkl-PG with AIM (Durugkar et al., 2021) and GAIL\n(Ho & Ermon, 2016). In Figure 8 we present additional baselines AIRL (Fu et al., 2017) and f-AIRL\n(Ghasemipour et al., 2019).", "md": "In Section 5.1, we had compared fkl-PG and rkl-PG with AIM (Durugkar et al., 2021) and GAIL\n(Ho & Ermon, 2016). In Figure 8 we present additional baselines AIRL (Fu et al., 2017) and f-AIRL\n(Ghasemipour et al., 2019)."}, {"type": "heading", "lvl": 2, "value": "Visualizing the learning signals", "md": "## Visualizing the learning signals"}, {"type": "heading", "lvl": 3, "value": "Description of the task", "md": "### Description of the task"}, {"type": "text", "value": "To visualize the learning signals, we use the Reacher environment (Figure 9) (Todorov et al., 2012). The task involves\nrotating a reacher arm (two joints with one end fixed). The applied actions (torques) would rotate the arm so that the free end\nreaches the goal. The goal is fixed to be at (-0.21, 0) and the\ngoal distribution is a normal centred at the goal with a standard\ndeviation of 0.02.\n\nState: The state of the original environment contains several\nthings but here we simplify the state space to simply be the\nposition of the free end and the target or the goal position.\n\nActions: The actions are two dimensional real numbers in\n[-1, 1] which correspond to the torques applied on the two joint respectively.", "md": "To visualize the learning signals, we use the Reacher environment (Figure 9) (Todorov et al., 2012). The task involves\nrotating a reacher arm (two joints with one end fixed). The applied actions (torques) would rotate the arm so that the free end\nreaches the goal. The goal is fixed to be at (-0.21, 0) and the\ngoal distribution is a normal centred at the goal with a standard\ndeviation of 0.02.\n\nState: The state of the original environment contains several\nthings but here we simplify the state space to simply be the\nposition of the free end and the target or the goal position.\n\nActions: The actions are two dimensional real numbers in\n[-1, 1] which correspond to the torques applied on the two joint respectively."}]}, {"page": 22, "text": "             (a) FKL                                (b) RKL                                  (c) AIM\n            (d) GAIL                                (e) AIRL                                (f) FAIRL\nFigure 8: Gridworld: The agent needs to move from the green circle to the red circle. The state visitations of\nthe final policies are shown when using our framework for training (fkl, rkl) compared with AIM and GAIL\ntrained on top of soft Q learning.\nReward: The reward is sparse i.e., 1 when the goal is reached by the tip of the arm. But f-PG does\nnot use rewards for training policies.\nD.2    Comparing different f-PG\nFigure 10 shows the evolution of the learning signals for the environment. The red regions correspond\nto signals having a higher value while the darker blue regions correspond to signals with low value.\nFor fkl, the scale of these rewards generally vary from \u221210 to 5 while for \u03c72, the scale varies from\n\u2212600 to \u221250. Also, for the same objective, as the policy trains, these scales generally get smaller in\nmagnitude.\nThe following can be observed from these plots:\n      1. In all the cases, the signals are maximum at the the goal pulling the state-visitations towards\n          the goal.\n      2. All of these also push for exploration. This is most pronounced in fkl and \u03c72. These\n          provide significant push towards the unexplored regions which show their inclination towards\n          entropy-maximization, confirming the theory (Lemma 5.1).\nE    PointMaze experiments\nPointMaze (Fu et al., 2020) are continuous state-space domains where the agent needs to navigate to\nthe goal in the 2D maze. The agent and the goal are spawned at a random location in the maze for\nevery episode. There are three levels based on the difficulty of the maze as shown in Figure 11.\nState: The state consists of the agent\u2019s 2D position and the velocity in the maze. The goal position is\nappended to the state.\nAction: The actions are 2D real numbers in the range [\u22121, 1] correspond to the force applied to the\nagent in each of the two directions.\n                                                       22", "md": "# Gridworld and PointMaze Experiments\n\n## Gridworld Experiments\n\n(a) FKL &emsp; (b) RKL &emsp; (c) AIM\n\n(d) GAIL &emsp; (e) AIRL &emsp; (f) FAIRL\n\nFigure 8: Gridworld: The agent needs to move from the green circle to the red circle. The state visitations of the final policies are shown when using our framework for training (fkl, rkl) compared with AIM and GAIL trained on top of soft Q learning.\n\nReward: The reward is sparse i.e., 1 when the goal is reached by the tip of the arm. But f-PG does not use rewards for training policies.\n\n### Comparing different f-PG\n\nFigure 10 shows the evolution of the learning signals for the environment. The red regions correspond to signals having a higher value while the darker blue regions correspond to signals with low value. For fkl, the scale of these rewards generally vary from $$-10$$ to $$5$$ while for $$\\chi^2$$, the scale varies from $$-600$$ to $$-50$$. Also, for the same objective, as the policy trains, these scales generally get smaller in magnitude.\n\nThe following can be observed from these plots:\n\n1. In all the cases, the signals are maximum at the goal pulling the state-visitations towards the goal.\n2. All of these also push for exploration. This is most pronounced in fkl and $\\chi^2$. These provide significant push towards the unexplored regions which show their inclination towards entropy-maximization, confirming the theory (Lemma 5.1).\n\n## PointMaze Experiments\n\nPointMaze (Fu et al., 2020) are continuous state-space domains where the agent needs to navigate to the goal in the 2D maze. The agent and the goal are spawned at a random location in the maze for every episode. There are three levels based on the difficulty of the maze as shown in Figure 11.\n\nState: The state consists of the agent\u2019s 2D position and the velocity in the maze. The goal position is appended to the state.\n\nAction: The actions are 2D real numbers in the range $$[-1, 1]$$ correspond to the force applied to the agent in each of the two directions.\n\n22", "images": [{"name": "page-22-1.jpg", "height": 120, "width": 119, "x": 246, "y": 71}, {"name": "page-22-0.jpg", "height": 119, "width": 119, "x": 108, "y": 72}, {"name": "page-22-3.jpg", "height": 120, "width": 119, "x": 108, "y": 208}, {"name": "page-22-5.jpg", "height": 120, "width": 119, "x": 385, "y": 208}, {"name": "page-22-2.jpg", "height": 120, "width": 119, "x": 385, "y": 71}, {"name": "page-22-4.jpg", "height": 120, "width": 119, "x": 246, "y": 207}], "items": [{"type": "heading", "lvl": 1, "value": "Gridworld and PointMaze Experiments", "md": "# Gridworld and PointMaze Experiments"}, {"type": "heading", "lvl": 2, "value": "Gridworld Experiments", "md": "## Gridworld Experiments"}, {"type": "text", "value": "(a) FKL &emsp; (b) RKL &emsp; (c) AIM\n\n(d) GAIL &emsp; (e) AIRL &emsp; (f) FAIRL\n\nFigure 8: Gridworld: The agent needs to move from the green circle to the red circle. The state visitations of the final policies are shown when using our framework for training (fkl, rkl) compared with AIM and GAIL trained on top of soft Q learning.\n\nReward: The reward is sparse i.e., 1 when the goal is reached by the tip of the arm. But f-PG does not use rewards for training policies.", "md": "(a) FKL &emsp; (b) RKL &emsp; (c) AIM\n\n(d) GAIL &emsp; (e) AIRL &emsp; (f) FAIRL\n\nFigure 8: Gridworld: The agent needs to move from the green circle to the red circle. The state visitations of the final policies are shown when using our framework for training (fkl, rkl) compared with AIM and GAIL trained on top of soft Q learning.\n\nReward: The reward is sparse i.e., 1 when the goal is reached by the tip of the arm. But f-PG does not use rewards for training policies."}, {"type": "heading", "lvl": 3, "value": "Comparing different f-PG", "md": "### Comparing different f-PG"}, {"type": "text", "value": "Figure 10 shows the evolution of the learning signals for the environment. The red regions correspond to signals having a higher value while the darker blue regions correspond to signals with low value. For fkl, the scale of these rewards generally vary from $$-10$$ to $$5$$ while for $$\\chi^2$$, the scale varies from $$-600$$ to $$-50$$. Also, for the same objective, as the policy trains, these scales generally get smaller in magnitude.\n\nThe following can be observed from these plots:\n\n1. In all the cases, the signals are maximum at the goal pulling the state-visitations towards the goal.\n2. All of these also push for exploration. This is most pronounced in fkl and $\\chi^2$. These provide significant push towards the unexplored regions which show their inclination towards entropy-maximization, confirming the theory (Lemma 5.1).", "md": "Figure 10 shows the evolution of the learning signals for the environment. The red regions correspond to signals having a higher value while the darker blue regions correspond to signals with low value. For fkl, the scale of these rewards generally vary from $$-10$$ to $$5$$ while for $$\\chi^2$$, the scale varies from $$-600$$ to $$-50$$. Also, for the same objective, as the policy trains, these scales generally get smaller in magnitude.\n\nThe following can be observed from these plots:\n\n1. In all the cases, the signals are maximum at the goal pulling the state-visitations towards the goal.\n2. All of these also push for exploration. This is most pronounced in fkl and $\\chi^2$. These provide significant push towards the unexplored regions which show their inclination towards entropy-maximization, confirming the theory (Lemma 5.1)."}, {"type": "heading", "lvl": 2, "value": "PointMaze Experiments", "md": "## PointMaze Experiments"}, {"type": "text", "value": "PointMaze (Fu et al., 2020) are continuous state-space domains where the agent needs to navigate to the goal in the 2D maze. The agent and the goal are spawned at a random location in the maze for every episode. There are three levels based on the difficulty of the maze as shown in Figure 11.\n\nState: The state consists of the agent\u2019s 2D position and the velocity in the maze. The goal position is appended to the state.\n\nAction: The actions are 2D real numbers in the range $$[-1, 1]$$ correspond to the force applied to the agent in each of the two directions.\n\n22", "md": "PointMaze (Fu et al., 2020) are continuous state-space domains where the agent needs to navigate to the goal in the 2D maze. The agent and the goal are spawned at a random location in the maze for every episode. There are three levels based on the difficulty of the maze as shown in Figure 11.\n\nState: The state consists of the agent\u2019s 2D position and the velocity in the maze. The goal position is appended to the state.\n\nAction: The actions are 2D real numbers in the range $$[-1, 1]$$ correspond to the force applied to the agent in each of the two directions.\n\n22"}]}, {"page": 23, "text": " Reward: Although f-PG does not use rewards, the baselines use the task reward which is sparse (1\n when the goal is reached and 0 everywhere else).\n                                                                                  E\n Figure 11: Description of PointMaze environments: PointMaze-U (left), PointMaze-Medium (mid-\n dle), PointMaze-Large(right).\n For the experiments in Section 6.2, the initial and goal states are sampled uniformly over all the\n\u201cVALID\u201d states i.e., states that can be reached by the agent. Such an initialization allows discriminator-\n based methods to fulfill their coverage assumptions. In Section 6.3, the initialization procedure is\n modified so that the initial state and the goal state are considerably far. This is done by restricting the\n sampling of the initial and goal states from disjoint (considerably far away) distributions as shown in\n Figure 5.\n                                                     23", "md": "Reward: Although f-PG does not use rewards, the baselines use the task reward which is sparse (1\nwhen the goal is reached and 0 everywhere else).\n\n$$\n\\begin{array}{c}\n\\text{Figure 11: Description of PointMaze environments:} \\\\\n\\text{PointMaze-U (left), PointMaze-Medium (middle), PointMaze-Large (right).}\n\\end{array}\n$$\n\nFor the experiments in Section 6.2, the initial and goal states are sampled uniformly over all the \"VALID\" states i.e., states that can be reached by the agent. Such an initialization allows discriminator-based methods to fulfill their coverage assumptions. In Section 6.3, the initialization procedure is modified so that the initial state and the goal state are considerably far. This is done by restricting the sampling of the initial and goal states from disjoint (considerably far away) distributions as shown in Figure 5.\n\n23", "images": [{"name": "page-23-0.jpg", "height": 119, "width": 119, "x": 108, "y": 107}, {"name": "page-23-1.jpg", "height": 119, "width": 119, "x": 246, "y": 107}, {"name": "page-23-2.jpg", "height": 119, "width": 119, "x": 385, "y": 107}], "items": [{"type": "text", "value": "Reward: Although f-PG does not use rewards, the baselines use the task reward which is sparse (1\nwhen the goal is reached and 0 everywhere else).\n\n$$\n\\begin{array}{c}\n\\text{Figure 11: Description of PointMaze environments:} \\\\\n\\text{PointMaze-U (left), PointMaze-Medium (middle), PointMaze-Large (right).}\n\\end{array}\n$$\n\nFor the experiments in Section 6.2, the initial and goal states are sampled uniformly over all the \"VALID\" states i.e., states that can be reached by the agent. Such an initialization allows discriminator-based methods to fulfill their coverage assumptions. In Section 6.3, the initialization procedure is modified so that the initial state and the goal state are considerably far. This is done by restricting the sampling of the initial and goal states from disjoint (considerably far away) distributions as shown in Figure 5.\n\n23", "md": "Reward: Although f-PG does not use rewards, the baselines use the task reward which is sparse (1\nwhen the goal is reached and 0 everywhere else).\n\n$$\n\\begin{array}{c}\n\\text{Figure 11: Description of PointMaze environments:} \\\\\n\\text{PointMaze-U (left), PointMaze-Medium (middle), PointMaze-Large (right).}\n\\end{array}\n$$\n\nFor the experiments in Section 6.2, the initial and goal states are sampled uniformly over all the \"VALID\" states i.e., states that can be reached by the agent. Such an initialization allows discriminator-based methods to fulfill their coverage assumptions. In Section 6.3, the initialization procedure is modified so that the initial state and the goal state are considerably far. This is done by restricting the sampling of the initial and goal states from disjoint (considerably far away) distributions as shown in Figure 5.\n\n23"}]}, {"page": 24, "text": "                                                      (a) FKL\n                                                      (b) RKL\n                                                       (c) JS\n                                                       (d) \u03c72\n Figure 10: Evolution of f \u2032( p\u03b8(s)\n                                pg(s)) along with the corresponding state-visitations for different f-divergences\n- fkl, rkl, js and \u03c72. The scales for the value of these signals are not shown but they are vary as the policy\n converges. f-PG provides dense signals for pushing towards exploration.\n                                                         24", "md": "**(a) FKL**\n\n**(b) RKL**\n\n**(c) JS**\n\n**(d) $\\chi^2$**\n\nFigure 10: Evolution of $$f'(p\\theta(s), p_g(s))$$ along with the corresponding state-visitations for different f-divergences - fkl, rkl, js and $$\\chi^2$$. The scales for the value of these signals are not shown but they are vary as the policy converges. f-PG provides dense signals for pushing towards exploration.\n\n24", "images": [{"name": "page-24-1.jpg", "height": 139, "width": 357, "x": 127, "y": 227}, {"name": "page-24-2.jpg", "height": 142, "width": 357, "x": 127, "y": 383}, {"name": "page-24-0.jpg", "height": 139, "width": 357, "x": 127, "y": 71}, {"name": "page-24-3.jpg", "height": 140, "width": 357, "x": 127, "y": 541}], "items": [{"type": "text", "value": "**(a) FKL**\n\n**(b) RKL**\n\n**(c) JS**\n\n**(d) $\\chi^2$**\n\nFigure 10: Evolution of $$f'(p\\theta(s), p_g(s))$$ along with the corresponding state-visitations for different f-divergences - fkl, rkl, js and $$\\chi^2$$. The scales for the value of these signals are not shown but they are vary as the policy converges. f-PG provides dense signals for pushing towards exploration.\n\n24", "md": "**(a) FKL**\n\n**(b) RKL**\n\n**(c) JS**\n\n**(d) $\\chi^2$**\n\nFigure 10: Evolution of $$f'(p\\theta(s), p_g(s))$$ along with the corresponding state-visitations for different f-divergences - fkl, rkl, js and $$\\chi^2$$. The scales for the value of these signals are not shown but they are vary as the policy converges. f-PG provides dense signals for pushing towards exploration.\n\n24"}]}], "job_id": "95aeab25-b57f-4c13-a28a-2d1262c440e6", "file_path": "./corpus/2310.06794v1.pdf"}