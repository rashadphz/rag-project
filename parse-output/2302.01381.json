{"pages": [{"page": 1, "text": "                         Effective Robustness against Natural Distribution\n                            Shifts for Models with Different Training Data\n                             Zhouxing Shi\u2217                 Nicholas Carlini                   Ananth Balashankar\n                                  UCLA                     Google Research                       Google Research\n                          zshi@cs.ucla.edu             ncarlini@google.com               ananthbshankar@google.com\narXiv:2302.01381v2  [cs.LG]  28 Oct 2023   Ludwig Schmidt                                   Cho-Jui Hsieh\n                                       University of Washington                             Google, UCLA\n                                   schmidt@cs.washington.edu                          chohsieh@cs.ucla.edu\n                                             Alex Beutel\u2217                                   Yao Qin\n                                               OpenAI                              UCSB, Google Research\n                                        alexb@openai.com                              yaoqin@ucsb.edu\n                                                                     Abstract\n                               \u201cEffective robustness\u201d measures the extra out-of-distribution (OOD) robustness\n                               beyond what can be predicted from the in-distribution (ID) performance. Existing\n                               effective robustness evaluations typically use a single test set such as ImageNet\n                               to evaluate the ID accuracy. This becomes problematic when evaluating models\n                               trained on different data distributions, e.g., comparing models trained on ImageNet\n                               vs. zero-shot language-image pre-trained models trained on LAION. In this paper,\n                               we propose a new evaluation metric to evaluate and compare the effective robustness\n                               of models trained on different data. To do this, we control for the accuracy on\n                               multiple ID test sets that cover the training distributions for all the evaluated models.\n                               Our new evaluation metric provides a better estimate of effective robustness when\n                               there are models with different training data. It may also explain the surprising\n                               effective robustness gains of zero-shot CLIP-like models exhibited in prior works\n                               that used ImageNet as the only ID test set, while the gains diminish under our new\n                               evaluation. Additional artifacts including interactive visualizations are provided at\n                               https://shizhouxing.github.io/effective-robustness.\n                     1    Introduction\n                     Robustness against distribution shifts is important for machine learning models to work reliably\n                     across various environments. For natural distribution shifts on image classification datasets, Taori\n                     et al. (2020) proposed the notion of effective robustness to control for in-distribution (ID) accuracy\n                     when evaluating out-of-distribution (OOD) accuracy. Following a long line of work that has found\n                     a strong correlation between ID and OOD accuracy on many test sets (Recht et al., 2019; Yadav &\n                     Bottou, 2019), effective robustness allows researchers to assess whether an apparently improved\n                     OOD accuracy is a result of effectively improved robustness or is simply an expected outcome of\n                     enhanced ID accuracy.\n                     Unfortunately, the current definition of effective robustness has a subtle limitation: it requires a fixed\n                     ID test set, which is typically ImageNet (Deng et al., 2009) when using ImageNet-like OOD test sets\n                        \u2217Work done while at Google.\n                     37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Effective Robustness against Natural Distribution Shifts\n\n# Effective Robustness against Natural Distribution Shifts for Models with Different Training Data\n\nZhouxing Shi* &emsp; Nicholas Carlini &emsp; Ananth Balashankar\n\nUCLA &emsp; Google Research &emsp; Google Research\n\nzshi@cs.ucla.edu &emsp; ncarlini@google.com &emsp; ananthbshankar@google.com\n\nLudwig Schmidt &emsp; Cho-Jui Hsieh\n\nUniversity of Washington &emsp; Google, UCLA\n\nschmidt@cs.washington.edu &emsp; chohsieh@cs.ucla.edu\n\nAlex Beutel* &emsp; Yao Qin\n\nOpenAI &emsp; UCSB, Google Research\n\nalexb@openai.com &emsp; yaoqin@ucsb.edu\n\n## Abstract\n\n\"Effective robustness\" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageNet as the only ID test set, while the gains diminish under our new evaluation. Additional artifacts including interactive visualizations are provided at https://shizhouxing.github.io/effective-robustness.\n\n## 1 Introduction\n\nRobustness against distribution shifts is important for machine learning models to work reliably across various environments. For natural distribution shifts on image classification datasets, Taori et al. (2020) proposed the notion of effective robustness to control for in-distribution (ID) accuracy when evaluating out-of-distribution (OOD) accuracy. Following a long line of work that has found a strong correlation between ID and OOD accuracy on many test sets (Recht et al., 2019; Yadav & Bottou, 2019), effective robustness allows researchers to assess whether an apparently improved OOD accuracy is a result of effectively improved robustness or is simply an expected outcome of enhanced ID accuracy.\n\nUnfortunately, the current definition of effective robustness has a subtle limitation: it requires a fixed ID test set, which is typically ImageNet (Deng et al., 2009) when using ImageNet-like OOD test sets. *Work done while at Google.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Effective Robustness against Natural Distribution Shifts", "md": "# Effective Robustness against Natural Distribution Shifts"}, {"type": "heading", "lvl": 1, "value": "Effective Robustness against Natural Distribution Shifts for Models with Different Training Data", "md": "# Effective Robustness against Natural Distribution Shifts for Models with Different Training Data"}, {"type": "text", "value": "Zhouxing Shi* &emsp; Nicholas Carlini &emsp; Ananth Balashankar\n\nUCLA &emsp; Google Research &emsp; Google Research\n\nzshi@cs.ucla.edu &emsp; ncarlini@google.com &emsp; ananthbshankar@google.com\n\nLudwig Schmidt &emsp; Cho-Jui Hsieh\n\nUniversity of Washington &emsp; Google, UCLA\n\nschmidt@cs.washington.edu &emsp; chohsieh@cs.ucla.edu\n\nAlex Beutel* &emsp; Yao Qin\n\nOpenAI &emsp; UCSB, Google Research\n\nalexb@openai.com &emsp; yaoqin@ucsb.edu", "md": "Zhouxing Shi* &emsp; Nicholas Carlini &emsp; Ananth Balashankar\n\nUCLA &emsp; Google Research &emsp; Google Research\n\nzshi@cs.ucla.edu &emsp; ncarlini@google.com &emsp; ananthbshankar@google.com\n\nLudwig Schmidt &emsp; Cho-Jui Hsieh\n\nUniversity of Washington &emsp; Google, UCLA\n\nschmidt@cs.washington.edu &emsp; chohsieh@cs.ucla.edu\n\nAlex Beutel* &emsp; Yao Qin\n\nOpenAI &emsp; UCSB, Google Research\n\nalexb@openai.com &emsp; yaoqin@ucsb.edu"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "\"Effective robustness\" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageNet as the only ID test set, while the gains diminish under our new evaluation. Additional artifacts including interactive visualizations are provided at https://shizhouxing.github.io/effective-robustness.", "md": "\"Effective robustness\" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageNet as the only ID test set, while the gains diminish under our new evaluation. Additional artifacts including interactive visualizations are provided at https://shizhouxing.github.io/effective-robustness."}, {"type": "heading", "lvl": 2, "value": "1 Introduction", "md": "## 1 Introduction"}, {"type": "text", "value": "Robustness against distribution shifts is important for machine learning models to work reliably across various environments. For natural distribution shifts on image classification datasets, Taori et al. (2020) proposed the notion of effective robustness to control for in-distribution (ID) accuracy when evaluating out-of-distribution (OOD) accuracy. Following a long line of work that has found a strong correlation between ID and OOD accuracy on many test sets (Recht et al., 2019; Yadav & Bottou, 2019), effective robustness allows researchers to assess whether an apparently improved OOD accuracy is a result of effectively improved robustness or is simply an expected outcome of enhanced ID accuracy.\n\nUnfortunately, the current definition of effective robustness has a subtle limitation: it requires a fixed ID test set, which is typically ImageNet (Deng et al., 2009) when using ImageNet-like OOD test sets. *Work done while at Google.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Robustness against distribution shifts is important for machine learning models to work reliably across various environments. For natural distribution shifts on image classification datasets, Taori et al. (2020) proposed the notion of effective robustness to control for in-distribution (ID) accuracy when evaluating out-of-distribution (OOD) accuracy. Following a long line of work that has found a strong correlation between ID and OOD accuracy on many test sets (Recht et al., 2019; Yadav & Bottou, 2019), effective robustness allows researchers to assess whether an apparently improved OOD accuracy is a result of effectively improved robustness or is simply an expected outcome of enhanced ID accuracy.\n\nUnfortunately, the current definition of effective robustness has a subtle limitation: it requires a fixed ID test set, which is typically ImageNet (Deng et al., 2009) when using ImageNet-like OOD test sets. *Work done while at Google.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "in Taori et al. (2020) or CIFAR-10 (Krizhevsky et al., 2009) when using CIFAR-like OOD test sets\nin Miller et al. (2021). It is acceptable when models are trained predominately on only one dataset.\nHowever, the emergence of many large-scale models trained on significantly different datasets makes\nit necessary to evaluate and compare models trained on different data distributions, under which it\nbecomes unclear which ID test set should be used.\nIn particular, models from Contrastive Language-Image Pre-training, such as CLIP (Radford et al.,\n2021) and ALIGN (Jia et al., 2021) have recently exhibited unprecedented effective robustness gains\nduring zero-shot inference (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022). However\nthese previous works simply take ImageNet as the single ID test set, even though the models are not\ntrained on ImageNet. We demonstrate that the results of evaluating effective robustness using a single\nID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise\ntreatment on the ID test set in existing works could end up exaggerating the effective robustness of\nzero-shot CLIP models compared to models that are exactly trained on ImageNet.\nIn this paper, we propose to more precisely evaluate and compare the effective robustness of models\ntrained on different datasets. Instead of controlling for a single ID accuracy that may bias towards\nmodels from a particular training distribution, we propose to use multiple ID test sets that cover the\ntraining distributions of all the models. In particular, previous works performed single-dimensional\nlinear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\net al., 2020). And they then evaluate the actual OOD accuracy of the models beyond the expected\nvalue that can be predicted from the fitting line, as the effective robustness. We expand on this\ndefinition by allowing for multiple ID test sets, and perform multi-dimensional linear regression to fit\na plane to predict OOD accuracy from the accuracy on multiple ID test sets.\nIn summary, we make the following contributions:\n\u2022 We reveal a limitation in the existing effective robustness evaluation when used to compare models\n  trained on different data distributions.\n\u2022 We then propose a new effective robustness evaluation which uses multiple ID test sets to more\n  precisely compare the effective robustness of models trained on different data.\n\u2022 We show that the OOD accuracy of various models including zero-shot CLIP models can usually\n  be better predicted from accuracies on multiple ID test sets compared to using only one ID test set.\n\u2022 Our results provide new understandings on the effective robustness gains of CLIP-like models\n  observed in prior works only using ImageNet as the ID test set, while the gains diminish under our\n  new evaluation.\n2    Background of Effective Robustness\nUnder natural distribution shifts, the OOD accuracy of a model is often correlated with the ID\naccuracy. After applying a logit transformation on the accuracy, a linear trend between the trans-\nformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from\nImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky\net al., 2009) to CIFAR-10.2 (Hendrycks & Dietterich, 2018)) and models with various architectures\nand training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most\nmodels showing higher OOD accuracies naturally resulted from better ID performance.\nTo eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed\neffective robustness that measures the OOD performance beyond the expected OOD accuracy given\nthe ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend\nof baseline models. Since they only use a single ID test set, we refer to this version of effective\nrobustness as single-ID effective robustness.\nSuppose there are n baseline models f1, f2, \u00b7 \u00b7 \u00b7 fn. A baseline function \u02dc\u03b2(x) is constructed to predict\nthe OOD accuracy of each baseline model, accood(fi) (1 \u2264        i \u2264 n), given the single ID accuracy of\nthe model x=accid(fi). The baseline function is instantiated as:\n                                     \u02dc\n                                    \u03b2(x) = expit(w logit(x) + b),                                      (1)\n                                                   x\nwhere w and b are parameters, logit(x) = ln(      1\u2212x) is the logit transformation, and expit(x) is the\ninverse of logit(x). Since logit(\u02dc\u03b2(x)) = w logit(x) + b, the baseline function is essentially a linear\n                                                    2", "md": "in Taori et al. (2020) or CIFAR-10 (Krizhevsky et al., 2009) when using CIFAR-like OOD test sets\nin Miller et al. (2021). It is acceptable when models are trained predominately on only one dataset.\nHowever, the emergence of many large-scale models trained on significantly different datasets makes\nit necessary to evaluate and compare models trained on different data distributions, under which it\nbecomes unclear which ID test set should be used.\nIn particular, models from Contrastive Language-Image Pre-training, such as CLIP (Radford et al.,\n2021) and ALIGN (Jia et al., 2021) have recently exhibited unprecedented effective robustness gains\nduring zero-shot inference (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022). However\nthese previous works simply take ImageNet as the single ID test set, even though the models are not\ntrained on ImageNet. We demonstrate that the results of evaluating effective robustness using a single\nID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise\ntreatment on the ID test set in existing works could end up exaggerating the effective robustness of\nzero-shot CLIP models compared to models that are exactly trained on ImageNet.\nIn this paper, we propose to more precisely evaluate and compare the effective robustness of models\ntrained on different datasets. Instead of controlling for a single ID accuracy that may bias towards\nmodels from a particular training distribution, we propose to use multiple ID test sets that cover the\ntraining distributions of all the models. In particular, previous works performed single-dimensional\nlinear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\net al., 2020). And they then evaluate the actual OOD accuracy of the models beyond the expected\nvalue that can be predicted from the fitting line, as the effective robustness. We expand on this\ndefinition by allowing for multiple ID test sets, and perform multi-dimensional linear regression to fit\na plane to predict OOD accuracy from the accuracy on multiple ID test sets.\n\nIn summary, we make the following contributions:\n- We reveal a limitation in the existing effective robustness evaluation when used to compare models\ntrained on different data distributions.\n- We then propose a new effective robustness evaluation which uses multiple ID test sets to more\nprecisely compare the effective robustness of models trained on different data.\n- We show that the OOD accuracy of various models including zero-shot CLIP models can usually\nbe better predicted from accuracies on multiple ID test sets compared to using only one ID test set.\n- Our results provide new understandings on the effective robustness gains of CLIP-like models\nobserved in prior works only using ImageNet as the ID test set, while the gains diminish under our\nnew evaluation.\n\n## Background of Effective Robustness\n\nUnder natural distribution shifts, the OOD accuracy of a model is often correlated with the ID\naccuracy. After applying a logit transformation on the accuracy, a linear trend between the transformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from\nImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky\net al., 2009) to CIFAR-10.2 (Hendrycks & Dietterich, 2018)) and models with various architectures\nand training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most\nmodels showing higher OOD accuracies naturally resulted from better ID performance.\n\nTo eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed\neffective robustness that measures the OOD performance beyond the expected OOD accuracy given\nthe ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend\nof baseline models. Since they only use a single ID test set, we refer to this version of effective\nrobustness as single-ID effective robustness.\n\nSuppose there are n baseline models \\( f_1, f_2, \\ldots, f_n \\). A baseline function \\( \\tilde{\\beta}(x) \\) is constructed to predict\nthe OOD accuracy of each baseline model, \\( acc_{ood}(f_i) \\) (\\( 1 \\leq i \\leq n \\)), given the single ID accuracy of\nthe model \\( x = acc_{id}(f_i) \\). The baseline function is instantiated as:\n\n$$\n\\tilde{\\beta}(x) = \\text{expit}(w \\text{logit}(x) + b) \\quad (1)\n$$\n\nwhere \\( w \\) and \\( b \\) are parameters, \\( \\text{logit}(x) = \\ln\\left(\\frac{1}{1-x}\\right) \\) is the logit transformation, and \\( \\text{expit}(x) \\) is the\ninverse of \\( \\text{logit}(x) \\). Since \\( \\text{logit}(\\tilde{\\beta}(x)) = w \\text{logit}(x) + b \\), the baseline function is essentially a linear", "images": [], "items": [{"type": "text", "value": "in Taori et al. (2020) or CIFAR-10 (Krizhevsky et al., 2009) when using CIFAR-like OOD test sets\nin Miller et al. (2021). It is acceptable when models are trained predominately on only one dataset.\nHowever, the emergence of many large-scale models trained on significantly different datasets makes\nit necessary to evaluate and compare models trained on different data distributions, under which it\nbecomes unclear which ID test set should be used.\nIn particular, models from Contrastive Language-Image Pre-training, such as CLIP (Radford et al.,\n2021) and ALIGN (Jia et al., 2021) have recently exhibited unprecedented effective robustness gains\nduring zero-shot inference (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022). However\nthese previous works simply take ImageNet as the single ID test set, even though the models are not\ntrained on ImageNet. We demonstrate that the results of evaluating effective robustness using a single\nID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise\ntreatment on the ID test set in existing works could end up exaggerating the effective robustness of\nzero-shot CLIP models compared to models that are exactly trained on ImageNet.\nIn this paper, we propose to more precisely evaluate and compare the effective robustness of models\ntrained on different datasets. Instead of controlling for a single ID accuracy that may bias towards\nmodels from a particular training distribution, we propose to use multiple ID test sets that cover the\ntraining distributions of all the models. In particular, previous works performed single-dimensional\nlinear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\net al., 2020). And they then evaluate the actual OOD accuracy of the models beyond the expected\nvalue that can be predicted from the fitting line, as the effective robustness. We expand on this\ndefinition by allowing for multiple ID test sets, and perform multi-dimensional linear regression to fit\na plane to predict OOD accuracy from the accuracy on multiple ID test sets.\n\nIn summary, we make the following contributions:\n- We reveal a limitation in the existing effective robustness evaluation when used to compare models\ntrained on different data distributions.\n- We then propose a new effective robustness evaluation which uses multiple ID test sets to more\nprecisely compare the effective robustness of models trained on different data.\n- We show that the OOD accuracy of various models including zero-shot CLIP models can usually\nbe better predicted from accuracies on multiple ID test sets compared to using only one ID test set.\n- Our results provide new understandings on the effective robustness gains of CLIP-like models\nobserved in prior works only using ImageNet as the ID test set, while the gains diminish under our\nnew evaluation.", "md": "in Taori et al. (2020) or CIFAR-10 (Krizhevsky et al., 2009) when using CIFAR-like OOD test sets\nin Miller et al. (2021). It is acceptable when models are trained predominately on only one dataset.\nHowever, the emergence of many large-scale models trained on significantly different datasets makes\nit necessary to evaluate and compare models trained on different data distributions, under which it\nbecomes unclear which ID test set should be used.\nIn particular, models from Contrastive Language-Image Pre-training, such as CLIP (Radford et al.,\n2021) and ALIGN (Jia et al., 2021) have recently exhibited unprecedented effective robustness gains\nduring zero-shot inference (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022). However\nthese previous works simply take ImageNet as the single ID test set, even though the models are not\ntrained on ImageNet. We demonstrate that the results of evaluating effective robustness using a single\nID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise\ntreatment on the ID test set in existing works could end up exaggerating the effective robustness of\nzero-shot CLIP models compared to models that are exactly trained on ImageNet.\nIn this paper, we propose to more precisely evaluate and compare the effective robustness of models\ntrained on different datasets. Instead of controlling for a single ID accuracy that may bias towards\nmodels from a particular training distribution, we propose to use multiple ID test sets that cover the\ntraining distributions of all the models. In particular, previous works performed single-dimensional\nlinear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\net al., 2020). And they then evaluate the actual OOD accuracy of the models beyond the expected\nvalue that can be predicted from the fitting line, as the effective robustness. We expand on this\ndefinition by allowing for multiple ID test sets, and perform multi-dimensional linear regression to fit\na plane to predict OOD accuracy from the accuracy on multiple ID test sets.\n\nIn summary, we make the following contributions:\n- We reveal a limitation in the existing effective robustness evaluation when used to compare models\ntrained on different data distributions.\n- We then propose a new effective robustness evaluation which uses multiple ID test sets to more\nprecisely compare the effective robustness of models trained on different data.\n- We show that the OOD accuracy of various models including zero-shot CLIP models can usually\nbe better predicted from accuracies on multiple ID test sets compared to using only one ID test set.\n- Our results provide new understandings on the effective robustness gains of CLIP-like models\nobserved in prior works only using ImageNet as the ID test set, while the gains diminish under our\nnew evaluation."}, {"type": "heading", "lvl": 2, "value": "Background of Effective Robustness", "md": "## Background of Effective Robustness"}, {"type": "text", "value": "Under natural distribution shifts, the OOD accuracy of a model is often correlated with the ID\naccuracy. After applying a logit transformation on the accuracy, a linear trend between the transformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from\nImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky\net al., 2009) to CIFAR-10.2 (Hendrycks & Dietterich, 2018)) and models with various architectures\nand training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most\nmodels showing higher OOD accuracies naturally resulted from better ID performance.\n\nTo eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed\neffective robustness that measures the OOD performance beyond the expected OOD accuracy given\nthe ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend\nof baseline models. Since they only use a single ID test set, we refer to this version of effective\nrobustness as single-ID effective robustness.\n\nSuppose there are n baseline models \\( f_1, f_2, \\ldots, f_n \\). A baseline function \\( \\tilde{\\beta}(x) \\) is constructed to predict\nthe OOD accuracy of each baseline model, \\( acc_{ood}(f_i) \\) (\\( 1 \\leq i \\leq n \\)), given the single ID accuracy of\nthe model \\( x = acc_{id}(f_i) \\). The baseline function is instantiated as:\n\n$$\n\\tilde{\\beta}(x) = \\text{expit}(w \\text{logit}(x) + b) \\quad (1)\n$$\n\nwhere \\( w \\) and \\( b \\) are parameters, \\( \\text{logit}(x) = \\ln\\left(\\frac{1}{1-x}\\right) \\) is the logit transformation, and \\( \\text{expit}(x) \\) is the\ninverse of \\( \\text{logit}(x) \\). Since \\( \\text{logit}(\\tilde{\\beta}(x)) = w \\text{logit}(x) + b \\), the baseline function is essentially a linear", "md": "Under natural distribution shifts, the OOD accuracy of a model is often correlated with the ID\naccuracy. After applying a logit transformation on the accuracy, a linear trend between the transformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from\nImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky\net al., 2009) to CIFAR-10.2 (Hendrycks & Dietterich, 2018)) and models with various architectures\nand training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most\nmodels showing higher OOD accuracies naturally resulted from better ID performance.\n\nTo eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed\neffective robustness that measures the OOD performance beyond the expected OOD accuracy given\nthe ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend\nof baseline models. Since they only use a single ID test set, we refer to this version of effective\nrobustness as single-ID effective robustness.\n\nSuppose there are n baseline models \\( f_1, f_2, \\ldots, f_n \\). A baseline function \\( \\tilde{\\beta}(x) \\) is constructed to predict\nthe OOD accuracy of each baseline model, \\( acc_{ood}(f_i) \\) (\\( 1 \\leq i \\leq n \\)), given the single ID accuracy of\nthe model \\( x = acc_{id}(f_i) \\). The baseline function is instantiated as:\n\n$$\n\\tilde{\\beta}(x) = \\text{expit}(w \\text{logit}(x) + b) \\quad (1)\n$$\n\nwhere \\( w \\) and \\( b \\) are parameters, \\( \\text{logit}(x) = \\ln\\left(\\frac{1}{1-x}\\right) \\) is the logit transformation, and \\( \\text{expit}(x) \\) is the\ninverse of \\( \\text{logit}(x) \\). Since \\( \\text{logit}(\\tilde{\\beta}(x)) = w \\text{logit}(x) + b \\), the baseline function is essentially a linear"}]}, {"page": 3, "text": "                                               ImageNet models                                                                                                            80                    ImageNet models\n                         95                    YFCC models                                                                                                                                      YFCC models\n                                                                                                                                                                          70\n                                               Linear fit for ImageNet models                                                                                                                   Linear fit for ImageNet models\n                         90\n                                               Linear fit for YFCC models                                                                                                                       Linear fit for YFCC models\n                         80                                                                                                                                               50\n                         70\n                         50                                                                                                                                               25\n                         25\n                                                                                                                                                                          10\n                         10\n                I\n                m\n                a\n                g\n                e\n                N\n                e\n                t\n                -\n                V\n                2\n                a\n                c\n                c\n                u\n                r\n                a\n                c\n                y\n                (\n                c\n                s\n                s\n                .\n                ,\n                %\n                )\n                                                                                                                                                                 I\n                                                                                                                                                                 m\n                                                                                                                                                                 a\n                                                                                                                                                                 g\n                                                                                                                                                                 e\n                                                                                                                                                                 N\n                                                                                                                                                                 e\n                                                                                                                                                                 t\n                                                                                                                                                                 -\n                                                                                                                                                                 V\n                                                                                                                                                                 2\n                                                                                                                                                                 a\n                                                                                                                                                                 c\n                                                                                                                                                                 c\n                                                                                                                                                                 u\n                                                                                                                                                                 r\n                                                                                                                                                                 a\n                                                                                                                                                                 c\n                                                                                                                                                                 y\n                                                                                                                                                                 (\n                                                                                                                                                                 c\n                                                                                                                                                                 s\n                                                                                                                                                                 s\n                                                                                                                                                                 .\n                                                                                                                                                                 ,\n                                                                                                                                                                 %\n                                                                                                                                                                 )\n                                                                        10                  25             50  70       80       90       95                                                                                            10          25                   50  70  80\n                                                                    ImageNet accuracy (css., %)                                                                                                                            YFCC accuracy (css., %)\n  (a) ImageNet-V2 accuracy against ImageNet accuracy.                                                                                                 (b) ImageNet-V2 accuracy against YFCC accuracy.\n Figure 1: Class-subsampled (\u201ccss.\u201d for short) ImageNet-V2 accuracy against ImageNet accuracy\n and YFCC accuracy, respectively, for 36 ImageNet models and 13 YFCC models that are also used\n in Table 3a. A linear fit is generated for ImageNet models and YFCC-15M models, respectively.\nAccuracies and linear fits are under the logit scale. Class-subsampling is used to only include classes\n that appear in all the involved test sets (see Section 5.1).\n function after applying a logit transformation on the accuracies, and it can be determined by solving\n a linear regression. Then the single-ID effective robustness of a model f is evaluated as\n                                                                                                                    \u02dc\nwhich subtracts the predicted OOD accuracy based on the ID accuracy accid(f                                        \u03c1(f      ) = accood(f         ) \u2212           \u03b2\u02dc    (accid(f                         )),                                               ), from the actual           (2)\n OOD accuracy accood(f                                                                           ).\n3                 Limitation of the Single ID Test Set\nThe existing effective robustness evaluation in Section 2 fixes a single ID test set for all the models,\nwhich is reflective of the ID performance only if all the models are trained on the same dataset that\n matches the ID test set. However, as large-scale pre-trained models emerge, it becomes necessary to\n compare models trained on different datasets, in order to know if the latest pre-training techniques\n can yield effective robustness gains. In this section, we use the comparison between zero-shot CLIP\n models and standard ImageNet models as an example to show the limitation of using a single ID test\n set: when only one ID test set is used, using different ID test sets leads to contradictory conclusions.\n Following Fang et al. (2022), we compare models trained on ImageNet (Deng et al., 2009) and\nYFCC-15M (Thomee et al., 2016), respectively. On ImageNet, we include standard classifiers, and\nwe also train CLIP models using templates filled with an ImageNet class name as the caption in a\n format of \u201cA photo of a {class name}\u201d. We also train CLIP models on YFCC-15M, a dataset with\n image-text pairs. And we use ImageNet-V2 (Recht et al., 2019) as the OOD test set. We consider\n two different ID test sets. One ID test set is simply ImageNet. The other ID test set is constructed\n from YFCC-15M, since we have CLIP models trained on YFCC. We refer to this test set as \u201cYFCC\n test set\u201d, and we refer to the accuracy on this test set as \u201cYFCC accuracy\u201d. We discuss its details in\n Section 5.1 and Appendix B.2. Both ID test sets we consider here match the training distribution of\n some of the models (ImageNet models and YFCC models respectively) but not all the models.\nWe then plot the ImageNet-V2 accuracy of the models against their ImageNet accuracy and YFCC\n accuracy, respectively. There is a strong linear trend between the scaled ID accuracy and OOD\n accuracy for ImageNet models and YFCC models, respectively, and we plot fitting lines for these two\n sets of models, respectively. When the ID test set is ImageNet, Fig. 1a shows that the fitting line for\nYFCC models is generally above the fitting line for ImageNet models (except for the regime with\n extremely low accuracies), which appears to suggest that YFCC models have effective robustness\n gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 1b which uses\nYFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models,\nwhich instead appears to suggest that ImageNet models have greater effective robustness than YFCC\n models. We observe that when there is a mismatch in the training data and the ID test data, the\n                                                                                                                                              3", "md": "# Document\n\n## ImageNet models\n\n| |80|ImageNet models|\n|---|---|---|\n|95|YFCC models|YFCC models|\n| |70| |\n\n## Linear fit for ImageNet models\n\n80\n\n70\n\n50\n\n25\n\n10\n\n## Linear fit for YFCC models\n\n50\n\n25\n\n10\n\n## ImageNet-V2 accuracy (css., %)\n\n10 25 50 70 80 90 95\n\n## ImageNet accuracy (css., %)\n\nYFCC accuracy (css., %)\n\n### (a) ImageNet-V2 accuracy against ImageNet accuracy.\n\n### (b) ImageNet-V2 accuracy against YFCC accuracy.\n\nFigure 1: Class-subsampled (\"css.\" for short) ImageNet-V2 accuracy against ImageNet accuracy and YFCC accuracy, respectively, for 36 ImageNet models and 13 YFCC models that are also used in Table 3a. A linear fit is generated for ImageNet models and YFCC-15M models, respectively. Accuracies and linear fits are under the logit scale. Class-subsampling is used to only include classes that appear in all the involved test sets (see Section 5.1).\n\nFunction after applying a logit transformation on the accuracies, and it can be determined by solving a linear regression. Then the single-ID effective robustness of a model f is evaluated as:\n\n$$\\tilde{\\rho}(f) = acc_{ood}(f) - \\beta\\tilde{}(acc_{id}(f))$$\n\nfrom the actual OOD accuracy $acc_{ood}(f)$.\n\n### Limitation of the Single ID Test Set\n\nThe existing effective robustness evaluation in Section 2 fixes a single ID test set for all the models, which is reflective of the ID performance only if all the models are trained on the same dataset that matches the ID test set. However, as large-scale pre-trained models emerge, it becomes necessary to compare models trained on different datasets, in order to know if the latest pre-training techniques can yield effective robustness gains. In this section, we use the comparison between zero-shot CLIP models and standard ImageNet models as an example to show the limitation of using a single ID test set: when only one ID test set is used, using different ID test sets leads to contradictory conclusions. Following Fang et al. (2022), we compare models trained on ImageNet (Deng et al., 2009) and YFCC-15M (Thomee et al., 2016), respectively. On ImageNet, we include standard classifiers, and we also train CLIP models using templates filled with an ImageNet class name as the caption in a format of \"A photo of a {class name}\". We also train CLIP models on YFCC-15M, a dataset with image-text pairs. And we use ImageNet-V2 (Recht et al., 2019) as the OOD test set. We consider two different ID test sets. One ID test set is simply ImageNet. The other ID test set is constructed from YFCC-15M, since we have CLIP models trained on YFCC. We refer to this test set as \"YFCC test set\", and we refer to the accuracy on this test set as \"YFCC accuracy\". We discuss its details in Section 5.1 and Appendix B.2. Both ID test sets we consider here match the training distribution of some of the models (ImageNet models and YFCC models respectively) but not all the models.\n\nWe then plot the ImageNet-V2 accuracy of the models against their ImageNet accuracy and YFCC accuracy, respectively. There is a strong linear trend between the scaled ID accuracy and OOD accuracy for ImageNet models and YFCC models, respectively, and we plot fitting lines for these two sets of models, respectively. When the ID test set is ImageNet, Fig. 1a shows that the fitting line for YFCC models is generally above the fitting line for ImageNet models (except for the regime with extremely low accuracies), which appears to suggest that YFCC models have effective robustness gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 1b which uses YFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models, which instead appears to suggest that ImageNet models have greater effective robustness than YFCC models. We observe that when there is a mismatch in the training data and the ID test data.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "ImageNet models", "md": "## ImageNet models"}, {"type": "table", "rows": [["", "80", "ImageNet models"], ["95", "YFCC models", "YFCC models"], ["", "70", ""]], "md": "| |80|ImageNet models|\n|---|---|---|\n|95|YFCC models|YFCC models|\n| |70| |", "isPerfectTable": true, "csv": "\"\",\"80\",\"ImageNet models\"\n\"95\",\"YFCC models\",\"YFCC models\"\n\"\",\"70\",\"\""}, {"type": "heading", "lvl": 2, "value": "Linear fit for ImageNet models", "md": "## Linear fit for ImageNet models"}, {"type": "text", "value": "80\n\n70\n\n50\n\n25\n\n10", "md": "80\n\n70\n\n50\n\n25\n\n10"}, {"type": "heading", "lvl": 2, "value": "Linear fit for YFCC models", "md": "## Linear fit for YFCC models"}, {"type": "text", "value": "50\n\n25\n\n10", "md": "50\n\n25\n\n10"}, {"type": "heading", "lvl": 2, "value": "ImageNet-V2 accuracy (css., %)", "md": "## ImageNet-V2 accuracy (css., %)"}, {"type": "text", "value": "10 25 50 70 80 90 95", "md": "10 25 50 70 80 90 95"}, {"type": "heading", "lvl": 2, "value": "ImageNet accuracy (css., %)", "md": "## ImageNet accuracy (css., %)"}, {"type": "text", "value": "YFCC accuracy (css., %)", "md": "YFCC accuracy (css., %)"}, {"type": "heading", "lvl": 3, "value": "(a) ImageNet-V2 accuracy against ImageNet accuracy.", "md": "### (a) ImageNet-V2 accuracy against ImageNet accuracy."}, {"type": "heading", "lvl": 3, "value": "(b) ImageNet-V2 accuracy against YFCC accuracy.", "md": "### (b) ImageNet-V2 accuracy against YFCC accuracy."}, {"type": "text", "value": "Figure 1: Class-subsampled (\"css.\" for short) ImageNet-V2 accuracy against ImageNet accuracy and YFCC accuracy, respectively, for 36 ImageNet models and 13 YFCC models that are also used in Table 3a. A linear fit is generated for ImageNet models and YFCC-15M models, respectively. Accuracies and linear fits are under the logit scale. Class-subsampling is used to only include classes that appear in all the involved test sets (see Section 5.1).\n\nFunction after applying a logit transformation on the accuracies, and it can be determined by solving a linear regression. Then the single-ID effective robustness of a model f is evaluated as:\n\n$$\\tilde{\\rho}(f) = acc_{ood}(f) - \\beta\\tilde{}(acc_{id}(f))$$\n\nfrom the actual OOD accuracy $acc_{ood}(f)$.", "md": "Figure 1: Class-subsampled (\"css.\" for short) ImageNet-V2 accuracy against ImageNet accuracy and YFCC accuracy, respectively, for 36 ImageNet models and 13 YFCC models that are also used in Table 3a. A linear fit is generated for ImageNet models and YFCC-15M models, respectively. Accuracies and linear fits are under the logit scale. Class-subsampling is used to only include classes that appear in all the involved test sets (see Section 5.1).\n\nFunction after applying a logit transformation on the accuracies, and it can be determined by solving a linear regression. Then the single-ID effective robustness of a model f is evaluated as:\n\n$$\\tilde{\\rho}(f) = acc_{ood}(f) - \\beta\\tilde{}(acc_{id}(f))$$\n\nfrom the actual OOD accuracy $acc_{ood}(f)$."}, {"type": "heading", "lvl": 3, "value": "Limitation of the Single ID Test Set", "md": "### Limitation of the Single ID Test Set"}, {"type": "text", "value": "The existing effective robustness evaluation in Section 2 fixes a single ID test set for all the models, which is reflective of the ID performance only if all the models are trained on the same dataset that matches the ID test set. However, as large-scale pre-trained models emerge, it becomes necessary to compare models trained on different datasets, in order to know if the latest pre-training techniques can yield effective robustness gains. In this section, we use the comparison between zero-shot CLIP models and standard ImageNet models as an example to show the limitation of using a single ID test set: when only one ID test set is used, using different ID test sets leads to contradictory conclusions. Following Fang et al. (2022), we compare models trained on ImageNet (Deng et al., 2009) and YFCC-15M (Thomee et al., 2016), respectively. On ImageNet, we include standard classifiers, and we also train CLIP models using templates filled with an ImageNet class name as the caption in a format of \"A photo of a {class name}\". We also train CLIP models on YFCC-15M, a dataset with image-text pairs. And we use ImageNet-V2 (Recht et al., 2019) as the OOD test set. We consider two different ID test sets. One ID test set is simply ImageNet. The other ID test set is constructed from YFCC-15M, since we have CLIP models trained on YFCC. We refer to this test set as \"YFCC test set\", and we refer to the accuracy on this test set as \"YFCC accuracy\". We discuss its details in Section 5.1 and Appendix B.2. Both ID test sets we consider here match the training distribution of some of the models (ImageNet models and YFCC models respectively) but not all the models.\n\nWe then plot the ImageNet-V2 accuracy of the models against their ImageNet accuracy and YFCC accuracy, respectively. There is a strong linear trend between the scaled ID accuracy and OOD accuracy for ImageNet models and YFCC models, respectively, and we plot fitting lines for these two sets of models, respectively. When the ID test set is ImageNet, Fig. 1a shows that the fitting line for YFCC models is generally above the fitting line for ImageNet models (except for the regime with extremely low accuracies), which appears to suggest that YFCC models have effective robustness gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 1b which uses YFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models, which instead appears to suggest that ImageNet models have greater effective robustness than YFCC models. We observe that when there is a mismatch in the training data and the ID test data.", "md": "The existing effective robustness evaluation in Section 2 fixes a single ID test set for all the models, which is reflective of the ID performance only if all the models are trained on the same dataset that matches the ID test set. However, as large-scale pre-trained models emerge, it becomes necessary to compare models trained on different datasets, in order to know if the latest pre-training techniques can yield effective robustness gains. In this section, we use the comparison between zero-shot CLIP models and standard ImageNet models as an example to show the limitation of using a single ID test set: when only one ID test set is used, using different ID test sets leads to contradictory conclusions. Following Fang et al. (2022), we compare models trained on ImageNet (Deng et al., 2009) and YFCC-15M (Thomee et al., 2016), respectively. On ImageNet, we include standard classifiers, and we also train CLIP models using templates filled with an ImageNet class name as the caption in a format of \"A photo of a {class name}\". We also train CLIP models on YFCC-15M, a dataset with image-text pairs. And we use ImageNet-V2 (Recht et al., 2019) as the OOD test set. We consider two different ID test sets. One ID test set is simply ImageNet. The other ID test set is constructed from YFCC-15M, since we have CLIP models trained on YFCC. We refer to this test set as \"YFCC test set\", and we refer to the accuracy on this test set as \"YFCC accuracy\". We discuss its details in Section 5.1 and Appendix B.2. Both ID test sets we consider here match the training distribution of some of the models (ImageNet models and YFCC models respectively) but not all the models.\n\nWe then plot the ImageNet-V2 accuracy of the models against their ImageNet accuracy and YFCC accuracy, respectively. There is a strong linear trend between the scaled ID accuracy and OOD accuracy for ImageNet models and YFCC models, respectively, and we plot fitting lines for these two sets of models, respectively. When the ID test set is ImageNet, Fig. 1a shows that the fitting line for YFCC models is generally above the fitting line for ImageNet models (except for the regime with extremely low accuracies), which appears to suggest that YFCC models have effective robustness gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 1b which uses YFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models, which instead appears to suggest that ImageNet models have greater effective robustness than YFCC models. We observe that when there is a mismatch in the training data and the ID test data."}]}, {"page": 4, "text": "models appear to have greater effective robustness (YFCC models in Figure 1a and ImageNet models\nin Figure 1b), as their performance on the ID test data and the OOD performance predicted from the\nsingle ID accuracy tend to be lower. This makes it diffi                              cult to compare models trained on different\ndata and leads to imprecise conclusions on effective robustness if only one ID test set is used.\n4      Multi-ID Effective Robustness\nConsidering the limitations of using a single ID test set, we propose a new way for effective robustness\nevaluation using multiple ID test sets that cover the training data distributions of all the involved\nmodels. We name it multi-ID effective robustness. Specifically, for each training distribution, we\npropose to prepare an ID test set that matches the training distribution, respectively. In particular, we\nfocus on comparing models trained on two different datasets at a time in this paper, and we thereby\nuse two ID test sets, where each of them corresponds to one of the training datasets.\nWhile we refer to them as ID test sets, each of them is only the exact ID test set for some of the\nconsidered models that are trained on the distribution matching the test set, and it is not exactly an\nID test set for all the considered models. However, we assume that the training distributions of all\nthe models are still relatively close compared to the OOD test distributions (e.g., images normally\ncollected from social medias in ImageNet (Deng et al., 2009), YFCC (Thomee et al., 2016), and\nLAION (Schuhmann et al., 2021) are relatively close compared to the OOD images in ImageNet-\nSketch (Wang et al., 2019) that consists of sketch images specifically). In this way, both ID test sets\nare relatively ID for all the models compared to the OOD test sets, and it can be meaningful to control\nfor the performance on these ID test sets when comparing the OOD performance.\nWe still use accood(\u00b7) to denote the OOD accuracy, and we use acc1(\u00b7) and acc2(\u00b7) to denote the\naccuracy on the two ID test sets, respectively. In contrast to the previous baseline function \u02dc                                              \u03b2(x) in\nEq. (1), we propose a new baseline function \u03b2(x, y) that predicts the OOD accuracy based on the\naccuracies x and y on the two ID test sets, respectively.\nAll the models in Figure 1 are trained on either Im-\nageNet or YFCC. Thus, to compare their effective                                                           ImageNet Models      YFCC Models\nrobustness under our new evaluation, we use two ID\ntest sets for ImageNet and YFCC at the same time, in                                  1\ncontrast to Figure 1a and 1b which use one ID test set\nseparately at each time and results on the two differ-\nent ID test sets lead to contradictory conclusions. As\nshown in Figure 2, we plot the OOD accuracy against\nthe two ID accuracies on both two ID test sets in a 3D\nspace. We observe that the data points approximately\nlie on a plane when plotted on the logit scale. This                                                                                        s\" 02\\'\nmotivates us to instantiate \u03b2(x, y) as:\n  \u03b2(x, y) = expit(wx logit(x) + wy logit(y) + b),\nwhere wx, wy, b are parameters. \u03b2(x, y), which is the                       (3)     Figure 2:          Class-subsampled (\u201ccss.\u201d                      for\nplane in Figure 2, is also a linear function w.r.t. x and                           short) ImageNet-V2 accuracy against both\ny under the logit scale, and thus it is a reasonable ex-                            ImageNet accuracy and YFCC accuracy for\ntension from \u02dc       \u03b2(x) by using a multi-dimensional lin-                         ImageNet models and YFCC models used in\near function on the logit scale. We determine the pa-                               Figure 1 which shows the projections when\nrameters by solving an ordinary least squares regres-                               only one of ImageNet accuracy and YFCC\nsion to fit the accuracies. Metrics for linear regression                           accuracy is used.\nsuch as the coeffi          cient of determination, a.k.a. R2,\ncan be used to evaluate the fitting quality of the baseline function. A high R2 value indicates that\nthe OOD accuracy is accurately predicted by the baseline function from the ID accuracies, and thus\nthe evaluated models have similar effective robustness. And our multi-ID effective robustness for a\nmodel f is defined as\n                                            \u03c1(f) = accood(f) \u2212              \u03b2(acc1(f), acc2(f)).\nCompared to the existing definition for effective robustness in Eq. (2), the major difference is the\ninclusion of two ID accuracies acc1(f) and acc2(f) in the baseline function, compared to using a\nsingle ID accuracy accid(f).\n                                                                            4", "md": "models appear to have greater effective robustness (YFCC models in Figure 1a and ImageNet models\nin Figure 1b), as their performance on the ID test data and the OOD performance predicted from the\nsingle ID accuracy tend to be lower. This makes it difficult to compare models trained on different\ndata and leads to imprecise conclusions on effective robustness if only one ID test set is used.\n\n#### Multi-ID Effective Robustness\n\nConsidering the limitations of using a single ID test set, we propose a new way for effective robustness\nevaluation using multiple ID test sets that cover the training data distributions of all the involved\nmodels. We name it multi-ID effective robustness. Specifically, for each training distribution, we\npropose to prepare an ID test set that matches the training distribution, respectively. In particular, we\nfocus on comparing models trained on two different datasets at a time in this paper, and we thereby\nuse two ID test sets, where each of them corresponds to one of the training datasets.\n\nWhile we refer to them as ID test sets, each of them is only the exact ID test set for some of the\nconsidered models that are trained on the distribution matching the test set, and it is not exactly an\nID test set for all the considered models. However, we assume that the training distributions of all\nthe models are still relatively close compared to the OOD test distributions (e.g., images normally\ncollected from social medias in ImageNet (Deng et al., 2009), YFCC (Thomee et al., 2016), and\nLAION (Schuhmann et al., 2021) are relatively close compared to the OOD images in ImageNet-\nSketch (Wang et al., 2019) that consists of sketch images specifically). In this way, both ID test sets\nare relatively ID for all the models compared to the OOD test sets, and it can be meaningful to control\nfor the performance on these ID test sets when comparing the OOD performance.\n\nWe still use $$acc_{ood}(\u00b7)$$ to denote the OOD accuracy, and we use $$acc_1(\u00b7)$$ and $$acc_2(\u00b7)$$ to denote the\naccuracy on the two ID test sets, respectively. In contrast to the previous baseline function $$\\tilde{\\beta}(x)$$ in\nEq. (1), we propose a new baseline function $$\\beta(x, y)$$ that predicts the OOD accuracy based on the\naccuracies x and y on the two ID test sets, respectively.\n\nAll the models in Figure 1 are trained on either ImageNet or YFCC. Thus, to compare their effective ImageNet Models YFCC Models\nrobustness under our new evaluation, we use two ID\ntest sets for ImageNet and YFCC at the same time, in 1\ncontrast to Figure 1a and 1b which use one ID test set\nseparately at each time and results on the two different\nID test sets lead to contradictory conclusions. As\nshown in Figure 2, we plot the OOD accuracy against\nthe two ID accuracies on both two ID test sets in a 3D\nspace. We observe that the data points approximately\nlie on a plane when plotted on the logit scale. This motivates us to instantiate $$\\beta(x, y)$$ as:\n\n$$\\beta(x, y) = expit(wx logit(x) + wy logit(y) + b),$$\n\nwhere wx, wy, b are parameters. $$\\beta(x, y)$$, which is the plane in Figure 2, is also a linear function w.r.t. x and\ny under the logit scale, and thus it is a reasonable extension from $$\\tilde{\\beta}(x)$$ by using a multi-dimensional linear function on the logit scale. We determine the parameters by solving an ordinary least squares regression to fit the accuracies. Metrics for linear regression\nsuch as the coefficient of determination, a.k.a. R2,\ncan be used to evaluate the fitting quality of the baseline function. A high R2 value indicates that\nthe OOD accuracy is accurately predicted by the baseline function from the ID accuracies, and thus\nthe evaluated models have similar effective robustness. And our multi-ID effective robustness for a\nmodel f is defined as\n\n$$\\rho(f) = acc_{ood}(f) - \\beta(acc_1(f), acc_2(f)).$$\n\nCompared to the existing definition for effective robustness in Eq. (2), the major difference is the\ninclusion of two ID accuracies $$acc_1(f)$$ and $$acc_2(f)$$ in the baseline function, compared to using a\nsingle ID accuracy $$acc_{id}(f)$$.", "images": [{"name": "page-4-0.jpg", "height": 206, "width": 206, "x": 316, "y": 354}], "items": [{"type": "text", "value": "models appear to have greater effective robustness (YFCC models in Figure 1a and ImageNet models\nin Figure 1b), as their performance on the ID test data and the OOD performance predicted from the\nsingle ID accuracy tend to be lower. This makes it difficult to compare models trained on different\ndata and leads to imprecise conclusions on effective robustness if only one ID test set is used.", "md": "models appear to have greater effective robustness (YFCC models in Figure 1a and ImageNet models\nin Figure 1b), as their performance on the ID test data and the OOD performance predicted from the\nsingle ID accuracy tend to be lower. This makes it difficult to compare models trained on different\ndata and leads to imprecise conclusions on effective robustness if only one ID test set is used."}, {"type": "heading", "lvl": 4, "value": "Multi-ID Effective Robustness", "md": "#### Multi-ID Effective Robustness"}, {"type": "text", "value": "Considering the limitations of using a single ID test set, we propose a new way for effective robustness\nevaluation using multiple ID test sets that cover the training data distributions of all the involved\nmodels. We name it multi-ID effective robustness. Specifically, for each training distribution, we\npropose to prepare an ID test set that matches the training distribution, respectively. In particular, we\nfocus on comparing models trained on two different datasets at a time in this paper, and we thereby\nuse two ID test sets, where each of them corresponds to one of the training datasets.\n\nWhile we refer to them as ID test sets, each of them is only the exact ID test set for some of the\nconsidered models that are trained on the distribution matching the test set, and it is not exactly an\nID test set for all the considered models. However, we assume that the training distributions of all\nthe models are still relatively close compared to the OOD test distributions (e.g., images normally\ncollected from social medias in ImageNet (Deng et al., 2009), YFCC (Thomee et al., 2016), and\nLAION (Schuhmann et al., 2021) are relatively close compared to the OOD images in ImageNet-\nSketch (Wang et al., 2019) that consists of sketch images specifically). In this way, both ID test sets\nare relatively ID for all the models compared to the OOD test sets, and it can be meaningful to control\nfor the performance on these ID test sets when comparing the OOD performance.\n\nWe still use $$acc_{ood}(\u00b7)$$ to denote the OOD accuracy, and we use $$acc_1(\u00b7)$$ and $$acc_2(\u00b7)$$ to denote the\naccuracy on the two ID test sets, respectively. In contrast to the previous baseline function $$\\tilde{\\beta}(x)$$ in\nEq. (1), we propose a new baseline function $$\\beta(x, y)$$ that predicts the OOD accuracy based on the\naccuracies x and y on the two ID test sets, respectively.\n\nAll the models in Figure 1 are trained on either ImageNet or YFCC. Thus, to compare their effective ImageNet Models YFCC Models\nrobustness under our new evaluation, we use two ID\ntest sets for ImageNet and YFCC at the same time, in 1\ncontrast to Figure 1a and 1b which use one ID test set\nseparately at each time and results on the two different\nID test sets lead to contradictory conclusions. As\nshown in Figure 2, we plot the OOD accuracy against\nthe two ID accuracies on both two ID test sets in a 3D\nspace. We observe that the data points approximately\nlie on a plane when plotted on the logit scale. This motivates us to instantiate $$\\beta(x, y)$$ as:\n\n$$\\beta(x, y) = expit(wx logit(x) + wy logit(y) + b),$$\n\nwhere wx, wy, b are parameters. $$\\beta(x, y)$$, which is the plane in Figure 2, is also a linear function w.r.t. x and\ny under the logit scale, and thus it is a reasonable extension from $$\\tilde{\\beta}(x)$$ by using a multi-dimensional linear function on the logit scale. We determine the parameters by solving an ordinary least squares regression to fit the accuracies. Metrics for linear regression\nsuch as the coefficient of determination, a.k.a. R2,\ncan be used to evaluate the fitting quality of the baseline function. A high R2 value indicates that\nthe OOD accuracy is accurately predicted by the baseline function from the ID accuracies, and thus\nthe evaluated models have similar effective robustness. And our multi-ID effective robustness for a\nmodel f is defined as\n\n$$\\rho(f) = acc_{ood}(f) - \\beta(acc_1(f), acc_2(f)).$$\n\nCompared to the existing definition for effective robustness in Eq. (2), the major difference is the\ninclusion of two ID accuracies $$acc_1(f)$$ and $$acc_2(f)$$ in the baseline function, compared to using a\nsingle ID accuracy $$acc_{id}(f)$$.", "md": "Considering the limitations of using a single ID test set, we propose a new way for effective robustness\nevaluation using multiple ID test sets that cover the training data distributions of all the involved\nmodels. We name it multi-ID effective robustness. Specifically, for each training distribution, we\npropose to prepare an ID test set that matches the training distribution, respectively. In particular, we\nfocus on comparing models trained on two different datasets at a time in this paper, and we thereby\nuse two ID test sets, where each of them corresponds to one of the training datasets.\n\nWhile we refer to them as ID test sets, each of them is only the exact ID test set for some of the\nconsidered models that are trained on the distribution matching the test set, and it is not exactly an\nID test set for all the considered models. However, we assume that the training distributions of all\nthe models are still relatively close compared to the OOD test distributions (e.g., images normally\ncollected from social medias in ImageNet (Deng et al., 2009), YFCC (Thomee et al., 2016), and\nLAION (Schuhmann et al., 2021) are relatively close compared to the OOD images in ImageNet-\nSketch (Wang et al., 2019) that consists of sketch images specifically). In this way, both ID test sets\nare relatively ID for all the models compared to the OOD test sets, and it can be meaningful to control\nfor the performance on these ID test sets when comparing the OOD performance.\n\nWe still use $$acc_{ood}(\u00b7)$$ to denote the OOD accuracy, and we use $$acc_1(\u00b7)$$ and $$acc_2(\u00b7)$$ to denote the\naccuracy on the two ID test sets, respectively. In contrast to the previous baseline function $$\\tilde{\\beta}(x)$$ in\nEq. (1), we propose a new baseline function $$\\beta(x, y)$$ that predicts the OOD accuracy based on the\naccuracies x and y on the two ID test sets, respectively.\n\nAll the models in Figure 1 are trained on either ImageNet or YFCC. Thus, to compare their effective ImageNet Models YFCC Models\nrobustness under our new evaluation, we use two ID\ntest sets for ImageNet and YFCC at the same time, in 1\ncontrast to Figure 1a and 1b which use one ID test set\nseparately at each time and results on the two different\nID test sets lead to contradictory conclusions. As\nshown in Figure 2, we plot the OOD accuracy against\nthe two ID accuracies on both two ID test sets in a 3D\nspace. We observe that the data points approximately\nlie on a plane when plotted on the logit scale. This motivates us to instantiate $$\\beta(x, y)$$ as:\n\n$$\\beta(x, y) = expit(wx logit(x) + wy logit(y) + b),$$\n\nwhere wx, wy, b are parameters. $$\\beta(x, y)$$, which is the plane in Figure 2, is also a linear function w.r.t. x and\ny under the logit scale, and thus it is a reasonable extension from $$\\tilde{\\beta}(x)$$ by using a multi-dimensional linear function on the logit scale. We determine the parameters by solving an ordinary least squares regression to fit the accuracies. Metrics for linear regression\nsuch as the coefficient of determination, a.k.a. R2,\ncan be used to evaluate the fitting quality of the baseline function. A high R2 value indicates that\nthe OOD accuracy is accurately predicted by the baseline function from the ID accuracies, and thus\nthe evaluated models have similar effective robustness. And our multi-ID effective robustness for a\nmodel f is defined as\n\n$$\\rho(f) = acc_{ood}(f) - \\beta(acc_1(f), acc_2(f)).$$\n\nCompared to the existing definition for effective robustness in Eq. (2), the major difference is the\ninclusion of two ID accuracies $$acc_1(f)$$ and $$acc_2(f)$$ in the baseline function, compared to using a\nsingle ID accuracy $$acc_{id}(f)$$."}]}, {"page": 5, "text": "Generalizing to more than two training datasets.               Although we focus on handling two training\ndatasets at a time, our method may be generalized to more than two datasets in principle, by defining\na baseline function based on multiple ID accuracies acc1(\u00b7), \u00b7 \u00b7 \u00b7 , acck(\u00b7). However, it could be costly\nas it would require training more models to fit a high-quality baseline function. We leave it for future\nworks to reduce the cost when dealing with a larger number of datasets.\n5    Experiments\n5.1   Settings\nModels.     In order to fit a baseline function, we need a large amount of models with diverse accuracies.\nTo this end, we follow Taori et al. (2020) to train models with various proportions of data by\nsubsampling from the entire training set (namely dataset subsampling), which effectively produces\nmodels with diverse accuracies. Moreover, we also combine examples from two datasets with\ndifferent sampling ratios and train models on these combined datasets. This produces models with\ntraining distributions varying between the two training datasets and it is supposed to yield different\ncombinations of the two ID accuracies. We use models trained on each single dataset as well as\nthe combined datasets in the same fitting process, so that the baseline functions do not bias towards\nmodels trained on certain data. Our experiments include the following models:\n\u2022 Standard classifiers on CIFAR-10 and ImageNet. We train standard classifiers on CIFAR-10 and\n  ImageNet (Deng et al., 2009). We use ResNet-18, ResNet-50, and ResNet-101 (He et al., 2016).\n  Additionally, we train models by combining CIFAR-10 and ImageNet at various ratios, where\n  we upsample CIFAR-10 images from 32 \u00d7 32 to 224 \u00d7 224. Furthermore, we include ViT-S/16,\n  ViT-B/16, ViT-L/16 models (Dosovitskiy et al., 2021) pre-trained on the whole ImageNet.\n\u2022 CLIP models. On YFCC-15M (Thomee et al., 2016) and LAION-15M (Schuhmann et al., 2021)\n  which consist of image-text pairs, we train CLIP models using ResNet-50 and ResNet-101. We also\n  train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard\n  models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded\n  ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on\n  ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide\n  additional details in Appendix B.1.\nWe use \u201c{Name_of_dataset} models\u201d to denote models trained only on the dataset, e.g., \u201cCIFAR-10\nmodels\u201d. And we use \u201c{Name_of_dataset_A}+{Name_of_dataset_B} models\u201d to represent models\ntrained on a combination of two datasets, e.g., \u201cCIFAR-10+ImageNet models\u201d.\nID test sets.     We focus on image classification. Labeled image classification datasets such as\nImageNet can be directly used for evaluating ID accuracy. For datasets that consist of image-text\npairs for language-image pre-training without original labels, including YFCC and LAION, we\nautomatically generate classification labels by matching captions with ImageNet classes, which has\nbeen similarly performed in Fang et al. (2022) for training classifiers using caption data, and we then\nhold out a balanced test set from the original dataset. More details are reported in Appendix B.2.\nAlthough it is possible to obtain a higher-quality test set by human labelling, we will show that using\nthe automatically labelled test sets can already produce reasonable results.\nOOD test sets.       To compare the effectiveness robustness of models trained on CIFAR-10 and\nImageNet, we use 3 CIFAR-like OOD test sets with natural distribution shifts, including CIFAR-\n10.1 (Recht et al., 2019), CIFAR-10.2 (Lu et al., 2020), and CINIC-10 (Darlow et al., 2018). We\nuse 4 ImageNet-like OOD test sets to compare models trained on ImageNet with models trained\non YFCC and LAION: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a),\nImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019). We do not use ImageNet-\nA (Hendrycks et al., 2021b) which involves adversarial filtering and has a different behavior in\neffective robustness evaluation (Taori et al., 2020; Fang et al., 2022).\nClass subsampling and mapping.               Considering that different test sets may not have the same\nclasses, we follow prior works (Taori et al., 2020; Fang et al., 2022) to use class subsampling2 to\n   2We reuse the term \u201cclass subsampling\u201d from prior works (Taori et al., 2020; Fang et al., 2022), although it is\nnot a random sampling.\n                                                        5", "md": "# Generalizing to more than two training datasets\n\n## Generalizing to more than two training datasets\n\nAlthough we focus on handling two training datasets at a time, our method may be generalized to more than two datasets in principle, by defining a baseline function based on multiple ID accuracies $acc_1(\\cdot), \\ldots, acc_k(\\cdot)$. However, it could be costly as it would require training more models to fit a high-quality baseline function. We leave it for future works to reduce the cost when dealing with a larger number of datasets.\n\n### Experiments\n\n#### Settings\n\nModels: In order to fit a baseline function, we need a large amount of models with diverse accuracies. To this end, we follow Taori et al. (2020) to train models with various proportions of data by subsampling from the entire training set (namely dataset subsampling), which effectively produces models with diverse accuracies. Moreover, we also combine examples from two datasets with different sampling ratios and train models on these combined datasets. This produces models with training distributions varying between the two training datasets and it is supposed to yield different combinations of the two ID accuracies. We use models trained on each single dataset as well as the combined datasets in the same fitting process, so that the baseline functions do not bias towards models trained on certain data. Our experiments include the following models:\n\n- Standard classifiers on CIFAR-10 and ImageNet. We train standard classifiers on CIFAR-10 and ImageNet (Deng et al., 2009). We use ResNet-18, ResNet-50, and ResNet-101 (He et al., 2016). Additionally, we train models by combining CIFAR-10 and ImageNet at various ratios, where we upsample CIFAR-10 images from 32x32 to 224x224. Furthermore, we include ViT-S/16, ViT-B/16, ViT-L/16 models (Dosovitskiy et al., 2021) pre-trained on the whole ImageNet.\n- CLIP models. On YFCC-15M (Thomee et al., 2016) and LAION-15M (Schuhmann et al., 2021) which consist of image-text pairs, we train CLIP models using ResNet-50 and ResNet-101. We also train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide additional details in Appendix B.1.\n\nWe use \"{Name_of_dataset} models\" to denote models trained only on the dataset, e.g., \"CIFAR-10 models\". And we use \"{Name_of_dataset_A}+{Name_of_dataset_B} models\" to represent models trained on a combination of two datasets, e.g., \"CIFAR-10+ImageNet models\".\n\nID test sets: We focus on image classification. Labeled image classification datasets such as ImageNet can be directly used for evaluating ID accuracy. For datasets that consist of image-text pairs for language-image pre-training without original labels, including YFCC and LAION, we automatically generate classification labels by matching captions with ImageNet classes, which has been similarly performed in Fang et al. (2022) for training classifiers using caption data, and we then hold out a balanced test set from the original dataset. More details are reported in Appendix B.2. Although it is possible to obtain a higher-quality test set by human labeling, we will show that using the automatically labeled test sets can already produce reasonable results.\n\nOOD test sets: To compare the effectiveness robustness of models trained on CIFAR-10 and ImageNet, we use 3 CIFAR-like OOD test sets with natural distribution shifts, including CIFAR-10.1 (Recht et al., 2019), CIFAR-10.2 (Lu et al., 2020), and CINIC-10 (Darlow et al., 2018). We use 4 ImageNet-like OOD test sets to compare models trained on ImageNet with models trained on YFCC and LAION: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019). We do not use ImageNet-A (Hendrycks et al., 2021b) which involves adversarial filtering and has a different behavior in effective robustness evaluation (Taori et al., 2020; Fang et al., 2022).\n\nClass subsampling and mapping: Considering that different test sets may not have the same classes, we follow prior works (Taori et al., 2020; Fang et al., 2022) to use class subsampling to\n\nWe reuse the term \"class subsampling\" from prior works (Taori et al., 2020; Fang et al., 2022), although it is not a random sampling.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Generalizing to more than two training datasets", "md": "# Generalizing to more than two training datasets"}, {"type": "heading", "lvl": 2, "value": "Generalizing to more than two training datasets", "md": "## Generalizing to more than two training datasets"}, {"type": "text", "value": "Although we focus on handling two training datasets at a time, our method may be generalized to more than two datasets in principle, by defining a baseline function based on multiple ID accuracies $acc_1(\\cdot), \\ldots, acc_k(\\cdot)$. However, it could be costly as it would require training more models to fit a high-quality baseline function. We leave it for future works to reduce the cost when dealing with a larger number of datasets.", "md": "Although we focus on handling two training datasets at a time, our method may be generalized to more than two datasets in principle, by defining a baseline function based on multiple ID accuracies $acc_1(\\cdot), \\ldots, acc_k(\\cdot)$. However, it could be costly as it would require training more models to fit a high-quality baseline function. We leave it for future works to reduce the cost when dealing with a larger number of datasets."}, {"type": "heading", "lvl": 3, "value": "Experiments", "md": "### Experiments"}, {"type": "heading", "lvl": 4, "value": "Settings", "md": "#### Settings"}, {"type": "text", "value": "Models: In order to fit a baseline function, we need a large amount of models with diverse accuracies. To this end, we follow Taori et al. (2020) to train models with various proportions of data by subsampling from the entire training set (namely dataset subsampling), which effectively produces models with diverse accuracies. Moreover, we also combine examples from two datasets with different sampling ratios and train models on these combined datasets. This produces models with training distributions varying between the two training datasets and it is supposed to yield different combinations of the two ID accuracies. We use models trained on each single dataset as well as the combined datasets in the same fitting process, so that the baseline functions do not bias towards models trained on certain data. Our experiments include the following models:\n\n- Standard classifiers on CIFAR-10 and ImageNet. We train standard classifiers on CIFAR-10 and ImageNet (Deng et al., 2009). We use ResNet-18, ResNet-50, and ResNet-101 (He et al., 2016). Additionally, we train models by combining CIFAR-10 and ImageNet at various ratios, where we upsample CIFAR-10 images from 32x32 to 224x224. Furthermore, we include ViT-S/16, ViT-B/16, ViT-L/16 models (Dosovitskiy et al., 2021) pre-trained on the whole ImageNet.\n- CLIP models. On YFCC-15M (Thomee et al., 2016) and LAION-15M (Schuhmann et al., 2021) which consist of image-text pairs, we train CLIP models using ResNet-50 and ResNet-101. We also train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide additional details in Appendix B.1.\n\nWe use \"{Name_of_dataset} models\" to denote models trained only on the dataset, e.g., \"CIFAR-10 models\". And we use \"{Name_of_dataset_A}+{Name_of_dataset_B} models\" to represent models trained on a combination of two datasets, e.g., \"CIFAR-10+ImageNet models\".\n\nID test sets: We focus on image classification. Labeled image classification datasets such as ImageNet can be directly used for evaluating ID accuracy. For datasets that consist of image-text pairs for language-image pre-training without original labels, including YFCC and LAION, we automatically generate classification labels by matching captions with ImageNet classes, which has been similarly performed in Fang et al. (2022) for training classifiers using caption data, and we then hold out a balanced test set from the original dataset. More details are reported in Appendix B.2. Although it is possible to obtain a higher-quality test set by human labeling, we will show that using the automatically labeled test sets can already produce reasonable results.\n\nOOD test sets: To compare the effectiveness robustness of models trained on CIFAR-10 and ImageNet, we use 3 CIFAR-like OOD test sets with natural distribution shifts, including CIFAR-10.1 (Recht et al., 2019), CIFAR-10.2 (Lu et al., 2020), and CINIC-10 (Darlow et al., 2018). We use 4 ImageNet-like OOD test sets to compare models trained on ImageNet with models trained on YFCC and LAION: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019). We do not use ImageNet-A (Hendrycks et al., 2021b) which involves adversarial filtering and has a different behavior in effective robustness evaluation (Taori et al., 2020; Fang et al., 2022).\n\nClass subsampling and mapping: Considering that different test sets may not have the same classes, we follow prior works (Taori et al., 2020; Fang et al., 2022) to use class subsampling to\n\nWe reuse the term \"class subsampling\" from prior works (Taori et al., 2020; Fang et al., 2022), although it is not a random sampling.", "md": "Models: In order to fit a baseline function, we need a large amount of models with diverse accuracies. To this end, we follow Taori et al. (2020) to train models with various proportions of data by subsampling from the entire training set (namely dataset subsampling), which effectively produces models with diverse accuracies. Moreover, we also combine examples from two datasets with different sampling ratios and train models on these combined datasets. This produces models with training distributions varying between the two training datasets and it is supposed to yield different combinations of the two ID accuracies. We use models trained on each single dataset as well as the combined datasets in the same fitting process, so that the baseline functions do not bias towards models trained on certain data. Our experiments include the following models:\n\n- Standard classifiers on CIFAR-10 and ImageNet. We train standard classifiers on CIFAR-10 and ImageNet (Deng et al., 2009). We use ResNet-18, ResNet-50, and ResNet-101 (He et al., 2016). Additionally, we train models by combining CIFAR-10 and ImageNet at various ratios, where we upsample CIFAR-10 images from 32x32 to 224x224. Furthermore, we include ViT-S/16, ViT-B/16, ViT-L/16 models (Dosovitskiy et al., 2021) pre-trained on the whole ImageNet.\n- CLIP models. On YFCC-15M (Thomee et al., 2016) and LAION-15M (Schuhmann et al., 2021) which consist of image-text pairs, we train CLIP models using ResNet-50 and ResNet-101. We also train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide additional details in Appendix B.1.\n\nWe use \"{Name_of_dataset} models\" to denote models trained only on the dataset, e.g., \"CIFAR-10 models\". And we use \"{Name_of_dataset_A}+{Name_of_dataset_B} models\" to represent models trained on a combination of two datasets, e.g., \"CIFAR-10+ImageNet models\".\n\nID test sets: We focus on image classification. Labeled image classification datasets such as ImageNet can be directly used for evaluating ID accuracy. For datasets that consist of image-text pairs for language-image pre-training without original labels, including YFCC and LAION, we automatically generate classification labels by matching captions with ImageNet classes, which has been similarly performed in Fang et al. (2022) for training classifiers using caption data, and we then hold out a balanced test set from the original dataset. More details are reported in Appendix B.2. Although it is possible to obtain a higher-quality test set by human labeling, we will show that using the automatically labeled test sets can already produce reasonable results.\n\nOOD test sets: To compare the effectiveness robustness of models trained on CIFAR-10 and ImageNet, we use 3 CIFAR-like OOD test sets with natural distribution shifts, including CIFAR-10.1 (Recht et al., 2019), CIFAR-10.2 (Lu et al., 2020), and CINIC-10 (Darlow et al., 2018). We use 4 ImageNet-like OOD test sets to compare models trained on ImageNet with models trained on YFCC and LAION: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019). We do not use ImageNet-A (Hendrycks et al., 2021b) which involves adversarial filtering and has a different behavior in effective robustness evaluation (Taori et al., 2020; Fang et al., 2022).\n\nClass subsampling and mapping: Considering that different test sets may not have the same classes, we follow prior works (Taori et al., 2020; Fang et al., 2022) to use class subsampling to\n\nWe reuse the term \"class subsampling\" from prior works (Taori et al., 2020; Fang et al., 2022), although it is not a random sampling."}]}, {"page": 6, "text": "                CIFAR-10 Models                                    ImageNet Models                                 CIFAR-10+ImageNet Models                                               ImageNet Models                                     YFCC Models      ImageNet+YFCC Models\n1      Cipa4-I0                                                                                             Imagete =                        {maoped                         1\n (a) Using CIFAR-10.2 as the OOD test set. The\n  ImageNet accuracy is mapped to CIFAR-10 classes\n (see Section 5.1).                                                                                                                                                               (b) Using ImageNet-R as the OOD test set.\n  Figure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline\n  function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website\n (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.\n  retain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of\n  ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models,.\n  5.2                   Evaluation on CIFAR-like OOD Test Sets\n                     95                   CIFAR-10                                                                                                                                              CIFAR-10\n                                          ImageNet                                                                                                                                              ImageNet\n                                                                                                                                                                           80\n                     90                   CIFAR-10+ImageNet                                                                                                                                     CIFAR-10+ImageNet\n                                                                                                                                                                           70\n                     80\n                     70                                                                                                                                                    50\n                     50\n                                                                                                                                                                           25\n                     25\n           C\n           I\n           F\n           A\n           R\n           -\n           1\n           0\n           .\n           2\n           a\n           c\n           c\n           u\n           r\n           a\n           c\n           y\n           (\n           %\n           )\n                                                                                                                                                                 C\n                                                                                                                                                                 I\n                                                                                                                                                                 F\n                                                                                                                                                                 A\n                                                                                                                                                                 R\n                                                                                                                                                                 -\n                                                                                                                                                                 1\n                                                                                                                                                                 0\n                                                                                                                                                                 .\n                                                                                                                                                                 2\n                                                                                                                                                                 a\n                                                                                                                                                                 c\n                                                                                                                                                                 c\n                                                                                                                                                                 u\n                                                                                                                                                                 r\n                                                                                                                                                                 a\n                                                                                                                                                                 c\n                                                                                                                                                                 y\n                                                                                                                                                                 (\n                                                                                                                                                                 %\n                                                                                                                                                                 )\n                                                                                                                                                                           10\n                     10\n                                 10                           25                           50                 70            80               90       95                                         10                                 25                     50            70          80\n                                                                       CIFAR-10 accuracy (%)                                                                                                                   ImageNet accuracy (mapped, %)\n       (a) CIFAR-10.2 accuracy against CIFAR-10 accuracy.                                                                                                    (b) CIFAR-10.2 accuracy against ImageNet accuracy.\n       ImageNet models have higher CIFAR-10.2 accuracy                                                                                                       ImageNet models have lower CIFAR-10.2 accuracy\n       compared to CIFAR-10 models when controlling for                                                                                                      compared to CIFAR-10 models when controlling for\n       CIFAR-10 accuracy only.                                                                                                                               ImageNet accuracy only.\n  Figure 4: Projected views of Figure 3a. Figure 4a and Figure 4b correspond to single-ID evaluations\n  using different ID test sets and yield contradictory conclusions on the effective robustness. Our\n  multi-ID evaluation provides a more holistic view where all these models are approximately on a\n  same plane and thus have similar effective robustness.\n We first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets.\n We show the fitting quality in Table 1a and the effective robustness of various models in Table 1b.\n  Compared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and\n  predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE), and\n  thus provides a more precise understanding on the effective robustness. Specifically, while both\n  single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on\n  CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms\n  of the effective robustness, under the single-ID evaluation, ImageNet models achieve 3.91\u00b12.20 (%)\n  and 2.77\u00b11.25 (%) effective robustness on CIFAR-10.2 and CINIC-10, respectively. The positive\n                                                                                                                                                          6", "md": "# Document\n\n## CIFAR-10 Models\n\n## ImageNet Models\n\n## CIFAR-10+ImageNet Models\n\n## ImageNet Models\n\n## YFCC Models\n\n## ImageNet+YFCC Models\n\n1. Cipa4-I0\n\n(a) Using CIFAR-10.2 as the OOD test set. The ImageNet accuracy is mapped to CIFAR-10 classes (see Section 5.1).\n\n(b) Using ImageNet-R as the OOD test set.\n\nFigure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.\n\nRetain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models.\n\n### 5.2 Evaluation on CIFAR-like OOD Test Sets\n\n| |CIFAR-10|ImageNet|\n|---|---|---|\n|95|CIFAR-10|ImageNet|\n|90|CIFAR-10+ImageNet|CIFAR-10+ImageNet|\n|80| | |\n|70| | |\n|50| | |\n|25| | |\n|CIFAR-10.2 accuracy (%)| | |\n\n(a) CIFAR-10.2 accuracy against CIFAR-10 accuracy. ImageNet models have higher CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for CIFAR-10 accuracy only.\n\n(b) CIFAR-10.2 accuracy against ImageNet accuracy. ImageNet models have lower CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for ImageNet accuracy only.\n\nFigure 4: Projected views of Figure 3a. Figure 4a and Figure 4b correspond to single-ID evaluations using different ID test sets and yield contradictory conclusions on the effective robustness. Our multi-ID evaluation provides a more holistic view where all these models are approximately on a same plane and thus have similar effective robustness.\n\nWe first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets. We show the fitting quality in Table 1a and the effective robustness of various models in Table 1b.\n\nCompared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE), and thus provides a more precise understanding on the effective robustness. Specifically, while both single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms of the effective robustness, under the single-ID evaluation, ImageNet models achieve $$3.91\\pm2.20\\%$$ and $$2.77\\pm1.25\\%$$ effective robustness on CIFAR-10.2 and CINIC-10, respectively.", "images": [{"name": "page-6-0.jpg", "height": 195, "width": 195, "x": 101, "y": 40}, {"name": "page-6-1.jpg", "height": 171, "width": 171, "x": 326, "y": 53}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "CIFAR-10 Models", "md": "## CIFAR-10 Models"}, {"type": "heading", "lvl": 2, "value": "ImageNet Models", "md": "## ImageNet Models"}, {"type": "heading", "lvl": 2, "value": "CIFAR-10+ImageNet Models", "md": "## CIFAR-10+ImageNet Models"}, {"type": "heading", "lvl": 2, "value": "ImageNet Models", "md": "## ImageNet Models"}, {"type": "heading", "lvl": 2, "value": "YFCC Models", "md": "## YFCC Models"}, {"type": "heading", "lvl": 2, "value": "ImageNet+YFCC Models", "md": "## ImageNet+YFCC Models"}, {"type": "text", "value": "1. Cipa4-I0\n\n(a) Using CIFAR-10.2 as the OOD test set. The ImageNet accuracy is mapped to CIFAR-10 classes (see Section 5.1).\n\n(b) Using ImageNet-R as the OOD test set.\n\nFigure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.\n\nRetain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models.", "md": "1. Cipa4-I0\n\n(a) Using CIFAR-10.2 as the OOD test set. The ImageNet accuracy is mapped to CIFAR-10 classes (see Section 5.1).\n\n(b) Using ImageNet-R as the OOD test set.\n\nFigure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.\n\nRetain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models."}, {"type": "heading", "lvl": 3, "value": "5.2 Evaluation on CIFAR-like OOD Test Sets", "md": "### 5.2 Evaluation on CIFAR-like OOD Test Sets"}, {"type": "table", "rows": [["", "CIFAR-10", "ImageNet"], ["95", "CIFAR-10", "ImageNet"], ["90", "CIFAR-10+ImageNet", "CIFAR-10+ImageNet"], ["80", "", ""], ["70", "", ""], ["50", "", ""], ["25", "", ""], ["CIFAR-10.2 accuracy (%)", "", ""]], "md": "| |CIFAR-10|ImageNet|\n|---|---|---|\n|95|CIFAR-10|ImageNet|\n|90|CIFAR-10+ImageNet|CIFAR-10+ImageNet|\n|80| | |\n|70| | |\n|50| | |\n|25| | |\n|CIFAR-10.2 accuracy (%)| | |", "isPerfectTable": true, "csv": "\"\",\"CIFAR-10\",\"ImageNet\"\n\"95\",\"CIFAR-10\",\"ImageNet\"\n\"90\",\"CIFAR-10+ImageNet\",\"CIFAR-10+ImageNet\"\n\"80\",\"\",\"\"\n\"70\",\"\",\"\"\n\"50\",\"\",\"\"\n\"25\",\"\",\"\"\n\"CIFAR-10.2 accuracy (%)\",\"\",\"\""}, {"type": "text", "value": "(a) CIFAR-10.2 accuracy against CIFAR-10 accuracy. ImageNet models have higher CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for CIFAR-10 accuracy only.\n\n(b) CIFAR-10.2 accuracy against ImageNet accuracy. ImageNet models have lower CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for ImageNet accuracy only.\n\nFigure 4: Projected views of Figure 3a. Figure 4a and Figure 4b correspond to single-ID evaluations using different ID test sets and yield contradictory conclusions on the effective robustness. Our multi-ID evaluation provides a more holistic view where all these models are approximately on a same plane and thus have similar effective robustness.\n\nWe first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets. We show the fitting quality in Table 1a and the effective robustness of various models in Table 1b.\n\nCompared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE), and thus provides a more precise understanding on the effective robustness. Specifically, while both single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms of the effective robustness, under the single-ID evaluation, ImageNet models achieve $$3.91\\pm2.20\\%$$ and $$2.77\\pm1.25\\%$$ effective robustness on CIFAR-10.2 and CINIC-10, respectively.", "md": "(a) CIFAR-10.2 accuracy against CIFAR-10 accuracy. ImageNet models have higher CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for CIFAR-10 accuracy only.\n\n(b) CIFAR-10.2 accuracy against ImageNet accuracy. ImageNet models have lower CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for ImageNet accuracy only.\n\nFigure 4: Projected views of Figure 3a. Figure 4a and Figure 4b correspond to single-ID evaluations using different ID test sets and yield contradictory conclusions on the effective robustness. Our multi-ID evaluation provides a more holistic view where all these models are approximately on a same plane and thus have similar effective robustness.\n\nWe first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets. We show the fitting quality in Table 1a and the effective robustness of various models in Table 1b.\n\nCompared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE), and thus provides a more precise understanding on the effective robustness. Specifically, while both single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms of the effective robustness, under the single-ID evaluation, ImageNet models achieve $$3.91\\pm2.20\\%$$ and $$2.77\\pm1.25\\%$$ effective robustness on CIFAR-10.2 and CINIC-10, respectively."}]}, {"page": 7, "text": "Table 1: Results on CIFAR-like OOD test sets. 148 models are included, including CIFAR-10\n models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID\n evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models\n and ImageNet models also become closer to 0.\n                          (a) Fitting quality evaluated by R2 and mean absolute error (MAE).\n                                   Test set              R2 (\u2191)                  MAE (%, \u2193)\n                                                 Single-ID     Multi-ID     Single-ID     Multi-ID\n                                CIFAR-10.1         0.996         0.997         1.07         0.93\n                                CIFAR-10.2         0.981         0.996         2.22         0.95\n                                 CINIC-10          0.978         0.990         2.41         1.49\n(b) Effective robustness values (%). We report the mean and standard deviation for three groups of models with\n different training data, respectively.\n                               Test set      Evaluation     CIFAR-10        ImageNet       CIFAR+IN\n                                                            21 models       89 models      38 models\n                             CIFAR-10.1       Single-ID     -1.68\u00b10.92      1.05\u00b11.27      0.02\u00b11.10\n                                              Multi-ID      -1.43\u00b10.92      0.10\u00b11.12      0.19\u00b11.01\n                             CIFAR-10.2       Single-ID     -1.65\u00b10.70      3.91\u00b12.20      -0.64\u00b11.79\n                                              Multi-ID      -0.76\u00b10.77      0.56\u00b11.27      0.03\u00b11.29\n                              CINIC-10        Single-ID     -0.96\u00b11.43      2.77\u00b11.25      -0.10\u00b12.81\n                                              Multi-ID      -0.08\u00b11.52     -0.52\u00b10.98      0.63\u00b12.10\n                               Average        Single-ID     -1.43\u00b10.53      2.58\u00b11.32      -0.24\u00b11.58\n                                              Multi-ID      -0.76\u00b10.63      0.04\u00b10.67      0.28\u00b11.04\n effective robustness values seems to suggest an advantage of ImageNet models compared to CIFAR-\n10 models, which is consistent with the findings in Miller et al. (2021). However, under the multi-ID\n evaluation, the advantage of ImageNet models diminishes, and the effective robustness values of both\n CIFAR-10 models and ImageNet models are much closer to 0. Therefore, the apparent advantage\n reported by prior works can be explained as the effect of training data on the single-ID evaluation,\n and our multi-ID evaluation resolves this confounder to provide a more precise understanding.\n In Figure 3a, we visualize the multi-ID effective robustness on CIFAR-10.2, where the accuracies of\n all the models approximately lie on a plane (the baseline function) on the logit scale, and thus these\n models have similar effective robustness as the OOD accuracy of all the models can be approximately\n predicted using a simple plane. We also show projected views of Figure 3a in Figure 4, where\n Figure 4a and Figure 4b correspond to the single-ID evaluation taking different ID test sets with\n contradictory conclusions. In contrast, our new evaluation provides a more holistic view.\n 5.3    Evaluation on ImageNet-like OOD Test Sets\nWe then conduct evaluation on ImageNet-like OOD test sets, and we compare ImageNet models\nwith models trained on YFCC and LAION, respectively. We show the fitting quality in Table 2 and\n the effective robustness in Tables 3a and 3b. Consistent with results in Section 5.2, our multi-ID\n evaluation improves the fitting quality over the single-ID evaluation to better predict and understand\n the OOD accuracies from ID accuracies. On effective robustness, single-ID evaluation leads to a\n perception of an effective robustness gain when there is mismatch between the training data and the\n single ID test set. Our multi-ID evaluation enjoys a holistic view of all the training distributions and\n suggests that all the models evaluated here have similar effective robustness.\n Specifically, the improvement of fitting quality is particularly significant for models involving LAION\n on ImageNet-R (R2 improved from 0.216 to 0.982 and MAE reduced from 9.23% to 1.32%) and\n ImageNet-Sketch (R2 improved from 0.281 to 0.937 and MAE reduced from 7.90% to 2.10%). On\n the effective robustness values, under the single-ID evaluation, YFCC and LAION models have\n positive effective robustness values (2.59\u00b12.43 (%) on average for YFCC models and 5.96\u00b14.96 (%)\n on average for LAION models), which is consistent with the findings in Fang et al. (2022); Nguyen\n et al. (2022). In contrast, under our multi-ID evaluation, the average effective robustness becomes\n 0.77\u00b10.85 (%) for YFCC models, and -0.00\u00b10.52 (%) for LAION models, much closer to 0. While\n single-ID evaluation used by prior works (Fang et al., 2022; Nguyen et al., 2022) suggests effective\n                                                                  7", "md": "# Results on CIFAR-like OOD test sets\n\n## Results on CIFAR-like OOD test sets\n\n148 models are included, including CIFAR-10 models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models and ImageNet models also become closer to 0.\n\n### (a) Fitting quality evaluated by R2 and mean absolute error (MAE)\n\n|Test set|R2 (\u2191)|MAE (% \u2193)|\n|---|---|---|\n|CIFAR-10.1|0.996|0.997|1.07|0.93|\n|CIFAR-10.2|0.981|0.996|2.22|0.95|\n|CINIC-10|0.978|0.990|2.41|1.49|\n\n### (b) Effective robustness values (%)\n\nWe report the mean and standard deviation for three groups of models with different training data, respectively.\n\n|Test set|Evaluation|CIFAR-10|ImageNet|CIFAR+IN|\n|---|---|---|---|---|\n|CIFAR-10.1|Single-ID|-1.68\u00b10.92|1.05\u00b11.27|0.02\u00b11.10|\n|CIFAR-10.1|Multi-ID|-1.43\u00b10.92|0.10\u00b11.12|0.19\u00b11.01|\n|CIFAR-10.2|Single-ID|-1.65\u00b10.70|3.91\u00b12.20|-0.64\u00b11.79|\n|CIFAR-10.2|Multi-ID|-0.76\u00b10.77|0.56\u00b11.27|0.03\u00b11.29|\n|CINIC-10|Single-ID|-0.96\u00b11.43|2.77\u00b11.25|-0.10\u00b12.81|\n|CINIC-10|Multi-ID|-0.08\u00b11.52|-0.52\u00b10.98|0.63\u00b12.10|\n|Average|Single-ID|-1.43\u00b10.53|2.58\u00b11.32|-0.24\u00b11.58|\n|Average|Multi-ID|-0.76\u00b10.63|0.04\u00b10.67|0.28\u00b11.04|\n\nEffective robustness values seem to suggest an advantage of ImageNet models compared to CIFAR-10 models, which is consistent with the findings in Miller et al. (2021). However, under the multi-ID evaluation, the advantage of ImageNet models diminishes, and the effective robustness values of both CIFAR-10 models and ImageNet models are much closer to 0. Therefore, the apparent advantage reported by prior works can be explained as the effect of training data on the single-ID evaluation, and our multi-ID evaluation resolves this confounder to provide a more precise understanding.\n\nIn Figure 3a, we visualize the multi-ID effective robustness on CIFAR-10.2, where the accuracies of all the models approximately lie on a plane (the baseline function) on the logit scale, and thus these models have similar effective robustness as the OOD accuracy of all the models can be approximately predicted using a simple plane. We also show projected views of Figure 3a in Figure 4, where Figure 4a and Figure 4b correspond to the single-ID evaluation taking different ID test sets with contradictory conclusions. In contrast, our new evaluation provides a more holistic view.\n\n### Evaluation on ImageNet-like OOD Test Sets\n\nWe then conduct evaluation on ImageNet-like OOD test sets, and we compare ImageNet models with models trained on YFCC and LAION, respectively. We show the fitting quality in Table 2 and the effective robustness in Tables 3a and 3b. Consistent with results in Section 5.2, our multi-ID evaluation improves the fitting quality over the single-ID evaluation to better predict and understand the OOD accuracies from ID accuracies. On effective robustness, single-ID evaluation leads to a perception of an effective robustness gain when there is a mismatch between the training data and the single ID test set. Our multi-ID evaluation enjoys a holistic view of all the training distributions and suggests that all the models evaluated here have similar effective robustness.\n\nSpecifically, the improvement of fitting quality is particularly significant for models involving LAION on ImageNet-R (R2 improved from 0.216 to 0.982 and MAE reduced from 9.23% to 1.32%) and ImageNet-Sketch (R2 improved from 0.281 to 0.937 and MAE reduced from 7.90% to 2.10%). On the effective robustness values, under the single-ID evaluation, YFCC and LAION models have positive effective robustness values (2.59\u00b12.43 (%) on average for YFCC models and 5.96\u00b14.96 (%) on average for LAION models), which is consistent with the findings in Fang et al. (2022); Nguyen et al. (2022). In contrast, under our multi-ID evaluation, the average effective robustness becomes 0.77\u00b10.85 (%) for YFCC models, and -0.00\u00b10.52 (%) for LAION models, much closer to 0. While single-ID evaluation used by prior works (Fang et al., 2022; Nguyen et al., 2022) suggests effective", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Results on CIFAR-like OOD test sets", "md": "# Results on CIFAR-like OOD test sets"}, {"type": "heading", "lvl": 2, "value": "Results on CIFAR-like OOD test sets", "md": "## Results on CIFAR-like OOD test sets"}, {"type": "text", "value": "148 models are included, including CIFAR-10 models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models and ImageNet models also become closer to 0.", "md": "148 models are included, including CIFAR-10 models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models and ImageNet models also become closer to 0."}, {"type": "heading", "lvl": 3, "value": "(a) Fitting quality evaluated by R2 and mean absolute error (MAE)", "md": "### (a) Fitting quality evaluated by R2 and mean absolute error (MAE)"}, {"type": "table", "rows": [["Test set", "R2 (\u2191)", "MAE (% \u2193)"], ["CIFAR-10.1", "0.996", "0.997", "1.07", "0.93"], ["CIFAR-10.2", "0.981", "0.996", "2.22", "0.95"], ["CINIC-10", "0.978", "0.990", "2.41", "1.49"]], "md": "|Test set|R2 (\u2191)|MAE (% \u2193)|\n|---|---|---|\n|CIFAR-10.1|0.996|0.997|1.07|0.93|\n|CIFAR-10.2|0.981|0.996|2.22|0.95|\n|CINIC-10|0.978|0.990|2.41|1.49|", "isPerfectTable": false, "csv": "\"Test set\",\"R2 (\u2191)\",\"MAE (% \u2193)\"\n\"CIFAR-10.1\",\"0.996\",\"0.997\",\"1.07\",\"0.93\"\n\"CIFAR-10.2\",\"0.981\",\"0.996\",\"2.22\",\"0.95\"\n\"CINIC-10\",\"0.978\",\"0.990\",\"2.41\",\"1.49\""}, {"type": "heading", "lvl": 3, "value": "(b) Effective robustness values (%)", "md": "### (b) Effective robustness values (%)"}, {"type": "text", "value": "We report the mean and standard deviation for three groups of models with different training data, respectively.", "md": "We report the mean and standard deviation for three groups of models with different training data, respectively."}, {"type": "table", "rows": [["Test set", "Evaluation", "CIFAR-10", "ImageNet", "CIFAR+IN"], ["CIFAR-10.1", "Single-ID", "-1.68\u00b10.92", "1.05\u00b11.27", "0.02\u00b11.10"], ["CIFAR-10.1", "Multi-ID", "-1.43\u00b10.92", "0.10\u00b11.12", "0.19\u00b11.01"], ["CIFAR-10.2", "Single-ID", "-1.65\u00b10.70", "3.91\u00b12.20", "-0.64\u00b11.79"], ["CIFAR-10.2", "Multi-ID", "-0.76\u00b10.77", "0.56\u00b11.27", "0.03\u00b11.29"], ["CINIC-10", "Single-ID", "-0.96\u00b11.43", "2.77\u00b11.25", "-0.10\u00b12.81"], ["CINIC-10", "Multi-ID", "-0.08\u00b11.52", "-0.52\u00b10.98", "0.63\u00b12.10"], ["Average", "Single-ID", "-1.43\u00b10.53", "2.58\u00b11.32", "-0.24\u00b11.58"], ["Average", "Multi-ID", "-0.76\u00b10.63", "0.04\u00b10.67", "0.28\u00b11.04"]], "md": "|Test set|Evaluation|CIFAR-10|ImageNet|CIFAR+IN|\n|---|---|---|---|---|\n|CIFAR-10.1|Single-ID|-1.68\u00b10.92|1.05\u00b11.27|0.02\u00b11.10|\n|CIFAR-10.1|Multi-ID|-1.43\u00b10.92|0.10\u00b11.12|0.19\u00b11.01|\n|CIFAR-10.2|Single-ID|-1.65\u00b10.70|3.91\u00b12.20|-0.64\u00b11.79|\n|CIFAR-10.2|Multi-ID|-0.76\u00b10.77|0.56\u00b11.27|0.03\u00b11.29|\n|CINIC-10|Single-ID|-0.96\u00b11.43|2.77\u00b11.25|-0.10\u00b12.81|\n|CINIC-10|Multi-ID|-0.08\u00b11.52|-0.52\u00b10.98|0.63\u00b12.10|\n|Average|Single-ID|-1.43\u00b10.53|2.58\u00b11.32|-0.24\u00b11.58|\n|Average|Multi-ID|-0.76\u00b10.63|0.04\u00b10.67|0.28\u00b11.04|", "isPerfectTable": true, "csv": "\"Test set\",\"Evaluation\",\"CIFAR-10\",\"ImageNet\",\"CIFAR+IN\"\n\"CIFAR-10.1\",\"Single-ID\",\"-1.68\u00b10.92\",\"1.05\u00b11.27\",\"0.02\u00b11.10\"\n\"CIFAR-10.1\",\"Multi-ID\",\"-1.43\u00b10.92\",\"0.10\u00b11.12\",\"0.19\u00b11.01\"\n\"CIFAR-10.2\",\"Single-ID\",\"-1.65\u00b10.70\",\"3.91\u00b12.20\",\"-0.64\u00b11.79\"\n\"CIFAR-10.2\",\"Multi-ID\",\"-0.76\u00b10.77\",\"0.56\u00b11.27\",\"0.03\u00b11.29\"\n\"CINIC-10\",\"Single-ID\",\"-0.96\u00b11.43\",\"2.77\u00b11.25\",\"-0.10\u00b12.81\"\n\"CINIC-10\",\"Multi-ID\",\"-0.08\u00b11.52\",\"-0.52\u00b10.98\",\"0.63\u00b12.10\"\n\"Average\",\"Single-ID\",\"-1.43\u00b10.53\",\"2.58\u00b11.32\",\"-0.24\u00b11.58\"\n\"Average\",\"Multi-ID\",\"-0.76\u00b10.63\",\"0.04\u00b10.67\",\"0.28\u00b11.04\""}, {"type": "text", "value": "Effective robustness values seem to suggest an advantage of ImageNet models compared to CIFAR-10 models, which is consistent with the findings in Miller et al. (2021). However, under the multi-ID evaluation, the advantage of ImageNet models diminishes, and the effective robustness values of both CIFAR-10 models and ImageNet models are much closer to 0. Therefore, the apparent advantage reported by prior works can be explained as the effect of training data on the single-ID evaluation, and our multi-ID evaluation resolves this confounder to provide a more precise understanding.\n\nIn Figure 3a, we visualize the multi-ID effective robustness on CIFAR-10.2, where the accuracies of all the models approximately lie on a plane (the baseline function) on the logit scale, and thus these models have similar effective robustness as the OOD accuracy of all the models can be approximately predicted using a simple plane. We also show projected views of Figure 3a in Figure 4, where Figure 4a and Figure 4b correspond to the single-ID evaluation taking different ID test sets with contradictory conclusions. In contrast, our new evaluation provides a more holistic view.", "md": "Effective robustness values seem to suggest an advantage of ImageNet models compared to CIFAR-10 models, which is consistent with the findings in Miller et al. (2021). However, under the multi-ID evaluation, the advantage of ImageNet models diminishes, and the effective robustness values of both CIFAR-10 models and ImageNet models are much closer to 0. Therefore, the apparent advantage reported by prior works can be explained as the effect of training data on the single-ID evaluation, and our multi-ID evaluation resolves this confounder to provide a more precise understanding.\n\nIn Figure 3a, we visualize the multi-ID effective robustness on CIFAR-10.2, where the accuracies of all the models approximately lie on a plane (the baseline function) on the logit scale, and thus these models have similar effective robustness as the OOD accuracy of all the models can be approximately predicted using a simple plane. We also show projected views of Figure 3a in Figure 4, where Figure 4a and Figure 4b correspond to the single-ID evaluation taking different ID test sets with contradictory conclusions. In contrast, our new evaluation provides a more holistic view."}, {"type": "heading", "lvl": 3, "value": "Evaluation on ImageNet-like OOD Test Sets", "md": "### Evaluation on ImageNet-like OOD Test Sets"}, {"type": "text", "value": "We then conduct evaluation on ImageNet-like OOD test sets, and we compare ImageNet models with models trained on YFCC and LAION, respectively. We show the fitting quality in Table 2 and the effective robustness in Tables 3a and 3b. Consistent with results in Section 5.2, our multi-ID evaluation improves the fitting quality over the single-ID evaluation to better predict and understand the OOD accuracies from ID accuracies. On effective robustness, single-ID evaluation leads to a perception of an effective robustness gain when there is a mismatch between the training data and the single ID test set. Our multi-ID evaluation enjoys a holistic view of all the training distributions and suggests that all the models evaluated here have similar effective robustness.\n\nSpecifically, the improvement of fitting quality is particularly significant for models involving LAION on ImageNet-R (R2 improved from 0.216 to 0.982 and MAE reduced from 9.23% to 1.32%) and ImageNet-Sketch (R2 improved from 0.281 to 0.937 and MAE reduced from 7.90% to 2.10%). On the effective robustness values, under the single-ID evaluation, YFCC and LAION models have positive effective robustness values (2.59\u00b12.43 (%) on average for YFCC models and 5.96\u00b14.96 (%) on average for LAION models), which is consistent with the findings in Fang et al. (2022); Nguyen et al. (2022). In contrast, under our multi-ID evaluation, the average effective robustness becomes 0.77\u00b10.85 (%) for YFCC models, and -0.00\u00b10.52 (%) for LAION models, much closer to 0. While single-ID evaluation used by prior works (Fang et al., 2022; Nguyen et al., 2022) suggests effective", "md": "We then conduct evaluation on ImageNet-like OOD test sets, and we compare ImageNet models with models trained on YFCC and LAION, respectively. We show the fitting quality in Table 2 and the effective robustness in Tables 3a and 3b. Consistent with results in Section 5.2, our multi-ID evaluation improves the fitting quality over the single-ID evaluation to better predict and understand the OOD accuracies from ID accuracies. On effective robustness, single-ID evaluation leads to a perception of an effective robustness gain when there is a mismatch between the training data and the single ID test set. Our multi-ID evaluation enjoys a holistic view of all the training distributions and suggests that all the models evaluated here have similar effective robustness.\n\nSpecifically, the improvement of fitting quality is particularly significant for models involving LAION on ImageNet-R (R2 improved from 0.216 to 0.982 and MAE reduced from 9.23% to 1.32%) and ImageNet-Sketch (R2 improved from 0.281 to 0.937 and MAE reduced from 7.90% to 2.10%). On the effective robustness values, under the single-ID evaluation, YFCC and LAION models have positive effective robustness values (2.59\u00b12.43 (%) on average for YFCC models and 5.96\u00b14.96 (%) on average for LAION models), which is consistent with the findings in Fang et al. (2022); Nguyen et al. (2022). In contrast, under our multi-ID evaluation, the average effective robustness becomes 0.77\u00b10.85 (%) for YFCC models, and -0.00\u00b10.52 (%) for LAION models, much closer to 0. While single-ID evaluation used by prior works (Fang et al., 2022; Nguyen et al., 2022) suggests effective"}]}, {"page": 8, "text": "Table 2: Fitting quality of single-ID and multi-ID effective robustness, respectively, on ImageNet-\nlike OOD test sets. For ImageNet v.s. YFCC, models involved include ImageNet, YFCC, and\nImageNet+YFCC models. For ImageNet v.s. LAION, models involved include ImageNet, LAION,\nand ImageNet+LAION models. Multi-ID evaluation improves the fitting quality.\n                              ImageNet v.s.            Test set                   R2 (\u2191)                    MAE (%, \u2193)\n                                                                         Single-ID      Multi-ID      Single-ID      Multi-ID\n                                                   ImageNet-V2              0.990         0.999          1.44           0.54\n                                  YFCC              ImageNet-R              0.879         0.965          2.55           1.30\n                              (103 models)       ImageNet-Sketch            0.928         0.945          1.56           1.31\n                                                     ObjectNet              0.903         0.936          2.60           1.98\n                                                   ImageNet-V2              0.992         0.999          1.33           0.51\n                                 LAION              ImageNet-R              0.216         0.982          9.23           1.32\n                              (107 models)       ImageNet-Sketch            0.281         0.937          7.90           2.10\n                                                     ObjectNet              0.849         0.906          2.88           2.38\nTable 3: Single-ID and multi-ID effective robustness (%) of the models on variants of ImageNet OOD\ntest sets. The effective robustness of all the models becomes close to 0 under the multi-ID evaluation.\n        (a) Models involving ImageNet and YFCC.                                         (b) Models involving ImageNet and LAION.\n        Models                Test set          Single-ID       Multi-ID                 Models                 Test set         Single-ID   Multi-ID\n                           ImageNet-V2         -1.23\u00b10.46     -0.19\u00b10.50                                     ImageNet-V2        -1.21\u00b10.56  0.05\u00b10.65\n      ImageNet             ImageNet-R          -2.80\u00b11.34     -0.41\u00b11.83                ImageNet             ImageNet-R         -9.45\u00b12.79  -0.54\u00b11.90\n     (36 models)         ImageNet-Sketch       -1.25\u00b11.90      0.14\u00b12.57               (37 models)         ImageNet-Sketch      -7.63\u00b13.40  -0.72\u00b13.03\n                             ObjectNet         -0.99\u00b14.23      0.74\u00b14.14                                       ObjectNet        -1.90\u00b14.48  1.14\u00b14.18\n                              Average          -1.57\u00b11.20      0.07\u00b11.68                                        Average         -5.05\u00b12.21  -0.02\u00b11.53\n                           ImageNet-V2         1.69\u00b11.84      -0.16\u00b10.57                                     ImageNet-V2         1.42\u00b11.73  -0.03\u00b10.57\n        YFCC               ImageNet-R          3.44\u00b13.25       1.07\u00b11.17                 LAION               ImageNet-R          9.48\u00b18.84  -0.65\u00b11.05\n     (13 models)         ImageNet-Sketch       1.90\u00b12.25       0.89\u00b11.29               (14 models)         ImageNet-Sketch       8.71\u00b17.15  -1.10\u00b11.98\n                             ObjectNet         3.32\u00b12.55       1.27\u00b10.85                                       ObjectNet         4.24\u00b12.39  1.77\u00b11.20\n                              Average          2.59\u00b12.43       0.77\u00b10.85                                        Average          5.96\u00b14.96  -0.00\u00b10.52\n                           ImageNet-V2         0.70\u00b11.75       0.13\u00b10.79                                     ImageNet-V2         0.65\u00b11.43  0.07\u00b10.61\n  ImageNet+YFCC            ImageNet-R          1.31\u00b12.75       0.16\u00b11.76           ImageNet+LAION            ImageNet-R          6.01\u00b18.43  0.56\u00b11.32\n     (54 models)         ImageNet-Sketch       0.69\u00b11.89       0.21\u00b11.52               (56 models)         ImageNet-Sketch       5.99\u00b17.39  1.04\u00b12.46\n                             ObjectNet         0.21\u00b11.98      -0.60\u00b11.01                                       ObjectNet         0.63\u00b12.20  -1.01\u00b11.44\n                              Average          0.73\u00b11.96      -0.03\u00b11.06                                        Average          3.32\u00b14.63  0.16\u00b10.65\nrobustness gains of YFCC models compared to ImageNet models (Figure 5a), all the models have\nsimilar effective robustness under our multi-ID evaluation. Additionally, we provide an ablation study\non using models with mixed training data in Appendix A.3 and additional interactive visualization on\nour website at https://shizhouxing.github.io/effective-robustness.\n5.4      Evaluation on Additional Models\nWe also evaluate additional models that are not used in fitting the baseline functions. We download\nmodels pre-trained by existing works, including OpenCLIP (Ilharco et al., 2021) and SLIP (Mu\net al., 2022). OpenCLIP provides CLIP models trained on YFCC and LAION, and SLIP provides\nYFCC models trained using CLIP and also a combination of self-supervised learning (Chen et al.,\n2020a,b) and CLIP (SimCLR+CLIP namely SLIP). And we also fine-tune CLIP models on ImageNet\nfor models we pre-train on YFCC and LAION. We use both vanilla fine-tuning and also WiSE-\nFT (Wortsman et al., 2022b) which aims to improve the robustness after fine-tuning, using a weight-\nspace ensembling of the pre-trained model and the fine-tuned model. Details are in Appendix B.1.\nIn Table 4, we show results involving YFCC and LAION, respectively. Since these models are not\nused in the fitting, we do not use R2, but we use MAE to measure the fitting quality. Our multi-ID\nevaluation generally reduces MAE compared to the single-ID evaluation, and thus the multi-ID\nevaluation can still more accurately predict the OOD accuracy from the ID accuracies for these\nmodels that are not used in the fitting. The effective robustness values of the models also generally\nbecome closer to 0, especially for the zero-shot CLIP models. The results further validate that\nzero-shot CLIP models, although may achieve high OOD performance if pre-trained with large-scale\ndata (Radford et al., 2021), generally do not improve effective robustness if we control for all the\nID accuracies. Among the models evaluated here, SLIP models on YFCC and WiSE-FT models\nfrom LAION achieve relatively higher average effective robustness compared to other models, under\nour multi-ID evaluation, although the gains are not consistently significant on all the test sets and\nbecome much smaller than those reflected in the single-ID evaluation. However, we are not certain\n                                                                              8", "md": "| |Test set|R2 (\u2191)|MAE (% \u2193)|\n|---|---|---|---|\n| |Single-ID|Multi-ID|Single-ID|Multi-ID|\n|ImageNet-V2| |0.990|0.999|1.44|0.54|\n|YFCC| |0.879|0.965|2.55|1.30|\n|ImageNet-Sketch| |0.928|0.945|1.56|1.31|\n| |ObjectNet|0.903|0.936|2.60|1.98|\n|ImageNet-V2| |0.992|0.999|1.33|0.51|\n|LAION| |0.216|0.982|9.23|1.32|\n|ImageNet-Sketch| |0.281|0.937|7.90|2.10|\n| |ObjectNet|0.849|0.906|2.88|2.38|\n\n|Test set|ImageNet|YFCC|\n|---|---|---|\n|Single-ID|Multi-ID|Single-ID|Multi-ID|\n|ImageNet-V2|-1.23\u00b10.46|-0.19\u00b10.50|1.69\u00b11.84|-0.16\u00b10.57|\n|ImageNet-R|-2.80\u00b11.34|-0.41\u00b11.83|3.44\u00b13.25|1.07\u00b11.17|\n|ImageNet-Sketch|-1.25\u00b11.90|0.14\u00b12.57|1.90\u00b12.25|0.89\u00b11.29|\n|ObjectNet|-0.99\u00b14.23|0.74\u00b14.14|3.32\u00b12.55|1.27\u00b10.85|\n|Average|-1.57\u00b11.20|0.07\u00b11.68|2.59\u00b12.43|0.77\u00b10.85|\n|ImageNet-V2| |-1.21\u00b10.56|0.05\u00b10.65|1.42\u00b11.73|-0.03\u00b10.57|\n|ImageNet-R| |-9.45\u00b12.79|-0.54\u00b11.90|9.48\u00b18.84|-0.65\u00b11.05|\n|ImageNet-Sketch| |-7.63\u00b13.40|-0.72\u00b13.03|8.71\u00b17.15|-1.10\u00b11.98|\n|ObjectNet| |-1.90\u00b14.48|1.14\u00b14.18|4.24\u00b12.39|1.77\u00b11.20|\n|Average| |-5.05\u00b12.21|-0.02\u00b11.53|5.96\u00b14.96|-0.00\u00b10.52|\n\nRobustness gains of YFCC models compared to ImageNet models (Figure 5a), all the models have similar effective robustness under our multi-ID evaluation. Additionally, we provide an ablation study on using models with mixed training data in Appendix A.3 and additional interactive visualization on our website at https://shizhouxing.github.io/effective-robustness.\n\n### Evaluation on Additional Models\n\nWe also evaluate additional models that are not used in fitting the baseline functions. We download models pre-trained by existing works, including OpenCLIP (Ilharco et al., 2021) and SLIP (Mu et al., 2022). OpenCLIP provides CLIP models trained on YFCC and LAION, and SLIP provides YFCC models trained using CLIP and also a combination of self-supervised learning (Chen et al., 2020a,b) and CLIP (SimCLR+CLIP namely SLIP). And we also fine-tune CLIP models on ImageNet for models we pre-train on YFCC and LAION. We use both vanilla fine-tuning and also WiSE-FT (Wortsman et al., 2022b) which aims to improve the robustness after fine-tuning, using a weight-space ensembling of the pre-trained model and the fine-tuned model. Details are in Appendix B.1.\n\nIn Table 4, we show results involving YFCC and LAION, respectively. Since these models are not used in the fitting, we do not use R2, but we use MAE to measure the fitting quality. Our multi-ID evaluation generally reduces MAE compared to the single-ID evaluation, and thus the multi-ID evaluation can still more accurately predict the OOD accuracy from the ID accuracies for these models that are not used in the fitting. The effective robustness values of the models also generally become closer to 0, especially for the zero-shot CLIP models. The results further validate that zero-shot CLIP models, although may achieve high OOD performance if pre-trained with large-scale data (Radford et al., 2021), generally do not improve effective robustness if we control for all the ID accuracies. Among the models evaluated here, SLIP models on YFCC and WiSE-FT models from LAION achieve relatively higher average effective robustness compared to other models, under our multi-ID evaluation, although the gains are not consistently significant on all the test sets and become much smaller than those reflected in the single-ID evaluation. However, we are not certain.", "images": [], "items": [{"type": "table", "rows": [["", "Test set", "R2 (\u2191)", "MAE (% \u2193)"], ["", "Single-ID", "Multi-ID", "Single-ID", "Multi-ID"], ["ImageNet-V2", "", "0.990", "0.999", "1.44", "0.54"], ["YFCC", "", "0.879", "0.965", "2.55", "1.30"], ["ImageNet-Sketch", "", "0.928", "0.945", "1.56", "1.31"], ["", "ObjectNet", "0.903", "0.936", "2.60", "1.98"], ["ImageNet-V2", "", "0.992", "0.999", "1.33", "0.51"], ["LAION", "", "0.216", "0.982", "9.23", "1.32"], ["ImageNet-Sketch", "", "0.281", "0.937", "7.90", "2.10"], ["", "ObjectNet", "0.849", "0.906", "2.88", "2.38"]], "md": "| |Test set|R2 (\u2191)|MAE (% \u2193)|\n|---|---|---|---|\n| |Single-ID|Multi-ID|Single-ID|Multi-ID|\n|ImageNet-V2| |0.990|0.999|1.44|0.54|\n|YFCC| |0.879|0.965|2.55|1.30|\n|ImageNet-Sketch| |0.928|0.945|1.56|1.31|\n| |ObjectNet|0.903|0.936|2.60|1.98|\n|ImageNet-V2| |0.992|0.999|1.33|0.51|\n|LAION| |0.216|0.982|9.23|1.32|\n|ImageNet-Sketch| |0.281|0.937|7.90|2.10|\n| |ObjectNet|0.849|0.906|2.88|2.38|", "isPerfectTable": false, "csv": "\"\",\"Test set\",\"R2 (\u2191)\",\"MAE (% \u2193)\"\n\"\",\"Single-ID\",\"Multi-ID\",\"Single-ID\",\"Multi-ID\"\n\"ImageNet-V2\",\"\",\"0.990\",\"0.999\",\"1.44\",\"0.54\"\n\"YFCC\",\"\",\"0.879\",\"0.965\",\"2.55\",\"1.30\"\n\"ImageNet-Sketch\",\"\",\"0.928\",\"0.945\",\"1.56\",\"1.31\"\n\"\",\"ObjectNet\",\"0.903\",\"0.936\",\"2.60\",\"1.98\"\n\"ImageNet-V2\",\"\",\"0.992\",\"0.999\",\"1.33\",\"0.51\"\n\"LAION\",\"\",\"0.216\",\"0.982\",\"9.23\",\"1.32\"\n\"ImageNet-Sketch\",\"\",\"0.281\",\"0.937\",\"7.90\",\"2.10\"\n\"\",\"ObjectNet\",\"0.849\",\"0.906\",\"2.88\",\"2.38\""}, {"type": "table", "rows": [["Test set", "ImageNet", "YFCC"], ["Single-ID", "Multi-ID", "Single-ID", "Multi-ID"], ["ImageNet-V2", "-1.23\u00b10.46", "-0.19\u00b10.50", "1.69\u00b11.84", "-0.16\u00b10.57"], ["ImageNet-R", "-2.80\u00b11.34", "-0.41\u00b11.83", "3.44\u00b13.25", "1.07\u00b11.17"], ["ImageNet-Sketch", "-1.25\u00b11.90", "0.14\u00b12.57", "1.90\u00b12.25", "0.89\u00b11.29"], ["ObjectNet", "-0.99\u00b14.23", "0.74\u00b14.14", "3.32\u00b12.55", "1.27\u00b10.85"], ["Average", "-1.57\u00b11.20", "0.07\u00b11.68", "2.59\u00b12.43", "0.77\u00b10.85"], ["ImageNet-V2", "", "-1.21\u00b10.56", "0.05\u00b10.65", "1.42\u00b11.73", "-0.03\u00b10.57"], ["ImageNet-R", "", "-9.45\u00b12.79", "-0.54\u00b11.90", "9.48\u00b18.84", "-0.65\u00b11.05"], ["ImageNet-Sketch", "", "-7.63\u00b13.40", "-0.72\u00b13.03", "8.71\u00b17.15", "-1.10\u00b11.98"], ["ObjectNet", "", "-1.90\u00b14.48", "1.14\u00b14.18", "4.24\u00b12.39", "1.77\u00b11.20"], ["Average", "", "-5.05\u00b12.21", "-0.02\u00b11.53", "5.96\u00b14.96", "-0.00\u00b10.52"]], "md": "|Test set|ImageNet|YFCC|\n|---|---|---|\n|Single-ID|Multi-ID|Single-ID|Multi-ID|\n|ImageNet-V2|-1.23\u00b10.46|-0.19\u00b10.50|1.69\u00b11.84|-0.16\u00b10.57|\n|ImageNet-R|-2.80\u00b11.34|-0.41\u00b11.83|3.44\u00b13.25|1.07\u00b11.17|\n|ImageNet-Sketch|-1.25\u00b11.90|0.14\u00b12.57|1.90\u00b12.25|0.89\u00b11.29|\n|ObjectNet|-0.99\u00b14.23|0.74\u00b14.14|3.32\u00b12.55|1.27\u00b10.85|\n|Average|-1.57\u00b11.20|0.07\u00b11.68|2.59\u00b12.43|0.77\u00b10.85|\n|ImageNet-V2| |-1.21\u00b10.56|0.05\u00b10.65|1.42\u00b11.73|-0.03\u00b10.57|\n|ImageNet-R| |-9.45\u00b12.79|-0.54\u00b11.90|9.48\u00b18.84|-0.65\u00b11.05|\n|ImageNet-Sketch| |-7.63\u00b13.40|-0.72\u00b13.03|8.71\u00b17.15|-1.10\u00b11.98|\n|ObjectNet| |-1.90\u00b14.48|1.14\u00b14.18|4.24\u00b12.39|1.77\u00b11.20|\n|Average| |-5.05\u00b12.21|-0.02\u00b11.53|5.96\u00b14.96|-0.00\u00b10.52|", "isPerfectTable": false, "csv": "\"Test set\",\"ImageNet\",\"YFCC\"\n\"Single-ID\",\"Multi-ID\",\"Single-ID\",\"Multi-ID\"\n\"ImageNet-V2\",\"-1.23\u00b10.46\",\"-0.19\u00b10.50\",\"1.69\u00b11.84\",\"-0.16\u00b10.57\"\n\"ImageNet-R\",\"-2.80\u00b11.34\",\"-0.41\u00b11.83\",\"3.44\u00b13.25\",\"1.07\u00b11.17\"\n\"ImageNet-Sketch\",\"-1.25\u00b11.90\",\"0.14\u00b12.57\",\"1.90\u00b12.25\",\"0.89\u00b11.29\"\n\"ObjectNet\",\"-0.99\u00b14.23\",\"0.74\u00b14.14\",\"3.32\u00b12.55\",\"1.27\u00b10.85\"\n\"Average\",\"-1.57\u00b11.20\",\"0.07\u00b11.68\",\"2.59\u00b12.43\",\"0.77\u00b10.85\"\n\"ImageNet-V2\",\"\",\"-1.21\u00b10.56\",\"0.05\u00b10.65\",\"1.42\u00b11.73\",\"-0.03\u00b10.57\"\n\"ImageNet-R\",\"\",\"-9.45\u00b12.79\",\"-0.54\u00b11.90\",\"9.48\u00b18.84\",\"-0.65\u00b11.05\"\n\"ImageNet-Sketch\",\"\",\"-7.63\u00b13.40\",\"-0.72\u00b13.03\",\"8.71\u00b17.15\",\"-1.10\u00b11.98\"\n\"ObjectNet\",\"\",\"-1.90\u00b14.48\",\"1.14\u00b14.18\",\"4.24\u00b12.39\",\"1.77\u00b11.20\"\n\"Average\",\"\",\"-5.05\u00b12.21\",\"-0.02\u00b11.53\",\"5.96\u00b14.96\",\"-0.00\u00b10.52\""}, {"type": "text", "value": "Robustness gains of YFCC models compared to ImageNet models (Figure 5a), all the models have similar effective robustness under our multi-ID evaluation. Additionally, we provide an ablation study on using models with mixed training data in Appendix A.3 and additional interactive visualization on our website at https://shizhouxing.github.io/effective-robustness.", "md": "Robustness gains of YFCC models compared to ImageNet models (Figure 5a), all the models have similar effective robustness under our multi-ID evaluation. Additionally, we provide an ablation study on using models with mixed training data in Appendix A.3 and additional interactive visualization on our website at https://shizhouxing.github.io/effective-robustness."}, {"type": "heading", "lvl": 3, "value": "Evaluation on Additional Models", "md": "### Evaluation on Additional Models"}, {"type": "text", "value": "We also evaluate additional models that are not used in fitting the baseline functions. We download models pre-trained by existing works, including OpenCLIP (Ilharco et al., 2021) and SLIP (Mu et al., 2022). OpenCLIP provides CLIP models trained on YFCC and LAION, and SLIP provides YFCC models trained using CLIP and also a combination of self-supervised learning (Chen et al., 2020a,b) and CLIP (SimCLR+CLIP namely SLIP). And we also fine-tune CLIP models on ImageNet for models we pre-train on YFCC and LAION. We use both vanilla fine-tuning and also WiSE-FT (Wortsman et al., 2022b) which aims to improve the robustness after fine-tuning, using a weight-space ensembling of the pre-trained model and the fine-tuned model. Details are in Appendix B.1.\n\nIn Table 4, we show results involving YFCC and LAION, respectively. Since these models are not used in the fitting, we do not use R2, but we use MAE to measure the fitting quality. Our multi-ID evaluation generally reduces MAE compared to the single-ID evaluation, and thus the multi-ID evaluation can still more accurately predict the OOD accuracy from the ID accuracies for these models that are not used in the fitting. The effective robustness values of the models also generally become closer to 0, especially for the zero-shot CLIP models. The results further validate that zero-shot CLIP models, although may achieve high OOD performance if pre-trained with large-scale data (Radford et al., 2021), generally do not improve effective robustness if we control for all the ID accuracies. Among the models evaluated here, SLIP models on YFCC and WiSE-FT models from LAION achieve relatively higher average effective robustness compared to other models, under our multi-ID evaluation, although the gains are not consistently significant on all the test sets and become much smaller than those reflected in the single-ID evaluation. However, we are not certain.", "md": "We also evaluate additional models that are not used in fitting the baseline functions. We download models pre-trained by existing works, including OpenCLIP (Ilharco et al., 2021) and SLIP (Mu et al., 2022). OpenCLIP provides CLIP models trained on YFCC and LAION, and SLIP provides YFCC models trained using CLIP and also a combination of self-supervised learning (Chen et al., 2020a,b) and CLIP (SimCLR+CLIP namely SLIP). And we also fine-tune CLIP models on ImageNet for models we pre-train on YFCC and LAION. We use both vanilla fine-tuning and also WiSE-FT (Wortsman et al., 2022b) which aims to improve the robustness after fine-tuning, using a weight-space ensembling of the pre-trained model and the fine-tuned model. Details are in Appendix B.1.\n\nIn Table 4, we show results involving YFCC and LAION, respectively. Since these models are not used in the fitting, we do not use R2, but we use MAE to measure the fitting quality. Our multi-ID evaluation generally reduces MAE compared to the single-ID evaluation, and thus the multi-ID evaluation can still more accurately predict the OOD accuracy from the ID accuracies for these models that are not used in the fitting. The effective robustness values of the models also generally become closer to 0, especially for the zero-shot CLIP models. The results further validate that zero-shot CLIP models, although may achieve high OOD performance if pre-trained with large-scale data (Radford et al., 2021), generally do not improve effective robustness if we control for all the ID accuracies. Among the models evaluated here, SLIP models on YFCC and WiSE-FT models from LAION achieve relatively higher average effective robustness compared to other models, under our multi-ID evaluation, although the gains are not consistently significant on all the test sets and become much smaller than those reflected in the single-ID evaluation. However, we are not certain."}]}, {"page": 9, "text": "Table 4: Fitting quality and effective robustness for downloaded and fine-tuned models. The models\nare not used in the fitting and directly evaluated. Note that MAE and effective robustness are different,\nwhere MAE takes absolute values but not effective robustness. For CLIP by Mu et al. (2022) and\nSLIP, only models pre-trained on YFCC are available.\n                                                      Models with pre-training on YFCC                         Models with pre-training on LAION\n       Model                  Test set              MAE (%, \u2193)           Effective Robustness (%)            MAE (%, \u2193)           Effective Robustness (%)\n                                               Single-ID Multi-ID         Single-ID        Multi-ID      Single-ID Multi-ID         Single-ID       Multi-ID\n                         ImageNetN-V2             3.95         0.45       3.95\u00b10.70      -0.33\u00b10.45         4.70         0.38      4.70\u00b10.01       0.38\u00b10.01\n                          ImageNetN-R             8.98         3.12       8.98\u00b11.24       3.12\u00b11.27        39.80         7.49     39.80\u00b10.03 7.49\u00b10.09\n     OpenCLIP          ImageNetN-Sketch           5.32         2.49       5.32\u00b11.07       2.49\u00b10.81        38.92         1.31     38.92\u00b10.00 -1.31\u00b10.23\n                            ObjectNet             5.68         1.04       5.68\u00b10.81      -0.22\u00b11.04         6.94         1.35      6.94\u00b10.09       -1.35\u00b10.04\n                             Average              5.98         1.77       5.98\u00b10.96       1.26\u00b10.89        22.59         2.63     22.59\u00b10.02 1.30\u00b10.07\n                         ImageNetN-V2             1.14         0.85       0.16\u00b11.23       0.77\u00b10.70         0.82         0.91      -0.12\u00b10.89      0.51\u00b10.96\n                          ImageNetN-R             2.90         1.82      -2.90\u00b11.86 -1.82\u00b10.92              4.63         2.38      -4.45\u00b12.89      2.27\u00b12.20\n     Vanilla FT        ImageNetN-Sketch           2.26         3.16       2.20\u00b11.86       3.16\u00b11.19         4.78         7.50      3.47\u00b14.67       7.50\u00b13.29\n                            ObjectNet             3.46         3.11      -3.42\u00b12.36 -3.11\u00b11.43              4.07         2.27      -4.07\u00b12.10 -2.26\u00b11.95\n                             Average              2.44         2.23      -0.99\u00b11.76 -0.25\u00b10.96              3.57         3.27      -1.29\u00b12.19      2.01\u00b11.38\n                         ImageNetN-V2             1.97         1.05       1.97\u00b10.76       1.05\u00b10.59         1.72         0.81      1.72\u00b10.45       0.81\u00b10.48\n                          ImageNetN-R             3.64         1.76       3.64\u00b10.61       1.76\u00b10.60        13.08         7.40     13.08\u00b11.85 7.40\u00b11.10\n     WiSE-FT           ImageNetN-Sketch           5.47         4.41       5.47\u00b11.20       4.41\u00b10.99        16.84        10.21     16.84\u00b11.58 10.21\u00b10.49\n                            ObjectNet             2.12         1.30       2.12\u00b11.38      -0.68\u00b11.34         1.65         1.65      1.52\u00b11.64       0.31\u00b11.67\n                             Average              3.30         2.13       3.30\u00b10.96       1.63\u00b10.80         8.32         5.02      8.29\u00b11.04       4.68\u00b10.55\n                         ImageNetN-V2             4.95         0.83       4.95\u00b10.29      -0.83\u00b10.45\n      CLIP by             ImageNetN-R             6.54         1.86       6.54\u00b12.66      -1.86\u00b11.15\n  Mu et al. (2022)     ImageNetN-Sketch           1.49         4.16       0.58\u00b11.68      -4.16\u00b10.44\n                            ObjectNet             9.32         1.49       9.32\u00b11.45       1.49\u00b10.44\n                             Average              5.57         2.09       5.35\u00b11.49      -1.34\u00b10.28                                 -\n                         ImageNetN-V2             5.25         0.64       5.25\u00b10.57      -0.12\u00b10.84\n                          ImageNetN-R             14.47        6.13      14.47\u00b15.98 5.75\u00b14.77\n        SLIP           ImageNetN-Sketch           7.71         2.90       7.71\u00b14.05       2.11\u00b12.79\n                            ObjectNet             14.79        5.02      14.79\u00b12.77 5.02\u00b11.53\n                             Average              10.56        3.67      10.56\u00b13.11 3.19\u00b12.05\non whether SLIP and WiSE-FT can alter the underlying training distribution, because SLIP uses\nSimCLR that introduces different training images, and WiSE-FT esseentially edits the weights of the\nmodels by weight-space ensembling. Thus, we do not draw a definite conclusion for the effective\nrobustness of SLIP and WiSE-FT models and leave further validation for future work.\n6      Related Work\nFor natural distribution shifts, the linear correlations between the ID and OOD performance\n(\u201caccuracy-on-the-line\u201d (Miller et al., 2021) in the single-ID effective robustness) have earlier\nbeen observed in dataset reproduction works (Recht et al., 2018, 2019; Yadav & Bottou, 2019;\nMiller et al., 2020). Taori et al. (2020) evaluated many ImageNet models on several ImageNet-like\nOOD test sets, and given the widely held linear correlations, they proposed to evaluate effective\nrobustness by controlling for ID accuracy. Miller et al. (2021) further validated accuracy-on-the-line\nwith a broader scope. Nevertheless, accuracy-on-the-line may not hold on some distribution shifts,\nsuch as some corruption shifts (Hendrycks & Dietterich, 2018) and shifts in the wild (Miller et al.,\n2021), and sometimes ID accuracy and OOD accuracy can inversely correlate (Miller et al., 2021;\nTeney et al., 2022). Baek et al. (2022) also observed a linear correlation between ID agreement and\nOOD agreement for a pair of neural networks (namely \u201cagreement-on-the-line\u201d) for testing whether\naccuracy-on-the-line holds by agreement-on-the-line which does not require labeled data. We focus\non distribution shifts where at least accuracy-on-the-line holds for models from each of the training\ndatasets, and we further propose \u201caccuracy-on-the-plane\u201d using multiple ID test sets.\nRecently, CLIP-like models with language image pre-training which has been studied earlier in works\nsuch as Sariyildiz et al. (2020); Zhang et al. (2022); Desai & Johnson (2021), were shown to achieve\nexceptional effective robustness in the single-ID evaluation (Radford et al., 2021; Jia et al., 2021).\nFang et al. (2022) analyzed the cause of the effective robustness gain of CLIP and concluded that the\npre-training data determined the robustness. Nguyen et al. (2022) experimented on more pre-training\ndata, and they observed difference in the single-ID effective robustness of models trained on different\ndata. While Fang et al. (2022); Nguyen et al. (2022) both suggested that the pre-training data could\ndetermine the effective robustness gains, our evaluation suggests that zero-shot CLIP models do not\nhave effective robustness gains. Besides, Kumar et al. (2021); Andreassen et al. (2022); Wortsman\n                                                                                 9", "md": "# Table and Text\n\n## Table 4: Fitting quality and effective robustness for downloaded and fine-tuned models\n\nThe models are not used in the fitting and directly evaluated. Note that MAE and effective robustness are different, where MAE takes absolute values but not effective robustness. For CLIP by Mu et al. (2022) and SLIP, only models pre-trained on YFCC are available.\n\nModel\nTest set\nModels with pre-training on YFCC\nModels with pre-training on LAION\n\nMAE (%, \u2193)\nEffective Robustness (%)\n\nSingle-ID\nMulti-ID\nSingle-ID\nMulti-ID\n\nImageNetN-V2\nImageNetN-R\n3.95\n0.45\n3.95\u00b10.70\n-0.33\u00b10.45\n4.70\n0.38\n4.70\u00b10.01\n0.38\u00b10.01\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Model} & \\text{Test set} & \\text{Models with pre-training on YFCC} & \\text{Models with pre-training on LAION} \\\\\n\\hline\n&  & \\text{MAE (\\%, $\\downarrow$)} &  &  & \\text{Effective Robustness (\\%)} &  &  &  &  \\\\\n\\hline\n&  & \\text{Single-ID} & \\text{Multi-ID} & \\text{Single-ID} & \\text{Multi-ID} &  &  &  &  \\\\\n\\hline\n\\text{ImageNetN-V2} & \\text{ImageNetN-R} & 3.95 & 0.45 & 3.95\\pm0.70 & -0.33\\pm0.45 & 4.70 & 0.38 & 4.70\\pm0.01 & 0.38\\pm0.01 \\\\\n\\hline\n\\end{array}\n$$\n\n...", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Table and Text", "md": "# Table and Text"}, {"type": "heading", "lvl": 2, "value": "Table 4: Fitting quality and effective robustness for downloaded and fine-tuned models", "md": "## Table 4: Fitting quality and effective robustness for downloaded and fine-tuned models"}, {"type": "text", "value": "The models are not used in the fitting and directly evaluated. Note that MAE and effective robustness are different, where MAE takes absolute values but not effective robustness. For CLIP by Mu et al. (2022) and SLIP, only models pre-trained on YFCC are available.\n\nModel\nTest set\nModels with pre-training on YFCC\nModels with pre-training on LAION\n\nMAE (%, \u2193)\nEffective Robustness (%)\n\nSingle-ID\nMulti-ID\nSingle-ID\nMulti-ID\n\nImageNetN-V2\nImageNetN-R\n3.95\n0.45\n3.95\u00b10.70\n-0.33\u00b10.45\n4.70\n0.38\n4.70\u00b10.01\n0.38\u00b10.01\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Model} & \\text{Test set} & \\text{Models with pre-training on YFCC} & \\text{Models with pre-training on LAION} \\\\\n\\hline\n&  & \\text{MAE (\\%, $\\downarrow$)} &  &  & \\text{Effective Robustness (\\%)} &  &  &  &  \\\\\n\\hline\n&  & \\text{Single-ID} & \\text{Multi-ID} & \\text{Single-ID} & \\text{Multi-ID} &  &  &  &  \\\\\n\\hline\n\\text{ImageNetN-V2} & \\text{ImageNetN-R} & 3.95 & 0.45 & 3.95\\pm0.70 & -0.33\\pm0.45 & 4.70 & 0.38 & 4.70\\pm0.01 & 0.38\\pm0.01 \\\\\n\\hline\n\\end{array}\n$$\n\n...", "md": "The models are not used in the fitting and directly evaluated. Note that MAE and effective robustness are different, where MAE takes absolute values but not effective robustness. For CLIP by Mu et al. (2022) and SLIP, only models pre-trained on YFCC are available.\n\nModel\nTest set\nModels with pre-training on YFCC\nModels with pre-training on LAION\n\nMAE (%, \u2193)\nEffective Robustness (%)\n\nSingle-ID\nMulti-ID\nSingle-ID\nMulti-ID\n\nImageNetN-V2\nImageNetN-R\n3.95\n0.45\n3.95\u00b10.70\n-0.33\u00b10.45\n4.70\n0.38\n4.70\u00b10.01\n0.38\u00b10.01\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Model} & \\text{Test set} & \\text{Models with pre-training on YFCC} & \\text{Models with pre-training on LAION} \\\\\n\\hline\n&  & \\text{MAE (\\%, $\\downarrow$)} &  &  & \\text{Effective Robustness (\\%)} &  &  &  &  \\\\\n\\hline\n&  & \\text{Single-ID} & \\text{Multi-ID} & \\text{Single-ID} & \\text{Multi-ID} &  &  &  &  \\\\\n\\hline\n\\text{ImageNetN-V2} & \\text{ImageNetN-R} & 3.95 & 0.45 & 3.95\\pm0.70 & -0.33\\pm0.45 & 4.70 & 0.38 & 4.70\\pm0.01 & 0.38\\pm0.01 \\\\\n\\hline\n\\end{array}\n$$\n\n..."}]}, {"page": 10, "text": "et al. (2022b) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al. (2021);\nSanturkar et al. (2022) studied the transfer performance of CLIP models, which is out of our scope\non the robustness against natural distribution shifts.\n7    Conclusion\nTo conclude, we propose a new and more precise effective robustness evaluation for models with\ndifferent training data. In our evaluation, the OOD accuracy can generally be better predicted from\nmultiple ID accuracies compared to previous effective robustness evaluation with a single ID test.\nWe find that zero-shot CLIP models pre-trained on language-image data do not have better effective\nrobustness compared to standard image classifiers, and we provide a new understanding of the\napparently significant effective robustness gains observed by prior works.\n8    Limitations and Future Work\nThere remain several limitations that may be addressed in future works:\n\u2022 Our method requires fully knowing the training distributions of all the evaluated models, which is\n  not directly applicable for large-scale pre-trained models with private training data. And this also\n  requires the training methods not to significantly alter the training distribution beyond basic data\n  augmentation, while some methods such as SLIP may alter training distributions more significantly,\n  and it is unclear how we can precisely define the training distribution for a model with post-training\n  processing, such as WiSE-FT (Wortsman et al., 2022b) and Model Soups (Wortsman et al., 2022a)\n  with weight-space ensembling. Future work may study how these techniques may impact the ID\n  performance evaluation (Section 5.4).\n\u2022 We assume that the two ID test sets have relatively close distributions compare to the OOD test\n  sets. We have not considered how the difference between the multiple ID test sets may affect the\n  evaluation, and how the effective robustness should be compared if models are trained on highly\n  different distributions.\n\u2022 We use fixed OOD test sets to evaluate the OOD performance, following previous works (Radford\n  et al., 2021; Fang et al., 2022; Nguyen et al., 2022; Schuhmann et al., 2021). When models are\n  pre-trained on large-scale data, it becomes unclear if these \u201cOOD\u201d test sets are still OOD, or if\n  these test sets could be less OOD for the pre-trained models compared to standard classifiers. There\n  may also be some distribution overlap between these test sets and the pre-training datasets, even\n  though Radford et al. (2021) mentioned that the likelihood of direct data overlapping is low.\n\u2022 We focus on distribution shifts where at least \u201caccuracy-on-the-line\u201d from existing works is known\n  to hold for models trained on the same data (Taori et al., 2020; Miller et al., 2021), yet there are\n  counterexamples where \u201caccuracy-on-the-line\u201d does not hold (Section 6) and requires further study.\n  We may set a threshold on the fitting quality and only adopt our method when the fitting quality is\n  sufficiently good. And there are also other OOD test sets (Singla & Feizi, 2021; Rusak et al., 2021;\n  Singla et al., 2022; Idrissi et al., 2022; Li et al., 2023; Moayeri et al., 2022; Vasudevan et al., 2022)\n  that have not been studied in the effective robustness works yet.\n\u2022 While we mostly focus on comparing CLIP-like models with standard image classifiers, due to the\n  notable OOD robustness of CLIP-like models studied in prior works (Radford et al., 2021; Fang\n  et al., 2022; Nguyen et al., 2022), this work may further be extended to cover other types of models\n  (Goyal et al., 2022; Singh et al., 2022) as well as other modalities such as distribution shifts on\n  language data (Miller et al., 2020; Awadalla et al., 2022).\n\u2022 Our multi-ID evaluation is intended for the scenario with models trained on different data. For\n  models trained on a single dataset, while there is often a correlation between the rankings derived\n  from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not\n  necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to\n  replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation\n  comprehensively.\n                                                    10", "md": "et al. (2022b) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al. (2021); Santurkar et al. (2022) studied the transfer performance of CLIP models, which is out of our scope on the robustness against natural distribution shifts.\n\n## Conclusion\n\nTo conclude, we propose a new and more precise effective robustness evaluation for models with different training data. In our evaluation, the OOD accuracy can generally be better predicted from multiple ID accuracies compared to previous effective robustness evaluation with a single ID test. We find that zero-shot CLIP models pre-trained on language-image data do not have better effective robustness compared to standard image classifiers, and we provide a new understanding of the apparently significant effective robustness gains observed by prior works.\n\n## Limitations and Future Work\n\nThere remain several limitations that may be addressed in future works:\n\n- Our method requires fully knowing the training distributions of all the evaluated models, which is not directly applicable for large-scale pre-trained models with private training data. And this also requires the training methods not to significantly alter the training distribution beyond basic data augmentation, while some methods such as SLIP may alter training distributions more significantly, and it is unclear how we can precisely define the training distribution for a model with post-training processing, such as WiSE-FT (Wortsman et al., 2022b) and Model Soups (Wortsman et al., 2022a) with weight-space ensembling. Future work may study how these techniques may impact the ID performance evaluation (Section 5.4).\n- We assume that the two ID test sets have relatively close distributions compared to the OOD test sets. We have not considered how the difference between the multiple ID test sets may affect the evaluation, and how the effective robustness should be compared if models are trained on highly different distributions.\n- We use fixed OOD test sets to evaluate the OOD performance, following previous works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022; Schuhmann et al., 2021). When models are pre-trained on large-scale data, it becomes unclear if these \u201cOOD\u201d test sets are still OOD, or if these test sets could be less OOD for the pre-trained models compared to standard classifiers. There may also be some distribution overlap between these test sets and the pre-training datasets, even though Radford et al. (2021) mentioned that the likelihood of direct data overlapping is low.\n- We focus on distribution shifts where at least \u201caccuracy-on-the-line\u201d from existing works is known to hold for models trained on the same data (Taori et al., 2020; Miller et al., 2021), yet there are counterexamples where \u201caccuracy-on-the-line\u201d does not hold (Section 6) and requires further study. We may set a threshold on the fitting quality and only adopt our method when the fitting quality is sufficiently good. And there are also other OOD test sets (Singla & Feizi, 2021; Rusak et al., 2021; Singla et al., 2022; Idrissi et al., 2022; Li et al., 2023; Moayeri et al., 2022; Vasudevan et al., 2022) that have not been studied in the effective robustness works yet.\n- While we mostly focus on comparing CLIP-like models with standard image classifiers, due to the notable OOD robustness of CLIP-like models studied in prior works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022), this work may further be extended to cover other types of models (Goyal et al., 2022; Singh et al., 2022) as well as other modalities such as distribution shifts on language data (Miller et al., 2020; Awadalla et al., 2022).\n- Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively.", "images": [], "items": [{"type": "text", "value": "et al. (2022b) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al. (2021); Santurkar et al. (2022) studied the transfer performance of CLIP models, which is out of our scope on the robustness against natural distribution shifts.", "md": "et al. (2022b) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al. (2021); Santurkar et al. (2022) studied the transfer performance of CLIP models, which is out of our scope on the robustness against natural distribution shifts."}, {"type": "heading", "lvl": 2, "value": "Conclusion", "md": "## Conclusion"}, {"type": "text", "value": "To conclude, we propose a new and more precise effective robustness evaluation for models with different training data. In our evaluation, the OOD accuracy can generally be better predicted from multiple ID accuracies compared to previous effective robustness evaluation with a single ID test. We find that zero-shot CLIP models pre-trained on language-image data do not have better effective robustness compared to standard image classifiers, and we provide a new understanding of the apparently significant effective robustness gains observed by prior works.", "md": "To conclude, we propose a new and more precise effective robustness evaluation for models with different training data. In our evaluation, the OOD accuracy can generally be better predicted from multiple ID accuracies compared to previous effective robustness evaluation with a single ID test. We find that zero-shot CLIP models pre-trained on language-image data do not have better effective robustness compared to standard image classifiers, and we provide a new understanding of the apparently significant effective robustness gains observed by prior works."}, {"type": "heading", "lvl": 2, "value": "Limitations and Future Work", "md": "## Limitations and Future Work"}, {"type": "text", "value": "There remain several limitations that may be addressed in future works:\n\n- Our method requires fully knowing the training distributions of all the evaluated models, which is not directly applicable for large-scale pre-trained models with private training data. And this also requires the training methods not to significantly alter the training distribution beyond basic data augmentation, while some methods such as SLIP may alter training distributions more significantly, and it is unclear how we can precisely define the training distribution for a model with post-training processing, such as WiSE-FT (Wortsman et al., 2022b) and Model Soups (Wortsman et al., 2022a) with weight-space ensembling. Future work may study how these techniques may impact the ID performance evaluation (Section 5.4).\n- We assume that the two ID test sets have relatively close distributions compared to the OOD test sets. We have not considered how the difference between the multiple ID test sets may affect the evaluation, and how the effective robustness should be compared if models are trained on highly different distributions.\n- We use fixed OOD test sets to evaluate the OOD performance, following previous works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022; Schuhmann et al., 2021). When models are pre-trained on large-scale data, it becomes unclear if these \u201cOOD\u201d test sets are still OOD, or if these test sets could be less OOD for the pre-trained models compared to standard classifiers. There may also be some distribution overlap between these test sets and the pre-training datasets, even though Radford et al. (2021) mentioned that the likelihood of direct data overlapping is low.\n- We focus on distribution shifts where at least \u201caccuracy-on-the-line\u201d from existing works is known to hold for models trained on the same data (Taori et al., 2020; Miller et al., 2021), yet there are counterexamples where \u201caccuracy-on-the-line\u201d does not hold (Section 6) and requires further study. We may set a threshold on the fitting quality and only adopt our method when the fitting quality is sufficiently good. And there are also other OOD test sets (Singla & Feizi, 2021; Rusak et al., 2021; Singla et al., 2022; Idrissi et al., 2022; Li et al., 2023; Moayeri et al., 2022; Vasudevan et al., 2022) that have not been studied in the effective robustness works yet.\n- While we mostly focus on comparing CLIP-like models with standard image classifiers, due to the notable OOD robustness of CLIP-like models studied in prior works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022), this work may further be extended to cover other types of models (Goyal et al., 2022; Singh et al., 2022) as well as other modalities such as distribution shifts on language data (Miller et al., 2020; Awadalla et al., 2022).\n- Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively.", "md": "There remain several limitations that may be addressed in future works:\n\n- Our method requires fully knowing the training distributions of all the evaluated models, which is not directly applicable for large-scale pre-trained models with private training data. And this also requires the training methods not to significantly alter the training distribution beyond basic data augmentation, while some methods such as SLIP may alter training distributions more significantly, and it is unclear how we can precisely define the training distribution for a model with post-training processing, such as WiSE-FT (Wortsman et al., 2022b) and Model Soups (Wortsman et al., 2022a) with weight-space ensembling. Future work may study how these techniques may impact the ID performance evaluation (Section 5.4).\n- We assume that the two ID test sets have relatively close distributions compared to the OOD test sets. We have not considered how the difference between the multiple ID test sets may affect the evaluation, and how the effective robustness should be compared if models are trained on highly different distributions.\n- We use fixed OOD test sets to evaluate the OOD performance, following previous works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022; Schuhmann et al., 2021). When models are pre-trained on large-scale data, it becomes unclear if these \u201cOOD\u201d test sets are still OOD, or if these test sets could be less OOD for the pre-trained models compared to standard classifiers. There may also be some distribution overlap between these test sets and the pre-training datasets, even though Radford et al. (2021) mentioned that the likelihood of direct data overlapping is low.\n- We focus on distribution shifts where at least \u201caccuracy-on-the-line\u201d from existing works is known to hold for models trained on the same data (Taori et al., 2020; Miller et al., 2021), yet there are counterexamples where \u201caccuracy-on-the-line\u201d does not hold (Section 6) and requires further study. We may set a threshold on the fitting quality and only adopt our method when the fitting quality is sufficiently good. And there are also other OOD test sets (Singla & Feizi, 2021; Rusak et al., 2021; Singla et al., 2022; Idrissi et al., 2022; Li et al., 2023; Moayeri et al., 2022; Vasudevan et al., 2022) that have not been studied in the effective robustness works yet.\n- While we mostly focus on comparing CLIP-like models with standard image classifiers, due to the notable OOD robustness of CLIP-like models studied in prior works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022), this work may further be extended to cover other types of models (Goyal et al., 2022; Singh et al., 2022) as well as other modalities such as distribution shifts on language data (Miller et al., 2020; Awadalla et al., 2022).\n- Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively."}]}, {"page": 11, "text": "Acknowledgments & Funding Disclosure\nWe thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive\nfeedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR\nN00014-23-1-2300:P00001.\nReferences\nAndreassen, A. J., Bahri, Y., Neyshabur, B., and Roelofs, R. The evolution of out-of-distribution\n  robustness throughout fine-tuning. Transactions on Machine Learning Research, 2022.\nAwadalla, A., Wortsman, M., Ilharco, G., Min, S., Magnusson, I., Hajishirzi, H., and Schmidt, L.\n  Exploring the landscape of distributional robustness for question answering models. In Findings of\n  the Association for Computational Linguistics: EMNLP 2022, pp. 5971\u20135987, 2022.\nBaek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the perfor-\n  mance of neural networks under distribution shift. Advances in Neural Information Processing\n  Systems, 35:19274\u201319289, 2022.\nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B.\n  Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models.\n  Advances in neural information processing systems, 32, 2019.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of\n  visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR,\n  2020a.\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models\n  are strong semi-supervised learners. Advances in neural information processing systems, 33:\n  22243\u201322255, 2020b.\nDarlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. Cinic-10 is not imagenet or cifar-10.\n  arXiv preprint arXiv:1810.03505, 2018.\nDeng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. Imagenet: A large-scale hierarchical image\n  database. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n  248\u2013255, 2009.\nDesai, K. and Johnson, J. Virtex: Learning visual representations from textual annotations. In\n  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11162\u2013\n  11173, 2021.\nDevillers, B., Choksi, B., Bielawski, R., and VanRullen, R. Does language help generalization in\n  vision models?    In Proceedings of the 25th Conference on Computational Natural Language\n  Learning, pp. 171\u2013182, 2021.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\n  M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16\n  words: Transformers for image recognition at scale. In International Conference on Learning\n  Representations, 2021.\nFang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data deter-\n  mines distributional robustness in contrastive language image pre-training (clip). In International\n  Conference on Machine Learning, pp. 6216\u20136234. PMLR, 2022.\nGoyal, P., Duval, Q., Seessel, I., Caron, M., Singh, M., Misra, I., Sagun, L., Joulin, A., and\n  Bojanowski, P. Vision models are more robust and fair when pretrained on uncurated images\n  without supervision. arXiv preprint arXiv:2202.08360, 2022.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In The\n  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016. doi:\n  10.1109/CVPR.2016.90.\n                                                  11", "md": "# Acknowledgments & Funding Disclosure\n\n## Acknowledgments & Funding Disclosure\n\nWe thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive feedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001.\n\n## References\n\n- Andreassen, A. J., Bahri, Y., Neyshabur, B., and Roelofs, R. The evolution of out-of-distribution robustness throughout fine-tuning. Transactions on Machine Learning Research, 2022.\n- Awadalla, A., Wortsman, M., Ilharco, G., Min, S., Magnusson, I., Hajishirzi, H., and Schmidt, L. Exploring the landscape of distributional robustness for question answering models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5971\u20135987, 2022.\n- Baek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. Advances in Neural Information Processing Systems, 35:19274\u201319289, 2022.\n- Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.\n- Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020a.\n- Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33: 22243\u201322255, 2020b.\n- Darlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505, 2018.\n- Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. Imagenet: A large-scale hierarchical image database. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248\u2013255, 2009.\n- Desai, K. and Johnson, J. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11162\u201311173, 2021.\n- Devillers, B., Choksi, B., Bielawski, R., and VanRullen, R. Does language help generalization in vision models? In Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 171\u2013182, 2021.\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n- Fang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pp. 6216\u20136234. PMLR, 2022.\n- Goyal, P., Duval, Q., Seessel, I., Caron, M., Singh, M., Misra, I., Sagun, L., Joulin, A., and Bojanowski, P. Vision models are more robust and fair when pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360, 2022.\n- He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgments & Funding Disclosure", "md": "# Acknowledgments & Funding Disclosure"}, {"type": "heading", "lvl": 2, "value": "Acknowledgments & Funding Disclosure", "md": "## Acknowledgments & Funding Disclosure"}, {"type": "text", "value": "We thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive feedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001.", "md": "We thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive feedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- Andreassen, A. J., Bahri, Y., Neyshabur, B., and Roelofs, R. The evolution of out-of-distribution robustness throughout fine-tuning. Transactions on Machine Learning Research, 2022.\n- Awadalla, A., Wortsman, M., Ilharco, G., Min, S., Magnusson, I., Hajishirzi, H., and Schmidt, L. Exploring the landscape of distributional robustness for question answering models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5971\u20135987, 2022.\n- Baek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. Advances in Neural Information Processing Systems, 35:19274\u201319289, 2022.\n- Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.\n- Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020a.\n- Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33: 22243\u201322255, 2020b.\n- Darlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505, 2018.\n- Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. Imagenet: A large-scale hierarchical image database. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248\u2013255, 2009.\n- Desai, K. and Johnson, J. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11162\u201311173, 2021.\n- Devillers, B., Choksi, B., Bielawski, R., and VanRullen, R. Does language help generalization in vision models? In Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 171\u2013182, 2021.\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n- Fang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pp. 6216\u20136234. PMLR, 2022.\n- Goyal, P., Duval, Q., Seessel, I., Caron, M., Singh, M., Misra, I., Sagun, L., Joulin, A., and Bojanowski, P. Vision models are more robust and fair when pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360, 2022.\n- He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90.", "md": "- Andreassen, A. J., Bahri, Y., Neyshabur, B., and Roelofs, R. The evolution of out-of-distribution robustness throughout fine-tuning. Transactions on Machine Learning Research, 2022.\n- Awadalla, A., Wortsman, M., Ilharco, G., Min, S., Magnusson, I., Hajishirzi, H., and Schmidt, L. Exploring the landscape of distributional robustness for question answering models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5971\u20135987, 2022.\n- Baek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. Advances in Neural Information Processing Systems, 35:19274\u201319289, 2022.\n- Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.\n- Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597\u20131607. PMLR, 2020a.\n- Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33: 22243\u201322255, 2020b.\n- Darlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505, 2018.\n- Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. Imagenet: A large-scale hierarchical image database. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248\u2013255, 2009.\n- Desai, K. and Johnson, J. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11162\u201311173, 2021.\n- Devillers, B., Choksi, B., Bielawski, R., and VanRullen, R. Does language help generalization in vision models? In Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 171\u2013182, 2021.\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n- Fang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pp. 6216\u20136234. PMLR, 2022.\n- Goyal, P., Duval, Q., Seessel, I., Caron, M., Singh, M., Misra, I., Sagun, L., Joulin, A., and Bojanowski, P. Vision models are more robust and fair when pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360, 2022.\n- He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770\u2013778, 2016. doi: 10.1109/CVPR.2016.90."}]}, {"page": 12, "text": "Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions\n  and perturbations. In International Conference on Learning Representations, 2018.\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli,\n  S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical\n  analysis of out-of-distribution generalization. ICCV, 2021a.\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples.\n  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n  15262\u201315271, 2021b.\nIdrissi, B. Y., Bouchacourt, D., Balestriero, R., Evtimov, I., Hazirbas, C., Ballas, N., Vincent, P.,\n  Drozdzal, M., Lopez-Paz, D., and Ibrahim, M. Imagenet-x: Understanding model mistakes\n  with factor of variation annotations. In The Eleventh International Conference on Learning\n  Representations, 2022.\nIlharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar,\n  V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. Openclip, 2021. URL\n  https://doi.org/10.5281/zenodo.5143773.\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig,\n  T. Scaling up visual and vision-language representation learning with noisy text supervision. In\n  International Conference on Machine Learning, pp. 4904\u20134916. PMLR, 2021.\nKendall, M. Rank correlation methods. 1948.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical\n  Report TR-2009, 2009.\nKumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. Fine-tuning can distort pre-\n  trained features and underperform out-of-distribution. In International Conference on Learning\n  Representations, 2021.\nLi, Z., Evtimov, I., Gordo, A., Hazirbas, C., Hassner, T., Ferrer, C. C., Xu, C., and Ibrahim, M.\n  A whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others.\n  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n  20071\u201320082, 2023.\nLu, S., Nott, B., Olson, A., Todeschini, A., Vahabi, H., Carmon, Y., and Schmidt, L. Harder or\n  different? a closer look at distribution shift in dataset reproduction. In ICML Workshop on\n  Uncertainty and Robustness in Deep Learning, 2020.\nMiller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question\n  answering models. In International Conference on Machine Learning, pp. 6905\u20136916. PMLR,\n  2020.\nMiller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., Liang, P., Carmon, Y.,\n  and Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and\n  in-distribution generalization. In International Conference on Machine Learning, pp. 7721\u20137735.\n  PMLR, 2021.\nMoayeri, M., Singla, S., and Feizi, S. Hard imagenet: Segmentations for objects with strong\n  spurious cues. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\n  Benchmarks Track, 2022.\nMu, N., Kirillov, A., Wagner, D., and Xie, S. Slip: Self-supervision meets language-image pre-\n  training. In European Conference on Computer Vision, pp. 529\u2013544. Springer, 2022.\nNguyen, T., Ilharco, G., Wortsman, M., Oh, S., and Schmidt, L. Quality not quantity: On the\n  interaction between dataset design and robustness of clip. Advances in Neural Information\n  Processing Systems, 35:21455\u201321469, 2022.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,\n  Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision.\n  In International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.\n                                                  12", "md": "# References\n\n# References\n\n- Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations, 2018.\n- Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli,\nS., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization. ICCV, 2021a.\n- Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n15262\u201315271, 2021b.\n- Idrissi, B. Y., Bouchacourt, D., Balestriero, R., Evtimov, I., Hazirbas, C., Ballas, N., Vincent, P.,\nDrozdzal, M., Lopez-Paz, D., and Ibrahim, M. Imagenet-x: Understanding model mistakes\nwith factor of variation annotations. In The Eleventh International Conference on Learning\nRepresentations, 2022.\n- Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar,\nV., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. Openclip, 2021. URL\nhttps://doi.org/10.5281/zenodo.5143773.\n- Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig,\nT. Scaling up visual and vision-language representation learning with noisy text supervision. In\nInternational Conference on Machine Learning, pp. 4904\u20134916. PMLR, 2021.\n- Kendall, M. Rank correlation methods. 1948.\n- Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical\nReport TR-2009, 2009.\n- Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. Fine-tuning can distort pre-\ntrained features and underperform out-of-distribution. In International Conference on Learning\nRepresentations, 2021.\n- Li, Z., Evtimov, I., Gordo, A., Hazirbas, C., Hassner, T., Ferrer, C. C., Xu, C., and Ibrahim, M.\nA whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n20071\u201320082, 2023.\n- Lu, S., Nott, B., Olson, A., Todeschini, A., Vahabi, H., Carmon, Y., and Schmidt, L. Harder or\ndifferent? a closer look at distribution shift in dataset reproduction. In ICML Workshop on\nUncertainty and Robustness in Deep Learning, 2020.\n- Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question\nanswering models. In International Conference on Machine Learning, pp. 6905\u20136916. PMLR,\n2020.\n- Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., Liang, P., Carmon, Y.,\nand Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and\nin-distribution generalization. In International Conference on Machine Learning, pp. 7721\u20137735.\nPMLR, 2021.\n- Moayeri, M., Singla, S., and Feizi, S. Hard imagenet: Segmentations for objects with strong\nspurious cues. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track, 2022.\n- Mu, N., Kirillov, A., Wagner, D., and Xie, S. Slip: Self-supervision meets language-image pre-\ntraining. In European Conference on Computer Vision, pp. 529\u2013544. Springer, 2022.\n- Nguyen, T., Ilharco, G., Wortsman, M., Oh, S., and Schmidt, L. Quality not quantity: On the\ninteraction between dataset design and robustness of clip. Advances in Neural Information\nProcessing Systems, 35:21455\u201321469, 2022.\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,\nMishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision.\nIn International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations, 2018.\n- Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli,\nS., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization. ICCV, 2021a.\n- Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n15262\u201315271, 2021b.\n- Idrissi, B. Y., Bouchacourt, D., Balestriero, R., Evtimov, I., Hazirbas, C., Ballas, N., Vincent, P.,\nDrozdzal, M., Lopez-Paz, D., and Ibrahim, M. Imagenet-x: Understanding model mistakes\nwith factor of variation annotations. In The Eleventh International Conference on Learning\nRepresentations, 2022.\n- Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar,\nV., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. Openclip, 2021. URL\nhttps://doi.org/10.5281/zenodo.5143773.\n- Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig,\nT. Scaling up visual and vision-language representation learning with noisy text supervision. In\nInternational Conference on Machine Learning, pp. 4904\u20134916. PMLR, 2021.\n- Kendall, M. Rank correlation methods. 1948.\n- Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical\nReport TR-2009, 2009.\n- Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. Fine-tuning can distort pre-\ntrained features and underperform out-of-distribution. In International Conference on Learning\nRepresentations, 2021.\n- Li, Z., Evtimov, I., Gordo, A., Hazirbas, C., Hassner, T., Ferrer, C. C., Xu, C., and Ibrahim, M.\nA whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n20071\u201320082, 2023.\n- Lu, S., Nott, B., Olson, A., Todeschini, A., Vahabi, H., Carmon, Y., and Schmidt, L. Harder or\ndifferent? a closer look at distribution shift in dataset reproduction. In ICML Workshop on\nUncertainty and Robustness in Deep Learning, 2020.\n- Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question\nanswering models. In International Conference on Machine Learning, pp. 6905\u20136916. PMLR,\n2020.\n- Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., Liang, P., Carmon, Y.,\nand Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and\nin-distribution generalization. In International Conference on Machine Learning, pp. 7721\u20137735.\nPMLR, 2021.\n- Moayeri, M., Singla, S., and Feizi, S. Hard imagenet: Segmentations for objects with strong\nspurious cues. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track, 2022.\n- Mu, N., Kirillov, A., Wagner, D., and Xie, S. Slip: Self-supervision meets language-image pre-\ntraining. In European Conference on Computer Vision, pp. 529\u2013544. Springer, 2022.\n- Nguyen, T., Ilharco, G., Wortsman, M., Oh, S., and Schmidt, L. Quality not quantity: On the\ninteraction between dataset design and robustness of clip. Advances in Neural Information\nProcessing Systems, 35:21455\u201321469, 2022.\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,\nMishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision.\nIn International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.", "md": "- Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations, 2018.\n- Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli,\nS., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization. ICCV, 2021a.\n- Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n15262\u201315271, 2021b.\n- Idrissi, B. Y., Bouchacourt, D., Balestriero, R., Evtimov, I., Hazirbas, C., Ballas, N., Vincent, P.,\nDrozdzal, M., Lopez-Paz, D., and Ibrahim, M. Imagenet-x: Understanding model mistakes\nwith factor of variation annotations. In The Eleventh International Conference on Learning\nRepresentations, 2022.\n- Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar,\nV., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. Openclip, 2021. URL\nhttps://doi.org/10.5281/zenodo.5143773.\n- Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig,\nT. Scaling up visual and vision-language representation learning with noisy text supervision. In\nInternational Conference on Machine Learning, pp. 4904\u20134916. PMLR, 2021.\n- Kendall, M. Rank correlation methods. 1948.\n- Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical\nReport TR-2009, 2009.\n- Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. Fine-tuning can distort pre-\ntrained features and underperform out-of-distribution. In International Conference on Learning\nRepresentations, 2021.\n- Li, Z., Evtimov, I., Gordo, A., Hazirbas, C., Hassner, T., Ferrer, C. C., Xu, C., and Ibrahim, M.\nA whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n20071\u201320082, 2023.\n- Lu, S., Nott, B., Olson, A., Todeschini, A., Vahabi, H., Carmon, Y., and Schmidt, L. Harder or\ndifferent? a closer look at distribution shift in dataset reproduction. In ICML Workshop on\nUncertainty and Robustness in Deep Learning, 2020.\n- Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question\nanswering models. In International Conference on Machine Learning, pp. 6905\u20136916. PMLR,\n2020.\n- Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., Liang, P., Carmon, Y.,\nand Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and\nin-distribution generalization. In International Conference on Machine Learning, pp. 7721\u20137735.\nPMLR, 2021.\n- Moayeri, M., Singla, S., and Feizi, S. Hard imagenet: Segmentations for objects with strong\nspurious cues. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track, 2022.\n- Mu, N., Kirillov, A., Wagner, D., and Xie, S. Slip: Self-supervision meets language-image pre-\ntraining. In European Conference on Computer Vision, pp. 529\u2013544. Springer, 2022.\n- Nguyen, T., Ilharco, G., Wortsman, M., Oh, S., and Schmidt, L. Quality not quantity: On the\ninteraction between dataset design and robustness of clip. Advances in Neural Information\nProcessing Systems, 35:21455\u201321469, 2022.\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,\nMishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision.\nIn International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021."}]}, {"page": 13, "text": "Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do cifar-10 classifiers generalize to cifar-10?\n   arXiv preprint arXiv:1806.00451, 2018.\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classifiers generalize to imagenet?\n   In International Conference on Machine Learning, pp. 5389\u20135400. PMLR, 2019.\nRusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and\n   Bethge, M. If your data distribution shifts, use self-learning. Transactions on Machine Learning\n   Research, 2021.\nSanturkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T. Is a caption worth a thousand\n   images? a controlled study for representation learning. arXiv preprint arXiv:2207.07635, 2022.\nSariyildiz, M. B., Perez, J., and Larlus, D. Learning visual representations with caption annotations.\n   In European Conference on Computer Vision, pp. 153\u2013170. Springer, 2020.\nSchuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T.,\n   Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-filtered 400 million image-text\n   pairs. arXiv preprint arXiv:2111.02114, 2021.\nSingh, M., Gustafson, L., Adcock, A., de Freitas Reis, V., Gedik, B., Kosaraju, R. P., Mahajan, D.,\n   Girshick, R., Doll\u00e1r, P., and Van Der Maaten, L. Revisiting weakly supervised pre-training of\n   visual perception models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n   Pattern Recognition, pp. 804\u2013814, 2022.\nSingla, S. and Feizi, S. Salient imagenet: How to discover spurious features in deep learning? In\n   International Conference on Learning Representations, 2021.\nSingla, S., Moayeri, M., and Feizi, S. Core risk minimization using salient imagenet. arXiv preprint\n   arXiv:2203.15566, 2022.\nTaori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L. Measuring robustness to\n   natural distribution shifts in image classification. Advances in Neural Information Processing\n   Systems, 33:18583\u201318599, 2020.\nTeney, D., Oh, S. J., and Abbasnejad, E. Id and ood performance are sometimes inversely correlated\n   on real-world datasets. arXiv preprint arXiv:2209.00613, 2022.\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J.\n   Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64\u201373,\n   2016.\nVasudevan, V., Caine, B., Gontijo Lopes, R., Fridovich-Keil, S., and Roelofs, R. When does dough\n   become a bagel? analyzing the remaining mistakes on imagenet. Advances in Neural Information\n   Processing Systems, 35:6720\u20136734, 2022.\nWang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning robust global representations by penalizing\n   local predictive power. Advances in Neural Information Processing Systems, 32, 2019.\nWortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong,\n   H., Farhadi, A., Carmon, Y., Kornblith, S., et al. Model soups: averaging weights of multiple fine-\n   tuned models improves accuracy without increasing inference time. In International Conference\n   on Machine Learning, pp. 23965\u201323998. PMLR, 2022a.\nWortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi,\n   H., Farhadi, A., Namkoong, H., et al. Robust fine-tuning of zero-shot models. In Proceedings of\n   the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959\u20137971, 2022b.\nYadav, C. and Bottou, L. Cold case: The lost mnist digits. Advances in neural information processing\n   systems, 32, 2019.\nZhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Langlotz, C. P. Contrastive learning of\n   medical visual representations from paired images and text. In Machine Learning for Healthcare\n   Conference, pp. 2\u201325. PMLR, 2022.\n                                                  13", "md": "# References\n\n# List of References\n\n- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do cifar-10 classifiers generalize to cifar-10?*\n\narXiv preprint arXiv:1806.00451, 2018.\n- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do imagenet classifiers generalize to imagenet?*\n\nIn International Conference on Machine Learning, pp. 5389\u20135400. PMLR, 2019.\n- Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and Bethge, M.\n*If your data distribution shifts, use self-learning.*\n\nTransactions on Machine Learning Research, 2021.\n- Santurkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T. *Is a caption worth a thousand images? a controlled study for representation learning.*\n\narXiv preprint arXiv:2207.07635, 2022.\n- Sariyildiz, M. B., Perez, J., and Larlus, D. *Learning visual representations with caption annotations.*\n\nIn European Conference on Computer Vision, pp. 153\u2013170. Springer, 2020.\n- Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A.\n*Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.*\n\narXiv preprint arXiv:2111.02114, 2021.\n- Singh, M., Gustafson, L., Adcock, A., de Freitas Reis, V., Gedik, B., Kosaraju, R. P., Mahajan, D., Girshick, R., Doll\u00e1r, P., and Van Der Maaten, L.\n*Revisiting weakly supervised pre-training of visual perception models.*\n\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 804\u2013814, 2022.\n- Singla, S. and Feizi, S. *Salient imagenet: How to discover spurious features in deep learning?*\n\nIn International Conference on Learning Representations, 2021.\n- Singla, S., Moayeri, M., and Feizi, S. *Core risk minimization using salient imagenet.*\n\narXiv preprint arXiv:2203.15566, 2022.\n- Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L.\n*Measuring robustness to natural distribution shifts in image classification.*\n\nAdvances in Neural Information Processing Systems, 33:18583\u201318599, 2020.\n- Teney, D., Oh, S. J., and Abbasnejad, E. *Id and ood performance are sometimes inversely correlated on real-world datasets.*\n\narXiv preprint arXiv:2209.00613, 2022.\n- Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J.\n*Yfcc100m: The new data in multimedia research.*\n\nCommunications of the ACM, 59(2):64\u201373, 2016.\n- Vasudevan, V., Caine, B., Gontijo Lopes, R., Fridovich-Keil, S., and Roelofs, R.\n*When does dough become a bagel? analyzing the remaining mistakes on imagenet.*\n\nAdvances in Neural Information Processing Systems, 35:6720\u20136734, 2022.\n- Wang, H., Ge, S., Lipton, Z., and Xing, E. P.\n*Learning robust global representations by penalizing local predictive power.*\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n- Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al.\n*Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.*\n\nIn International Conference on Machine Learning, pp. 23965\u201323998. PMLR, 2022a.\n- Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.\n*Robust fine-tuning of zero-shot models.*\n\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959\u20137971, 2022b.\n- Yadav, C. and Bottou, L. *Cold case: The lost mnist digits.*\n\nAdvances in neural information processing systems, 32, 2019.\n- Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Langlotz, C. P.\n*Contrastive learning of medical visual representations from paired images and text.*\n\nIn Machine Learning for Healthcare Conference, pp. 2\u201325. PMLR, 2022.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do cifar-10 classifiers generalize to cifar-10?*\n\narXiv preprint arXiv:1806.00451, 2018.\n- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do imagenet classifiers generalize to imagenet?*\n\nIn International Conference on Machine Learning, pp. 5389\u20135400. PMLR, 2019.\n- Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and Bethge, M.\n*If your data distribution shifts, use self-learning.*\n\nTransactions on Machine Learning Research, 2021.\n- Santurkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T. *Is a caption worth a thousand images? a controlled study for representation learning.*\n\narXiv preprint arXiv:2207.07635, 2022.\n- Sariyildiz, M. B., Perez, J., and Larlus, D. *Learning visual representations with caption annotations.*\n\nIn European Conference on Computer Vision, pp. 153\u2013170. Springer, 2020.\n- Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A.\n*Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.*\n\narXiv preprint arXiv:2111.02114, 2021.\n- Singh, M., Gustafson, L., Adcock, A., de Freitas Reis, V., Gedik, B., Kosaraju, R. P., Mahajan, D., Girshick, R., Doll\u00e1r, P., and Van Der Maaten, L.\n*Revisiting weakly supervised pre-training of visual perception models.*\n\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 804\u2013814, 2022.\n- Singla, S. and Feizi, S. *Salient imagenet: How to discover spurious features in deep learning?*\n\nIn International Conference on Learning Representations, 2021.\n- Singla, S., Moayeri, M., and Feizi, S. *Core risk minimization using salient imagenet.*\n\narXiv preprint arXiv:2203.15566, 2022.\n- Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L.\n*Measuring robustness to natural distribution shifts in image classification.*\n\nAdvances in Neural Information Processing Systems, 33:18583\u201318599, 2020.\n- Teney, D., Oh, S. J., and Abbasnejad, E. *Id and ood performance are sometimes inversely correlated on real-world datasets.*\n\narXiv preprint arXiv:2209.00613, 2022.\n- Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J.\n*Yfcc100m: The new data in multimedia research.*\n\nCommunications of the ACM, 59(2):64\u201373, 2016.\n- Vasudevan, V., Caine, B., Gontijo Lopes, R., Fridovich-Keil, S., and Roelofs, R.\n*When does dough become a bagel? analyzing the remaining mistakes on imagenet.*\n\nAdvances in Neural Information Processing Systems, 35:6720\u20136734, 2022.\n- Wang, H., Ge, S., Lipton, Z., and Xing, E. P.\n*Learning robust global representations by penalizing local predictive power.*\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n- Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al.\n*Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.*\n\nIn International Conference on Machine Learning, pp. 23965\u201323998. PMLR, 2022a.\n- Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.\n*Robust fine-tuning of zero-shot models.*\n\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959\u20137971, 2022b.\n- Yadav, C. and Bottou, L. *Cold case: The lost mnist digits.*\n\nAdvances in neural information processing systems, 32, 2019.\n- Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Langlotz, C. P.\n*Contrastive learning of medical visual representations from paired images and text.*\n\nIn Machine Learning for Healthcare Conference, pp. 2\u201325. PMLR, 2022.", "md": "- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do cifar-10 classifiers generalize to cifar-10?*\n\narXiv preprint arXiv:1806.00451, 2018.\n- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do imagenet classifiers generalize to imagenet?*\n\nIn International Conference on Machine Learning, pp. 5389\u20135400. PMLR, 2019.\n- Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and Bethge, M.\n*If your data distribution shifts, use self-learning.*\n\nTransactions on Machine Learning Research, 2021.\n- Santurkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T. *Is a caption worth a thousand images? a controlled study for representation learning.*\n\narXiv preprint arXiv:2207.07635, 2022.\n- Sariyildiz, M. B., Perez, J., and Larlus, D. *Learning visual representations with caption annotations.*\n\nIn European Conference on Computer Vision, pp. 153\u2013170. Springer, 2020.\n- Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A.\n*Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.*\n\narXiv preprint arXiv:2111.02114, 2021.\n- Singh, M., Gustafson, L., Adcock, A., de Freitas Reis, V., Gedik, B., Kosaraju, R. P., Mahajan, D., Girshick, R., Doll\u00e1r, P., and Van Der Maaten, L.\n*Revisiting weakly supervised pre-training of visual perception models.*\n\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 804\u2013814, 2022.\n- Singla, S. and Feizi, S. *Salient imagenet: How to discover spurious features in deep learning?*\n\nIn International Conference on Learning Representations, 2021.\n- Singla, S., Moayeri, M., and Feizi, S. *Core risk minimization using salient imagenet.*\n\narXiv preprint arXiv:2203.15566, 2022.\n- Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L.\n*Measuring robustness to natural distribution shifts in image classification.*\n\nAdvances in Neural Information Processing Systems, 33:18583\u201318599, 2020.\n- Teney, D., Oh, S. J., and Abbasnejad, E. *Id and ood performance are sometimes inversely correlated on real-world datasets.*\n\narXiv preprint arXiv:2209.00613, 2022.\n- Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J.\n*Yfcc100m: The new data in multimedia research.*\n\nCommunications of the ACM, 59(2):64\u201373, 2016.\n- Vasudevan, V., Caine, B., Gontijo Lopes, R., Fridovich-Keil, S., and Roelofs, R.\n*When does dough become a bagel? analyzing the remaining mistakes on imagenet.*\n\nAdvances in Neural Information Processing Systems, 35:6720\u20136734, 2022.\n- Wang, H., Ge, S., Lipton, Z., and Xing, E. P.\n*Learning robust global representations by penalizing local predictive power.*\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n- Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al.\n*Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.*\n\nIn International Conference on Machine Learning, pp. 23965\u201323998. PMLR, 2022a.\n- Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.\n*Robust fine-tuning of zero-shot models.*\n\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959\u20137971, 2022b.\n- Yadav, C. and Bottou, L. *Cold case: The lost mnist digits.*\n\nAdvances in neural information processing systems, 32, 2019.\n- Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Langlotz, C. P.\n*Contrastive learning of medical visual representations from paired images and text.*\n\nIn Machine Learning for Healthcare Conference, pp. 2\u201325. PMLR, 2022."}]}, {"page": 14, "text": " A                   Additional Results\n A.1                     Projected Views\n In Figure 5, we show projected views of Figure 3b.\n                                                   ImageNet                                                                                                                                             ImageNet\n                               95                  YFCC                                                                                                                             50                  YFCC\n                                                   ImageNet+YFCC                                                                                                                                        ImageNet+YFCC\n                               90\n                               80\n                                                                                                                                                                                    25\n                               70\n                               50\n                                                                                                                                                                                    10\n                               25\n                               10\n                      I\n                      m\n                      a\n                      g\n                      e\n                      N\n                      e\n                      t\n                      -\n                      R\n                      a\n                      c\n                      c\n                      u\n                      r\n                      a\n                      c\n                      y\n                      (\n                      c\n                      s\n                      s\n                      .\n                      ,\n                      %\n                      )\n                                                                                                                                                                           I\n                                                                                                                                                                           m\n                                                                                                                                                                           a\n                                                                                                                                                                           g\n                                                                                                                                                                           e\n                                                                                                                                                                           N\n                                                                                                                                                                           e\n                                                                                                                                                                           t\n                                                                                                                                                                           -\n                                                                                                                                                                           R\n                                                                                                                                                                           a\n                                                                                                                                                                           c\n                                                                                                                                                                           c\n                                                                                                                                                                           u\n                                                                                                                                                                           r\n                                                                                                                                                                           a\n                                                                                                                                                                           c\n                                                                                                                                                                           y\n                                                                                                                                                                           (\n                                                                                                                                                                           c\n                                                                                                                                                                           s\n                                                                                                                                                                           s\n                                                                                                                                                                           .\n                                                                                                                                                                           ,\n                                                                                                                                                                           %\n                                                                                                                                                                           )\n                                                                       10                 25                  50             70  80  90  95                                                                                                 10  25  50\n                                                                      ImageNet accuracy (css., %)                                                                                                                      YFCC accuracy (css., %)\n              (a) ImageNet-R accuracy against ImageNet accu-                                                                                                       (b) ImageNet-R accuracy against YFCC accuracy.\n              racy. YFCC models have higher ImageNet-R accu-                                                                                                      YFCC models have similar ImageNet-R accuracy\n              racy compared to ImageNet models when control-                                                                                                       compared to ImageNet models when controlling\n              ling for ImageNet accuracy only.                                                                                                                     for YFCC accuracy only.\nFigure 5: Projected views of Figure 3b. Figure 5a and Figure 5b correspond to single-ID evaluation\n using diffrent ID test sets, Figure 5a suggests effective robustness gains of YFCC models but the\n gains diminish in Figure 5b. Our multi-ID evaluation shows a holistic view where all the models have\n a similar effective robustness.\n A.2                     Agreement between Single-ID and Multi-ID Evaluation\nWe conduct an experiment to check the correlation between the single-ID evaluation and our new\n multi-ID evaluation, in terms of the relative ranking between different models trained on the same\n data. We use Kendall\u2019s rank correlation test (Kendall, 1948) and we report the \u03c4 statistic computed\n by scipy.stats.kendalltau in Tables 5 to 7. Results show that the rankings on the single-ID\n effective robustness and multi-ID effective robustness are positively correlated for CIFAR-10 and\n ImageNet models. There is also a weaker positive correlation for YFCC models. For LAION models,\n there is sometimes a negative correlation. Overall, while there is often a positive correlation between\n the rankings provided by the single-ID evaluation and the multi-ID evaluation, respectively, it is not\n necessarily consistent on all the datasets. Thus, when comparing models trained on the same data,\n our multi-ID evaluation is not intended to replace the single-ID evaluation. Our multi-ID evaluation\n is mainly for comparing models trained on different data, and may be used as a supplementary\n evaluation if all the models are trained on a single dataset.\nTable 5: \u03c4 statistics in the Kendall\u2019s rank correlation test for evaluating the correlation between the\n rankings provided by the single-ID evaluation and the multi-ID evaluation, respectively, for models\n trained on the same data. We consider CIFAR-10 models and ImageNet models, respecitively, on\n CIFAR-like OOD test sets. For ImageNet models, ImageNet instead of CIFAR-10 is used as the ID\n test set in the single-ID evaluation.\n                                                                                                                   Test set                  CIFAR-10.1                         CIFAR-10.2                               CINIC-10\n                                                                                                 CIFAR-10 models                                      0.9619                             0.6667                               0.8095\n                                                                                                  ImageNet models                                     0.1664                             0.1522                               0.4907\n A.3                     Models with Mixed Training Data in the Fitting\nAs mentioned in Section 5.1, we train models with mixed data to obtain models with diverse\n accuracies. In Tables 8 and 9, we show that if we do not include models with mixed training data\n in the fitting, the MAE for these models can become higher, although the difference is not large.\n                                                                                                                                                              14", "md": "## Additional Results\n\n### Projected Views\n\nIn Figure 5, we show projected views of Figure 3b.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{ImageNet} & 95 & \\text{YFCC} & 50 & \\text{ImageNet+YFCC} \\\\\n\\hline\n90 & & & & 25 \\\\\n80 & & & & 10 \\\\\n70 & & & & \\\\\n50 & & & & \\\\\n25 & & & & \\\\\n10 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n10 & 25 & 50 & 70 & 80 & 90 & 95 & & & & \\\\\n\\text{ImageNet accuracy (css., %)} & & & & & & & \\text{YFCC accuracy (css., %)} & & & \\\\\n\\hline\n\\end{array}\n$$\n\n(a) ImageNet-R accuracy against ImageNet accuracy. YFCC models have higher ImageNet-R accuracy compared to ImageNet models when controlling for ImageNet accuracy only.\n\n(b) ImageNet-R accuracy against YFCC accuracy. YFCC models have similar ImageNet-R accuracy compared to ImageNet models when controlling for YFCC accuracy only.\n\nFigure 5: Projected views of Figure 3b. Figure 5a and Figure 5b correspond to single-ID evaluation using different ID test sets, Figure 5a suggests effective robustness gains of YFCC models but the gains diminish in Figure 5b. Our multi-ID evaluation shows a holistic view where all the models have a similar effective robustness.\n\n### Agreement between Single-ID and Multi-ID Evaluation\n\nWe conduct an experiment to check the correlation between the single-ID evaluation and our new multi-ID evaluation, in terms of the relative ranking between different models trained on the same data. We use Kendall\u2019s rank correlation test (Kendall, 1948) and we report the \u03c4 statistic computed by scipy.stats.kendalltau in Tables 5 to 7. Results show that the rankings on the single-ID effective robustness and multi-ID effective robustness are positively correlated for CIFAR-10 and ImageNet models. There is also a weaker positive correlation for YFCC models. For LAION models, there is sometimes a negative correlation. Overall, while there is often a positive correlation between the rankings provided by the single-ID evaluation and the multi-ID evaluation, respectively, it is not necessarily consistent on all the datasets. Thus, when comparing models trained on the same data, our multi-ID evaluation is not intended to replace the single-ID evaluation. Our multi-ID evaluation is mainly for comparing models trained on different data, and may be used as a supplementary evaluation if all the models are trained on a single dataset.\n\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{Test set} & \\text{CIFAR-10.1} & \\text{CIFAR-10.2} & \\text{CINIC-10} \\\\\n\\hline\n\\text{CIFAR-10 models} & 0.9619 & 0.6667 & 0.8095 \\\\\n\\text{ImageNet models} & 0.1664 & 0.1522 & 0.4907 \\\\\n\\hline\n\\end{array}\n$$\n\n### Models with Mixed Training Data in the Fitting\n\nAs mentioned in Section 5.1, we train models with mixed data to obtain models with diverse accuracies. In Tables 8 and 9, we show that if we do not include models with mixed training data in the fitting, the MAE for these models can become higher, although the difference is not large.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Additional Results", "md": "## Additional Results"}, {"type": "heading", "lvl": 3, "value": "Projected Views", "md": "### Projected Views"}, {"type": "text", "value": "In Figure 5, we show projected views of Figure 3b.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{ImageNet} & 95 & \\text{YFCC} & 50 & \\text{ImageNet+YFCC} \\\\\n\\hline\n90 & & & & 25 \\\\\n80 & & & & 10 \\\\\n70 & & & & \\\\\n50 & & & & \\\\\n25 & & & & \\\\\n10 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n10 & 25 & 50 & 70 & 80 & 90 & 95 & & & & \\\\\n\\text{ImageNet accuracy (css., %)} & & & & & & & \\text{YFCC accuracy (css., %)} & & & \\\\\n\\hline\n\\end{array}\n$$\n\n(a) ImageNet-R accuracy against ImageNet accuracy. YFCC models have higher ImageNet-R accuracy compared to ImageNet models when controlling for ImageNet accuracy only.\n\n(b) ImageNet-R accuracy against YFCC accuracy. YFCC models have similar ImageNet-R accuracy compared to ImageNet models when controlling for YFCC accuracy only.\n\nFigure 5: Projected views of Figure 3b. Figure 5a and Figure 5b correspond to single-ID evaluation using different ID test sets, Figure 5a suggests effective robustness gains of YFCC models but the gains diminish in Figure 5b. Our multi-ID evaluation shows a holistic view where all the models have a similar effective robustness.", "md": "In Figure 5, we show projected views of Figure 3b.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{ImageNet} & 95 & \\text{YFCC} & 50 & \\text{ImageNet+YFCC} \\\\\n\\hline\n90 & & & & 25 \\\\\n80 & & & & 10 \\\\\n70 & & & & \\\\\n50 & & & & \\\\\n25 & & & & \\\\\n10 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n10 & 25 & 50 & 70 & 80 & 90 & 95 & & & & \\\\\n\\text{ImageNet accuracy (css., %)} & & & & & & & \\text{YFCC accuracy (css., %)} & & & \\\\\n\\hline\n\\end{array}\n$$\n\n(a) ImageNet-R accuracy against ImageNet accuracy. YFCC models have higher ImageNet-R accuracy compared to ImageNet models when controlling for ImageNet accuracy only.\n\n(b) ImageNet-R accuracy against YFCC accuracy. YFCC models have similar ImageNet-R accuracy compared to ImageNet models when controlling for YFCC accuracy only.\n\nFigure 5: Projected views of Figure 3b. Figure 5a and Figure 5b correspond to single-ID evaluation using different ID test sets, Figure 5a suggests effective robustness gains of YFCC models but the gains diminish in Figure 5b. Our multi-ID evaluation shows a holistic view where all the models have a similar effective robustness."}, {"type": "heading", "lvl": 3, "value": "Agreement between Single-ID and Multi-ID Evaluation", "md": "### Agreement between Single-ID and Multi-ID Evaluation"}, {"type": "text", "value": "We conduct an experiment to check the correlation between the single-ID evaluation and our new multi-ID evaluation, in terms of the relative ranking between different models trained on the same data. We use Kendall\u2019s rank correlation test (Kendall, 1948) and we report the \u03c4 statistic computed by scipy.stats.kendalltau in Tables 5 to 7. Results show that the rankings on the single-ID effective robustness and multi-ID effective robustness are positively correlated for CIFAR-10 and ImageNet models. There is also a weaker positive correlation for YFCC models. For LAION models, there is sometimes a negative correlation. Overall, while there is often a positive correlation between the rankings provided by the single-ID evaluation and the multi-ID evaluation, respectively, it is not necessarily consistent on all the datasets. Thus, when comparing models trained on the same data, our multi-ID evaluation is not intended to replace the single-ID evaluation. Our multi-ID evaluation is mainly for comparing models trained on different data, and may be used as a supplementary evaluation if all the models are trained on a single dataset.\n\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{Test set} & \\text{CIFAR-10.1} & \\text{CIFAR-10.2} & \\text{CINIC-10} \\\\\n\\hline\n\\text{CIFAR-10 models} & 0.9619 & 0.6667 & 0.8095 \\\\\n\\text{ImageNet models} & 0.1664 & 0.1522 & 0.4907 \\\\\n\\hline\n\\end{array}\n$$", "md": "We conduct an experiment to check the correlation between the single-ID evaluation and our new multi-ID evaluation, in terms of the relative ranking between different models trained on the same data. We use Kendall\u2019s rank correlation test (Kendall, 1948) and we report the \u03c4 statistic computed by scipy.stats.kendalltau in Tables 5 to 7. Results show that the rankings on the single-ID effective robustness and multi-ID effective robustness are positively correlated for CIFAR-10 and ImageNet models. There is also a weaker positive correlation for YFCC models. For LAION models, there is sometimes a negative correlation. Overall, while there is often a positive correlation between the rankings provided by the single-ID evaluation and the multi-ID evaluation, respectively, it is not necessarily consistent on all the datasets. Thus, when comparing models trained on the same data, our multi-ID evaluation is not intended to replace the single-ID evaluation. Our multi-ID evaluation is mainly for comparing models trained on different data, and may be used as a supplementary evaluation if all the models are trained on a single dataset.\n\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{Test set} & \\text{CIFAR-10.1} & \\text{CIFAR-10.2} & \\text{CINIC-10} \\\\\n\\hline\n\\text{CIFAR-10 models} & 0.9619 & 0.6667 & 0.8095 \\\\\n\\text{ImageNet models} & 0.1664 & 0.1522 & 0.4907 \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "heading", "lvl": 3, "value": "Models with Mixed Training Data in the Fitting", "md": "### Models with Mixed Training Data in the Fitting"}, {"type": "text", "value": "As mentioned in Section 5.1, we train models with mixed data to obtain models with diverse accuracies. In Tables 8 and 9, we show that if we do not include models with mixed training data in the fitting, the MAE for these models can become higher, although the difference is not large.", "md": "As mentioned in Section 5.1, we train models with mixed data to obtain models with diverse accuracies. In Tables 8 and 9, we show that if we do not include models with mixed training data in the fitting, the MAE for these models can become higher, although the difference is not large."}]}, {"page": 15, "text": "Table 6: \u03c4 statistics (similar to Table 5) for ImageNet models and YFCC models on ImageNet-like\n OOD test sets. For YFCC models, YFCC instead of ImageNet is used as the ID test set in the\n single-ID evaluation.\n                            Test set         ImageNet-V2      ImageNet-R       ImageNet-Sketch      ObjectNet\n                       ImageNet models          0.5142           0.4412             0.8031            0.8063\n                        YFCC models             0.0256           0.8461             0.7179            0.2564\nTable 7: \u03c4 statistics (similar to Table 5) for ImageNet models and LAION models on ImageNet-like\n OOD test sets. For LAION models, LAION instead of ImageNet is used as the ID test set in the\n single-ID evaluation.\n                            Test set         ImageNet-V2      ImageNet-R       ImageNet-Sketch      ObjectNet\n                       ImageNet models          0.3123           0.4384             0.6216            0.9729\n                        LAION models            -0.4945          0.6483             -0.2747           0.6483\n In this work, we train models with data mixed at various ratios and consider models with diverse\n combinations of accuracies, to obtain more convincing conclusions on the effective robustness.\nTable 8: MAE (%) for ImageNet+YFCC models when they are excluded and included in the fitting,\n respectively, where comparing the effective robustness of ImageNet models and YFCC models.\n                                       Test set          ImageNet+YFCC models in the fitting\n                                                         Excluded                Included\n                                   ImageNet-V2              0.71                    0.64\n                                    ImageNet-R              1.30                    1.21\n                                 ImageNet-Sketch            0.98                    0.94\n                                     ObjectNet              1.73                    0.95\n B     Experimental Details\n B.1     Details of Models\nWe use TF-Vision3 under the Apache License Version 2.0 to train standard classifiers on CIFAR-10\n and ImageNet. We follow the configurations provided in TF-Vision for vanilla ResNet training on\n ImageNet and we train ResNet-18, ResNet-50 and ResNet-101 models. We reuse the configurations\n to train models on CIFAR-10, where we only change the dataset, number of classes, and image size,\nwithout tuning hyperparameters for the training. And we load checkpoints of ViT-S/16, ViT-B/16,\n and ViT-L/16 models pre-trained on ImageNet, provided by TF-Vision.\n For training CLIP models, we mostly follow hyperparameters provided in Fang et al. (2022) and the\n implementation in Open-CLIP (Ilharco et al., 2021). While Fang et al. (2022) used a batch size of\n1024, we use 2048 for more parallelism. We use YFCC-15M in Radford et al. (2021), which is a\n subset of YFCC-100M (Thomee et al., 2016). And we use LAION-15M which we uniformly sample\n from LAION-400M (Schuhmann et al., 2021). For fine-tuning CLIP models, we fine-tune for 50,000\n steps, using learning rates 3\u00d710\u22125 and 1\u00d710\u22124, respectively. For WiSE-FT, we take \u03b1 = 0.5 which\n is the coeffi  cient for weight-space ensembling. For OpenCLIP models4, we use ViT-B/32 models\n trained on LAION-400M. For SLIP 5, we use all the CLIP and SLIP models trained on YFCC-15M.\n For data subsampling, we uniformly sample a proportion of training examples from the entire dataset,\n at ratios of {5%, 10%, 20%, 30%, 40%, 50%}, respectively. For combining two training datasets at\nvarious ratios, given a coefficient \u03bb (0 < \u03bb < 1), we uniformly sample a proportion of data from the\n two datasets at ratios of \u03bb and (1 \u2212            \u03bb), respectively, and then we combine the two subsets. When\n combining ImageNet and CIFAR-10, we take \u03bb \u2208                          {0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.995}; when\n combining ImageNet with YFCC and LAION, respectively, we take \u03bb \u2208                                {0.01, 0.1, 0.25, 0.5}.\n     3https://github.com/tensorflow/models/tree/master/official/vision\n     4https://github.com/mlfoundations/open_clip\n     5https://github.com/facebookresearch/SLIP\n                                                                 15", "md": "# Research Results\n\n## Table 6: \u03c4 statistics for ImageNet models and YFCC models on ImageNet-like OOD test sets\n\n|Test set|ImageNet-V2|ImageNet-R|ImageNet-Sketch|ObjectNet|\n|---|---|---|---|---|\n|ImageNet models|0.5142|0.4412|0.8031|0.8063|\n|YFCC models|0.0256|0.8461|0.7179|0.2564|\n\n## Table 7: \u03c4 statistics for ImageNet models and LAION models on ImageNet-like OOD test sets\n\n|Test set|ImageNet-V2|ImageNet-R|ImageNet-Sketch|ObjectNet|\n|---|---|---|---|---|\n|ImageNet models|0.3123|0.4384|0.6216|0.9729|\n|LAION models|-0.4945|0.6483|-0.2747|0.6483|\n\n## Table 8: MAE (%) for ImageNet+YFCC models\n\nWhen they are excluded and included in the fitting, respectively, comparing the effective robustness of ImageNet models and YFCC models.\n\n|Test set|ImageNet+YFCC models in the fitting|Excluded|Included|\n|---|---|---|---|\n|ImageNet-V2| |0.71|0.64|\n|ImageNet-R| |1.30|1.21|\n|ImageNet-Sketch| |0.98|0.94|\n|ObjectNet| |1.73|0.95|\n\n## Experimental Details\n\n### Details of Models\n\nWe use TF-Vision3 under the Apache License Version 2.0 to train standard classifiers on CIFAR-10 and ImageNet. We follow the configurations provided in TF-Vision for vanilla ResNet training on ImageNet and we train ResNet-18, ResNet-50 and ResNet-101 models. We reuse the configurations to train models on CIFAR-10, where we only change the dataset, number of classes, and image size, without tuning hyperparameters for the training. And we load checkpoints of ViT-S/16, ViT-B/16, and ViT-L/16 models pre-trained on ImageNet, provided by TF-Vision.\n\nFor training CLIP models, we mostly follow hyperparameters provided in Fang et al. (2022) and the implementation in Open-CLIP (Ilharco et al., 2021). While Fang et al. (2022) used a batch size of 1024, we use 2048 for more parallelism. We use YFCC-15M in Radford et al. (2021), which is a subset of YFCC-100M (Thomee et al., 2016). And we use LAION-15M which we uniformly sample from LAION-400M (Schuhmann et al., 2021). For fine-tuning CLIP models, we fine-tune for 50,000 steps, using learning rates $$3 \\times 10^{-5}$$ and $$1 \\times 10^{-4}$$, respectively. For WiSE-FT, we take $$\\alpha = 0.5$$ which is the coefficient for weight-space ensembling. For OpenCLIP models, we use ViT-B/32 models trained on LAION-400M. For SLIP, we use all the CLIP and SLIP models trained on YFCC-15M.\n\nFor data subsampling, we uniformly sample a proportion of training examples from the entire dataset, at ratios of {5%, 10%, 20%, 30%, 40%, 50%}, respectively. For combining two training datasets at various ratios, given a coefficient $$\\lambda$$ (0 < $$\\lambda$$ < 1), we uniformly sample a proportion of data from the two datasets at ratios of $$\\lambda$$ and $$(1 - \\lambda)$$, respectively, and then we combine the two subsets. When combining ImageNet and CIFAR-10, we take $$\\lambda \\in \\{0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.995\\}$$; when combining ImageNet with YFCC and LAION, respectively, we take $$\\lambda \\in \\{0.01, 0.1, 0.25, 0.5\\}$$.\n\nSources: TF-Vision, Open-CLIP, SLIP", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Results", "md": "# Research Results"}, {"type": "heading", "lvl": 2, "value": "Table 6: \u03c4 statistics for ImageNet models and YFCC models on ImageNet-like OOD test sets", "md": "## Table 6: \u03c4 statistics for ImageNet models and YFCC models on ImageNet-like OOD test sets"}, {"type": "table", "rows": [["Test set", "ImageNet-V2", "ImageNet-R", "ImageNet-Sketch", "ObjectNet"], ["ImageNet models", "0.5142", "0.4412", "0.8031", "0.8063"], ["YFCC models", "0.0256", "0.8461", "0.7179", "0.2564"]], "md": "|Test set|ImageNet-V2|ImageNet-R|ImageNet-Sketch|ObjectNet|\n|---|---|---|---|---|\n|ImageNet models|0.5142|0.4412|0.8031|0.8063|\n|YFCC models|0.0256|0.8461|0.7179|0.2564|", "isPerfectTable": true, "csv": "\"Test set\",\"ImageNet-V2\",\"ImageNet-R\",\"ImageNet-Sketch\",\"ObjectNet\"\n\"ImageNet models\",\"0.5142\",\"0.4412\",\"0.8031\",\"0.8063\"\n\"YFCC models\",\"0.0256\",\"0.8461\",\"0.7179\",\"0.2564\""}, {"type": "heading", "lvl": 2, "value": "Table 7: \u03c4 statistics for ImageNet models and LAION models on ImageNet-like OOD test sets", "md": "## Table 7: \u03c4 statistics for ImageNet models and LAION models on ImageNet-like OOD test sets"}, {"type": "table", "rows": [["Test set", "ImageNet-V2", "ImageNet-R", "ImageNet-Sketch", "ObjectNet"], ["ImageNet models", "0.3123", "0.4384", "0.6216", "0.9729"], ["LAION models", "-0.4945", "0.6483", "-0.2747", "0.6483"]], "md": "|Test set|ImageNet-V2|ImageNet-R|ImageNet-Sketch|ObjectNet|\n|---|---|---|---|---|\n|ImageNet models|0.3123|0.4384|0.6216|0.9729|\n|LAION models|-0.4945|0.6483|-0.2747|0.6483|", "isPerfectTable": true, "csv": "\"Test set\",\"ImageNet-V2\",\"ImageNet-R\",\"ImageNet-Sketch\",\"ObjectNet\"\n\"ImageNet models\",\"0.3123\",\"0.4384\",\"0.6216\",\"0.9729\"\n\"LAION models\",\"-0.4945\",\"0.6483\",\"-0.2747\",\"0.6483\""}, {"type": "heading", "lvl": 2, "value": "Table 8: MAE (%) for ImageNet+YFCC models", "md": "## Table 8: MAE (%) for ImageNet+YFCC models"}, {"type": "text", "value": "When they are excluded and included in the fitting, respectively, comparing the effective robustness of ImageNet models and YFCC models.", "md": "When they are excluded and included in the fitting, respectively, comparing the effective robustness of ImageNet models and YFCC models."}, {"type": "table", "rows": [["Test set", "ImageNet+YFCC models in the fitting", "Excluded", "Included"], ["ImageNet-V2", "", "0.71", "0.64"], ["ImageNet-R", "", "1.30", "1.21"], ["ImageNet-Sketch", "", "0.98", "0.94"], ["ObjectNet", "", "1.73", "0.95"]], "md": "|Test set|ImageNet+YFCC models in the fitting|Excluded|Included|\n|---|---|---|---|\n|ImageNet-V2| |0.71|0.64|\n|ImageNet-R| |1.30|1.21|\n|ImageNet-Sketch| |0.98|0.94|\n|ObjectNet| |1.73|0.95|", "isPerfectTable": true, "csv": "\"Test set\",\"ImageNet+YFCC models in the fitting\",\"Excluded\",\"Included\"\n\"ImageNet-V2\",\"\",\"0.71\",\"0.64\"\n\"ImageNet-R\",\"\",\"1.30\",\"1.21\"\n\"ImageNet-Sketch\",\"\",\"0.98\",\"0.94\"\n\"ObjectNet\",\"\",\"1.73\",\"0.95\""}, {"type": "heading", "lvl": 2, "value": "Experimental Details", "md": "## Experimental Details"}, {"type": "heading", "lvl": 3, "value": "Details of Models", "md": "### Details of Models"}, {"type": "text", "value": "We use TF-Vision3 under the Apache License Version 2.0 to train standard classifiers on CIFAR-10 and ImageNet. We follow the configurations provided in TF-Vision for vanilla ResNet training on ImageNet and we train ResNet-18, ResNet-50 and ResNet-101 models. We reuse the configurations to train models on CIFAR-10, where we only change the dataset, number of classes, and image size, without tuning hyperparameters for the training. And we load checkpoints of ViT-S/16, ViT-B/16, and ViT-L/16 models pre-trained on ImageNet, provided by TF-Vision.\n\nFor training CLIP models, we mostly follow hyperparameters provided in Fang et al. (2022) and the implementation in Open-CLIP (Ilharco et al., 2021). While Fang et al. (2022) used a batch size of 1024, we use 2048 for more parallelism. We use YFCC-15M in Radford et al. (2021), which is a subset of YFCC-100M (Thomee et al., 2016). And we use LAION-15M which we uniformly sample from LAION-400M (Schuhmann et al., 2021). For fine-tuning CLIP models, we fine-tune for 50,000 steps, using learning rates $$3 \\times 10^{-5}$$ and $$1 \\times 10^{-4}$$, respectively. For WiSE-FT, we take $$\\alpha = 0.5$$ which is the coefficient for weight-space ensembling. For OpenCLIP models, we use ViT-B/32 models trained on LAION-400M. For SLIP, we use all the CLIP and SLIP models trained on YFCC-15M.\n\nFor data subsampling, we uniformly sample a proportion of training examples from the entire dataset, at ratios of {5%, 10%, 20%, 30%, 40%, 50%}, respectively. For combining two training datasets at various ratios, given a coefficient $$\\lambda$$ (0 < $$\\lambda$$ < 1), we uniformly sample a proportion of data from the two datasets at ratios of $$\\lambda$$ and $$(1 - \\lambda)$$, respectively, and then we combine the two subsets. When combining ImageNet and CIFAR-10, we take $$\\lambda \\in \\{0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.995\\}$$; when combining ImageNet with YFCC and LAION, respectively, we take $$\\lambda \\in \\{0.01, 0.1, 0.25, 0.5\\}$$.\n\nSources: TF-Vision, Open-CLIP, SLIP", "md": "We use TF-Vision3 under the Apache License Version 2.0 to train standard classifiers on CIFAR-10 and ImageNet. We follow the configurations provided in TF-Vision for vanilla ResNet training on ImageNet and we train ResNet-18, ResNet-50 and ResNet-101 models. We reuse the configurations to train models on CIFAR-10, where we only change the dataset, number of classes, and image size, without tuning hyperparameters for the training. And we load checkpoints of ViT-S/16, ViT-B/16, and ViT-L/16 models pre-trained on ImageNet, provided by TF-Vision.\n\nFor training CLIP models, we mostly follow hyperparameters provided in Fang et al. (2022) and the implementation in Open-CLIP (Ilharco et al., 2021). While Fang et al. (2022) used a batch size of 1024, we use 2048 for more parallelism. We use YFCC-15M in Radford et al. (2021), which is a subset of YFCC-100M (Thomee et al., 2016). And we use LAION-15M which we uniformly sample from LAION-400M (Schuhmann et al., 2021). For fine-tuning CLIP models, we fine-tune for 50,000 steps, using learning rates $$3 \\times 10^{-5}$$ and $$1 \\times 10^{-4}$$, respectively. For WiSE-FT, we take $$\\alpha = 0.5$$ which is the coefficient for weight-space ensembling. For OpenCLIP models, we use ViT-B/32 models trained on LAION-400M. For SLIP, we use all the CLIP and SLIP models trained on YFCC-15M.\n\nFor data subsampling, we uniformly sample a proportion of training examples from the entire dataset, at ratios of {5%, 10%, 20%, 30%, 40%, 50%}, respectively. For combining two training datasets at various ratios, given a coefficient $$\\lambda$$ (0 < $$\\lambda$$ < 1), we uniformly sample a proportion of data from the two datasets at ratios of $$\\lambda$$ and $$(1 - \\lambda)$$, respectively, and then we combine the two subsets. When combining ImageNet and CIFAR-10, we take $$\\lambda \\in \\{0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.995\\}$$; when combining ImageNet with YFCC and LAION, respectively, we take $$\\lambda \\in \\{0.01, 0.1, 0.25, 0.5\\}$$.\n\nSources: TF-Vision, Open-CLIP, SLIP"}]}, {"page": 16, "text": "Table 9: MAE (%) for ImageNet+LAION models when they are excluded and included in the fitting,\nrespectively, where comparing the effective robustness of ImageNet models and LAION models.\n                                   Test set         ImageNet+LAION models in the fitting\n                                                    Excluded               Included\n                                ImageNet-V2            0.54                  0.51\n                                 ImageNet-R            1.56                  1.19\n                              ImageNet-Sketch          2.62                  2.03\n                                  ObjectNet            2.91                  1.47\nModels are trained using 4\u00d74 or 4\u00d78 TPU v2 Pods, and they are evaluated using NVIDIA V100\nGPUs, on the cloud.\nB.2     Details of ID Test Sets\nWe construct an ID test set from YFCC-15M and LAION-15M, respectively, and we automatically\ngenerate classification labels by matching text with ImageNet class names, which has also been\nsimilarly performed in Fang et al. (2022) for training classifiers using caption data. On YFCC-15M\nwhich contains metadata, we use tags for matching. On LAION-15M which does not provide\nmetadata such as tags, we simply use the entire text for matching. We adopt a label only if a unique\nImageNet class can be determined by matching for the involved image. We then construct a balanced\nand labelled test set by keeping 50 examples for each class that has at least 100 examples in the\nlabelled images. The test examples are then held out from the training data. For the YFCC test set,\nthere are 22550 examples for 451 classes; and for the LAION test set, there are 20400 examples 408\nclasses.\nB.3     Licenses of the Datasets\nWe have used the following datasets:\n         \u2022 CIFAR-10 (Krizhevsky et al., 2009). License is not clearly known.\n         \u2022 YFCC6 under various Creative Commons licenses.\n         \u2022 LAION7 under the Creative Common CC-BY 4.0 license for the metadata, and the images\n           are under the copyright of the original authors.\n         \u2022 CIFAR-10.18 under the MIT license.\n         \u2022 CIFAR-10.29. License is not clearly known.\n         \u2022 CINIC-1010 under the MIT license.\n         \u2022 ImageNet-V211 under the MIT license.\n         \u2022 ImageNet-R12 under the MIT license.\n         \u2022 ImageNet-Sketch13 under the MIT license.\n         \u2022 ObjectNet14 with a license provided on the webpage.\n    6https://multimediacommons.wordpress.com/yfcc100m-core-dataset\n    7https://laion.ai/blog/laion-400-open-dataset\n    8https://github.com/modestyachts/CIFAR-10.1\n    9https://github.com/modestyachts/cifar-10.2\n   10https://github.com/BayesWatch/cinic-10\n   11https://github.com/modestyachts/ImageNetV2\n   12https://github.com/hendrycks/imagenet-r\n   13https://github.com/HaohanWang/ImageNet-Sketch\n   14https://objectnet.dev/download.html\n                                                            16", "md": "|Test set|ImageNet+LAION models in the fitting| |Excluded|Included|\n|---|---|---|---|---|\n| |ImageNet-V2|0.54|0.51| |\n| |ImageNet-R|1.56|1.19| |\n|ImageNet-Sketch|2.62|2.03| | |\n| |ObjectNet|2.91|1.47| |\n\nModels are trained using 4x4 or 4x8 TPU v2 Pods, and they are evaluated using NVIDIA V100 GPUs, on the cloud.\n\n## Details of ID Test Sets\n\nWe construct an ID test set from YFCC-15M and LAION-15M, respectively, and we automatically generate classification labels by matching text with ImageNet class names, which has also been similarly performed in Fang et al. (2022) for training classifiers using caption data. On YFCC-15M which contains metadata, we use tags for matching. On LAION-15M which does not provide metadata such as tags, we simply use the entire text for matching. We adopt a label only if a unique ImageNet class can be determined by matching for the involved image. We then construct a balanced and labelled test set by keeping 50 examples for each class that has at least 100 examples in the labelled images. The test examples are then held out from the training data. For the YFCC test set, there are 22550 examples for 451 classes; and for the LAION test set, there are 20400 examples 408 classes.\n\n## Licenses of the Datasets\n\nWe have used the following datasets:\n\n- CIFAR-10 (Krizhevsky et al., 2009). License is not clearly known.\n- YFCC6 under various Creative Commons licenses.\n- LAION7 under the Creative Common CC-BY 4.0 license for the metadata, and the images are under the copyright of the original authors.\n- CIFAR-10.18 under the MIT license.\n- CIFAR-10.29. License is not clearly known.\n- CINIC-1010 under the MIT license.\n- ImageNet-V211 under the MIT license.\n- ImageNet-R12 under the MIT license.\n- ImageNet-Sketch13 under the MIT license.\n- ObjectNet14 with a license provided on the webpage.", "images": [], "items": [{"type": "table", "rows": [["Test set", "ImageNet+LAION models in the fitting", "", "Excluded", "Included"], ["", "ImageNet-V2", "0.54", "0.51", ""], ["", "ImageNet-R", "1.56", "1.19", ""], ["ImageNet-Sketch", "2.62", "2.03", "", ""], ["", "ObjectNet", "2.91", "1.47", ""]], "md": "|Test set|ImageNet+LAION models in the fitting| |Excluded|Included|\n|---|---|---|---|---|\n| |ImageNet-V2|0.54|0.51| |\n| |ImageNet-R|1.56|1.19| |\n|ImageNet-Sketch|2.62|2.03| | |\n| |ObjectNet|2.91|1.47| |", "isPerfectTable": true, "csv": "\"Test set\",\"ImageNet+LAION models in the fitting\",\"\",\"Excluded\",\"Included\"\n\"\",\"ImageNet-V2\",\"0.54\",\"0.51\",\"\"\n\"\",\"ImageNet-R\",\"1.56\",\"1.19\",\"\"\n\"ImageNet-Sketch\",\"2.62\",\"2.03\",\"\",\"\"\n\"\",\"ObjectNet\",\"2.91\",\"1.47\",\"\""}, {"type": "text", "value": "Models are trained using 4x4 or 4x8 TPU v2 Pods, and they are evaluated using NVIDIA V100 GPUs, on the cloud.", "md": "Models are trained using 4x4 or 4x8 TPU v2 Pods, and they are evaluated using NVIDIA V100 GPUs, on the cloud."}, {"type": "heading", "lvl": 2, "value": "Details of ID Test Sets", "md": "## Details of ID Test Sets"}, {"type": "text", "value": "We construct an ID test set from YFCC-15M and LAION-15M, respectively, and we automatically generate classification labels by matching text with ImageNet class names, which has also been similarly performed in Fang et al. (2022) for training classifiers using caption data. On YFCC-15M which contains metadata, we use tags for matching. On LAION-15M which does not provide metadata such as tags, we simply use the entire text for matching. We adopt a label only if a unique ImageNet class can be determined by matching for the involved image. We then construct a balanced and labelled test set by keeping 50 examples for each class that has at least 100 examples in the labelled images. The test examples are then held out from the training data. For the YFCC test set, there are 22550 examples for 451 classes; and for the LAION test set, there are 20400 examples 408 classes.", "md": "We construct an ID test set from YFCC-15M and LAION-15M, respectively, and we automatically generate classification labels by matching text with ImageNet class names, which has also been similarly performed in Fang et al. (2022) for training classifiers using caption data. On YFCC-15M which contains metadata, we use tags for matching. On LAION-15M which does not provide metadata such as tags, we simply use the entire text for matching. We adopt a label only if a unique ImageNet class can be determined by matching for the involved image. We then construct a balanced and labelled test set by keeping 50 examples for each class that has at least 100 examples in the labelled images. The test examples are then held out from the training data. For the YFCC test set, there are 22550 examples for 451 classes; and for the LAION test set, there are 20400 examples 408 classes."}, {"type": "heading", "lvl": 2, "value": "Licenses of the Datasets", "md": "## Licenses of the Datasets"}, {"type": "text", "value": "We have used the following datasets:\n\n- CIFAR-10 (Krizhevsky et al., 2009). License is not clearly known.\n- YFCC6 under various Creative Commons licenses.\n- LAION7 under the Creative Common CC-BY 4.0 license for the metadata, and the images are under the copyright of the original authors.\n- CIFAR-10.18 under the MIT license.\n- CIFAR-10.29. License is not clearly known.\n- CINIC-1010 under the MIT license.\n- ImageNet-V211 under the MIT license.\n- ImageNet-R12 under the MIT license.\n- ImageNet-Sketch13 under the MIT license.\n- ObjectNet14 with a license provided on the webpage.", "md": "We have used the following datasets:\n\n- CIFAR-10 (Krizhevsky et al., 2009). License is not clearly known.\n- YFCC6 under various Creative Commons licenses.\n- LAION7 under the Creative Common CC-BY 4.0 license for the metadata, and the images are under the copyright of the original authors.\n- CIFAR-10.18 under the MIT license.\n- CIFAR-10.29. License is not clearly known.\n- CINIC-1010 under the MIT license.\n- ImageNet-V211 under the MIT license.\n- ImageNet-R12 under the MIT license.\n- ImageNet-Sketch13 under the MIT license.\n- ObjectNet14 with a license provided on the webpage."}]}], "job_id": "ef725e87-9f67-4a94-859d-ec9285017cf0", "file_path": "./corpus/2302.01381.pdf"}