{"pages": [{"page": 1, "text": "                             A Competitive Algorithm for Agnostic Active\n                                                                Learning\n                                           Eric Price                                     Yihan Zhou\n                               Department of Computer Science                  Department of Computer Science\n                                 University of Texas at Austin                   University of Texas at Austin\narXiv:2310.18786v2  [cs.LG]  16 Dec 2023\n                                  ecprice@cs.utexas.edu                           joeyzhou@cs.utexas.edu\n                                                                   Abstract\n                              For some hypothesis classes and input distributions, active agnostic learning needs\n                              exponentially fewer samples than passive learning; for other classes and distribu-\n                              tions, it offers little to no improvement. The most popular algorithms for agnostic\n                              active learning express their performance in terms of a parameter called the dis-\n                              agreement coefficient, but it is known that these algorithms are inefficient on some\n                              inputs.\n                              We take a different approach to agnostic active learning, getting an algorithm that\n                              is competitive with the optimal algorithm for any binary hypothesis class H and\n                              distribution DX over X. In particular, if any algorithm can use m\u2217         queries to\n                              get O(\u03b7) error, then our algorithm uses O(m\u2217      log |H|) queries to get O(\u03b7) error.\n                              Our algorithm lies in the vein of the splitting-based approach of Dasgupta [2004],\n                              which gets a similar result for the realizable (\u03b7 = 0) setting.\n                              We also show that it is NP-hard to do better than our algorithm\u2019s O(log |H|) over-\n                              head in general.\n                    1    Introduction\n                    Active learning is motivated by settings where unlabeled data is cheap but labeling it is expen-\n                    sive. By carefully choosing which points to label, one can often achieve significant reductions in\n                    label complexity [Cohn et al., 1994]. A canonical example with exponential improvement is one-\n                    dimensional threshold functions h\u03c4(x) := 1x\u2265\u03c4: in the noiseless setting, an active learner can use\n                    binary search to find an \u03b5-approximation solution in O     log 1  queries, while a passive learner needs\n                                                                                   \u03b5\n                    \u0398   1  samples [Cohn et al., 1994, Dasgupta, 2005, Nowak, 2011].\n                        \u03b5\n                    In this paper we are concerned with agnostic binary classification. We are given a hypothesis class\n                    H of binary hypotheses h : X \u2192      {0, 1} such that some h\u2217   \u2208  H has err(h\u2217) \u2264    \u03b7, where the error\n                                                          err(h) :=    Pr\n                                                                     (x,y)\u223cD[h(x) \u0338= y]\n                    is measured with respect to an unknown distribution D over X \u00d7{0, 1}. In our active setting, we\n                    also know the marginal distribution DX of x, and can query any point x of our choosing to receive a\n                    sample y \u223c   (Y | X = x) for (X, Y ) \u223c     D. The goal is to output some   h with err( h) \u2264  \u03b7 + \u03b5, using\n                    as few queries as possible.\n                    The first interesting results for agnostic active learning were shown by Balcan et al. [2006], who\n                    gave an algorithm called Agnostic Active (A2) that gets logarithmic dependence on \u03b5 in some natural\n                    settings: it needs O   log 1   samples for the 1d linear threshold setting (binary search), as long as\n                                               \u03b5\n                    as \u03b5 > 16\u03b7, and   O   d2 log 1  samples for d-dimensional linear thresholds when DX is the uniform\n                                                 \u03b5\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# A Competitive Algorithm for Agnostic Active Learning\n\n# A Competitive Algorithm for Agnostic Active Learning\n\nEric Price - Department of Computer Science, University of Texas at Austin\n\nYihan Zhou - Department of Computer Science, University of Texas at Austin\n\narXiv:2310.18786v2 [cs.LG] 16 Dec 2023\n\nEmail: ecprice@cs.utexas.edu, joeyzhou@cs.utexas.edu\n\n## Abstract\n\nFor some hypothesis classes and input distributions, active agnostic learning needs exponentially fewer samples than passive learning; for other classes and distributions, it offers little to no improvement. The most popular algorithms for agnostic active learning express their performance in terms of a parameter called the disagreement coefficient, but it is known that these algorithms are inefficient on some inputs.\n\nWe take a different approach to agnostic active learning, getting an algorithm that is competitive with the optimal algorithm for any binary hypothesis class H and distribution DX over X. In particular, if any algorithm can use $$m^*$$ queries to get $$O(\\eta)$$ error, then our algorithm uses $$O(m^* \\log |H|)$$ queries to get $$O(\\eta)$$ error.\n\nOur algorithm lies in the vein of the splitting-based approach of Dasgupta [2004], which gets a similar result for the realizable ($$\\eta = 0$$) setting.\n\nWe also show that it is NP-hard to do better than our algorithm\u2019s $$O(\\log |H|)$$ overhead in general.\n\n## Introduction\n\nActive learning is motivated by settings where unlabeled data is cheap but labeling it is expensive. By carefully choosing which points to label, one can often achieve significant reductions in label complexity [Cohn et al., 1994]. A canonical example with exponential improvement is one-dimensional threshold functions $$h_{\\tau}(x) := 1_{x \\geq \\tau}$$: in the noiseless setting, an active learner can use binary search to find an $$\\epsilon$$-approximation solution in $$O(\\log \\frac{1}{\\epsilon})$$ queries, while a passive learner needs $$\\Theta(\\frac{1}{\\epsilon})$$ samples [Cohn et al., 1994, Dasgupta, 2005, Nowak, 2011].\n\nIn this paper we are concerned with agnostic binary classification. We are given a hypothesis class H of binary hypotheses $$h : X \\rightarrow \\{0, 1\\}$$ such that some $$h^* \\in H$$ has $$err(h^*) \\leq \\eta$$, where the error $$err(h) := \\mathbb{E}_{(x,y) \\sim D}[h(x) \\neq y]$$ is measured with respect to an unknown distribution D over $$X \\times \\{0, 1\\}$$. In our active setting, we also know the marginal distribution $$D_X$$ of x, and can query any point x of our choosing to receive a sample $$y \\sim (Y | X = x)$$ for $$(X, Y) \\sim D$$. The goal is to output some $$h$$ with $$err(h) \\leq \\eta + \\epsilon$$, using as few queries as possible.\n\nThe first interesting results for agnostic active learning were shown by Balcan et al. [2006], who gave an algorithm called Agnostic Active (A2) that gets logarithmic dependence on $$\\epsilon$$ in some natural settings: it needs $$O(\\log \\frac{1}{\\epsilon})$$ samples for the 1d linear threshold setting (binary search), as long as $$\\epsilon > 16\\eta$$, and $$O(d^2 \\log \\frac{1}{\\epsilon})$$ samples for d-dimensional linear thresholds when $$D_X$$ is the uniform.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "A Competitive Algorithm for Agnostic Active Learning", "md": "# A Competitive Algorithm for Agnostic Active Learning"}, {"type": "heading", "lvl": 1, "value": "A Competitive Algorithm for Agnostic Active Learning", "md": "# A Competitive Algorithm for Agnostic Active Learning"}, {"type": "text", "value": "Eric Price - Department of Computer Science, University of Texas at Austin\n\nYihan Zhou - Department of Computer Science, University of Texas at Austin\n\narXiv:2310.18786v2 [cs.LG] 16 Dec 2023\n\nEmail: ecprice@cs.utexas.edu, joeyzhou@cs.utexas.edu", "md": "Eric Price - Department of Computer Science, University of Texas at Austin\n\nYihan Zhou - Department of Computer Science, University of Texas at Austin\n\narXiv:2310.18786v2 [cs.LG] 16 Dec 2023\n\nEmail: ecprice@cs.utexas.edu, joeyzhou@cs.utexas.edu"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "For some hypothesis classes and input distributions, active agnostic learning needs exponentially fewer samples than passive learning; for other classes and distributions, it offers little to no improvement. The most popular algorithms for agnostic active learning express their performance in terms of a parameter called the disagreement coefficient, but it is known that these algorithms are inefficient on some inputs.\n\nWe take a different approach to agnostic active learning, getting an algorithm that is competitive with the optimal algorithm for any binary hypothesis class H and distribution DX over X. In particular, if any algorithm can use $$m^*$$ queries to get $$O(\\eta)$$ error, then our algorithm uses $$O(m^* \\log |H|)$$ queries to get $$O(\\eta)$$ error.\n\nOur algorithm lies in the vein of the splitting-based approach of Dasgupta [2004], which gets a similar result for the realizable ($$\\eta = 0$$) setting.\n\nWe also show that it is NP-hard to do better than our algorithm\u2019s $$O(\\log |H|)$$ overhead in general.", "md": "For some hypothesis classes and input distributions, active agnostic learning needs exponentially fewer samples than passive learning; for other classes and distributions, it offers little to no improvement. The most popular algorithms for agnostic active learning express their performance in terms of a parameter called the disagreement coefficient, but it is known that these algorithms are inefficient on some inputs.\n\nWe take a different approach to agnostic active learning, getting an algorithm that is competitive with the optimal algorithm for any binary hypothesis class H and distribution DX over X. In particular, if any algorithm can use $$m^*$$ queries to get $$O(\\eta)$$ error, then our algorithm uses $$O(m^* \\log |H|)$$ queries to get $$O(\\eta)$$ error.\n\nOur algorithm lies in the vein of the splitting-based approach of Dasgupta [2004], which gets a similar result for the realizable ($$\\eta = 0$$) setting.\n\nWe also show that it is NP-hard to do better than our algorithm\u2019s $$O(\\log |H|)$$ overhead in general."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Active learning is motivated by settings where unlabeled data is cheap but labeling it is expensive. By carefully choosing which points to label, one can often achieve significant reductions in label complexity [Cohn et al., 1994]. A canonical example with exponential improvement is one-dimensional threshold functions $$h_{\\tau}(x) := 1_{x \\geq \\tau}$$: in the noiseless setting, an active learner can use binary search to find an $$\\epsilon$$-approximation solution in $$O(\\log \\frac{1}{\\epsilon})$$ queries, while a passive learner needs $$\\Theta(\\frac{1}{\\epsilon})$$ samples [Cohn et al., 1994, Dasgupta, 2005, Nowak, 2011].\n\nIn this paper we are concerned with agnostic binary classification. We are given a hypothesis class H of binary hypotheses $$h : X \\rightarrow \\{0, 1\\}$$ such that some $$h^* \\in H$$ has $$err(h^*) \\leq \\eta$$, where the error $$err(h) := \\mathbb{E}_{(x,y) \\sim D}[h(x) \\neq y]$$ is measured with respect to an unknown distribution D over $$X \\times \\{0, 1\\}$$. In our active setting, we also know the marginal distribution $$D_X$$ of x, and can query any point x of our choosing to receive a sample $$y \\sim (Y | X = x)$$ for $$(X, Y) \\sim D$$. The goal is to output some $$h$$ with $$err(h) \\leq \\eta + \\epsilon$$, using as few queries as possible.\n\nThe first interesting results for agnostic active learning were shown by Balcan et al. [2006], who gave an algorithm called Agnostic Active (A2) that gets logarithmic dependence on $$\\epsilon$$ in some natural settings: it needs $$O(\\log \\frac{1}{\\epsilon})$$ samples for the 1d linear threshold setting (binary search), as long as $$\\epsilon > 16\\eta$$, and $$O(d^2 \\log \\frac{1}{\\epsilon})$$ samples for d-dimensional linear thresholds when $$D_X$$ is the uniform.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Active learning is motivated by settings where unlabeled data is cheap but labeling it is expensive. By carefully choosing which points to label, one can often achieve significant reductions in label complexity [Cohn et al., 1994]. A canonical example with exponential improvement is one-dimensional threshold functions $$h_{\\tau}(x) := 1_{x \\geq \\tau}$$: in the noiseless setting, an active learner can use binary search to find an $$\\epsilon$$-approximation solution in $$O(\\log \\frac{1}{\\epsilon})$$ queries, while a passive learner needs $$\\Theta(\\frac{1}{\\epsilon})$$ samples [Cohn et al., 1994, Dasgupta, 2005, Nowak, 2011].\n\nIn this paper we are concerned with agnostic binary classification. We are given a hypothesis class H of binary hypotheses $$h : X \\rightarrow \\{0, 1\\}$$ such that some $$h^* \\in H$$ has $$err(h^*) \\leq \\eta$$, where the error $$err(h) := \\mathbb{E}_{(x,y) \\sim D}[h(x) \\neq y]$$ is measured with respect to an unknown distribution D over $$X \\times \\{0, 1\\}$$. In our active setting, we also know the marginal distribution $$D_X$$ of x, and can query any point x of our choosing to receive a sample $$y \\sim (Y | X = x)$$ for $$(X, Y) \\sim D$$. The goal is to output some $$h$$ with $$err(h) \\leq \\eta + \\epsilon$$, using as few queries as possible.\n\nThe first interesting results for agnostic active learning were shown by Balcan et al. [2006], who gave an algorithm called Agnostic Active (A2) that gets logarithmic dependence on $$\\epsilon$$ in some natural settings: it needs $$O(\\log \\frac{1}{\\epsilon})$$ samples for the 1d linear threshold setting (binary search), as long as $$\\epsilon > 16\\eta$$, and $$O(d^2 \\log \\frac{1}{\\epsilon})$$ samples for d-dimensional linear thresholds when $$D_X$$ is the uniform.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "sphere and \u03b5 >       \u221a  d\u03b7. This stands in contrast to the polynomial dependence on \u03b5 necessary in\nthe passive setting. The bound\u2019s requirement that \u03b5 \u2273                  \u03b7 is quite natural given a lower bound\nof \u2126    d \u03b72    due to [K\u00e4\u00e4ri\u00e4inen, 2006, Beygelzimer et al., 2009], where d is the VC dimension.\n          \u03b52\nSubsequent works have given new algorithms [Dasgupta et al., 2007, Beygelzimer et al., 2010] and\nnew analyses [Hanneke, 2007a] to get bounds for more general problems, parameterized by the\n\u201cdisagreement coefficient\u201d of the problem. But while these can give better bounds in specific cases,\nthey do not give a good competitive ratio to the optimum algorithm: see (Hanneke [2014], Section\n8.2.5) for a realizable example where O            log 1   queries are possible, but disagreement-coefficient\nbased bounds lead to \u2126       1\u03b5   queries.             \u03b5\nBy contrast, in the realizable, identifiable setting (\u03b7 = \u03b5 = 0), a simple greedy algorithm is com-\npetitive with the optimal algorithm. In particular, Dasgupta [2004] shows that if any algorithm can\nidentify the true hypothesis in m queries, then the greedy algorithm that repeatedly queries the point\nthat splits the most hypotheses will identify the true hypothesis in O(m log |H|) queries. This ex-\ntra factor of log |H| is computationally necessary: as we will show in Theorem 1.2, avoiding it is\nNP-hard in general. This approach can extend [Dasgupta, 2005] to the PAC setting (so \u03b5 > 0, but\nstill \u03b7 = 0), showing that if any algorithm gets error \u03b5 in m\u2217            queries, then this algorithm gets error\n8\u03b5 in roughly    O(m\u2217    \u00b7 log |H|) queries (but see the discussion after Theorem 8.2 of Hanneke [2014],\nwhich points out that one of the logarithmic factors is in an uncontrolled parameter \u03c4, and states that\n\u201cResolving the issue of this extra factor of log 1     \u03c4 remains an important open problem in the theory of\nactive learning.\u201d).\nThe natural question is: can we find an agnostic active learning algorithm that is competitive with\nthe optimal one in the agnostic setting?\nOur Results.       Our main result is just such a competitive bound. We say an active agnostic learning\nalgorithm A solves an instance (H, DX, \u03b7, \u03b5, \u03b4) with m measurements if, for every distribution D\nwith marginal DX and for which some h\u2217              \u2208  H has err(h\u2217) \u2264      \u03b7, with probability 1 \u2212      \u03b4, A uses at\nmost m queries and outputs         h \u2208  H with err      h    \u2264  \u03b7 + \u03b5. Let m\u2217(H, DX, \u03b7, \u03b5, \u03b4) be the optimal\nnumber of queries for this problem, i.e., the smallest m for which any A can solve (H, DX, \u03b7, \u03b5, \u03b4).\nDefine N(H, DX, \u03b1) to be the size of the smallest \u03b1-cover over H, i.e., the smallest set S \u2286                        H\nsuch that for every h \u2208      H there exists h\u2032 \u2208     S with Prx\u223cDX[h(x) \u0338= h\u2032(x)] \u2264           \u03b1. When the context\nis clear, we drop the parameters and simply use N. Of course, N is at most |H|.\nTheorem 1.1 (Competitive Bound). There exist some constants c1, c2 and c3 such that for any\ninstance (H, DX, \u03b7, \u03b5, \u03b4) with \u03b5 \u2265        c1\u03b7, Algorithm 1 solves the instance with sample complexity\n          m(H, DX, \u03b7, \u03b5, \u03b4) \u2272         m\u2217     H, DX, c2\u03b7, c3\u03b5, 99  100     + log 1\u03b4    \u00b7 log N(H, DX,\u03b4\u03b7)\nand polynomial time.\nEven the case of \u03b7 = 0 is interesting, given the discussion in [Hanneke, 2014] of the gap\nin [Dasgupta, 2005]\u2019s bound, but the main contribution is the ability to handle the agnostic setting\nof \u03b7 > 0. The requirement that \u03b5 \u2265          O(\u03b7) is in line with prior work [Balcan et al., 2006, Dasgupta,\n2005]. Up to constants in \u03b7 and \u03b5, Theorem 1.1 shows that our algorithm is within a log N \u2264                   log |H|\nfactor of the optimal query complexity.\nWe show that it NP-hard to avoid this log N factor, even in the realizable (\u03b7 = \u03b5 = \u03b4 = 0) case:\nTheorem 1.2 (Lower Bound). It is NP-hard to find a query strategy for every agnostic active learn-\ning instance within an c log |H| for some constant c > 0 factor of the optimal sample complexity.\nThis    is    a   relatively    simple      reduction     from     the    hardness      of   approximating        SET-\nCOVER [Dinur and Steurer, 2014]. The lower bound instance has \u03b7 = \u03b5 = \u03b4 = 0, although\n                                                                              1                1\nthese can be relaxed to being small polynomials (e.g., \u03b5 = \u03b7 =              3|X| and \u03b4 =     3|H|).\nExtension.      We give an improved bound for our algorithm in the case of noisy binary search (i.e.,\nH consists of 1d threshold functions). When \u03b7 = \u0398(\u03b5), N(H, DX, \u03b5) = \u0398( 1                    \u03b5) and m\u2217(\u03b7, \u03b5, .99) =\nO(log 1 \u03b5). Thus Theorem 1.1 immediately gives a bound of O(log2 1                \u03b5\u03b4), which is nontrivial but not\n                                                           2", "md": "# Math Equations and Text\n\nsphere and $$\\varepsilon > \\sqrt{d}\\eta$$. This stands in contrast to the polynomial dependence on $$\\varepsilon$$ necessary in\nthe passive setting. The bound\u2019s requirement that $$\\varepsilon \\gtrsim \\eta$$ is quite natural given a lower bound\nof $$\\Omega(d\\eta^2)$$ due to [K\u00e4\u00e4ri\u00e4inen, 2006, Beygelzimer et al., 2009], where d is the VC dimension.\n\nSubsequent works have given new algorithms [Dasgupta et al., 2007, Beygelzimer et al., 2010] and\nnew analyses [Hanneke, 2007a] to get bounds for more general problems, parameterized by the\n\u201cdisagreement coefficient\u201d of the problem. But while these can give better bounds in specific cases,\nthey do not give a good competitive ratio to the optimum algorithm: see (Hanneke [2014], Section\n8.2.5) for a realizable example where $$O(\\log 1)$$ queries are possible, but disagreement-coefficient\nbased bounds lead to $$\\Omega(\\frac{1}{\\varepsilon})$$ queries.\n\nBy contrast, in the realizable, identifiable setting ($$\\eta = \\varepsilon = 0$$), a simple greedy algorithm is competitive with the optimal algorithm. In particular, Dasgupta [2004] shows that if any algorithm can identify the true hypothesis in m queries, then the greedy algorithm that repeatedly queries the point that splits the most hypotheses will identify the true hypothesis in $$O(m \\log |H|)$$ queries. This extra factor of $$\\log |H|$$ is computationally necessary: as we will show in Theorem 1.2, avoiding it is NP-hard in general. This approach can extend [Dasgupta, 2005] to the PAC setting (so $$\\varepsilon > 0$$, but still $$\\eta = 0$$), showing that if any algorithm gets error $$\\varepsilon$$ in $$m^*$$ queries, then this algorithm gets error $$8\\varepsilon$$ in roughly $$O(m^* \\cdot \\log |H|)$$ queries (but see the discussion after Theorem 8.2 of Hanneke [2014], which points out that one of the logarithmic factors is in an uncontrolled parameter $$\\tau$$, and states that \u201cResolving the issue of this extra factor of $$\\log 1$$ remains an important open problem in the theory of active learning.\u201d).\n\nThe natural question is: can we find an agnostic active learning algorithm that is competitive with\nthe optimal one in the agnostic setting?\n\n## Our Results\n\nOur main result is just such a competitive bound. We say an active agnostic learning\nalgorithm A solves an instance (H, DX, $$\\eta, \\varepsilon, \\delta$$) with m measurements if, for every distribution D\nwith marginal DX and for which some $$h^* \\in H$$ has $$err(h^*) \\leq \\eta$$, with probability $$1 - \\delta$$, A uses at\nmost m queries and outputs $$h \\in H$$ with $$err(h) \\leq \\eta + \\varepsilon$$. Let $$m^*(H, DX, \\eta, \\varepsilon, \\delta)$$ be the optimal\nnumber of queries for this problem, i.e., the smallest m for which any A can solve (H, DX, $$\\eta, \\varepsilon, \\delta$$).\n\nDefine $$N(H, DX, \\alpha)$$ to be the size of the smallest $$\\alpha$$-cover over H, i.e., the smallest set S $$\\subseteq H$$\nsuch that for every $$h \\in H$$ there exists $$h' \\in S$$ with $$Pr_{x \\sim DX}[h(x) \\neq h'(x)] \\leq \\alpha$$. When the context\nis clear, we drop the parameters and simply use N. Of course, N is at most $$|H|$$.\n\n### Theorem 1.1 (Competitive Bound)\n\nThere exist some constants c1, c2 and c3 such that for any\ninstance (H, DX, $$\\eta, \\varepsilon, \\delta$$) with $$\\varepsilon \\geq c1\\eta$$, Algorithm 1 solves the instance with sample complexity\n$$m(H, DX, \\eta, \\varepsilon, \\delta) \\lesssim m^*(H, DX, c2\\eta, c3\\varepsilon, \\frac{99}{100} + \\log \\frac{1}{\\delta}) \\cdot \\log N(H, DX, \\delta\\eta)$$\nand polynomial time.\n\nEven the case of $$\\eta = 0$$ is interesting, given the discussion in [Hanneke, 2014] of the gap\nin [Dasgupta, 2005]\u2019s bound, but the main contribution is the ability to handle the agnostic setting\nof $$\\eta > 0$$. The requirement that $$\\varepsilon \\geq O(\\eta)$$ is in line with prior work [Balcan et al., 2006, Dasgupta,\n2005]. Up to constants in $$\\eta$$ and $$\\varepsilon$$, Theorem 1.1 shows that our algorithm is within a $$\\log N \\leq \\log |H|$$\nfactor of the optimal query complexity.\n\nWe show that it NP-hard to avoid this $$\\log N$$ factor, even in the realizable ($$\\eta = \\varepsilon = \\delta = 0$$) case:\n\n### Theorem 1.2 (Lower Bound)\n\nIt is NP-hard to find a query strategy for every agnostic active learn-\ning instance within a c $$\\log |H|$$ for some constant c > 0 factor of the optimal sample complexity.\n\nThis is a relatively simple reduction from the hardness of approximating SET-\nCOVER [Dinur and Steurer, 2014]. The lower bound instance has $$\\eta = \\varepsilon = \\delta = 0$$, although\nthese can be relaxed to being small polynomials (e.g., $$\\varepsilon = \\eta = \\frac{1}{3|X|}$$ and $$\\delta = \\frac{1}{3|H|}$$).\n\n## Extension\n\nWe give an improved bound for our algorithm in the case of noisy binary search (i.e.,\nH consists of 1d threshold functions). When $$\\eta = \\Theta(\\varepsilon)$$, $$N(H, DX, \\varepsilon) = \\Theta(\\frac{1}{\\varepsilon})$$ and $$m^*(\\eta, \\varepsilon, .99) = O(\\log \\frac{1}{\\varepsilon})$$. Thus Theorem 1.1 immediately gives a bound of $$O(\\log^2 \\frac{1}{\\varepsilon\\delta})$$, which is nontrivial but not", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "sphere and $$\\varepsilon > \\sqrt{d}\\eta$$. This stands in contrast to the polynomial dependence on $$\\varepsilon$$ necessary in\nthe passive setting. The bound\u2019s requirement that $$\\varepsilon \\gtrsim \\eta$$ is quite natural given a lower bound\nof $$\\Omega(d\\eta^2)$$ due to [K\u00e4\u00e4ri\u00e4inen, 2006, Beygelzimer et al., 2009], where d is the VC dimension.\n\nSubsequent works have given new algorithms [Dasgupta et al., 2007, Beygelzimer et al., 2010] and\nnew analyses [Hanneke, 2007a] to get bounds for more general problems, parameterized by the\n\u201cdisagreement coefficient\u201d of the problem. But while these can give better bounds in specific cases,\nthey do not give a good competitive ratio to the optimum algorithm: see (Hanneke [2014], Section\n8.2.5) for a realizable example where $$O(\\log 1)$$ queries are possible, but disagreement-coefficient\nbased bounds lead to $$\\Omega(\\frac{1}{\\varepsilon})$$ queries.\n\nBy contrast, in the realizable, identifiable setting ($$\\eta = \\varepsilon = 0$$), a simple greedy algorithm is competitive with the optimal algorithm. In particular, Dasgupta [2004] shows that if any algorithm can identify the true hypothesis in m queries, then the greedy algorithm that repeatedly queries the point that splits the most hypotheses will identify the true hypothesis in $$O(m \\log |H|)$$ queries. This extra factor of $$\\log |H|$$ is computationally necessary: as we will show in Theorem 1.2, avoiding it is NP-hard in general. This approach can extend [Dasgupta, 2005] to the PAC setting (so $$\\varepsilon > 0$$, but still $$\\eta = 0$$), showing that if any algorithm gets error $$\\varepsilon$$ in $$m^*$$ queries, then this algorithm gets error $$8\\varepsilon$$ in roughly $$O(m^* \\cdot \\log |H|)$$ queries (but see the discussion after Theorem 8.2 of Hanneke [2014], which points out that one of the logarithmic factors is in an uncontrolled parameter $$\\tau$$, and states that \u201cResolving the issue of this extra factor of $$\\log 1$$ remains an important open problem in the theory of active learning.\u201d).\n\nThe natural question is: can we find an agnostic active learning algorithm that is competitive with\nthe optimal one in the agnostic setting?", "md": "sphere and $$\\varepsilon > \\sqrt{d}\\eta$$. This stands in contrast to the polynomial dependence on $$\\varepsilon$$ necessary in\nthe passive setting. The bound\u2019s requirement that $$\\varepsilon \\gtrsim \\eta$$ is quite natural given a lower bound\nof $$\\Omega(d\\eta^2)$$ due to [K\u00e4\u00e4ri\u00e4inen, 2006, Beygelzimer et al., 2009], where d is the VC dimension.\n\nSubsequent works have given new algorithms [Dasgupta et al., 2007, Beygelzimer et al., 2010] and\nnew analyses [Hanneke, 2007a] to get bounds for more general problems, parameterized by the\n\u201cdisagreement coefficient\u201d of the problem. But while these can give better bounds in specific cases,\nthey do not give a good competitive ratio to the optimum algorithm: see (Hanneke [2014], Section\n8.2.5) for a realizable example where $$O(\\log 1)$$ queries are possible, but disagreement-coefficient\nbased bounds lead to $$\\Omega(\\frac{1}{\\varepsilon})$$ queries.\n\nBy contrast, in the realizable, identifiable setting ($$\\eta = \\varepsilon = 0$$), a simple greedy algorithm is competitive with the optimal algorithm. In particular, Dasgupta [2004] shows that if any algorithm can identify the true hypothesis in m queries, then the greedy algorithm that repeatedly queries the point that splits the most hypotheses will identify the true hypothesis in $$O(m \\log |H|)$$ queries. This extra factor of $$\\log |H|$$ is computationally necessary: as we will show in Theorem 1.2, avoiding it is NP-hard in general. This approach can extend [Dasgupta, 2005] to the PAC setting (so $$\\varepsilon > 0$$, but still $$\\eta = 0$$), showing that if any algorithm gets error $$\\varepsilon$$ in $$m^*$$ queries, then this algorithm gets error $$8\\varepsilon$$ in roughly $$O(m^* \\cdot \\log |H|)$$ queries (but see the discussion after Theorem 8.2 of Hanneke [2014], which points out that one of the logarithmic factors is in an uncontrolled parameter $$\\tau$$, and states that \u201cResolving the issue of this extra factor of $$\\log 1$$ remains an important open problem in the theory of active learning.\u201d).\n\nThe natural question is: can we find an agnostic active learning algorithm that is competitive with\nthe optimal one in the agnostic setting?"}, {"type": "heading", "lvl": 2, "value": "Our Results", "md": "## Our Results"}, {"type": "text", "value": "Our main result is just such a competitive bound. We say an active agnostic learning\nalgorithm A solves an instance (H, DX, $$\\eta, \\varepsilon, \\delta$$) with m measurements if, for every distribution D\nwith marginal DX and for which some $$h^* \\in H$$ has $$err(h^*) \\leq \\eta$$, with probability $$1 - \\delta$$, A uses at\nmost m queries and outputs $$h \\in H$$ with $$err(h) \\leq \\eta + \\varepsilon$$. Let $$m^*(H, DX, \\eta, \\varepsilon, \\delta)$$ be the optimal\nnumber of queries for this problem, i.e., the smallest m for which any A can solve (H, DX, $$\\eta, \\varepsilon, \\delta$$).\n\nDefine $$N(H, DX, \\alpha)$$ to be the size of the smallest $$\\alpha$$-cover over H, i.e., the smallest set S $$\\subseteq H$$\nsuch that for every $$h \\in H$$ there exists $$h' \\in S$$ with $$Pr_{x \\sim DX}[h(x) \\neq h'(x)] \\leq \\alpha$$. When the context\nis clear, we drop the parameters and simply use N. Of course, N is at most $$|H|$$.", "md": "Our main result is just such a competitive bound. We say an active agnostic learning\nalgorithm A solves an instance (H, DX, $$\\eta, \\varepsilon, \\delta$$) with m measurements if, for every distribution D\nwith marginal DX and for which some $$h^* \\in H$$ has $$err(h^*) \\leq \\eta$$, with probability $$1 - \\delta$$, A uses at\nmost m queries and outputs $$h \\in H$$ with $$err(h) \\leq \\eta + \\varepsilon$$. Let $$m^*(H, DX, \\eta, \\varepsilon, \\delta)$$ be the optimal\nnumber of queries for this problem, i.e., the smallest m for which any A can solve (H, DX, $$\\eta, \\varepsilon, \\delta$$).\n\nDefine $$N(H, DX, \\alpha)$$ to be the size of the smallest $$\\alpha$$-cover over H, i.e., the smallest set S $$\\subseteq H$$\nsuch that for every $$h \\in H$$ there exists $$h' \\in S$$ with $$Pr_{x \\sim DX}[h(x) \\neq h'(x)] \\leq \\alpha$$. When the context\nis clear, we drop the parameters and simply use N. Of course, N is at most $$|H|$$."}, {"type": "heading", "lvl": 3, "value": "Theorem 1.1 (Competitive Bound)", "md": "### Theorem 1.1 (Competitive Bound)"}, {"type": "text", "value": "There exist some constants c1, c2 and c3 such that for any\ninstance (H, DX, $$\\eta, \\varepsilon, \\delta$$) with $$\\varepsilon \\geq c1\\eta$$, Algorithm 1 solves the instance with sample complexity\n$$m(H, DX, \\eta, \\varepsilon, \\delta) \\lesssim m^*(H, DX, c2\\eta, c3\\varepsilon, \\frac{99}{100} + \\log \\frac{1}{\\delta}) \\cdot \\log N(H, DX, \\delta\\eta)$$\nand polynomial time.\n\nEven the case of $$\\eta = 0$$ is interesting, given the discussion in [Hanneke, 2014] of the gap\nin [Dasgupta, 2005]\u2019s bound, but the main contribution is the ability to handle the agnostic setting\nof $$\\eta > 0$$. The requirement that $$\\varepsilon \\geq O(\\eta)$$ is in line with prior work [Balcan et al., 2006, Dasgupta,\n2005]. Up to constants in $$\\eta$$ and $$\\varepsilon$$, Theorem 1.1 shows that our algorithm is within a $$\\log N \\leq \\log |H|$$\nfactor of the optimal query complexity.\n\nWe show that it NP-hard to avoid this $$\\log N$$ factor, even in the realizable ($$\\eta = \\varepsilon = \\delta = 0$$) case:", "md": "There exist some constants c1, c2 and c3 such that for any\ninstance (H, DX, $$\\eta, \\varepsilon, \\delta$$) with $$\\varepsilon \\geq c1\\eta$$, Algorithm 1 solves the instance with sample complexity\n$$m(H, DX, \\eta, \\varepsilon, \\delta) \\lesssim m^*(H, DX, c2\\eta, c3\\varepsilon, \\frac{99}{100} + \\log \\frac{1}{\\delta}) \\cdot \\log N(H, DX, \\delta\\eta)$$\nand polynomial time.\n\nEven the case of $$\\eta = 0$$ is interesting, given the discussion in [Hanneke, 2014] of the gap\nin [Dasgupta, 2005]\u2019s bound, but the main contribution is the ability to handle the agnostic setting\nof $$\\eta > 0$$. The requirement that $$\\varepsilon \\geq O(\\eta)$$ is in line with prior work [Balcan et al., 2006, Dasgupta,\n2005]. Up to constants in $$\\eta$$ and $$\\varepsilon$$, Theorem 1.1 shows that our algorithm is within a $$\\log N \\leq \\log |H|$$\nfactor of the optimal query complexity.\n\nWe show that it NP-hard to avoid this $$\\log N$$ factor, even in the realizable ($$\\eta = \\varepsilon = \\delta = 0$$) case:"}, {"type": "heading", "lvl": 3, "value": "Theorem 1.2 (Lower Bound)", "md": "### Theorem 1.2 (Lower Bound)"}, {"type": "text", "value": "It is NP-hard to find a query strategy for every agnostic active learn-\ning instance within a c $$\\log |H|$$ for some constant c > 0 factor of the optimal sample complexity.\n\nThis is a relatively simple reduction from the hardness of approximating SET-\nCOVER [Dinur and Steurer, 2014]. The lower bound instance has $$\\eta = \\varepsilon = \\delta = 0$$, although\nthese can be relaxed to being small polynomials (e.g., $$\\varepsilon = \\eta = \\frac{1}{3|X|}$$ and $$\\delta = \\frac{1}{3|H|}$$).", "md": "It is NP-hard to find a query strategy for every agnostic active learn-\ning instance within a c $$\\log |H|$$ for some constant c > 0 factor of the optimal sample complexity.\n\nThis is a relatively simple reduction from the hardness of approximating SET-\nCOVER [Dinur and Steurer, 2014]. The lower bound instance has $$\\eta = \\varepsilon = \\delta = 0$$, although\nthese can be relaxed to being small polynomials (e.g., $$\\varepsilon = \\eta = \\frac{1}{3|X|}$$ and $$\\delta = \\frac{1}{3|H|}$$)."}, {"type": "heading", "lvl": 2, "value": "Extension", "md": "## Extension"}, {"type": "text", "value": "We give an improved bound for our algorithm in the case of noisy binary search (i.e.,\nH consists of 1d threshold functions). When $$\\eta = \\Theta(\\varepsilon)$$, $$N(H, DX, \\varepsilon) = \\Theta(\\frac{1}{\\varepsilon})$$ and $$m^*(\\eta, \\varepsilon, .99) = O(\\log \\frac{1}{\\varepsilon})$$. Thus Theorem 1.1 immediately gives a bound of $$O(\\log^2 \\frac{1}{\\varepsilon\\delta})$$, which is nontrivial but not", "md": "We give an improved bound for our algorithm in the case of noisy binary search (i.e.,\nH consists of 1d threshold functions). When $$\\eta = \\Theta(\\varepsilon)$$, $$N(H, DX, \\varepsilon) = \\Theta(\\frac{1}{\\varepsilon})$$ and $$m^*(\\eta, \\varepsilon, .99) = O(\\log \\frac{1}{\\varepsilon})$$. Thus Theorem 1.1 immediately gives a bound of $$O(\\log^2 \\frac{1}{\\varepsilon\\delta})$$, which is nontrivial but not"}]}, {"page": 3, "text": "ideal. (For \u03b7 \u226a      \u03b5, the same bound holds since the problem is strictly easier when \u03b7 is smaller.)\nHowever, the bound in Theorem 1.1 is quite loose in this setting, and we can instead give a bound of\n                                            O   log 1\u03b5\u03b4 log log\u03b41\u03b5\nfor the same algorithm, Algorithm 1. This matches the bound given by disagreement coefficient\nbased algorithms for constant \u03b4. The proof of this improved dependence comes from bounding a\nnew parameter measuring the complexity of an H, Dx pair; this parameter is always at least \u2126( 1             m\u2217)\nbut may be much larger (and is constant for 1d threshold functions). See Theorem 2.3 for details.\n1.1    Related Work\nActive learning is a widely studied topic, taking many forms beyond the directly related work on\nagnostic active learning discussed above [Settles, 2009]. Our algorithm can be viewed as similar\nto \u201cuncertainty sampling\u201d [Lewis, 1995, Lewis and Catlett, 1994], a popular empirical approach to\nactive learning, though we need some modifications to tolerate adversarial noise.\nOne problem related to the one studied in this paper is noisy binary search, which corresponds\nto active learning of 1d thresholds.         This has been extensively studied in the setting of i.i.d.\nnoise [Burnashev and Zigangirov, 1974, Ben-Or and Hassidim, 2008, Dereniowski et al., 2021] as\nwell as monotonic queries [Karp and Kleinberg, 2007]. Some work in this vein has extended beyond\nbinary search to (essentially) active binary classification [Nowak, 2008, 2011]. These algorithms are\nall fairly similar to ours, in that they do multiplicative weights/Bayesian updates, but they query the\nsingle maximally informative point. This is fine in the i.i.d. noise setting, but in an agnostic set-\nting the adversary can corrupt that query. For this reason, our algorithm needs to find a set of\nhigh-information points to query.\nAnother related problems is decision tree learning. The realizable, noiseless case \u03b7 = \u03b5 = 0 of\nour problem can be reduced to learning a binary decision tree with minimal depth. Heged\u02dd              us [1995]\nstudied this problem and gave basically the same upper and lower bound as in Dasgupta [2005].\nKosaraju et al. [2002] studied a split tree problem, which is a generalization of binary decision tree\nlearning, and also gave similar bounds. Azad et al. [2022] is a monograph focusing on decision tree\nlearning, in which many variations are studied, including learning with noise. However, this line\nof work usually allows different forms of queries so their results are not directly comparable from\nresults in the active learning literature.\nFor much more work on the agnostic active binary classification problem, see Hanneke [2014] and\nreferences therein. Many of these papers give bounds in terms of the disagreement coefficient, but\nsometimes in terms of other parameters. For example, Katz-Samuels et al. [2021] has a query bound\nthat is always competitive with the disagreement coefficient-based methods, and sometimes much\nbetter; still, it is not competitive with the optimum in all cases.\nIn terms of the lower bound, it is shown in Laurent and Rivest [1976] that the problem is NP-\ncomplete, in the realizable and noiseless setting. To the best of our knowledge, our Theorem 1.2\nshowing hardness of approximation to within a O(log |H|) factor is new.\nMinimax sample complexity bounds.              Hanneke and Yang [2015] and Hanneke [2007b] have also\ngiven \u201cminimax\u201d sample complexity bounds for their algorithms, also getting a sample complexity\nwithin O(log |H|) of optimal. However, these results are optimal with respect to the sample com-\nplexity for the worst-case distribution over y and x. But the unlabeled data x is given as input. So\none should hope for a bound with respect to optimal for the actual x and only worst-case over y;\nthis is our bound.\nWe give the following example to illustrate that our bound, and indeed our algorithm, can be much\nbetter.\nExample 1.3. Define a hypothesis class of N hypotheses h1, \u00b7 \u00b7 \u00b7 , hN , and log N + N data points\nx1, \u00b7 \u00b7 \u00b7 , xlog N+N. For each hypothesis hj, the labels of the first N points express j in unary and\nthe labels of the last log N points express j in binary. We set \u03b7 = \u03b5 = 0 and consider the realizable\ncase.\nIn the above example, the binary region is far more informative than the unary region, but disagree-\nment coefficient-based algorithms just note that every point has disagreement. Our algorithm will\n                                                        3", "md": "ideal. (For $$\\eta \\ll \\epsilon$$, the same bound holds since the problem is strictly easier when $$\\eta$$ is smaller.)\n\nHowever, the bound in Theorem 1.1 is quite loose in this setting, and we can instead give a bound of\n\n$$\nO\\left(\\frac{\\log \\frac{1}{\\epsilon\\delta} \\log \\log \\frac{1}{\\delta\\epsilon}}{\\delta}\\right)\n$$\n\nfor the same algorithm, Algorithm 1. This matches the bound given by disagreement coefficient based algorithms for constant $$\\delta$$. The proof of this improved dependence comes from bounding a new parameter measuring the complexity of an $$\\mathcal{H}, D_x$$ pair; this parameter is always at least $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ but may be much larger (and is constant for 1d threshold functions). See Theorem 2.3 for details.\n\n## 1.1 Related Work\n\nActive learning is a widely studied topic, taking many forms beyond the directly related work on agnostic active learning discussed above [Settles, 2009]. Our algorithm can be viewed as similar to \u201cuncertainty sampling\u201d [Lewis, 1995, Lewis and Catlett, 1994], a popular empirical approach to active learning, though we need some modifications to tolerate adversarial noise.\n\nOne problem related to the one studied in this paper is noisy binary search, which corresponds to active learning of 1d thresholds. This has been extensively studied in the setting of i.i.d. noise [Burnashev and Zigangirov, 1974, Ben-Or and Hassidim, 2008, Dereniowski et al., 2021] as well as monotonic queries [Karp and Kleinberg, 2007]. Some work in this vein has extended beyond binary search to (essentially) active binary classification [Nowak, 2008, 2011]. These algorithms are all fairly similar to ours, in that they do multiplicative weights/Bayesian updates, but they query the single maximally informative point. This is fine in the i.i.d. noise setting, but in an agnostic setting the adversary can corrupt that query. For this reason, our algorithm needs to find a set of high-information points to query.\n\nAnother related problem is decision tree learning. The realizable, noiseless case $$\\eta = \\epsilon = 0$$ of our problem can be reduced to learning a binary decision tree with minimal depth. Heged\u02dd us [1995] studied this problem and gave basically the same upper and lower bound as in Dasgupta [2005]. Kosaraju et al. [2002] studied a split tree problem, which is a generalization of binary decision tree learning, and also gave similar bounds. Azad et al. [2022] is a monograph focusing on decision tree learning, in which many variations are studied, including learning with noise. However, this line of work usually allows different forms of queries so their results are not directly comparable from results in the active learning literature.\n\nFor much more work on the agnostic active binary classification problem, see Hanneke [2014] and references therein. Many of these papers give bounds in terms of the disagreement coefficient, but sometimes in terms of other parameters. For example, Katz-Samuels et al. [2021] has a query bound that is always competitive with the disagreement coefficient-based methods, and sometimes much better; still, it is not competitive with the optimum in all cases.\n\nIn terms of the lower bound, it is shown in Laurent and Rivest [1976] that the problem is NP-complete, in the realizable and noiseless setting. To the best of our knowledge, our Theorem 1.2 showing hardness of approximation to within a $$O(\\log |\\mathcal{H}|)$$ factor is new.\n\nMinimax sample complexity bounds. Hanneke and Yang [2015] and Hanneke [2007b] have also given \u201cminimax\u201d sample complexity bounds for their algorithms, also getting a sample complexity within $$O(\\log |\\mathcal{H}|)$$ of optimal. However, these results are optimal with respect to the sample complexity for the worst-case distribution over $$y$$ and $$x$$. But the unlabeled data $$x$$ is given as input. So one should hope for a bound with respect to optimal for the actual $$x$$ and only worst-case over $$y$$; this is our bound.\n\nWe give the following example to illustrate that our bound, and indeed our algorithm, can be much better.\n\n### Example 1.3\n\nDefine a hypothesis class of $$N$$ hypotheses $$h_1, \\ldots, h_N$$, and $$\\log N + N$$ data points $$x_1, \\ldots, x^{\\log N+N}$$. For each hypothesis $$h_j$$, the labels of the first $$N$$ points express $$j$$ in unary and the labels of the last $$\\log N$$ points express $$j$$ in binary. We set $$\\eta = \\epsilon = 0$$ and consider the realizable case.\n\nIn the above example, the binary region is far more informative than the unary region, but disagreement coefficient-based algorithms just note that every point has disagreement. Our algorithm will", "images": [], "items": [{"type": "text", "value": "ideal. (For $$\\eta \\ll \\epsilon$$, the same bound holds since the problem is strictly easier when $$\\eta$$ is smaller.)\n\nHowever, the bound in Theorem 1.1 is quite loose in this setting, and we can instead give a bound of\n\n$$\nO\\left(\\frac{\\log \\frac{1}{\\epsilon\\delta} \\log \\log \\frac{1}{\\delta\\epsilon}}{\\delta}\\right)\n$$\n\nfor the same algorithm, Algorithm 1. This matches the bound given by disagreement coefficient based algorithms for constant $$\\delta$$. The proof of this improved dependence comes from bounding a new parameter measuring the complexity of an $$\\mathcal{H}, D_x$$ pair; this parameter is always at least $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ but may be much larger (and is constant for 1d threshold functions). See Theorem 2.3 for details.", "md": "ideal. (For $$\\eta \\ll \\epsilon$$, the same bound holds since the problem is strictly easier when $$\\eta$$ is smaller.)\n\nHowever, the bound in Theorem 1.1 is quite loose in this setting, and we can instead give a bound of\n\n$$\nO\\left(\\frac{\\log \\frac{1}{\\epsilon\\delta} \\log \\log \\frac{1}{\\delta\\epsilon}}{\\delta}\\right)\n$$\n\nfor the same algorithm, Algorithm 1. This matches the bound given by disagreement coefficient based algorithms for constant $$\\delta$$. The proof of this improved dependence comes from bounding a new parameter measuring the complexity of an $$\\mathcal{H}, D_x$$ pair; this parameter is always at least $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ but may be much larger (and is constant for 1d threshold functions). See Theorem 2.3 for details."}, {"type": "heading", "lvl": 2, "value": "1.1 Related Work", "md": "## 1.1 Related Work"}, {"type": "text", "value": "Active learning is a widely studied topic, taking many forms beyond the directly related work on agnostic active learning discussed above [Settles, 2009]. Our algorithm can be viewed as similar to \u201cuncertainty sampling\u201d [Lewis, 1995, Lewis and Catlett, 1994], a popular empirical approach to active learning, though we need some modifications to tolerate adversarial noise.\n\nOne problem related to the one studied in this paper is noisy binary search, which corresponds to active learning of 1d thresholds. This has been extensively studied in the setting of i.i.d. noise [Burnashev and Zigangirov, 1974, Ben-Or and Hassidim, 2008, Dereniowski et al., 2021] as well as monotonic queries [Karp and Kleinberg, 2007]. Some work in this vein has extended beyond binary search to (essentially) active binary classification [Nowak, 2008, 2011]. These algorithms are all fairly similar to ours, in that they do multiplicative weights/Bayesian updates, but they query the single maximally informative point. This is fine in the i.i.d. noise setting, but in an agnostic setting the adversary can corrupt that query. For this reason, our algorithm needs to find a set of high-information points to query.\n\nAnother related problem is decision tree learning. The realizable, noiseless case $$\\eta = \\epsilon = 0$$ of our problem can be reduced to learning a binary decision tree with minimal depth. Heged\u02dd us [1995] studied this problem and gave basically the same upper and lower bound as in Dasgupta [2005]. Kosaraju et al. [2002] studied a split tree problem, which is a generalization of binary decision tree learning, and also gave similar bounds. Azad et al. [2022] is a monograph focusing on decision tree learning, in which many variations are studied, including learning with noise. However, this line of work usually allows different forms of queries so their results are not directly comparable from results in the active learning literature.\n\nFor much more work on the agnostic active binary classification problem, see Hanneke [2014] and references therein. Many of these papers give bounds in terms of the disagreement coefficient, but sometimes in terms of other parameters. For example, Katz-Samuels et al. [2021] has a query bound that is always competitive with the disagreement coefficient-based methods, and sometimes much better; still, it is not competitive with the optimum in all cases.\n\nIn terms of the lower bound, it is shown in Laurent and Rivest [1976] that the problem is NP-complete, in the realizable and noiseless setting. To the best of our knowledge, our Theorem 1.2 showing hardness of approximation to within a $$O(\\log |\\mathcal{H}|)$$ factor is new.\n\nMinimax sample complexity bounds. Hanneke and Yang [2015] and Hanneke [2007b] have also given \u201cminimax\u201d sample complexity bounds for their algorithms, also getting a sample complexity within $$O(\\log |\\mathcal{H}|)$$ of optimal. However, these results are optimal with respect to the sample complexity for the worst-case distribution over $$y$$ and $$x$$. But the unlabeled data $$x$$ is given as input. So one should hope for a bound with respect to optimal for the actual $$x$$ and only worst-case over $$y$$; this is our bound.\n\nWe give the following example to illustrate that our bound, and indeed our algorithm, can be much better.", "md": "Active learning is a widely studied topic, taking many forms beyond the directly related work on agnostic active learning discussed above [Settles, 2009]. Our algorithm can be viewed as similar to \u201cuncertainty sampling\u201d [Lewis, 1995, Lewis and Catlett, 1994], a popular empirical approach to active learning, though we need some modifications to tolerate adversarial noise.\n\nOne problem related to the one studied in this paper is noisy binary search, which corresponds to active learning of 1d thresholds. This has been extensively studied in the setting of i.i.d. noise [Burnashev and Zigangirov, 1974, Ben-Or and Hassidim, 2008, Dereniowski et al., 2021] as well as monotonic queries [Karp and Kleinberg, 2007]. Some work in this vein has extended beyond binary search to (essentially) active binary classification [Nowak, 2008, 2011]. These algorithms are all fairly similar to ours, in that they do multiplicative weights/Bayesian updates, but they query the single maximally informative point. This is fine in the i.i.d. noise setting, but in an agnostic setting the adversary can corrupt that query. For this reason, our algorithm needs to find a set of high-information points to query.\n\nAnother related problem is decision tree learning. The realizable, noiseless case $$\\eta = \\epsilon = 0$$ of our problem can be reduced to learning a binary decision tree with minimal depth. Heged\u02dd us [1995] studied this problem and gave basically the same upper and lower bound as in Dasgupta [2005]. Kosaraju et al. [2002] studied a split tree problem, which is a generalization of binary decision tree learning, and also gave similar bounds. Azad et al. [2022] is a monograph focusing on decision tree learning, in which many variations are studied, including learning with noise. However, this line of work usually allows different forms of queries so their results are not directly comparable from results in the active learning literature.\n\nFor much more work on the agnostic active binary classification problem, see Hanneke [2014] and references therein. Many of these papers give bounds in terms of the disagreement coefficient, but sometimes in terms of other parameters. For example, Katz-Samuels et al. [2021] has a query bound that is always competitive with the disagreement coefficient-based methods, and sometimes much better; still, it is not competitive with the optimum in all cases.\n\nIn terms of the lower bound, it is shown in Laurent and Rivest [1976] that the problem is NP-complete, in the realizable and noiseless setting. To the best of our knowledge, our Theorem 1.2 showing hardness of approximation to within a $$O(\\log |\\mathcal{H}|)$$ factor is new.\n\nMinimax sample complexity bounds. Hanneke and Yang [2015] and Hanneke [2007b] have also given \u201cminimax\u201d sample complexity bounds for their algorithms, also getting a sample complexity within $$O(\\log |\\mathcal{H}|)$$ of optimal. However, these results are optimal with respect to the sample complexity for the worst-case distribution over $$y$$ and $$x$$. But the unlabeled data $$x$$ is given as input. So one should hope for a bound with respect to optimal for the actual $$x$$ and only worst-case over $$y$$; this is our bound.\n\nWe give the following example to illustrate that our bound, and indeed our algorithm, can be much better."}, {"type": "heading", "lvl": 3, "value": "Example 1.3", "md": "### Example 1.3"}, {"type": "text", "value": "Define a hypothesis class of $$N$$ hypotheses $$h_1, \\ldots, h_N$$, and $$\\log N + N$$ data points $$x_1, \\ldots, x^{\\log N+N}$$. For each hypothesis $$h_j$$, the labels of the first $$N$$ points express $$j$$ in unary and the labels of the last $$\\log N$$ points express $$j$$ in binary. We set $$\\eta = \\epsilon = 0$$ and consider the realizable case.\n\nIn the above example, the binary region is far more informative than the unary region, but disagreement coefficient-based algorithms just note that every point has disagreement. Our algorithm will", "md": "Define a hypothesis class of $$N$$ hypotheses $$h_1, \\ldots, h_N$$, and $$\\log N + N$$ data points $$x_1, \\ldots, x^{\\log N+N}$$. For each hypothesis $$h_j$$, the labels of the first $$N$$ points express $$j$$ in unary and the labels of the last $$\\log N$$ points express $$j$$ in binary. We set $$\\eta = \\epsilon = 0$$ and consider the realizable case.\n\nIn the above example, the binary region is far more informative than the unary region, but disagreement coefficient-based algorithms just note that every point has disagreement. Our algorithm will"}]}, {"page": 4, "text": " query the binary encoding region and take O(log N) queries. Disagreement coefficient based algo-\n rithms, including those in Hanneke and Yang [2015] and Hanneke [2007b], will rely on essentially\n uniform sampling for the first \u2126(N/ log N) queries. These algorithms are \u201cminimax\u201d over x, in the\n sense that if you didn\u2019t see any x from the binary region, you would need almost as many samples\n as they use. But you do see x from the binary region, so the algorithm should make use of it to get\n exponential improvement.\n Future Work.            Our upper bound assumes full knowledge of DX and the ability to query arbitrary\n points x. Often in active learning, the algorithm receives a large but not infinite set of unlabeled\n sample points x, and can only query the labels of those points. How well our results adapt to this\n setting we leave as an open question.\n Similarly, our bound is polynomial in the number of hypotheses and the domain size. This is hard\n to avoid in full generality\u2014if you don\u2019t evaluate most hypotheses on most data points, you might be\n missing the most informative points\u2014but perhaps it can be avoided in structured examples.\n 2     Algorithm Overview\n Our algorithm is based on a Bayesian/multiplicative weights type approach to the problem, and is\n along the lines of the splitting-based approach of Dasgupta [2004].\nWe maintain a set of weights w(h) for each h \u2208                        H, starting at 1; these induce a distribution \u03bb(h) :=\n    w(h)\n  h w(h) which we can think of as our posterior over the \u201ctrue\u201d h\u2217.\n Realizable setting.            As initial intuition, consider the realizable case of \u03b7 = \u03b5 = 0 where we want\n to find the true h\u2217. If h\u2217           really were drawn from our prior \u03bb, and we query a point x, we will see\n a 1 with probability Eh\u223c\u03bb h(x). Then the most informative point to query is the one we are least\n confident in, i.e., the point x\u2217           maximizing\n                                         r(x) := min            E                                      .\n                                                               h\u223c\u03bb[h(x)], 1 \u2212          E\n                                                                                     h\u223c\u03bb[h(x)]\n Suppose an algorithm queries x1, . . . , xm and receives the majority label under h \u223c                                        \u03bb each time.\n Then the fraction of h \u223c            \u03bb that agree with all the queries is at least 1 \u2212                   m  i=1 r(xi) \u2265       1 \u2212   mr(x\u2217).\n                                              1\n This suggests that, if r(x\u2217) \u226a               m, it will be hard to uniquely identify h\u2217. It is not hard to formalize\n this, showing that: if no single hypothesis has 75% probability under \u03bb, and any algorithm exists                                         1\n with sample complexity m and 90% success probability at finding h\u2217, we must have r(x\u2217) \u2265                                                10m.\n This immediately gives an algorithm for the \u03b7 = \u03b5 = 0 setting: query the point x maximizing r(x),\n set w(h) = 0 for all hypotheses h that disagree, and repeat. As long as at least two hypotheses\n remain, the maximum probability will be 50% < 90% and each iteration will remove an \u2126( 1                                                   m)\n fraction of the remaining hypotheses; thus after O(m log H) rounds, only h\u2217                                       will remain. This is\n the basis for Dasgupta [2004].\n Handling noise: initial attempt.                     There are two obvious problems with the above algorithm in\n the agnostic setting, where a (possibly adversarial) \u03b7 fraction of locations x will not match h\u2217.\n First, a single error will cause the algorithm to forever reject the true hypothesis; and second, the\n algorithm makes deterministic queries, which means adversarial noise could be placed precisely on\n the locations queried to make the algorithm learn nothing.\n To fix the first problem, we can adjust the algorithm to perform multiplicative weights: if in round i\n we query a point xi and see yi, we set\n                                            wi+1(h) =         wi(h)               if h(xi) = yi\n                                                               e\u2212\u03b1wi(h)           if h(xi) \u0338= yi\n for a small constant \u03b1 = 1           5. To fix the second problem, we don\u2019t query the single x\u2217                             of maximum\n r(x\u2217), but instead choose x according to distribution q over many points x with large r(x).\n To understand this algorithm, consider how log \u03bbi(h\u2217) evolves in expectation in each step. This\n increases if the query is correct, and decreases if it has an error. A correct query increases \u03bbi in\n                                                                       4", "md": "Query the binary encoding region and take $$O(\\log N)$$ queries. Disagreement coefficient based algorithms, including those in Hanneke and Yang [2015] and Hanneke [2007b], will rely on essentially uniform sampling for the first $$\\Omega(N/\\log N)$$ queries. These algorithms are \u201cminimax\u201d over x, in the sense that if you didn\u2019t see any x from the binary region, you would need almost as many samples as they use. But you do see x from the binary region, so the algorithm should make use of it to get exponential improvement.\n\nFuture Work. Our upper bound assumes full knowledge of DX and the ability to query arbitrary points x. Often in active learning, the algorithm receives a large but not infinite set of unlabeled sample points x, and can only query the labels of those points. How well our results adapt to this setting we leave as an open question.\n\nSimilarly, our bound is polynomial in the number of hypotheses and the domain size. This is hard to avoid in full generality\u2014if you don\u2019t evaluate most hypotheses on most data points, you might be missing the most informative points\u2014but perhaps it can be avoided in structured examples.\n\n## Algorithm Overview\n\nOur algorithm is based on a Bayesian/multiplicative weights type approach to the problem, and is along the lines of the splitting-based approach of Dasgupta [2004].\n\nWe maintain a set of weights $$w(h)$$ for each $$h \\in H$$, starting at 1; these induce a distribution $$\\lambda(h) = \\frac{w(h)}{\\sum_{h'} w(h')}$$ which we can think of as our posterior over the \u201ctrue\u201d $$h^*$$.\n\nRealizable setting. As initial intuition, consider the realizable case of $$\\eta = \\epsilon = 0$$ where we want to find the true $$h^*$$. If $$h^*$$ really were drawn from our prior $$\\lambda$$, and we query a point x, we will see a 1 with probability $$E_{h \\sim \\lambda} [h(x)]$$. Then the most informative point to query is the one we are least confident in, i.e., the point $$x^*$$ maximizing $$r(x) := \\min_{h \\sim \\lambda} E[h(x)], 1 - E[h(x)]$$.\n\nSuppose an algorithm queries $$x_1, ..., x_m$$ and receives the majority label under $$h \\sim \\lambda$$ each time. Then the fraction of $$h \\sim \\lambda$$ that agree with all the queries is at least $$1 - \\frac{1}{m} \\sum_{i=1}^m r(x_i) \\geq 1 - m r(x^*)$$.\n\nThis suggests that, if $$r(x^*) \\ll m$$, it will be hard to uniquely identify $$h^*$$. It is not hard to formalize this, showing that: if no single hypothesis has 75% probability under $$\\lambda$$, and any algorithm exists with sample complexity m and 90% success probability at finding $$h^*$$, we must have $$r(x^*) \\geq \\frac{1}{10}m$$.\n\nThis immediately gives an algorithm for the $$\\eta = \\epsilon = 0$$ setting: query the point x maximizing $$r(x)$$, set $$w(h) = 0$$ for all hypotheses h that disagree, and repeat. As long as at least two hypotheses remain, the maximum probability will be 50% < 90% and each iteration will remove an $$\\Omega(\\frac{1}{m})$$ fraction of the remaining hypotheses; thus after $$O(m \\log H)$$ rounds, only $$h^*$$ will remain. This is the basis for Dasgupta [2004].\n\nHandling noise: initial attempt. There are two obvious problems with the above algorithm in the agnostic setting, where a (possibly adversarial) $$\\eta$$ fraction of locations x will not match $$h^*$$. First, a single error will cause the algorithm to forever reject the true hypothesis; and second, the algorithm makes deterministic queries, which means adversarial noise could be placed precisely on the locations queried to make the algorithm learn nothing.\n\nTo fix the first problem, we can adjust the algorithm to perform multiplicative weights: if in round i we query a point $$x_i$$ and see $$y_i$$, we set\n\n$$\nw_{i+1}(h) = \\begin{cases}\nw_i(h) & \\text{if } h(x_i) = y_i \\\\\ne^{-\\alpha w_i(h)} & \\text{if } h(x_i) \\neq y_i\n\\end{cases}\n$$\n\nfor a small constant $$\\alpha = \\frac{1}{5}$$. To fix the second problem, we don\u2019t query the single $$x^*$$ of maximum $$r(x^*)$$, but instead choose x according to distribution q over many points x with large $$r(x)$$.\n\nTo understand this algorithm, consider how $$\\log \\lambda_i(h^*)$$ evolves in expectation in each step. This increases if the query is correct, and decreases if it has an error. A correct query increases $$\\lambda_i$$ in", "images": [], "items": [{"type": "text", "value": "Query the binary encoding region and take $$O(\\log N)$$ queries. Disagreement coefficient based algorithms, including those in Hanneke and Yang [2015] and Hanneke [2007b], will rely on essentially uniform sampling for the first $$\\Omega(N/\\log N)$$ queries. These algorithms are \u201cminimax\u201d over x, in the sense that if you didn\u2019t see any x from the binary region, you would need almost as many samples as they use. But you do see x from the binary region, so the algorithm should make use of it to get exponential improvement.\n\nFuture Work. Our upper bound assumes full knowledge of DX and the ability to query arbitrary points x. Often in active learning, the algorithm receives a large but not infinite set of unlabeled sample points x, and can only query the labels of those points. How well our results adapt to this setting we leave as an open question.\n\nSimilarly, our bound is polynomial in the number of hypotheses and the domain size. This is hard to avoid in full generality\u2014if you don\u2019t evaluate most hypotheses on most data points, you might be missing the most informative points\u2014but perhaps it can be avoided in structured examples.", "md": "Query the binary encoding region and take $$O(\\log N)$$ queries. Disagreement coefficient based algorithms, including those in Hanneke and Yang [2015] and Hanneke [2007b], will rely on essentially uniform sampling for the first $$\\Omega(N/\\log N)$$ queries. These algorithms are \u201cminimax\u201d over x, in the sense that if you didn\u2019t see any x from the binary region, you would need almost as many samples as they use. But you do see x from the binary region, so the algorithm should make use of it to get exponential improvement.\n\nFuture Work. Our upper bound assumes full knowledge of DX and the ability to query arbitrary points x. Often in active learning, the algorithm receives a large but not infinite set of unlabeled sample points x, and can only query the labels of those points. How well our results adapt to this setting we leave as an open question.\n\nSimilarly, our bound is polynomial in the number of hypotheses and the domain size. This is hard to avoid in full generality\u2014if you don\u2019t evaluate most hypotheses on most data points, you might be missing the most informative points\u2014but perhaps it can be avoided in structured examples."}, {"type": "heading", "lvl": 2, "value": "Algorithm Overview", "md": "## Algorithm Overview"}, {"type": "text", "value": "Our algorithm is based on a Bayesian/multiplicative weights type approach to the problem, and is along the lines of the splitting-based approach of Dasgupta [2004].\n\nWe maintain a set of weights $$w(h)$$ for each $$h \\in H$$, starting at 1; these induce a distribution $$\\lambda(h) = \\frac{w(h)}{\\sum_{h'} w(h')}$$ which we can think of as our posterior over the \u201ctrue\u201d $$h^*$$.\n\nRealizable setting. As initial intuition, consider the realizable case of $$\\eta = \\epsilon = 0$$ where we want to find the true $$h^*$$. If $$h^*$$ really were drawn from our prior $$\\lambda$$, and we query a point x, we will see a 1 with probability $$E_{h \\sim \\lambda} [h(x)]$$. Then the most informative point to query is the one we are least confident in, i.e., the point $$x^*$$ maximizing $$r(x) := \\min_{h \\sim \\lambda} E[h(x)], 1 - E[h(x)]$$.\n\nSuppose an algorithm queries $$x_1, ..., x_m$$ and receives the majority label under $$h \\sim \\lambda$$ each time. Then the fraction of $$h \\sim \\lambda$$ that agree with all the queries is at least $$1 - \\frac{1}{m} \\sum_{i=1}^m r(x_i) \\geq 1 - m r(x^*)$$.\n\nThis suggests that, if $$r(x^*) \\ll m$$, it will be hard to uniquely identify $$h^*$$. It is not hard to formalize this, showing that: if no single hypothesis has 75% probability under $$\\lambda$$, and any algorithm exists with sample complexity m and 90% success probability at finding $$h^*$$, we must have $$r(x^*) \\geq \\frac{1}{10}m$$.\n\nThis immediately gives an algorithm for the $$\\eta = \\epsilon = 0$$ setting: query the point x maximizing $$r(x)$$, set $$w(h) = 0$$ for all hypotheses h that disagree, and repeat. As long as at least two hypotheses remain, the maximum probability will be 50% < 90% and each iteration will remove an $$\\Omega(\\frac{1}{m})$$ fraction of the remaining hypotheses; thus after $$O(m \\log H)$$ rounds, only $$h^*$$ will remain. This is the basis for Dasgupta [2004].\n\nHandling noise: initial attempt. There are two obvious problems with the above algorithm in the agnostic setting, where a (possibly adversarial) $$\\eta$$ fraction of locations x will not match $$h^*$$. First, a single error will cause the algorithm to forever reject the true hypothesis; and second, the algorithm makes deterministic queries, which means adversarial noise could be placed precisely on the locations queried to make the algorithm learn nothing.\n\nTo fix the first problem, we can adjust the algorithm to perform multiplicative weights: if in round i we query a point $$x_i$$ and see $$y_i$$, we set\n\n$$\nw_{i+1}(h) = \\begin{cases}\nw_i(h) & \\text{if } h(x_i) = y_i \\\\\ne^{-\\alpha w_i(h)} & \\text{if } h(x_i) \\neq y_i\n\\end{cases}\n$$\n\nfor a small constant $$\\alpha = \\frac{1}{5}$$. To fix the second problem, we don\u2019t query the single $$x^*$$ of maximum $$r(x^*)$$, but instead choose x according to distribution q over many points x with large $$r(x)$$.\n\nTo understand this algorithm, consider how $$\\log \\lambda_i(h^*)$$ evolves in expectation in each step. This increases if the query is correct, and decreases if it has an error. A correct query increases $$\\lambda_i$$ in", "md": "Our algorithm is based on a Bayesian/multiplicative weights type approach to the problem, and is along the lines of the splitting-based approach of Dasgupta [2004].\n\nWe maintain a set of weights $$w(h)$$ for each $$h \\in H$$, starting at 1; these induce a distribution $$\\lambda(h) = \\frac{w(h)}{\\sum_{h'} w(h')}$$ which we can think of as our posterior over the \u201ctrue\u201d $$h^*$$.\n\nRealizable setting. As initial intuition, consider the realizable case of $$\\eta = \\epsilon = 0$$ where we want to find the true $$h^*$$. If $$h^*$$ really were drawn from our prior $$\\lambda$$, and we query a point x, we will see a 1 with probability $$E_{h \\sim \\lambda} [h(x)]$$. Then the most informative point to query is the one we are least confident in, i.e., the point $$x^*$$ maximizing $$r(x) := \\min_{h \\sim \\lambda} E[h(x)], 1 - E[h(x)]$$.\n\nSuppose an algorithm queries $$x_1, ..., x_m$$ and receives the majority label under $$h \\sim \\lambda$$ each time. Then the fraction of $$h \\sim \\lambda$$ that agree with all the queries is at least $$1 - \\frac{1}{m} \\sum_{i=1}^m r(x_i) \\geq 1 - m r(x^*)$$.\n\nThis suggests that, if $$r(x^*) \\ll m$$, it will be hard to uniquely identify $$h^*$$. It is not hard to formalize this, showing that: if no single hypothesis has 75% probability under $$\\lambda$$, and any algorithm exists with sample complexity m and 90% success probability at finding $$h^*$$, we must have $$r(x^*) \\geq \\frac{1}{10}m$$.\n\nThis immediately gives an algorithm for the $$\\eta = \\epsilon = 0$$ setting: query the point x maximizing $$r(x)$$, set $$w(h) = 0$$ for all hypotheses h that disagree, and repeat. As long as at least two hypotheses remain, the maximum probability will be 50% < 90% and each iteration will remove an $$\\Omega(\\frac{1}{m})$$ fraction of the remaining hypotheses; thus after $$O(m \\log H)$$ rounds, only $$h^*$$ will remain. This is the basis for Dasgupta [2004].\n\nHandling noise: initial attempt. There are two obvious problems with the above algorithm in the agnostic setting, where a (possibly adversarial) $$\\eta$$ fraction of locations x will not match $$h^*$$. First, a single error will cause the algorithm to forever reject the true hypothesis; and second, the algorithm makes deterministic queries, which means adversarial noise could be placed precisely on the locations queried to make the algorithm learn nothing.\n\nTo fix the first problem, we can adjust the algorithm to perform multiplicative weights: if in round i we query a point $$x_i$$ and see $$y_i$$, we set\n\n$$\nw_{i+1}(h) = \\begin{cases}\nw_i(h) & \\text{if } h(x_i) = y_i \\\\\ne^{-\\alpha w_i(h)} & \\text{if } h(x_i) \\neq y_i\n\\end{cases}\n$$\n\nfor a small constant $$\\alpha = \\frac{1}{5}$$. To fix the second problem, we don\u2019t query the single $$x^*$$ of maximum $$r(x^*)$$, but instead choose x according to distribution q over many points x with large $$r(x)$$.\n\nTo understand this algorithm, consider how $$\\log \\lambda_i(h^*)$$ evolves in expectation in each step. This increases if the query is correct, and decreases if it has an error. A correct query increases $$\\lambda_i$$ in"}]}, {"page": 5, "text": "                                                             \u03bb(h)            Values h(x)\n                                                h1            0.9             1111 1111\n                                                h2      0.1 \u2212    10\u22126         1111 0000\n                                                h3           10\u22126             0000 1110\n                                                 y                            0000 1111\nFigure 1: An example demonstrating that the weight of the true hypothesis can decrease if \u03bb is concentrated on the wrong ball. In this example,\nthe true labels y are closest to h3. But if the prior \u03bb on hypotheses puts far more weight on h1 and h2, the algorithm will query uniformly\nover where h1 and h2 disagree: the second half of points. Over this query distribution, h1 is more correct than h3, so the weight of h3 can\nactually decrease if \u03bb(h1) is very large.\nproportion to the fraction of \u03bb placed on hypotheses that get the query wrong, which is at least r(x);\n                                                                          q(x)\nand the probability of an error is at most \u03b7 maxx                        Dx(x). If at iteration i the algorithm uses query\ndistribution q, some calculation gives that\n                   E                                                          E                                 q(x)       .               (1)\n                   q [log \u03bbi+1(h\u2217) \u2212          log \u03bbi(h\u2217)] \u2265        0.9\u03b1      x\u223cq[r(x)] \u2212       2.3\u03b7 max  x    Dx(x)\nThe algorithm can choose q to maximize this bound on the potential gain. There\u2019s a tradeoff between\nconcentrating the samples over the x of largest r(x), and spreading out the samples so the adversary\ncan\u2019t raise the error probability too high. We show that if learning is possible by any algorithm (for\na constant factor larger \u03b7), then there exists a q for which this potential gain is significant.\nLemma 2.1 (Connection to OPT). Define \u2225h \u2212                            h\u2032\u2225   = Prx\u223cDx[h(x) \u0338= h\u2032(x)]. Let \u03bb be a distribu-\ntion over H such that no radius-(2\u03b7 + \u03b5) ball B centered on h \u2208                                  H has probability at least 80%.\nLet m\u2217     = m\u2217      H, DX, \u03b7, \u03b5, 99     100   . Then there exists a query distribution q over X with\n                                          E                                 q(x)              9\n                                         x\u223cq[r(x)] \u2212         1       x\n                                                            10\u03b7 max       DX(x) \u2265         100m\u2217      .\nAt a very high level, the proof is: imagine h\u2217                     \u223c   \u03bb. If the algorithm only sees the majority label y\non every query it performs, then its output                  h is independent of h\u2217            and cannot be valid for more than\n80% of inputs by the ball assumption; hence a 99% successful algorithm must have a 19% chance\nof seeing a minority label. But for m\u2217                   queries x drawn with marginal distribution q, without noise\nthe expected number of minority labels seen is m\u2217                          E[r(x)], so E[r(x)] \u2273              1/m\u2217. With noise, the\nadversary can corrupt the minority labels in h\u2217                    back toward the majority, leading to the given bound.\nThe query distribution optimizing (1) has a simple structure: take a threshold \u03c4 for r(x), sample\nfrom Dx conditioned on r(x) > \u03c4, and possibly sample x with r(x) = \u03c4 at a lower rate. This means\nthe algorithm can efficiently find the optimal q.\nExcept for the caveat about \u03bb not already concentrating in a small ball, applying Lemma 2.1 com-\nbined with (1) shows that log \u03bb(h\u2217) grows by \u2126( 1                      m\u2217   ) in expectation for each query. It starts out at\nlog \u03bb(h\u2217) = \u2212         log H, so after O(m\u2217           log H) queries we would have \u03bb(h\u2217) being a large constant in\nexpectation (and with high probability, by Freedman\u2019s inequality for concentration of martingales).\nOf course \u03bb(h\u2217) can\u2019t grow past 1, which features in this argument in that once \u03bb(h\u2217) > 80%, a\nsmall ball will have large probability and Lemma 2.1 no longer applies, but at that point we can just\noutput any hypothesis in the heavy ball.\nHandling noise: the challenge.                    There is one omission in the above argument that is surprisingly\nchallenging to fix, and ends up requiring significant changes to the algorithm: if at an intermediate\nstep \u03bbi concentrates in the wrong small ball, the algorithm will not necessarily make progress. It is\nentirely possible that \u03bbi concentrates in a small ball, even in the first iteration\u2014perhaps 99% of the\nhypotheses in H are close to each other. And if that happens, then we will have r(x) \u2264                                             0.01 for\nmost x, which could make the RHS of (1) negative for all q.\nIn fact, it seems like a reasonable Bayesian-inspired algorithm really must allow \u03bb(h\u2217) to decrease\nin some situations. Consider the setting of Figure 1. We have three hypotheses, h1, h2, and h3, and a\nprior \u03bb = (0.9, 0.099999, 10\u22126). Because \u03bb(h3) is so tiny, the algorithm presumably should ignore\nh3 and query essentially uniformly from the locations where h1 and h2 disagree. In this example,\nh3 agrees with h1 on all but an \u03b7 mass in those locations, so even if h\u2217                               = h3, the query distribution\ncan match h1 perfectly and not h3. Then w(h1) stays constant while w(h3) shrinks. w(h2) shrinks\n                                                                      5", "md": "# Math Equations and Text\n\n## Math Equations and Text\n\nFigure 1: An example demonstrating that the weight of the true hypothesis can decrease if \u03bb is concentrated on the wrong ball. In this example, the true labels y are closest to h3. But if the prior \u03bb on hypotheses puts far more weight on h1 and h2, the algorithm will query uniformly over where h1 and h2 disagree: the second half of points. Over this query distribution, h1 is more correct than h3, so the weight of h3 can actually decrease if \u03bb(h1) is very large.\n\nproportion to the fraction of \u03bb placed on hypotheses that get the query wrong, which is at least r(x);\n\n$$q(x)$$\nand the probability of an error is at most \u03b7 maxx $$D_x(x)$$. If at iteration i the algorithm uses query distribution q, some calculation gives that\n\n$$E[q[\\log \\lambda_{i+1}(h^*) - \\log \\lambda_i(h^*)]] \\geq 0.9\\alpha \\mathbb{E}_{x \\sim q}[r(x)] - 2.3\\eta \\max_x D_x(x)$$\nThe algorithm can choose q to maximize this bound on the potential gain. There\u2019s a tradeoff between concentrating the samples over the x of largest $$r(x)$$, and spreading out the samples so the adversary can\u2019t raise the error probability too high. We show that if learning is possible by any algorithm (for a constant factor larger \u03b7), then there exists a q for which this potential gain is significant.\n\nLemma 2.1 (Connection to OPT). Define $$\\|h - h'\\| = \\Pr_{x \\sim D_x}[h(x) \\neq h'(x)]$$. Let \u03bb be a distribution over H such that no radius-(2\u03b7 + \u03b5) ball B centered on $$h \\in H$$ has probability at least 80%. Let $$m^* = m^*_{H, D_x, \\eta, \\epsilon, 99/100}$$. Then there exists a query distribution q over X with\n\n$$\\mathbb{E}_{x \\sim q}[r(x)] - \\frac{1}{10} \\max_x D_x(x) \\geq \\frac{9}{100}m^*$$\nAt a very high level, the proof is: imagine $$h^* \\sim \\lambda$$. If the algorithm only sees the majority label y on every query it performs, then its output h is independent of $$h^*$$ and cannot be valid for more than 80% of inputs by the ball assumption; hence a 99% successful algorithm must have a 19% chance of seeing a minority label. But for $$m^*$$ queries x drawn with marginal distribution q, without noise the expected number of minority labels seen is $$m^* \\mathbb{E}[r(x)]$$, so $$\\mathbb{E}[r(x)] \\gtrsim \\frac{1}{m^*}$$. With noise, the adversary can corrupt the minority labels in $$h^*$$ back toward the majority, leading to the given bound.\n\nThe query distribution optimizing (1) has a simple structure: take a threshold \u03c4 for $$r(x)$$, sample from $$D_x$$ conditioned on $$r(x) > \\tau$$, and possibly sample x with $$r(x) = \\tau$$ at a lower rate. This means the algorithm can efficiently find the optimal q.\n\nExcept for the caveat about \u03bb not already concentrating in a small ball, applying Lemma 2.1 combined with (1) shows that $$\\log \\lambda(h^*)$$ grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in expectation for each query. It starts out at $$\\log \\lambda(h^*) = -\\log H$$, so after $$O(m^* \\log H)$$ queries we would have $$\\lambda(h^*)$$ being a large constant in expectation (and with high probability, by Freedman\u2019s inequality for concentration of martingales).\n\nOf course $$\\lambda(h^*)$$ can\u2019t grow past 1, which features in this argument in that once $$\\lambda(h^*) > 80%$$, a small ball will have large probability and Lemma 2.1 no longer applies, but at that point we can just output any hypothesis in the heavy ball.\n\nHandling noise: the challenge. There is one omission in the above argument that is surprisingly challenging to fix, and ends up requiring significant changes to the algorithm: if at an intermediate step $$\\lambda_i$$ concentrates in the wrong small ball, the algorithm will not necessarily make progress. It is entirely possible that $$\\lambda_i$$ concentrates in a small ball, even in the first iteration\u2014perhaps 99% of the hypotheses in H are close to each other. And if that happens, then we will have $$r(x) \\leq 0.01$$ for most x, which could make the RHS of (1) negative for all q.\n\nIn fact, it seems like a reasonable Bayesian-inspired algorithm really must allow $$\\lambda(h^*)$$ to decrease in some situations. Consider the setting of Figure 1. We have three hypotheses, h1, h2, and h3, and a prior $$\\lambda = (0.9, 0.099999, 10^{-6})$$. Because $$\\lambda(h3)$$ is so tiny, the algorithm presumably should ignore h3 and query essentially uniformly from the locations where h1 and h2 disagree. In this example, h3 agrees with h1 on all but an $$\\eta$$ mass in those locations, so even if $$h^* = h3$$, the query distribution can match h1 perfectly and not h3. Then $$w(h1)$$ stays constant while $$w(h3)$$ shrinks $$w(h2)$$ shrinks", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Math Equations and Text", "md": "## Math Equations and Text"}, {"type": "text", "value": "Figure 1: An example demonstrating that the weight of the true hypothesis can decrease if \u03bb is concentrated on the wrong ball. In this example, the true labels y are closest to h3. But if the prior \u03bb on hypotheses puts far more weight on h1 and h2, the algorithm will query uniformly over where h1 and h2 disagree: the second half of points. Over this query distribution, h1 is more correct than h3, so the weight of h3 can actually decrease if \u03bb(h1) is very large.\n\nproportion to the fraction of \u03bb placed on hypotheses that get the query wrong, which is at least r(x);\n\n$$q(x)$$\nand the probability of an error is at most \u03b7 maxx $$D_x(x)$$. If at iteration i the algorithm uses query distribution q, some calculation gives that\n\n$$E[q[\\log \\lambda_{i+1}(h^*) - \\log \\lambda_i(h^*)]] \\geq 0.9\\alpha \\mathbb{E}_{x \\sim q}[r(x)] - 2.3\\eta \\max_x D_x(x)$$\nThe algorithm can choose q to maximize this bound on the potential gain. There\u2019s a tradeoff between concentrating the samples over the x of largest $$r(x)$$, and spreading out the samples so the adversary can\u2019t raise the error probability too high. We show that if learning is possible by any algorithm (for a constant factor larger \u03b7), then there exists a q for which this potential gain is significant.\n\nLemma 2.1 (Connection to OPT). Define $$\\|h - h'\\| = \\Pr_{x \\sim D_x}[h(x) \\neq h'(x)]$$. Let \u03bb be a distribution over H such that no radius-(2\u03b7 + \u03b5) ball B centered on $$h \\in H$$ has probability at least 80%. Let $$m^* = m^*_{H, D_x, \\eta, \\epsilon, 99/100}$$. Then there exists a query distribution q over X with\n\n$$\\mathbb{E}_{x \\sim q}[r(x)] - \\frac{1}{10} \\max_x D_x(x) \\geq \\frac{9}{100}m^*$$\nAt a very high level, the proof is: imagine $$h^* \\sim \\lambda$$. If the algorithm only sees the majority label y on every query it performs, then its output h is independent of $$h^*$$ and cannot be valid for more than 80% of inputs by the ball assumption; hence a 99% successful algorithm must have a 19% chance of seeing a minority label. But for $$m^*$$ queries x drawn with marginal distribution q, without noise the expected number of minority labels seen is $$m^* \\mathbb{E}[r(x)]$$, so $$\\mathbb{E}[r(x)] \\gtrsim \\frac{1}{m^*}$$. With noise, the adversary can corrupt the minority labels in $$h^*$$ back toward the majority, leading to the given bound.\n\nThe query distribution optimizing (1) has a simple structure: take a threshold \u03c4 for $$r(x)$$, sample from $$D_x$$ conditioned on $$r(x) > \\tau$$, and possibly sample x with $$r(x) = \\tau$$ at a lower rate. This means the algorithm can efficiently find the optimal q.\n\nExcept for the caveat about \u03bb not already concentrating in a small ball, applying Lemma 2.1 combined with (1) shows that $$\\log \\lambda(h^*)$$ grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in expectation for each query. It starts out at $$\\log \\lambda(h^*) = -\\log H$$, so after $$O(m^* \\log H)$$ queries we would have $$\\lambda(h^*)$$ being a large constant in expectation (and with high probability, by Freedman\u2019s inequality for concentration of martingales).\n\nOf course $$\\lambda(h^*)$$ can\u2019t grow past 1, which features in this argument in that once $$\\lambda(h^*) > 80%$$, a small ball will have large probability and Lemma 2.1 no longer applies, but at that point we can just output any hypothesis in the heavy ball.\n\nHandling noise: the challenge. There is one omission in the above argument that is surprisingly challenging to fix, and ends up requiring significant changes to the algorithm: if at an intermediate step $$\\lambda_i$$ concentrates in the wrong small ball, the algorithm will not necessarily make progress. It is entirely possible that $$\\lambda_i$$ concentrates in a small ball, even in the first iteration\u2014perhaps 99% of the hypotheses in H are close to each other. And if that happens, then we will have $$r(x) \\leq 0.01$$ for most x, which could make the RHS of (1) negative for all q.\n\nIn fact, it seems like a reasonable Bayesian-inspired algorithm really must allow $$\\lambda(h^*)$$ to decrease in some situations. Consider the setting of Figure 1. We have three hypotheses, h1, h2, and h3, and a prior $$\\lambda = (0.9, 0.099999, 10^{-6})$$. Because $$\\lambda(h3)$$ is so tiny, the algorithm presumably should ignore h3 and query essentially uniformly from the locations where h1 and h2 disagree. In this example, h3 agrees with h1 on all but an $$\\eta$$ mass in those locations, so even if $$h^* = h3$$, the query distribution can match h1 perfectly and not h3. Then $$w(h1)$$ stays constant while $$w(h3)$$ shrinks $$w(h2)$$ shrinks", "md": "Figure 1: An example demonstrating that the weight of the true hypothesis can decrease if \u03bb is concentrated on the wrong ball. In this example, the true labels y are closest to h3. But if the prior \u03bb on hypotheses puts far more weight on h1 and h2, the algorithm will query uniformly over where h1 and h2 disagree: the second half of points. Over this query distribution, h1 is more correct than h3, so the weight of h3 can actually decrease if \u03bb(h1) is very large.\n\nproportion to the fraction of \u03bb placed on hypotheses that get the query wrong, which is at least r(x);\n\n$$q(x)$$\nand the probability of an error is at most \u03b7 maxx $$D_x(x)$$. If at iteration i the algorithm uses query distribution q, some calculation gives that\n\n$$E[q[\\log \\lambda_{i+1}(h^*) - \\log \\lambda_i(h^*)]] \\geq 0.9\\alpha \\mathbb{E}_{x \\sim q}[r(x)] - 2.3\\eta \\max_x D_x(x)$$\nThe algorithm can choose q to maximize this bound on the potential gain. There\u2019s a tradeoff between concentrating the samples over the x of largest $$r(x)$$, and spreading out the samples so the adversary can\u2019t raise the error probability too high. We show that if learning is possible by any algorithm (for a constant factor larger \u03b7), then there exists a q for which this potential gain is significant.\n\nLemma 2.1 (Connection to OPT). Define $$\\|h - h'\\| = \\Pr_{x \\sim D_x}[h(x) \\neq h'(x)]$$. Let \u03bb be a distribution over H such that no radius-(2\u03b7 + \u03b5) ball B centered on $$h \\in H$$ has probability at least 80%. Let $$m^* = m^*_{H, D_x, \\eta, \\epsilon, 99/100}$$. Then there exists a query distribution q over X with\n\n$$\\mathbb{E}_{x \\sim q}[r(x)] - \\frac{1}{10} \\max_x D_x(x) \\geq \\frac{9}{100}m^*$$\nAt a very high level, the proof is: imagine $$h^* \\sim \\lambda$$. If the algorithm only sees the majority label y on every query it performs, then its output h is independent of $$h^*$$ and cannot be valid for more than 80% of inputs by the ball assumption; hence a 99% successful algorithm must have a 19% chance of seeing a minority label. But for $$m^*$$ queries x drawn with marginal distribution q, without noise the expected number of minority labels seen is $$m^* \\mathbb{E}[r(x)]$$, so $$\\mathbb{E}[r(x)] \\gtrsim \\frac{1}{m^*}$$. With noise, the adversary can corrupt the minority labels in $$h^*$$ back toward the majority, leading to the given bound.\n\nThe query distribution optimizing (1) has a simple structure: take a threshold \u03c4 for $$r(x)$$, sample from $$D_x$$ conditioned on $$r(x) > \\tau$$, and possibly sample x with $$r(x) = \\tau$$ at a lower rate. This means the algorithm can efficiently find the optimal q.\n\nExcept for the caveat about \u03bb not already concentrating in a small ball, applying Lemma 2.1 combined with (1) shows that $$\\log \\lambda(h^*)$$ grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in expectation for each query. It starts out at $$\\log \\lambda(h^*) = -\\log H$$, so after $$O(m^* \\log H)$$ queries we would have $$\\lambda(h^*)$$ being a large constant in expectation (and with high probability, by Freedman\u2019s inequality for concentration of martingales).\n\nOf course $$\\lambda(h^*)$$ can\u2019t grow past 1, which features in this argument in that once $$\\lambda(h^*) > 80%$$, a small ball will have large probability and Lemma 2.1 no longer applies, but at that point we can just output any hypothesis in the heavy ball.\n\nHandling noise: the challenge. There is one omission in the above argument that is surprisingly challenging to fix, and ends up requiring significant changes to the algorithm: if at an intermediate step $$\\lambda_i$$ concentrates in the wrong small ball, the algorithm will not necessarily make progress. It is entirely possible that $$\\lambda_i$$ concentrates in a small ball, even in the first iteration\u2014perhaps 99% of the hypotheses in H are close to each other. And if that happens, then we will have $$r(x) \\leq 0.01$$ for most x, which could make the RHS of (1) negative for all q.\n\nIn fact, it seems like a reasonable Bayesian-inspired algorithm really must allow $$\\lambda(h^*)$$ to decrease in some situations. Consider the setting of Figure 1. We have three hypotheses, h1, h2, and h3, and a prior $$\\lambda = (0.9, 0.099999, 10^{-6})$$. Because $$\\lambda(h3)$$ is so tiny, the algorithm presumably should ignore h3 and query essentially uniformly from the locations where h1 and h2 disagree. In this example, h3 agrees with h1 on all but an $$\\eta$$ mass in those locations, so even if $$h^* = h3$$, the query distribution can match h1 perfectly and not h3. Then $$w(h1)$$ stays constant while $$w(h3)$$ shrinks $$w(h2)$$ shrinks"}]}, {"page": 6, "text": " much faster, of course, but since the denominator is dominated by w(h1) , \u03bb(h3) will still shrink.\n However, despite \u03bb(h3) shrinking, the algorithm is still making progress in this example: \u03bb(h2) is\n shrinking fast, and once it becomes small relative to \u03bb(h3) then the algorithm will start querying\n points to distinguish h3 from h1, at which point \u03bb(h3) will start an inexorable rise.\n Our solution is to \u201ccap\u201d the large density balls in \u03bb, dividing their probability by two, when applying\n Lemma 2.1. Our algorithm maintains a set S \u2286                        H of the \u201chigh-density region,\u201d such that the capped\n distribution:                               \u03bb(h) :=      1  2\u03bb(h)        2 Pr[h\u2208S]        h \u2208   S\n                                                            \u03bb(h) \u00b7 1\u22121  1\u2212Pr[h\u2208S]          h /\u2208  S\n has no large ball. Then Lemma 2.1 applies to \u03bb, giving the existence of a query distribution q so that\n the corresponding r(x) is large. We then define the potential function\n                                                                                        \u03bbi(h\u2217)\n                                          \u03d5i(h\u2217) := log \u03bbi(h\u2217) + log                   h/                                                   (2)\n for h\u2217     /                                                                           \u2208Si \u03bbi(h)\n            \u2208   Si, and \u03d5i = 0 for h\u2217                 \u2208   Si. We show that \u03d5i grows by \u2126( 1                     m\u2217  ) in expectation in\n each iteration. Thus, as in the example of Figure 1, either \u03bb(h\u2217) grows as a fraction of the whole\n distribution, or as a fraction of the \u201clow-density\u201d region.\n If at any iteration we find that \u03bb has some heavy ball B(\u00b5, 2\u03b7 + \u03b5) so Lemma 2.1 would not apply,\n we add B (\u00b5\u2032, 6\u03b7 + 3\u03b5) to S, where B (\u00b5\u2032, 2\u03b7 + \u03b5) is the heaviest ball before capping. We show that\n this ensures that no small heavy ball exists in the capped distribution \u03bb. Expanding S only increases\n the potential function, and then the lack of heavy ball implies the potential will continue to grow.\n Thus the potential (2) starts at \u22122 log |H|, and grows by \u2126( 1                      m\u2217   ) in each iteration. After O(m\u2217              log H)\n iterations, we will have \u03d5i \u2265             0 in expectation (and with high probability by Freedman\u2019s inequality).\n This is only possible if h\u2217            \u2208   S, which means that one of the centers \u00b5 of the balls added to S is a\n valid answer.\n In fact, with some careful analysis we can show that with 1 \u2212                                 \u03b4 probability that one of the first\n O(log H   \u03b4 ) balls added to S is a valid answer. The algorithm can then check all the centers of these\n balls, using the following active agnostic learning algorithm:\n Theorem 2.2. Active agnostic learning can be solved for \u03b5 = 3\u03b7 with O                                      |H| log |H|  \u03b4      samples.\n Proof. The algorithm is the following. Take any pair h, h\u2032 with \u2225h\u2212h\u2032\u2225                                  \u2265   3\u03b7. Sample O           log |H|\u03b4\n observations randomly from (x \u223c                    Dx | h(x) \u0338= h\u2032(x)). One of h, h\u2032 is wrong on at least half the\n queries; remove it from H and repeat. At the end, return any remaining h.\n To analyze this, let h\u2217          \u2208   H be the hypothesis with error \u03b7. If h\u2217                   is chosen in a round, the other h\u2032\n must have error at least 2\u03b7. Therefore the chance we remove h\u2217                                 is at most \u03b4/ |H|. In each round\n we remove a hypothesis, so there are at most |H| rounds and at most \u03b4 probability of ever crossing\n off h\u2217. If we never cross off h\u2217, at the end we output some h with \u2225h \u2212                                      h\u2217\u2225    \u2264   3\u03b7, which gives\n \u03b5 = 3\u03b7.\n The linear dependence on |H| makes the Theorem 2.2 algorithm quite bad in most circumstances,\n but the dependence only on |H| makes it perfect for our second stage (where we have reduced to\n O(log |H|) candidate hypotheses).\n Overall, this argument gives an O                  m\u2217    log |H|\u03b4 + log |H|    \u03b4 log log|H| \u03b4       sample algorithm for agnostic\n active learning. One can simplify this bound by observing that the set of centers C added by our\n algorithm form a packing, and must therefore all be distinguishable by the optimal algorithm, so\n m\u2217   \u2265   log C. This gives a bound of             O     (m\u2217    + log 1   \u03b4 ) log |H|\u03b4       .\n By starting with an \u03b7-net of size N, we can reduce |H| to N with a constant factor increase in \u03b7.\nWith some properly chosen constants c4 and c5, the entire algorithm is formally described in Algo-\n rithm 1.\n                                                                       6", "md": "much faster, of course, but since the denominator is dominated by w(h1) , \u03bb(h3) will still shrink.\nHowever, despite $$\\lambda(h3)$$ shrinking, the algorithm is still making progress in this example: $$\\lambda(h2)$$ is\nshrinking fast, and once it becomes small relative to $$\\lambda(h3)$$ then the algorithm will start querying\npoints to distinguish h3 from h1, at which point $$\\lambda(h3)$$ will start an inexorable rise.\n\nOur solution is to \u201ccap\u201d the large density balls in $$\\lambda$$, dividing their probability by two, when applying\nLemma 2.1. Our algorithm maintains a set $$S \\subseteq H$$ of the \u201chigh-density region,\u201d such that the capped\ndistribution: $$\\lambda(h) := \\begin{cases} \\frac{1}{2}\\lambda(h) & \\text{if } h \\in S \\\\ \\lambda(h) \\cdot \\frac{1 - \\frac{1}{1 - Pr[h \\in S]}}{1 - Pr[h \\in S]} & \\text{if } h \\notin S \\end{cases}$$\nhas no large ball. Then Lemma 2.1 applies to $$\\lambda$$, giving the existence of a query distribution q so that\nthe corresponding r(x) is large. We then define the potential function\n$$\\phi_i(h^*) := \\log \\lambda_i(h^*) + \\log \\frac{\\lambda_i(h^*)}{h \\notin S}$$\nfor $$h^* \\notin S$$, and $$\\phi_i = 0$$ for $$h^* \\in S$$. We show that $$\\phi_i$$ grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in\neach iteration. Thus, as in the example of Figure 1, either $$\\lambda(h^*)$$ grows as a fraction of the whole\ndistribution, or as a fraction of the \u201clow-density\u201d region.\n\nIf at any iteration we find that $$\\lambda$$ has some heavy ball $$B(\\mu, 2\\eta + \\epsilon)$$ so Lemma 2.1 would not apply,\nwe add $$B(\\mu', 6\\eta + 3\\epsilon)$$ to $$S$$, where $$B(\\mu', 2\\eta + \\epsilon)$$ is the heaviest ball before capping. We show that\nthis ensures that no small heavy ball exists in the capped distribution $$\\lambda$$. Expanding $$S$$ only increases\nthe potential function, and then the lack of heavy ball implies the potential will continue to grow.\nThus the potential (2) starts at $$-2 \\log |H|$$, and grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in each iteration. After $$O(m^* \\log H)$$\niterations, we will have $$\\phi_i \\geq 0$$ in expectation (and with high probability by Freedman\u2019s inequality).\nThis is only possible if $$h^* \\in S$$, which means that one of the centers $$\\mu$$ of the balls added to $$S$$ is a\nvalid answer.\n\nIn fact, with some careful analysis we can show that with $$1 - \\delta$$ probability that one of the first\n$$O(\\log H / \\delta)$$ balls added to $$S$$ is a valid answer. The algorithm can then check all the centers of these\nballs, using the following active agnostic learning algorithm:\n\nTheorem 2.2. Active agnostic learning can be solved for $$\\epsilon = 3\\eta$$ with $$O(|H| \\log |H| / \\delta)$$ samples.\n\nProof. The algorithm is the following. Take any pair $$h, h'$$ with $$\\|h-h'\\| \\geq 3\\eta$$. Sample $$O(\\log |H|/\\delta)$$\nobservations randomly from $$(x \\sim D_x | h(x) \\neq h'(x))$$. One of $$h, h'$$ is wrong on at least half the\nqueries; remove it from $$H$$ and repeat. At the end, return any remaining $$h$$.\n\nTo analyze this, let $$h^* \\in H$$ be the hypothesis with error $$\\eta$$. If $$h^*$$ is chosen in a round, the other $$h'$$\nmust have error at least $$2\\eta$$. Therefore the chance we remove $$h^*$$ is at most $$\\delta/|H|$$. In each round\nwe remove a hypothesis, so there are at most $$|H|$$ rounds and at most $$\\delta$$ probability of ever crossing\noff $$h^*$$. If we never cross off $$h^*$$, at the end we output some $$h$$ with $$\\|h - h^*\\| \\leq 3\\eta$$, which gives\n$$\\epsilon = 3\\eta$$.\n\nThe linear dependence on $$|H|$$ makes the Theorem 2.2 algorithm quite bad in most circumstances,\nbut the dependence only on $$|H|$$ makes it perfect for our second stage (where we have reduced to\n$$O(\\log |H|)$$ candidate hypotheses).\n\nOverall, this argument gives an $$O(m^* \\log |H|/\\delta + \\log |H|/\\delta \\log \\log|H|/\\delta)$$ sample algorithm for agnostic\nactive learning. One can simplify this bound by observing that the set of centers $$C$$ added by our\nalgorithm form a packing, and must therefore all be distinguishable by the optimal algorithm, so\n$$m^* \\geq \\log C$$. This gives a bound of $$O((m^* + \\log 1/\\delta) \\log |H|/\\delta)$$. By starting with an $$\\eta$$-net of size $$N$$, we can reduce $$|H|$$ to $$N$$ with a constant factor increase in $$\\eta$$.\nWith some properly chosen constants $$c4$$ and $$c5$$, the entire algorithm is formally described in Algorithm 1.\n\n6", "images": [], "items": [{"type": "text", "value": "much faster, of course, but since the denominator is dominated by w(h1) , \u03bb(h3) will still shrink.\nHowever, despite $$\\lambda(h3)$$ shrinking, the algorithm is still making progress in this example: $$\\lambda(h2)$$ is\nshrinking fast, and once it becomes small relative to $$\\lambda(h3)$$ then the algorithm will start querying\npoints to distinguish h3 from h1, at which point $$\\lambda(h3)$$ will start an inexorable rise.\n\nOur solution is to \u201ccap\u201d the large density balls in $$\\lambda$$, dividing their probability by two, when applying\nLemma 2.1. Our algorithm maintains a set $$S \\subseteq H$$ of the \u201chigh-density region,\u201d such that the capped\ndistribution: $$\\lambda(h) := \\begin{cases} \\frac{1}{2}\\lambda(h) & \\text{if } h \\in S \\\\ \\lambda(h) \\cdot \\frac{1 - \\frac{1}{1 - Pr[h \\in S]}}{1 - Pr[h \\in S]} & \\text{if } h \\notin S \\end{cases}$$\nhas no large ball. Then Lemma 2.1 applies to $$\\lambda$$, giving the existence of a query distribution q so that\nthe corresponding r(x) is large. We then define the potential function\n$$\\phi_i(h^*) := \\log \\lambda_i(h^*) + \\log \\frac{\\lambda_i(h^*)}{h \\notin S}$$\nfor $$h^* \\notin S$$, and $$\\phi_i = 0$$ for $$h^* \\in S$$. We show that $$\\phi_i$$ grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in\neach iteration. Thus, as in the example of Figure 1, either $$\\lambda(h^*)$$ grows as a fraction of the whole\ndistribution, or as a fraction of the \u201clow-density\u201d region.\n\nIf at any iteration we find that $$\\lambda$$ has some heavy ball $$B(\\mu, 2\\eta + \\epsilon)$$ so Lemma 2.1 would not apply,\nwe add $$B(\\mu', 6\\eta + 3\\epsilon)$$ to $$S$$, where $$B(\\mu', 2\\eta + \\epsilon)$$ is the heaviest ball before capping. We show that\nthis ensures that no small heavy ball exists in the capped distribution $$\\lambda$$. Expanding $$S$$ only increases\nthe potential function, and then the lack of heavy ball implies the potential will continue to grow.\nThus the potential (2) starts at $$-2 \\log |H|$$, and grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in each iteration. After $$O(m^* \\log H)$$\niterations, we will have $$\\phi_i \\geq 0$$ in expectation (and with high probability by Freedman\u2019s inequality).\nThis is only possible if $$h^* \\in S$$, which means that one of the centers $$\\mu$$ of the balls added to $$S$$ is a\nvalid answer.\n\nIn fact, with some careful analysis we can show that with $$1 - \\delta$$ probability that one of the first\n$$O(\\log H / \\delta)$$ balls added to $$S$$ is a valid answer. The algorithm can then check all the centers of these\nballs, using the following active agnostic learning algorithm:\n\nTheorem 2.2. Active agnostic learning can be solved for $$\\epsilon = 3\\eta$$ with $$O(|H| \\log |H| / \\delta)$$ samples.\n\nProof. The algorithm is the following. Take any pair $$h, h'$$ with $$\\|h-h'\\| \\geq 3\\eta$$. Sample $$O(\\log |H|/\\delta)$$\nobservations randomly from $$(x \\sim D_x | h(x) \\neq h'(x))$$. One of $$h, h'$$ is wrong on at least half the\nqueries; remove it from $$H$$ and repeat. At the end, return any remaining $$h$$.\n\nTo analyze this, let $$h^* \\in H$$ be the hypothesis with error $$\\eta$$. If $$h^*$$ is chosen in a round, the other $$h'$$\nmust have error at least $$2\\eta$$. Therefore the chance we remove $$h^*$$ is at most $$\\delta/|H|$$. In each round\nwe remove a hypothesis, so there are at most $$|H|$$ rounds and at most $$\\delta$$ probability of ever crossing\noff $$h^*$$. If we never cross off $$h^*$$, at the end we output some $$h$$ with $$\\|h - h^*\\| \\leq 3\\eta$$, which gives\n$$\\epsilon = 3\\eta$$.\n\nThe linear dependence on $$|H|$$ makes the Theorem 2.2 algorithm quite bad in most circumstances,\nbut the dependence only on $$|H|$$ makes it perfect for our second stage (where we have reduced to\n$$O(\\log |H|)$$ candidate hypotheses).\n\nOverall, this argument gives an $$O(m^* \\log |H|/\\delta + \\log |H|/\\delta \\log \\log|H|/\\delta)$$ sample algorithm for agnostic\nactive learning. One can simplify this bound by observing that the set of centers $$C$$ added by our\nalgorithm form a packing, and must therefore all be distinguishable by the optimal algorithm, so\n$$m^* \\geq \\log C$$. This gives a bound of $$O((m^* + \\log 1/\\delta) \\log |H|/\\delta)$$. By starting with an $$\\eta$$-net of size $$N$$, we can reduce $$|H|$$ to $$N$$ with a constant factor increase in $$\\eta$$.\nWith some properly chosen constants $$c4$$ and $$c5$$, the entire algorithm is formally described in Algorithm 1.\n\n6", "md": "much faster, of course, but since the denominator is dominated by w(h1) , \u03bb(h3) will still shrink.\nHowever, despite $$\\lambda(h3)$$ shrinking, the algorithm is still making progress in this example: $$\\lambda(h2)$$ is\nshrinking fast, and once it becomes small relative to $$\\lambda(h3)$$ then the algorithm will start querying\npoints to distinguish h3 from h1, at which point $$\\lambda(h3)$$ will start an inexorable rise.\n\nOur solution is to \u201ccap\u201d the large density balls in $$\\lambda$$, dividing their probability by two, when applying\nLemma 2.1. Our algorithm maintains a set $$S \\subseteq H$$ of the \u201chigh-density region,\u201d such that the capped\ndistribution: $$\\lambda(h) := \\begin{cases} \\frac{1}{2}\\lambda(h) & \\text{if } h \\in S \\\\ \\lambda(h) \\cdot \\frac{1 - \\frac{1}{1 - Pr[h \\in S]}}{1 - Pr[h \\in S]} & \\text{if } h \\notin S \\end{cases}$$\nhas no large ball. Then Lemma 2.1 applies to $$\\lambda$$, giving the existence of a query distribution q so that\nthe corresponding r(x) is large. We then define the potential function\n$$\\phi_i(h^*) := \\log \\lambda_i(h^*) + \\log \\frac{\\lambda_i(h^*)}{h \\notin S}$$\nfor $$h^* \\notin S$$, and $$\\phi_i = 0$$ for $$h^* \\in S$$. We show that $$\\phi_i$$ grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in\neach iteration. Thus, as in the example of Figure 1, either $$\\lambda(h^*)$$ grows as a fraction of the whole\ndistribution, or as a fraction of the \u201clow-density\u201d region.\n\nIf at any iteration we find that $$\\lambda$$ has some heavy ball $$B(\\mu, 2\\eta + \\epsilon)$$ so Lemma 2.1 would not apply,\nwe add $$B(\\mu', 6\\eta + 3\\epsilon)$$ to $$S$$, where $$B(\\mu', 2\\eta + \\epsilon)$$ is the heaviest ball before capping. We show that\nthis ensures that no small heavy ball exists in the capped distribution $$\\lambda$$. Expanding $$S$$ only increases\nthe potential function, and then the lack of heavy ball implies the potential will continue to grow.\nThus the potential (2) starts at $$-2 \\log |H|$$, and grows by $$\\Omega\\left(\\frac{1}{m^*}\\right)$$ in each iteration. After $$O(m^* \\log H)$$\niterations, we will have $$\\phi_i \\geq 0$$ in expectation (and with high probability by Freedman\u2019s inequality).\nThis is only possible if $$h^* \\in S$$, which means that one of the centers $$\\mu$$ of the balls added to $$S$$ is a\nvalid answer.\n\nIn fact, with some careful analysis we can show that with $$1 - \\delta$$ probability that one of the first\n$$O(\\log H / \\delta)$$ balls added to $$S$$ is a valid answer. The algorithm can then check all the centers of these\nballs, using the following active agnostic learning algorithm:\n\nTheorem 2.2. Active agnostic learning can be solved for $$\\epsilon = 3\\eta$$ with $$O(|H| \\log |H| / \\delta)$$ samples.\n\nProof. The algorithm is the following. Take any pair $$h, h'$$ with $$\\|h-h'\\| \\geq 3\\eta$$. Sample $$O(\\log |H|/\\delta)$$\nobservations randomly from $$(x \\sim D_x | h(x) \\neq h'(x))$$. One of $$h, h'$$ is wrong on at least half the\nqueries; remove it from $$H$$ and repeat. At the end, return any remaining $$h$$.\n\nTo analyze this, let $$h^* \\in H$$ be the hypothesis with error $$\\eta$$. If $$h^*$$ is chosen in a round, the other $$h'$$\nmust have error at least $$2\\eta$$. Therefore the chance we remove $$h^*$$ is at most $$\\delta/|H|$$. In each round\nwe remove a hypothesis, so there are at most $$|H|$$ rounds and at most $$\\delta$$ probability of ever crossing\noff $$h^*$$. If we never cross off $$h^*$$, at the end we output some $$h$$ with $$\\|h - h^*\\| \\leq 3\\eta$$, which gives\n$$\\epsilon = 3\\eta$$.\n\nThe linear dependence on $$|H|$$ makes the Theorem 2.2 algorithm quite bad in most circumstances,\nbut the dependence only on $$|H|$$ makes it perfect for our second stage (where we have reduced to\n$$O(\\log |H|)$$ candidate hypotheses).\n\nOverall, this argument gives an $$O(m^* \\log |H|/\\delta + \\log |H|/\\delta \\log \\log|H|/\\delta)$$ sample algorithm for agnostic\nactive learning. One can simplify this bound by observing that the set of centers $$C$$ added by our\nalgorithm form a packing, and must therefore all be distinguishable by the optimal algorithm, so\n$$m^* \\geq \\log C$$. This gives a bound of $$O((m^* + \\log 1/\\delta) \\log |H|/\\delta)$$. By starting with an $$\\eta$$-net of size $$N$$, we can reduce $$|H|$$ to $$N$$ with a constant factor increase in $$\\eta$$.\nWith some properly chosen constants $$c4$$ and $$c5$$, the entire algorithm is formally described in Algorithm 1.\n\n6"}]}, {"page": 7, "text": "Remark 1:       As stated, the algorithm requires knowing m\u2217          to set the target sample complexity /\nnumber of rounds k. This restriction could be removed with the following idea. m\u2217                  only enters\nthe analysis through the fact that O       1   is a lower bound on the expected increase of the potential\n                                          m\u2217\nfunction in each iteration. However, the algorithm knows a bound on its expected increase in each\nround i; it is the value\n                               \u03c4i = max    E                             q(x)\n                                      q   x\u223cq[ri,Si(x)] \u2212    c4     x\n                                                             20\u03b7 max    DX(x).\noptimized in the algorithm. Therefore, we could use an adaptive termination criterion that stops at\niteration k if  k i=1 \u03c4i \u2265   O(log |H|\n                                     \u03b4 ). This will guarantee that when terminating, the potential will\nbe above 0 with high probability so our analysis holds.\nRemark 2:       The algorithm\u2019s running time is polynomial in |H|. This is in general not avoidable,\nsince the input is a truth table for H. The bottleneck of the computation is the step where the\nalgorithm checks if the heaviest ball has mass greater than 80%. This step could be accelerated by\nrandomly sampling hypothesis and points to estimate and find heavy balls; this would improve the\ndependence to nearly linear in |H|. If the hypothesis class has some structure, like the binary search\nexample, the algorithm can be implemented more efficiently.\nAlgorithm 1 Competitive Algorithm for Active Agnostic Learning\n   Compute a 2\u03b7 maximal packing H\u2032\n   Let w0 = 1 for every h \u2208     H\u2032.\n   S0 \u2190   \u2205\n   C \u2190   \u2205\n   for i = 1, . . . , k = O  m\u2217  log |H\u2032|   do\n                                       \u03b4\n       Compute \u03bbi(h) =          wi\u22121(h)\n                               h\u2208H wi\u22121(h) for every h \u2208    H\n       if there exists c4\u03b7 + c5\u03b5 ball with probability > 80% over \u03bbi,Si\u22121 then\n           Si \u2190   Si \u222a  B (\u00b5\u2032, 3c4\u03b7 + 3c5\u03b5) where B (\u00b5\u2032, c4\u03b7 + c5\u03b5) is the heaviest radius c4\u03b7 + c5\u03b5\n           ball over \u03bbi\n       elseC \u2190    C \u222a  {\u00b5\u2032}\n           Si \u2190   Si\u22121      1 2\u03bbi(h)                        h \u2208  Si\n       Compute \u03bbi,S   i =    \u03bbi(h) \u00b7  1\u221212 Prh\u223c\u03bbi[h\u2208Si]     h /\n       Compute ri,S                    1\u2212Prh\u223c\u03bbi[h\u2208Si]         \u2208  Si\n                      i(x) = min      Eh\u223c\u03bbi,Si [h(x)], 1 \u2212   Eh\u223c\u03bbi,Si [h(x)]    for every x \u2208   X\n       Find a query distribution by solving\n       q\u2217 = max     E                             q(x)                  E                         (3)\n               q   x\u223cq[ri,Si(x)] \u2212    c4     x                        x\u223cDX [q(x)] \u2264    3\u03b7\n                                     20\u03b7 max     DX(x) subject to\n       Query x \u223c    q\u2217, getting label y\n       Set wi(h) =     wi\u22121(h)           if h(x) = y\n                        e\u2212\u03b1wi\u22121(h)       if h(x) \u0338= y for every h \u2208    H\u2032\n   Find the best hypothesis \u02c6  h in C using the stage two algorithm in Theorem 2.2\n   return \u02c6h\nGeneralization for Better Bounds.           To get a better dependence for 1d threshold functions, we\nseparate out the Lemma 2.1 bound on (1) from the analysis of the algorithm given a bound on (1).\nThen for particular instances like 1d threshold functions, we get a better bound on the algorithm by\ngiving a larger bound on (1).\nTheorem 2.3. Suppose that Dx and H are such that, for any distribution \u03bb over H such that no\nradius-(c4\u03b7 + c5\u03b5) ball has probability more than 80%, there exists a distribution q over X such\nthat\n                                     E                         q(x)\n                                    x\u223cq[r(x)] \u2212    c4     x\n                                                   20\u03b7 max    Dx(x) \u2265    \u03b2\n                                                       7", "md": "# Algorithm for Active Agnostic Learning\n\n## Remark 1:\n\nAs stated, the algorithm requires knowing $$m^*$$ to set the target sample complexity / number of rounds k. This restriction could be removed with the following idea. $$m^*$$ only enters the analysis through the fact that $$O(1)$$ is a lower bound on the expected increase of the potential function in each iteration. However, the algorithm knows a bound on its expected increase in each round i; it is the value\n\n$$\\tau_i = \\max_{q} E_{x\\sim q[r_i,S_i(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)}$$\n\noptimized in the algorithm. Therefore, we could use an adaptive termination criterion that stops at iteration k if $$\\sum_{i=1}^{k} \\tau_i \\geq O(\\log |H| / \\delta)$$. This will guarantee that when terminating, the potential will be above 0 with high probability so our analysis holds.\n\n## Remark 2:\n\nThe algorithm\u2019s running time is polynomial in |H|. This is in general not avoidable, since the input is a truth table for H. The bottleneck of the computation is the step where the algorithm checks if the heaviest ball has mass greater than 80%. This step could be accelerated by randomly sampling hypothesis and points to estimate and find heavy balls; this would improve the dependence to nearly linear in |H|. If the hypothesis class has some structure, like the binary search example, the algorithm can be implemented more efficiently.\n\n## Algorithm 1 Competitive Algorithm for Active Agnostic Learning\n\n1. Compute a $2\\eta$ maximal packing $H'$\n2. Let $w_0 = 1$ for every $h \\in H'$.\n3. $S_0 \\leftarrow \\emptyset$\n4. $C \\leftarrow \\emptyset$\n5. for $i = 1, ..., k = O(m^* \\log |H'| / \\delta)$ do\n6.\nCompute $\\lambda_i(h) = \\frac{w_{i-1}(h)}{\\sum_{h\\in H} w_{i-1}(h)}$ for every $h \\in H$\n7. if there exists $c4\\eta + c5\\epsilon$ ball with probability > 80% over $\\lambda_i,S_{i-1}$ then\n8. 1. $S_i \\leftarrow S_{i-1} \\cup B(\\mu', 3c4\\eta + 3c5\\epsilon)$ where $B(\\mu', c4\\eta + c5\\epsilon)$ is the heaviest radius $c4\\eta + c5\\epsilon$ ball over $\\lambda_i$\n\nelse\n9. 1. $C \\leftarrow C \\cup \\{\\mu'\\}$\n2. $S_i \\leftarrow S_{i-1} \\cup \\left\\{h \\in S_{i-1} : \\frac{1}{2}\\lambda_i(h)\\right\\}$\n\nCompute $\\lambda_i,S_i = \\lambda_i(h) \\cdot \\frac{1 - \\frac{1}{2}Pr_{h\\sim\\lambda_i}[h\\in S_i]}{1 - Pr_{h\\sim\\lambda_i}[h\\in S_i]}$ for every $x \\in X$\n10. Compute $r_i,S_i(x) = \\min\\left(E_{h\\sim\\lambda_i,S_i}[h(x)], 1 - E_{h\\sim\\lambda_i,S_i}[h(x)]\\right)$ for every $x \\in X$\n11. Find a query distribution by solving\n12. $q^* = \\max_{q} E_{x\\sim q[r_i,S_i(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)}$ subject to $\\max_{x} DX(x) \\leq 3\\eta$\n\nQuery $x \\sim q^*$, getting label y\n13. Set $w_i(h) = \\begin{cases} w_{i-1}(h) & \\text{if } h(x) = y \\\\ e^{-\\alpha w_{i-1}(h)} & \\text{if } h(x) \\neq y \\end{cases}$ for every $h \\in H'$\n\nFind the best hypothesis $$\\hat{h}$$ in C using the stage two algorithm in Theorem 2.2\nreturn $$\\hat{h}$$\n\n## Generalization for Better Bounds\n\nTo get a better dependence for 1d threshold functions, we separate out the Lemma 2.1 bound on (1) from the analysis of the algorithm given a bound on (1). Then for particular instances like 1d threshold functions, we get a better bound on the algorithm by giving a larger bound on (1).\n\n### Theorem 2.3:\n\nSuppose that $$D_x$$ and $$H$$ are such that, for any distribution $$\\lambda$$ over $$H$$ such that no radius-($$c4\\eta + c5\\epsilon$$) ball has probability more than 80%, there exists a distribution $$q$$ over $$X$$ such that\n\n$$E_{x\\sim q[r(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)} \\geq \\beta$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Algorithm for Active Agnostic Learning", "md": "# Algorithm for Active Agnostic Learning"}, {"type": "heading", "lvl": 2, "value": "Remark 1:", "md": "## Remark 1:"}, {"type": "text", "value": "As stated, the algorithm requires knowing $$m^*$$ to set the target sample complexity / number of rounds k. This restriction could be removed with the following idea. $$m^*$$ only enters the analysis through the fact that $$O(1)$$ is a lower bound on the expected increase of the potential function in each iteration. However, the algorithm knows a bound on its expected increase in each round i; it is the value\n\n$$\\tau_i = \\max_{q} E_{x\\sim q[r_i,S_i(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)}$$\n\noptimized in the algorithm. Therefore, we could use an adaptive termination criterion that stops at iteration k if $$\\sum_{i=1}^{k} \\tau_i \\geq O(\\log |H| / \\delta)$$. This will guarantee that when terminating, the potential will be above 0 with high probability so our analysis holds.", "md": "As stated, the algorithm requires knowing $$m^*$$ to set the target sample complexity / number of rounds k. This restriction could be removed with the following idea. $$m^*$$ only enters the analysis through the fact that $$O(1)$$ is a lower bound on the expected increase of the potential function in each iteration. However, the algorithm knows a bound on its expected increase in each round i; it is the value\n\n$$\\tau_i = \\max_{q} E_{x\\sim q[r_i,S_i(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)}$$\n\noptimized in the algorithm. Therefore, we could use an adaptive termination criterion that stops at iteration k if $$\\sum_{i=1}^{k} \\tau_i \\geq O(\\log |H| / \\delta)$$. This will guarantee that when terminating, the potential will be above 0 with high probability so our analysis holds."}, {"type": "heading", "lvl": 2, "value": "Remark 2:", "md": "## Remark 2:"}, {"type": "text", "value": "The algorithm\u2019s running time is polynomial in |H|. This is in general not avoidable, since the input is a truth table for H. The bottleneck of the computation is the step where the algorithm checks if the heaviest ball has mass greater than 80%. This step could be accelerated by randomly sampling hypothesis and points to estimate and find heavy balls; this would improve the dependence to nearly linear in |H|. If the hypothesis class has some structure, like the binary search example, the algorithm can be implemented more efficiently.", "md": "The algorithm\u2019s running time is polynomial in |H|. This is in general not avoidable, since the input is a truth table for H. The bottleneck of the computation is the step where the algorithm checks if the heaviest ball has mass greater than 80%. This step could be accelerated by randomly sampling hypothesis and points to estimate and find heavy balls; this would improve the dependence to nearly linear in |H|. If the hypothesis class has some structure, like the binary search example, the algorithm can be implemented more efficiently."}, {"type": "heading", "lvl": 2, "value": "Algorithm 1 Competitive Algorithm for Active Agnostic Learning", "md": "## Algorithm 1 Competitive Algorithm for Active Agnostic Learning"}, {"type": "text", "value": "1. Compute a $2\\eta$ maximal packing $H'$\n2. Let $w_0 = 1$ for every $h \\in H'$.\n3. $S_0 \\leftarrow \\emptyset$\n4. $C \\leftarrow \\emptyset$\n5. for $i = 1, ..., k = O(m^* \\log |H'| / \\delta)$ do\n6.\nCompute $\\lambda_i(h) = \\frac{w_{i-1}(h)}{\\sum_{h\\in H} w_{i-1}(h)}$ for every $h \\in H$\n7. if there exists $c4\\eta + c5\\epsilon$ ball with probability > 80% over $\\lambda_i,S_{i-1}$ then\n8. 1. $S_i \\leftarrow S_{i-1} \\cup B(\\mu', 3c4\\eta + 3c5\\epsilon)$ where $B(\\mu', c4\\eta + c5\\epsilon)$ is the heaviest radius $c4\\eta + c5\\epsilon$ ball over $\\lambda_i$\n\nelse\n9. 1. $C \\leftarrow C \\cup \\{\\mu'\\}$\n2. $S_i \\leftarrow S_{i-1} \\cup \\left\\{h \\in S_{i-1} : \\frac{1}{2}\\lambda_i(h)\\right\\}$\n\nCompute $\\lambda_i,S_i = \\lambda_i(h) \\cdot \\frac{1 - \\frac{1}{2}Pr_{h\\sim\\lambda_i}[h\\in S_i]}{1 - Pr_{h\\sim\\lambda_i}[h\\in S_i]}$ for every $x \\in X$\n10. Compute $r_i,S_i(x) = \\min\\left(E_{h\\sim\\lambda_i,S_i}[h(x)], 1 - E_{h\\sim\\lambda_i,S_i}[h(x)]\\right)$ for every $x \\in X$\n11. Find a query distribution by solving\n12. $q^* = \\max_{q} E_{x\\sim q[r_i,S_i(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)}$ subject to $\\max_{x} DX(x) \\leq 3\\eta$\n\nQuery $x \\sim q^*$, getting label y\n13. Set $w_i(h) = \\begin{cases} w_{i-1}(h) & \\text{if } h(x) = y \\\\ e^{-\\alpha w_{i-1}(h)} & \\text{if } h(x) \\neq y \\end{cases}$ for every $h \\in H'$\n\nFind the best hypothesis $$\\hat{h}$$ in C using the stage two algorithm in Theorem 2.2\nreturn $$\\hat{h}$$", "md": "1. Compute a $2\\eta$ maximal packing $H'$\n2. Let $w_0 = 1$ for every $h \\in H'$.\n3. $S_0 \\leftarrow \\emptyset$\n4. $C \\leftarrow \\emptyset$\n5. for $i = 1, ..., k = O(m^* \\log |H'| / \\delta)$ do\n6.\nCompute $\\lambda_i(h) = \\frac{w_{i-1}(h)}{\\sum_{h\\in H} w_{i-1}(h)}$ for every $h \\in H$\n7. if there exists $c4\\eta + c5\\epsilon$ ball with probability > 80% over $\\lambda_i,S_{i-1}$ then\n8. 1. $S_i \\leftarrow S_{i-1} \\cup B(\\mu', 3c4\\eta + 3c5\\epsilon)$ where $B(\\mu', c4\\eta + c5\\epsilon)$ is the heaviest radius $c4\\eta + c5\\epsilon$ ball over $\\lambda_i$\n\nelse\n9. 1. $C \\leftarrow C \\cup \\{\\mu'\\}$\n2. $S_i \\leftarrow S_{i-1} \\cup \\left\\{h \\in S_{i-1} : \\frac{1}{2}\\lambda_i(h)\\right\\}$\n\nCompute $\\lambda_i,S_i = \\lambda_i(h) \\cdot \\frac{1 - \\frac{1}{2}Pr_{h\\sim\\lambda_i}[h\\in S_i]}{1 - Pr_{h\\sim\\lambda_i}[h\\in S_i]}$ for every $x \\in X$\n10. Compute $r_i,S_i(x) = \\min\\left(E_{h\\sim\\lambda_i,S_i}[h(x)], 1 - E_{h\\sim\\lambda_i,S_i}[h(x)]\\right)$ for every $x \\in X$\n11. Find a query distribution by solving\n12. $q^* = \\max_{q} E_{x\\sim q[r_i,S_i(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)}$ subject to $\\max_{x} DX(x) \\leq 3\\eta$\n\nQuery $x \\sim q^*$, getting label y\n13. Set $w_i(h) = \\begin{cases} w_{i-1}(h) & \\text{if } h(x) = y \\\\ e^{-\\alpha w_{i-1}(h)} & \\text{if } h(x) \\neq y \\end{cases}$ for every $h \\in H'$\n\nFind the best hypothesis $$\\hat{h}$$ in C using the stage two algorithm in Theorem 2.2\nreturn $$\\hat{h}$$"}, {"type": "heading", "lvl": 2, "value": "Generalization for Better Bounds", "md": "## Generalization for Better Bounds"}, {"type": "text", "value": "To get a better dependence for 1d threshold functions, we separate out the Lemma 2.1 bound on (1) from the analysis of the algorithm given a bound on (1). Then for particular instances like 1d threshold functions, we get a better bound on the algorithm by giving a larger bound on (1).", "md": "To get a better dependence for 1d threshold functions, we separate out the Lemma 2.1 bound on (1) from the analysis of the algorithm given a bound on (1). Then for particular instances like 1d threshold functions, we get a better bound on the algorithm by giving a larger bound on (1)."}, {"type": "heading", "lvl": 3, "value": "Theorem 2.3:", "md": "### Theorem 2.3:"}, {"type": "text", "value": "Suppose that $$D_x$$ and $$H$$ are such that, for any distribution $$\\lambda$$ over $$H$$ such that no radius-($$c4\\eta + c5\\epsilon$$) ball has probability more than 80%, there exists a distribution $$q$$ over $$X$$ such that\n\n$$E_{x\\sim q[r(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)} \\geq \\beta$$", "md": "Suppose that $$D_x$$ and $$H$$ are such that, for any distribution $$\\lambda$$ over $$H$$ such that no radius-($$c4\\eta + c5\\epsilon$$) ball has probability more than 80%, there exists a distribution $$q$$ over $$X$$ such that\n\n$$E_{x\\sim q[r(x)]} - \\frac{c4x}{20\\eta \\max_{x} DX(x)} \\geq \\beta$$"}]}, {"page": 8, "text": "                                                                    1\n for some \u03b2 > 0. Then for \u03b5 \u2265           c1\u03b7, c4 \u2265     300, c5 =     10 and c1 \u2265      90c4, let N = N(H, Dx, \u03b7)\n be the size of an \u03b7-cover of H.              Algorithm 1 solves (\u03b7, \u03b5, \u03b4) active agnostic learning with\n O   1\u03b2 log N\u03b4 + log N  \u03b4 log log\u03b4N    samples.\n Corollary 2.4. There exists a constant c1 > 1 such that, for 1d threshold functions and \u03b5 > c1\u03b7,\nAlgorithm 1 solves (\u03b7, \u03b5, \u03b4) active agnostic learning with O             log 1\u03b5\u03b4 log log\u03b41\u03b5   samples.\n Proof. Because the problem is only harder if \u03b7 is larger, we can raise \u03b7 to be \u03b7 = \u03b5/C, where\n C > 1 is a sufficiently large constant that Theorem 2.3 applies. Then 1d threshold functions have\n an \u03b7-cover of size N = O(1/\u03b5). To get the result by Theorem 2.3, it suffices to show \u03b2 = \u0398(1).\n Each hypothesis is of the form h(x) = 1x\u2265\u03c4, and corresponds to a threshold \u03c4. So we can consider\n \u03bb to be a distribution over \u03c4.\n Let \u03bb be any distribution for which no radius-R with probability greater than 80% ball exists, for\n R = c4\u03b7 + c5\u03b5. For any percent p between 0 and 100, let \u03c4p denote the pth percentile of \u03c4 under \u03bb\n (i.e., the smallest t such that Pr[\u03c4 \u2264      t] \u2265  p/100). By the ball assumption, \u03c410 and \u03c490 do not lie in\n the same radius-R ball. Hence \u2225h\u03c410 \u2212       Pr h\u03c490\u2225  > R, or\n                                              x [\u03c410 \u2264   x < \u03c490] > R.\n We let q denote (Dx | \u03c410 \u2264       x < \u03c490). Then for all x \u2208      supp(q) we have r(x) \u2265         0.1 and\n                                     q(x)                    1\n Therefore we can set               Dx(x) =      Pr x\u223cDx[x \u2208     supp(q)] < 1   R.\n                     \u03b2 = E x\u223cq[r(x)] \u2212     c4     x     q(x)                     c4\u03b7\n as needed.                               20\u03b7 max     Dx(x) \u2265      0.1 \u2212  20(c4\u03b7 + c5\u03b5) \u2273       1,\n 3    Proof of Lemma 2.1\n Lemma 2.1 (Connection to OPT). Define \u2225h \u2212               h\u2032\u2225  = Prx\u223cDx[h(x) \u0338= h\u2032(x)]. Let \u03bb be a distribu-\n tion over H such that no radius-(2\u03b7 + \u03b5) ball B centered on h \u2208                 H has probability at least 80%.\n Let m\u2217   = m\u2217     H, DX, \u03b7, \u03b5, 99 100  . Then there exists a query distribution q over X with\n                                    E                          q(x)            9\n                                   x\u223cq[r(x)] \u2212     1      x\n                                                  10\u03b7 max     DX(x) \u2265      100m\u2217    .\n Proof. WLOG, we assume that Prh\u223c\u03bb [h(x) = 0] \u2265                    Prh\u223c\u03bb [h(x) = 1] for every x \u2208           X. This\n means r(x) = Eh\u223c\u03bb[h(x)]. This can be achieved by flipping all h(x) and observations y for all x\n not satisfying this property; such an operation doesn\u2019t affect the lemma statement.\n We will consider an adversary defined by a function g : X \u2192                   [0, 1]. The adversary takes a hy-\n pothesis h \u2208    H and outputs a distribution over y \u2208           {0, 1}X such that 0 \u2264       y(x) \u2264    h(x) always,\n and err(h) = Ex,y[h(x) \u2212          y(x)] \u2264    \u03b7 always. For a hypothesis h, the adversary sets y(x) = 0\n for all x with h(x) = 0, and y(x) = 0 independently with probability g(x) if h(x) = 1\u2014unless\n Ex[h(x)g(x)] > \u03b7, in which case the adversary instead simply outputs y = h to ensure the expected\n error is at most \u03b7 always.\n We consider the agnostic learning instance where x \u223c             Dx, h \u223c    \u03bb, and y is given by this adversary.\n Let A be an (\u03b7, \u03b5) algorithm which uses m measurements and succeeds with 99% probability. Then\n it must also succeed with 99% probability over this distribution. For the algorithm to succeed on a\n sample h, its output    h must have \u2225h\u2212      h\u2225  \u2264  2\u03b7 +\u03b5. By the bounded ball assumption, for any choice\n of adversary, no fixed output succeeds with more than 80% probability over h \u223c                  \u03bb.\n Now, let A0 be the behavior of A if it observes y = 0 for all its queries, rather than the truth; A0\n is independent of the input. A0 has some distribution over m queries, outputs some distribution of\n                                                           8", "md": "for some $$\\beta > 0$$. Then for $$\\epsilon \\geq c_1\\eta$$, $$c_4 \\geq 300$$, $$c_5 = 10$$ and $$c_1 \\geq 90c_4$$, let $$N = N(H, D_x, \\eta)$$ be the size of an $$\\eta$$-cover of $$H$$. Algorithm 1 solves $$(\\eta, \\epsilon, \\delta)$$ active agnostic learning with $$O(1\\beta \\log N\\delta + \\log N \\delta \\log \\log \\delta N)$$ samples.\n\nCorollary 2.4. There exists a constant $$c_1 > 1$$ such that, for 1d threshold functions and $$\\epsilon > c_1\\eta$$, Algorithm 1 solves $$(\\eta, \\epsilon, \\delta)$$ active agnostic learning with $$O(\\log \\frac{1}{\\epsilon\\delta} \\log \\log \\delta \\frac{1}{\\epsilon})$$ samples.\n\nProof. Because the problem is only harder if $$\\eta$$ is larger, we can raise $$\\eta$$ to be $$\\eta = \\frac{\\epsilon}{C}$$, where $$C > 1$$ is a sufficiently large constant that Theorem 2.3 applies. Then 1d threshold functions have an $$\\eta$$-cover of size $$N = O(\\frac{1}{\\epsilon})$$. To get the result by Theorem 2.3, it suffices to show $$\\beta = \\Theta(1)$$. Each hypothesis is of the form $$h(x) = 1_{x \\geq \\tau}$$, and corresponds to a threshold $$\\tau$$. So we can consider $$\\lambda$$ to be a distribution over $$\\tau$$.\n\nLet $$\\lambda$$ be any distribution for which no radius-$$R$$ with probability greater than 80% ball exists, for $$R = c_4\\eta + c_5\\epsilon$$. For any percent $$p$$ between 0 and 100, let $$\\tau_p$$ denote the $$p$$th percentile of $$\\tau$$ under $$\\lambda$$ (i.e., the smallest $$t$$ such that $$\\text{Pr}[\\tau \\leq t] \\geq \\frac{p}{100}$$). By the ball assumption, $$\\tau_{10}$$ and $$\\tau_{90}$$ do not lie in the same radius-$$R$$ ball. Hence $$\\|h_{\\tau_{10}} - \\text{Pr} h_{\\tau_{90}}\\| > R$$, or $$x [\\tau_{10} \\leq x < \\tau_{90}] > R$$.\n\nWe let $$q$$ denote $$(D_x | \\tau_{10} \\leq x < \\tau_{90})$$. Then for all $$x \\in \\text{supp}(q)$$ we have $$r(x) \\geq 0.1$$ and $$q(x) < 1$$. Therefore we can set $$D_x(x) = \\text{Pr} [x \\sim D_x[x \\in \\text{supp}(q)]] < \\frac{1}{R}$$.\n\n$$\\beta = E[x \\sim q][r(x)] - c_4 \\cdot \\max_x q(x) = \\frac{1}{20\\eta} \\max_x D_x(x) \\geq 0.1 - \\frac{20(c_4\\eta + c_5\\epsilon)}{\\eta} \\gg 1$$,\n\nProof of Lemma 2.1\n\nLemma 2.1 (Connection to OPT). Define $$\\|h - h'\\| = \\text{Pr}_{x \\sim D_x}[h(x) \\neq h'(x)]$$. Let $$\\lambda$$ be a distribution over $$H$$ such that no radius-$$2\\eta + \\epsilon$$ ball $$B$$ centered on $$h \\in H$$ has probability at least 80%.\n\nLet $$m^* = m^*(H, D_X, \\eta, \\epsilon, \\frac{99}{100})$$. Then there exists a query distribution $$q$$ over $$X$$ with $$E[x \\sim q][r(x)] - \\frac{1}{10} \\max_x D_X(x) \\geq \\frac{100m^*}{9}$$.\n\nProof. WLOG, we assume that $$\\text{Pr}_{h \\sim \\lambda}[h(x) = 0] \\geq \\text{Pr}_{h \\sim \\lambda}[h(x) = 1]$$ for every $$x \\in X$$. This means $$r(x) = E_{h \\sim \\lambda}[h(x)]$$. This can be achieved by flipping all $$h(x)$$ and observations $$y$$ for all $$x$$ not satisfying this property; such an operation doesn\u2019t affect the lemma statement.\n\nWe will consider an adversary defined by a function $$g : X \\rightarrow [0, 1]$$. The adversary takes a hypothesis $$h \\in H$$ and outputs a distribution over $$y \\in \\{0, 1\\}^X$$ such that $$0 \\leq y(x) \\leq h(x)$$ always, and $$\\text{err}(h) = E_{x,y}[h(x) - y(x)] \\leq \\eta$$ always. For a hypothesis $$h$$, the adversary sets $$y(x) = 0$$ for all $$x$$ with $$h(x) = 0$$, and $$y(x) = 0$$ independently with probability $$g(x)$$ if $$h(x) = 1$$\u2014unless $$E_x[h(x)g(x)] > \\eta$$, in which case the adversary instead simply outputs $$y = h$$ to ensure the expected error is at most $$\\eta$$ always.\n\nWe consider the agnostic learning instance where $$x \\sim D_X$$, $$h \\sim \\lambda$$, and $$y$$ is given by this adversary. Let $$A$$ be an $$(\\eta, \\epsilon)$$ algorithm which uses $$m$$ measurements and succeeds with 99% probability. Then it must also succeed with 99% probability over this distribution. For the algorithm to succeed on a sample $$h$$, its output $$h$$ must have $$\\|h - h\\| \\leq 2\\eta + \\epsilon$$. By the bounded ball assumption, for any choice of adversary, no fixed output succeeds with more than 80% probability over $$h \\sim \\lambda$$.\n\nNow, let $$A_0$$ be the behavior of $$A$$ if it observes $$y = 0$$ for all its queries, rather than the truth; $$A_0$$ is independent of the input. $$A_0$$ has some distribution over $$m$$ queries, outputs some distribution of", "images": [], "items": [{"type": "text", "value": "for some $$\\beta > 0$$. Then for $$\\epsilon \\geq c_1\\eta$$, $$c_4 \\geq 300$$, $$c_5 = 10$$ and $$c_1 \\geq 90c_4$$, let $$N = N(H, D_x, \\eta)$$ be the size of an $$\\eta$$-cover of $$H$$. Algorithm 1 solves $$(\\eta, \\epsilon, \\delta)$$ active agnostic learning with $$O(1\\beta \\log N\\delta + \\log N \\delta \\log \\log \\delta N)$$ samples.\n\nCorollary 2.4. There exists a constant $$c_1 > 1$$ such that, for 1d threshold functions and $$\\epsilon > c_1\\eta$$, Algorithm 1 solves $$(\\eta, \\epsilon, \\delta)$$ active agnostic learning with $$O(\\log \\frac{1}{\\epsilon\\delta} \\log \\log \\delta \\frac{1}{\\epsilon})$$ samples.\n\nProof. Because the problem is only harder if $$\\eta$$ is larger, we can raise $$\\eta$$ to be $$\\eta = \\frac{\\epsilon}{C}$$, where $$C > 1$$ is a sufficiently large constant that Theorem 2.3 applies. Then 1d threshold functions have an $$\\eta$$-cover of size $$N = O(\\frac{1}{\\epsilon})$$. To get the result by Theorem 2.3, it suffices to show $$\\beta = \\Theta(1)$$. Each hypothesis is of the form $$h(x) = 1_{x \\geq \\tau}$$, and corresponds to a threshold $$\\tau$$. So we can consider $$\\lambda$$ to be a distribution over $$\\tau$$.\n\nLet $$\\lambda$$ be any distribution for which no radius-$$R$$ with probability greater than 80% ball exists, for $$R = c_4\\eta + c_5\\epsilon$$. For any percent $$p$$ between 0 and 100, let $$\\tau_p$$ denote the $$p$$th percentile of $$\\tau$$ under $$\\lambda$$ (i.e., the smallest $$t$$ such that $$\\text{Pr}[\\tau \\leq t] \\geq \\frac{p}{100}$$). By the ball assumption, $$\\tau_{10}$$ and $$\\tau_{90}$$ do not lie in the same radius-$$R$$ ball. Hence $$\\|h_{\\tau_{10}} - \\text{Pr} h_{\\tau_{90}}\\| > R$$, or $$x [\\tau_{10} \\leq x < \\tau_{90}] > R$$.\n\nWe let $$q$$ denote $$(D_x | \\tau_{10} \\leq x < \\tau_{90})$$. Then for all $$x \\in \\text{supp}(q)$$ we have $$r(x) \\geq 0.1$$ and $$q(x) < 1$$. Therefore we can set $$D_x(x) = \\text{Pr} [x \\sim D_x[x \\in \\text{supp}(q)]] < \\frac{1}{R}$$.\n\n$$\\beta = E[x \\sim q][r(x)] - c_4 \\cdot \\max_x q(x) = \\frac{1}{20\\eta} \\max_x D_x(x) \\geq 0.1 - \\frac{20(c_4\\eta + c_5\\epsilon)}{\\eta} \\gg 1$$,\n\nProof of Lemma 2.1\n\nLemma 2.1 (Connection to OPT). Define $$\\|h - h'\\| = \\text{Pr}_{x \\sim D_x}[h(x) \\neq h'(x)]$$. Let $$\\lambda$$ be a distribution over $$H$$ such that no radius-$$2\\eta + \\epsilon$$ ball $$B$$ centered on $$h \\in H$$ has probability at least 80%.\n\nLet $$m^* = m^*(H, D_X, \\eta, \\epsilon, \\frac{99}{100})$$. Then there exists a query distribution $$q$$ over $$X$$ with $$E[x \\sim q][r(x)] - \\frac{1}{10} \\max_x D_X(x) \\geq \\frac{100m^*}{9}$$.\n\nProof. WLOG, we assume that $$\\text{Pr}_{h \\sim \\lambda}[h(x) = 0] \\geq \\text{Pr}_{h \\sim \\lambda}[h(x) = 1]$$ for every $$x \\in X$$. This means $$r(x) = E_{h \\sim \\lambda}[h(x)]$$. This can be achieved by flipping all $$h(x)$$ and observations $$y$$ for all $$x$$ not satisfying this property; such an operation doesn\u2019t affect the lemma statement.\n\nWe will consider an adversary defined by a function $$g : X \\rightarrow [0, 1]$$. The adversary takes a hypothesis $$h \\in H$$ and outputs a distribution over $$y \\in \\{0, 1\\}^X$$ such that $$0 \\leq y(x) \\leq h(x)$$ always, and $$\\text{err}(h) = E_{x,y}[h(x) - y(x)] \\leq \\eta$$ always. For a hypothesis $$h$$, the adversary sets $$y(x) = 0$$ for all $$x$$ with $$h(x) = 0$$, and $$y(x) = 0$$ independently with probability $$g(x)$$ if $$h(x) = 1$$\u2014unless $$E_x[h(x)g(x)] > \\eta$$, in which case the adversary instead simply outputs $$y = h$$ to ensure the expected error is at most $$\\eta$$ always.\n\nWe consider the agnostic learning instance where $$x \\sim D_X$$, $$h \\sim \\lambda$$, and $$y$$ is given by this adversary. Let $$A$$ be an $$(\\eta, \\epsilon)$$ algorithm which uses $$m$$ measurements and succeeds with 99% probability. Then it must also succeed with 99% probability over this distribution. For the algorithm to succeed on a sample $$h$$, its output $$h$$ must have $$\\|h - h\\| \\leq 2\\eta + \\epsilon$$. By the bounded ball assumption, for any choice of adversary, no fixed output succeeds with more than 80% probability over $$h \\sim \\lambda$$.\n\nNow, let $$A_0$$ be the behavior of $$A$$ if it observes $$y = 0$$ for all its queries, rather than the truth; $$A_0$$ is independent of the input. $$A_0$$ has some distribution over $$m$$ queries, outputs some distribution of", "md": "for some $$\\beta > 0$$. Then for $$\\epsilon \\geq c_1\\eta$$, $$c_4 \\geq 300$$, $$c_5 = 10$$ and $$c_1 \\geq 90c_4$$, let $$N = N(H, D_x, \\eta)$$ be the size of an $$\\eta$$-cover of $$H$$. Algorithm 1 solves $$(\\eta, \\epsilon, \\delta)$$ active agnostic learning with $$O(1\\beta \\log N\\delta + \\log N \\delta \\log \\log \\delta N)$$ samples.\n\nCorollary 2.4. There exists a constant $$c_1 > 1$$ such that, for 1d threshold functions and $$\\epsilon > c_1\\eta$$, Algorithm 1 solves $$(\\eta, \\epsilon, \\delta)$$ active agnostic learning with $$O(\\log \\frac{1}{\\epsilon\\delta} \\log \\log \\delta \\frac{1}{\\epsilon})$$ samples.\n\nProof. Because the problem is only harder if $$\\eta$$ is larger, we can raise $$\\eta$$ to be $$\\eta = \\frac{\\epsilon}{C}$$, where $$C > 1$$ is a sufficiently large constant that Theorem 2.3 applies. Then 1d threshold functions have an $$\\eta$$-cover of size $$N = O(\\frac{1}{\\epsilon})$$. To get the result by Theorem 2.3, it suffices to show $$\\beta = \\Theta(1)$$. Each hypothesis is of the form $$h(x) = 1_{x \\geq \\tau}$$, and corresponds to a threshold $$\\tau$$. So we can consider $$\\lambda$$ to be a distribution over $$\\tau$$.\n\nLet $$\\lambda$$ be any distribution for which no radius-$$R$$ with probability greater than 80% ball exists, for $$R = c_4\\eta + c_5\\epsilon$$. For any percent $$p$$ between 0 and 100, let $$\\tau_p$$ denote the $$p$$th percentile of $$\\tau$$ under $$\\lambda$$ (i.e., the smallest $$t$$ such that $$\\text{Pr}[\\tau \\leq t] \\geq \\frac{p}{100}$$). By the ball assumption, $$\\tau_{10}$$ and $$\\tau_{90}$$ do not lie in the same radius-$$R$$ ball. Hence $$\\|h_{\\tau_{10}} - \\text{Pr} h_{\\tau_{90}}\\| > R$$, or $$x [\\tau_{10} \\leq x < \\tau_{90}] > R$$.\n\nWe let $$q$$ denote $$(D_x | \\tau_{10} \\leq x < \\tau_{90})$$. Then for all $$x \\in \\text{supp}(q)$$ we have $$r(x) \\geq 0.1$$ and $$q(x) < 1$$. Therefore we can set $$D_x(x) = \\text{Pr} [x \\sim D_x[x \\in \\text{supp}(q)]] < \\frac{1}{R}$$.\n\n$$\\beta = E[x \\sim q][r(x)] - c_4 \\cdot \\max_x q(x) = \\frac{1}{20\\eta} \\max_x D_x(x) \\geq 0.1 - \\frac{20(c_4\\eta + c_5\\epsilon)}{\\eta} \\gg 1$$,\n\nProof of Lemma 2.1\n\nLemma 2.1 (Connection to OPT). Define $$\\|h - h'\\| = \\text{Pr}_{x \\sim D_x}[h(x) \\neq h'(x)]$$. Let $$\\lambda$$ be a distribution over $$H$$ such that no radius-$$2\\eta + \\epsilon$$ ball $$B$$ centered on $$h \\in H$$ has probability at least 80%.\n\nLet $$m^* = m^*(H, D_X, \\eta, \\epsilon, \\frac{99}{100})$$. Then there exists a query distribution $$q$$ over $$X$$ with $$E[x \\sim q][r(x)] - \\frac{1}{10} \\max_x D_X(x) \\geq \\frac{100m^*}{9}$$.\n\nProof. WLOG, we assume that $$\\text{Pr}_{h \\sim \\lambda}[h(x) = 0] \\geq \\text{Pr}_{h \\sim \\lambda}[h(x) = 1]$$ for every $$x \\in X$$. This means $$r(x) = E_{h \\sim \\lambda}[h(x)]$$. This can be achieved by flipping all $$h(x)$$ and observations $$y$$ for all $$x$$ not satisfying this property; such an operation doesn\u2019t affect the lemma statement.\n\nWe will consider an adversary defined by a function $$g : X \\rightarrow [0, 1]$$. The adversary takes a hypothesis $$h \\in H$$ and outputs a distribution over $$y \\in \\{0, 1\\}^X$$ such that $$0 \\leq y(x) \\leq h(x)$$ always, and $$\\text{err}(h) = E_{x,y}[h(x) - y(x)] \\leq \\eta$$ always. For a hypothesis $$h$$, the adversary sets $$y(x) = 0$$ for all $$x$$ with $$h(x) = 0$$, and $$y(x) = 0$$ independently with probability $$g(x)$$ if $$h(x) = 1$$\u2014unless $$E_x[h(x)g(x)] > \\eta$$, in which case the adversary instead simply outputs $$y = h$$ to ensure the expected error is at most $$\\eta$$ always.\n\nWe consider the agnostic learning instance where $$x \\sim D_X$$, $$h \\sim \\lambda$$, and $$y$$ is given by this adversary. Let $$A$$ be an $$(\\eta, \\epsilon)$$ algorithm which uses $$m$$ measurements and succeeds with 99% probability. Then it must also succeed with 99% probability over this distribution. For the algorithm to succeed on a sample $$h$$, its output $$h$$ must have $$\\|h - h\\| \\leq 2\\eta + \\epsilon$$. By the bounded ball assumption, for any choice of adversary, no fixed output succeeds with more than 80% probability over $$h \\sim \\lambda$$.\n\nNow, let $$A_0$$ be the behavior of $$A$$ if it observes $$y = 0$$ for all its queries, rather than the truth; $$A_0$$ is independent of the input. $$A_0$$ has some distribution over $$m$$ queries, outputs some distribution of"}]}, {"page": 9, "text": "answers    h. Let q(x) =      m1Pr[A0 queries x], so q is a distribution over X. Since A0 outputs a fixed\ndistribution, by the bounded ball assumption, for h \u223c               \u03bb and arbitrary adversary function g,\n                                              Pr\n                                              h\u223c\u03bb[A0 succeeds] \u2264        80%.\nBut A behaves identically to A0 until it sees its first nonzero y. Thus,\nand so            99% \u2264     Pr[A succeeds] \u2264        Pr[A0 succeeds] + Pr[A sees a non-zero y]\n                                          Pr[A sees a non-zero y] \u2265         19%.\nSince A behaves like A0 until the first nonzero, we have\n                              19% \u2264     Pr[A sees a non-zero y]\n                                     = Pr[A0 makes a query x with y(x) = 1]\n                                     \u2264  E[Number queries x by A0 with y(x) = 1]\n                                     = m E  h\u223c\u03bb Ey Ex\u223cq[y(x)].\nAs an initial note, observe that Eh,y[y(x)] \u2264       E   Eh[h(x)] = r(x) so\n                                                  x\u223cq[r(x)] \u2265      0.19\n                                                                    m .\nThus the lemma statement holds for \u03b7 = 0.\nHandling \u03b7 > 0.           Consider the behavior when the adversary\u2019s function g : X \u2192                      [0, 1] satisfies\nEx\u223cDx[g(x)r(x)] \u2264          \u03b7/10. We denote the class of all adversary satisfying this condition as G. We\nhave that                      E       E                    =    E\n                              h\u223c\u03bb    x\u223cDx[g(x)h(x)]            x\u223cD  x[g(x)r(x)] \u2264      \u03b7/10.\nLet Eh denote the event that Ex\u223cDx[g(x)h(x)] \u2264                 \u03b7, so Pr[Eh] \u2264      10%. Furthermore, the adversary\nis designed such that under Eh, Ey[y(x)] = h(x)(1 \u2212                 g(x)) for every x. Therefore:\n                      0.19 \u2264   Pr[A0 makes a query x with y(x) = 1]\n                            \u2264  Pr[Eh] + Pr[A0 makes a query x with y(x) = 1 \u2229                   Eh]\n                            \u2264  0.1 + E[Number queries x by A0 with y(x) = 1 and Eh]\n                            = 0.1 + m E    h   1Eh Ex\u223cq[Ey y(x)]\n                            = 0.1 + m E    h   1Eh Ex\u223cq[h(x)(1 \u2212      g(x))]\n                            \u2264  0.1 + m E  x\u223cq[Eh[h(x)](1 \u2212      g(x))]\nThus                        = 0.1 + m E   x\u223cq[r(x)(1 \u2212      g(x))].\n                                     max   min                                    9                                      (4)\n                                       q    g\u2208G Ex\u223cq[r(x)(1 \u2212      g(x))] \u2265    100m\nover all distributions q and functions g : X \u2192             [0, 1] satisfying Ex\u223cDx[g(x)r(x)] \u2264            \u03b7/10. We now\ntry to understand the structure of the q, g optimizing the LHS of (4).\nLet g\u2217    denote an optimizer of the objective.               First, we show that the constraint is tight, i.e.,\nEx\u223cDx[g\u2217(x)r(x)] = \u03b7/10. Since increasing g improves the constraint, the only way this could\nnot happen is if the maximum possible function, g(x) = 1 for all x, lies in G. But for this function,\nthe LHS of (4) would be 0, which is a contradiction; hence we know increasing g to improve the\nobjective at some point hits the constraint, and hence Ex\u223cDx[g\u2217(x)r(x)] = \u03b7/10.\nFor any q, define \u03c4q \u2265       0 to be the minimum threshold such that\n                                           E     r(x) \u00b7 1    q(x)        < \u03b7/10.\n                                         x\u223cDx              DX (x) >\u03c4q\n                                                             9", "md": "# Math Equations\n\nanswers h. Let $$q(x) = m1Pr[A0 queries x]$$, so q is a distribution over X. Since A0 outputs a fixed\ndistribution, by the bounded ball assumption, for $$h \\sim \\lambda$$ and arbitrary adversary function g,\n$$\nPr[h \\sim \\lambda][A0 succeeds] \\leq 80\\%.\n$$\nBut A behaves identically to A0 until it sees its first nonzero y. Thus,\nand so $$99\\% \\leq Pr[A succeeds] \\leq Pr[A0 succeeds] + Pr[A sees a non-zero y]$$\n$$\nPr[A sees a non-zero y] \\geq 19\\%.\n$$\nSince A behaves like A0 until the first nonzero, we have\n$$\n19\\% \\leq Pr[A sees a non-zero y] = Pr[A0 makes a query x with y(x) = 1] \\leq E[Number queries x by A0 with y(x) = 1] = mE_{h \\sim \\lambda}E_{x \\sim q}[y(x)].\n$$\nAs an initial note, observe that $$E_{h,y}[y(x)] \\leq E_{x \\sim q}[h(x)] = r(x)$$ so\n$$\nE_{x \\sim q}[r(x)] \\geq 0.19m.\n$$\nThus the lemma statement holds for $$\\eta = 0$$.\nHandling $$\\eta > 0$$. Consider the behavior when the adversary\u2019s function $$g : X \\rightarrow [0, 1]$$ satisfies\n$$\nE_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}.\n$$\nWe denote the class of all adversary satisfying this condition as G. We have that\n$$\nE_{h \\sim \\lambda}E_{x \\sim Dx}[g(x)h(x)] \\leq E_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}.\n$$\nLet Eh denote the event that $$E_{x \\sim Dx}[g(x)h(x)] \\leq \\eta$$, so $$Pr[Eh] \\leq 10\\%$$. Furthermore, the adversary\nis designed such that under Eh, $$E_{y}[y(x)] = h(x)(1 - g(x))$$ for every x. Therefore:\n$$\n0.19 \\leq Pr[A0 makes a query x with y(x) = 1] \\leq Pr[Eh] + Pr[A0 makes a query x with y(x) = 1 \\cap Eh] \\leq 0.1 + E[Number queries x by A0 with y(x) = 1 and Eh] = 0.1 + mE_{h}1_{Eh}E_{x \\sim q}[E_{y}y(x)] = 0.1 + mE_{h}1_{Eh}E_{x \\sim q}[h(x)(1 - g(x))] \\leq 0.1 + mE_{x \\sim q}[Eh[h(x)](1 - g(x))]\n$$\nThus\n$$\n= 0.1 + mE_{x \\sim q}[r(x)(1 - g(x))].\n$$\n$$\n\\max_{q} \\min_{g \\in G} E_{x \\sim q}[r(x)(1 - g(x))] \\geq 100m\n$$\nover all distributions q and functions $$g : X \\rightarrow [0, 1]$$ satisfying $$E_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}$$. We now try to understand the structure of the q, g optimizing the LHS of (4).\nLet $$g^*$$ denote an optimizer of the objective. First, we show that the constraint is tight, i.e.,\n$$E_{x \\sim Dx}[g^*(x)r(x)] = \\frac{\\eta}{10}$$. Since increasing g improves the constraint, the only way this could\nnot happen is if the maximum possible function, $$g(x) = 1$$ for all x, lies in G. But for this function,\nthe LHS of (4) would be 0, which is a contradiction; hence we know increasing g to improve the\nobjective at some point hits the constraint, and hence $$E_{x \\sim Dx}[g^*(x)r(x)] = \\frac{\\eta}{10}$$.\nFor any q, define $$\\tau_q \\geq 0$$ to be the minimum threshold such that\n$$\nE_{x \\sim Dx}[r(x) \\cdot 1_{q(x)}] < \\frac{\\eta}{10}.\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "answers h. Let $$q(x) = m1Pr[A0 queries x]$$, so q is a distribution over X. Since A0 outputs a fixed\ndistribution, by the bounded ball assumption, for $$h \\sim \\lambda$$ and arbitrary adversary function g,\n$$\nPr[h \\sim \\lambda][A0 succeeds] \\leq 80\\%.\n$$\nBut A behaves identically to A0 until it sees its first nonzero y. Thus,\nand so $$99\\% \\leq Pr[A succeeds] \\leq Pr[A0 succeeds] + Pr[A sees a non-zero y]$$\n$$\nPr[A sees a non-zero y] \\geq 19\\%.\n$$\nSince A behaves like A0 until the first nonzero, we have\n$$\n19\\% \\leq Pr[A sees a non-zero y] = Pr[A0 makes a query x with y(x) = 1] \\leq E[Number queries x by A0 with y(x) = 1] = mE_{h \\sim \\lambda}E_{x \\sim q}[y(x)].\n$$\nAs an initial note, observe that $$E_{h,y}[y(x)] \\leq E_{x \\sim q}[h(x)] = r(x)$$ so\n$$\nE_{x \\sim q}[r(x)] \\geq 0.19m.\n$$\nThus the lemma statement holds for $$\\eta = 0$$.\nHandling $$\\eta > 0$$. Consider the behavior when the adversary\u2019s function $$g : X \\rightarrow [0, 1]$$ satisfies\n$$\nE_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}.\n$$\nWe denote the class of all adversary satisfying this condition as G. We have that\n$$\nE_{h \\sim \\lambda}E_{x \\sim Dx}[g(x)h(x)] \\leq E_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}.\n$$\nLet Eh denote the event that $$E_{x \\sim Dx}[g(x)h(x)] \\leq \\eta$$, so $$Pr[Eh] \\leq 10\\%$$. Furthermore, the adversary\nis designed such that under Eh, $$E_{y}[y(x)] = h(x)(1 - g(x))$$ for every x. Therefore:\n$$\n0.19 \\leq Pr[A0 makes a query x with y(x) = 1] \\leq Pr[Eh] + Pr[A0 makes a query x with y(x) = 1 \\cap Eh] \\leq 0.1 + E[Number queries x by A0 with y(x) = 1 and Eh] = 0.1 + mE_{h}1_{Eh}E_{x \\sim q}[E_{y}y(x)] = 0.1 + mE_{h}1_{Eh}E_{x \\sim q}[h(x)(1 - g(x))] \\leq 0.1 + mE_{x \\sim q}[Eh[h(x)](1 - g(x))]\n$$\nThus\n$$\n= 0.1 + mE_{x \\sim q}[r(x)(1 - g(x))].\n$$\n$$\n\\max_{q} \\min_{g \\in G} E_{x \\sim q}[r(x)(1 - g(x))] \\geq 100m\n$$\nover all distributions q and functions $$g : X \\rightarrow [0, 1]$$ satisfying $$E_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}$$. We now try to understand the structure of the q, g optimizing the LHS of (4).\nLet $$g^*$$ denote an optimizer of the objective. First, we show that the constraint is tight, i.e.,\n$$E_{x \\sim Dx}[g^*(x)r(x)] = \\frac{\\eta}{10}$$. Since increasing g improves the constraint, the only way this could\nnot happen is if the maximum possible function, $$g(x) = 1$$ for all x, lies in G. But for this function,\nthe LHS of (4) would be 0, which is a contradiction; hence we know increasing g to improve the\nobjective at some point hits the constraint, and hence $$E_{x \\sim Dx}[g^*(x)r(x)] = \\frac{\\eta}{10}$$.\nFor any q, define $$\\tau_q \\geq 0$$ to be the minimum threshold such that\n$$\nE_{x \\sim Dx}[r(x) \\cdot 1_{q(x)}] < \\frac{\\eta}{10}.\n$$", "md": "answers h. Let $$q(x) = m1Pr[A0 queries x]$$, so q is a distribution over X. Since A0 outputs a fixed\ndistribution, by the bounded ball assumption, for $$h \\sim \\lambda$$ and arbitrary adversary function g,\n$$\nPr[h \\sim \\lambda][A0 succeeds] \\leq 80\\%.\n$$\nBut A behaves identically to A0 until it sees its first nonzero y. Thus,\nand so $$99\\% \\leq Pr[A succeeds] \\leq Pr[A0 succeeds] + Pr[A sees a non-zero y]$$\n$$\nPr[A sees a non-zero y] \\geq 19\\%.\n$$\nSince A behaves like A0 until the first nonzero, we have\n$$\n19\\% \\leq Pr[A sees a non-zero y] = Pr[A0 makes a query x with y(x) = 1] \\leq E[Number queries x by A0 with y(x) = 1] = mE_{h \\sim \\lambda}E_{x \\sim q}[y(x)].\n$$\nAs an initial note, observe that $$E_{h,y}[y(x)] \\leq E_{x \\sim q}[h(x)] = r(x)$$ so\n$$\nE_{x \\sim q}[r(x)] \\geq 0.19m.\n$$\nThus the lemma statement holds for $$\\eta = 0$$.\nHandling $$\\eta > 0$$. Consider the behavior when the adversary\u2019s function $$g : X \\rightarrow [0, 1]$$ satisfies\n$$\nE_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}.\n$$\nWe denote the class of all adversary satisfying this condition as G. We have that\n$$\nE_{h \\sim \\lambda}E_{x \\sim Dx}[g(x)h(x)] \\leq E_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}.\n$$\nLet Eh denote the event that $$E_{x \\sim Dx}[g(x)h(x)] \\leq \\eta$$, so $$Pr[Eh] \\leq 10\\%$$. Furthermore, the adversary\nis designed such that under Eh, $$E_{y}[y(x)] = h(x)(1 - g(x))$$ for every x. Therefore:\n$$\n0.19 \\leq Pr[A0 makes a query x with y(x) = 1] \\leq Pr[Eh] + Pr[A0 makes a query x with y(x) = 1 \\cap Eh] \\leq 0.1 + E[Number queries x by A0 with y(x) = 1 and Eh] = 0.1 + mE_{h}1_{Eh}E_{x \\sim q}[E_{y}y(x)] = 0.1 + mE_{h}1_{Eh}E_{x \\sim q}[h(x)(1 - g(x))] \\leq 0.1 + mE_{x \\sim q}[Eh[h(x)](1 - g(x))]\n$$\nThus\n$$\n= 0.1 + mE_{x \\sim q}[r(x)(1 - g(x))].\n$$\n$$\n\\max_{q} \\min_{g \\in G} E_{x \\sim q}[r(x)(1 - g(x))] \\geq 100m\n$$\nover all distributions q and functions $$g : X \\rightarrow [0, 1]$$ satisfying $$E_{x \\sim Dx}[g(x)r(x)] \\leq \\frac{\\eta}{10}$$. We now try to understand the structure of the q, g optimizing the LHS of (4).\nLet $$g^*$$ denote an optimizer of the objective. First, we show that the constraint is tight, i.e.,\n$$E_{x \\sim Dx}[g^*(x)r(x)] = \\frac{\\eta}{10}$$. Since increasing g improves the constraint, the only way this could\nnot happen is if the maximum possible function, $$g(x) = 1$$ for all x, lies in G. But for this function,\nthe LHS of (4) would be 0, which is a contradiction; hence we know increasing g to improve the\nobjective at some point hits the constraint, and hence $$E_{x \\sim Dx}[g^*(x)r(x)] = \\frac{\\eta}{10}$$.\nFor any q, define $$\\tau_q \\geq 0$$ to be the minimum threshold such that\n$$\nE_{x \\sim Dx}[r(x) \\cdot 1_{q(x)}] < \\frac{\\eta}{10}.\n$$"}]}, {"page": 10, "text": " and define gq by                                    \uf8f1 1     q(x)\n                                                     \uf8f4      DX(x) > \u03c4q\n                                         gq(x) :=    \uf8f2 \u03b1     q(x)\n                                                     \uf8f4      DX(x) = \u03c4q\n                                                     \uf8f3 0     q(x)\n                                                            DX(x) < \u03c4q\n where \u03b1 \u2208     [0, 1] is chosen such that Ex\u223cDx[r(x)gq(x)] = \u03b7/10; such an \u03b1 always exists by the\n choice of \u03c4q.\n For any q, we claim that the optimal g\u2217      in the LHS of (4) is gq. It needs to maximize\n                                             E       q(x)\n                                           x\u223cDX     DX(x)r(x)g(x)                                         q(x)\n subject to a constraint on Ex\u223cDX[r(x)g(x)]; therefore moving mass to points of larger                   DX(x) is\n always an improvement, and gq is optimal.\n                                                             q(x)                                    q(x\u2032)\n We now claim that the q maximizing (4) has max             DX(x) = \u03c4q. If not, some x\u2032 has         DX(x\u2032) > \u03c4q.\n Then gq(x\u2032) = 1, so the x\u2032 entry contributes nothing to Ex\u223cq[r(x)(1\u2212gq(x))]; thus decreasing q(x)\n halfway towards \u03c4q (which wouldn\u2019t change gq), and adding the savings uniformly across all q(x)\n (which also doesn\u2019t change gq) would increase the objective.\n So there exists a q satisfying (4) for which Pr             q(x)           = 0, and therefore the set T =\n        q(x)                                               DX(x) > \u03c4q\n   x | DX(x) = \u03c4q      satisfies EDX [r(x)1x\u2208T ] \u2265      \u03b7/10 and a gq minimizing (4) is\n                                         gq(x) = \u03b7           1 x\u2208T\n Therefore                                         10  EDX[r(x)1x\u2208T ].\n                        E                      E       q(x)                   1x\u2208T\n                       x\u223cq[r(x)gq(x)] =      x\u223cDX     DX(x)r(x) \u03b7   10 EDX[r(x)1x\u2208T ]\n                                          = \u03b7           q(x)\n and so by (4),                      E       10 maxx   DX(x)  q(x)          9\n                                   x\u223cq[r(x)] \u2212     \u03b7    x\n as desired.                                      10 max    DX(x) \u2265      100m\n 4    Conclusion\n We have given an algorithm that solves agnostic active learning with (for constant \u03b4) at most an\n O(log |H|) factor more queries than the optimal algorithm. It is NP-hard to improve upon this\n O(log |H|) factor in general, but for specific cases it can be avoided. We have shown that 1d thresh-\n old functions, i.e. binary search with adversarial noise, is one such example where our algorithm\n matches the performance of disagreement coefficient-based methods and is within a log log 1              \u03b5 factor\n of optimal.\n 5    Acknowledgments\nYihan Zhou and Eric Price were supported by NSF awards CCF-2008868, CCF-1751040 (CA-\n REER), and the NSF AI Institute for Foundations of Machine Learning (IFML).\n References\n Mohammad Azad, Igor Chikalov, Shahid Hussain, Mikhail Moshkov, and Beata Zielosko.                            De-\n    cision Trees with Hypotheses.           Springer International Publishing, 2022.              doi:   10.1007/\n    978-3-031-08585-7. URL https://doi.org/10.1007/978-3-031-08585-7.\n                                                        10", "md": "# Math Equations and Text\n\n## Math Equations and Text\n\nDefine \\( g_q \\) by\n\n$$\ng_q(x) = \\begin{cases}\n1 & \\text{if } DX(x) > \\tau_q \\\\\n\\alpha & \\text{if } DX(x) = \\tau_q \\\\\n0 & \\text{if } DX(x) < \\tau_q\n\\end{cases}\n$$\n\nwhere \\( \\alpha \\in [0, 1] \\) is chosen such that \\( E_{x \\sim D_x}[r(x)g_q(x)] = \\frac{\\eta}{10} \\); such an \\( \\alpha \\) always exists by the choice of \\( \\tau_q \\).\n\nFor any \\( q \\), we claim that the optimal \\( g^* \\) in the LHS of (4) is \\( g_q \\). It needs to maximize\n\n$$\nE_{x \\sim D_X}[DX(x)r(x)g(x)]_{q(x)}\n$$\nsubject to a constraint on \\( E_{x \\sim D_X}[r(x)g(x)] \\); therefore moving mass to points of larger \\( DX(x) \\) is always an improvement, and \\( g_q \\) is optimal.\n\nWe now claim that the \\( q \\) maximizing (4) has max \\( DX(x) = \\tau_q \\). If not, some \\( x' \\) has \\( DX(x') > \\tau_q \\). Then \\( g_q(x') = 1 \\), so the \\( x' \\) entry contributes nothing to \\( E_{x \\sim q}[r(x)(1-g_q(x))] \\); thus decreasing \\( q(x) \\) halfway towards \\( \\tau_q \\) (which wouldn\u2019t change \\( g_q \\)), and adding the savings uniformly across all \\( q(x) \\) (which also doesn\u2019t change \\( g_q \\)) would increase the objective.\n\nSo there exists a \\( q \\) satisfying (4) for which \\( Pr_{q(x)}[DX(x) > \\tau_q] = 0 \\), and therefore the set \\( T = \\{ x | DX(x) = \\tau_q \\} \\) satisfies \\( E_{DX}[r(x)1_{x \\in T}] \\geq \\frac{\\eta}{10} \\) and a \\( g_q \\) minimizing (4) is\n\n$$\ng_q(x) = \\frac{\\eta}{10} 1_{x \\in T}\n$$\n\nTherefore\n\n$$\nE_{x \\sim q}[r(x)g_q(x)] = E_{x \\sim D_X}[DX(x)r(x) \\frac{\\eta}{10} 1_{x \\in T}]\n$$\nand so by (4),\n\n$$\nE_{x \\sim q}[10 \\max_x DX(x) q(x)] \\geq 9\n$$\nas desired.\n\n### Conclusion\n\nWe have given an algorithm that solves agnostic active learning with (for constant \\( \\delta \\)) at most an \\( O(\\log |H|) \\) factor more queries than the optimal algorithm. It is NP-hard to improve upon this \\( O(\\log |H|) \\) factor in general, but for specific cases it can be avoided. We have shown that 1d threshold functions, i.e. binary search with adversarial noise, is one such example where our algorithm matches the performance of disagreement coefficient-based methods and is within a \\( \\log \\log \\frac{1}{\\epsilon} \\) factor of optimal.\n\n### Acknowledgments\n\nYihan Zhou and Eric Price were supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML).\n\n### References\n\n1. Mohammad Azad, Igor Chikalov, Shahid Hussain, Mikhail Moshkov, and Beata Zielosko. Decision Trees with Hypotheses. Springer International Publishing, 2022. doi: 10.1007/978-3-031-08585-7. URL https://doi.org/10.1007/978-3-031-08585-7.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Math Equations and Text", "md": "## Math Equations and Text"}, {"type": "text", "value": "Define \\( g_q \\) by\n\n$$\ng_q(x) = \\begin{cases}\n1 & \\text{if } DX(x) > \\tau_q \\\\\n\\alpha & \\text{if } DX(x) = \\tau_q \\\\\n0 & \\text{if } DX(x) < \\tau_q\n\\end{cases}\n$$\n\nwhere \\( \\alpha \\in [0, 1] \\) is chosen such that \\( E_{x \\sim D_x}[r(x)g_q(x)] = \\frac{\\eta}{10} \\); such an \\( \\alpha \\) always exists by the choice of \\( \\tau_q \\).\n\nFor any \\( q \\), we claim that the optimal \\( g^* \\) in the LHS of (4) is \\( g_q \\). It needs to maximize\n\n$$\nE_{x \\sim D_X}[DX(x)r(x)g(x)]_{q(x)}\n$$\nsubject to a constraint on \\( E_{x \\sim D_X}[r(x)g(x)] \\); therefore moving mass to points of larger \\( DX(x) \\) is always an improvement, and \\( g_q \\) is optimal.\n\nWe now claim that the \\( q \\) maximizing (4) has max \\( DX(x) = \\tau_q \\). If not, some \\( x' \\) has \\( DX(x') > \\tau_q \\). Then \\( g_q(x') = 1 \\), so the \\( x' \\) entry contributes nothing to \\( E_{x \\sim q}[r(x)(1-g_q(x))] \\); thus decreasing \\( q(x) \\) halfway towards \\( \\tau_q \\) (which wouldn\u2019t change \\( g_q \\)), and adding the savings uniformly across all \\( q(x) \\) (which also doesn\u2019t change \\( g_q \\)) would increase the objective.\n\nSo there exists a \\( q \\) satisfying (4) for which \\( Pr_{q(x)}[DX(x) > \\tau_q] = 0 \\), and therefore the set \\( T = \\{ x | DX(x) = \\tau_q \\} \\) satisfies \\( E_{DX}[r(x)1_{x \\in T}] \\geq \\frac{\\eta}{10} \\) and a \\( g_q \\) minimizing (4) is\n\n$$\ng_q(x) = \\frac{\\eta}{10} 1_{x \\in T}\n$$\n\nTherefore\n\n$$\nE_{x \\sim q}[r(x)g_q(x)] = E_{x \\sim D_X}[DX(x)r(x) \\frac{\\eta}{10} 1_{x \\in T}]\n$$\nand so by (4),\n\n$$\nE_{x \\sim q}[10 \\max_x DX(x) q(x)] \\geq 9\n$$\nas desired.", "md": "Define \\( g_q \\) by\n\n$$\ng_q(x) = \\begin{cases}\n1 & \\text{if } DX(x) > \\tau_q \\\\\n\\alpha & \\text{if } DX(x) = \\tau_q \\\\\n0 & \\text{if } DX(x) < \\tau_q\n\\end{cases}\n$$\n\nwhere \\( \\alpha \\in [0, 1] \\) is chosen such that \\( E_{x \\sim D_x}[r(x)g_q(x)] = \\frac{\\eta}{10} \\); such an \\( \\alpha \\) always exists by the choice of \\( \\tau_q \\).\n\nFor any \\( q \\), we claim that the optimal \\( g^* \\) in the LHS of (4) is \\( g_q \\). It needs to maximize\n\n$$\nE_{x \\sim D_X}[DX(x)r(x)g(x)]_{q(x)}\n$$\nsubject to a constraint on \\( E_{x \\sim D_X}[r(x)g(x)] \\); therefore moving mass to points of larger \\( DX(x) \\) is always an improvement, and \\( g_q \\) is optimal.\n\nWe now claim that the \\( q \\) maximizing (4) has max \\( DX(x) = \\tau_q \\). If not, some \\( x' \\) has \\( DX(x') > \\tau_q \\). Then \\( g_q(x') = 1 \\), so the \\( x' \\) entry contributes nothing to \\( E_{x \\sim q}[r(x)(1-g_q(x))] \\); thus decreasing \\( q(x) \\) halfway towards \\( \\tau_q \\) (which wouldn\u2019t change \\( g_q \\)), and adding the savings uniformly across all \\( q(x) \\) (which also doesn\u2019t change \\( g_q \\)) would increase the objective.\n\nSo there exists a \\( q \\) satisfying (4) for which \\( Pr_{q(x)}[DX(x) > \\tau_q] = 0 \\), and therefore the set \\( T = \\{ x | DX(x) = \\tau_q \\} \\) satisfies \\( E_{DX}[r(x)1_{x \\in T}] \\geq \\frac{\\eta}{10} \\) and a \\( g_q \\) minimizing (4) is\n\n$$\ng_q(x) = \\frac{\\eta}{10} 1_{x \\in T}\n$$\n\nTherefore\n\n$$\nE_{x \\sim q}[r(x)g_q(x)] = E_{x \\sim D_X}[DX(x)r(x) \\frac{\\eta}{10} 1_{x \\in T}]\n$$\nand so by (4),\n\n$$\nE_{x \\sim q}[10 \\max_x DX(x) q(x)] \\geq 9\n$$\nas desired."}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "We have given an algorithm that solves agnostic active learning with (for constant \\( \\delta \\)) at most an \\( O(\\log |H|) \\) factor more queries than the optimal algorithm. It is NP-hard to improve upon this \\( O(\\log |H|) \\) factor in general, but for specific cases it can be avoided. We have shown that 1d threshold functions, i.e. binary search with adversarial noise, is one such example where our algorithm matches the performance of disagreement coefficient-based methods and is within a \\( \\log \\log \\frac{1}{\\epsilon} \\) factor of optimal.", "md": "We have given an algorithm that solves agnostic active learning with (for constant \\( \\delta \\)) at most an \\( O(\\log |H|) \\) factor more queries than the optimal algorithm. It is NP-hard to improve upon this \\( O(\\log |H|) \\) factor in general, but for specific cases it can be avoided. We have shown that 1d threshold functions, i.e. binary search with adversarial noise, is one such example where our algorithm matches the performance of disagreement coefficient-based methods and is within a \\( \\log \\log \\frac{1}{\\epsilon} \\) factor of optimal."}, {"type": "heading", "lvl": 3, "value": "Acknowledgments", "md": "### Acknowledgments"}, {"type": "text", "value": "Yihan Zhou and Eric Price were supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML).", "md": "Yihan Zhou and Eric Price were supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML)."}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "text", "value": "1. Mohammad Azad, Igor Chikalov, Shahid Hussain, Mikhail Moshkov, and Beata Zielosko. Decision Trees with Hypotheses. Springer International Publishing, 2022. doi: 10.1007/978-3-031-08585-7. URL https://doi.org/10.1007/978-3-031-08585-7.", "md": "1. Mohammad Azad, Igor Chikalov, Shahid Hussain, Mikhail Moshkov, and Beata Zielosko. Decision Trees with Hypotheses. Springer International Publishing, 2022. doi: 10.1007/978-3-031-08585-7. URL https://doi.org/10.1007/978-3-031-08585-7."}]}, {"page": 11, "text": "Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Pro-\n   ceedings of the 23rd international conference on Machine learning, pages 65\u201372, 2006.\nMichael Ben-Or and Avinatan Hassidim. The bayesian learner is optimal for noisy binary search\n  (and pretty good for quantum as well). In 2008 49th Annual IEEE Symposium on Foundations of\n  Computer Science, pages 221\u2013230, 2008. doi: 10.1109/FOCS.2008.58.\nAlina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In\n  Proceedings of the 26th annual international conference on machine learning, pages 49\u201356, 2009.\nAlina Beygelzimer, Daniel J Hsu, John Langford, and Tong Zhang. Agnostic active learning without\n   constraints. Advances in neural information processing systems, 23, 2010.\nMarat Valievich Burnashev and Kamil\u2019Shamil\u2019evich Zigangirov. An interval estimation problem for\n   controlled observations. Problemy Peredachi Informatsii, 10(3):51\u201361, 1974.\nDavid Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine\n   learning, 15:201\u2013221, 1994.\nSanjoy Dasgupta. Analysis of a greedy active learning strategy. Advances in neural information\n   processing systems, 17, 2004.\nSanjoy Dasgupta. Coarse sample complexity bounds for active learning. Advances in neural infor-\n   mation processing systems, 18, 2005.\nSanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm.\n  Advances in neural information processing systems, 20, 2007.\nDariusz    Dereniowski,      Aleksander      Lukasiewicz,     and    Przemyslaw      Uznanski.          Noisy\n   searching:      simple,    fast   and    correct.        CoRR,     abs/2107.05753,      2021.         URL\n   https://arxiv.org/abs/2107.05753.\nIrit Dinur and David Steurer. Analytical approach to parallel repetition. In Proceedings of the\n   forty-sixth annual ACM symposium on Theory of computing, pages 624\u2013633, 2014.\nSteve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the\n  24th international conference on Machine learning, pages 353\u2013360, 2007a.\nSteve Hanneke. Teaching dimension and the complexity of active learning. In International confer-\n   ence on computational learning theory, pages 66\u201381. Springer, 2007b.\nSteve Hanneke. Theory of disagreement-based active learning. Foundations and Trends\u00ae in Ma-\n   chine Learning, 7(2-3):131\u2013309, 2014.\nSteve Hanneke and Liu Yang. Minimax analysis of active learning. J. Mach. Learn. Res., 16(1):\n   3487\u20133602, 2015.\nTibor Heged\u02dd  us. Generalized teaching dimensions and the query complexity of learning. In Proceed-\n   ings of the eighth annual conference on Computational learning theory, pages 108\u2013117, 1995.\nMatti K\u00e4\u00e4ri\u00e4inen. Active learning in the non-realizable case. In Algorithmic Learning Theory:\n  17th International Conference, ALT 2006, Barcelona, Spain, October 7-10, 2006. Proceedings\n  17, pages 63\u201377. Springer, 2006.\nRichard M. Karp and Robert Kleinberg. Noisy binary search and its applications. In Proceedings of\n   the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201907, page 881\u2013890,\n   USA, 2007. Society for Industrial and Applied Mathematics. ISBN 9780898716245.\nJulian Katz-Samuels, Jifan Zhang, Lalit Jain, and Kevin Jamieson. Improved algorithms for agnostic\n   pool-based active classification. In International Conference on Machine Learning, pages 5334\u2013\n   5344. PMLR, 2021.\nS Rao Kosaraju, Teresa M Przytycka, and Ryan Borgstrom.                 On an optimal split tree problem.\n   In Algorithms and Data Structures: 6th International Workshop, WADS\u201999 Vancouver, Canada,\n  August 11\u201314, 1999 Proceedings, pages 157\u2013168. Springer, 2002.\n                                                      11", "md": "# List of References\n\n# List of References\n\n- Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning, pages 65\u201372, 2006.\n- Michael Ben-Or and Avinatan Hassidim. The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well). In 2008 49th Annual IEEE Symposium on Foundations of Computer Science, pages 221\u2013230, 2008. doi: 10.1109/FOCS.2008.58.\n- Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In Proceedings of the 26th annual international conference on machine learning, pages 49\u201356, 2009.\n- Alina Beygelzimer, Daniel J Hsu, John Langford, and Tong Zhang. Agnostic active learning without constraints. Advances in neural information processing systems, 23, 2010.\n- Marat Valievich Burnashev and Kamil\u2019Shamil\u2019evich Zigangirov. An interval estimation problem for controlled observations. Problemy Peredachi Informatsii, 10(3):51\u201361, 1974.\n- David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine learning, 15:201\u2013221, 1994.\n- Sanjoy Dasgupta. Analysis of a greedy active learning strategy. Advances in neural information processing systems, 17, 2004.\n- Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. Advances in neural information processing systems, 18, 2005.\n- Sanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm. Advances in neural information processing systems, 20, 2007.\n- Dariusz Dereniowski, Aleksander Lukasiewicz, and Przemyslaw Uznanski. Noisy searching: simple, fast and correct. CoRR, abs/2107.05753, 2021. URL https://arxiv.org/abs/2107.05753.\n- Irit Dinur and David Steurer. Analytical approach to parallel repetition. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 624\u2013633, 2014.\n- Steve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th international conference on Machine learning, pages 353\u2013360, 2007a.\n- Steve Hanneke. Teaching dimension and the complexity of active learning. In International conference on computational learning theory, pages 66\u201381. Springer, 2007b.\n- Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends\u00ae in Machine Learning, 7(2-3):131\u2013309, 2014.\n- Steve Hanneke and Liu Yang. Minimax analysis of active learning. J. Mach. Learn. Res., 16(1):3487\u20133602, 2015.\n- Tibor Heged\u02ddus. Generalized teaching dimensions and the query complexity of learning. In Proceedings of the eighth annual conference on Computational learning theory, pages 108\u2013117, 1995.\n- Matti K\u00e4\u00e4ri\u00e4inen. Active learning in the non-realizable case. In Algorithmic Learning Theory: 17th International Conference, ALT 2006, Barcelona, Spain, October 7-10, 2006. Proceedings 17, pages 63\u201377. Springer, 2006.\n- Richard M. Karp and Robert Kleinberg. Noisy binary search and its applications. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201907, page 881\u2013890, USA, 2007. Society for Industrial and Applied Mathematics. ISBN 9780898716245.\n- Julian Katz-Samuels, Jifan Zhang, Lalit Jain, and Kevin Jamieson. Improved algorithms for agnostic pool-based active classification. In International Conference on Machine Learning, pages 5334\u20135344. PMLR, 2021.\n- S Rao Kosaraju, Teresa M Przytycka, and Ryan Borgstrom. On an optimal split tree problem. In Algorithms and Data Structures: 6th International Workshop, WADS\u201999 Vancouver, Canada, August 11\u201314, 1999 Proceedings, pages 157\u2013168. Springer, 2002.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "- Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning, pages 65\u201372, 2006.\n- Michael Ben-Or and Avinatan Hassidim. The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well). In 2008 49th Annual IEEE Symposium on Foundations of Computer Science, pages 221\u2013230, 2008. doi: 10.1109/FOCS.2008.58.\n- Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In Proceedings of the 26th annual international conference on machine learning, pages 49\u201356, 2009.\n- Alina Beygelzimer, Daniel J Hsu, John Langford, and Tong Zhang. Agnostic active learning without constraints. Advances in neural information processing systems, 23, 2010.\n- Marat Valievich Burnashev and Kamil\u2019Shamil\u2019evich Zigangirov. An interval estimation problem for controlled observations. Problemy Peredachi Informatsii, 10(3):51\u201361, 1974.\n- David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine learning, 15:201\u2013221, 1994.\n- Sanjoy Dasgupta. Analysis of a greedy active learning strategy. Advances in neural information processing systems, 17, 2004.\n- Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. Advances in neural information processing systems, 18, 2005.\n- Sanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm. Advances in neural information processing systems, 20, 2007.\n- Dariusz Dereniowski, Aleksander Lukasiewicz, and Przemyslaw Uznanski. Noisy searching: simple, fast and correct. CoRR, abs/2107.05753, 2021. URL https://arxiv.org/abs/2107.05753.\n- Irit Dinur and David Steurer. Analytical approach to parallel repetition. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 624\u2013633, 2014.\n- Steve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th international conference on Machine learning, pages 353\u2013360, 2007a.\n- Steve Hanneke. Teaching dimension and the complexity of active learning. In International conference on computational learning theory, pages 66\u201381. Springer, 2007b.\n- Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends\u00ae in Machine Learning, 7(2-3):131\u2013309, 2014.\n- Steve Hanneke and Liu Yang. Minimax analysis of active learning. J. Mach. Learn. Res., 16(1):3487\u20133602, 2015.\n- Tibor Heged\u02ddus. Generalized teaching dimensions and the query complexity of learning. In Proceedings of the eighth annual conference on Computational learning theory, pages 108\u2013117, 1995.\n- Matti K\u00e4\u00e4ri\u00e4inen. Active learning in the non-realizable case. In Algorithmic Learning Theory: 17th International Conference, ALT 2006, Barcelona, Spain, October 7-10, 2006. Proceedings 17, pages 63\u201377. Springer, 2006.\n- Richard M. Karp and Robert Kleinberg. Noisy binary search and its applications. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201907, page 881\u2013890, USA, 2007. Society for Industrial and Applied Mathematics. ISBN 9780898716245.\n- Julian Katz-Samuels, Jifan Zhang, Lalit Jain, and Kevin Jamieson. Improved algorithms for agnostic pool-based active classification. In International Conference on Machine Learning, pages 5334\u20135344. PMLR, 2021.\n- S Rao Kosaraju, Teresa M Przytycka, and Ryan Borgstrom. On an optimal split tree problem. In Algorithms and Data Structures: 6th International Workshop, WADS\u201999 Vancouver, Canada, August 11\u201314, 1999 Proceedings, pages 157\u2013168. Springer, 2002.", "md": "- Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning, pages 65\u201372, 2006.\n- Michael Ben-Or and Avinatan Hassidim. The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well). In 2008 49th Annual IEEE Symposium on Foundations of Computer Science, pages 221\u2013230, 2008. doi: 10.1109/FOCS.2008.58.\n- Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In Proceedings of the 26th annual international conference on machine learning, pages 49\u201356, 2009.\n- Alina Beygelzimer, Daniel J Hsu, John Langford, and Tong Zhang. Agnostic active learning without constraints. Advances in neural information processing systems, 23, 2010.\n- Marat Valievich Burnashev and Kamil\u2019Shamil\u2019evich Zigangirov. An interval estimation problem for controlled observations. Problemy Peredachi Informatsii, 10(3):51\u201361, 1974.\n- David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine learning, 15:201\u2013221, 1994.\n- Sanjoy Dasgupta. Analysis of a greedy active learning strategy. Advances in neural information processing systems, 17, 2004.\n- Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. Advances in neural information processing systems, 18, 2005.\n- Sanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm. Advances in neural information processing systems, 20, 2007.\n- Dariusz Dereniowski, Aleksander Lukasiewicz, and Przemyslaw Uznanski. Noisy searching: simple, fast and correct. CoRR, abs/2107.05753, 2021. URL https://arxiv.org/abs/2107.05753.\n- Irit Dinur and David Steurer. Analytical approach to parallel repetition. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 624\u2013633, 2014.\n- Steve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th international conference on Machine learning, pages 353\u2013360, 2007a.\n- Steve Hanneke. Teaching dimension and the complexity of active learning. In International conference on computational learning theory, pages 66\u201381. Springer, 2007b.\n- Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends\u00ae in Machine Learning, 7(2-3):131\u2013309, 2014.\n- Steve Hanneke and Liu Yang. Minimax analysis of active learning. J. Mach. Learn. Res., 16(1):3487\u20133602, 2015.\n- Tibor Heged\u02ddus. Generalized teaching dimensions and the query complexity of learning. In Proceedings of the eighth annual conference on Computational learning theory, pages 108\u2013117, 1995.\n- Matti K\u00e4\u00e4ri\u00e4inen. Active learning in the non-realizable case. In Algorithmic Learning Theory: 17th International Conference, ALT 2006, Barcelona, Spain, October 7-10, 2006. Proceedings 17, pages 63\u201377. Springer, 2006.\n- Richard M. Karp and Robert Kleinberg. Noisy binary search and its applications. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201907, page 881\u2013890, USA, 2007. Society for Industrial and Applied Mathematics. ISBN 9780898716245.\n- Julian Katz-Samuels, Jifan Zhang, Lalit Jain, and Kevin Jamieson. Improved algorithms for agnostic pool-based active classification. In International Conference on Machine Learning, pages 5334\u20135344. PMLR, 2021.\n- S Rao Kosaraju, Teresa M Przytycka, and Ryan Borgstrom. On an optimal split tree problem. In Algorithms and Data Structures: 6th International Workshop, WADS\u201999 Vancouver, Canada, August 11\u201314, 1999 Proceedings, pages 157\u2013168. Springer, 2002."}]}, {"page": 12, "text": "Hyafil Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete.\n   Information processing letters, 5(1):15\u201317, 1976.\nDavid D Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional\n   data. In Acm Sigir Forum, volume 29, pages 13\u201319. ACM New York, NY, USA, 1995.\nDavid D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In\n   Machine learning proceedings 1994, pages 148\u2013156. Elsevier, 1994.\nRobert Nowak. Generalized binary search. In 2008 46th Annual Allerton Conference on Communi-\n   cation, Control, and Computing, pages 568\u2013574. IEEE, 2008.\nRobert D Nowak. The geometry of generalized binary search. IEEE Transactions on Information\n   Theory, 57(12):7893\u20137906, 2011.\nBurr Settles. Active learning literature survey. Computer Sciences Technical Report 1648, Univer-\n   sity of Wisconsin\u2013Madison, 2009.\nJoel Tropp. Freedman\u2019s inequality for matrix martingales. 2011.\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science,\n   volume 47. Cambridge university press, 2018.\n                                                12", "md": "# References\n\n# References\n\n- Hyafil Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete. Information processing letters, 5(1):15\u201317, 1976.\n- David D Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data. In Acm Sigir Forum, volume 29, pages 13\u201319. ACM New York, NY, USA, 1995.\n- David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148\u2013156. Elsevier, 1994.\n- Robert Nowak. Generalized binary search. In 2008 46th Annual Allerton Conference on Communication, Control, and Computing, pages 568\u2013574. IEEE, 2008.\n- Robert D Nowak. The geometry of generalized binary search. IEEE Transactions on Information Theory, 57(12):7893\u20137906, 2011.\n- Burr Settles. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin\u2013Madison, 2009.\n- Joel Tropp. Freedman\u2019s inequality for matrix martingales. 2011.\n- Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- Hyafil Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete. Information processing letters, 5(1):15\u201317, 1976.\n- David D Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data. In Acm Sigir Forum, volume 29, pages 13\u201319. ACM New York, NY, USA, 1995.\n- David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148\u2013156. Elsevier, 1994.\n- Robert Nowak. Generalized binary search. In 2008 46th Annual Allerton Conference on Communication, Control, and Computing, pages 568\u2013574. IEEE, 2008.\n- Robert D Nowak. The geometry of generalized binary search. IEEE Transactions on Information Theory, 57(12):7893\u20137906, 2011.\n- Burr Settles. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin\u2013Madison, 2009.\n- Joel Tropp. Freedman\u2019s inequality for matrix martingales. 2011.\n- Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.", "md": "- Hyafil Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete. Information processing letters, 5(1):15\u201317, 1976.\n- David D Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data. In Acm Sigir Forum, volume 29, pages 13\u201319. ACM New York, NY, USA, 1995.\n- David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148\u2013156. Elsevier, 1994.\n- Robert Nowak. Generalized binary search. In 2008 46th Annual Allerton Conference on Communication, Control, and Computing, pages 568\u2013574. IEEE, 2008.\n- Robert D Nowak. The geometry of generalized binary search. IEEE Transactions on Information Theory, 57(12):7893\u20137906, 2011.\n- Burr Settles. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin\u2013Madison, 2009.\n- Joel Tropp. Freedman\u2019s inequality for matrix martingales. 2011.\n- Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018."}]}, {"page": 13, "text": " A      Query Complexity Upper Bound\n In this section we present the whole proof of the query complexity upper bound of Algorithm 1, as\n stated in Theorem 1.1.\n A.1      Notation\nWe remind the readers about some definitions first. Remember that wi(h) denote the weight of\n                                                                   wi(h)\n hypothesis h in iteration i and \u03bbi,S(h) =                       h\u2032\u2208S wi(h\u2032) for some S \u2286             H denote the proportion of h\n in S. We view \u03bbi,S as a distribution of hypotheses in S so for h /                          \u2208   S, \u03bbi,S(h) = 0. For a set S \u2286               H\n of hypotheses, we define wi(S) :=  h\u2208S w(h) and \u03bbi(h) = \u03bbi,H(h).\n Define r\u03bb,h\u2217(x) := Prh\u223c\u03bb[h(x) \u0338= h\u2217(x)], and r\u03bb(x) = miny\u2208{0,1} Prh\u223c\u03bb[h(x) \u0338= y], so r\u03bb(x) =\n min(r\u03bb,h\u2217(x), 1 \u2212          r\u03bb,h\u2217(x)).\n Define             \u03bbi,S(h) := 1                                           1  2\u03bbi(h)     1\u22121 2 Prh\u223c\u03bbi[h\u2208S]         h \u2208    S                 (5)\n                                     2\u03bbi(h) + 1     2\u03bbi,H\\S(h) =             \u03bbi(h) \u00b7      1\u2212Prh\u223c\u03bbi[h\u2208S]            h / \u2208  S\n as the \"capped\" distribution in iteration i.\n Finally, for notational convenience define ri,S := r\u03bb                      i,S, ri,S,h := r\u03bbi,S,h and ri,S := r             \u03bbi,S.\n The main focus of our proof would be analyzing the potential function\n                                  \u03d5i(h\u2217) =        log \u03bbi(h\u2217) + log \u03bbi,H\\Si(h\u2217)                    h\u2217  \u2208/  Si\n where h\u2217      is the best hypothesis in H. We would0like to show that \u03d5i+1(h\u2217) \u2212                 h\u2217  \u2208   Si,         \u03d5i(h\u2217) is growing\n at a proper rate in each iteration. We pick Si to be an expanding series of sets, i.e., Si \u2286                                       Si+1 for\n any i \u2265     1. However, the change of the \"capped\" set Si makes this task challenging. Therefore, we\n instead analyze the following quantity defined as\n                                                   log \u03bbi+1(h\u2217)               \u03bbi+1,H\\Si(h\u2217)          h\u2217  \u2208/  Si\n                               \u2206i(h\u2217) :=           0      \u03bbi(h\u2217) + log          \u03bbi,H\\S  i(h\u2217)        h\u2217  \u2208   Si,\n and \u03d5i+1(h\u2217)\u2212\u03d5i(h\u2217) = \u2206i(h\u2217)+log                          \u03bb\u03bbi+1,H\\Si+1(h\u2217)              \u2208   Si+1. Further, we define \u03c8k(h\u2217) :=\n                                                              i+1,H\\Si(h\u2217) if h\u2217          /            \u03bbi+1,H\\Si+1(h\u2217)               \u2208   Si+1.\n    i<k \u2206i(h\u2217) so by definition \u03d5k(h\u2217) = \u03d50(h\u2217)+\u03c8k(h\u2217)+ i<k log                                          \u03bbi+1,H\\Si(h\u2217) if h\u2217          /\n In the following text, we will drop the parameter h\u2217                         when the context is clear and just use \u03d5i, \u2206i\n and \u03c8i instead.\n A.2      Potential Growth\nWe will lower bound the conditional per iteration potential increase by first introducing a lemma that\n relates the potential change to the optimization problem (3).\n Lemma A.1. Assume that err(h\u2217) \u2264                        \u03b7, then for any set S of hypotheses containing h\u2217                       and query\n distribution q, we have\n                     E    log \u03bbi+1,S(h\u2217)                \u2265   0.9\u03b1        E                                       q(x)\n for \u03b1 \u2264     0.2. Moreover,       \u03bbi,S(h\u2217)        Fi                  x\u223cq[ri,S,h(x)] \u2212         2.3\u03b7 max  x    DX(x)\n                                E    max       0, log \u03bbi+1,S(h\u2217)             Fi     \u2264  \u03b1 Ex\u223cq[ri,S,h\u2217(x)].\n                                                          \u03bbi,S(h\u2217)\n Proof. For notational convenience, define                    r(x) := ri,S,h\u2217(x).\n Observe that    \u03bbi,S(h\u2217)              wi(h\u2217)           h\u2208S wi+1,S(h)          =     wi(h\u2217)           E      wi+1,S(h)           .\n                \u03bbi+1,S(h\u2217) =          wi+1(h\u2217)            h\u2208S wi,S(h)              wi+1(h\u2217)       h\u223c\u03bbi,S        wi,S(h)\n                                                                       13", "md": "## Query Complexity Upper Bound\n\nIn this section we present the whole proof of the query complexity upper bound of Algorithm 1, as stated in Theorem 1.1.\n\n### Notation\n\nWe remind the readers about some definitions first. Remember that $$w_i(h)$$ denote the weight of hypothesis h in iteration i and $$\\lambda_{i,S}(h) = \\frac{\\sum_{h' \\in S} w_i(h')}{\\text{for some } S \\subseteq H}$$ denote the proportion of h in S. We view $$\\lambda_{i,S}$$ as a distribution of hypotheses in S so for $$h \\notin S$$, $$\\lambda_{i,S}(h) = 0$$. For a set $$S \\subseteq H$$ of hypotheses, we define $$w_i(S) := \\sum_{h \\in S} w(h)$$ and $$\\lambda_i(h) = \\lambda_{i,H}(h)$$.\n\nDefine $$r_{\\lambda,h^*}(x) := \\Pr_{h \\sim \\lambda}[h(x) \\neq h^*(x)]$$, and $$r_{\\lambda}(x) = \\min_{y \\in \\{0,1\\}} \\Pr_{h \\sim \\lambda}[h(x) \\neq y]$$, so $$r_{\\lambda}(x) = \\min(r_{\\lambda,h^*}(x), 1 - r_{\\lambda,h^*}(x))$$.\n\nDefine\n\n$$\n\\lambda_{i,S}(h) := \\frac{1}{2\\lambda_i(h) + 1} \\left(1 - \\frac{1}{2} \\Pr_{h \\sim \\lambda_i}[h \\in S \\mid h \\in S]\\right) \\text{ for } h \\in S \\quad (5)\n$$\n$$\n\\lambda_{i,H\\setminus S}(h) = \\lambda_i(h) \\cdot \\left(1 - \\Pr_{h \\sim \\lambda_i}[h \\in S] \\mid h \\notin S\\right)\n$$\nas the \"capped\" distribution in iteration i.\n\nFinally, for notational convenience define $$r_{i,S} := r_{\\lambda_i,S}$$, $$r_{i,S,h} := r_{\\lambda_i,S,h}$$ and $$r_{i,S} := r_{\\lambda_i,S}$$.\n\nThe main focus of our proof would be analyzing the potential function\n\n$$\n\\phi_i(h^*) = \\sum \\log \\lambda_i(h^*) + \\log \\lambda_{i,H\\setminus S_i}(h^*) \\text{ for } h^* \\in S_i\n$$\nwhere $$h^*$$ is the best hypothesis in H. We would like to show that $$\\phi_{i+1}(h^*) - \\phi_i(h^*)$$ is growing at a proper rate in each iteration. We pick $$S_i$$ to be an expanding series of sets, i.e., $$S_i \\subseteq S_{i+1}$$ for any $$i \\geq 1$$. However, the change of the \"capped\" set $$S_i$$ makes this task challenging. Therefore, we instead analyze the following quantity defined as\n\n$$\n\\Delta_i(h^*) := \\begin{cases} \\log \\lambda_{i+1}(h^*) & h^* \\in S_i \\\\ 0 & h^* \\notin S_i \\end{cases}\n$$\nand $$\\phi_{i+1}(h^*) - \\phi_i(h^*) = \\Delta_i(h^*) + \\log \\lambda_{i+1,H\\setminus S_{i+1}}$$ for $$h^* \\in S_{i+1}$$. Further, we define\n\n$$\n\\psi_k(h^*) := \\begin{cases} \\lambda_{i+1,H\\setminus S_i}(h^*) & \\text{if } h^* \\notin \\lambda_{i+1,H\\setminus S_{i+1}} \\text{ for } i < k \\\\ \\Delta_i(h^*) & \\text{so by definition } \\phi_k(h^*) = \\phi_0(h^*) + \\psi_k(h^*) + \\sum_{i < k} \\log \\lambda_{i+1,H\\setminus S_i}(h^*) \\text{ if } h^* \\notin \\lambda_{i+1,H\\setminus S_{i+1}} \\end{cases}\n$$\n\nIn the following text, we will drop the parameter $$h^*$$ when the context is clear and just use $$\\phi_i$$, $$\\Delta_i$$ and $$\\psi_i$$ instead.\n\n### Potential Growth\n\nWe will lower bound the conditional per iteration potential increase by first introducing a lemma that relates the potential change to the optimization problem (3).\n\nLemma A.1. Assume that $$\\text{err}(h^*) \\leq \\eta$$, then for any set S of hypotheses containing $$h^*$$ and query distribution q, we have\n\n$$\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)] \\geq 0.9\\alpha \\mathbb{E}[q(x)]\n$$\nfor $$\\alpha \\leq 0.2$$. Moreover,\n\n$$\n\\frac{\\lambda_{i,S}(h^*)}{\\mathbb{E}[\\max(0, \\log \\lambda_{i+1,S}(h^*))]} \\leq \\alpha \\mathbb{E}[x \\sim q][r_{i,S,h^*(x)}]\n$$\n\n$$\n\\mathbb{E}[\\max(0, \\log \\lambda_{i+1,S}(h^*))] \\leq \\alpha \\mathbb{E}[x \\sim q][r_{i,S,h^*(x)}]\n$$\nProof. For notational convenience, define $$r(x) := r_{i,S,h^*(x)}$$.\n\nObserve that\n\n$$\n\\lambda_{i,S}(h^*) = \\frac{\\sum_{h \\in S} w_i(h^*)}{\\sum_{h \\in S} w_i(h)} = \\frac{w_i(h^*)}{\\mathbb{E}[w_{i,S}(h)]}\n$$\n$$\n\\lambda_{i+1,S}(h^*) = \\frac{w_{i+1}(h^*)}{\\sum_{h \\sim \\lambda_{i,S}} w_{i,S}(h)} = \\frac{w_{i+1}(h^*)}{\\mathbb{E}[w_{i,S}(h)]}\n$$", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Query Complexity Upper Bound", "md": "## Query Complexity Upper Bound"}, {"type": "text", "value": "In this section we present the whole proof of the query complexity upper bound of Algorithm 1, as stated in Theorem 1.1.", "md": "In this section we present the whole proof of the query complexity upper bound of Algorithm 1, as stated in Theorem 1.1."}, {"type": "heading", "lvl": 3, "value": "Notation", "md": "### Notation"}, {"type": "text", "value": "We remind the readers about some definitions first. Remember that $$w_i(h)$$ denote the weight of hypothesis h in iteration i and $$\\lambda_{i,S}(h) = \\frac{\\sum_{h' \\in S} w_i(h')}{\\text{for some } S \\subseteq H}$$ denote the proportion of h in S. We view $$\\lambda_{i,S}$$ as a distribution of hypotheses in S so for $$h \\notin S$$, $$\\lambda_{i,S}(h) = 0$$. For a set $$S \\subseteq H$$ of hypotheses, we define $$w_i(S) := \\sum_{h \\in S} w(h)$$ and $$\\lambda_i(h) = \\lambda_{i,H}(h)$$.\n\nDefine $$r_{\\lambda,h^*}(x) := \\Pr_{h \\sim \\lambda}[h(x) \\neq h^*(x)]$$, and $$r_{\\lambda}(x) = \\min_{y \\in \\{0,1\\}} \\Pr_{h \\sim \\lambda}[h(x) \\neq y]$$, so $$r_{\\lambda}(x) = \\min(r_{\\lambda,h^*}(x), 1 - r_{\\lambda,h^*}(x))$$.\n\nDefine\n\n$$\n\\lambda_{i,S}(h) := \\frac{1}{2\\lambda_i(h) + 1} \\left(1 - \\frac{1}{2} \\Pr_{h \\sim \\lambda_i}[h \\in S \\mid h \\in S]\\right) \\text{ for } h \\in S \\quad (5)\n$$\n$$\n\\lambda_{i,H\\setminus S}(h) = \\lambda_i(h) \\cdot \\left(1 - \\Pr_{h \\sim \\lambda_i}[h \\in S] \\mid h \\notin S\\right)\n$$\nas the \"capped\" distribution in iteration i.\n\nFinally, for notational convenience define $$r_{i,S} := r_{\\lambda_i,S}$$, $$r_{i,S,h} := r_{\\lambda_i,S,h}$$ and $$r_{i,S} := r_{\\lambda_i,S}$$.\n\nThe main focus of our proof would be analyzing the potential function\n\n$$\n\\phi_i(h^*) = \\sum \\log \\lambda_i(h^*) + \\log \\lambda_{i,H\\setminus S_i}(h^*) \\text{ for } h^* \\in S_i\n$$\nwhere $$h^*$$ is the best hypothesis in H. We would like to show that $$\\phi_{i+1}(h^*) - \\phi_i(h^*)$$ is growing at a proper rate in each iteration. We pick $$S_i$$ to be an expanding series of sets, i.e., $$S_i \\subseteq S_{i+1}$$ for any $$i \\geq 1$$. However, the change of the \"capped\" set $$S_i$$ makes this task challenging. Therefore, we instead analyze the following quantity defined as\n\n$$\n\\Delta_i(h^*) := \\begin{cases} \\log \\lambda_{i+1}(h^*) & h^* \\in S_i \\\\ 0 & h^* \\notin S_i \\end{cases}\n$$\nand $$\\phi_{i+1}(h^*) - \\phi_i(h^*) = \\Delta_i(h^*) + \\log \\lambda_{i+1,H\\setminus S_{i+1}}$$ for $$h^* \\in S_{i+1}$$. Further, we define\n\n$$\n\\psi_k(h^*) := \\begin{cases} \\lambda_{i+1,H\\setminus S_i}(h^*) & \\text{if } h^* \\notin \\lambda_{i+1,H\\setminus S_{i+1}} \\text{ for } i < k \\\\ \\Delta_i(h^*) & \\text{so by definition } \\phi_k(h^*) = \\phi_0(h^*) + \\psi_k(h^*) + \\sum_{i < k} \\log \\lambda_{i+1,H\\setminus S_i}(h^*) \\text{ if } h^* \\notin \\lambda_{i+1,H\\setminus S_{i+1}} \\end{cases}\n$$\n\nIn the following text, we will drop the parameter $$h^*$$ when the context is clear and just use $$\\phi_i$$, $$\\Delta_i$$ and $$\\psi_i$$ instead.", "md": "We remind the readers about some definitions first. Remember that $$w_i(h)$$ denote the weight of hypothesis h in iteration i and $$\\lambda_{i,S}(h) = \\frac{\\sum_{h' \\in S} w_i(h')}{\\text{for some } S \\subseteq H}$$ denote the proportion of h in S. We view $$\\lambda_{i,S}$$ as a distribution of hypotheses in S so for $$h \\notin S$$, $$\\lambda_{i,S}(h) = 0$$. For a set $$S \\subseteq H$$ of hypotheses, we define $$w_i(S) := \\sum_{h \\in S} w(h)$$ and $$\\lambda_i(h) = \\lambda_{i,H}(h)$$.\n\nDefine $$r_{\\lambda,h^*}(x) := \\Pr_{h \\sim \\lambda}[h(x) \\neq h^*(x)]$$, and $$r_{\\lambda}(x) = \\min_{y \\in \\{0,1\\}} \\Pr_{h \\sim \\lambda}[h(x) \\neq y]$$, so $$r_{\\lambda}(x) = \\min(r_{\\lambda,h^*}(x), 1 - r_{\\lambda,h^*}(x))$$.\n\nDefine\n\n$$\n\\lambda_{i,S}(h) := \\frac{1}{2\\lambda_i(h) + 1} \\left(1 - \\frac{1}{2} \\Pr_{h \\sim \\lambda_i}[h \\in S \\mid h \\in S]\\right) \\text{ for } h \\in S \\quad (5)\n$$\n$$\n\\lambda_{i,H\\setminus S}(h) = \\lambda_i(h) \\cdot \\left(1 - \\Pr_{h \\sim \\lambda_i}[h \\in S] \\mid h \\notin S\\right)\n$$\nas the \"capped\" distribution in iteration i.\n\nFinally, for notational convenience define $$r_{i,S} := r_{\\lambda_i,S}$$, $$r_{i,S,h} := r_{\\lambda_i,S,h}$$ and $$r_{i,S} := r_{\\lambda_i,S}$$.\n\nThe main focus of our proof would be analyzing the potential function\n\n$$\n\\phi_i(h^*) = \\sum \\log \\lambda_i(h^*) + \\log \\lambda_{i,H\\setminus S_i}(h^*) \\text{ for } h^* \\in S_i\n$$\nwhere $$h^*$$ is the best hypothesis in H. We would like to show that $$\\phi_{i+1}(h^*) - \\phi_i(h^*)$$ is growing at a proper rate in each iteration. We pick $$S_i$$ to be an expanding series of sets, i.e., $$S_i \\subseteq S_{i+1}$$ for any $$i \\geq 1$$. However, the change of the \"capped\" set $$S_i$$ makes this task challenging. Therefore, we instead analyze the following quantity defined as\n\n$$\n\\Delta_i(h^*) := \\begin{cases} \\log \\lambda_{i+1}(h^*) & h^* \\in S_i \\\\ 0 & h^* \\notin S_i \\end{cases}\n$$\nand $$\\phi_{i+1}(h^*) - \\phi_i(h^*) = \\Delta_i(h^*) + \\log \\lambda_{i+1,H\\setminus S_{i+1}}$$ for $$h^* \\in S_{i+1}$$. Further, we define\n\n$$\n\\psi_k(h^*) := \\begin{cases} \\lambda_{i+1,H\\setminus S_i}(h^*) & \\text{if } h^* \\notin \\lambda_{i+1,H\\setminus S_{i+1}} \\text{ for } i < k \\\\ \\Delta_i(h^*) & \\text{so by definition } \\phi_k(h^*) = \\phi_0(h^*) + \\psi_k(h^*) + \\sum_{i < k} \\log \\lambda_{i+1,H\\setminus S_i}(h^*) \\text{ if } h^* \\notin \\lambda_{i+1,H\\setminus S_{i+1}} \\end{cases}\n$$\n\nIn the following text, we will drop the parameter $$h^*$$ when the context is clear and just use $$\\phi_i$$, $$\\Delta_i$$ and $$\\psi_i$$ instead."}, {"type": "heading", "lvl": 3, "value": "Potential Growth", "md": "### Potential Growth"}, {"type": "text", "value": "We will lower bound the conditional per iteration potential increase by first introducing a lemma that relates the potential change to the optimization problem (3).\n\nLemma A.1. Assume that $$\\text{err}(h^*) \\leq \\eta$$, then for any set S of hypotheses containing $$h^*$$ and query distribution q, we have\n\n$$\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)] \\geq 0.9\\alpha \\mathbb{E}[q(x)]\n$$\nfor $$\\alpha \\leq 0.2$$. Moreover,\n\n$$\n\\frac{\\lambda_{i,S}(h^*)}{\\mathbb{E}[\\max(0, \\log \\lambda_{i+1,S}(h^*))]} \\leq \\alpha \\mathbb{E}[x \\sim q][r_{i,S,h^*(x)}]\n$$\n\n$$\n\\mathbb{E}[\\max(0, \\log \\lambda_{i+1,S}(h^*))] \\leq \\alpha \\mathbb{E}[x \\sim q][r_{i,S,h^*(x)}]\n$$\nProof. For notational convenience, define $$r(x) := r_{i,S,h^*(x)}$$.\n\nObserve that\n\n$$\n\\lambda_{i,S}(h^*) = \\frac{\\sum_{h \\in S} w_i(h^*)}{\\sum_{h \\in S} w_i(h)} = \\frac{w_i(h^*)}{\\mathbb{E}[w_{i,S}(h)]}\n$$\n$$\n\\lambda_{i+1,S}(h^*) = \\frac{w_{i+1}(h^*)}{\\sum_{h \\sim \\lambda_{i,S}} w_{i,S}(h)} = \\frac{w_{i+1}(h^*)}{\\mathbb{E}[w_{i,S}(h)]}\n$$", "md": "We will lower bound the conditional per iteration potential increase by first introducing a lemma that relates the potential change to the optimization problem (3).\n\nLemma A.1. Assume that $$\\text{err}(h^*) \\leq \\eta$$, then for any set S of hypotheses containing $$h^*$$ and query distribution q, we have\n\n$$\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)] \\geq 0.9\\alpha \\mathbb{E}[q(x)]\n$$\nfor $$\\alpha \\leq 0.2$$. Moreover,\n\n$$\n\\frac{\\lambda_{i,S}(h^*)}{\\mathbb{E}[\\max(0, \\log \\lambda_{i+1,S}(h^*))]} \\leq \\alpha \\mathbb{E}[x \\sim q][r_{i,S,h^*(x)}]\n$$\n\n$$\n\\mathbb{E}[\\max(0, \\log \\lambda_{i+1,S}(h^*))] \\leq \\alpha \\mathbb{E}[x \\sim q][r_{i,S,h^*(x)}]\n$$\nProof. For notational convenience, define $$r(x) := r_{i,S,h^*(x)}$$.\n\nObserve that\n\n$$\n\\lambda_{i,S}(h^*) = \\frac{\\sum_{h \\in S} w_i(h^*)}{\\sum_{h \\in S} w_i(h)} = \\frac{w_i(h^*)}{\\mathbb{E}[w_{i,S}(h)]}\n$$\n$$\n\\lambda_{i+1,S}(h^*) = \\frac{w_{i+1}(h^*)}{\\sum_{h \\sim \\lambda_{i,S}} w_{i,S}(h)} = \\frac{w_{i+1}(h^*)}{\\mathbb{E}[w_{i,S}(h)]}\n$$"}]}, {"page": 14, "text": "Let p(x) = Pry\u223c(Y |X)[y \u0338= h\u2217(x)] denote the probability of error if we query x, so\n                                                    E\n                                                 x\u223cDX[p(x)] \u2264       \u03b7.\nSuppose we query a point x and do not get an error. Then the hypotheses that disagree with h\u2217                       are\ndownweighted by an e\u2212\u03b1 factor, so\n                  \u03bbi,S(h\u2217)           E                                                           r(x).\n                \u03bbi+1,S(h\u2217) =      h\u223c\u03bbi,S[1 + (e\u2212\u03b1 \u2212       1)1h(x)\u0338=h\u2217(x)] = 1 \u2212      (1 \u2212  e\u2212\u03b1)\nOn the other hand, if we do get an error then the disagreeing hypotheses are effectively upweighted\nby e\u03b1:                                   \u03bbi,S(h\u2217)\nTherefore                               \u03bbi+1,S(h\u2217) = 1 + (e\u03b1 \u2212         1) r(x).\n                 E    log \u03bbi+1,S(h\u2217)     Fi\n                y|x         \u03bbi,S(h\u2217)\n                 = \u2212(1 \u2212    p(x)) log    1 \u2212   (1 \u2212  e\u2212\u03b1)  r(x)    \u2212  p(x) log (1 + (e\u03b1 \u2212     1) r(x))              (6)\n                 \u2265  (1 \u2212  p(x))(1 \u2212    e\u2212\u03b1)  r(x) \u2212    p(x)(e\u03b1 \u2212    1) r(x)\nUsing that       = (1 \u2212   e\u2212\u03b1)  r(x) \u2212   p(x)  r(x)(e\u03b1 \u2212    e\u2212\u03b1).\n             r(x) \u2264    1, we have\n                E   log \u03bbi+1,S(h\u2217)      Fi    \u2265  (1 \u2212  e\u2212\u03b1) E x\u223cq[ r(x)] \u2212   (e\u03b1 \u2212   e\u2212\u03b1) E x\u223cq[p(x)]\n                           \u03bbi,S(h\u2217)           \u2265  0.9\u03b1 Ex\u223cq[ r(x) \u2212   2.3p(x)],\nwhere the last step uses \u03b1 \u2264       0.2. Finally,\n                             E                E      p(x) q(x)        \u2264  \u03b7 max     q(x)\n                            x\u223cq[p(x)] =     x\u223cDX           DX(x)             x    DX(x).\nThis proves the first desired result. For the second, note that if we query x, then conditioned on Fi\n           max     0, log \u03bbi+1,S(h\u2217)        =   0log(1 + (1 \u2212     e\u2212\u03b1)            with probability p(x),\n                            \u03bbi,S(h\u2217)                                    r(x))     otherwise.\nSince log(1 + (1 \u2212      e\u2212\u03b1)  r(x)) \u2264    (1 \u2212   e\u2212\u03b1)  r(x) \u2264    \u03b1 r(x), taking the expectation over x gives the\nresult.\nThe above lemma, combined with Lemma 2.1, proves the potential will grow at desired rate at each\niteration. But remember that Lemma 2.1 requires the condition that no ball has probability greater\nthan 80%, so we need to check this condition is satisfied. The following lemma shows that if we cap\nthe set Si, then the probability is not concentrated on any small balls.\nLemma A.2. In Algorithm 1, for every iteration i, Si is such that no radius c4\u03b7 + c5\u03b5 ball has more\nthan 80% probability under \u03bbi,S        i.\nProof. If Si = Si\u22121, then by the construction of Si, there are no radius c4\u03b7 + c5\u03b5 balls have\nprobability greater than 80% under \u03bbi,S          i\u22121 = \u03bbi,Si. Otherwise, we have Si\u22121 \u0338= Si and a ball\nB(\u00b5, 3c4\u03b7 + 3c5\u03b5) is added to Si in this iteration. We first prove a useful claim below.\nClaim A.3. If a ball B\u2032 = (\u00b5, 3c4\u03b7+3c5\u03b5) is added to Si at some iteration i, \u03bbi(B(\u00b5, c4\u03b7+c5\u03b5)) \u2265\n0.6.\nProof. If B\u2032 is added to Si at the iteration i, then there exists some ball D with radius c4\u03b7 + c5\u03b5\nsuch that \u00af \u03bbi,Si\u22121(D) \u2265       0.8. If a set of hypotheses gains probability after capping, the gained\nprobability comes from the reduced probability of other hypotheses not in this set. Therefore, the\ngained probability of any set is upper bounded by half of the probability of the complement of that\nset before capping. This means \u03bbi(D) \u2265              0.6 because otherwise after capping \u00af        \u03bbi,Si\u22121(D) < 0.8,\nwhich is a contradiction. As a result, \u03bbi(B(\u00b5, c4\u03b7 + c5\u03b5)) \u2265              \u03bbi(D) \u2265    0.6.\n                                                          14", "md": "Let \\( p(x) = \\text{Pr}_{y \\sim Y|X}[y \\neq h^*(x)] \\) denote the probability of error if we query \\( x \\), so\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim D_X}[p(x)] \\leq \\eta.\n\\end{align*}\n$$\nSuppose we query a point \\( x \\) and do not get an error. Then the hypotheses that disagree with \\( h^* \\) are downweighted by an \\( e^{-\\alpha} \\) factor, so\n\n$$\n\\begin{align*}\n\\lambda_{i,S}(h^*) \\leq \\mathbb{E}[r(x)], \\\\\n\\lambda_{i+1,S}(h^*) = h \\sim \\lambda_{i,S}[1 + (e^{-\\alpha} - 1)1_{h(x) \\neq h^*(x)}] = 1 - (1 - e^{-\\alpha})\n\\end{align*}\n$$\nOn the other hand, if we do get an error then the disagreeing hypotheses are effectively upweighted by \\( e^{\\alpha} \\):\n\n$$\n\\begin{align*}\n\\lambda_{i,S}(h^*) \\\\\n\\lambda_{i+1,S}(h^*) = 1 + (e^{\\alpha} - 1) r(x).\n\\end{align*}\n$$\nTherefore\n\n$$\n\\begin{align*}\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)|y,x] \\geq (1 - p(x))(1 - e^{-\\alpha})r(x) - p(x)(e^{\\alpha} - 1)r(x)\n\\end{align*}\n$$\nUsing that \\( r(x) \\leq 1 \\), we have\n\n$$\n\\begin{align*}\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)|y,x] \\geq (1 - e^{-\\alpha})\\mathbb{E}_{x \\sim q}[r(x)] - (e^{\\alpha} - e^{-\\alpha})\\mathbb{E}_{x \\sim q}[p(x)] \\\\\n\\lambda_{i,S}(h^*) \\geq 0.9\\alpha \\mathbb{E}_{x \\sim q}[r(x) - 2.3p(x)],\n\\end{align*}\n$$\nwhere the last step uses \\( \\alpha \\leq 0.2 \\). Finally,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim q}[p(x)] = \\mathbb{E}_{x \\sim D_X}[p(x)q(x)] \\leq \\eta \\max_{x} q(x)\n\\end{align*}\n$$\nThis proves the first desired result. For the second, note that if we query \\( x \\), then conditioned on \\( F_i \\)\n\n$$\n\\begin{align*}\n\\max_{x} 0, \\log \\lambda_{i+1,S}(h^*) = \\begin{cases} 0 & \\text{log}(1 + (1 - e^{-\\alpha})r(x)) \\text{ with probability } p(x), \\\\ (1 - e^{-\\alpha})r(x) & \\text{otherwise.} \\end{cases}\n\\end{align*}\n$$\nSince \\( \\text{log}(1 + (1 - e^{-\\alpha})r(x)) \\leq (1 - e^{-\\alpha})r(x) \\leq \\alpha r(x) \\), taking the expectation over \\( x \\) gives the result.\n\nThe above lemma, combined with Lemma 2.1, proves the potential will grow at desired rate at each iteration. But remember that Lemma 2.1 requires the condition that no ball has probability greater than 80%, so we need to check this condition is satisfied. The following lemma shows that if we cap the set \\( S_i \\), then the probability is not concentrated on any small balls.\n\nLemma A.2. In Algorithm 1, for every iteration \\( i \\), \\( S_i \\) is such that no radius \\( c4\\eta + c5\\epsilon \\) ball has more than 80% probability under \\( \\lambda_{i,S} \\).\n\nProof. If \\( S_i = S_{i-1} \\), then by the construction of \\( S_i \\), there are no radius \\( c4\\eta + c5\\epsilon \\) balls have probability greater than 80% under \\( \\lambda_{i,S} = \\lambda_{i,S_i} \\). Otherwise, we have \\( S_{i-1} \\neq S_i \\) and a ball \\( B(\\mu, 3c4\\eta + 3c5\\epsilon) \\) is added to \\( S_i \\) in this iteration. We first prove a useful claim below.\n\nClaim A.3. If a ball \\( B' = (\\mu, 3c4\\eta+3c5\\epsilon) \\) is added to \\( S_i \\) at some iteration \\( i \\), \\( \\lambda_i(B(\\mu, c4\\eta+c5\\epsilon)) \\geq 0.6 \\).\n\nProof. If \\( B' \\) is added to \\( S_i \\) at the iteration \\( i \\), then there exists some ball \\( D \\) with radius \\( c4\\eta + c5\\epsilon \\) such that \\( \\overline{\\lambda_{i,S_{i-1}}}(D) \\geq 0.8 \\). If a set of hypotheses gains probability after capping, the gained probability comes from the reduced probability of other hypotheses not in this set. Therefore, the gained probability of any set is upper bounded by half of the probability of the complement of that set before capping. This means \\( \\lambda_i(D) \\geq 0.6 \\) because otherwise after capping \\( \\overline{\\lambda_{i,S_{i-1}}}(D) < 0.8 \\), which is a contradiction. As a result, \\( \\lambda_i(B(\\mu, c4\\eta + c5\\epsilon)) \\geq \\lambda_i(D) \\geq 0.6 \\).\n\n14", "images": [], "items": [{"type": "text", "value": "Let \\( p(x) = \\text{Pr}_{y \\sim Y|X}[y \\neq h^*(x)] \\) denote the probability of error if we query \\( x \\), so\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim D_X}[p(x)] \\leq \\eta.\n\\end{align*}\n$$\nSuppose we query a point \\( x \\) and do not get an error. Then the hypotheses that disagree with \\( h^* \\) are downweighted by an \\( e^{-\\alpha} \\) factor, so\n\n$$\n\\begin{align*}\n\\lambda_{i,S}(h^*) \\leq \\mathbb{E}[r(x)], \\\\\n\\lambda_{i+1,S}(h^*) = h \\sim \\lambda_{i,S}[1 + (e^{-\\alpha} - 1)1_{h(x) \\neq h^*(x)}] = 1 - (1 - e^{-\\alpha})\n\\end{align*}\n$$\nOn the other hand, if we do get an error then the disagreeing hypotheses are effectively upweighted by \\( e^{\\alpha} \\):\n\n$$\n\\begin{align*}\n\\lambda_{i,S}(h^*) \\\\\n\\lambda_{i+1,S}(h^*) = 1 + (e^{\\alpha} - 1) r(x).\n\\end{align*}\n$$\nTherefore\n\n$$\n\\begin{align*}\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)|y,x] \\geq (1 - p(x))(1 - e^{-\\alpha})r(x) - p(x)(e^{\\alpha} - 1)r(x)\n\\end{align*}\n$$\nUsing that \\( r(x) \\leq 1 \\), we have\n\n$$\n\\begin{align*}\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)|y,x] \\geq (1 - e^{-\\alpha})\\mathbb{E}_{x \\sim q}[r(x)] - (e^{\\alpha} - e^{-\\alpha})\\mathbb{E}_{x \\sim q}[p(x)] \\\\\n\\lambda_{i,S}(h^*) \\geq 0.9\\alpha \\mathbb{E}_{x \\sim q}[r(x) - 2.3p(x)],\n\\end{align*}\n$$\nwhere the last step uses \\( \\alpha \\leq 0.2 \\). Finally,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim q}[p(x)] = \\mathbb{E}_{x \\sim D_X}[p(x)q(x)] \\leq \\eta \\max_{x} q(x)\n\\end{align*}\n$$\nThis proves the first desired result. For the second, note that if we query \\( x \\), then conditioned on \\( F_i \\)\n\n$$\n\\begin{align*}\n\\max_{x} 0, \\log \\lambda_{i+1,S}(h^*) = \\begin{cases} 0 & \\text{log}(1 + (1 - e^{-\\alpha})r(x)) \\text{ with probability } p(x), \\\\ (1 - e^{-\\alpha})r(x) & \\text{otherwise.} \\end{cases}\n\\end{align*}\n$$\nSince \\( \\text{log}(1 + (1 - e^{-\\alpha})r(x)) \\leq (1 - e^{-\\alpha})r(x) \\leq \\alpha r(x) \\), taking the expectation over \\( x \\) gives the result.\n\nThe above lemma, combined with Lemma 2.1, proves the potential will grow at desired rate at each iteration. But remember that Lemma 2.1 requires the condition that no ball has probability greater than 80%, so we need to check this condition is satisfied. The following lemma shows that if we cap the set \\( S_i \\), then the probability is not concentrated on any small balls.\n\nLemma A.2. In Algorithm 1, for every iteration \\( i \\), \\( S_i \\) is such that no radius \\( c4\\eta + c5\\epsilon \\) ball has more than 80% probability under \\( \\lambda_{i,S} \\).\n\nProof. If \\( S_i = S_{i-1} \\), then by the construction of \\( S_i \\), there are no radius \\( c4\\eta + c5\\epsilon \\) balls have probability greater than 80% under \\( \\lambda_{i,S} = \\lambda_{i,S_i} \\). Otherwise, we have \\( S_{i-1} \\neq S_i \\) and a ball \\( B(\\mu, 3c4\\eta + 3c5\\epsilon) \\) is added to \\( S_i \\) in this iteration. We first prove a useful claim below.\n\nClaim A.3. If a ball \\( B' = (\\mu, 3c4\\eta+3c5\\epsilon) \\) is added to \\( S_i \\) at some iteration \\( i \\), \\( \\lambda_i(B(\\mu, c4\\eta+c5\\epsilon)) \\geq 0.6 \\).\n\nProof. If \\( B' \\) is added to \\( S_i \\) at the iteration \\( i \\), then there exists some ball \\( D \\) with radius \\( c4\\eta + c5\\epsilon \\) such that \\( \\overline{\\lambda_{i,S_{i-1}}}(D) \\geq 0.8 \\). If a set of hypotheses gains probability after capping, the gained probability comes from the reduced probability of other hypotheses not in this set. Therefore, the gained probability of any set is upper bounded by half of the probability of the complement of that set before capping. This means \\( \\lambda_i(D) \\geq 0.6 \\) because otherwise after capping \\( \\overline{\\lambda_{i,S_{i-1}}}(D) < 0.8 \\), which is a contradiction. As a result, \\( \\lambda_i(B(\\mu, c4\\eta + c5\\epsilon)) \\geq \\lambda_i(D) \\geq 0.6 \\).\n\n14", "md": "Let \\( p(x) = \\text{Pr}_{y \\sim Y|X}[y \\neq h^*(x)] \\) denote the probability of error if we query \\( x \\), so\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim D_X}[p(x)] \\leq \\eta.\n\\end{align*}\n$$\nSuppose we query a point \\( x \\) and do not get an error. Then the hypotheses that disagree with \\( h^* \\) are downweighted by an \\( e^{-\\alpha} \\) factor, so\n\n$$\n\\begin{align*}\n\\lambda_{i,S}(h^*) \\leq \\mathbb{E}[r(x)], \\\\\n\\lambda_{i+1,S}(h^*) = h \\sim \\lambda_{i,S}[1 + (e^{-\\alpha} - 1)1_{h(x) \\neq h^*(x)}] = 1 - (1 - e^{-\\alpha})\n\\end{align*}\n$$\nOn the other hand, if we do get an error then the disagreeing hypotheses are effectively upweighted by \\( e^{\\alpha} \\):\n\n$$\n\\begin{align*}\n\\lambda_{i,S}(h^*) \\\\\n\\lambda_{i+1,S}(h^*) = 1 + (e^{\\alpha} - 1) r(x).\n\\end{align*}\n$$\nTherefore\n\n$$\n\\begin{align*}\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)|y,x] \\geq (1 - p(x))(1 - e^{-\\alpha})r(x) - p(x)(e^{\\alpha} - 1)r(x)\n\\end{align*}\n$$\nUsing that \\( r(x) \\leq 1 \\), we have\n\n$$\n\\begin{align*}\n\\mathbb{E}[\\log \\lambda_{i+1,S}(h^*)|y,x] \\geq (1 - e^{-\\alpha})\\mathbb{E}_{x \\sim q}[r(x)] - (e^{\\alpha} - e^{-\\alpha})\\mathbb{E}_{x \\sim q}[p(x)] \\\\\n\\lambda_{i,S}(h^*) \\geq 0.9\\alpha \\mathbb{E}_{x \\sim q}[r(x) - 2.3p(x)],\n\\end{align*}\n$$\nwhere the last step uses \\( \\alpha \\leq 0.2 \\). Finally,\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim q}[p(x)] = \\mathbb{E}_{x \\sim D_X}[p(x)q(x)] \\leq \\eta \\max_{x} q(x)\n\\end{align*}\n$$\nThis proves the first desired result. For the second, note that if we query \\( x \\), then conditioned on \\( F_i \\)\n\n$$\n\\begin{align*}\n\\max_{x} 0, \\log \\lambda_{i+1,S}(h^*) = \\begin{cases} 0 & \\text{log}(1 + (1 - e^{-\\alpha})r(x)) \\text{ with probability } p(x), \\\\ (1 - e^{-\\alpha})r(x) & \\text{otherwise.} \\end{cases}\n\\end{align*}\n$$\nSince \\( \\text{log}(1 + (1 - e^{-\\alpha})r(x)) \\leq (1 - e^{-\\alpha})r(x) \\leq \\alpha r(x) \\), taking the expectation over \\( x \\) gives the result.\n\nThe above lemma, combined with Lemma 2.1, proves the potential will grow at desired rate at each iteration. But remember that Lemma 2.1 requires the condition that no ball has probability greater than 80%, so we need to check this condition is satisfied. The following lemma shows that if we cap the set \\( S_i \\), then the probability is not concentrated on any small balls.\n\nLemma A.2. In Algorithm 1, for every iteration \\( i \\), \\( S_i \\) is such that no radius \\( c4\\eta + c5\\epsilon \\) ball has more than 80% probability under \\( \\lambda_{i,S} \\).\n\nProof. If \\( S_i = S_{i-1} \\), then by the construction of \\( S_i \\), there are no radius \\( c4\\eta + c5\\epsilon \\) balls have probability greater than 80% under \\( \\lambda_{i,S} = \\lambda_{i,S_i} \\). Otherwise, we have \\( S_{i-1} \\neq S_i \\) and a ball \\( B(\\mu, 3c4\\eta + 3c5\\epsilon) \\) is added to \\( S_i \\) in this iteration. We first prove a useful claim below.\n\nClaim A.3. If a ball \\( B' = (\\mu, 3c4\\eta+3c5\\epsilon) \\) is added to \\( S_i \\) at some iteration \\( i \\), \\( \\lambda_i(B(\\mu, c4\\eta+c5\\epsilon)) \\geq 0.6 \\).\n\nProof. If \\( B' \\) is added to \\( S_i \\) at the iteration \\( i \\), then there exists some ball \\( D \\) with radius \\( c4\\eta + c5\\epsilon \\) such that \\( \\overline{\\lambda_{i,S_{i-1}}}(D) \\geq 0.8 \\). If a set of hypotheses gains probability after capping, the gained probability comes from the reduced probability of other hypotheses not in this set. Therefore, the gained probability of any set is upper bounded by half of the probability of the complement of that set before capping. This means \\( \\lambda_i(D) \\geq 0.6 \\) because otherwise after capping \\( \\overline{\\lambda_{i,S_{i-1}}}(D) < 0.8 \\), which is a contradiction. As a result, \\( \\lambda_i(B(\\mu, c4\\eta + c5\\epsilon)) \\geq \\lambda_i(D) \\geq 0.6 \\).\n\n14"}]}, {"page": 15, "text": " By Claim A.3, the probability of B(\u00b5, c4\u03b7 + c5\u03b5) is at least 0.6 over the uncapped distribution\n \u03bbi. So any ball not intersecting B(\u00b5, c4\u03b7 + c5\u03b5) has probability at most 0.4 before capping. After\n capping these balls will have probability no more than 0.7. At the same time, any ball intersects\n B(\u00b5, c4\u03b7 + c5\u03b5) would be completely inside B(\u00b5, 3c4\u03b7 + 3c5\u03b5) so its probability would be at most\n 0.5 after capping.\n Now we are ready to apply Lemma A.1 and Lemma 2.1 except one caution. Remember that in the\n beginning of the algorithm, we compute a 2\u03b7-packing H\u2032 \u2286                              H of the instance. From the well-known\n relationship between packing and covering (for example, see Vershynin [2018, Lemma 4.2.8]), we\n have |H\u2032| \u2264       N(H, \u03b7). Every hypothesis in H is within 2\u03b7 to some hypothesis in H\u2032, so there exists\n a hypothesis in H\u2032 with error less than 3\u03b7. This means that the best hypothesis h\u2217                                       \u2208   H\u2032 has error\n 3\u03b7 instead of \u03b7. The following lemma serves as the cornerstone of the proof of the query complexity\n upper bound, which states that the potential grows at rate \u2126                             1     in each iteration.\n                                                                                         m\u2217\n Lemma A.4. Given c4 \u2265                 300 and err(h\u2217) \u2264           3\u03b7, there exists a sampling distribution q such that\n    E[\u2206i|Fi] \u2265        E [\u2206i|Fi] \u2212        2\u03b1\u03b7 max         q(x)                                 \u03b1                           if    h\u2217   /\n                                                  x    DX(x) \u2273         m\u2217     H, DX, c4\u03b7, c5\u03b5 \u2212           2\u03b7, 99100                  \u2208  Si,\n as well as |\u2206i| \u2264         \u03b1 always and Var[\u2206i|Fi] \u2264                 \u03b1 E [|\u2206i||Fi] \u2272         \u03b1 E[\u2206i|Fi].\n Proof. For the sake of bookkeeping, we let m\u2217                       = m\u2217      H, DX, c4\u03b7, c5\u03b5 \u2212            2\u03b7, 99       in this proof and\n                                                                                                                 100\n the following text. We first bound the expectation. By Lemma A.1 applied to S \u2208                                       {H, H \\ Si} with\n 3\u03b7, we have\n   E [\u2206i|Fi] \u2212        2\u03b1\u03b7 max         q(x)                    E                                                                   q(x)\n                               x    DX(x) \u22650.9\u03b1              x\u223cq[ri,H,h\u2217(x) + ri,H\\Si,h\u2217(x)] \u2212                 13.8\u03b7 max   x    DX(x)\n                                                                       q(x)\n                                                   \u2212   2\u03b1\u03b7 max  x    DX(x),\n where q is the query distribution of the algorithm at iteration i. Now, by the definition of\n                                                       \u03bbi,S = 1   2\u03bbi + 1    2\u03bbi,H\\S,\n we have for any x that\n                                          ri,Si,h\u2217(x) = 1     2(ri,h\u2217(x) + ri,H\\Si,h\u2217(x))\n and thus\n                                                          q(x)\n                       E [\u2206i|Fi] \u2212       2\u03b1\u03b7 max  x     DX(x)\n                       \u2265   1.8\u03b1        E                                         q(x)         \u2212  2\u03b1\u03b7 max          q(x)                      (7)\n                                     x\u223cq[ri,Si,h\u2217(x)] \u2212          6.9\u03b7 max x    DX(x)                       x    DX(x)\n                       \u2265   1.8\u03b1        E                                     q(x)        .\n                                     x\u223cq[ri,Si(x)] \u2212        8.1\u03b7 max  x    DX(x)\n Algorithm 1 chooses the sampling distribution q to maximize Ex\u223cq[ri,Si(x)] \u2212                                       c4               q(x)\n                                            q(x)                                                                    20\u03b7 maxx       DX(x) \u2264\n Ex\u223cq[ri,Si(x)] \u2212          15\u03b7 maxx       DX(x) because c4 \u2265            300. By Lemma A.2, \u03bbi,Si over H\u2032 has no radius-\n(c4\u03b7 + c5\u03b5) ball with probability larger than 80%, so by Lemma 2.1 q satisfies\n  E                                   q(x)          x\u223cq[ri,Si(x)]\u2212        c4       x      q(x)                              1                   .\n x\u223cq[ri,Si(x)]\u221215\u03b7 max         x    DX(x) \u2265          E                    20\u03b7 max       DX(x) \u2273         m\u2217     H\u2032, DX, c4\u03b7, c5\u03b5, 99       100\n Because H\u2032 \u2286          H is a maximal 2\u03b7-packing, every hypothesis in H is within 2\u03b7 of some hypothesis in\n H\u2032. The problem             H, DX, c4\u03b7, c5\u03b5 \u2212           2\u03b7, 99       is harder than the problem               H\u2032, DX, c4\u03b7, c5\u03b5, 99\n                                                              100                                                                         100\n because we can reduce the latter to the former by simply adding more hypotheses and solve\n                                                                       15", "md": "By Claim A.3, the probability of \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) is at least 0.6 over the uncapped distribution \\(\\lambda_i\\). So any ball not intersecting \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) has probability at most 0.4 before capping. After capping, these balls will have probability no more than 0.7. At the same time, any ball that intersects \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) would be completely inside \\(B(\\mu, 3c4\\eta + 3c5\\varepsilon)\\), so its probability would be at most 0.5 after capping.\n\nNow we are ready to apply Lemma A.1 and Lemma 2.1 except one caution. Remember that in the beginning of the algorithm, we compute a \\(2\\eta\\)-packing \\(H' \\subseteq H\\) of the instance. From the well-known relationship between packing and covering (for example, see Vershynin [2018, Lemma 4.2.8]), we have \\(|H'|\\leq N(H, \\eta)\\). Every hypothesis in \\(H\\) is within \\(2\\eta\\) to some hypothesis in \\(H'\\), so there exists a hypothesis in \\(H'\\) with error less than \\(3\\eta\\). This means that the best hypothesis \\(h^* \\in H'\\) has error \\(3\\eta\\) instead of \\(\\eta\\). The following lemma serves as the cornerstone of the proof of the query complexity upper bound, which states that the potential grows at rate \\(\\Omega \\frac{1}{m^*}\\) in each iteration.\n\n$$\n\\text{Lemma A.4. Given } c4 \\geq 300 \\text{ and } \\text{err}(h^*) \\leq 3\\eta, \\text{ there exists a sampling distribution } q \\text{ such that}\n$$\n\n$$\nE[\\Delta_i|F_i] \\geq E[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} q(x) \\frac{\\alpha}{\\sum_{x}DX(x)} \\text{ if } h^* \\notin S_i,\n$$\n\n$$\n\\text{as well as } |\\Delta_i| \\leq \\alpha \\text{ always and } \\text{Var}[\\Delta_i|F_i] \\leq \\alpha E[|\\Delta_i||F_i] \\lesssim \\alpha E[\\Delta_i|F_i].\n$$\n\nProof. For the sake of bookkeeping, we let \\(m^* = m^* H, DX, c4\\eta, c5\\varepsilon - 2\\eta, 99\\) in this proof and the following text. We first bound the expectation. By Lemma A.1 applied to \\(S \\in \\{H, H \\setminus S_i\\}\\) with \\(3\\eta\\), we have\n\n$$\nE[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\geq 0.9\\alpha E[q(x)] - 13.8\\eta \\max_{x\\in \\mathcal{X}} DX(x) - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x),\n$$\n\nwhere \\(q\\) is the query distribution of the algorithm at iteration \\(i\\). Now, by the definition of\n\n$$\n\\lambda_{i,S} = \\frac{1}{2}\\lambda_i + \\frac{1}{2}\\lambda_{i,H\\setminus S},\n$$\n\nwe have for any \\(x\\) that\n\n$$\nr_{i,S_i,h^*}(x) = \\frac{1}{2}(r_{i,h^*}(x) + r_{i,H\\setminus S_i,h^*}(x))\n$$\n\nand thus\n\n$$\nE[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\geq 1.8\\alpha E[q(x)] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\text{ (7)}\n$$\n\n$$\n\\geq 1.8\\alpha E[q(x)] - 8.1\\eta \\max_{x\\in \\mathcal{X}} DX(x).\n$$\n\nAlgorithm 1 chooses the sampling distribution \\(q\\) to maximize \\(E_{x\\sim q}[r_{i,S_i}(x)] - c4 q(x) \\leq 20\\eta \\max_{x} DX(x)\\). Because \\(c4 \\geq 300\\), by Lemma A.2, \\(\\lambda_{i,S_i}\\) over \\(H'\\) has no radius-\\(c4\\eta + c5\\varepsilon\\) ball with probability larger than 80%, so by Lemma 2.1, \\(q\\) satisfies\n\n$$\nE[q(x)] - q(x) \\geq E[q(x)] - c4 q(x) \\leq 1.\n$$\n\n$$\nE[q(x)] - 15\\eta \\max_{x} DX(x) \\geq E[20\\eta \\max_{x} DX(x) \\gtrsim m^* H', DX, c4\\eta, c5\\varepsilon, 99.\n$$\n\nBecause \\(H' \\subseteq H\\) is a maximal \\(2\\eta\\)-packing, every hypothesis in \\(H\\) is within \\(2\\eta\\) of some hypothesis in \\(H'\\). The problem \\(H, DX, c4\\eta, c5\\varepsilon - 2\\eta, 99\\) is harder than the problem \\(H', DX, c4\\eta, c5\\varepsilon, 99\\) because we can reduce the latter to the former by simply adding more hypotheses and solve.", "images": [], "items": [{"type": "text", "value": "By Claim A.3, the probability of \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) is at least 0.6 over the uncapped distribution \\(\\lambda_i\\). So any ball not intersecting \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) has probability at most 0.4 before capping. After capping, these balls will have probability no more than 0.7. At the same time, any ball that intersects \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) would be completely inside \\(B(\\mu, 3c4\\eta + 3c5\\varepsilon)\\), so its probability would be at most 0.5 after capping.\n\nNow we are ready to apply Lemma A.1 and Lemma 2.1 except one caution. Remember that in the beginning of the algorithm, we compute a \\(2\\eta\\)-packing \\(H' \\subseteq H\\) of the instance. From the well-known relationship between packing and covering (for example, see Vershynin [2018, Lemma 4.2.8]), we have \\(|H'|\\leq N(H, \\eta)\\). Every hypothesis in \\(H\\) is within \\(2\\eta\\) to some hypothesis in \\(H'\\), so there exists a hypothesis in \\(H'\\) with error less than \\(3\\eta\\). This means that the best hypothesis \\(h^* \\in H'\\) has error \\(3\\eta\\) instead of \\(\\eta\\). The following lemma serves as the cornerstone of the proof of the query complexity upper bound, which states that the potential grows at rate \\(\\Omega \\frac{1}{m^*}\\) in each iteration.\n\n$$\n\\text{Lemma A.4. Given } c4 \\geq 300 \\text{ and } \\text{err}(h^*) \\leq 3\\eta, \\text{ there exists a sampling distribution } q \\text{ such that}\n$$\n\n$$\nE[\\Delta_i|F_i] \\geq E[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} q(x) \\frac{\\alpha}{\\sum_{x}DX(x)} \\text{ if } h^* \\notin S_i,\n$$\n\n$$\n\\text{as well as } |\\Delta_i| \\leq \\alpha \\text{ always and } \\text{Var}[\\Delta_i|F_i] \\leq \\alpha E[|\\Delta_i||F_i] \\lesssim \\alpha E[\\Delta_i|F_i].\n$$\n\nProof. For the sake of bookkeeping, we let \\(m^* = m^* H, DX, c4\\eta, c5\\varepsilon - 2\\eta, 99\\) in this proof and the following text. We first bound the expectation. By Lemma A.1 applied to \\(S \\in \\{H, H \\setminus S_i\\}\\) with \\(3\\eta\\), we have\n\n$$\nE[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\geq 0.9\\alpha E[q(x)] - 13.8\\eta \\max_{x\\in \\mathcal{X}} DX(x) - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x),\n$$\n\nwhere \\(q\\) is the query distribution of the algorithm at iteration \\(i\\). Now, by the definition of\n\n$$\n\\lambda_{i,S} = \\frac{1}{2}\\lambda_i + \\frac{1}{2}\\lambda_{i,H\\setminus S},\n$$\n\nwe have for any \\(x\\) that\n\n$$\nr_{i,S_i,h^*}(x) = \\frac{1}{2}(r_{i,h^*}(x) + r_{i,H\\setminus S_i,h^*}(x))\n$$\n\nand thus\n\n$$\nE[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\geq 1.8\\alpha E[q(x)] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\text{ (7)}\n$$\n\n$$\n\\geq 1.8\\alpha E[q(x)] - 8.1\\eta \\max_{x\\in \\mathcal{X}} DX(x).\n$$\n\nAlgorithm 1 chooses the sampling distribution \\(q\\) to maximize \\(E_{x\\sim q}[r_{i,S_i}(x)] - c4 q(x) \\leq 20\\eta \\max_{x} DX(x)\\). Because \\(c4 \\geq 300\\), by Lemma A.2, \\(\\lambda_{i,S_i}\\) over \\(H'\\) has no radius-\\(c4\\eta + c5\\varepsilon\\) ball with probability larger than 80%, so by Lemma 2.1, \\(q\\) satisfies\n\n$$\nE[q(x)] - q(x) \\geq E[q(x)] - c4 q(x) \\leq 1.\n$$\n\n$$\nE[q(x)] - 15\\eta \\max_{x} DX(x) \\geq E[20\\eta \\max_{x} DX(x) \\gtrsim m^* H', DX, c4\\eta, c5\\varepsilon, 99.\n$$\n\nBecause \\(H' \\subseteq H\\) is a maximal \\(2\\eta\\)-packing, every hypothesis in \\(H\\) is within \\(2\\eta\\) of some hypothesis in \\(H'\\). The problem \\(H, DX, c4\\eta, c5\\varepsilon - 2\\eta, 99\\) is harder than the problem \\(H', DX, c4\\eta, c5\\varepsilon, 99\\) because we can reduce the latter to the former by simply adding more hypotheses and solve.", "md": "By Claim A.3, the probability of \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) is at least 0.6 over the uncapped distribution \\(\\lambda_i\\). So any ball not intersecting \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) has probability at most 0.4 before capping. After capping, these balls will have probability no more than 0.7. At the same time, any ball that intersects \\(B(\\mu, c4\\eta + c5\\varepsilon)\\) would be completely inside \\(B(\\mu, 3c4\\eta + 3c5\\varepsilon)\\), so its probability would be at most 0.5 after capping.\n\nNow we are ready to apply Lemma A.1 and Lemma 2.1 except one caution. Remember that in the beginning of the algorithm, we compute a \\(2\\eta\\)-packing \\(H' \\subseteq H\\) of the instance. From the well-known relationship between packing and covering (for example, see Vershynin [2018, Lemma 4.2.8]), we have \\(|H'|\\leq N(H, \\eta)\\). Every hypothesis in \\(H\\) is within \\(2\\eta\\) to some hypothesis in \\(H'\\), so there exists a hypothesis in \\(H'\\) with error less than \\(3\\eta\\). This means that the best hypothesis \\(h^* \\in H'\\) has error \\(3\\eta\\) instead of \\(\\eta\\). The following lemma serves as the cornerstone of the proof of the query complexity upper bound, which states that the potential grows at rate \\(\\Omega \\frac{1}{m^*}\\) in each iteration.\n\n$$\n\\text{Lemma A.4. Given } c4 \\geq 300 \\text{ and } \\text{err}(h^*) \\leq 3\\eta, \\text{ there exists a sampling distribution } q \\text{ such that}\n$$\n\n$$\nE[\\Delta_i|F_i] \\geq E[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} q(x) \\frac{\\alpha}{\\sum_{x}DX(x)} \\text{ if } h^* \\notin S_i,\n$$\n\n$$\n\\text{as well as } |\\Delta_i| \\leq \\alpha \\text{ always and } \\text{Var}[\\Delta_i|F_i] \\leq \\alpha E[|\\Delta_i||F_i] \\lesssim \\alpha E[\\Delta_i|F_i].\n$$\n\nProof. For the sake of bookkeeping, we let \\(m^* = m^* H, DX, c4\\eta, c5\\varepsilon - 2\\eta, 99\\) in this proof and the following text. We first bound the expectation. By Lemma A.1 applied to \\(S \\in \\{H, H \\setminus S_i\\}\\) with \\(3\\eta\\), we have\n\n$$\nE[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\geq 0.9\\alpha E[q(x)] - 13.8\\eta \\max_{x\\in \\mathcal{X}} DX(x) - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x),\n$$\n\nwhere \\(q\\) is the query distribution of the algorithm at iteration \\(i\\). Now, by the definition of\n\n$$\n\\lambda_{i,S} = \\frac{1}{2}\\lambda_i + \\frac{1}{2}\\lambda_{i,H\\setminus S},\n$$\n\nwe have for any \\(x\\) that\n\n$$\nr_{i,S_i,h^*}(x) = \\frac{1}{2}(r_{i,h^*}(x) + r_{i,H\\setminus S_i,h^*}(x))\n$$\n\nand thus\n\n$$\nE[\\Delta_i|F_i] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\geq 1.8\\alpha E[q(x)] - 2\\alpha\\eta \\max_{x\\in \\mathcal{X}} DX(x) \\text{ (7)}\n$$\n\n$$\n\\geq 1.8\\alpha E[q(x)] - 8.1\\eta \\max_{x\\in \\mathcal{X}} DX(x).\n$$\n\nAlgorithm 1 chooses the sampling distribution \\(q\\) to maximize \\(E_{x\\sim q}[r_{i,S_i}(x)] - c4 q(x) \\leq 20\\eta \\max_{x} DX(x)\\). Because \\(c4 \\geq 300\\), by Lemma A.2, \\(\\lambda_{i,S_i}\\) over \\(H'\\) has no radius-\\(c4\\eta + c5\\varepsilon\\) ball with probability larger than 80%, so by Lemma 2.1, \\(q\\) satisfies\n\n$$\nE[q(x)] - q(x) \\geq E[q(x)] - c4 q(x) \\leq 1.\n$$\n\n$$\nE[q(x)] - 15\\eta \\max_{x} DX(x) \\geq E[20\\eta \\max_{x} DX(x) \\gtrsim m^* H', DX, c4\\eta, c5\\varepsilon, 99.\n$$\n\nBecause \\(H' \\subseteq H\\) is a maximal \\(2\\eta\\)-packing, every hypothesis in \\(H\\) is within \\(2\\eta\\) of some hypothesis in \\(H'\\). The problem \\(H, DX, c4\\eta, c5\\varepsilon - 2\\eta, 99\\) is harder than the problem \\(H', DX, c4\\eta, c5\\varepsilon, 99\\) because we can reduce the latter to the former by simply adding more hypotheses and solve."}]}, {"page": 16, "text": " it then map the solution back by returning the closest hypothesis in H\u2032.                                                Hence, m\u2217           \u2265\n m\u2217    H\u2032, DX, c4\u03b7, c5\u03b5, 99       100    . Therefore,\n           E [\u2206i|Fi] \u2212        2\u03b1\u03b7 max  x      q(x)                      E                              x      q(x)         \u2273    \u03b1\n                                            DX(x) \u2265         1.8\u03b1      x\u223cq[ri,Si(x)] \u2212        8.1\u03b7 max       DX(x)              m\u2217   .\nWe now bound the variance. The value of \u2206i may be positive or negative, but it is bounded by\n |\u2206i| \u2264     \u03b1. Thus                       Var[\u2206i|Fi] \u2264         E    \u22062 i |Fi   \u2264   \u03b1 E[|\u2206i| |Fi].\n By Lemma A.1 and (7) we have\n                    E[|\u2206i| |Fi] = E[2 max {\u2206i, 0} \u2212             E    \u2206i|Fi]                            q(x)\n                    \u2264   4\u03b1 E x\u223cq [ri,Si(x)] \u2212        1.8\u03b1      x\u223cq[ri,Si(x)] \u2212        8.1\u03b7 max  x    DX(x)\n                    \u2264   2.2\u03b1        E                                        q(x)\n                                  x\u223cq[ri,S,h\u2217(x)] + 6.7\u03b7 max          x    DX(x)\n                    \u2264   2.2\u03b1                                          x      q(x)                             x      q(x)\n                        1.8\u03b1 E [\u2206i|Fi] + 2.2\u03b1 \u00b7 6.9\u03b7 max         q(x)      D  X(x) + 2.2\u03b1 \u00b7 6.7\u03b7 max               DX(x)\n                    \u2264   1.3 E[\u2206i|Fi] + 30\u03b1\u03b7 max           x    DX(x).\n                                                   q(x)          1\n Since Ex\u223cq[\u2206i|Fi] \u2212             2\u03b1\u03b7 maxx        DX(x) \u2273       m\u2217    \u2265   0, we have\n                                                            q(x)\n                                               \u03b7 maxx     DX(x) \u2264          1   x\u223cq [\u2206i|Fi] ,\n                                                                          2\u03b1 E\n and thus\n                                        Var[\u2206i|Fi] \u2264          \u03b1 E[|\u2206i| |Fi] \u2272         \u03b1 E [\u2206i|Fi] .\n A.3      Concentration of potential\nWe have showed that the potential will grow at \u2126                            1     per iteration, but only in expectation, while\n                                                                           m\u2217\n our goal is to obtain a high probability bound. Let \u00b5k :=  i<k E[\u2206i|Fi\u22121] \u2273                                       k/m\u2217, then\n               E [(\u03c8k \u2212      \u00b5k) \u2212    (\u03c8k\u22121 \u2212       \u00b5k\u22121) |Fk\u22121] = E [\u03c8k \u2212               \u03c8k\u22121|Fk\u22121] \u2212          (\u00b5k \u2212     \u00b5k\u22121)\n                                                                         = E [\u2206i|Fi] \u2212         E [\u2206i|Fi] \u2265        0.\n Apparently |\u03c8k \u2212          \u00b5k| is upper bounded, so \u03c8k \u2212\u00b5k is a supermartingale. To show a high probability\n bound, we will use Freedman\u2019s inequality. A version is stated in Tropp [2011]. We slighted modify\n it so it can be applied to supermartingale as the following. (XXX Not sure if the following is correct.\n I can\u2019t find a version of supermartingale.)\n Theorem           A.5        (Freedman\u2019s            Inequality).         Consider          a      real-valued          supermartingale\n {Yk : k = 0, 1, 2, \u00b7 \u00b7 \u00b7 } that is adapted to the filtration F0 \u2286                               F1 \u2286        F2 \u2286        \u00b7 \u00b7 \u00b7 \u2286     F with\n difference sequence {Xk : k = 1, 2, 3, \u00b7 \u00b7 \u00b7 }.                     Assume that the difference sequence is uniformly\n bounded:\n                                           Xk \u2264     R almost surely for k = 1, 2, 3, \u00b7 \u00b7 \u00b7\n Define the predictable quadratic variation process of the supermartingale:\n                                                     k\nThen, for all t \u2265        0 and \u03c32 > 0,  Wk :=      j=1   E   X2 j |Fj\u22121       for k = 1, 2, 3, \u00b7 \u00b7 \u00b7\n                          Pr    \u2203k \u2265     0 : Yk \u2264     \u2212t and Wk \u2264           \u03c32    \u2264   exp      \u2212       t2/2           .\n                                                                                                  \u03c32 + Rt/3\n                                                                       16", "md": "It then map the solution back by returning the closest hypothesis in \\(H'\\). Hence, \\(m^* \\geq m^* H', DX, c4\\eta, c5\\epsilon, \\frac{99}{100}\\). Therefore,\n\\[E [\\Delta_i|F_i] - 2\\alpha\\eta \\max_x q(x) E x q(x) \\geq \\alpha DX(x) \\geq 1.8\\alpha x \\sim q[ri,Si(x)] - 8.1\\eta \\max DX(x) m^*.\\]\n\nWe now bound the variance. The value of \\(\\Delta_i\\) may be positive or negative, but it is bounded by \\(|\\Delta_i| \\leq \\alpha\\). Thus\n\\[Var[\\Delta_i|F_i] \\leq E \\Delta^2_i |F_i \\leq \\alpha E[|\\Delta_i| |F_i].\\]\nBy Lemma A.1 and (7) we have\n\\[E[|\\Delta_i| |F_i] = E[2 \\max \\{\\Delta_i, 0\\} - E \\Delta_i|F_i q(x) \\leq 4\\alpha E x \\sim q [ri,Si(x)] - 1.8\\alpha x \\sim q[ri,Si(x)] - 8.1\\eta \\max x DX(x) \\leq 2.2\\alpha E q(x) x \\sim q[ri,S,h^*(x)] + 6.7\\eta \\max x DX(x) \\leq 2.2\\alpha x q(x) x q(x) 1.8\\alpha E [\\Delta_i|F_i] + 2.2\\alpha \\cdot 6.9\\eta \\max q(x) D X(x) + 2.2\\alpha \\cdot 6.7\\eta \\max DX(x) \\leq 1.3 E[\\Delta_i|F_i] + 30\\alpha\\eta \\max x DX(x) q(x) 1.\\]\nSince \\(E x \\sim q[\\Delta_i|F_i] - 2\\alpha\\eta \\max x DX(x) \\geq m^* \\geq 0\\), we have\n\\[\\eta \\max x DX(x) \\leq \\frac{1}{2\\alpha} E x \\sim q [\\Delta_i|F_i],\\]\nand thus\n\\[Var[\\Delta_i|F_i] \\leq \\alpha E[|\\Delta_i| |F_i] \\lesssim \\alpha E [\\Delta_i|F_i].\n\nA.3 Concentration of potential\n\nWe have showed that the potential will grow at \\(\\Omega \\frac{1}{m^*}\\) per iteration, but only in expectation, while our goal is to obtain a high probability bound. Let \\(\\mu_k := \\sum_{i<k} E[\\Delta_i|F_{i-1}] \\gtrsim \\frac{k}{m^*}\\), then\n\\[E [(\\psi_k - \\mu_k) - (\\psi_{k-1} - \\mu_{k-1}) |F_{k-1}] = E [\\psi_k - \\psi_{k-1}|F_{k-1}] - (\\mu_k - \\mu_{k-1}) = E [\\Delta_i|F_i] - E [\\Delta_i|F_i] \\geq 0.\\]\nApparently \\(|\\psi_k - \\mu_k|\\) is upper bounded, so \\(\\psi_k - \\mu_k\\) is a supermartingale. To show a high probability bound, we will use Freedman\u2019s inequality. A version is stated in Tropp [2011]. We slighted modify it so it can be applied to supermartingale as the following. (XXX Not sure if the following is correct. I can\u2019t find a version of supermartingale.)\n\nTheorem A.5 (Freedman\u2019s Inequality). Consider a real-valued supermartingale \\(\\{Y_k : k = 0, 1, 2, \\ldots \\}\\) that is adapted to the filtration \\(F_0 \\subseteq F_1 \\subseteq F_2 \\subseteq \\ldots \\subseteq F\\) with difference sequence \\(\\{X_k : k = 1, 2, 3, \\ldots \\}\\). Assume that the difference sequence is uniformly bounded:\n\\[X_k \\leq R \\text{ almost surely for } k = 1, 2, 3, \\ldots\\]\nDefine the predictable quadratic variation process of the supermartingale:\n\\[W_k := \\sum_{j=1}^k E X^2_j |F_{j-1} \\text{ for } k = 1, 2, 3, \\ldots\\]\nThen, for all \\(t \\geq 0\\) and \\(\\sigma^2 > 0\\),\n\\[Pr \\left\\{ \\exists k \\geq 0 : Y_k \\leq -t \\text{ and } W_k \\leq \\sigma^2 \\right\\} \\leq \\exp \\left( -\\frac{t^2}{2\\sigma^2 + Rt/3} \\right).\\]", "images": [], "items": [{"type": "text", "value": "It then map the solution back by returning the closest hypothesis in \\(H'\\). Hence, \\(m^* \\geq m^* H', DX, c4\\eta, c5\\epsilon, \\frac{99}{100}\\). Therefore,\n\\[E [\\Delta_i|F_i] - 2\\alpha\\eta \\max_x q(x) E x q(x) \\geq \\alpha DX(x) \\geq 1.8\\alpha x \\sim q[ri,Si(x)] - 8.1\\eta \\max DX(x) m^*.\\]\n\nWe now bound the variance. The value of \\(\\Delta_i\\) may be positive or negative, but it is bounded by \\(|\\Delta_i| \\leq \\alpha\\). Thus\n\\[Var[\\Delta_i|F_i] \\leq E \\Delta^2_i |F_i \\leq \\alpha E[|\\Delta_i| |F_i].\\]\nBy Lemma A.1 and (7) we have\n\\[E[|\\Delta_i| |F_i] = E[2 \\max \\{\\Delta_i, 0\\} - E \\Delta_i|F_i q(x) \\leq 4\\alpha E x \\sim q [ri,Si(x)] - 1.8\\alpha x \\sim q[ri,Si(x)] - 8.1\\eta \\max x DX(x) \\leq 2.2\\alpha E q(x) x \\sim q[ri,S,h^*(x)] + 6.7\\eta \\max x DX(x) \\leq 2.2\\alpha x q(x) x q(x) 1.8\\alpha E [\\Delta_i|F_i] + 2.2\\alpha \\cdot 6.9\\eta \\max q(x) D X(x) + 2.2\\alpha \\cdot 6.7\\eta \\max DX(x) \\leq 1.3 E[\\Delta_i|F_i] + 30\\alpha\\eta \\max x DX(x) q(x) 1.\\]\nSince \\(E x \\sim q[\\Delta_i|F_i] - 2\\alpha\\eta \\max x DX(x) \\geq m^* \\geq 0\\), we have\n\\[\\eta \\max x DX(x) \\leq \\frac{1}{2\\alpha} E x \\sim q [\\Delta_i|F_i],\\]\nand thus\n\\[Var[\\Delta_i|F_i] \\leq \\alpha E[|\\Delta_i| |F_i] \\lesssim \\alpha E [\\Delta_i|F_i].\n\nA.3 Concentration of potential\n\nWe have showed that the potential will grow at \\(\\Omega \\frac{1}{m^*}\\) per iteration, but only in expectation, while our goal is to obtain a high probability bound. Let \\(\\mu_k := \\sum_{i<k} E[\\Delta_i|F_{i-1}] \\gtrsim \\frac{k}{m^*}\\), then\n\\[E [(\\psi_k - \\mu_k) - (\\psi_{k-1} - \\mu_{k-1}) |F_{k-1}] = E [\\psi_k - \\psi_{k-1}|F_{k-1}] - (\\mu_k - \\mu_{k-1}) = E [\\Delta_i|F_i] - E [\\Delta_i|F_i] \\geq 0.\\]\nApparently \\(|\\psi_k - \\mu_k|\\) is upper bounded, so \\(\\psi_k - \\mu_k\\) is a supermartingale. To show a high probability bound, we will use Freedman\u2019s inequality. A version is stated in Tropp [2011]. We slighted modify it so it can be applied to supermartingale as the following. (XXX Not sure if the following is correct. I can\u2019t find a version of supermartingale.)\n\nTheorem A.5 (Freedman\u2019s Inequality). Consider a real-valued supermartingale \\(\\{Y_k : k = 0, 1, 2, \\ldots \\}\\) that is adapted to the filtration \\(F_0 \\subseteq F_1 \\subseteq F_2 \\subseteq \\ldots \\subseteq F\\) with difference sequence \\(\\{X_k : k = 1, 2, 3, \\ldots \\}\\). Assume that the difference sequence is uniformly bounded:\n\\[X_k \\leq R \\text{ almost surely for } k = 1, 2, 3, \\ldots\\]\nDefine the predictable quadratic variation process of the supermartingale:\n\\[W_k := \\sum_{j=1}^k E X^2_j |F_{j-1} \\text{ for } k = 1, 2, 3, \\ldots\\]\nThen, for all \\(t \\geq 0\\) and \\(\\sigma^2 > 0\\),\n\\[Pr \\left\\{ \\exists k \\geq 0 : Y_k \\leq -t \\text{ and } W_k \\leq \\sigma^2 \\right\\} \\leq \\exp \\left( -\\frac{t^2}{2\\sigma^2 + Rt/3} \\right).\\]", "md": "It then map the solution back by returning the closest hypothesis in \\(H'\\). Hence, \\(m^* \\geq m^* H', DX, c4\\eta, c5\\epsilon, \\frac{99}{100}\\). Therefore,\n\\[E [\\Delta_i|F_i] - 2\\alpha\\eta \\max_x q(x) E x q(x) \\geq \\alpha DX(x) \\geq 1.8\\alpha x \\sim q[ri,Si(x)] - 8.1\\eta \\max DX(x) m^*.\\]\n\nWe now bound the variance. The value of \\(\\Delta_i\\) may be positive or negative, but it is bounded by \\(|\\Delta_i| \\leq \\alpha\\). Thus\n\\[Var[\\Delta_i|F_i] \\leq E \\Delta^2_i |F_i \\leq \\alpha E[|\\Delta_i| |F_i].\\]\nBy Lemma A.1 and (7) we have\n\\[E[|\\Delta_i| |F_i] = E[2 \\max \\{\\Delta_i, 0\\} - E \\Delta_i|F_i q(x) \\leq 4\\alpha E x \\sim q [ri,Si(x)] - 1.8\\alpha x \\sim q[ri,Si(x)] - 8.1\\eta \\max x DX(x) \\leq 2.2\\alpha E q(x) x \\sim q[ri,S,h^*(x)] + 6.7\\eta \\max x DX(x) \\leq 2.2\\alpha x q(x) x q(x) 1.8\\alpha E [\\Delta_i|F_i] + 2.2\\alpha \\cdot 6.9\\eta \\max q(x) D X(x) + 2.2\\alpha \\cdot 6.7\\eta \\max DX(x) \\leq 1.3 E[\\Delta_i|F_i] + 30\\alpha\\eta \\max x DX(x) q(x) 1.\\]\nSince \\(E x \\sim q[\\Delta_i|F_i] - 2\\alpha\\eta \\max x DX(x) \\geq m^* \\geq 0\\), we have\n\\[\\eta \\max x DX(x) \\leq \\frac{1}{2\\alpha} E x \\sim q [\\Delta_i|F_i],\\]\nand thus\n\\[Var[\\Delta_i|F_i] \\leq \\alpha E[|\\Delta_i| |F_i] \\lesssim \\alpha E [\\Delta_i|F_i].\n\nA.3 Concentration of potential\n\nWe have showed that the potential will grow at \\(\\Omega \\frac{1}{m^*}\\) per iteration, but only in expectation, while our goal is to obtain a high probability bound. Let \\(\\mu_k := \\sum_{i<k} E[\\Delta_i|F_{i-1}] \\gtrsim \\frac{k}{m^*}\\), then\n\\[E [(\\psi_k - \\mu_k) - (\\psi_{k-1} - \\mu_{k-1}) |F_{k-1}] = E [\\psi_k - \\psi_{k-1}|F_{k-1}] - (\\mu_k - \\mu_{k-1}) = E [\\Delta_i|F_i] - E [\\Delta_i|F_i] \\geq 0.\\]\nApparently \\(|\\psi_k - \\mu_k|\\) is upper bounded, so \\(\\psi_k - \\mu_k\\) is a supermartingale. To show a high probability bound, we will use Freedman\u2019s inequality. A version is stated in Tropp [2011]. We slighted modify it so it can be applied to supermartingale as the following. (XXX Not sure if the following is correct. I can\u2019t find a version of supermartingale.)\n\nTheorem A.5 (Freedman\u2019s Inequality). Consider a real-valued supermartingale \\(\\{Y_k : k = 0, 1, 2, \\ldots \\}\\) that is adapted to the filtration \\(F_0 \\subseteq F_1 \\subseteq F_2 \\subseteq \\ldots \\subseteq F\\) with difference sequence \\(\\{X_k : k = 1, 2, 3, \\ldots \\}\\). Assume that the difference sequence is uniformly bounded:\n\\[X_k \\leq R \\text{ almost surely for } k = 1, 2, 3, \\ldots\\]\nDefine the predictable quadratic variation process of the supermartingale:\n\\[W_k := \\sum_{j=1}^k E X^2_j |F_{j-1} \\text{ for } k = 1, 2, 3, \\ldots\\]\nThen, for all \\(t \\geq 0\\) and \\(\\sigma^2 > 0\\),\n\\[Pr \\left\\{ \\exists k \\geq 0 : Y_k \\leq -t \\text{ and } W_k \\leq \\sigma^2 \\right\\} \\leq \\exp \\left( -\\frac{t^2}{2\\sigma^2 + Rt/3} \\right).\\]"}]}, {"page": 17, "text": "Then we can prove a high probability bound as the following.\nLemma A.6. With probability 1 \u2212                         \u03b4, \u03d5i = 0 for some i = O                   m\u2217    log |H| \u03b4     so h\u2217     \u2208   Si.\nProof. Remember we have that\n                                              \u03d5k = \u03d50 + \u03c8k +                    log \u03bbi,H\\Si+1(h\u2217)\n                                                                          i<k           \u03bb i,H\\Si(h\u2217) .\nSince Si+1 \u2287           Si for all i, \u03bbi,H\\Si+1(h\u2217) \u2265                 \u03bbi,H\\Si(h\u2217) if h\u2217            /\n                                                       \u03d5k \u2265      \u03d50 + \u03c8k           if h\u2217    /     \u2208  Si+1, we have\n                                                                                            \u2208  Sk.\n                                         . Let\u2019s assume by contradiction that \u03d5K < 0 for for, then h\u2217                                          /\nLet K = O              m\u2217   log |H| \u03b4                                                                                                          \u2208   Si for\ni \u2264    K. We know by Lemma A.4 that                    \u00b5k :=            E[\u2206i|Fi\u22121] \u2273             k\nand that  i<k Var [\u2206i] \u2264                    1                    i<k                            m\u2217\nFreedman\u2019s inequality, let\u2019s set the RHS    4\u00b5k by picking \u03b1 small enough. Moreover, |\u2206i| \u2264                                     \u03b1 always. To use\n                                                         exp       \u2212       t2  /2           \u2264   \u03b4.\nSolving the above quadratic equation, one solution is that t \u2265        \u03c32 + Rt/3                     R3 log 1  \u03b4 +     R2  9 log2 1    \u03b4 + 2\u03c32 log 1       \u03b4 .\nLet\u2019s substitute in R = \u03b1 and \u03c32 =  i<k Vari\u22121(\u2206i), with 1 \u2212                                                 \u03b4 probability we have for any\nk > O(m\u2217          log 1 \u03b4 ) that\n                               \u03c8k \u2265      \u00b5k \u2212          \u03b12                           Var\n                                                        9 log2 1    \u03b4 + 2     i<k    i\u22121(\u2206i) log 1       \u03b4 \u2212     \u03b13 log 1 \u03b4\n                                     \u2265   \u00b5k \u2212      \u03b12   9 log2 1    \u03b4 + 1   2\u00b5k log 1    \u03b4 \u2212     \u03b13 log 1  \u03b4\n                                     \u2265   \u00b5k \u2212     max      \u221a     2\u03b1    log 1         \u00b5 k log 1         \u2212   \u03b1\n                                                                 3           \u03b4 ,                \u03b4          3 log 1   \u03b4\n                                     \u2265    1\n                                          2\u00b5k\n                                     \u2273     k\n                                          m\u2217   .\nThe second last inequality is because the first term outscales all of the rest.                                                         Since K          =\nO     m\u2217    log |H| \u03b4     , we have                               \u03c8K \u2265       2 log |H|            1\nwith 1\u2212\u03b4 probability. Then \u03d5K \u2265                       \u03d50+\u03c8k because \u03d50 \u2265                  log   2|H| \u2265      \u22122 log |H| and this contradicts\nh\u2217   \u2208/  SK. Therefore, with probability at least 1 \u2212                            \u03b4, h\u2217    \u2208   SK and by definition, \u03d5i = 0 for some\ni \u2264    K as desired.\nA.4       Bounding the Size of |C|\nSo far we\u2019ve shown that after O                      m\u2217    log |H| \u03b4      iterations, h\u2217        will be included in the set Si. The last\nthing we need to prove Theorem 1.1 is that with high probability, C is small, which is equivalent to\nshow that not many balls will be added to Si after O                                 m\u2217    log |H| \u03b4      iterations. To show this, we first\nneed to relate the number of balls added to Si to \u03c8i. Let Ei denote the number of errors h\u2217                                                    made up\nto iteration i (and set Ei = Ei\u22121 if h\u2217                       \u2208   Si) and Ni denote the number of balls added to Si up to\niteration i (again set Ni = Ni\u22121 if h\u2217                      \u2208   Si).\n                                                                            17", "md": "# Math Equations\n\nThen we can prove a high probability bound as the following.\n\nLemma A.6. With probability $$1 - \\delta$$, $$\\phi_i = 0$$ for some $$i = O(m^* \\log |H| \\delta)$$ so $$h^* \\in S_i$$.\n\nProof. Remember we have that\n\n$$\\phi_k = \\phi_0 + \\psi_k + \\log \\lambda_{i,H\\backslash S_i+1}(h^*) - \\lambda_{i,H\\backslash S_i}(h^*)$$.\n\nSince $$S_i+1 \\supseteq S_i$$ for all $$i$$, $$\\lambda_{i,H\\backslash S_i+1}(h^*) \\geq \\lambda_{i,H\\backslash S_i}(h^*)$$ if $$h^* \\notin S_i+1$$, we have $$\\in S_k$$.\n\nLet\u2019s assume by contradiction that $$\\phi_K < 0$$ for for, then $$h^*$$\n\nLet $$K = O(m^* \\log |H| \\delta)$$ $$\\in S_i$$ for $$i \\leq K$$. We know by Lemma A.4 that $$\\mu_k := E[\\Delta_i|F_{i-1}] \\gtrsim k$$ and that $$\\sum_{i<k} \\text{Var}[\\Delta_i] \\leq \\frac{1}{m^*}$$ Freedman\u2019s inequality, let\u2019s set the RHS $$4\\mu_k$$ by picking $$\\alpha$$ small enough. Moreover, $$|\\Delta_i| \\leq \\alpha$$ always. To use\n\n$$\\exp(-t^2/2) \\leq \\delta$$.\n\nSolving the above quadratic equation, one solution is that $$t \\geq \\sigma^2 + Rt/3 + R^3 \\log \\frac{1}{\\delta} + R^2 \\frac{9}{\\log^2} \\frac{1}{\\delta} + 2\\sigma^2 \\log \\frac{1}{\\delta}$$.\n\nLet\u2019s substitute in $$R = \\alpha$$ and $$\\sigma^2 = \\sum_{i<k} \\text{Var}[\\Delta_i]$$, with $$1 - \\delta$$ probability we have for any\n\n$$k > O(m^* \\log \\frac{1}{\\delta})$$ that\n\n$$\\psi_k \\geq \\mu_k - \\alpha^2 \\frac{\\text{Var}}{9 \\log^2 \\frac{1}{\\delta} + 2 \\sum_{i<k} \\text{Var}[\\Delta_i] \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\mu_k - \\alpha^2 \\frac{9 \\log^2 \\frac{1}{\\delta} + \\frac{1}{2}\\mu_k \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\mu_k - \\max\\left(\\sqrt{2\\alpha \\log \\frac{1}{\\mu_k \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}}, \\frac{\\delta}{3} \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\frac{1}{2}\\mu_k \\gtrsim k$$\n\nThe second last inequality is because the first term outscales all of the rest. Since $$K = O(m^* \\log |H| \\delta)$$, we have $$\\psi_K \\geq 2 \\log |H| \\frac{1}{1 - \\delta}$$ with $$1-\\delta$$ probability. Then $$\\phi_K \\geq \\phi_0 + \\psi_k$$ because $$\\phi_0 \\geq \\log 2|H| \\geq -2 \\log |H|$$ and this contradicts $$h^* \\notin S_K$$. Therefore, with probability at least $$1 - \\delta$$, $$h^* \\in S_K$$ and by definition, $$\\phi_i = 0$$ for some $$i \\leq K$$ as desired.\n\nA.4 Bounding the Size of |C|\n\nSo far we\u2019ve shown that after $$O(m^* \\log |H| \\delta)$$ iterations, $$h^*$$ will be included in the set $$S_i$$. The last\n\nthing we need to prove Theorem 1.1 is that with high probability, C is small, which is equivalent to\n\nshow that not many balls will be added to $$S_i$$ after $$O(m^* \\log |H| \\delta)$$ iterations. To show this, we first\n\nneed to relate the number of balls added to $$S_i$$ to $$\\psi_i$$. Let $$E_i$$ denote the number of errors $$h^*$$\n\nmade up to iteration $$i$$ (and set $$E_i = E_{i-1}$$ if $$h^* \\in S_i$$) and $$N_i$$ denote the number of balls added to $$S_i$$ up to\n\niteration $$i$$ (again set $$N_i = N_{i-1}$$ if $$h^* \\in S_i$$).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Then we can prove a high probability bound as the following.\n\nLemma A.6. With probability $$1 - \\delta$$, $$\\phi_i = 0$$ for some $$i = O(m^* \\log |H| \\delta)$$ so $$h^* \\in S_i$$.\n\nProof. Remember we have that\n\n$$\\phi_k = \\phi_0 + \\psi_k + \\log \\lambda_{i,H\\backslash S_i+1}(h^*) - \\lambda_{i,H\\backslash S_i}(h^*)$$.\n\nSince $$S_i+1 \\supseteq S_i$$ for all $$i$$, $$\\lambda_{i,H\\backslash S_i+1}(h^*) \\geq \\lambda_{i,H\\backslash S_i}(h^*)$$ if $$h^* \\notin S_i+1$$, we have $$\\in S_k$$.\n\nLet\u2019s assume by contradiction that $$\\phi_K < 0$$ for for, then $$h^*$$\n\nLet $$K = O(m^* \\log |H| \\delta)$$ $$\\in S_i$$ for $$i \\leq K$$. We know by Lemma A.4 that $$\\mu_k := E[\\Delta_i|F_{i-1}] \\gtrsim k$$ and that $$\\sum_{i<k} \\text{Var}[\\Delta_i] \\leq \\frac{1}{m^*}$$ Freedman\u2019s inequality, let\u2019s set the RHS $$4\\mu_k$$ by picking $$\\alpha$$ small enough. Moreover, $$|\\Delta_i| \\leq \\alpha$$ always. To use\n\n$$\\exp(-t^2/2) \\leq \\delta$$.\n\nSolving the above quadratic equation, one solution is that $$t \\geq \\sigma^2 + Rt/3 + R^3 \\log \\frac{1}{\\delta} + R^2 \\frac{9}{\\log^2} \\frac{1}{\\delta} + 2\\sigma^2 \\log \\frac{1}{\\delta}$$.\n\nLet\u2019s substitute in $$R = \\alpha$$ and $$\\sigma^2 = \\sum_{i<k} \\text{Var}[\\Delta_i]$$, with $$1 - \\delta$$ probability we have for any\n\n$$k > O(m^* \\log \\frac{1}{\\delta})$$ that\n\n$$\\psi_k \\geq \\mu_k - \\alpha^2 \\frac{\\text{Var}}{9 \\log^2 \\frac{1}{\\delta} + 2 \\sum_{i<k} \\text{Var}[\\Delta_i] \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\mu_k - \\alpha^2 \\frac{9 \\log^2 \\frac{1}{\\delta} + \\frac{1}{2}\\mu_k \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\mu_k - \\max\\left(\\sqrt{2\\alpha \\log \\frac{1}{\\mu_k \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}}, \\frac{\\delta}{3} \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\frac{1}{2}\\mu_k \\gtrsim k$$\n\nThe second last inequality is because the first term outscales all of the rest. Since $$K = O(m^* \\log |H| \\delta)$$, we have $$\\psi_K \\geq 2 \\log |H| \\frac{1}{1 - \\delta}$$ with $$1-\\delta$$ probability. Then $$\\phi_K \\geq \\phi_0 + \\psi_k$$ because $$\\phi_0 \\geq \\log 2|H| \\geq -2 \\log |H|$$ and this contradicts $$h^* \\notin S_K$$. Therefore, with probability at least $$1 - \\delta$$, $$h^* \\in S_K$$ and by definition, $$\\phi_i = 0$$ for some $$i \\leq K$$ as desired.\n\nA.4 Bounding the Size of |C|\n\nSo far we\u2019ve shown that after $$O(m^* \\log |H| \\delta)$$ iterations, $$h^*$$ will be included in the set $$S_i$$. The last\n\nthing we need to prove Theorem 1.1 is that with high probability, C is small, which is equivalent to\n\nshow that not many balls will be added to $$S_i$$ after $$O(m^* \\log |H| \\delta)$$ iterations. To show this, we first\n\nneed to relate the number of balls added to $$S_i$$ to $$\\psi_i$$. Let $$E_i$$ denote the number of errors $$h^*$$\n\nmade up to iteration $$i$$ (and set $$E_i = E_{i-1}$$ if $$h^* \\in S_i$$) and $$N_i$$ denote the number of balls added to $$S_i$$ up to\n\niteration $$i$$ (again set $$N_i = N_{i-1}$$ if $$h^* \\in S_i$$).", "md": "Then we can prove a high probability bound as the following.\n\nLemma A.6. With probability $$1 - \\delta$$, $$\\phi_i = 0$$ for some $$i = O(m^* \\log |H| \\delta)$$ so $$h^* \\in S_i$$.\n\nProof. Remember we have that\n\n$$\\phi_k = \\phi_0 + \\psi_k + \\log \\lambda_{i,H\\backslash S_i+1}(h^*) - \\lambda_{i,H\\backslash S_i}(h^*)$$.\n\nSince $$S_i+1 \\supseteq S_i$$ for all $$i$$, $$\\lambda_{i,H\\backslash S_i+1}(h^*) \\geq \\lambda_{i,H\\backslash S_i}(h^*)$$ if $$h^* \\notin S_i+1$$, we have $$\\in S_k$$.\n\nLet\u2019s assume by contradiction that $$\\phi_K < 0$$ for for, then $$h^*$$\n\nLet $$K = O(m^* \\log |H| \\delta)$$ $$\\in S_i$$ for $$i \\leq K$$. We know by Lemma A.4 that $$\\mu_k := E[\\Delta_i|F_{i-1}] \\gtrsim k$$ and that $$\\sum_{i<k} \\text{Var}[\\Delta_i] \\leq \\frac{1}{m^*}$$ Freedman\u2019s inequality, let\u2019s set the RHS $$4\\mu_k$$ by picking $$\\alpha$$ small enough. Moreover, $$|\\Delta_i| \\leq \\alpha$$ always. To use\n\n$$\\exp(-t^2/2) \\leq \\delta$$.\n\nSolving the above quadratic equation, one solution is that $$t \\geq \\sigma^2 + Rt/3 + R^3 \\log \\frac{1}{\\delta} + R^2 \\frac{9}{\\log^2} \\frac{1}{\\delta} + 2\\sigma^2 \\log \\frac{1}{\\delta}$$.\n\nLet\u2019s substitute in $$R = \\alpha$$ and $$\\sigma^2 = \\sum_{i<k} \\text{Var}[\\Delta_i]$$, with $$1 - \\delta$$ probability we have for any\n\n$$k > O(m^* \\log \\frac{1}{\\delta})$$ that\n\n$$\\psi_k \\geq \\mu_k - \\alpha^2 \\frac{\\text{Var}}{9 \\log^2 \\frac{1}{\\delta} + 2 \\sum_{i<k} \\text{Var}[\\Delta_i] \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\mu_k - \\alpha^2 \\frac{9 \\log^2 \\frac{1}{\\delta} + \\frac{1}{2}\\mu_k \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\mu_k - \\max\\left(\\sqrt{2\\alpha \\log \\frac{1}{\\mu_k \\log \\frac{1}{\\delta} - \\alpha^3 \\log \\frac{1}{\\delta}}, \\frac{\\delta}{3} \\log \\frac{1}{\\delta}$$\n\n$$\\geq \\frac{1}{2}\\mu_k \\gtrsim k$$\n\nThe second last inequality is because the first term outscales all of the rest. Since $$K = O(m^* \\log |H| \\delta)$$, we have $$\\psi_K \\geq 2 \\log |H| \\frac{1}{1 - \\delta}$$ with $$1-\\delta$$ probability. Then $$\\phi_K \\geq \\phi_0 + \\psi_k$$ because $$\\phi_0 \\geq \\log 2|H| \\geq -2 \\log |H|$$ and this contradicts $$h^* \\notin S_K$$. Therefore, with probability at least $$1 - \\delta$$, $$h^* \\in S_K$$ and by definition, $$\\phi_i = 0$$ for some $$i \\leq K$$ as desired.\n\nA.4 Bounding the Size of |C|\n\nSo far we\u2019ve shown that after $$O(m^* \\log |H| \\delta)$$ iterations, $$h^*$$ will be included in the set $$S_i$$. The last\n\nthing we need to prove Theorem 1.1 is that with high probability, C is small, which is equivalent to\n\nshow that not many balls will be added to $$S_i$$ after $$O(m^* \\log |H| \\delta)$$ iterations. To show this, we first\n\nneed to relate the number of balls added to $$S_i$$ to $$\\psi_i$$. Let $$E_i$$ denote the number of errors $$h^*$$\n\nmade up to iteration $$i$$ (and set $$E_i = E_{i-1}$$ if $$h^* \\in S_i$$) and $$N_i$$ denote the number of balls added to $$S_i$$ up to\n\niteration $$i$$ (again set $$N_i = N_{i-1}$$ if $$h^* \\in S_i$$)."}]}, {"page": 18, "text": " Lemma A.7. The following inequality holds for every i:\n                                                      Ni \u2264     5(\u03c8i + 2\u03b1Ei) + 1.\n Proof. We divide the i iterations into phases. A new phase begins and an old phase ends if at\n this iteration a new ball is added to the set Si. We use p1, . . . , pk for k \u2264                                     i to denote phases\n and i1, . . . , ik to denote the starting iteration of the phases. We analyse how the potential changes\n from the phase pj to the phase pj+1. Let\u2019s say the ball B2 = (\u00b52, 3c4\u03b7 + 3c5\u03b5) is added at the\n beginning of pj+1 and B1 = (\u00b51, 3c4\u03b7 + 3c5\u03b5) is the ball added at the beginning of pj. Then the\n ball B\u2032  2 = (\u00b52, c4\u03b7 + c5\u03b5) and the ball B\u2032                1 = (\u00b51, c4\u03b7 + c5\u03b5) are disjoint. Otherwise, B\u2032                     2 \u2286    B1 so\n B2 would not have been added by the algorithm. At the beginning of pj, B\u2032                                   1 has probability no less\n than 0.6 by Claim A.3. Therefore, B\u2032                 2 has probability no more than 0.4. Similarly, at the beginning\n of pj+1, B\u2032     2 has probability at least 0.6 by Claim A.3. Since during one iteration the weight of a\n hypothesis cannot change too much, at iteration ij+1 \u2212                           1, B\u2032  2 has weight at least 0.5 by picking \u03b1\n small enough. Therefore, we have log \u03bbi                    j+1\u22121(B\u2032    2) \u2212    log \u03bbij(B\u2032    2) \u2265    log 0.5\n                                                                                                            0.4 \u2265     1\n                                                                                                                      5. Moreover, note\n that Si does not change from iteration ij to iteration ij+1 \u2212                          1 by the definition of phases. Now we\n compute\n        ij+1\u22121    \u2206l = log \u03bbij+1\u22121(h\u2217)              + log    \u03bbij+1\u22121,H\\Sij (h\u2217)         ,\n          l=ij                      \u03bbi j(h\u2217)                    \u03bbij,H\\Sij (h\u2217)\n                       = log wij+1\u22121(h\u2217)                   h\u2208H wi1(h)                                            wij(H \\ Sij)\n The change of the weight of h\u2217     wi1(h\u2217)    is       h\u2208H wij+1\u22121(h) + log wij+1\u22121(h\u2217)     wij(h\u2217)         wij+1\u22121(H \\ Sij).\n                                                         wij+1(h\u2217)       = e\u2212\u03b1Epj ,\n                                                          wi  j(h\u2217)\n where Epj is the number of errors h\u2217                 made in pj. Consequently,\n                    ij+1\u22121   \u2206l = \u22122\u03b1Epj + log                      h\u2208H wij(h)                    w   wij(H \\ Sij)\n                     l=ij          \u2265  \u22122\u03b1Epj + 1       5.       h\u2208H wij+1\u22121(h) + log                 ij+1\u22121(H \\ Sij)\n The last step above comes from\n  log          h\u2208H wij(h)                         h\u2208B\u2032  2 wij+1\u22121(h)               h\u2208H wij(h)                         \u03bbi           2)  \u2265   1\n and        h\u2208H wij+1\u22121(h) \u2265             log          h\u2208B\u2032 2 wij(h)             h\u2208H wij+1\u22121(h) = log \u03bbij+1\u22121 (B\u2032         j (B\u2032 2)          5,\n                                                              wi  j(H \\ Sij)\n                                                     log   wij+1\u22121(H \\ Sij) \u2265             0\n because the weight w(h) only decreases. Summing over all phases j and we get\n                                                    \u03c8i \u2265    \u22122\u03b1Ei + 1      5 (Ni \u2212      1) .\n Since i may not exactly be the end of a phase, the last phase may end early so we have Ni\u22121 instead\n of Ni. Rearrange and the proof finishes.\nWe have already bounded \u03c8i, so we just need to bound Ei in order to bound Ni by the following\n lemma.\n Lemma A.8. For every k, with probability at least 1 \u2212                           \u03b4,\n                                                    Ek \u2264     1     \u03c8k +    \u221a  2 log 1       .\n                                                             \u03b1                        \u03b4\n                                                                       18", "md": "# Mathematical Inequality\n\nLemma A.7. The following inequality holds for every i:\n\n$$\nN_i \\leq 5(\\psi_i + 2\\alpha E_i) + 1.\n$$\nProof. We divide the i iterations into phases. A new phase begins and an old phase ends if at this iteration a new ball is added to the set $S_i$. We use $p_1, ..., p_k$ for $k \\leq i$ to denote phases and $i_1, ..., i_k$ to denote the starting iteration of the phases. We analyze how the potential changes from the phase $p_j$ to the phase $p_{j+1}$. Let\u2019s say the ball $B_2 = (\\mu_2, 3c4\\eta + 3c5\\epsilon)$ is added at the beginning of $p_{j+1}$ and $B_1 = (\\mu_1, 3c4\\eta + 3c5\\epsilon)$ is the ball added at the beginning of $p_j$. Then the ball $B'_2 = (\\mu_2, c4\\eta + c5\\epsilon)$ and the ball $B'_1 = (\\mu_1, c4\\eta + c5\\epsilon)$ are disjoint. Otherwise, $B'_2 \\subseteq B_1$ so $B_2$ would not have been added by the algorithm. At the beginning of $p_j$, $B'_1$ has probability no less than 0.6 by Claim A.3. Therefore, $B'_2$ has probability no more than 0.4. Similarly, at the beginning of $p_{j+1}$, $B'_2$ has probability at least 0.6 by Claim A.3. Since during one iteration the weight of a hypothesis cannot change too much, at iteration $i_{j+1} - 1$, $B'_2$ has weight at least 0.5 by picking $\\alpha$ small enough. Therefore, we have $$\\log \\lambda_{ij+1-1}(B'_2) - \\log \\lambda_{ij}(B'_2) \\geq \\log 0.5/0.4 \\geq 1/5$$. Moreover, note that $S_i$ does not change from iteration $ij$ to iteration $ij+1-1$ by the definition of phases. Now we compute\n\n$$\n\\Delta l = \\log \\lambda_{ij+1-1}(h^*) + \\log \\frac{\\lambda_{ij+1-1,H\\S_i}(h^*)}{\\lambda_{ij}(h^*)} = \\log w_{ij+1-1}(h^*) \\frac{h \\in H}{w_{i1}(h)} \\frac{w_{ij}(H \\ S_i)}{w_{ij+1-1}(H \\ S_i)}.\n$$\nThe change of the weight of $h^*$, $w_{i1}(h^*)$ is $\\frac{w_{ij+1-1}(h^*)}{w_{ij}(h^*)} = e^{-\\alpha E_{pj}}$, where $E_{pj}$ is the number of errors $h^*$ made in $p_j$. Consequently,\n\n$$\n\\Delta l = -2\\alpha E_{pj} + \\log \\frac{h \\in H}{w_{ij}(h)} \\frac{w_{ij+1-1}(H \\ S_i)}{w_{ij+1}(h^*)} \\geq -2\\alpha E_{pj} + 1/5 \\frac{h \\in H}{w_{ij+1-1}(h)} + \\log \\frac{ij+1-1(H \\ S_i)}.\n$$\nThe last step above comes from\n\n$$\n\\log \\frac{h \\in H}{w_{ij}(h)} \\frac{h \\in B'_2}{w_{ij+1-1}(h)} \\frac{h \\in H}{w_{ij}(h)} \\lambda_{ij}(B'_2) \\geq 1\n$$\nand\n\n$$\n\\frac{h \\in H}{w_{ij+1-1}(h)} \\geq \\log \\frac{h \\in B'_2}{w_{ij}(h)} \\frac{h \\in H}{w_{ij+1-1}(h)} = \\log \\lambda_{ij+1-1}(B'_j(B'_2) 5,\n$$\n$$\n\\log \\frac{w_{ij+1-1}(H \\ S_i)} \\geq 0\n$$\nbecause the weight $w(h)$ only decreases. Summing over all phases $j$ and we get\n\n$$\n\\psi_i \\geq -2\\alpha E_i + 1/5 (N_i - 1).\n$$\nSince $i$ may not exactly be the end of a phase, the last phase may end early so we have $N_{i-1}$ instead of $N_i$. Rearrange and the proof finishes.\n\nWe have already bounded $\\psi_i$, so we just need to bound $E_i$ in order to bound $N_i$ by the following lemma.\n\nLemma A.8. For every $k$, with probability at least $1 - \\delta$,\n\n$$\nE_k \\leq \\frac{1}{\\alpha \\psi_k + \\sqrt{2 \\log \\frac{1}{\\delta}}}{18}.\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Mathematical Inequality", "md": "# Mathematical Inequality"}, {"type": "text", "value": "Lemma A.7. The following inequality holds for every i:\n\n$$\nN_i \\leq 5(\\psi_i + 2\\alpha E_i) + 1.\n$$\nProof. We divide the i iterations into phases. A new phase begins and an old phase ends if at this iteration a new ball is added to the set $S_i$. We use $p_1, ..., p_k$ for $k \\leq i$ to denote phases and $i_1, ..., i_k$ to denote the starting iteration of the phases. We analyze how the potential changes from the phase $p_j$ to the phase $p_{j+1}$. Let\u2019s say the ball $B_2 = (\\mu_2, 3c4\\eta + 3c5\\epsilon)$ is added at the beginning of $p_{j+1}$ and $B_1 = (\\mu_1, 3c4\\eta + 3c5\\epsilon)$ is the ball added at the beginning of $p_j$. Then the ball $B'_2 = (\\mu_2, c4\\eta + c5\\epsilon)$ and the ball $B'_1 = (\\mu_1, c4\\eta + c5\\epsilon)$ are disjoint. Otherwise, $B'_2 \\subseteq B_1$ so $B_2$ would not have been added by the algorithm. At the beginning of $p_j$, $B'_1$ has probability no less than 0.6 by Claim A.3. Therefore, $B'_2$ has probability no more than 0.4. Similarly, at the beginning of $p_{j+1}$, $B'_2$ has probability at least 0.6 by Claim A.3. Since during one iteration the weight of a hypothesis cannot change too much, at iteration $i_{j+1} - 1$, $B'_2$ has weight at least 0.5 by picking $\\alpha$ small enough. Therefore, we have $$\\log \\lambda_{ij+1-1}(B'_2) - \\log \\lambda_{ij}(B'_2) \\geq \\log 0.5/0.4 \\geq 1/5$$. Moreover, note that $S_i$ does not change from iteration $ij$ to iteration $ij+1-1$ by the definition of phases. Now we compute\n\n$$\n\\Delta l = \\log \\lambda_{ij+1-1}(h^*) + \\log \\frac{\\lambda_{ij+1-1,H\\S_i}(h^*)}{\\lambda_{ij}(h^*)} = \\log w_{ij+1-1}(h^*) \\frac{h \\in H}{w_{i1}(h)} \\frac{w_{ij}(H \\ S_i)}{w_{ij+1-1}(H \\ S_i)}.\n$$\nThe change of the weight of $h^*$, $w_{i1}(h^*)$ is $\\frac{w_{ij+1-1}(h^*)}{w_{ij}(h^*)} = e^{-\\alpha E_{pj}}$, where $E_{pj}$ is the number of errors $h^*$ made in $p_j$. Consequently,\n\n$$\n\\Delta l = -2\\alpha E_{pj} + \\log \\frac{h \\in H}{w_{ij}(h)} \\frac{w_{ij+1-1}(H \\ S_i)}{w_{ij+1}(h^*)} \\geq -2\\alpha E_{pj} + 1/5 \\frac{h \\in H}{w_{ij+1-1}(h)} + \\log \\frac{ij+1-1(H \\ S_i)}.\n$$\nThe last step above comes from\n\n$$\n\\log \\frac{h \\in H}{w_{ij}(h)} \\frac{h \\in B'_2}{w_{ij+1-1}(h)} \\frac{h \\in H}{w_{ij}(h)} \\lambda_{ij}(B'_2) \\geq 1\n$$\nand\n\n$$\n\\frac{h \\in H}{w_{ij+1-1}(h)} \\geq \\log \\frac{h \\in B'_2}{w_{ij}(h)} \\frac{h \\in H}{w_{ij+1-1}(h)} = \\log \\lambda_{ij+1-1}(B'_j(B'_2) 5,\n$$\n$$\n\\log \\frac{w_{ij+1-1}(H \\ S_i)} \\geq 0\n$$\nbecause the weight $w(h)$ only decreases. Summing over all phases $j$ and we get\n\n$$\n\\psi_i \\geq -2\\alpha E_i + 1/5 (N_i - 1).\n$$\nSince $i$ may not exactly be the end of a phase, the last phase may end early so we have $N_{i-1}$ instead of $N_i$. Rearrange and the proof finishes.\n\nWe have already bounded $\\psi_i$, so we just need to bound $E_i$ in order to bound $N_i$ by the following lemma.\n\nLemma A.8. For every $k$, with probability at least $1 - \\delta$,\n\n$$\nE_k \\leq \\frac{1}{\\alpha \\psi_k + \\sqrt{2 \\log \\frac{1}{\\delta}}}{18}.\n$$", "md": "Lemma A.7. The following inequality holds for every i:\n\n$$\nN_i \\leq 5(\\psi_i + 2\\alpha E_i) + 1.\n$$\nProof. We divide the i iterations into phases. A new phase begins and an old phase ends if at this iteration a new ball is added to the set $S_i$. We use $p_1, ..., p_k$ for $k \\leq i$ to denote phases and $i_1, ..., i_k$ to denote the starting iteration of the phases. We analyze how the potential changes from the phase $p_j$ to the phase $p_{j+1}$. Let\u2019s say the ball $B_2 = (\\mu_2, 3c4\\eta + 3c5\\epsilon)$ is added at the beginning of $p_{j+1}$ and $B_1 = (\\mu_1, 3c4\\eta + 3c5\\epsilon)$ is the ball added at the beginning of $p_j$. Then the ball $B'_2 = (\\mu_2, c4\\eta + c5\\epsilon)$ and the ball $B'_1 = (\\mu_1, c4\\eta + c5\\epsilon)$ are disjoint. Otherwise, $B'_2 \\subseteq B_1$ so $B_2$ would not have been added by the algorithm. At the beginning of $p_j$, $B'_1$ has probability no less than 0.6 by Claim A.3. Therefore, $B'_2$ has probability no more than 0.4. Similarly, at the beginning of $p_{j+1}$, $B'_2$ has probability at least 0.6 by Claim A.3. Since during one iteration the weight of a hypothesis cannot change too much, at iteration $i_{j+1} - 1$, $B'_2$ has weight at least 0.5 by picking $\\alpha$ small enough. Therefore, we have $$\\log \\lambda_{ij+1-1}(B'_2) - \\log \\lambda_{ij}(B'_2) \\geq \\log 0.5/0.4 \\geq 1/5$$. Moreover, note that $S_i$ does not change from iteration $ij$ to iteration $ij+1-1$ by the definition of phases. Now we compute\n\n$$\n\\Delta l = \\log \\lambda_{ij+1-1}(h^*) + \\log \\frac{\\lambda_{ij+1-1,H\\S_i}(h^*)}{\\lambda_{ij}(h^*)} = \\log w_{ij+1-1}(h^*) \\frac{h \\in H}{w_{i1}(h)} \\frac{w_{ij}(H \\ S_i)}{w_{ij+1-1}(H \\ S_i)}.\n$$\nThe change of the weight of $h^*$, $w_{i1}(h^*)$ is $\\frac{w_{ij+1-1}(h^*)}{w_{ij}(h^*)} = e^{-\\alpha E_{pj}}$, where $E_{pj}$ is the number of errors $h^*$ made in $p_j$. Consequently,\n\n$$\n\\Delta l = -2\\alpha E_{pj} + \\log \\frac{h \\in H}{w_{ij}(h)} \\frac{w_{ij+1-1}(H \\ S_i)}{w_{ij+1}(h^*)} \\geq -2\\alpha E_{pj} + 1/5 \\frac{h \\in H}{w_{ij+1-1}(h)} + \\log \\frac{ij+1-1(H \\ S_i)}.\n$$\nThe last step above comes from\n\n$$\n\\log \\frac{h \\in H}{w_{ij}(h)} \\frac{h \\in B'_2}{w_{ij+1-1}(h)} \\frac{h \\in H}{w_{ij}(h)} \\lambda_{ij}(B'_2) \\geq 1\n$$\nand\n\n$$\n\\frac{h \\in H}{w_{ij+1-1}(h)} \\geq \\log \\frac{h \\in B'_2}{w_{ij}(h)} \\frac{h \\in H}{w_{ij+1-1}(h)} = \\log \\lambda_{ij+1-1}(B'_j(B'_2) 5,\n$$\n$$\n\\log \\frac{w_{ij+1-1}(H \\ S_i)} \\geq 0\n$$\nbecause the weight $w(h)$ only decreases. Summing over all phases $j$ and we get\n\n$$\n\\psi_i \\geq -2\\alpha E_i + 1/5 (N_i - 1).\n$$\nSince $i$ may not exactly be the end of a phase, the last phase may end early so we have $N_{i-1}$ instead of $N_i$. Rearrange and the proof finishes.\n\nWe have already bounded $\\psi_i$, so we just need to bound $E_i$ in order to bound $N_i$ by the following lemma.\n\nLemma A.8. For every $k$, with probability at least $1 - \\delta$,\n\n$$\nE_k \\leq \\frac{1}{\\alpha \\psi_k + \\sqrt{2 \\log \\frac{1}{\\delta}}}{18}.\n$$"}]}, {"page": 19, "text": "Proof. Let q be the query distribution at iteration i\u22121 and p(x) be the probability that x is corrupted\nby the adversary. Then the conditional expectation of Ei \u2212                                    Ei\u22121 is                                           q(x)\n    E [Ei \u2212      Ei\u22121|Fi] = Pr      x\u223cq [h\u2217(x) is wrong] = E               x\u223cq [p(x)] =           E       p(x) q(x)           \u2264   \u03b7 max x\nThen if h\u2217        /                                                                             x\u223cD               D(x)                        DX(x).\n                  \u2208  S, from Lemma A.4                                                                          q(x)             1\n                        E[\u2206i \u2212        2\u03b1(Ei \u2212       Ei\u22121)|Fi] \u2265          E [\u2206i|Fi] \u2212          2\u03b1\u03b7 max   x    DX(x) \u2273           m\u2217    .\nTherefore, E[\u03b1 (Ei \u2212              Ei\u22121) |Fi] \u2264           1\n                                                         2 E [\u2206i|Fi] and E [\u2206i \u2212                 \u03b1(Ei \u2212      Ei\u22121)|Fi] \u2265           1\nmeans that \u03c8k \u2212\u03b1Ek \u2212                 1                                                                                             2 E [\u2206i|Fi]. This\n                                     2\u00b5k is a supermartingale. We then bound Var [\u2206i \u2212                                   \u03b1(Ei \u2212      Ei\u22121)|Fi]. Note\nthat |\u2206i \u2212        \u03b1(Ei \u2212      Ei\u22121)| \u2264        2\u03b1, so\n Var[\u2206i \u2212         \u03b1(Ei \u2212      Ei\u22121)|Fi] \u2264          E    (\u2206i \u2212       \u03b1(Ei \u2212      Ei\u22121))2       Fi      \u2264   2\u03b1 E [|\u2206i \u2212         \u03b1(Ei \u2212       Ei\u22121)| |Fi] .\nFurthermore,\n                                                                                                                      q(x)\nAs a result,                 E [|\u2206i \u2212        \u03b1(Ei \u2212      Ei\u22121)| |Fi] \u2264          E [|\u2206i||Fi] + \u03b1\u03b7 max          x     DX(x).\n                        Var[\u2206i \u2212         \u03b1(Ei \u2212       Ei\u22121)|Fi] \u2264          2\u03b1      E [|\u2206i||Fi] + \u03b1\u03b7 max                  q(x)\n                                                                                                                  x    DX(x)\n                                                                      \u2264    2\u03b1      E [|\u2206i||Fi] + 1        2 E [\u2206i|Fi]\n                                                                      \u2264    3\u03b1 E [|\u2206i||Fi]\nBy      picking        \u03b1     small       enough,                      \u2272    \u03b1 E [\u2206i|Fi] .                               \u2264       1               Moreover,\n                                                              i<k Var [\u2206i \u2212          \u03b1(Ei \u2212       Ei\u22121)|Fi]                    8\u00b5k.\n|\u2206i \u2212      \u03b1(Ei \u2212      Ei\u22121)| \u2264         2\u03b1 always. Therefore by Freedman\u2019s inequality, with 1 \u2212                                           \u03b4 probability\nwe have for any k that\n           \u03c8k \u2212     \u03b1Ek \u2265       \u00b5k \u2212          4\u03b12     log2 1                  Var\n                                                9             \u03b4 + 2     i<k    i\u22121 [\u2206i \u2212        \u03b1(Ei \u2212      Ei\u22121)] log 1     \u03b4 \u2212     2\u03b13 log 1   \u03b4\n                            \u2265   \u00b5k \u2212       4\u03b12  9     log2 1  \u03b4 + 1   4\u00b5k log 1    \u03b4 \u2212     2\u03b13 log 1   \u03b4\n                            \u2265   \u00b5k \u2212      max        2 \u221a   2\u03b1    log 1      \u221a  2     \u00b5k log 1          \u2212    2\u03b1\n                                                         3             \u03b4 ,   2                   \u03b4           3 log 1    \u03b4\n                                                   \u221a     2             \u221a  2          \u2212    2\u03b1\n                            \u2265   \u00b5k \u2212      max          2 log 1    \u03b4 ,    2 \u00b5k              3 log 1    \u03b4\n                            \u2265       1 \u2212    \u221a 22      \u00b5k \u2212     \u221a  2 log 1  \u03b4\n                            \u2265   \u2212  \u221a   2 log 1  \u03b4\nRearrange and we proved the lemma.\nCombining Lemma A.7 and Lemma A.8, we can show C is small with high probability as the lemma\nfollows.\nLemma A.9. For k = O                          m\u2217    log |H| \u03b4     , with probability at least 1 \u2212                     2\u03b4, h\u2217      \u2208    Sk and |C| \u2264\nO     log |H| \u03b4      at iteration k.\nProof. By union bound, with probability at least 1 \u2212                                  2\u03b4, Lemma A.6 and A.8 will hold at the same\ntime. This means h\u2217              is added to Sk. By definition, 0 \u2265                     \u03d5k \u2265     \u03d50   +\u03c8k, so \u03c8k \u2264            2 log |H|. Therefore,\nby Lemma A.7 and A.8, the number of balls added |C| is O                                          log |H| + log 1        \u03b4    = O       log |H| \u03b4     .\n                                                                             19", "md": "# Math Equations\n\nProof. Let q be the query distribution at iteration i\u22121 and p(x) be the probability that x is corrupted by the adversary. Then the conditional expectation of $$E_i - E_{i-1}$$ is $$q(x)$$\n\n$$\nE [E_i - E_{i-1} | F_i] = Pr [x \\sim q [h^*(x) \\text{ is wrong}] = E [x \\sim q [p(x)]] = E [p(x) q(x)] \\leq \\eta \\max_x D_X(x).\n$$\nThen if $$h^* \\not\\in S$$, from Lemma A.4\n\n$$\nE[\\Delta_i - 2\\alpha(E_i - E_{i-1}) | F_i] \\geq E[\\Delta_i | F_i] - 2\\alpha \\eta \\max_x D_X(x) \\gtrapprox m^*.\n$$\nTherefore, $$E[\\alpha (E_i - E_{i-1}) | F_i] \\leq \\frac{1}{2} E[\\Delta_i | F_i]$$ and $$E[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\geq \\frac{1}{2} E[\\Delta_i | F_i]$$. This $$\\frac{\\psi_k - \\alpha E_k}{2\\mu_k}$$ is a supermartingale. We then bound $$\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i]$$. Note that $$|\\Delta_i - \\alpha(E_i - E_{i-1})| \\leq 2\\alpha$$, so\n\n$$\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\leq 2\\alpha E[|\\Delta_i| | F_i].\n$$\nFurthermore,\n\n$$\n\\begin{align*}\nE[|\\Delta_i - \\alpha(E_i - E_{i-1})| | F_i] &\\leq E[|\\Delta_i| | F_i] + \\alpha \\eta \\max_x D_X(x). \\\\\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] &\\leq 2\\alpha E[|\\Delta_i| | F_i] + \\alpha \\eta \\max_x q(x) D_X(x) \\\\\n&\\leq 2\\alpha E[|\\Delta_i| | F_i] + \\frac{1}{2} E[\\Delta_i | F_i] \\\\\n&\\leq 3\\alpha E[|\\Delta_i| | F_i].\n\\end{align*}\n$$\nBy picking $$\\alpha$$ small enough, $$\\lesssim \\alpha E[\\Delta_i | F_i] \\leq 1$$. Moreover,\n\n$$\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\leq 8\\mu_k.\n$$\n$$|\\Delta_i - \\alpha(E_i - E_{i-1})| \\leq 2\\alpha$$ always. Therefore by Freedman\u2019s inequality, with $$1 - \\delta$$ probability we have for any k that\n\n$$\n\\begin{align*}\n\\psi_k - \\alpha E_k &\\geq \\mu_k - 4\\alpha^2 \\log_2 \\frac{1}{9\\delta} + 2 \\sum_{i<k} E[\\Delta_i - \\alpha(E_i - E_{i-1})] \\log \\frac{1}{\\delta} - 2\\alpha^3 \\log \\frac{1}{\\delta} \\\\\n&\\geq \\mu_k - \\frac{4\\alpha^2}{9} \\log_2 \\frac{1}{\\delta} + \\frac{1}{4} \\mu_k \\log \\frac{1}{\\delta} - 2\\alpha^3 \\log \\frac{1}{\\delta} \\\\\n&\\geq \\mu_k - \\max \\left(2 \\sqrt{2\\alpha} \\log \\frac{1}{\\sqrt{2}} \\mu_k \\log \\frac{1}{\\delta} - 2\\alpha, \\frac{\\mu_k - \\sqrt{2} \\log \\frac{1}{\\delta}}{\\sqrt{2}} \\right) \\\\\n&\\geq \\mu_k - \\max \\left(2 \\log \\frac{1}{\\delta}, 2\\mu_k \\frac{3}{\\log \\frac{1}{\\delta}} \\right) \\\\\n&\\geq 1 - \\sqrt{2}^2 \\mu_k - \\sqrt{2} \\log \\frac{1}{\\delta} \\\\\n&\\geq - \\sqrt{2} \\log \\frac{1}{\\delta}.\n\\end{align*}\n$$\nRearrange and we proved the lemma.\n\nCombining Lemma A.7 and Lemma A.8, we can show C is small with high probability as the lemma follows.\n\nLemma A.9. For $$k = O(m^* \\log |H| \\delta)$$, with probability at least $$1 - 2\\delta$$, $$h^* \\in S_k$$ and $$|C| \\leq O(\\log |H| \\delta)$$ at iteration k.\n\nProof. By union bound, with probability at least $$1 - 2\\delta$$, Lemma A.6 and A.8 will hold at the same time. This means $$h^*$$ is added to $$S_k$$. By definition, $$0 \\geq \\phi_k \\geq \\phi_0 + \\psi_k$$, so $$\\psi_k \\leq 2 \\log |H|$$. Therefore, by Lemma A.7 and A.8, the number of balls added $$|C|$$ is $$O(\\log |H| + \\log \\frac{1}{\\delta}) = O(\\log |H| \\delta)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Proof. Let q be the query distribution at iteration i\u22121 and p(x) be the probability that x is corrupted by the adversary. Then the conditional expectation of $$E_i - E_{i-1}$$ is $$q(x)$$\n\n$$\nE [E_i - E_{i-1} | F_i] = Pr [x \\sim q [h^*(x) \\text{ is wrong}] = E [x \\sim q [p(x)]] = E [p(x) q(x)] \\leq \\eta \\max_x D_X(x).\n$$\nThen if $$h^* \\not\\in S$$, from Lemma A.4\n\n$$\nE[\\Delta_i - 2\\alpha(E_i - E_{i-1}) | F_i] \\geq E[\\Delta_i | F_i] - 2\\alpha \\eta \\max_x D_X(x) \\gtrapprox m^*.\n$$\nTherefore, $$E[\\alpha (E_i - E_{i-1}) | F_i] \\leq \\frac{1}{2} E[\\Delta_i | F_i]$$ and $$E[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\geq \\frac{1}{2} E[\\Delta_i | F_i]$$. This $$\\frac{\\psi_k - \\alpha E_k}{2\\mu_k}$$ is a supermartingale. We then bound $$\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i]$$. Note that $$|\\Delta_i - \\alpha(E_i - E_{i-1})| \\leq 2\\alpha$$, so\n\n$$\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\leq 2\\alpha E[|\\Delta_i| | F_i].\n$$\nFurthermore,\n\n$$\n\\begin{align*}\nE[|\\Delta_i - \\alpha(E_i - E_{i-1})| | F_i] &\\leq E[|\\Delta_i| | F_i] + \\alpha \\eta \\max_x D_X(x). \\\\\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] &\\leq 2\\alpha E[|\\Delta_i| | F_i] + \\alpha \\eta \\max_x q(x) D_X(x) \\\\\n&\\leq 2\\alpha E[|\\Delta_i| | F_i] + \\frac{1}{2} E[\\Delta_i | F_i] \\\\\n&\\leq 3\\alpha E[|\\Delta_i| | F_i].\n\\end{align*}\n$$\nBy picking $$\\alpha$$ small enough, $$\\lesssim \\alpha E[\\Delta_i | F_i] \\leq 1$$. Moreover,\n\n$$\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\leq 8\\mu_k.\n$$\n$$|\\Delta_i - \\alpha(E_i - E_{i-1})| \\leq 2\\alpha$$ always. Therefore by Freedman\u2019s inequality, with $$1 - \\delta$$ probability we have for any k that\n\n$$\n\\begin{align*}\n\\psi_k - \\alpha E_k &\\geq \\mu_k - 4\\alpha^2 \\log_2 \\frac{1}{9\\delta} + 2 \\sum_{i<k} E[\\Delta_i - \\alpha(E_i - E_{i-1})] \\log \\frac{1}{\\delta} - 2\\alpha^3 \\log \\frac{1}{\\delta} \\\\\n&\\geq \\mu_k - \\frac{4\\alpha^2}{9} \\log_2 \\frac{1}{\\delta} + \\frac{1}{4} \\mu_k \\log \\frac{1}{\\delta} - 2\\alpha^3 \\log \\frac{1}{\\delta} \\\\\n&\\geq \\mu_k - \\max \\left(2 \\sqrt{2\\alpha} \\log \\frac{1}{\\sqrt{2}} \\mu_k \\log \\frac{1}{\\delta} - 2\\alpha, \\frac{\\mu_k - \\sqrt{2} \\log \\frac{1}{\\delta}}{\\sqrt{2}} \\right) \\\\\n&\\geq \\mu_k - \\max \\left(2 \\log \\frac{1}{\\delta}, 2\\mu_k \\frac{3}{\\log \\frac{1}{\\delta}} \\right) \\\\\n&\\geq 1 - \\sqrt{2}^2 \\mu_k - \\sqrt{2} \\log \\frac{1}{\\delta} \\\\\n&\\geq - \\sqrt{2} \\log \\frac{1}{\\delta}.\n\\end{align*}\n$$\nRearrange and we proved the lemma.\n\nCombining Lemma A.7 and Lemma A.8, we can show C is small with high probability as the lemma follows.\n\nLemma A.9. For $$k = O(m^* \\log |H| \\delta)$$, with probability at least $$1 - 2\\delta$$, $$h^* \\in S_k$$ and $$|C| \\leq O(\\log |H| \\delta)$$ at iteration k.\n\nProof. By union bound, with probability at least $$1 - 2\\delta$$, Lemma A.6 and A.8 will hold at the same time. This means $$h^*$$ is added to $$S_k$$. By definition, $$0 \\geq \\phi_k \\geq \\phi_0 + \\psi_k$$, so $$\\psi_k \\leq 2 \\log |H|$$. Therefore, by Lemma A.7 and A.8, the number of balls added $$|C|$$ is $$O(\\log |H| + \\log \\frac{1}{\\delta}) = O(\\log |H| \\delta)$$.", "md": "Proof. Let q be the query distribution at iteration i\u22121 and p(x) be the probability that x is corrupted by the adversary. Then the conditional expectation of $$E_i - E_{i-1}$$ is $$q(x)$$\n\n$$\nE [E_i - E_{i-1} | F_i] = Pr [x \\sim q [h^*(x) \\text{ is wrong}] = E [x \\sim q [p(x)]] = E [p(x) q(x)] \\leq \\eta \\max_x D_X(x).\n$$\nThen if $$h^* \\not\\in S$$, from Lemma A.4\n\n$$\nE[\\Delta_i - 2\\alpha(E_i - E_{i-1}) | F_i] \\geq E[\\Delta_i | F_i] - 2\\alpha \\eta \\max_x D_X(x) \\gtrapprox m^*.\n$$\nTherefore, $$E[\\alpha (E_i - E_{i-1}) | F_i] \\leq \\frac{1}{2} E[\\Delta_i | F_i]$$ and $$E[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\geq \\frac{1}{2} E[\\Delta_i | F_i]$$. This $$\\frac{\\psi_k - \\alpha E_k}{2\\mu_k}$$ is a supermartingale. We then bound $$\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i]$$. Note that $$|\\Delta_i - \\alpha(E_i - E_{i-1})| \\leq 2\\alpha$$, so\n\n$$\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\leq 2\\alpha E[|\\Delta_i| | F_i].\n$$\nFurthermore,\n\n$$\n\\begin{align*}\nE[|\\Delta_i - \\alpha(E_i - E_{i-1})| | F_i] &\\leq E[|\\Delta_i| | F_i] + \\alpha \\eta \\max_x D_X(x). \\\\\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] &\\leq 2\\alpha E[|\\Delta_i| | F_i] + \\alpha \\eta \\max_x q(x) D_X(x) \\\\\n&\\leq 2\\alpha E[|\\Delta_i| | F_i] + \\frac{1}{2} E[\\Delta_i | F_i] \\\\\n&\\leq 3\\alpha E[|\\Delta_i| | F_i].\n\\end{align*}\n$$\nBy picking $$\\alpha$$ small enough, $$\\lesssim \\alpha E[\\Delta_i | F_i] \\leq 1$$. Moreover,\n\n$$\n\\text{Var}[\\Delta_i - \\alpha(E_i - E_{i-1}) | F_i] \\leq 8\\mu_k.\n$$\n$$|\\Delta_i - \\alpha(E_i - E_{i-1})| \\leq 2\\alpha$$ always. Therefore by Freedman\u2019s inequality, with $$1 - \\delta$$ probability we have for any k that\n\n$$\n\\begin{align*}\n\\psi_k - \\alpha E_k &\\geq \\mu_k - 4\\alpha^2 \\log_2 \\frac{1}{9\\delta} + 2 \\sum_{i<k} E[\\Delta_i - \\alpha(E_i - E_{i-1})] \\log \\frac{1}{\\delta} - 2\\alpha^3 \\log \\frac{1}{\\delta} \\\\\n&\\geq \\mu_k - \\frac{4\\alpha^2}{9} \\log_2 \\frac{1}{\\delta} + \\frac{1}{4} \\mu_k \\log \\frac{1}{\\delta} - 2\\alpha^3 \\log \\frac{1}{\\delta} \\\\\n&\\geq \\mu_k - \\max \\left(2 \\sqrt{2\\alpha} \\log \\frac{1}{\\sqrt{2}} \\mu_k \\log \\frac{1}{\\delta} - 2\\alpha, \\frac{\\mu_k - \\sqrt{2} \\log \\frac{1}{\\delta}}{\\sqrt{2}} \\right) \\\\\n&\\geq \\mu_k - \\max \\left(2 \\log \\frac{1}{\\delta}, 2\\mu_k \\frac{3}{\\log \\frac{1}{\\delta}} \\right) \\\\\n&\\geq 1 - \\sqrt{2}^2 \\mu_k - \\sqrt{2} \\log \\frac{1}{\\delta} \\\\\n&\\geq - \\sqrt{2} \\log \\frac{1}{\\delta}.\n\\end{align*}\n$$\nRearrange and we proved the lemma.\n\nCombining Lemma A.7 and Lemma A.8, we can show C is small with high probability as the lemma follows.\n\nLemma A.9. For $$k = O(m^* \\log |H| \\delta)$$, with probability at least $$1 - 2\\delta$$, $$h^* \\in S_k$$ and $$|C| \\leq O(\\log |H| \\delta)$$ at iteration k.\n\nProof. By union bound, with probability at least $$1 - 2\\delta$$, Lemma A.6 and A.8 will hold at the same time. This means $$h^*$$ is added to $$S_k$$. By definition, $$0 \\geq \\phi_k \\geq \\phi_0 + \\psi_k$$, so $$\\psi_k \\leq 2 \\log |H|$$. Therefore, by Lemma A.7 and A.8, the number of balls added $$|C|$$ is $$O(\\log |H| + \\log \\frac{1}{\\delta}) = O(\\log |H| \\delta)$$."}]}, {"page": 20, "text": "A.5    Putting Everything Together\nWe proved that after O       m\u2217   log |H|    iterations, h\u2217   \u2208  Si and C is small with high probability. Hence,\n                                        \u03b4\nrunning the stage two algorithm to return a desired hypothesis will not take much more queries. We\nare ready to put everything together and finally prove Theorem 1.1.\nTheorem 1.1 (Competitive Bound). There exist some constants c1, c2 and c3 such that for any\ninstance (H, DX, \u03b7, \u03b5, \u03b4) with \u03b5 \u2265         c1\u03b7, Algorithm 1 solves the instance with sample complexity\n          m(H, DX, \u03b7, \u03b5, \u03b4) \u2272         m\u2217     H, DX, c2\u03b7, c3\u03b5, 99           + log 1     \u00b7 log N(H, DX, \u03b7)\n                                                                   100             \u03b4                  \u03b4\nand polynomial time.\nProof. Let\u2019s pick c1, c4, c5 as in Theorem 2.3 and pick the confidence parameter to be \u03b4                   3. Then by\nLemma A.9, with probability 1 \u2212           2\u03b4                  log |H|    ball added to Si will contain h\u2217. Since\n                                           3 , the first O          \u03b4\neach ball added to C has radius 3c4\u03b7 + 3c5\u03b5, the best hypothesis in C has error (3 + 3c4)\u03b7 + 3c5\u03b5.\nBy Theorem 2.2, with probability 1 \u2212              \u03b4\n                                                  3, the algorithm will return a hypothesis with error (9 +\n9c4)\u03b7 + 9c5\u03b5 \u2264       \u03b7 + \u03b5. Therefore, by union bound, the algorithm will return a desired hypothesis\nwith probability 1 \u2212     \u03b4. This proves the correctness of the algorithm.\nThe stage one algorithm makes\n O     m\u2217    H, DX, c4\u03b7, c5\u03b5 \u2212       2\u03b7, 99       log |H|     \u2264   O    m\u2217     H, DX, c4\u03b7, c5                log |H|\n                                          100           \u03b4                                     2 \u03b5, 99\n                                                                                                    100           \u03b4\nqueries.    The stage two algorithm makes O                |C| log |C|     queries by Theorem 2.2.           Note that\n                                                                     \u03b4\nC is a c4\u03b7 + c5\u03b5-packing because the center of added balls are at least c4\u03b7 + c5\u03b5 away, so\nm\u2217   H, DX, c4   2 \u03b7, c5           \u2265  log |C|. Since |C| \u2264        log |H|   by Lemma A.9, stage two algorithm\n                       2 \u03b5, 99\n                            100                                         \u03b4\ntakes O      m\u2217   H, DX, c4   2 \u03b7, c5          + log 1     log |H|    queries. Picking c2 = c4, c3 = c5\n                                    2 \u03b5, 99\n                                         100           \u03b4         \u03b4                                           2 , we get\nthe desired sample complexity bound.\nTo compute the packing at the beginning of the algorithm, we need to compute the distance of every\npair of hypotheses, which takes O(|H|2| X |) time. Computing r in each round takes O(|H|| X |)\ntime and solving the optimization problem takes O(| X |) time. Therefore, the remaining steps in     \u03b4\nstage one takes O       m\u2217|H|| X | log |H|       time. Stage two takes O         log |H|                  time. There-\n                                            \u03b4                                          \u03b4 log log |H|\n                                                                                                   \u03b4\nfore, the overall running time is polynomial of the size of the problem.\nSimilarly, we can prove Theorem 2.3, which is a stronger and more specific version of Theorem 1.1.\nTheorem 2.3. Suppose that Dx and H are such that, for any distribution \u03bb over H such that no\nradius-(c4\u03b7 + c5\u03b5) ball has probability more than 80%, there exists a distribution q over X such\nthat\n                                        E                           q(x)\n                                       x\u223cq[r(x)] \u2212     c4      x\n                                                       20\u03b7 max     Dx(x) \u2265      \u03b2\n                                                                      1\nfor some \u03b2 > 0. Then for \u03b5 \u2265             c1\u03b7, c4 \u2265    300, c5 =      10 and c1 \u2265      90c4, let N = N(H, Dx, \u03b7)\nbe the size of an \u03b7-cover of H.               Algorithm 1 solves (\u03b7, \u03b5, \u03b4) active agnostic learning with\nO    1                                  samples.\n     \u03b2 log N\u03b4 + log N   \u03b4 log log\u03b4N\n                                                             1                                                  \u03b4\nProof. By Lemma A.9 (with m\u2217                replaced by     \u03b2 and setting confidence parameter to               3) after\nO    1           queries, with probability at least 1 \u2212       2\u03b4\n     \u03b2 log N\u03b4                                                  3 , a hypothesis in C will be within c4\u03b7 + c5\u03b5 to\nh\u2217  and |C| = O       log N    . From Theorem 2.2, with probability at least 1 \u2212              \u03b4\nthen outputs a hypothesis \u02c6\u03b4                                                     \u02c6           3, stage two algorithm\n                                h that is 9c4\u03b7 + 9c5\u03b5 from h\u2032 so err             h    \u2264  9c4\u03b7 + 9c5\u03b5 \u2264        \u03b7 + \u03b5 by\nthe choice of the constants. The stage two algorithm makes O                   log N\u03b4 log log N \u03b4    queries. Overall,\n                                                                                              \u03b4\n                                                           20", "md": "A.5 Putting Everything Together\n\nWe proved that after $$O m^* \\log |H|$$ iterations, $$h^* \\in S_i$$ and C is small with high probability. Hence,\n\nrunning the stage two algorithm to return a desired hypothesis will not take much more queries. We\n\nare ready to put everything together and finally prove Theorem 1.1.\n\nTheorem 1.1 (Competitive Bound). There exist some constants c1, c2 and c3 such that for any\n\ninstance (H, DX, \u03b7, \u03b5, \u03b4) with \u03b5 \u2265 $$c_1\u03b7$$, Algorithm 1 solves the instance with sample complexity\n\n$$m(H, DX, \u03b7, \u03b5, \u03b4) \\lesssim m^* H, DX, c_2\u03b7, c_3\u03b5, 99 + \\log \\frac{1}{100} \\cdot \\log N(H, DX, \u03b7)$$\n\n$$\\frac{\u03b4}{\u03b4}$$\n\nand polynomial time.\n\nProof. Let\u2019s pick c1, c4, c5 as in Theorem 2.3 and pick the confidence parameter to be $$\u03b4 \\over 3$$. Then by\n\nLemma A.9, with probability $$1 - \\frac{2\u03b4}{3} \\log |H|$$ ball added to Si will contain $$h^*$$. Since\n\neach ball added to C has radius $$3c_4\u03b7 + 3c_5\u03b5$$, the best hypothesis in C has error $$(3 + 3c_4)\u03b7 + 3c_5\u03b5$$.\n\nBy Theorem 2.2, with probability $$1 - \\frac{\u03b4}{3}$$, the algorithm will return a hypothesis with error $$(9 + 9c_4)\u03b7 + 9c_5\u03b5 \\leq \u03b7 + \u03b5$$. Therefore, by union bound, the algorithm will return a desired hypothesis\n\nwith probability $$1 - \u03b4$$. This proves the correctness of the algorithm.\n\nThe stage one algorithm makes\n\n$$O m^* H, DX, c_4\u03b7, c_5\u03b5 - 2\u03b7, 99 \\log |H| \\leq O m^* H, DX, c_4\u03b7, c_5 \\log |H|$$\n\n$$\\frac{100}{\u03b4}$$\n\nqueries. The stage two algorithm makes $$O |C| \\log |C|$$ queries by Theorem 2.2. Note that\n\n$$\\frac{\u03b4}{\u03b4}$$\n\nC is a $$c_4\u03b7 + c_5\u03b5$$-packing because the center of added balls are at least $$c_4\u03b7 + c_5\u03b5$$ away, so\n\n$$m^* H, DX, c_4 \\frac{2\u03b7}{c_5} \u2265 \\log |C|$$. Since $$|C| \\leq \\log |H|$$ by Lemma A.9, stage two algorithm\n\ntakes $$O m^* H, DX, c_4 \\frac{2\u03b7}{c_5} + \\log \\frac{1}{100} \\log |H|$$ queries. Picking $$c_2 = c_4, c_3 = c_5$$\n\n$$\\frac{2}{\u03b4}$$, we get the desired sample complexity bound.\n\nTo compute the packing at the beginning of the algorithm, we need to compute the distance of every\n\npair of hypotheses, which takes $$O(|H|^2| X |)$$ time. Computing r in each round takes $$O(|H|| X |)$$\n\ntime and solving the optimization problem takes $$O(| X |)$$ time. Therefore, the remaining steps in $$\u03b4$$\n\nstage one takes $$O m^*|H|| X | \\log |H|$$ time. Stage two takes $$O \\log |H|$$ time. There-\n\n$$\\frac{\u03b4}{\u03b4} \\log \\log |H|$$\n\nfore, the overall running time is polynomial of the size of the problem.\n\nSimilarly, we can prove Theorem 2.3, which is a stronger and more specific version of Theorem 1.1.\n\nTheorem 2.3. Suppose that Dx and H are such that, for any distribution \u03bb over H such that no\n\nradius-($$c_4\u03b7 + c_5\u03b5$$) ball has probability more than 80%, there exists a distribution q over X such\n\nthat\n\n$$E_{x \\sim q}[r(x)] - c_4 \\frac{20\u03b7}{\\max Dx(x)} \\geq \u03b2$$\n\nfor some $$\u03b2 > 0$$. Then for \u03b5 \u2265 $$c_1\u03b7, c_4 \\geq 300, c_5 = 10$$ and $$c_1 \\geq 90c_4$$, let $$N = N(H, Dx, \u03b7)$$\n\nbe the size of an \u03b7-cover of H. Algorithm 1 solves $$(\u03b7, \u03b5, \u03b4)$$ active agnostic learning with\n\n$$O \\frac{1}{\u03b2} \\log N\u03b4 + \\log N \\frac{\u03b4 \\log \\log\u03b4N}{\u03b4}$$\n\nProof. By Lemma A.9 (with $$m^*$$ replaced by $$\u03b2$$ and setting confidence parameter to $$\\frac{3}{\u03b4}$$) after\n\n$$O \\frac{1}{\u03b2} \\log N\u03b4$$ queries, with probability at least $$1 - \\frac{2\u03b4}{3}$$, a hypothesis in C will be within $$c_4\u03b7 + c_5\u03b5$$ to\n\n$$h^*$$ and $$|C| = O \\log N$$. From Theorem 2.2, with probability at least $$1 - \\frac{\u03b4}{3}$$,\n\nthen outputs a hypothesis $$\\hat{h}$$ that is $$9c_4\u03b7 + 9c_5\u03b5$$ from $$h'$$ so $$err_{\\hat{h}} \\leq 9c_4\u03b7 + 9c_5\u03b5 \\leq \u03b7 + \u03b5$$ by\n\nthe choice of the constants. The stage two algorithm makes $$O \\log N\u03b4 \\log \\log N \u03b4$$ queries. Overall,\n\n$$20$$", "images": [], "items": [{"type": "text", "value": "A.5 Putting Everything Together\n\nWe proved that after $$O m^* \\log |H|$$ iterations, $$h^* \\in S_i$$ and C is small with high probability. Hence,\n\nrunning the stage two algorithm to return a desired hypothesis will not take much more queries. We\n\nare ready to put everything together and finally prove Theorem 1.1.\n\nTheorem 1.1 (Competitive Bound). There exist some constants c1, c2 and c3 such that for any\n\ninstance (H, DX, \u03b7, \u03b5, \u03b4) with \u03b5 \u2265 $$c_1\u03b7$$, Algorithm 1 solves the instance with sample complexity\n\n$$m(H, DX, \u03b7, \u03b5, \u03b4) \\lesssim m^* H, DX, c_2\u03b7, c_3\u03b5, 99 + \\log \\frac{1}{100} \\cdot \\log N(H, DX, \u03b7)$$\n\n$$\\frac{\u03b4}{\u03b4}$$\n\nand polynomial time.\n\nProof. Let\u2019s pick c1, c4, c5 as in Theorem 2.3 and pick the confidence parameter to be $$\u03b4 \\over 3$$. Then by\n\nLemma A.9, with probability $$1 - \\frac{2\u03b4}{3} \\log |H|$$ ball added to Si will contain $$h^*$$. Since\n\neach ball added to C has radius $$3c_4\u03b7 + 3c_5\u03b5$$, the best hypothesis in C has error $$(3 + 3c_4)\u03b7 + 3c_5\u03b5$$.\n\nBy Theorem 2.2, with probability $$1 - \\frac{\u03b4}{3}$$, the algorithm will return a hypothesis with error $$(9 + 9c_4)\u03b7 + 9c_5\u03b5 \\leq \u03b7 + \u03b5$$. Therefore, by union bound, the algorithm will return a desired hypothesis\n\nwith probability $$1 - \u03b4$$. This proves the correctness of the algorithm.\n\nThe stage one algorithm makes\n\n$$O m^* H, DX, c_4\u03b7, c_5\u03b5 - 2\u03b7, 99 \\log |H| \\leq O m^* H, DX, c_4\u03b7, c_5 \\log |H|$$\n\n$$\\frac{100}{\u03b4}$$\n\nqueries. The stage two algorithm makes $$O |C| \\log |C|$$ queries by Theorem 2.2. Note that\n\n$$\\frac{\u03b4}{\u03b4}$$\n\nC is a $$c_4\u03b7 + c_5\u03b5$$-packing because the center of added balls are at least $$c_4\u03b7 + c_5\u03b5$$ away, so\n\n$$m^* H, DX, c_4 \\frac{2\u03b7}{c_5} \u2265 \\log |C|$$. Since $$|C| \\leq \\log |H|$$ by Lemma A.9, stage two algorithm\n\ntakes $$O m^* H, DX, c_4 \\frac{2\u03b7}{c_5} + \\log \\frac{1}{100} \\log |H|$$ queries. Picking $$c_2 = c_4, c_3 = c_5$$\n\n$$\\frac{2}{\u03b4}$$, we get the desired sample complexity bound.\n\nTo compute the packing at the beginning of the algorithm, we need to compute the distance of every\n\npair of hypotheses, which takes $$O(|H|^2| X |)$$ time. Computing r in each round takes $$O(|H|| X |)$$\n\ntime and solving the optimization problem takes $$O(| X |)$$ time. Therefore, the remaining steps in $$\u03b4$$\n\nstage one takes $$O m^*|H|| X | \\log |H|$$ time. Stage two takes $$O \\log |H|$$ time. There-\n\n$$\\frac{\u03b4}{\u03b4} \\log \\log |H|$$\n\nfore, the overall running time is polynomial of the size of the problem.\n\nSimilarly, we can prove Theorem 2.3, which is a stronger and more specific version of Theorem 1.1.\n\nTheorem 2.3. Suppose that Dx and H are such that, for any distribution \u03bb over H such that no\n\nradius-($$c_4\u03b7 + c_5\u03b5$$) ball has probability more than 80%, there exists a distribution q over X such\n\nthat\n\n$$E_{x \\sim q}[r(x)] - c_4 \\frac{20\u03b7}{\\max Dx(x)} \\geq \u03b2$$\n\nfor some $$\u03b2 > 0$$. Then for \u03b5 \u2265 $$c_1\u03b7, c_4 \\geq 300, c_5 = 10$$ and $$c_1 \\geq 90c_4$$, let $$N = N(H, Dx, \u03b7)$$\n\nbe the size of an \u03b7-cover of H. Algorithm 1 solves $$(\u03b7, \u03b5, \u03b4)$$ active agnostic learning with\n\n$$O \\frac{1}{\u03b2} \\log N\u03b4 + \\log N \\frac{\u03b4 \\log \\log\u03b4N}{\u03b4}$$\n\nProof. By Lemma A.9 (with $$m^*$$ replaced by $$\u03b2$$ and setting confidence parameter to $$\\frac{3}{\u03b4}$$) after\n\n$$O \\frac{1}{\u03b2} \\log N\u03b4$$ queries, with probability at least $$1 - \\frac{2\u03b4}{3}$$, a hypothesis in C will be within $$c_4\u03b7 + c_5\u03b5$$ to\n\n$$h^*$$ and $$|C| = O \\log N$$. From Theorem 2.2, with probability at least $$1 - \\frac{\u03b4}{3}$$,\n\nthen outputs a hypothesis $$\\hat{h}$$ that is $$9c_4\u03b7 + 9c_5\u03b5$$ from $$h'$$ so $$err_{\\hat{h}} \\leq 9c_4\u03b7 + 9c_5\u03b5 \\leq \u03b7 + \u03b5$$ by\n\nthe choice of the constants. The stage two algorithm makes $$O \\log N\u03b4 \\log \\log N \u03b4$$ queries. Overall,\n\n$$20$$", "md": "A.5 Putting Everything Together\n\nWe proved that after $$O m^* \\log |H|$$ iterations, $$h^* \\in S_i$$ and C is small with high probability. Hence,\n\nrunning the stage two algorithm to return a desired hypothesis will not take much more queries. We\n\nare ready to put everything together and finally prove Theorem 1.1.\n\nTheorem 1.1 (Competitive Bound). There exist some constants c1, c2 and c3 such that for any\n\ninstance (H, DX, \u03b7, \u03b5, \u03b4) with \u03b5 \u2265 $$c_1\u03b7$$, Algorithm 1 solves the instance with sample complexity\n\n$$m(H, DX, \u03b7, \u03b5, \u03b4) \\lesssim m^* H, DX, c_2\u03b7, c_3\u03b5, 99 + \\log \\frac{1}{100} \\cdot \\log N(H, DX, \u03b7)$$\n\n$$\\frac{\u03b4}{\u03b4}$$\n\nand polynomial time.\n\nProof. Let\u2019s pick c1, c4, c5 as in Theorem 2.3 and pick the confidence parameter to be $$\u03b4 \\over 3$$. Then by\n\nLemma A.9, with probability $$1 - \\frac{2\u03b4}{3} \\log |H|$$ ball added to Si will contain $$h^*$$. Since\n\neach ball added to C has radius $$3c_4\u03b7 + 3c_5\u03b5$$, the best hypothesis in C has error $$(3 + 3c_4)\u03b7 + 3c_5\u03b5$$.\n\nBy Theorem 2.2, with probability $$1 - \\frac{\u03b4}{3}$$, the algorithm will return a hypothesis with error $$(9 + 9c_4)\u03b7 + 9c_5\u03b5 \\leq \u03b7 + \u03b5$$. Therefore, by union bound, the algorithm will return a desired hypothesis\n\nwith probability $$1 - \u03b4$$. This proves the correctness of the algorithm.\n\nThe stage one algorithm makes\n\n$$O m^* H, DX, c_4\u03b7, c_5\u03b5 - 2\u03b7, 99 \\log |H| \\leq O m^* H, DX, c_4\u03b7, c_5 \\log |H|$$\n\n$$\\frac{100}{\u03b4}$$\n\nqueries. The stage two algorithm makes $$O |C| \\log |C|$$ queries by Theorem 2.2. Note that\n\n$$\\frac{\u03b4}{\u03b4}$$\n\nC is a $$c_4\u03b7 + c_5\u03b5$$-packing because the center of added balls are at least $$c_4\u03b7 + c_5\u03b5$$ away, so\n\n$$m^* H, DX, c_4 \\frac{2\u03b7}{c_5} \u2265 \\log |C|$$. Since $$|C| \\leq \\log |H|$$ by Lemma A.9, stage two algorithm\n\ntakes $$O m^* H, DX, c_4 \\frac{2\u03b7}{c_5} + \\log \\frac{1}{100} \\log |H|$$ queries. Picking $$c_2 = c_4, c_3 = c_5$$\n\n$$\\frac{2}{\u03b4}$$, we get the desired sample complexity bound.\n\nTo compute the packing at the beginning of the algorithm, we need to compute the distance of every\n\npair of hypotheses, which takes $$O(|H|^2| X |)$$ time. Computing r in each round takes $$O(|H|| X |)$$\n\ntime and solving the optimization problem takes $$O(| X |)$$ time. Therefore, the remaining steps in $$\u03b4$$\n\nstage one takes $$O m^*|H|| X | \\log |H|$$ time. Stage two takes $$O \\log |H|$$ time. There-\n\n$$\\frac{\u03b4}{\u03b4} \\log \\log |H|$$\n\nfore, the overall running time is polynomial of the size of the problem.\n\nSimilarly, we can prove Theorem 2.3, which is a stronger and more specific version of Theorem 1.1.\n\nTheorem 2.3. Suppose that Dx and H are such that, for any distribution \u03bb over H such that no\n\nradius-($$c_4\u03b7 + c_5\u03b5$$) ball has probability more than 80%, there exists a distribution q over X such\n\nthat\n\n$$E_{x \\sim q}[r(x)] - c_4 \\frac{20\u03b7}{\\max Dx(x)} \\geq \u03b2$$\n\nfor some $$\u03b2 > 0$$. Then for \u03b5 \u2265 $$c_1\u03b7, c_4 \\geq 300, c_5 = 10$$ and $$c_1 \\geq 90c_4$$, let $$N = N(H, Dx, \u03b7)$$\n\nbe the size of an \u03b7-cover of H. Algorithm 1 solves $$(\u03b7, \u03b5, \u03b4)$$ active agnostic learning with\n\n$$O \\frac{1}{\u03b2} \\log N\u03b4 + \\log N \\frac{\u03b4 \\log \\log\u03b4N}{\u03b4}$$\n\nProof. By Lemma A.9 (with $$m^*$$ replaced by $$\u03b2$$ and setting confidence parameter to $$\\frac{3}{\u03b4}$$) after\n\n$$O \\frac{1}{\u03b2} \\log N\u03b4$$ queries, with probability at least $$1 - \\frac{2\u03b4}{3}$$, a hypothesis in C will be within $$c_4\u03b7 + c_5\u03b5$$ to\n\n$$h^*$$ and $$|C| = O \\log N$$. From Theorem 2.2, with probability at least $$1 - \\frac{\u03b4}{3}$$,\n\nthen outputs a hypothesis $$\\hat{h}$$ that is $$9c_4\u03b7 + 9c_5\u03b5$$ from $$h'$$ so $$err_{\\hat{h}} \\leq 9c_4\u03b7 + 9c_5\u03b5 \\leq \u03b7 + \u03b5$$ by\n\nthe choice of the constants. The stage two algorithm makes $$O \\log N\u03b4 \\log \\log N \u03b4$$ queries. Overall,\n\n$$20$$"}]}, {"page": 21, "text": "the algorithm makes O            1                                \u03b4    queries and succeeds with probability at least\n                                 \u03b2 log N \u03b4 + log N   \u03b4 log log N\u03b4\n1 \u2212  \u03b4.\nB      Query Complexity Lower Bound\nIn this section we derive a lower bound for the agnostic binary classification problem, which we\ndenote by AGNOSTICLEARNING. The lower bound is obtained from a reduction from minimum set\ncover, which we denote by SETCOVER. The problem SETCOVER consists a pair (U, S), where U\nis a ground set and S is a collection of subsets of U. The goal is to find a set cover C \u2286                         S such that\n S\u2208C S = U of minimal size |C|. We use K to denote the cardinality of the minimum set cover.\nLemma B.1 (Dinur and Steurer [2014], Corollary 1.5). There exists hard instances SETCOVER-\nHARD with the property K \u2265                 log |U| such that for every \u03b3 > 0, it is NP-hard to approximate\nSETCOVERHARD to within (1 \u2212                 \u03b3) ln |U|.\nProof. This lemma directly follows from Dinur and Steurer [2014, Corollary 1.5]. In their proof,\nthey constructed a hard instance of SETCOVER from LABELCOVER. The size of the minimum\ncover K \u2265       |V1| = Dn1 and log |U| = (D + 1) ln n1 \u2264                   K. So the instance in their proof satisfies\nthe desired property.\nThen we prove the following lemma by giving a ratio-preserving reduction from SETCOVER to\nAGNOSTICLEARNING.\nLemma          B.2.      If    there      exists      a     deterministic         \u03b1-approximation            algorithm        for\nAGNOSTICLEARNING                H, Dx,       1       1       1     , there exists a deterministic 2\u03b1-approximation\nalgorithm for SETCOVERHARD.                3| X |, 3| X |, 4|H|\nProof. Given an instance of SETCOVERHARD, for each s \u2208                            S, number the elements u \u2208             s in an\narbitrary order; let f(s, u) denote the index of u in s\u2019s list (and padding 0 to the left with the extra\nbit). We construct an instance of AGNOSTICLEARNING as the following:\n        1. Let the domain X have three pieces: U, V := {(s, j) | s \u2208                           S, j \u2208     [1 + log |s|]}, and\n            D = {1, . . . , log |U|}, an extra set of log |U| more coordinates.\n        2. On this domain, we define the following set of hypotheses:\n             (a) For u \u2208     U, define hu which only evaluates 1 on u \u2208                U and on (s, j) \u2208       V if u \u2208    s and\n                  the j\u2019th bit of (2f(s, u) + 1) is 1.\n             (b) For d \u2208     D, define hd which only evaluates 1 on d.\n             (c) Define h0 which evaluates everything to 0.\n                                                                                  1                  1                 1\n        3. Let DX be uniform distribution over X and set \u03b7 =1                    3|X| and \u03b5 =      3|X|. Set \u03b4 =     4|H|.\nAny two hypotheses satisfy \u2225h1 \u2212                h2\u2225   \u2265    |X| > \u03b5 = \u03b7, so err(h\u2217) = 0. First we show that\nm\u2217    H, Dx,       1        1      1      \u2264   K + log |U|. Indeed there exists a deterministic algorithm using\n                 3| X |, 3| X |,  4|H|\nK + log |U| queries to identify any hypothesis with probability 1. Given a smallest set cover C,\nthe algorithm first queries all (s, 0) \u2208            V for s \u2208      C. If h\u2217     = hu for some u, then for the s \u2208               S\nthat covers u, (s, 0) will evaluate to true. The identity of u can then be read out by querying (s, j)\nfor all j. The other possibilities\u2013hd for some d or 0\u2014can be identified by evaluating on all of\nD with log U queries. The total number of queries is then at most K + log |U| in all cases, so\nm\u2217   \u2264   K + log |U| \u2264       2K.\nWe now show how to reconstruct a good approximation to set cover from a good approximate query\nalgorithm. We feed the query algorithm y = 0 on every query it makes, and let C be the set of all s\nfor which it queries (s, j) for some j. Also, every time the algorithm queries some u \u2208                             U, we add\nan arbitrary set containing u to C. Then the size of C is at most the number of queries. We claim\nthat C is a set cover: if C does not cover some element u, then hu is zero on all queries made by\nthe algorithm, so hu is indistinguishable from h0 and the algortihm would fail on either input h0\n                                                               21", "md": "# Math Equations and Text\n\nThe algorithm makes $$O(1 + \\frac{\\delta}{\\beta} \\log N \\delta + \\log N \\delta \\log \\log N \\delta)$$ queries and succeeds with probability at least $$1 - \\delta$$.\n\n## Query Complexity Lower Bound\n\nIn this section we derive a lower bound for the agnostic binary classification problem, which we denote by AGNOSTICLEARNING. The lower bound is obtained from a reduction from minimum set cover, which we denote by SETCOVER. The problem SETCOVER consists a pair (U, S), where U is a ground set and S is a collection of subsets of U. The goal is to find a set cover $$C \\subseteq S$$ such that $$\\bigcup_{S \\in C} S = U$$ of minimal size $$|C|$$. We use K to denote the cardinality of the minimum set cover.\n\nLemma B.1 (Dinur and Steurer [2014], Corollary 1.5). There exists hard instances SETCOVER-HARD with the property $$K \\geq \\log |U|$$ such that for every $$\\gamma > 0$$, it is NP-hard to approximate SETCOVERHARD to within $$(1 - \\gamma) \\ln |U|$$.\n\nProof. This lemma directly follows from Dinur and Steurer [2014, Corollary 1.5]. In their proof, they constructed a hard instance of SETCOVER from LABELCOVER. The size of the minimum cover $$K \\geq |V_1| = Dn_1$$ and $$\\log |U| = (D + 1) \\ln n_1 \\leq K$$. So the instance in their proof satisfies the desired property.\n\nThen we prove the following lemma by giving a ratio-preserving reduction from SETCOVER to AGNOSTICLEARNING.\n\nLemma B.2. If there exists a deterministic $$\\alpha$$-approximation algorithm for AGNOSTICLEARNING $$H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}$$, there exists a deterministic $$2\\alpha$$-approximation algorithm for SETCOVERHARD.\n\nProof. Given an instance of SETCOVERHARD, for each $$s \\in S$$, number the elements $$u \\in s$$ in an arbitrary order; let $$f(s, u)$$ denote the index of $$u$$ in $$s$$'s list (and padding 0 to the left with the extra bit). We construct an instance of AGNOSTICLEARNING as the following:\n\n1. Let the domain X have three pieces: U, V := $(s, j) | s \\in S, j \\in [1 + \\log |s|]$, and D = {1, ..., $\\log |U|$}, an extra set of $\\log |U|$ more coordinates.\n2. On this domain, we define the following set of hypotheses:\n- For $u \\in U$, define $h_u$ which only evaluates 1 on $u \\in U$ and on $(s, j) \\in V$ if $u \\in s$ and the $j\n\nAny two hypotheses satisfy $$\\|h_1 - h_2\\| \\geq |X| > \\epsilon = \\eta$$, so $$err(h^*) = 0$$. First we show that $$m^*_{H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}} \\leq K + \\log |U|$$. Indeed there exists a deterministic algorithm using $$K + \\log |U|$$ queries to identify any hypothesis with probability 1. Given a smallest set cover C, the algorithm first queries all $$(s, 0) \\in V$$ for $$s \\in C$$. If $$h^* = h_u$$ for some $$u$$, then for the $$s \\in S$$ that covers $$u$$, $$(s, 0)$$ will evaluate to true. The identity of $$u$$ can then be read out by querying $$(s, j)$$ for all $$j$$. The other possibilities - $$h_d$$ for some $$d$$ or $$h_0$$ - can be identified by evaluating on all of D with $$\\log U$$ queries. The total number of queries is then at most $$K + \\log |U|$$ in all cases, so $$m^* \\leq K + \\log |U| \\leq 2K$$.\n\nWe now show how to reconstruct a good approximation to set cover from a good approximate query algorithm. We feed the query algorithm $$y = 0$$ on every query it makes, and let C be the set of all $$s$$ for which it queries $$(s, j)$$ for some $$j$$. Also, every time the algorithm queries some $$u \\in U$$, we add an arbitrary set containing $$u$$ to C. Then the size of C is at most the number of queries. We claim that C is a set cover: if C does not cover some element $$u$$, then $$h_u$$ is zero on all queries made by the algorithm, so $$h_u$$ is indistinguishable from $$h_0$$ and the algorithm would fail on either input $$h_0$$.\n\nth bit of $(2f(s, u) + 1)$ is 1.\n- For $d \\in D$, define $h_d$ which only evaluates 1 on $d$.\n- Define $h_0$ which evaluates everything to 0.\n3. Let $D_X$ be uniform distribution over X and set $\\eta = \\frac{1}{3|X|}$ and $\\epsilon = \\frac{1}{3|X|}$. Set $\\delta = \\frac{1}{4|H|}$.\n\nAny two hypotheses satisfy $$\\|h_1 - h_2\\| \\geq |X| > \\epsilon = \\eta$$, so $$err(h^*) = 0$$. First we show that $$m^*_{H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}} \\leq K + \\log |U|$$. Indeed there exists a deterministic algorithm using $$K + \\log |U|$$ queries to identify any hypothesis with probability 1. Given a smallest set cover C, the algorithm first queries all $$(s, 0) \\in V$$ for $$s \\in C$$. If $$h^* = h_u$$ for some $$u$$, then for the $$s \\in S$$ that covers $$u$$, $$(s, 0)$$ will evaluate to true. The identity of $$u$$ can then be read out by querying $$(s, j)$$ for all $$j$$. The other possibilities - $$h_d$$ for some $$d$$ or $$h_0$$ - can be identified by evaluating on all of D with $$\\log U$$ queries. The total number of queries is then at most $$K + \\log |U|$$ in all cases, so $$m^* \\leq K + \\log |U| \\leq 2K$$.\n\nWe now show how to reconstruct a good approximation to set cover from a good approximate query algorithm. We feed the query algorithm $$y = 0$$ on every query it makes, and let C be the set of all $$s$$ for which it queries $$(s, j)$$ for some $$j$$. Also, every time the algorithm queries some $$u \\in U$$, we add an arbitrary set containing $$u$$ to C. Then the size of C is at most the number of queries. We claim that C is a set cover: if C does not cover some element $$u$$, then $$h_u$$ is zero on all queries made by the algorithm, so $$h_u$$ is indistinguishable from $$h_0$$ and the algorithm would fail on either input $$h_0$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "The algorithm makes $$O(1 + \\frac{\\delta}{\\beta} \\log N \\delta + \\log N \\delta \\log \\log N \\delta)$$ queries and succeeds with probability at least $$1 - \\delta$$.", "md": "The algorithm makes $$O(1 + \\frac{\\delta}{\\beta} \\log N \\delta + \\log N \\delta \\log \\log N \\delta)$$ queries and succeeds with probability at least $$1 - \\delta$$."}, {"type": "heading", "lvl": 2, "value": "Query Complexity Lower Bound", "md": "## Query Complexity Lower Bound"}, {"type": "text", "value": "In this section we derive a lower bound for the agnostic binary classification problem, which we denote by AGNOSTICLEARNING. The lower bound is obtained from a reduction from minimum set cover, which we denote by SETCOVER. The problem SETCOVER consists a pair (U, S), where U is a ground set and S is a collection of subsets of U. The goal is to find a set cover $$C \\subseteq S$$ such that $$\\bigcup_{S \\in C} S = U$$ of minimal size $$|C|$$. We use K to denote the cardinality of the minimum set cover.\n\nLemma B.1 (Dinur and Steurer [2014], Corollary 1.5). There exists hard instances SETCOVER-HARD with the property $$K \\geq \\log |U|$$ such that for every $$\\gamma > 0$$, it is NP-hard to approximate SETCOVERHARD to within $$(1 - \\gamma) \\ln |U|$$.\n\nProof. This lemma directly follows from Dinur and Steurer [2014, Corollary 1.5]. In their proof, they constructed a hard instance of SETCOVER from LABELCOVER. The size of the minimum cover $$K \\geq |V_1| = Dn_1$$ and $$\\log |U| = (D + 1) \\ln n_1 \\leq K$$. So the instance in their proof satisfies the desired property.\n\nThen we prove the following lemma by giving a ratio-preserving reduction from SETCOVER to AGNOSTICLEARNING.\n\nLemma B.2. If there exists a deterministic $$\\alpha$$-approximation algorithm for AGNOSTICLEARNING $$H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}$$, there exists a deterministic $$2\\alpha$$-approximation algorithm for SETCOVERHARD.\n\nProof. Given an instance of SETCOVERHARD, for each $$s \\in S$$, number the elements $$u \\in s$$ in an arbitrary order; let $$f(s, u)$$ denote the index of $$u$$ in $$s$$'s list (and padding 0 to the left with the extra bit). We construct an instance of AGNOSTICLEARNING as the following:\n\n1. Let the domain X have three pieces: U, V := $(s, j) | s \\in S, j \\in [1 + \\log |s|]$, and D = {1, ..., $\\log |U|$}, an extra set of $\\log |U|$ more coordinates.\n2. On this domain, we define the following set of hypotheses:\n- For $u \\in U$, define $h_u$ which only evaluates 1 on $u \\in U$ and on $(s, j) \\in V$ if $u \\in s$ and the $j\n\nAny two hypotheses satisfy $$\\|h_1 - h_2\\| \\geq |X| > \\epsilon = \\eta$$, so $$err(h^*) = 0$$. First we show that $$m^*_{H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}} \\leq K + \\log |U|$$. Indeed there exists a deterministic algorithm using $$K + \\log |U|$$ queries to identify any hypothesis with probability 1. Given a smallest set cover C, the algorithm first queries all $$(s, 0) \\in V$$ for $$s \\in C$$. If $$h^* = h_u$$ for some $$u$$, then for the $$s \\in S$$ that covers $$u$$, $$(s, 0)$$ will evaluate to true. The identity of $$u$$ can then be read out by querying $$(s, j)$$ for all $$j$$. The other possibilities - $$h_d$$ for some $$d$$ or $$h_0$$ - can be identified by evaluating on all of D with $$\\log U$$ queries. The total number of queries is then at most $$K + \\log |U|$$ in all cases, so $$m^* \\leq K + \\log |U| \\leq 2K$$.\n\nWe now show how to reconstruct a good approximation to set cover from a good approximate query algorithm. We feed the query algorithm $$y = 0$$ on every query it makes, and let C be the set of all $$s$$ for which it queries $$(s, j)$$ for some $$j$$. Also, every time the algorithm queries some $$u \\in U$$, we add an arbitrary set containing $$u$$ to C. Then the size of C is at most the number of queries. We claim that C is a set cover: if C does not cover some element $$u$$, then $$h_u$$ is zero on all queries made by the algorithm, so $$h_u$$ is indistinguishable from $$h_0$$ and the algorithm would fail on either input $$h_0$$.\n\nth bit of $(2f(s, u) + 1)$ is 1.\n- For $d \\in D$, define $h_d$ which only evaluates 1 on $d$.\n- Define $h_0$ which evaluates everything to 0.\n3. Let $D_X$ be uniform distribution over X and set $\\eta = \\frac{1}{3|X|}$ and $\\epsilon = \\frac{1}{3|X|}$. Set $\\delta = \\frac{1}{4|H|}$.\n\nAny two hypotheses satisfy $$\\|h_1 - h_2\\| \\geq |X| > \\epsilon = \\eta$$, so $$err(h^*) = 0$$. First we show that $$m^*_{H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}} \\leq K + \\log |U|$$. Indeed there exists a deterministic algorithm using $$K + \\log |U|$$ queries to identify any hypothesis with probability 1. Given a smallest set cover C, the algorithm first queries all $$(s, 0) \\in V$$ for $$s \\in C$$. If $$h^* = h_u$$ for some $$u$$, then for the $$s \\in S$$ that covers $$u$$, $$(s, 0)$$ will evaluate to true. The identity of $$u$$ can then be read out by querying $$(s, j)$$ for all $$j$$. The other possibilities - $$h_d$$ for some $$d$$ or $$h_0$$ - can be identified by evaluating on all of D with $$\\log U$$ queries. The total number of queries is then at most $$K + \\log |U|$$ in all cases, so $$m^* \\leq K + \\log |U| \\leq 2K$$.\n\nWe now show how to reconstruct a good approximation to set cover from a good approximate query algorithm. We feed the query algorithm $$y = 0$$ on every query it makes, and let C be the set of all $$s$$ for which it queries $$(s, j)$$ for some $$j$$. Also, every time the algorithm queries some $$u \\in U$$, we add an arbitrary set containing $$u$$ to C. Then the size of C is at most the number of queries. We claim that C is a set cover: if C does not cover some element $$u$$, then $$h_u$$ is zero on all queries made by the algorithm, so $$h_u$$ is indistinguishable from $$h_0$$ and the algorithm would fail on either input $$h_0$$.", "md": "In this section we derive a lower bound for the agnostic binary classification problem, which we denote by AGNOSTICLEARNING. The lower bound is obtained from a reduction from minimum set cover, which we denote by SETCOVER. The problem SETCOVER consists a pair (U, S), where U is a ground set and S is a collection of subsets of U. The goal is to find a set cover $$C \\subseteq S$$ such that $$\\bigcup_{S \\in C} S = U$$ of minimal size $$|C|$$. We use K to denote the cardinality of the minimum set cover.\n\nLemma B.1 (Dinur and Steurer [2014], Corollary 1.5). There exists hard instances SETCOVER-HARD with the property $$K \\geq \\log |U|$$ such that for every $$\\gamma > 0$$, it is NP-hard to approximate SETCOVERHARD to within $$(1 - \\gamma) \\ln |U|$$.\n\nProof. This lemma directly follows from Dinur and Steurer [2014, Corollary 1.5]. In their proof, they constructed a hard instance of SETCOVER from LABELCOVER. The size of the minimum cover $$K \\geq |V_1| = Dn_1$$ and $$\\log |U| = (D + 1) \\ln n_1 \\leq K$$. So the instance in their proof satisfies the desired property.\n\nThen we prove the following lemma by giving a ratio-preserving reduction from SETCOVER to AGNOSTICLEARNING.\n\nLemma B.2. If there exists a deterministic $$\\alpha$$-approximation algorithm for AGNOSTICLEARNING $$H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}$$, there exists a deterministic $$2\\alpha$$-approximation algorithm for SETCOVERHARD.\n\nProof. Given an instance of SETCOVERHARD, for each $$s \\in S$$, number the elements $$u \\in s$$ in an arbitrary order; let $$f(s, u)$$ denote the index of $$u$$ in $$s$$'s list (and padding 0 to the left with the extra bit). We construct an instance of AGNOSTICLEARNING as the following:\n\n1. Let the domain X have three pieces: U, V := $(s, j) | s \\in S, j \\in [1 + \\log |s|]$, and D = {1, ..., $\\log |U|$}, an extra set of $\\log |U|$ more coordinates.\n2. On this domain, we define the following set of hypotheses:\n- For $u \\in U$, define $h_u$ which only evaluates 1 on $u \\in U$ and on $(s, j) \\in V$ if $u \\in s$ and the $j\n\nAny two hypotheses satisfy $$\\|h_1 - h_2\\| \\geq |X| > \\epsilon = \\eta$$, so $$err(h^*) = 0$$. First we show that $$m^*_{H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}} \\leq K + \\log |U|$$. Indeed there exists a deterministic algorithm using $$K + \\log |U|$$ queries to identify any hypothesis with probability 1. Given a smallest set cover C, the algorithm first queries all $$(s, 0) \\in V$$ for $$s \\in C$$. If $$h^* = h_u$$ for some $$u$$, then for the $$s \\in S$$ that covers $$u$$, $$(s, 0)$$ will evaluate to true. The identity of $$u$$ can then be read out by querying $$(s, j)$$ for all $$j$$. The other possibilities - $$h_d$$ for some $$d$$ or $$h_0$$ - can be identified by evaluating on all of D with $$\\log U$$ queries. The total number of queries is then at most $$K + \\log |U|$$ in all cases, so $$m^* \\leq K + \\log |U| \\leq 2K$$.\n\nWe now show how to reconstruct a good approximation to set cover from a good approximate query algorithm. We feed the query algorithm $$y = 0$$ on every query it makes, and let C be the set of all $$s$$ for which it queries $$(s, j)$$ for some $$j$$. Also, every time the algorithm queries some $$u \\in U$$, we add an arbitrary set containing $$u$$ to C. Then the size of C is at most the number of queries. We claim that C is a set cover: if C does not cover some element $$u$$, then $$h_u$$ is zero on all queries made by the algorithm, so $$h_u$$ is indistinguishable from $$h_0$$ and the algorithm would fail on either input $$h_0$$.\n\nth bit of $(2f(s, u) + 1)$ is 1.\n- For $d \\in D$, define $h_d$ which only evaluates 1 on $d$.\n- Define $h_0$ which evaluates everything to 0.\n3. Let $D_X$ be uniform distribution over X and set $\\eta = \\frac{1}{3|X|}$ and $\\epsilon = \\frac{1}{3|X|}$. Set $\\delta = \\frac{1}{4|H|}$.\n\nAny two hypotheses satisfy $$\\|h_1 - h_2\\| \\geq |X| > \\epsilon = \\eta$$, so $$err(h^*) = 0$$. First we show that $$m^*_{H, D_x, \\frac{1}{3|X|}, \\frac{1}{3|X|}, \\frac{1}{4|H|}} \\leq K + \\log |U|$$. Indeed there exists a deterministic algorithm using $$K + \\log |U|$$ queries to identify any hypothesis with probability 1. Given a smallest set cover C, the algorithm first queries all $$(s, 0) \\in V$$ for $$s \\in C$$. If $$h^* = h_u$$ for some $$u$$, then for the $$s \\in S$$ that covers $$u$$, $$(s, 0)$$ will evaluate to true. The identity of $$u$$ can then be read out by querying $$(s, j)$$ for all $$j$$. The other possibilities - $$h_d$$ for some $$d$$ or $$h_0$$ - can be identified by evaluating on all of D with $$\\log U$$ queries. The total number of queries is then at most $$K + \\log |U|$$ in all cases, so $$m^* \\leq K + \\log |U| \\leq 2K$$.\n\nWe now show how to reconstruct a good approximation to set cover from a good approximate query algorithm. We feed the query algorithm $$y = 0$$ on every query it makes, and let C be the set of all $$s$$ for which it queries $$(s, j)$$ for some $$j$$. Also, every time the algorithm queries some $$u \\in U$$, we add an arbitrary set containing $$u$$ to C. Then the size of C is at most the number of queries. We claim that C is a set cover: if C does not cover some element $$u$$, then $$h_u$$ is zero on all queries made by the algorithm, so $$h_u$$ is indistinguishable from $$h_0$$ and the algorithm would fail on either input $$h_0$$."}]}, {"page": 22, "text": "or hu. Thus if A is a deterministic \u03b1-approximation algorithm for AGNOSTICLEARNING, we will\nrecover a set cover of size at most \u03b1m\u2217               \u2264   \u03b1 (K + log |U|) \u2264         2\u03b1K, so this gives a deterministic\n2\u03b1-approximation algorithm for SETCOVERHARD.\nSimilar results also holds for randomized algorithms, we just need to be slightly careful about prob-\nabilities.\nLemma              B.3.        If        there          exists          a        randomized              algorithm            for\n                                             1       1       1     , there exists a randomized 2\u03b1-approximation\nAGNOSTICLEARNING                H, Dx,     3| X |, 3| X |, 4|H|\nalgorithm for SETCOVERHARD with success probability at least 2                        3.\nProof. We use the same reduction as in Lemma B.2.                                    Let A be an algorithm solves\n                                             1       1       1     . To obtain a set cover using A, we keeping giv-\nAGNOSTICLEARNING                H, Dx,     3| X |, 3| X |, 4|H|\ning A label 0 and construct the set C as before. Let qC be a distribution over the reconstructed set\nC. Assume that by contradiction with probability at least 1                 3, C is not a set cover. Then, with proba-\nbility at least 1/3, there is some element v such that both hv and h0 are consistent on all queries the\nalgorithm made; call such a query set \u201cambiguous\u201d.\nThen what is the probability that the agnostic learning algorithm fails on the input distribution that\nchooses h\u2217      uniformly from H? Any given ambiguous query set is equally likely to come from any\nof the consistent hypotheses, so the algorithm\u2019s success probability on ambiguous query sets is at\n                                                                                2         1\nmost 1/2. The chance the query set is ambiguous is at least                    3|H|: a   3H chance that the true h\u2217         is h0\nand the query set is ambiguous, and at least as much from the other hypotheses making it ambiguous.\n                                                                                            1\nThus the algorithm\u2019s fails to learn the true hypothesis with at least                     3|H| probability, contradicting\n                   1\nthe assumed      4|H| failure probability.\nTherefore, a set cover of size at most 2\u03b1K can be recovered with probability at least 1                            3 using the\nagnostic learning approximation algorithm.\nThe following theorem will then follow.\nTheorem 1.2 (Lower Bound). It is NP-hard to find a query strategy for every agnostic active learn-\ning instance within an c log |H| for some constant c > 0 factor of the optimal sample complexity.\nProof. Let\u2019s consider the instance of set cover constructed in Lemma B.2. Let c = 0.1 and note that\n0.1 log |H| \u2264      0.49 log |H| 2 . If there exists a polynomial time 0.49 log |H|           2 approximation algorithm\nfor the instance, then there exists a polynomial time 0.98 log |H|                   2    \u2264   0.98 log |U| approximation\nalgorithm for SETCOVERHARD, which is a contradiction to Lemma B.1.\n                                                               22", "md": "# Math Equations and Text\n\nor hu. Thus if A is a deterministic \u03b1-approximation algorithm for AGNOSTICLEARNING, we will recover a set cover of size at most \u03b1m* \u2264 \u03b1 (K + log |U|) \u2264 2\u03b1K, so this gives a deterministic 2\u03b1-approximation algorithm for SETCOVERHARD. Similar results also holds for randomized algorithms, we just need to be slightly careful about probabilities.\n\nLemma B.3. If there exists a randomized algorithm for AGNOSTICLEARNING H, Dx, 3|X|, 3|X|, 4|H|, there exists a randomized 2\u03b1-approximation algorithm for SETCOVERHARD with success probability at least 2/3.\n\nProof. We use the same reduction as in Lemma B.2. Let A be an algorithm solves AGNOSTICLEARNING H, Dx, 3|X|, 3|X|, 4|H|. To obtain a set cover using A, we keep giving A label 0 and construct the set C as before. Let qC be a distribution over the reconstructed set C. Assume that by contradiction with probability at least 1/3, C is not a set cover. Then, with probability at least 1/3, there is some element v such that both h_v and h_0 are consistent on all queries the algorithm made; call such a query set \"ambiguous\".\n\nThen what is the probability that the agnostic learning algorithm fails on the input distribution that chooses h* uniformly from H? Any given ambiguous query set is equally likely to come from any of the consistent hypotheses, so the algorithm's success probability on ambiguous query sets is at most 1/2. The chance the query set is ambiguous is at least 1/3|H|: a 1/3|H| chance that the true h* is h_0 and the query set is ambiguous, and at least as much from the other hypotheses making it ambiguous.\n\nThus the algorithm's fails to learn the true hypothesis with at least 1/3|H| probability, contradicting the assumed 1/4|H| failure probability. Therefore, a set cover of size at most 2\u03b1K can be recovered with probability at least 1/3 using the agnostic learning approximation algorithm.\n\nThe following theorem will then follow.\n\nTheorem 1.2 (Lower Bound). It is NP-hard to find a query strategy for every agnostic active learning instance within an c log |H| for some constant c > 0 factor of the optimal sample complexity.\n\nProof. Let\u2019s consider the instance of set cover constructed in Lemma B.2. Let c = 0.1 and note that 0.1 log |H| \u2264 0.49 log |H|2. If there exists a polynomial time 0.49 log |H|2 approximation algorithm for the instance, then there exists a polynomial time 0.98 log |H|2 \u2264 0.98 log |U| approximation algorithm for SETCOVERHARD, which is a contradiction to Lemma B.1.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "or hu. Thus if A is a deterministic \u03b1-approximation algorithm for AGNOSTICLEARNING, we will recover a set cover of size at most \u03b1m* \u2264 \u03b1 (K + log |U|) \u2264 2\u03b1K, so this gives a deterministic 2\u03b1-approximation algorithm for SETCOVERHARD. Similar results also holds for randomized algorithms, we just need to be slightly careful about probabilities.\n\nLemma B.3. If there exists a randomized algorithm for AGNOSTICLEARNING H, Dx, 3|X|, 3|X|, 4|H|, there exists a randomized 2\u03b1-approximation algorithm for SETCOVERHARD with success probability at least 2/3.\n\nProof. We use the same reduction as in Lemma B.2. Let A be an algorithm solves AGNOSTICLEARNING H, Dx, 3|X|, 3|X|, 4|H|. To obtain a set cover using A, we keep giving A label 0 and construct the set C as before. Let qC be a distribution over the reconstructed set C. Assume that by contradiction with probability at least 1/3, C is not a set cover. Then, with probability at least 1/3, there is some element v such that both h_v and h_0 are consistent on all queries the algorithm made; call such a query set \"ambiguous\".\n\nThen what is the probability that the agnostic learning algorithm fails on the input distribution that chooses h* uniformly from H? Any given ambiguous query set is equally likely to come from any of the consistent hypotheses, so the algorithm's success probability on ambiguous query sets is at most 1/2. The chance the query set is ambiguous is at least 1/3|H|: a 1/3|H| chance that the true h* is h_0 and the query set is ambiguous, and at least as much from the other hypotheses making it ambiguous.\n\nThus the algorithm's fails to learn the true hypothesis with at least 1/3|H| probability, contradicting the assumed 1/4|H| failure probability. Therefore, a set cover of size at most 2\u03b1K can be recovered with probability at least 1/3 using the agnostic learning approximation algorithm.\n\nThe following theorem will then follow.\n\nTheorem 1.2 (Lower Bound). It is NP-hard to find a query strategy for every agnostic active learning instance within an c log |H| for some constant c > 0 factor of the optimal sample complexity.\n\nProof. Let\u2019s consider the instance of set cover constructed in Lemma B.2. Let c = 0.1 and note that 0.1 log |H| \u2264 0.49 log |H|2. If there exists a polynomial time 0.49 log |H|2 approximation algorithm for the instance, then there exists a polynomial time 0.98 log |H|2 \u2264 0.98 log |U| approximation algorithm for SETCOVERHARD, which is a contradiction to Lemma B.1.", "md": "or hu. Thus if A is a deterministic \u03b1-approximation algorithm for AGNOSTICLEARNING, we will recover a set cover of size at most \u03b1m* \u2264 \u03b1 (K + log |U|) \u2264 2\u03b1K, so this gives a deterministic 2\u03b1-approximation algorithm for SETCOVERHARD. Similar results also holds for randomized algorithms, we just need to be slightly careful about probabilities.\n\nLemma B.3. If there exists a randomized algorithm for AGNOSTICLEARNING H, Dx, 3|X|, 3|X|, 4|H|, there exists a randomized 2\u03b1-approximation algorithm for SETCOVERHARD with success probability at least 2/3.\n\nProof. We use the same reduction as in Lemma B.2. Let A be an algorithm solves AGNOSTICLEARNING H, Dx, 3|X|, 3|X|, 4|H|. To obtain a set cover using A, we keep giving A label 0 and construct the set C as before. Let qC be a distribution over the reconstructed set C. Assume that by contradiction with probability at least 1/3, C is not a set cover. Then, with probability at least 1/3, there is some element v such that both h_v and h_0 are consistent on all queries the algorithm made; call such a query set \"ambiguous\".\n\nThen what is the probability that the agnostic learning algorithm fails on the input distribution that chooses h* uniformly from H? Any given ambiguous query set is equally likely to come from any of the consistent hypotheses, so the algorithm's success probability on ambiguous query sets is at most 1/2. The chance the query set is ambiguous is at least 1/3|H|: a 1/3|H| chance that the true h* is h_0 and the query set is ambiguous, and at least as much from the other hypotheses making it ambiguous.\n\nThus the algorithm's fails to learn the true hypothesis with at least 1/3|H| probability, contradicting the assumed 1/4|H| failure probability. Therefore, a set cover of size at most 2\u03b1K can be recovered with probability at least 1/3 using the agnostic learning approximation algorithm.\n\nThe following theorem will then follow.\n\nTheorem 1.2 (Lower Bound). It is NP-hard to find a query strategy for every agnostic active learning instance within an c log |H| for some constant c > 0 factor of the optimal sample complexity.\n\nProof. Let\u2019s consider the instance of set cover constructed in Lemma B.2. Let c = 0.1 and note that 0.1 log |H| \u2264 0.49 log |H|2. If there exists a polynomial time 0.49 log |H|2 approximation algorithm for the instance, then there exists a polynomial time 0.98 log |H|2 \u2264 0.98 log |U| approximation algorithm for SETCOVERHARD, which is a contradiction to Lemma B.1."}]}], "job_id": "cc86e053-c82f-4139-b1d1-0dd95a2648dd", "file_path": "./corpus/2310.18786.pdf"}