{"pages": [{"page": 1, "text": "                     Accelerating Exploration with Unlabeled Prior Data\n                             Qiyang Li\u03b1\u03b3\u2217, Jason Zhang\u03b1\u2217, Dibya Ghosh\u03b1, Amy Zhang\u03b2\u03b3, Sergey Levine\u03b1\n                                                     UC Berkeley\u03b1, UT Austin\u03b2, Meta\u03b3\n                                         {qcli,jason.z,dibya.ghosh}@berkeley.edu\n                            amy.zhang@austin.utexas.edu, svlevine@eecs.berkeley.edu\narXiv:2311.05067v2  [cs.LG]  21 Nov 2023\n                                                                 Abstract\n                             Learning to solve tasks from a sparse reward signal is a major challenge for stan-\n                             dard reinforcement learning (RL) algorithms. However, in the real world, agents\n                             rarely need to solve sparse reward tasks entirely from scratch. More often, we\n                             might possess prior experience to draw on that provides considerable guidance\n                             about which actions and outcomes are possible in the world, which we can use\n                             to explore more effectively for new tasks. In this work, we study how prior data\n                             without reward labels may be used to guide and accelerate exploration for an agent\n                             solving a new sparse reward task. We propose a simple approach that learns a re-\n                             ward model from online experience, labels the unlabeled prior data with optimistic\n                             rewards, and then uses it concurrently alongside the online data for downstream\n                             policy and critic optimization. This general formula leads to rapid exploration in\n                             several challenging sparse-reward domains where tabula rasa exploration is insuf-\n                             ficient, including the AntMaze domain, Adroit hand manipulation domain, and\n                             a visual simulated robotic manipulation domain. Our results highlight the ease\n                             of incorporating unlabeled prior data into existing online RL algorithms, and the\n                             (perhaps surprising) effectiveness of doing so.\n                    1   Introduction\n                    Exploration, particularly in sparse reward environments, presents a major challenge for reinforce-\n                    ment learning, and standard exploration methods typically need to seek out all potentially novel\n                    states to cover all the places where high rewards may be located. Luckily, in many real-world RL\n                    settings, it is straightforward to obtain prior data that can help the agent understand how the world\n                    works. For example, if we are trying to find where we left our keys, we would not relearn how to\n                    navigate our environment, but rather might revisit locations that we recall from memory. If the data\n                    has reward annotations, pre-training with offline RL provides one solution to accelerate online fine-\n                    tuning. However, in many domains of interest, it is more likely for prior datasets to be task-agnostic,\n                    and not be labeled with the reward for the new task we hope to solve. Despite not having reward\n                    labels, such prior data can be useful for an agent exploring for a new task since it describes the\n                    environment dynamics and indicates regions of the state space that may be interesting to explore.\n                    How can this unlabeled data be utilized by a deep RL algorithm? Prior data informs the agent about\n                    potentially high-reward or high-information states, and behaviors to reach them. Initially, the agent\n                    may act to reach these states, leading to directed exploration in a more informative subspace rather\n                    than the complete state space. This should be an iterative process, since the agent can use the new\n                    online experience to refine what directions from the prior data it chooses to explore in. After the\n                    agent begins to receive reward signals, the prior data can help the agent to more reliably reach and\n                    exploit the reward signals, steering the exploration towards a more promising region.\n                       \u2217Equal Contribution\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Accelerating Exploration with Unlabeled Prior Data\n\n## Accelerating Exploration with Unlabeled Prior Data\n\nQiyang Li\u03b1\u03b3\u2217, Jason Zhang\u03b1\u2217, Dibya Ghosh\u03b1, Amy Zhang\u03b2\u03b3, Sergey Levine\u03b1\n\nUC Berkeley\u03b1, UT Austin\u03b2, Meta\u03b3\n\n{qcli,jason.z,dibya.ghosh}@berkeley.edu\n\namy.zhang@austin.utexas.edu, svlevine@eecs.berkeley.edu\n\narXiv:2311.05067v2 [cs.LG] 21 Nov 2023\n\n### Abstract\n\nLearning to solve tasks from a sparse reward signal is a major challenge for standard reinforcement learning (RL) algorithms. However, in the real world, agents rarely need to solve sparse reward tasks entirely from scratch. More often, we might possess prior experience to draw on that provides considerable guidance about which actions and outcomes are possible in the world, which we can use to explore more effectively for new tasks. In this work, we study how prior data without reward labels may be used to guide and accelerate exploration for an agent solving a new sparse reward task. We propose a simple approach that learns a reward model from online experience, labels the unlabeled prior data with optimistic rewards, and then uses it concurrently alongside the online data for downstream policy and critic optimization. This general formula leads to rapid exploration in several challenging sparse-reward domains where tabula rasa exploration is insufficient, including the AntMaze domain, Adroit hand manipulation domain, and a visual simulated robotic manipulation domain. Our results highlight the ease of incorporating unlabeled prior data into existing online RL algorithms, and the (perhaps surprising) effectiveness of doing so.\n\n### Introduction\n\nExploration, particularly in sparse reward environments, presents a major challenge for reinforcement learning, and standard exploration methods typically need to seek out all potentially novel states to cover all the places where high rewards may be located. Luckily, in many real-world RL settings, it is straightforward to obtain prior data that can help the agent understand how the world works. For example, if we are trying to find where we left our keys, we would not relearn how to navigate our environment, but rather might revisit locations that we recall from memory. If the data has reward annotations, pre-training with offline RL provides one solution to accelerate online fine-tuning. However, in many domains of interest, it is more likely for prior datasets to be task-agnostic, and not be labeled with the reward for the new task we hope to solve. Despite not having reward labels, such prior data can be useful for an agent exploring for a new task since it describes the environment dynamics and indicates regions of the state space that may be interesting to explore. How can this unlabeled data be utilized by a deep RL algorithm? Prior data informs the agent about potentially high-reward or high-information states, and behaviors to reach them. Initially, the agent may act to reach these states, leading to directed exploration in a more informative subspace rather than the complete state space. This should be an iterative process, since the agent can use the new online experience to refine what directions from the prior data it chooses to explore in. After the agent begins to receive reward signals, the prior data can help the agent to more reliably reach and exploit the reward signals, steering the exploration towards a more promising region.\n\n\u2217Equal Contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Accelerating Exploration with Unlabeled Prior Data", "md": "# Accelerating Exploration with Unlabeled Prior Data"}, {"type": "heading", "lvl": 2, "value": "Accelerating Exploration with Unlabeled Prior Data", "md": "## Accelerating Exploration with Unlabeled Prior Data"}, {"type": "text", "value": "Qiyang Li\u03b1\u03b3\u2217, Jason Zhang\u03b1\u2217, Dibya Ghosh\u03b1, Amy Zhang\u03b2\u03b3, Sergey Levine\u03b1\n\nUC Berkeley\u03b1, UT Austin\u03b2, Meta\u03b3\n\n{qcli,jason.z,dibya.ghosh}@berkeley.edu\n\namy.zhang@austin.utexas.edu, svlevine@eecs.berkeley.edu\n\narXiv:2311.05067v2 [cs.LG] 21 Nov 2023", "md": "Qiyang Li\u03b1\u03b3\u2217, Jason Zhang\u03b1\u2217, Dibya Ghosh\u03b1, Amy Zhang\u03b2\u03b3, Sergey Levine\u03b1\n\nUC Berkeley\u03b1, UT Austin\u03b2, Meta\u03b3\n\n{qcli,jason.z,dibya.ghosh}@berkeley.edu\n\namy.zhang@austin.utexas.edu, svlevine@eecs.berkeley.edu\n\narXiv:2311.05067v2 [cs.LG] 21 Nov 2023"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "Learning to solve tasks from a sparse reward signal is a major challenge for standard reinforcement learning (RL) algorithms. However, in the real world, agents rarely need to solve sparse reward tasks entirely from scratch. More often, we might possess prior experience to draw on that provides considerable guidance about which actions and outcomes are possible in the world, which we can use to explore more effectively for new tasks. In this work, we study how prior data without reward labels may be used to guide and accelerate exploration for an agent solving a new sparse reward task. We propose a simple approach that learns a reward model from online experience, labels the unlabeled prior data with optimistic rewards, and then uses it concurrently alongside the online data for downstream policy and critic optimization. This general formula leads to rapid exploration in several challenging sparse-reward domains where tabula rasa exploration is insufficient, including the AntMaze domain, Adroit hand manipulation domain, and a visual simulated robotic manipulation domain. Our results highlight the ease of incorporating unlabeled prior data into existing online RL algorithms, and the (perhaps surprising) effectiveness of doing so.", "md": "Learning to solve tasks from a sparse reward signal is a major challenge for standard reinforcement learning (RL) algorithms. However, in the real world, agents rarely need to solve sparse reward tasks entirely from scratch. More often, we might possess prior experience to draw on that provides considerable guidance about which actions and outcomes are possible in the world, which we can use to explore more effectively for new tasks. In this work, we study how prior data without reward labels may be used to guide and accelerate exploration for an agent solving a new sparse reward task. We propose a simple approach that learns a reward model from online experience, labels the unlabeled prior data with optimistic rewards, and then uses it concurrently alongside the online data for downstream policy and critic optimization. This general formula leads to rapid exploration in several challenging sparse-reward domains where tabula rasa exploration is insufficient, including the AntMaze domain, Adroit hand manipulation domain, and a visual simulated robotic manipulation domain. Our results highlight the ease of incorporating unlabeled prior data into existing online RL algorithms, and the (perhaps surprising) effectiveness of doing so."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "Exploration, particularly in sparse reward environments, presents a major challenge for reinforcement learning, and standard exploration methods typically need to seek out all potentially novel states to cover all the places where high rewards may be located. Luckily, in many real-world RL settings, it is straightforward to obtain prior data that can help the agent understand how the world works. For example, if we are trying to find where we left our keys, we would not relearn how to navigate our environment, but rather might revisit locations that we recall from memory. If the data has reward annotations, pre-training with offline RL provides one solution to accelerate online fine-tuning. However, in many domains of interest, it is more likely for prior datasets to be task-agnostic, and not be labeled with the reward for the new task we hope to solve. Despite not having reward labels, such prior data can be useful for an agent exploring for a new task since it describes the environment dynamics and indicates regions of the state space that may be interesting to explore. How can this unlabeled data be utilized by a deep RL algorithm? Prior data informs the agent about potentially high-reward or high-information states, and behaviors to reach them. Initially, the agent may act to reach these states, leading to directed exploration in a more informative subspace rather than the complete state space. This should be an iterative process, since the agent can use the new online experience to refine what directions from the prior data it chooses to explore in. After the agent begins to receive reward signals, the prior data can help the agent to more reliably reach and exploit the reward signals, steering the exploration towards a more promising region.\n\n\u2217Equal Contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Exploration, particularly in sparse reward environments, presents a major challenge for reinforcement learning, and standard exploration methods typically need to seek out all potentially novel states to cover all the places where high rewards may be located. Luckily, in many real-world RL settings, it is straightforward to obtain prior data that can help the agent understand how the world works. For example, if we are trying to find where we left our keys, we would not relearn how to navigate our environment, but rather might revisit locations that we recall from memory. If the data has reward annotations, pre-training with offline RL provides one solution to accelerate online fine-tuning. However, in many domains of interest, it is more likely for prior datasets to be task-agnostic, and not be labeled with the reward for the new task we hope to solve. Despite not having reward labels, such prior data can be useful for an agent exploring for a new task since it describes the environment dynamics and indicates regions of the state space that may be interesting to explore. How can this unlabeled data be utilized by a deep RL algorithm? Prior data informs the agent about potentially high-reward or high-information states, and behaviors to reach them. Initially, the agent may act to reach these states, leading to directed exploration in a more informative subspace rather than the complete state space. This should be an iterative process, since the agent can use the new online experience to refine what directions from the prior data it chooses to explore in. After the agent begins to receive reward signals, the prior data can help the agent to more reliably reach and exploit the reward signals, steering the exploration towards a more promising region.\n\n\u2217Equal Contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "                                                                                   Interaction Data\n             Unlabeled                     Online Replay                                   (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032, \ud835\udc93)\n            Prior Data                           Buffer\n              (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032)                   (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032, \ud835\udc93)\n                                     update\n            Optimistic\n         Reward Bonus\n            (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032, \u0ddc                                                  \ud835\udc94\u2032~ \ud835\udc77(\ud835\udc94\u2032 \u2223   \ud835\udc94, \ud835\udc82)\n                         \ud835\udc93)\n                                                                               \ud835\udc82 ~ \ud835\udf45(\ud835\udc82   \u2223 \ud835\udc94)\n                                             Train\n                                             Batch\n                                                                   Agent\nFigure 1: How to leverage unlabeled prior data for efficient online exploration? We label the prior data\nwith an optimistic reward estimate and run RL on both the online and offline data. This allows more efficient\nexploration around the trajectories in the prior data, improving sample efficiency especially for hard exploration\ntasks.\nPerhaps surprisingly, such informed exploration can be elicited in a very simple way: by labeling\nthe prior data with optimistic rewards and adding it to the online experience of an off-policy RL\nalgorithm (Figure 1). Specifically, we choose to impute the missing reward on the unlabeled prior\ndata with an optimistic upper-confidence bound (UCB) of the reward estimates obtained through the\nagent\u2019s collected online experience. Early in training, these optimistic reward labels will be high\nfor states from the prior data, and optimizing the RL objective with this data will elicit policies that\nattempt to reach regions of state present in the prior data. As states in the prior data are visited, the\nlabeled reward for these states will regress towards their ground-truth values, leading to a policy that\neither explores other states from the prior data (when true reward is low) or returns to the region more\nconsistently (when high). By training an RL policy with a UCB estimate of the reward, exploration\nis guided by both task rewards and prior data, focusing only on visiting states from the prior data that\nmay have high reward. In practice, we find that this general recipe works with different off-policy\nalgorithms [Ball et al., 2023, Kostrikov et al., 2022].\nOur main contribution is a simple approach to leverage unlabeled prior data to improve online explo-\nration and sample efficiency, particularly for sparse long-horizon tasks. Our empirical evaluations,\nconducted over domains with different observation modalities (e.g., states and images), such as sim-\nulated robot navigation and arm and hand manipulation, show that our simple optimistic reward\nlabeling strategy can utilize the unlabeled prior data effectively, often as well as the prior best ap-\nproach that has access to the same prior data with labeled rewards. In addition, we can leverage\noffline pre-training of task-agnostic value functions (e.g., ICVF [Ghosh et al., 2023]) from offline\ndata to initialize our optimistic reward model to further boost the online sample efficiency in more\ncomplex settings, such as image-based environments. In contrast to other approaches in the same\nsetting, our method is conceptually simpler and easier to be integrated into current off-policy online\nRL algorithms, and performs better as we will show in our experiments. 2\n   2We have open-sourced code at https://github.com/facebookresearch/explore.\n                                                          2", "md": "# Interaction Data\n\n## Interaction Data\n\nUnlabeled Prior Data: (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032)\n\nOnline Replay Buffer: (\ud835\udc94, \ud835\udc82, \ud835\udc92\u2032, \ud835\udc93)\n\nOptimistic Reward Bonus: (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032, \ud835\udc93)\n\nupdate\n\nTrain Batch Agent\n\nFigure 1: How to leverage unlabeled prior data for efficient online exploration? We label the prior data with an optimistic reward estimate and run RL on both the online and offline data. This allows more efficient exploration around the trajectories in the prior data, improving sample efficiency especially for hard exploration tasks.\n\nPerhaps surprisingly, such informed exploration can be elicited in a very simple way: by labeling the prior data with optimistic rewards and adding it to the online experience of an off-policy RL algorithm (Figure 1). Specifically, we choose to impute the missing reward on the unlabeled prior data with an optimistic upper-confidence bound (UCB) of the reward estimates obtained through the agent\u2019s collected online experience. Early in training, these optimistic reward labels will be high for states from the prior data, and optimizing the RL objective with this data will elicit policies that attempt to reach regions of state present in the prior data. As states in the prior data are visited, the labeled reward for these states will regress towards their ground-truth values, leading to a policy that either explores other states from the prior data (when true reward is low) or returns to the region more consistently (when high). By training an RL policy with a UCB estimate of the reward, exploration is guided by both task rewards and prior data, focusing only on visiting states from the prior data that may have high reward. In practice, we find that this general recipe works with different off-policy algorithms [Ball et al., 2023, Kostrikov et al., 2022].\n\nOur main contribution is a simple approach to leverage unlabeled prior data to improve online exploration and sample efficiency, particularly for sparse long-horizon tasks. Our empirical evaluations, conducted over domains with different observation modalities (e.g., states and images), such as simulated robot navigation and arm and hand manipulation, show that our simple optimistic reward labeling strategy can utilize the unlabeled prior data effectively, often as well as the prior best approach that has access to the same prior data with labeled rewards. In addition, we can leverage offline pre-training of task-agnostic value functions (e.g., ICVF [Ghosh et al., 2023]) from offline data to initialize our optimistic reward model to further boost the online sample efficiency in more complex settings, such as image-based environments. In contrast to other approaches in the same setting, our method is conceptually simpler and easier to be integrated into current off-policy online RL algorithms, and performs better as we will show in our experiments. 2\n\n2We have open-sourced code at https://github.com/facebookresearch/explore.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Interaction Data", "md": "# Interaction Data"}, {"type": "heading", "lvl": 2, "value": "Interaction Data", "md": "## Interaction Data"}, {"type": "text", "value": "Unlabeled Prior Data: (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032)\n\nOnline Replay Buffer: (\ud835\udc94, \ud835\udc82, \ud835\udc92\u2032, \ud835\udc93)\n\nOptimistic Reward Bonus: (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032, \ud835\udc93)\n\nupdate\n\nTrain Batch Agent\n\nFigure 1: How to leverage unlabeled prior data for efficient online exploration? We label the prior data with an optimistic reward estimate and run RL on both the online and offline data. This allows more efficient exploration around the trajectories in the prior data, improving sample efficiency especially for hard exploration tasks.\n\nPerhaps surprisingly, such informed exploration can be elicited in a very simple way: by labeling the prior data with optimistic rewards and adding it to the online experience of an off-policy RL algorithm (Figure 1). Specifically, we choose to impute the missing reward on the unlabeled prior data with an optimistic upper-confidence bound (UCB) of the reward estimates obtained through the agent\u2019s collected online experience. Early in training, these optimistic reward labels will be high for states from the prior data, and optimizing the RL objective with this data will elicit policies that attempt to reach regions of state present in the prior data. As states in the prior data are visited, the labeled reward for these states will regress towards their ground-truth values, leading to a policy that either explores other states from the prior data (when true reward is low) or returns to the region more consistently (when high). By training an RL policy with a UCB estimate of the reward, exploration is guided by both task rewards and prior data, focusing only on visiting states from the prior data that may have high reward. In practice, we find that this general recipe works with different off-policy algorithms [Ball et al., 2023, Kostrikov et al., 2022].\n\nOur main contribution is a simple approach to leverage unlabeled prior data to improve online exploration and sample efficiency, particularly for sparse long-horizon tasks. Our empirical evaluations, conducted over domains with different observation modalities (e.g., states and images), such as simulated robot navigation and arm and hand manipulation, show that our simple optimistic reward labeling strategy can utilize the unlabeled prior data effectively, often as well as the prior best approach that has access to the same prior data with labeled rewards. In addition, we can leverage offline pre-training of task-agnostic value functions (e.g., ICVF [Ghosh et al., 2023]) from offline data to initialize our optimistic reward model to further boost the online sample efficiency in more complex settings, such as image-based environments. In contrast to other approaches in the same setting, our method is conceptually simpler and easier to be integrated into current off-policy online RL algorithms, and performs better as we will show in our experiments. 2\n\n2We have open-sourced code at https://github.com/facebookresearch/explore.", "md": "Unlabeled Prior Data: (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032)\n\nOnline Replay Buffer: (\ud835\udc94, \ud835\udc82, \ud835\udc92\u2032, \ud835\udc93)\n\nOptimistic Reward Bonus: (\ud835\udc94, \ud835\udc82, \ud835\udc94\u2032, \ud835\udc93)\n\nupdate\n\nTrain Batch Agent\n\nFigure 1: How to leverage unlabeled prior data for efficient online exploration? We label the prior data with an optimistic reward estimate and run RL on both the online and offline data. This allows more efficient exploration around the trajectories in the prior data, improving sample efficiency especially for hard exploration tasks.\n\nPerhaps surprisingly, such informed exploration can be elicited in a very simple way: by labeling the prior data with optimistic rewards and adding it to the online experience of an off-policy RL algorithm (Figure 1). Specifically, we choose to impute the missing reward on the unlabeled prior data with an optimistic upper-confidence bound (UCB) of the reward estimates obtained through the agent\u2019s collected online experience. Early in training, these optimistic reward labels will be high for states from the prior data, and optimizing the RL objective with this data will elicit policies that attempt to reach regions of state present in the prior data. As states in the prior data are visited, the labeled reward for these states will regress towards their ground-truth values, leading to a policy that either explores other states from the prior data (when true reward is low) or returns to the region more consistently (when high). By training an RL policy with a UCB estimate of the reward, exploration is guided by both task rewards and prior data, focusing only on visiting states from the prior data that may have high reward. In practice, we find that this general recipe works with different off-policy algorithms [Ball et al., 2023, Kostrikov et al., 2022].\n\nOur main contribution is a simple approach to leverage unlabeled prior data to improve online exploration and sample efficiency, particularly for sparse long-horizon tasks. Our empirical evaluations, conducted over domains with different observation modalities (e.g., states and images), such as simulated robot navigation and arm and hand manipulation, show that our simple optimistic reward labeling strategy can utilize the unlabeled prior data effectively, often as well as the prior best approach that has access to the same prior data with labeled rewards. In addition, we can leverage offline pre-training of task-agnostic value functions (e.g., ICVF [Ghosh et al., 2023]) from offline data to initialize our optimistic reward model to further boost the online sample efficiency in more complex settings, such as image-based environments. In contrast to other approaches in the same setting, our method is conceptually simpler and easier to be integrated into current off-policy online RL algorithms, and performs better as we will show in our experiments. 2\n\n2We have open-sourced code at https://github.com/facebookresearch/explore."}]}, {"page": 3, "text": "2     Problem Formulation\nWe formalize our problem in an infinite-horizon Markov decision process (MDP), defined as M =\n(S, A, P , r, \u03b3, \u03c1). It consists of a state space S, an action space A, transition dynamics P : S\u00d7A  \u2192\nP(S), a reward function r : S\u00d7A  \u2192         R, a discount factor \u03b3 \u2208    (0, 1], an initial state distribution of in-\nterest \u03c1. Algorithms interact in the MDP online to learn policies \u03c0(a|s) : S  \u2192           P(A) that maximize\nthe discounted return under the policy \u03b7(\u03c0) = Est+1\u223cP (\u00b7|st,at),at\u223c\u03c0(\u00b7|st),s0\u223c\u03c1 [ \u221e             t=0 \u03b3tr(st, at)].\nWe presume access to a dataset of transitions with no reward labels, D = {(si, ai, s\u2032              i)}i that was\npreviously collected from M, for example by a random policy, human demonstrator, or an agent\nsolving a different task in the same environment. The core challenge in this problem setting is to\nexplore in a sparse reward environment where you do not know where the reward is. While struc-\nturally similar to offline RL or online RL with prior data, where the agent receives a reward-labelled\ndataset, the reward-less setting poses a significant exploration challenge not present when reward\nlabels are available. Typically offline RL algorithms must find the best way to use the prior data to\nexploit the reward signal. In our setting, algorithms must use the prior data to improve exploration to\nacquire a reward signal. Using this prior data requires bridging the misalignment between the offline\nprior dataset (no reward) and the online collected data (contains reward), since standard model-free\nRL algorithms cannot directly use experiential data that has no reward labels.\n3     Exploration from Prior Data by Labeling Optimistic Reward\n      (EXPLORE)\nIn this section, we detail a simple way to use the unlabeled prior data directly in a standard online\nRL algorithm. Our general approach will be to label the reward-free data with learned optimistic\nrewards and include this generated experience into the buffer of an off-policy algorithm. When these\nrewards are correctly chosen, this will encourage the RL agent to explore along directions that were\npresent in the prior data, focusing on states perceived to have high rewards or not yet seen in the\nagent\u2019s online experience.\nAlgorithm 1 EXPLORE\n  1: Input: prior unlabeled data Doffline\n  2: Initialize the UCB estimate of the reward function: UCBR(s, a)\n  3: Online replay buffer D \u2190       \u2205\n  4: Initialize off-policy RL algorithm with a policy \u03c0.\n  5: for each environment step do\n  6:      Collect (s, a, s\u2032, r) using the policy \u03c0 and add it to the replay buffer D.\n  7:      Update the UCB estimate UCBR(s, a) using the new transition (s, a, s\u2032, r)\n  8:      Relabel each transition (s, a, s\u2032) in Doffline with \u02c6  r = UCBR(s, a)\n  9:      Run off-policy RL update on both D and the relabeled Doffline to improve the policy \u03c0.\n 10: end for\nGeneral algorithm.         Since the prior data contains no reward labels, the agent can only acquire\ninformation about the reward function from the online experience it collects. Specifically, at any\nstage of training, the collected online experience D informs some posterior or confidence set over\nthe true reward function that it must optimize. To make use of the prior data, we label it with an\nupper confidence bound (UCB) estimate of the true reward function. Optimizing an RL algorithm\nwith this relabeled prior experience results in a policy that is \u201coptimistic in the face of uncertainty\u201d,\nchoosing to guide its exploration towards prior data states either where it knows the reward to be\nhigh, and prior data states where it is highly uncertain about the reward. Unlike standard exploration\nwith optimistic rewards for states seen online, the prior data may include states that are further away\nthan those seen online, or may exclude states that are less important. Consequently, the RL agent\nseeks to visit regions in the prior data that are promising or has not visited before, accelerating online\nlearning. To explore with prior data, our algorithm maintains an uncertainty model of the reward,\nto be used for UCB estimates, and an off-policy agent that trains jointly on relabelled prior data and\nonline experience. As new experience is added through online data collection, we use this data to\nupdate the agent\u2019s uncertainty model over rewards. When training the off-policy agent, we sample\ndata from both the online replay buffer (already reward labeled) and from the prior data (no reward\n                                                         3", "md": "# Problem Formulation\n\n## Problem Formulation\n\nWe formalize our problem in an infinite-horizon Markov decision process (MDP), defined as M = (S, A, P , r, \u03b3, \u03c1). It consists of a state space S, an action space A, transition dynamics P : S\u00d7A \u2192 P(S), a reward function r : S\u00d7A \u2192 R, a discount factor \u03b3 \u2208 (0, 1], an initial state distribution of interest \u03c1. Algorithms interact in the MDP online to learn policies $$\\pi(a|s) : S \\rightarrow P(A)$$ that maximize the discounted return under the policy $$\\eta(\\pi) = E_{s_{t+1} \\sim P(\\cdot|s_t,a_t), a_t \\sim \\pi(\\cdot|s_t), s_0 \\sim \\rho} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$$.\n\nWe presume access to a dataset of transitions with no reward labels, $$D = \\{(s_i, a_i, s'_i)\\}_i$$ that was previously collected from M, for example by a random policy, human demonstrator, or an agent solving a different task in the same environment. The core challenge in this problem setting is to explore in a sparse reward environment where you do not know where the reward is. While structurally similar to offline RL or online RL with prior data, where the agent receives a reward-labelled dataset, the reward-less setting poses a significant exploration challenge not present when reward labels are available. Typically offline RL algorithms must find the best way to use the prior data to exploit the reward signal. In our setting, algorithms must use the prior data to improve exploration to acquire a reward signal. Using this prior data requires bridging the misalignment between the offline prior dataset (no reward) and the online collected data (contains reward), since standard model-free RL algorithms cannot directly use experiential data that has no reward labels.\n\n## Exploration from Prior Data by Labeling Optimistic Reward (EXPLORE)\n\nIn this section, we detail a simple way to use the unlabeled prior data directly in a standard online RL algorithm. Our general approach will be to label the reward-free data with learned optimistic rewards and include this generated experience into the buffer of an off-policy algorithm. When these rewards are correctly chosen, this will encourage the RL agent to explore along directions that were present in the prior data, focusing on states perceived to have high rewards or not yet seen in the agent\u2019s online experience.\n\n### Algorithm 1 EXPLORE\n\n1. Input: prior unlabeled data $D_{\\text{offline}}$\n2. Initialize the UCB estimate of the reward function: $UCBR(s, a)$\n3. Online replay buffer $D \\leftarrow \\emptyset$\n4. Initialize off-policy RL algorithm with a policy $\\pi$\n5. for each environment step do\n6. 1. Collect $(s, a, s', r)$ using the policy $\\pi$ and add it to the replay buffer $D$\n2. Update the UCB estimate $UCBR(s, a)$ using the new transition $(s, a, s', r)$\n3. Relabel each transition $(s, a, s')$ in $D_{\\text{offline}}$ with $\\hat{r} = UCBR(s, a)$\n4. Run off-policy RL update on both $D$ and the relabeled $D_{\\text{offline}}$ to improve the policy $\\pi$\n\nend for\n\nGeneral algorithm. Since the prior data contains no reward labels, the agent can only acquire information about the reward function from the online experience it collects. Specifically, at any stage of training, the collected online experience $$D$$ informs some posterior or confidence set over the true reward function that it must optimize. To make use of the prior data, we label it with an upper confidence bound (UCB) estimate of the true reward function. Optimizing an RL algorithm with this relabeled prior experience results in a policy that is \u201coptimistic in the face of uncertainty\u201d, choosing to guide its exploration towards prior data states either where it knows the reward to be high, and prior data states where it is highly uncertain about the reward. Unlike standard exploration with optimistic rewards for states seen online, the prior data may include states that are further away than those seen online, or may exclude states that are less important. Consequently, the RL agent seeks to visit regions in the prior data that are promising or has not visited before, accelerating online learning. To explore with prior data, our algorithm maintains an uncertainty model of the reward, to be used for UCB estimates, and an off-policy agent that trains jointly on relabeled prior data and online experience. As new experience is added through online data collection, we use this data to update the agent\u2019s uncertainty model over rewards. When training the off-policy agent, we sample data from both the online replay buffer (already reward labeled) and from the prior data (no reward).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Problem Formulation", "md": "# Problem Formulation"}, {"type": "heading", "lvl": 2, "value": "Problem Formulation", "md": "## Problem Formulation"}, {"type": "text", "value": "We formalize our problem in an infinite-horizon Markov decision process (MDP), defined as M = (S, A, P , r, \u03b3, \u03c1). It consists of a state space S, an action space A, transition dynamics P : S\u00d7A \u2192 P(S), a reward function r : S\u00d7A \u2192 R, a discount factor \u03b3 \u2208 (0, 1], an initial state distribution of interest \u03c1. Algorithms interact in the MDP online to learn policies $$\\pi(a|s) : S \\rightarrow P(A)$$ that maximize the discounted return under the policy $$\\eta(\\pi) = E_{s_{t+1} \\sim P(\\cdot|s_t,a_t), a_t \\sim \\pi(\\cdot|s_t), s_0 \\sim \\rho} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$$.\n\nWe presume access to a dataset of transitions with no reward labels, $$D = \\{(s_i, a_i, s'_i)\\}_i$$ that was previously collected from M, for example by a random policy, human demonstrator, or an agent solving a different task in the same environment. The core challenge in this problem setting is to explore in a sparse reward environment where you do not know where the reward is. While structurally similar to offline RL or online RL with prior data, where the agent receives a reward-labelled dataset, the reward-less setting poses a significant exploration challenge not present when reward labels are available. Typically offline RL algorithms must find the best way to use the prior data to exploit the reward signal. In our setting, algorithms must use the prior data to improve exploration to acquire a reward signal. Using this prior data requires bridging the misalignment between the offline prior dataset (no reward) and the online collected data (contains reward), since standard model-free RL algorithms cannot directly use experiential data that has no reward labels.", "md": "We formalize our problem in an infinite-horizon Markov decision process (MDP), defined as M = (S, A, P , r, \u03b3, \u03c1). It consists of a state space S, an action space A, transition dynamics P : S\u00d7A \u2192 P(S), a reward function r : S\u00d7A \u2192 R, a discount factor \u03b3 \u2208 (0, 1], an initial state distribution of interest \u03c1. Algorithms interact in the MDP online to learn policies $$\\pi(a|s) : S \\rightarrow P(A)$$ that maximize the discounted return under the policy $$\\eta(\\pi) = E_{s_{t+1} \\sim P(\\cdot|s_t,a_t), a_t \\sim \\pi(\\cdot|s_t), s_0 \\sim \\rho} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$$.\n\nWe presume access to a dataset of transitions with no reward labels, $$D = \\{(s_i, a_i, s'_i)\\}_i$$ that was previously collected from M, for example by a random policy, human demonstrator, or an agent solving a different task in the same environment. The core challenge in this problem setting is to explore in a sparse reward environment where you do not know where the reward is. While structurally similar to offline RL or online RL with prior data, where the agent receives a reward-labelled dataset, the reward-less setting poses a significant exploration challenge not present when reward labels are available. Typically offline RL algorithms must find the best way to use the prior data to exploit the reward signal. In our setting, algorithms must use the prior data to improve exploration to acquire a reward signal. Using this prior data requires bridging the misalignment between the offline prior dataset (no reward) and the online collected data (contains reward), since standard model-free RL algorithms cannot directly use experiential data that has no reward labels."}, {"type": "heading", "lvl": 2, "value": "Exploration from Prior Data by Labeling Optimistic Reward (EXPLORE)", "md": "## Exploration from Prior Data by Labeling Optimistic Reward (EXPLORE)"}, {"type": "text", "value": "In this section, we detail a simple way to use the unlabeled prior data directly in a standard online RL algorithm. Our general approach will be to label the reward-free data with learned optimistic rewards and include this generated experience into the buffer of an off-policy algorithm. When these rewards are correctly chosen, this will encourage the RL agent to explore along directions that were present in the prior data, focusing on states perceived to have high rewards or not yet seen in the agent\u2019s online experience.", "md": "In this section, we detail a simple way to use the unlabeled prior data directly in a standard online RL algorithm. Our general approach will be to label the reward-free data with learned optimistic rewards and include this generated experience into the buffer of an off-policy algorithm. When these rewards are correctly chosen, this will encourage the RL agent to explore along directions that were present in the prior data, focusing on states perceived to have high rewards or not yet seen in the agent\u2019s online experience."}, {"type": "heading", "lvl": 3, "value": "Algorithm 1 EXPLORE", "md": "### Algorithm 1 EXPLORE"}, {"type": "text", "value": "1. Input: prior unlabeled data $D_{\\text{offline}}$\n2. Initialize the UCB estimate of the reward function: $UCBR(s, a)$\n3. Online replay buffer $D \\leftarrow \\emptyset$\n4. Initialize off-policy RL algorithm with a policy $\\pi$\n5. for each environment step do\n6. 1. Collect $(s, a, s', r)$ using the policy $\\pi$ and add it to the replay buffer $D$\n2. Update the UCB estimate $UCBR(s, a)$ using the new transition $(s, a, s', r)$\n3. Relabel each transition $(s, a, s')$ in $D_{\\text{offline}}$ with $\\hat{r} = UCBR(s, a)$\n4. Run off-policy RL update on both $D$ and the relabeled $D_{\\text{offline}}$ to improve the policy $\\pi$\n\nend for\n\nGeneral algorithm. Since the prior data contains no reward labels, the agent can only acquire information about the reward function from the online experience it collects. Specifically, at any stage of training, the collected online experience $$D$$ informs some posterior or confidence set over the true reward function that it must optimize. To make use of the prior data, we label it with an upper confidence bound (UCB) estimate of the true reward function. Optimizing an RL algorithm with this relabeled prior experience results in a policy that is \u201coptimistic in the face of uncertainty\u201d, choosing to guide its exploration towards prior data states either where it knows the reward to be high, and prior data states where it is highly uncertain about the reward. Unlike standard exploration with optimistic rewards for states seen online, the prior data may include states that are further away than those seen online, or may exclude states that are less important. Consequently, the RL agent seeks to visit regions in the prior data that are promising or has not visited before, accelerating online learning. To explore with prior data, our algorithm maintains an uncertainty model of the reward, to be used for UCB estimates, and an off-policy agent that trains jointly on relabeled prior data and online experience. As new experience is added through online data collection, we use this data to update the agent\u2019s uncertainty model over rewards. When training the off-policy agent, we sample data from both the online replay buffer (already reward labeled) and from the prior data (no reward).", "md": "1. Input: prior unlabeled data $D_{\\text{offline}}$\n2. Initialize the UCB estimate of the reward function: $UCBR(s, a)$\n3. Online replay buffer $D \\leftarrow \\emptyset$\n4. Initialize off-policy RL algorithm with a policy $\\pi$\n5. for each environment step do\n6. 1. Collect $(s, a, s', r)$ using the policy $\\pi$ and add it to the replay buffer $D$\n2. Update the UCB estimate $UCBR(s, a)$ using the new transition $(s, a, s', r)$\n3. Relabel each transition $(s, a, s')$ in $D_{\\text{offline}}$ with $\\hat{r} = UCBR(s, a)$\n4. Run off-policy RL update on both $D$ and the relabeled $D_{\\text{offline}}$ to improve the policy $\\pi$\n\nend for\n\nGeneral algorithm. Since the prior data contains no reward labels, the agent can only acquire information about the reward function from the online experience it collects. Specifically, at any stage of training, the collected online experience $$D$$ informs some posterior or confidence set over the true reward function that it must optimize. To make use of the prior data, we label it with an upper confidence bound (UCB) estimate of the true reward function. Optimizing an RL algorithm with this relabeled prior experience results in a policy that is \u201coptimistic in the face of uncertainty\u201d, choosing to guide its exploration towards prior data states either where it knows the reward to be high, and prior data states where it is highly uncertain about the reward. Unlike standard exploration with optimistic rewards for states seen online, the prior data may include states that are further away than those seen online, or may exclude states that are less important. Consequently, the RL agent seeks to visit regions in the prior data that are promising or has not visited before, accelerating online learning. To explore with prior data, our algorithm maintains an uncertainty model of the reward, to be used for UCB estimates, and an off-policy agent that trains jointly on relabeled prior data and online experience. As new experience is added through online data collection, we use this data to update the agent\u2019s uncertainty model over rewards. When training the off-policy agent, we sample data from both the online replay buffer (already reward labeled) and from the prior data (no reward)."}]}, {"page": 4, "text": "           Online                  Online + RND                           Naive                            Ours                  Env Steps (\u00d7103)\n                                                                                                                             45\n                                                                                                                             30\n                                                                                                                             15\n                                                                                                                             0\nFigure 2: Visualizations of the agent\u2019s exploration behaviors using our optimistic reward labeling strat-\negy on antmaze-medium. The dots shown (500 points each) are sampled uniformly from the online replay\nbuffer (up to 60K environment steps), and colored by the training environment step at which they were added to\nthe buffer. Both Online and Online + RND do not use prior data, and the latter uses RND reward bonus [Burda\net al., 2018] on top of the received online reward to encourage online exploration. Na\u00efve learns a reward model\nand relabels the unlabeled prior data with its reward estimate with no optimism. Ours labels the prior data with\nan optimistic reward estimate.\nlabels). The prior data is labelled with rewards from the UCB estimate of our uncertainty model,\nand then the online and prior data is jointly trained upon. Training with this data leads to a policy\nthat acts optimistically with respect to the agent\u2019s uncertainty about the reward function, which we\nuse to guide exploration and collect new transitions. Algorithm 1 presents a sketch of our method.\nNotice that while this strategy is similar to exploration algorithms that use novelty-based reward\nbonuses [Bellemare et al., 2016, Tang et al., 2017, Pathak et al., 2017, Burda et al., 2018, Pathak\net al., 2019, Gupta et al., 2022], since the optimism is being added to the prior data, the resulting\nexploration behaviors differ qualitatively. Adding novelty bonuses to online experience guides the\nagent to states it has already reached in the online phase, whereas adding optimism to the prior\nexperience encourages the agent to learn to reach new states beyond the regions it has visited through\nonline exploration. Intuitively, while standard optimism leads to exploration along the frontier of\nonline data, optimism on prior data leads to exploration beyond the frontier of online data.\nPractical implementation.                  In practice, we obtain UCB estimates of the rewards using a combina-\ntion of a reward model and random-network-distillation (RND) [Burda et al., 2018], a novelty-based\nreward bonus technique. The former provides a reward estimate of the true reward value and the lat-\nter (RND) quantifies the uncertainty of the estimate, allowing us to get a UCB reward estimate.\nFor the reward model, we update a randomly initialized network r\u03b8(s, a) simultaneously with the\nRL agent on the online replay buffer by minimizing the squared reward loss:\n                                            L(\u03b8) = E(s,a,r)\u223cD             (r\u03b8(s, a) \u2212      r)2    .\nRND randomly initializes two networks f\u03d5(s, a), \u00af                       f(s, a) that each output an L-dimensional feature\nvector for each state and action. We keep one network, \u00af                         f, frozen and update the parameters of the\nother network \u03d5 to minimize the mean-square-error between the predicted features and the frozen\nones once on every new transition (snew, anew, s\u2032                     new, rnew) encountered:\n                                     L(\u03d5) = 1                                    f(snew, anew)\u22252       2.\n                                                  L\u2225f\u03d5(snew, anew) \u2212              \u00af\nDuring online exploration, \u2225f\u03d5(s, a) \u2212                    \u00af\n                                                         f(s, a)\u22252   2 forms a measure of the agent\u2019s uncertainty about\n(s, a), leading us to the following approximate UCB estimate of the reward function\n                                  UCBR(s, a) \u2190            r\u03b8(s, a) + 1                         f(s, a)\u22252   2.\n                                                                          L\u2225f\u03d5(s, a) \u2212         \u00af\nFor the RL agent, we use RLPD [Ball et al., 2023], which is specifically designed for efficient online\nRL and performs learning updates on both online and labeled offline data. Although we only have\naccess to unlabeled data in our setting, we constantly label the prior data with UCB reward estimates,\nenabling a direct application of RLPD updates. It is worth noting that our method is compatible with\npotentially any other off-policy RL algorithms that can leverage labeled online and offline data. We\nexperiment with an alternative based on implicit-Q learning in Appendix C.4.\n                                                                      4", "md": "# Exploration Behaviors in Reinforcement Learning\n\n## Visualizations of Exploration Behaviors\n\nFigure 2: Visualizations of the agent\u2019s exploration behaviors using our optimistic reward labeling strategy on antmaze-medium. The dots shown (500 points each) are sampled uniformly from the online replay buffer (up to 60K environment steps), and colored by the training environment step at which they were added to the buffer. Both Online and Online + RND do not use prior data, and the latter uses RND reward bonus [Burda et al., 2018] on top of the received online reward to encourage online exploration. Na\u00efve learns a reward model and relabels the unlabeled prior data with its reward estimate with no optimism. Ours labels the prior data with an optimistic reward estimate.\n\n## Optimistic Reward Labeling Strategy\n\nThe prior data is labelled with rewards from the UCB estimate of our uncertainty model, and then the online and prior data is jointly trained upon. Training with this data leads to a policy that acts optimistically with respect to the agent\u2019s uncertainty about the reward function, which we use to guide exploration and collect new transitions. Algorithm 1 presents a sketch of our method.\n\nNotice that while this strategy is similar to exploration algorithms that use novelty-based reward bonuses [Bellemare et al., 2016, Tang et al., 2017, Pathak et al., 2017, Burda et al., 2018, Pathak et al., 2019, Gupta et al., 2022], since the optimism is being added to the prior data, the resulting exploration behaviors differ qualitatively. Adding novelty bonuses to online experience guides the agent to states it has already reached in the online phase, whereas adding optimism to the prior experience encourages the agent to learn to reach new states beyond the regions it has visited through online exploration. Intuitively, while standard optimism leads to exploration along the frontier of online data, optimism on prior data leads to exploration beyond the frontier of online data.\n\n## Practical Implementation\n\nIn practice, we obtain UCB estimates of the rewards using a combination of a reward model and random-network-distillation (RND) [Burda et al., 2018], a novelty-based reward bonus technique. The former provides a reward estimate of the true reward value and the latter (RND) quantifies the uncertainty of the estimate, allowing us to get a UCB reward estimate.\n\nFor the reward model, we update a randomly initialized network $$r_{\\theta}(s, a)$$ simultaneously with the RL agent on the online replay buffer by minimizing the squared reward loss:\n\n$$L(\\theta) = E(s,a,r) \\sim D (r_{\\theta}(s, a) - r)^2$$\n\nRND randomly initializes two networks $$f_{\\phi}(s, a)$$, $$\\overline{f}(s, a)$$ that each output an L-dimensional feature vector for each state and action. We keep one network, $$\\overline{f}$$, frozen and update the parameters of the other network $$\\phi$$ to minimize the mean-square-error between the predicted features and the frozen ones once on every new transition $$(s_{new}, a_{new}, s'_{new}, r_{new})$$ encountered:\n\n$$L(\\phi) = \\frac{1}{L} \\lVert f(s_{new}, a_{new}) - \\overline{f}(s_{new}, a_{new}) \\rVert^2$$\n\nDuring online exploration, $$\\lVert f_{\\phi}(s, a) - \\overline{f}(s, a) \\rVert^2$$ forms a measure of the agent\u2019s uncertainty about $$(s, a)$$, leading us to the following approximate UCB estimate of the reward function:\n\n$$UCB_{R}(s, a) \\leftarrow r_{\\theta}(s, a) + \\frac{1}{L} \\lVert f(s, a) - \\overline{f}(s, a) \\rVert^2$$\n\nFor the RL agent, we use RLPD [Ball et al., 2023], which is specifically designed for efficient online RL and performs learning updates on both online and labeled offline data. Although we only have access to unlabeled data in our setting, we constantly label the prior data with UCB reward estimates, enabling a direct application of RLPD updates. It is worth noting that our method is compatible with potentially any other off-policy RL algorithms that can leverage labeled online and offline data. We experiment with an alternative based on implicit-Q learning in Appendix C.4.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Exploration Behaviors in Reinforcement Learning", "md": "# Exploration Behaviors in Reinforcement Learning"}, {"type": "heading", "lvl": 2, "value": "Visualizations of Exploration Behaviors", "md": "## Visualizations of Exploration Behaviors"}, {"type": "text", "value": "Figure 2: Visualizations of the agent\u2019s exploration behaviors using our optimistic reward labeling strategy on antmaze-medium. The dots shown (500 points each) are sampled uniformly from the online replay buffer (up to 60K environment steps), and colored by the training environment step at which they were added to the buffer. Both Online and Online + RND do not use prior data, and the latter uses RND reward bonus [Burda et al., 2018] on top of the received online reward to encourage online exploration. Na\u00efve learns a reward model and relabels the unlabeled prior data with its reward estimate with no optimism. Ours labels the prior data with an optimistic reward estimate.", "md": "Figure 2: Visualizations of the agent\u2019s exploration behaviors using our optimistic reward labeling strategy on antmaze-medium. The dots shown (500 points each) are sampled uniformly from the online replay buffer (up to 60K environment steps), and colored by the training environment step at which they were added to the buffer. Both Online and Online + RND do not use prior data, and the latter uses RND reward bonus [Burda et al., 2018] on top of the received online reward to encourage online exploration. Na\u00efve learns a reward model and relabels the unlabeled prior data with its reward estimate with no optimism. Ours labels the prior data with an optimistic reward estimate."}, {"type": "heading", "lvl": 2, "value": "Optimistic Reward Labeling Strategy", "md": "## Optimistic Reward Labeling Strategy"}, {"type": "text", "value": "The prior data is labelled with rewards from the UCB estimate of our uncertainty model, and then the online and prior data is jointly trained upon. Training with this data leads to a policy that acts optimistically with respect to the agent\u2019s uncertainty about the reward function, which we use to guide exploration and collect new transitions. Algorithm 1 presents a sketch of our method.\n\nNotice that while this strategy is similar to exploration algorithms that use novelty-based reward bonuses [Bellemare et al., 2016, Tang et al., 2017, Pathak et al., 2017, Burda et al., 2018, Pathak et al., 2019, Gupta et al., 2022], since the optimism is being added to the prior data, the resulting exploration behaviors differ qualitatively. Adding novelty bonuses to online experience guides the agent to states it has already reached in the online phase, whereas adding optimism to the prior experience encourages the agent to learn to reach new states beyond the regions it has visited through online exploration. Intuitively, while standard optimism leads to exploration along the frontier of online data, optimism on prior data leads to exploration beyond the frontier of online data.", "md": "The prior data is labelled with rewards from the UCB estimate of our uncertainty model, and then the online and prior data is jointly trained upon. Training with this data leads to a policy that acts optimistically with respect to the agent\u2019s uncertainty about the reward function, which we use to guide exploration and collect new transitions. Algorithm 1 presents a sketch of our method.\n\nNotice that while this strategy is similar to exploration algorithms that use novelty-based reward bonuses [Bellemare et al., 2016, Tang et al., 2017, Pathak et al., 2017, Burda et al., 2018, Pathak et al., 2019, Gupta et al., 2022], since the optimism is being added to the prior data, the resulting exploration behaviors differ qualitatively. Adding novelty bonuses to online experience guides the agent to states it has already reached in the online phase, whereas adding optimism to the prior experience encourages the agent to learn to reach new states beyond the regions it has visited through online exploration. Intuitively, while standard optimism leads to exploration along the frontier of online data, optimism on prior data leads to exploration beyond the frontier of online data."}, {"type": "heading", "lvl": 2, "value": "Practical Implementation", "md": "## Practical Implementation"}, {"type": "text", "value": "In practice, we obtain UCB estimates of the rewards using a combination of a reward model and random-network-distillation (RND) [Burda et al., 2018], a novelty-based reward bonus technique. The former provides a reward estimate of the true reward value and the latter (RND) quantifies the uncertainty of the estimate, allowing us to get a UCB reward estimate.\n\nFor the reward model, we update a randomly initialized network $$r_{\\theta}(s, a)$$ simultaneously with the RL agent on the online replay buffer by minimizing the squared reward loss:\n\n$$L(\\theta) = E(s,a,r) \\sim D (r_{\\theta}(s, a) - r)^2$$\n\nRND randomly initializes two networks $$f_{\\phi}(s, a)$$, $$\\overline{f}(s, a)$$ that each output an L-dimensional feature vector for each state and action. We keep one network, $$\\overline{f}$$, frozen and update the parameters of the other network $$\\phi$$ to minimize the mean-square-error between the predicted features and the frozen ones once on every new transition $$(s_{new}, a_{new}, s'_{new}, r_{new})$$ encountered:\n\n$$L(\\phi) = \\frac{1}{L} \\lVert f(s_{new}, a_{new}) - \\overline{f}(s_{new}, a_{new}) \\rVert^2$$\n\nDuring online exploration, $$\\lVert f_{\\phi}(s, a) - \\overline{f}(s, a) \\rVert^2$$ forms a measure of the agent\u2019s uncertainty about $$(s, a)$$, leading us to the following approximate UCB estimate of the reward function:\n\n$$UCB_{R}(s, a) \\leftarrow r_{\\theta}(s, a) + \\frac{1}{L} \\lVert f(s, a) - \\overline{f}(s, a) \\rVert^2$$\n\nFor the RL agent, we use RLPD [Ball et al., 2023], which is specifically designed for efficient online RL and performs learning updates on both online and labeled offline data. Although we only have access to unlabeled data in our setting, we constantly label the prior data with UCB reward estimates, enabling a direct application of RLPD updates. It is worth noting that our method is compatible with potentially any other off-policy RL algorithms that can leverage labeled online and offline data. We experiment with an alternative based on implicit-Q learning in Appendix C.4.", "md": "In practice, we obtain UCB estimates of the rewards using a combination of a reward model and random-network-distillation (RND) [Burda et al., 2018], a novelty-based reward bonus technique. The former provides a reward estimate of the true reward value and the latter (RND) quantifies the uncertainty of the estimate, allowing us to get a UCB reward estimate.\n\nFor the reward model, we update a randomly initialized network $$r_{\\theta}(s, a)$$ simultaneously with the RL agent on the online replay buffer by minimizing the squared reward loss:\n\n$$L(\\theta) = E(s,a,r) \\sim D (r_{\\theta}(s, a) - r)^2$$\n\nRND randomly initializes two networks $$f_{\\phi}(s, a)$$, $$\\overline{f}(s, a)$$ that each output an L-dimensional feature vector for each state and action. We keep one network, $$\\overline{f}$$, frozen and update the parameters of the other network $$\\phi$$ to minimize the mean-square-error between the predicted features and the frozen ones once on every new transition $$(s_{new}, a_{new}, s'_{new}, r_{new})$$ encountered:\n\n$$L(\\phi) = \\frac{1}{L} \\lVert f(s_{new}, a_{new}) - \\overline{f}(s_{new}, a_{new}) \\rVert^2$$\n\nDuring online exploration, $$\\lVert f_{\\phi}(s, a) - \\overline{f}(s, a) \\rVert^2$$ forms a measure of the agent\u2019s uncertainty about $$(s, a)$$, leading us to the following approximate UCB estimate of the reward function:\n\n$$UCB_{R}(s, a) \\leftarrow r_{\\theta}(s, a) + \\frac{1}{L} \\lVert f(s, a) - \\overline{f}(s, a) \\rVert^2$$\n\nFor the RL agent, we use RLPD [Ball et al., 2023], which is specifically designed for efficient online RL and performs learning updates on both online and labeled offline data. Although we only have access to unlabeled data in our setting, we constantly label the prior data with UCB reward estimates, enabling a direct application of RLPD updates. It is worth noting that our method is compatible with potentially any other off-policy RL algorithms that can leverage labeled online and offline data. We experiment with an alternative based on implicit-Q learning in Appendix C.4."}]}, {"page": 5, "text": "4    Related Work\nExploration with reward bonuses. One standard approach for targeted exploration is to add ex-\nploration bonuses to the reward signal and train value functions and policies with respect to these\noptimistic rewards. Exploration bonuses seek to reward novelty, for example using an approximate\ndensity model [Bellemare et al., 2016, Tang et al., 2017], curiosity [Pathak et al., 2017, 2019], model\nerror [Stadie et al., 2015, Houthooft et al., 2016, Achiam and Sastry, 2017, Lin and Jabri, 2023], or\neven prediction error to a randomly initialized function [Burda et al., 2018]. When optimistic re-\nwards are calibrated correctly, the formal counterparts to these algorithms [Strehl and Littman, 2008,\nDann et al., 2017] enjoy strong theoretical guarantees for minimizing regret [Azar et al., 2017, Jin\net al., 2018]. Prior works typically augment the reward bonuses to the online replay buffer to en-\ncourage exploration of novel states, whereas in our work, we add reward bonuses to the offline data,\nencouraging the RL agent to traverse through trajectories in the prior data to explore more efficiently.\nUsing prior data without reward labels. Prior experience without reward annotations can be\nused for downstream RL in a variety of ways. One strategy is to use this data to pre-train state\nrepresentations and extract feature backbones for downstream networks, whether using standard\nimage-level objectives [Srinivas et al., 2020, Xiao et al., 2022, Radosavovic et al., 2022], or those\nthat emphasize the temporal structure of the environment [Yang and Nachum, 2021, Ma et al., 2023,\nFarebrother et al., 2023, Ghosh et al., 2023]. These approaches are complementary to our method,\nand in our experiments, we find that optimistic reward labeling using pre-trained representations\nlead to more coherent exploration and faster learning. Prior data may also inform behavioral priors\nor skills [Ajay et al., 2021, Tirumala et al., 2020, Pertsch et al., 2021b, Nasiriany et al., 2022];\nonce learned, these learned behaviors can provide a bootstrap for exploration [Uchendu et al., 2023,\nLi et al., 2023], or to transform the agent\u2019s action space to a higher level of abstraction [Pertsch\net al., 2021a, Singh et al., 2021a]. In addition, if the prior data consists of optimal demonstrations\nfor the downstream task, there exists a rich literature for steering online RL with demonstrations\n[Schaal, 1996, Vecerik et al., 2017, Nair et al., 2017, Zhu et al., 2020]. Finally, for model-based RL\napproaches, one in theory may additionally train a dynamics model on prior data and use it to plan\nand reach novel states [Sekar et al., 2020, Mendonca et al., 2021] for improved exploration, but we\nare not aware of works that explicitly use models to explore from reward-free offline prior data.\nUsing prior data with reward labels. To train on unlabeled prior data with optimistic reward\nlabels, we use RLPD [Ball et al., 2023], a sample-efficient online RL algorithm designed to train\nsimultaneously on reward-labeled offline data and incoming online experience. RLPD is an offline-\nto-online algorithm [Xie et al., 2021, Lee et al., 2021, Agarwal et al., 2022, Zheng et al., 2023,\nHao et al., 2023, Song et al., 2023] seeking to accelerate online RL using reward-labeled prior\nexperience. While this setting shares many challenges with our unlabeled prior data, algorithms in\nthis setting tend to focus on correcting errors in value function stemming from training on offline\ndata [Nakamoto et al., 2023], and correctly balancing between using highly off-policy transitions\nfrom the prior data and new online experience [Lee et al., 2021, Luo et al., 2023]. Our optimistic\nreward labeling mechanism is largely orthogonal to these axes, since our approach is compatible\nwith any online RL algorithm that can train on prior data.\nOnline unsupervised RL.        Online unsupervised RL is an adjacent setting in which agents collect\naction-free data to accelerate finetuning on some downstream task [Laskin et al., 2021]. While both\ndeal with reward-free learning, they crucially differ in motivation: our setting asks how existing\n(pre-collected) data can be used to inform downstream exploration for a known task, while unsuper-\nvised RL studies how to actively collect new datasets for a (yet to be specified) task [Yarats et al.,\n2022, Brandfonbrener et al., 2022]. Methods for unsupervised RL seek broad and undirected explo-\nration strategies, for example through information gain [Sekar et al., 2020, Liu and Abbeel, 2021]\nor state entropy maximization [Lee et al., 2019, Jin et al., 2020, Wang et al., 2020] \u2013 instead of the\ntask-relevant exploration that is advantageous in our setting. Successful mechanisms developed for\nunsupervised RL can inform useful strategies for our setting, since both seek to explore in directed\nways, whether undirected as in unsupervised RL or guided by prior data, as in ours.\nOffline meta reinforcement learning.       Offline meta-reinforcement learning [Dorfman et al., 2021,\nPong et al., 2021, Lin et al., 2022] also seeks to use prior datasets from a task distribution to accel-\nerate learning in new tasks sampled from the same distribution. These approaches generally tend\nto focus on the single-shot or few-shot setting, with methods that attempt to (approximately) solve\n                                                   5", "md": "# Related Work\n\n## Related Work\n\nExploration with reward bonuses. One standard approach for targeted exploration is to add exploration bonuses to the reward signal and train value functions and policies with respect to these optimistic rewards. Exploration bonuses seek to reward novelty, for example using an approximate density model [Bellemare et al., 2016, Tang et al., 2017], curiosity [Pathak et al., 2017, 2019], model error [Stadie et al., 2015, Houthooft et al., 2016, Achiam and Sastry, 2017, Lin and Jabri, 2023], or even prediction error to a randomly initialized function [Burda et al., 2018]. When optimistic rewards are calibrated correctly, the formal counterparts to these algorithms [Strehl and Littman, 2008, Dann et al., 2017] enjoy strong theoretical guarantees for minimizing regret [Azar et al., 2017, Jin et al., 2018]. Prior works typically augment the reward bonuses to the online replay buffer to encourage exploration of novel states, whereas in our work, we add reward bonuses to the offline data, encouraging the RL agent to traverse through trajectories in the prior data to explore more efficiently.\n\nUsing prior data without reward labels. Prior experience without reward annotations can be used for downstream RL in a variety of ways. One strategy is to use this data to pre-train state representations and extract feature backbones for downstream networks, whether using standard image-level objectives [Srinivas et al., 2020, Xiao et al., 2022, Radosavovic et al., 2022], or those that emphasize the temporal structure of the environment [Yang and Nachum, 2021, Ma et al., 2023, Farebrother et al., 2023, Ghosh et al., 2023]. These approaches are complementary to our method, and in our experiments, we find that optimistic reward labeling using pre-trained representations lead to more coherent exploration and faster learning. Prior data may also inform behavioral priors or skills [Ajay et al., 2021, Tirumala et al., 2020, Pertsch et al., 2021b, Nasiriany et al., 2022]; once learned, these learned behaviors can provide a bootstrap for exploration [Uchendu et al., 2023, Li et al., 2023], or to transform the agent\u2019s action space to a higher level of abstraction [Pertsch et al., 2021a, Singh et al., 2021a]. In addition, if the prior data consists of optimal demonstrations for the downstream task, there exists a rich literature for steering online RL with demonstrations [Schaal, 1996, Vecerik et al., 2017, Nair et al., 2017, Zhu et al., 2020]. Finally, for model-based RL approaches, one in theory may additionally train a dynamics model on prior data and use it to plan and reach novel states [Sekar et al., 2020, Mendonca et al., 2021] for improved exploration, but we are not aware of works that explicitly use models to explore from reward-free offline prior data.\n\nUsing prior data with reward labels. To train on unlabeled prior data with optimistic reward labels, we use RLPD [Ball et al., 2023], a sample-efficient online RL algorithm designed to train simultaneously on reward-labeled offline data and incoming online experience. RLPD is an offline-to-online algorithm [Xie et al., 2021, Lee et al., 2021, Agarwal et al., 2022, Zheng et al., 2023, Hao et al., 2023, Song et al., 2023] seeking to accelerate online RL using reward-labeled prior experience. While this setting shares many challenges with our unlabeled prior data, algorithms in this setting tend to focus on correcting errors in value function stemming from training on offline data [Nakamoto et al., 2023], and correctly balancing between using highly off-policy transitions from the prior data and new online experience [Lee et al., 2021, Luo et al., 2023]. Our optimistic reward labeling mechanism is largely orthogonal to these axes, since our approach is compatible with any online RL algorithm that can train on prior data.\n\nOnline unsupervised RL. Online unsupervised RL is an adjacent setting in which agents collect action-free data to accelerate finetuning on some downstream task [Laskin et al., 2021]. While both deal with reward-free learning, they crucially differ in motivation: our setting asks how existing (pre-collected) data can be used to inform downstream exploration for a known task, while unsupervised RL studies how to actively collect new datasets for a (yet to be specified) task [Yarats et al., 2022, Brandfonbrener et al., 2022]. Methods for unsupervised RL seek broad and undirected exploration strategies, for example through information gain [Sekar et al., 2020, Liu and Abbeel, 2021] or state entropy maximization [Lee et al., 2019, Jin et al., 2020, Wang et al., 2020] \u2013 instead of the task-relevant exploration that is advantageous in our setting. Successful mechanisms developed for unsupervised RL can inform useful strategies for our setting, since both seek to explore in directed ways, whether undirected as in unsupervised RL or guided by prior data, as in ours.\n\nOffline meta reinforcement learning. Offline meta-reinforcement learning [Dorfman et al., 2021, Pong et al., 2021, Lin et al., 2022] also seeks to use prior datasets from a task distribution to accelerate learning in new tasks sampled from the same distribution. These approaches generally tend to focus on the single-shot or few-shot setting, with methods that attempt to (approximately) solve", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Related Work", "md": "# Related Work"}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "Exploration with reward bonuses. One standard approach for targeted exploration is to add exploration bonuses to the reward signal and train value functions and policies with respect to these optimistic rewards. Exploration bonuses seek to reward novelty, for example using an approximate density model [Bellemare et al., 2016, Tang et al., 2017], curiosity [Pathak et al., 2017, 2019], model error [Stadie et al., 2015, Houthooft et al., 2016, Achiam and Sastry, 2017, Lin and Jabri, 2023], or even prediction error to a randomly initialized function [Burda et al., 2018]. When optimistic rewards are calibrated correctly, the formal counterparts to these algorithms [Strehl and Littman, 2008, Dann et al., 2017] enjoy strong theoretical guarantees for minimizing regret [Azar et al., 2017, Jin et al., 2018]. Prior works typically augment the reward bonuses to the online replay buffer to encourage exploration of novel states, whereas in our work, we add reward bonuses to the offline data, encouraging the RL agent to traverse through trajectories in the prior data to explore more efficiently.\n\nUsing prior data without reward labels. Prior experience without reward annotations can be used for downstream RL in a variety of ways. One strategy is to use this data to pre-train state representations and extract feature backbones for downstream networks, whether using standard image-level objectives [Srinivas et al., 2020, Xiao et al., 2022, Radosavovic et al., 2022], or those that emphasize the temporal structure of the environment [Yang and Nachum, 2021, Ma et al., 2023, Farebrother et al., 2023, Ghosh et al., 2023]. These approaches are complementary to our method, and in our experiments, we find that optimistic reward labeling using pre-trained representations lead to more coherent exploration and faster learning. Prior data may also inform behavioral priors or skills [Ajay et al., 2021, Tirumala et al., 2020, Pertsch et al., 2021b, Nasiriany et al., 2022]; once learned, these learned behaviors can provide a bootstrap for exploration [Uchendu et al., 2023, Li et al., 2023], or to transform the agent\u2019s action space to a higher level of abstraction [Pertsch et al., 2021a, Singh et al., 2021a]. In addition, if the prior data consists of optimal demonstrations for the downstream task, there exists a rich literature for steering online RL with demonstrations [Schaal, 1996, Vecerik et al., 2017, Nair et al., 2017, Zhu et al., 2020]. Finally, for model-based RL approaches, one in theory may additionally train a dynamics model on prior data and use it to plan and reach novel states [Sekar et al., 2020, Mendonca et al., 2021] for improved exploration, but we are not aware of works that explicitly use models to explore from reward-free offline prior data.\n\nUsing prior data with reward labels. To train on unlabeled prior data with optimistic reward labels, we use RLPD [Ball et al., 2023], a sample-efficient online RL algorithm designed to train simultaneously on reward-labeled offline data and incoming online experience. RLPD is an offline-to-online algorithm [Xie et al., 2021, Lee et al., 2021, Agarwal et al., 2022, Zheng et al., 2023, Hao et al., 2023, Song et al., 2023] seeking to accelerate online RL using reward-labeled prior experience. While this setting shares many challenges with our unlabeled prior data, algorithms in this setting tend to focus on correcting errors in value function stemming from training on offline data [Nakamoto et al., 2023], and correctly balancing between using highly off-policy transitions from the prior data and new online experience [Lee et al., 2021, Luo et al., 2023]. Our optimistic reward labeling mechanism is largely orthogonal to these axes, since our approach is compatible with any online RL algorithm that can train on prior data.\n\nOnline unsupervised RL. Online unsupervised RL is an adjacent setting in which agents collect action-free data to accelerate finetuning on some downstream task [Laskin et al., 2021]. While both deal with reward-free learning, they crucially differ in motivation: our setting asks how existing (pre-collected) data can be used to inform downstream exploration for a known task, while unsupervised RL studies how to actively collect new datasets for a (yet to be specified) task [Yarats et al., 2022, Brandfonbrener et al., 2022]. Methods for unsupervised RL seek broad and undirected exploration strategies, for example through information gain [Sekar et al., 2020, Liu and Abbeel, 2021] or state entropy maximization [Lee et al., 2019, Jin et al., 2020, Wang et al., 2020] \u2013 instead of the task-relevant exploration that is advantageous in our setting. Successful mechanisms developed for unsupervised RL can inform useful strategies for our setting, since both seek to explore in directed ways, whether undirected as in unsupervised RL or guided by prior data, as in ours.\n\nOffline meta reinforcement learning. Offline meta-reinforcement learning [Dorfman et al., 2021, Pong et al., 2021, Lin et al., 2022] also seeks to use prior datasets from a task distribution to accelerate learning in new tasks sampled from the same distribution. These approaches generally tend to focus on the single-shot or few-shot setting, with methods that attempt to (approximately) solve", "md": "Exploration with reward bonuses. One standard approach for targeted exploration is to add exploration bonuses to the reward signal and train value functions and policies with respect to these optimistic rewards. Exploration bonuses seek to reward novelty, for example using an approximate density model [Bellemare et al., 2016, Tang et al., 2017], curiosity [Pathak et al., 2017, 2019], model error [Stadie et al., 2015, Houthooft et al., 2016, Achiam and Sastry, 2017, Lin and Jabri, 2023], or even prediction error to a randomly initialized function [Burda et al., 2018]. When optimistic rewards are calibrated correctly, the formal counterparts to these algorithms [Strehl and Littman, 2008, Dann et al., 2017] enjoy strong theoretical guarantees for minimizing regret [Azar et al., 2017, Jin et al., 2018]. Prior works typically augment the reward bonuses to the online replay buffer to encourage exploration of novel states, whereas in our work, we add reward bonuses to the offline data, encouraging the RL agent to traverse through trajectories in the prior data to explore more efficiently.\n\nUsing prior data without reward labels. Prior experience without reward annotations can be used for downstream RL in a variety of ways. One strategy is to use this data to pre-train state representations and extract feature backbones for downstream networks, whether using standard image-level objectives [Srinivas et al., 2020, Xiao et al., 2022, Radosavovic et al., 2022], or those that emphasize the temporal structure of the environment [Yang and Nachum, 2021, Ma et al., 2023, Farebrother et al., 2023, Ghosh et al., 2023]. These approaches are complementary to our method, and in our experiments, we find that optimistic reward labeling using pre-trained representations lead to more coherent exploration and faster learning. Prior data may also inform behavioral priors or skills [Ajay et al., 2021, Tirumala et al., 2020, Pertsch et al., 2021b, Nasiriany et al., 2022]; once learned, these learned behaviors can provide a bootstrap for exploration [Uchendu et al., 2023, Li et al., 2023], or to transform the agent\u2019s action space to a higher level of abstraction [Pertsch et al., 2021a, Singh et al., 2021a]. In addition, if the prior data consists of optimal demonstrations for the downstream task, there exists a rich literature for steering online RL with demonstrations [Schaal, 1996, Vecerik et al., 2017, Nair et al., 2017, Zhu et al., 2020]. Finally, for model-based RL approaches, one in theory may additionally train a dynamics model on prior data and use it to plan and reach novel states [Sekar et al., 2020, Mendonca et al., 2021] for improved exploration, but we are not aware of works that explicitly use models to explore from reward-free offline prior data.\n\nUsing prior data with reward labels. To train on unlabeled prior data with optimistic reward labels, we use RLPD [Ball et al., 2023], a sample-efficient online RL algorithm designed to train simultaneously on reward-labeled offline data and incoming online experience. RLPD is an offline-to-online algorithm [Xie et al., 2021, Lee et al., 2021, Agarwal et al., 2022, Zheng et al., 2023, Hao et al., 2023, Song et al., 2023] seeking to accelerate online RL using reward-labeled prior experience. While this setting shares many challenges with our unlabeled prior data, algorithms in this setting tend to focus on correcting errors in value function stemming from training on offline data [Nakamoto et al., 2023], and correctly balancing between using highly off-policy transitions from the prior data and new online experience [Lee et al., 2021, Luo et al., 2023]. Our optimistic reward labeling mechanism is largely orthogonal to these axes, since our approach is compatible with any online RL algorithm that can train on prior data.\n\nOnline unsupervised RL. Online unsupervised RL is an adjacent setting in which agents collect action-free data to accelerate finetuning on some downstream task [Laskin et al., 2021]. While both deal with reward-free learning, they crucially differ in motivation: our setting asks how existing (pre-collected) data can be used to inform downstream exploration for a known task, while unsupervised RL studies how to actively collect new datasets for a (yet to be specified) task [Yarats et al., 2022, Brandfonbrener et al., 2022]. Methods for unsupervised RL seek broad and undirected exploration strategies, for example through information gain [Sekar et al., 2020, Liu and Abbeel, 2021] or state entropy maximization [Lee et al., 2019, Jin et al., 2020, Wang et al., 2020] \u2013 instead of the task-relevant exploration that is advantageous in our setting. Successful mechanisms developed for unsupervised RL can inform useful strategies for our setting, since both seek to explore in directed ways, whether undirected as in unsupervised RL or guided by prior data, as in ours.\n\nOffline meta reinforcement learning. Offline meta-reinforcement learning [Dorfman et al., 2021, Pong et al., 2021, Lin et al., 2022] also seeks to use prior datasets from a task distribution to accelerate learning in new tasks sampled from the same distribution. These approaches generally tend to focus on the single-shot or few-shot setting, with methods that attempt to (approximately) solve"}]}, {"page": 6, "text": "the Bayesian exploration POMDP [Duan et al., 2016, Zintgraf et al., 2019, Rakelly et al., 2019].\nWhile these methods are efficient when few-shot adaptation is possible, they struggle in scenarios\nrequiring more complex exploration or when the underlying task distribution lacks structure [Ajay\net al., 2022, Mandi et al., 2022]. Meta RL methods also rely on prior data containing meaningful,\ninteresting tasks, while our optimistic relabeling strategy is agnostic to the prior task distribution.\n5    Experiments\nOur experiments are aimed at evaluating how effectively our method can improve exploration by\nleveraging prior data. Through extensive experiments, we will establish that our method is able to\nleverage the unlabeled prior data to boost online sample efficiency consistently, often matching the\nsample efficiency of the previous best approach that uses labeled prior data (whereas we have no\naccess to labeled rewards). More concretely, we will answer the following questions:\n      1. Can we leverage unlabeled prior data to accelerate online learning?\n      2. Can representation learning help obtain better reward labels?\n      3. Does optimistic reward labeling help online learning by improving online exploration?\n      4. How robust is our method in handling different offline data corruptions?\n5.1   Experimental Setup\nThe environments that we evaluate our methods on are all challenging sparse reward tasks, including\nsix D4RL AntMaze tasks [Fu et al., 2020], three sparse-reward Adroit hand manipulation tasks [Nair\net al., 2021] following the setup in RLPD [Ball et al., 2023], and three image-based robotic manipu-\nlation tasks used by COG [Singh et al., 2021b]. See Appendix B for details about each task. For all\nthree domains, we explicitly remove the reward information from the offline dataset so that the agent\nmust explore to solve the task. We choose these environments as they cover a range of different char-\nacteristics of the prior data, where each can present a unique exploration and learning challenge. In\nAdroit, the prior data consists of a few expert trajectories on the target task and a much larger set of\ndata from a behavior cloning policy. In AntMaze, the prior data consists of goal-directed trajectories\nwith varying start and end locations. In COG, the prior data is mostly demonstration trajectories for\nsub-tasks of a larger multi-stage task. These tasks are interesting for the following reasons:\nSparse reward (All). Due to the sparse reward nature of the these tasks, the agent has no knowledge\nof the task, it must explore to solve the task successfully. For example, on AntMaze tasks, the goal is\nfor an ant robot to navigate in a maze to reach a fixed goal from a fixed starting point. The prior data\nfor this task consists of sub-optimal trajectories, using starting points and goals potentially different\nfrom the current task. Without knowledge of the goal location, the agent must explore each corner\nof the maze to identify the actual goal location before solving the task is possible.\nHigh-dimensional image observation (COG). The three COG tasks use image observations, which\nposes a much harder exploration challenge compared to state observations because of high dimen-\nsionality. As we will show, even with such high-dimensional observation spaces, our algorithm is\nstill able to efficiently explore and accelerate online learning more than the baselines. More im-\nportantly, we will demonstrate that our algorithm can easily incorporate pre-training representations\nthat can further accelerate online learning.\nMulti-stage exploration (COG). The robotic manipulation tasks in COG have two stages, meaning\nthat the robot needs to perform some sub-task (e.g., opening a drawer) before it is able to complete\nthe full task (e.g., picking out a ball from the drawer). See Figure 3 for visualizations of images\nfrom the COG dataset showing the two stage structure.\nThe prior data consists only of trajectories performing individual stages by themselves, and there\nis no prior data of complete task executions. The agent must learn to explore past the first stage\nof the task without any reward information, before it can receive the reward for completing the full\ntask. Therefore, this multi-stage experiment requires the agent to be able to effectively make use of\nincomplete trajectories to aid exploration. This setting is closer to the availability of the data one\nmay encounter in the real world. For example, there may be a lot of data of robots performing tasks\nthat are only partially related to the current task of interest. This data could also be unlabeled, as the\nrewards for other tasks may not match the current task. Despite being incomplete, we would still\nlike to be able to utilize this data.\n                                                      6", "md": "# Exploration in Meta Reinforcement Learning\n\n# Exploration in Meta Reinforcement Learning\n\nThe Bayesian exploration POMDP [Duan et al., 2016, Zintgraf et al., 2019, Rakelly et al., 2019]. While these methods are efficient when few-shot adaptation is possible, they struggle in scenarios requiring more complex exploration or when the underlying task distribution lacks structure [Ajay et al., 2022, Mandi et al., 2022]. Meta RL methods also rely on prior data containing meaningful, interesting tasks, while our optimistic relabeling strategy is agnostic to the prior task distribution.\n\n## Experiments\n\nOur experiments are aimed at evaluating how effectively our method can improve exploration by leveraging prior data. Through extensive experiments, we will establish that our method is able to leverage the unlabeled prior data to boost online sample efficiency consistently, often matching the sample efficiency of the previous best approach that uses labeled prior data (whereas we have no access to labeled rewards). More concretely, we will answer the following questions:\n\n1. Can we leverage unlabeled prior data to accelerate online learning?\n2. Can representation learning help obtain better reward labels?\n3. Does optimistic reward labeling help online learning by improving online exploration?\n4. How robust is our method in handling different offline data corruptions?\n\n### Experimental Setup\n\nThe environments that we evaluate our methods on are all challenging sparse reward tasks, including six D4RL AntMaze tasks [Fu et al., 2020], three sparse-reward Adroit hand manipulation tasks [Nair et al., 2021] following the setup in RLPD [Ball et al., 2023], and three image-based robotic manipulation tasks used by COG [Singh et al., 2021b]. See Appendix B for details about each task. For all three domains, we explicitly remove the reward information from the offline dataset so that the agent must explore to solve the task. We choose these environments as they cover a range of different characteristics of the prior data, where each can present a unique exploration and learning challenge.\n\nSparse reward (All): Due to the sparse reward nature of these tasks, the agent has no knowledge of the task, it must explore to solve the task successfully. For example, on AntMaze tasks, the goal is for an ant robot to navigate in a maze to reach a fixed goal from a fixed starting point. The prior data for this task consists of sub-optimal trajectories, using starting points and goals potentially different from the current task. Without knowledge of the goal location, the agent must explore each corner of the maze to identify the actual goal location before solving the task is possible.\n\nHigh-dimensional image observation (COG): The three COG tasks use image observations, which poses a much harder exploration challenge compared to state observations because of high dimensionality. As we will show, even with such high-dimensional observation spaces, our algorithm is still able to efficiently explore and accelerate online learning more than the baselines. More importantly, we will demonstrate that our algorithm can easily incorporate pre-training representations that can further accelerate online learning.\n\nMulti-stage exploration (COG): The robotic manipulation tasks in COG have two stages, meaning that the robot needs to perform some sub-task (e.g., opening a drawer) before it is able to complete the full task (e.g., picking out a ball from the drawer). The prior data consists only of trajectories performing individual stages by themselves, and there is no prior data of complete task executions. The agent must learn to explore past the first stage of the task without any reward information, before it can receive the reward for completing the full task. Therefore, this multi-stage experiment requires the agent to be able to effectively make use of incomplete trajectories to aid exploration. This setting is closer to the availability of the data one may encounter in the real world. For example, there may be a lot of data of robots performing tasks that are only partially related to the current task of interest. This data could also be unlabeled, as the rewards for other tasks may not match the current task. Despite being incomplete, we would still like to be able to utilize this data.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Exploration in Meta Reinforcement Learning", "md": "# Exploration in Meta Reinforcement Learning"}, {"type": "heading", "lvl": 1, "value": "Exploration in Meta Reinforcement Learning", "md": "# Exploration in Meta Reinforcement Learning"}, {"type": "text", "value": "The Bayesian exploration POMDP [Duan et al., 2016, Zintgraf et al., 2019, Rakelly et al., 2019]. While these methods are efficient when few-shot adaptation is possible, they struggle in scenarios requiring more complex exploration or when the underlying task distribution lacks structure [Ajay et al., 2022, Mandi et al., 2022]. Meta RL methods also rely on prior data containing meaningful, interesting tasks, while our optimistic relabeling strategy is agnostic to the prior task distribution.", "md": "The Bayesian exploration POMDP [Duan et al., 2016, Zintgraf et al., 2019, Rakelly et al., 2019]. While these methods are efficient when few-shot adaptation is possible, they struggle in scenarios requiring more complex exploration or when the underlying task distribution lacks structure [Ajay et al., 2022, Mandi et al., 2022]. Meta RL methods also rely on prior data containing meaningful, interesting tasks, while our optimistic relabeling strategy is agnostic to the prior task distribution."}, {"type": "heading", "lvl": 2, "value": "Experiments", "md": "## Experiments"}, {"type": "text", "value": "Our experiments are aimed at evaluating how effectively our method can improve exploration by leveraging prior data. Through extensive experiments, we will establish that our method is able to leverage the unlabeled prior data to boost online sample efficiency consistently, often matching the sample efficiency of the previous best approach that uses labeled prior data (whereas we have no access to labeled rewards). More concretely, we will answer the following questions:\n\n1. Can we leverage unlabeled prior data to accelerate online learning?\n2. Can representation learning help obtain better reward labels?\n3. Does optimistic reward labeling help online learning by improving online exploration?\n4. How robust is our method in handling different offline data corruptions?", "md": "Our experiments are aimed at evaluating how effectively our method can improve exploration by leveraging prior data. Through extensive experiments, we will establish that our method is able to leverage the unlabeled prior data to boost online sample efficiency consistently, often matching the sample efficiency of the previous best approach that uses labeled prior data (whereas we have no access to labeled rewards). More concretely, we will answer the following questions:\n\n1. Can we leverage unlabeled prior data to accelerate online learning?\n2. Can representation learning help obtain better reward labels?\n3. Does optimistic reward labeling help online learning by improving online exploration?\n4. How robust is our method in handling different offline data corruptions?"}, {"type": "heading", "lvl": 3, "value": "Experimental Setup", "md": "### Experimental Setup"}, {"type": "text", "value": "The environments that we evaluate our methods on are all challenging sparse reward tasks, including six D4RL AntMaze tasks [Fu et al., 2020], three sparse-reward Adroit hand manipulation tasks [Nair et al., 2021] following the setup in RLPD [Ball et al., 2023], and three image-based robotic manipulation tasks used by COG [Singh et al., 2021b]. See Appendix B for details about each task. For all three domains, we explicitly remove the reward information from the offline dataset so that the agent must explore to solve the task. We choose these environments as they cover a range of different characteristics of the prior data, where each can present a unique exploration and learning challenge.\n\nSparse reward (All): Due to the sparse reward nature of these tasks, the agent has no knowledge of the task, it must explore to solve the task successfully. For example, on AntMaze tasks, the goal is for an ant robot to navigate in a maze to reach a fixed goal from a fixed starting point. The prior data for this task consists of sub-optimal trajectories, using starting points and goals potentially different from the current task. Without knowledge of the goal location, the agent must explore each corner of the maze to identify the actual goal location before solving the task is possible.\n\nHigh-dimensional image observation (COG): The three COG tasks use image observations, which poses a much harder exploration challenge compared to state observations because of high dimensionality. As we will show, even with such high-dimensional observation spaces, our algorithm is still able to efficiently explore and accelerate online learning more than the baselines. More importantly, we will demonstrate that our algorithm can easily incorporate pre-training representations that can further accelerate online learning.\n\nMulti-stage exploration (COG): The robotic manipulation tasks in COG have two stages, meaning that the robot needs to perform some sub-task (e.g., opening a drawer) before it is able to complete the full task (e.g., picking out a ball from the drawer). The prior data consists only of trajectories performing individual stages by themselves, and there is no prior data of complete task executions. The agent must learn to explore past the first stage of the task without any reward information, before it can receive the reward for completing the full task. Therefore, this multi-stage experiment requires the agent to be able to effectively make use of incomplete trajectories to aid exploration. This setting is closer to the availability of the data one may encounter in the real world. For example, there may be a lot of data of robots performing tasks that are only partially related to the current task of interest. This data could also be unlabeled, as the rewards for other tasks may not match the current task. Despite being incomplete, we would still like to be able to utilize this data.", "md": "The environments that we evaluate our methods on are all challenging sparse reward tasks, including six D4RL AntMaze tasks [Fu et al., 2020], three sparse-reward Adroit hand manipulation tasks [Nair et al., 2021] following the setup in RLPD [Ball et al., 2023], and three image-based robotic manipulation tasks used by COG [Singh et al., 2021b]. See Appendix B for details about each task. For all three domains, we explicitly remove the reward information from the offline dataset so that the agent must explore to solve the task. We choose these environments as they cover a range of different characteristics of the prior data, where each can present a unique exploration and learning challenge.\n\nSparse reward (All): Due to the sparse reward nature of these tasks, the agent has no knowledge of the task, it must explore to solve the task successfully. For example, on AntMaze tasks, the goal is for an ant robot to navigate in a maze to reach a fixed goal from a fixed starting point. The prior data for this task consists of sub-optimal trajectories, using starting points and goals potentially different from the current task. Without knowledge of the goal location, the agent must explore each corner of the maze to identify the actual goal location before solving the task is possible.\n\nHigh-dimensional image observation (COG): The three COG tasks use image observations, which poses a much harder exploration challenge compared to state observations because of high dimensionality. As we will show, even with such high-dimensional observation spaces, our algorithm is still able to efficiently explore and accelerate online learning more than the baselines. More importantly, we will demonstrate that our algorithm can easily incorporate pre-training representations that can further accelerate online learning.\n\nMulti-stage exploration (COG): The robotic manipulation tasks in COG have two stages, meaning that the robot needs to perform some sub-task (e.g., opening a drawer) before it is able to complete the full task (e.g., picking out a ball from the drawer). The prior data consists only of trajectories performing individual stages by themselves, and there is no prior data of complete task executions. The agent must learn to explore past the first stage of the task without any reward information, before it can receive the reward for completing the full task. Therefore, this multi-stage experiment requires the agent to be able to effectively make use of incomplete trajectories to aid exploration. This setting is closer to the availability of the data one may encounter in the real world. For example, there may be a lot of data of robots performing tasks that are only partially related to the current task of interest. This data could also be unlabeled, as the rewards for other tasks may not match the current task. Despite being incomplete, we would still like to be able to utilize this data."}]}, {"page": 7, "text": "Figure 3: Visualizations of data used in the COG tasks. The left block contains the first stage of the task,\nand the right block contains the second stage of the task. The first row is for the Pick and Place task, the\nsecond row is picking out a ball from a closed drawer, the Grasp from Closed Drawer task, and the\nthird row is picking out a ball from a blocked drawer, the Grasp from Blocked Drawer 1 task.\n5.2   Comparisons\nWhile most prior works do not study the setting where the prior data is unlabeled, focusing on either\nthe standard fully labeled data setting or utilizing an online unsupervised learning phase, we can\nadapt some of these prior works as comparisons in our setting (see Appendix A for details):\nOnline: A data-efficient off-policy RL agent that does not make use of the prior data at all.\nOnline + RND: An augmentation of online RL with RND as a novelty bonus. This baseline uses\nonly online data, augmenting the online batch with RND bonus [Burda et al., 2018]. To be clear, of\nthe comparisons listed here in bold, this is the only one that uses an online RND bonus.\nNa\u00efve Reward Labeling: This comparison labels the prior data reward with an unbiased reward\nestimate; this is implemented using our method but omitting the RND novelty score.\nNa\u00efve + BC: This baseline additionally uses a behavioral cloning loss to follow the behaviors as\nseen in prior data, inspired by similar regularization in offline RL [Vecerik et al., 2017].\nMinR: This is an adaptation of UDS [Yu et al., 2022] to the online fine-tuning setting. The original\nUDS method uses unlabeled prior data to improve offline RL on a smaller reward-labeled dataset,\nusing the minimum reward of the task to relabel the unlabeled data. MinR uses the same labeling\nstrategy, but with RLPD for online RL.\nBC + JSRL: This baseline is an adaptation of JSRL [Uchendu et al., 2023]. The original JSRL\nmethod uses offline RL algorithms to pre-train a guide policy using a fully labeled prior dataset.\nThen, at each online episode, it uses the guide policy to roll out the trajectory up to a random\nnumber of steps, then switches to an exploration policy such that the initial state distribution of the\nexploration policy is shaped by the state visitation of the guide-policy, inducing faster learning of\nthe exploration policy. Since in our setting the prior data has no labels, we use behavior cloning\n(BC) pre-training to initialize the guide-policy instead of using offline RL algorithms.\nOracle: This is an oracle baseline that assumes access to ground truth reward labels, using the same\nbase off-policy RL algorithm [Ball et al., 2023] with true reward labels on prior data.\nWe also include a behavior prior baseline (inspired by the behavior prior learning line of work [Ajay\net al., 2021, Tirumala et al., 2020, Pertsch et al., 2021b, Nasiriany et al., 2022, Singh et al., 2021a])\ncomparison in Appendix C.5 on AntMaze. The baseline uses pre-trained goal-conditioned policies\nas behavioral priors to transform the agent\u2019s action space and use online RL with exploration bonus\non this transformed action space.\n5.3   Does optimistic labeling of prior data accelerate online learning?\nFigure 4 shows the aggregated performance of our approach on the two state-based domains and\none image-based domain. On AntMaze and COG domains, our optimistic reward labeling is able to\noutperform na\u00efve reward labeling significantly, highlighting its effectiveness in leveraging the prior\ndata to accelerate online learning. Interestingly, in the two state-based domains, we show that with-\nout access to prior rewards, our method can nearly match the performance of the oracle baseline with\n                                                     7", "md": "# Document\n\n## Figure 3: Visualizations of data used in the COG tasks\n\nThe left block contains the first stage of the task, and the right block contains the second stage of the task. The first row is for the Pick and Place task, the second row is picking out a ball from a closed drawer, the Grasp from Closed Drawer task, and the third row is picking out a ball from a blocked drawer, the Grasp from Blocked Drawer 1 task.\n\n### 5.2 Comparisons\n\nWhile most prior works do not study the setting where the prior data is unlabeled, focusing on either the standard fully labeled data setting or utilizing an online unsupervised learning phase, we can adapt some of these prior works as comparisons in our setting (see Appendix A for details):\n\n- Online: A data-efficient off-policy RL agent that does not make use of the prior data at all.\n- Online + RND: An augmentation of online RL with RND as a novelty bonus. This baseline uses only online data, augmenting the online batch with RND bonus [Burda et al., 2018]. To be clear, of the comparisons listed here in bold, this is the only one that uses an online RND bonus.\n- Na\u00efve Reward Labeling: This comparison labels the prior data reward with an unbiased reward estimate; this is implemented using our method but omitting the RND novelty score.\n- Na\u00efve + BC: This baseline additionally uses a behavioral cloning loss to follow the behaviors as seen in prior data, inspired by similar regularization in offline RL [Vecerik et al., 2017].\n- MinR: This is an adaptation of UDS [Yu et al., 2022] to the online fine-tuning setting. The original UDS method uses unlabeled prior data to improve offline RL on a smaller reward-labeled dataset, using the minimum reward of the task to relabel the unlabeled data. MinR uses the same labeling strategy, but with RLPD for online RL.\n- BC + JSRL: This baseline is an adaptation of JSRL [Uchendu et al., 2023]. The original JSRL method uses offline RL algorithms to pre-train a guide policy using a fully labeled prior dataset. Then, at each online episode, it uses the guide policy to roll out the trajectory up to a random number of steps, then switches to an exploration policy such that the initial state distribution of the exploration policy is shaped by the state visitation of the guide-policy, inducing faster learning of the exploration policy. Since in our setting the prior data has no labels, we use behavior cloning (BC) pre-training to initialize the guide-policy instead of using offline RL algorithms.\n- Oracle: This is an oracle baseline that assumes access to ground truth reward labels, using the same base off-policy RL algorithm [Ball et al., 2023] with true reward labels on prior data.\n\nWe also include a behavior prior baseline (inspired by the behavior prior learning line of work [Ajay et al., 2021, Tirumala et al., 2020, Pertsch et al., 2021b, Nasiriany et al., 2022, Singh et al., 2021a]) comparison in Appendix C.5 on AntMaze. The baseline uses pre-trained goal-conditioned policies as behavioral priors to transform the agent\u2019s action space and use online RL with exploration bonus on this transformed action space.\n\n### 5.3 Does optimistic labeling of prior data accelerate online learning?\n\nFigure 4 shows the aggregated performance of our approach on the two state-based domains and one image-based domain. On AntMaze and COG domains, our optimistic reward labeling is able to outperform na\u00efve reward labeling significantly, highlighting its effectiveness in leveraging the prior data to accelerate online learning. Interestingly, in the two state-based domains, we show that without access to prior rewards, our method can nearly match the performance of the oracle baseline.", "images": [{"name": "page-7-1.jpg", "height": 97, "width": 71, "x": 183, "y": 76}, {"name": "page-7-3.jpg", "height": 97, "width": 191, "x": 309, "y": 76}, {"name": "page-7-2.jpg", "height": 97, "width": 36, "x": 254, "y": 76}, {"name": "page-7-0.jpg", "height": 97, "width": 71, "x": 112, "y": 76}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Figure 3: Visualizations of data used in the COG tasks", "md": "## Figure 3: Visualizations of data used in the COG tasks"}, {"type": "text", "value": "The left block contains the first stage of the task, and the right block contains the second stage of the task. The first row is for the Pick and Place task, the second row is picking out a ball from a closed drawer, the Grasp from Closed Drawer task, and the third row is picking out a ball from a blocked drawer, the Grasp from Blocked Drawer 1 task.", "md": "The left block contains the first stage of the task, and the right block contains the second stage of the task. The first row is for the Pick and Place task, the second row is picking out a ball from a closed drawer, the Grasp from Closed Drawer task, and the third row is picking out a ball from a blocked drawer, the Grasp from Blocked Drawer 1 task."}, {"type": "heading", "lvl": 3, "value": "5.2 Comparisons", "md": "### 5.2 Comparisons"}, {"type": "text", "value": "While most prior works do not study the setting where the prior data is unlabeled, focusing on either the standard fully labeled data setting or utilizing an online unsupervised learning phase, we can adapt some of these prior works as comparisons in our setting (see Appendix A for details):\n\n- Online: A data-efficient off-policy RL agent that does not make use of the prior data at all.\n- Online + RND: An augmentation of online RL with RND as a novelty bonus. This baseline uses only online data, augmenting the online batch with RND bonus [Burda et al., 2018]. To be clear, of the comparisons listed here in bold, this is the only one that uses an online RND bonus.\n- Na\u00efve Reward Labeling: This comparison labels the prior data reward with an unbiased reward estimate; this is implemented using our method but omitting the RND novelty score.\n- Na\u00efve + BC: This baseline additionally uses a behavioral cloning loss to follow the behaviors as seen in prior data, inspired by similar regularization in offline RL [Vecerik et al., 2017].\n- MinR: This is an adaptation of UDS [Yu et al., 2022] to the online fine-tuning setting. The original UDS method uses unlabeled prior data to improve offline RL on a smaller reward-labeled dataset, using the minimum reward of the task to relabel the unlabeled data. MinR uses the same labeling strategy, but with RLPD for online RL.\n- BC + JSRL: This baseline is an adaptation of JSRL [Uchendu et al., 2023]. The original JSRL method uses offline RL algorithms to pre-train a guide policy using a fully labeled prior dataset. Then, at each online episode, it uses the guide policy to roll out the trajectory up to a random number of steps, then switches to an exploration policy such that the initial state distribution of the exploration policy is shaped by the state visitation of the guide-policy, inducing faster learning of the exploration policy. Since in our setting the prior data has no labels, we use behavior cloning (BC) pre-training to initialize the guide-policy instead of using offline RL algorithms.\n- Oracle: This is an oracle baseline that assumes access to ground truth reward labels, using the same base off-policy RL algorithm [Ball et al., 2023] with true reward labels on prior data.\n\nWe also include a behavior prior baseline (inspired by the behavior prior learning line of work [Ajay et al., 2021, Tirumala et al., 2020, Pertsch et al., 2021b, Nasiriany et al., 2022, Singh et al., 2021a]) comparison in Appendix C.5 on AntMaze. The baseline uses pre-trained goal-conditioned policies as behavioral priors to transform the agent\u2019s action space and use online RL with exploration bonus on this transformed action space.", "md": "While most prior works do not study the setting where the prior data is unlabeled, focusing on either the standard fully labeled data setting or utilizing an online unsupervised learning phase, we can adapt some of these prior works as comparisons in our setting (see Appendix A for details):\n\n- Online: A data-efficient off-policy RL agent that does not make use of the prior data at all.\n- Online + RND: An augmentation of online RL with RND as a novelty bonus. This baseline uses only online data, augmenting the online batch with RND bonus [Burda et al., 2018]. To be clear, of the comparisons listed here in bold, this is the only one that uses an online RND bonus.\n- Na\u00efve Reward Labeling: This comparison labels the prior data reward with an unbiased reward estimate; this is implemented using our method but omitting the RND novelty score.\n- Na\u00efve + BC: This baseline additionally uses a behavioral cloning loss to follow the behaviors as seen in prior data, inspired by similar regularization in offline RL [Vecerik et al., 2017].\n- MinR: This is an adaptation of UDS [Yu et al., 2022] to the online fine-tuning setting. The original UDS method uses unlabeled prior data to improve offline RL on a smaller reward-labeled dataset, using the minimum reward of the task to relabel the unlabeled data. MinR uses the same labeling strategy, but with RLPD for online RL.\n- BC + JSRL: This baseline is an adaptation of JSRL [Uchendu et al., 2023]. The original JSRL method uses offline RL algorithms to pre-train a guide policy using a fully labeled prior dataset. Then, at each online episode, it uses the guide policy to roll out the trajectory up to a random number of steps, then switches to an exploration policy such that the initial state distribution of the exploration policy is shaped by the state visitation of the guide-policy, inducing faster learning of the exploration policy. Since in our setting the prior data has no labels, we use behavior cloning (BC) pre-training to initialize the guide-policy instead of using offline RL algorithms.\n- Oracle: This is an oracle baseline that assumes access to ground truth reward labels, using the same base off-policy RL algorithm [Ball et al., 2023] with true reward labels on prior data.\n\nWe also include a behavior prior baseline (inspired by the behavior prior learning line of work [Ajay et al., 2021, Tirumala et al., 2020, Pertsch et al., 2021b, Nasiriany et al., 2022, Singh et al., 2021a]) comparison in Appendix C.5 on AntMaze. The baseline uses pre-trained goal-conditioned policies as behavioral priors to transform the agent\u2019s action space and use online RL with exploration bonus on this transformed action space."}, {"type": "heading", "lvl": 3, "value": "5.3 Does optimistic labeling of prior data accelerate online learning?", "md": "### 5.3 Does optimistic labeling of prior data accelerate online learning?"}, {"type": "text", "value": "Figure 4 shows the aggregated performance of our approach on the two state-based domains and one image-based domain. On AntMaze and COG domains, our optimistic reward labeling is able to outperform na\u00efve reward labeling significantly, highlighting its effectiveness in leveraging the prior data to accelerate online learning. Interestingly, in the two state-based domains, we show that without access to prior rewards, our method can nearly match the performance of the oracle baseline.", "md": "Figure 4 shows the aggregated performance of our approach on the two state-based domains and one image-based domain. On AntMaze and COG domains, our optimistic reward labeling is able to outperform na\u00efve reward labeling significantly, highlighting its effectiveness in leveraging the prior data to accelerate online learning. Interestingly, in the two state-based domains, we show that without access to prior rewards, our method can nearly match the performance of the oracle baseline."}]}, {"page": 8, "text": "           Oracle                 Online                           MinR                  Naive + BC\n           Ours                   Online + RND                     Naive                 BC + JSRL\n                AntMaze (6 Tasks)            Sparse Adroit (3 Tasks)              COG (3 Tasks)\n       Normalized Return\n          1.0\n          0.8\n          0.6\n          0.4\n          0.2\n          0.00.0     0.1      0.2     0.3   0.0    0.2   0.4    0.6   0.8   0.0       0.1        0.2\n                                Environment Steps (\u00d7106)\nFigure 4: Aggregated results for 6 AntMaze tasks, 3 Adroit tasks and 3 COG tasks. Ours is the only method\nthat largely recovers the performance of vanilla RLPD (Oracle), which has access to true rewards (Ours does\nnot). The baselines that do not use the prior unlabeled data (Online and Online + RND) perform poorly on\nboth AntMaze and Adroit. Na\u00efvely labeling the prior data without optimism (Na\u00efve) performs competitively\non Adroit but poorly on AntMaze. Section 5.2 contains the description for the baselines we compare with.\nprior rewards. This suggests that optimistic reward labeling can allow RL to utilize unlabeled prior\ndata almost as effectively as labeled prior data on these three domains. In particular, on the sparse\nAdroit relocate task, using optimistic rewards can even outpace the oracle baseline (Appendix\nC.2, Figure 12). In the COG domain, there is a larger performance gap between our method and\nthe oracle. We hypothesize that this is due to the high-dimensional image observation space, caus-\ning the exploration problem to be more challenging. On the Adroit domain, we find that periodic\nresetting of the learned reward function is necessary to avoid overfitting to the collected online expe-\nrience \u2013 with the resetting, we find that even na\u00efve reward relabeling results in exploratory behavior,\ndiminishing the gap when performing explicit reward uncertainty estimation with RND.\n5.4   Can representation learning help obtain better reward labels?\nWe now investigate whether we can improve exploration by basing our UCB reward estimates on\ntop of pre-trained representations acquired from the prior data. We chose to use representations\nfrom ICVF [Ghosh et al., 2023], a method that trains feature representations by pre-training gen-\neral successor value functions on the offline data. Value-aware pre-training is a natural choice for\ndownstream reward uncertainty quantification, because the pre-training encourages representations\nto obey the spatiality of the environment \u2013 that states close to one another have similar representa-\ntions, while states that are difficult to reach from one another are far away.\nWe pre-train an ICVF model on the prior dataset and extract the visual encoder \u03beICVF(s), which\nencodes an image into a low-dimensional feature vector. These pre-trained encoder parameters are\nused to initialize the encoder for both the reward model r\u03b8(\u03be\u03b8(s)) and the RND network f\u03d5(\u03be\u03d5(s)).\nNote that these encoder parameters are not shared and not frozen, meaning that once training begins,\nthe encoder parameters for the reward model will be different from the encoder parameters for the\nRND model, and also diverge from the pre-trained initialization.\nAs shown in Figure 5, ICVF consistently improves the online learning efficiency across all image-\ninput tasks on top of optimistic reward labeling. It is worth noting that the encoder initialization\nfrom pre-training alone is not sufficient for the learning agent to succeed online (Na\u00efve + ICVF\nstill fails almost everywhere). On most tasks, without optimistic reward labeling, the agent simply\nmakes zero progress, further highlighting the effectiveness of optimistic labeling in aiding online\nexploration and learning.\nTo understand how the pre-trained representation initialization influences online exploration, we an-\nalyze its effect on the UCB reward estimate early in the training. Figure 6 shows the UCB reward\n                                                     8", "md": "# Document\n\n## Oracle Online MinR Naive + BC\n\n## Ours Online + RND Naive BC + JSRL\n\n## AntMaze (6 Tasks) Sparse Adroit (3 Tasks) COG (3 Tasks)\n\n### Normalized Return\n\n- 1.0\n- 0.8\n- 0.6\n- 0.4\n- 0.2\n- 0.0\n\nEnvironment Steps (\u00d7106)\n\nFigure 4: Aggregated results for 6 AntMaze tasks, 3 Adroit tasks and 3 COG tasks. Ours is the only method\nthat largely recovers the performance of vanilla RLPD (Oracle), which has access to true rewards (Ours does\nnot). The baselines that do not use the prior unlabeled data (Online and Online + RND) perform poorly on\nboth AntMaze and Adroit. Na\u00efvely labeling the prior data without optimism (Na\u00efve) performs competitively\non Adroit but poorly on AntMaze. Section 5.2 contains the description for the baselines we compare with.\nprior rewards. This suggests that optimistic reward labeling can allow RL to utilize unlabeled prior\ndata almost as effectively as labeled prior data on these three domains. In particular, on the sparse\nAdroit relocate task, using optimistic rewards can even outpace the oracle baseline (Appendix\nC.2, Figure 12). In the COG domain, there is a larger performance gap between our method and\nthe oracle. We hypothesize that this is due to the high-dimensional image observation space, causing\nthe exploration problem to be more challenging. On the Adroit domain, we find that periodic\nresetting of the learned reward function is necessary to avoid overfitting to the collected online experience \u2013 with the resetting, we find that even na\u00efve reward relabeling results in exploratory behavior,\ndiminishing the gap when performing explicit reward uncertainty estimation with RND.\n\n### 5.4 Can representation learning help obtain better reward labels?\n\nWe now investigate whether we can improve exploration by basing our UCB reward estimates on\ntop of pre-trained representations acquired from the prior data. We chose to use representations\nfrom ICVF [Ghosh et al., 2023], a method that trains feature representations by pre-training gen-\neral successor value functions on the offline data. Value-aware pre-training is a natural choice for\ndownstream reward uncertainty quantification, because the pre-training encourages representations\nto obey the spatiality of the environment \u2013 that states close to one another have similar representa-\ntions, while states that are difficult to reach from one another are far away.\n\nWe pre-train an ICVF model on the prior dataset and extract the visual encoder \u03beICVF(s), which\nencodes an image into a low-dimensional feature vector. These pre-trained encoder parameters are\nused to initialize the encoder for both the reward model r\u03b8(\u03be\u03b8(s)) and the RND network f\u03d5(\u03be\u03d5(s)).\nNote that these encoder parameters are not shared and not frozen, meaning that once training begins,\nthe encoder parameters for the reward model will be different from the encoder parameters for the\nRND model, and also diverge from the pre-trained initialization.\n\nAs shown in Figure 5, ICVF consistently improves the online learning efficiency across all image-\ninput tasks on top of optimistic reward labeling. It is worth noting that the encoder initialization\nfrom pre-training alone is not sufficient for the learning agent to succeed online (Na\u00efve + ICVF\nstill fails almost everywhere). On most tasks, without optimistic reward labeling, the agent simply\nmakes zero progress, further highlighting the effectiveness of optimistic labeling in aiding online\nexploration and learning.\n\nTo understand how the pre-trained representation initialization influences online exploration, we an-\nalyze its effect on the UCB reward estimate early in the training. Figure 6 shows the UCB reward", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Oracle Online MinR Naive + BC", "md": "## Oracle Online MinR Naive + BC"}, {"type": "heading", "lvl": 2, "value": "Ours Online + RND Naive BC + JSRL", "md": "## Ours Online + RND Naive BC + JSRL"}, {"type": "heading", "lvl": 2, "value": "AntMaze (6 Tasks) Sparse Adroit (3 Tasks) COG (3 Tasks)", "md": "## AntMaze (6 Tasks) Sparse Adroit (3 Tasks) COG (3 Tasks)"}, {"type": "heading", "lvl": 3, "value": "Normalized Return", "md": "### Normalized Return"}, {"type": "text", "value": "- 1.0\n- 0.8\n- 0.6\n- 0.4\n- 0.2\n- 0.0\n\nEnvironment Steps (\u00d7106)\n\nFigure 4: Aggregated results for 6 AntMaze tasks, 3 Adroit tasks and 3 COG tasks. Ours is the only method\nthat largely recovers the performance of vanilla RLPD (Oracle), which has access to true rewards (Ours does\nnot). The baselines that do not use the prior unlabeled data (Online and Online + RND) perform poorly on\nboth AntMaze and Adroit. Na\u00efvely labeling the prior data without optimism (Na\u00efve) performs competitively\non Adroit but poorly on AntMaze. Section 5.2 contains the description for the baselines we compare with.\nprior rewards. This suggests that optimistic reward labeling can allow RL to utilize unlabeled prior\ndata almost as effectively as labeled prior data on these three domains. In particular, on the sparse\nAdroit relocate task, using optimistic rewards can even outpace the oracle baseline (Appendix\nC.2, Figure 12). In the COG domain, there is a larger performance gap between our method and\nthe oracle. We hypothesize that this is due to the high-dimensional image observation space, causing\nthe exploration problem to be more challenging. On the Adroit domain, we find that periodic\nresetting of the learned reward function is necessary to avoid overfitting to the collected online experience \u2013 with the resetting, we find that even na\u00efve reward relabeling results in exploratory behavior,\ndiminishing the gap when performing explicit reward uncertainty estimation with RND.", "md": "- 1.0\n- 0.8\n- 0.6\n- 0.4\n- 0.2\n- 0.0\n\nEnvironment Steps (\u00d7106)\n\nFigure 4: Aggregated results for 6 AntMaze tasks, 3 Adroit tasks and 3 COG tasks. Ours is the only method\nthat largely recovers the performance of vanilla RLPD (Oracle), which has access to true rewards (Ours does\nnot). The baselines that do not use the prior unlabeled data (Online and Online + RND) perform poorly on\nboth AntMaze and Adroit. Na\u00efvely labeling the prior data without optimism (Na\u00efve) performs competitively\non Adroit but poorly on AntMaze. Section 5.2 contains the description for the baselines we compare with.\nprior rewards. This suggests that optimistic reward labeling can allow RL to utilize unlabeled prior\ndata almost as effectively as labeled prior data on these three domains. In particular, on the sparse\nAdroit relocate task, using optimistic rewards can even outpace the oracle baseline (Appendix\nC.2, Figure 12). In the COG domain, there is a larger performance gap between our method and\nthe oracle. We hypothesize that this is due to the high-dimensional image observation space, causing\nthe exploration problem to be more challenging. On the Adroit domain, we find that periodic\nresetting of the learned reward function is necessary to avoid overfitting to the collected online experience \u2013 with the resetting, we find that even na\u00efve reward relabeling results in exploratory behavior,\ndiminishing the gap when performing explicit reward uncertainty estimation with RND."}, {"type": "heading", "lvl": 3, "value": "5.4 Can representation learning help obtain better reward labels?", "md": "### 5.4 Can representation learning help obtain better reward labels?"}, {"type": "text", "value": "We now investigate whether we can improve exploration by basing our UCB reward estimates on\ntop of pre-trained representations acquired from the prior data. We chose to use representations\nfrom ICVF [Ghosh et al., 2023], a method that trains feature representations by pre-training gen-\neral successor value functions on the offline data. Value-aware pre-training is a natural choice for\ndownstream reward uncertainty quantification, because the pre-training encourages representations\nto obey the spatiality of the environment \u2013 that states close to one another have similar representa-\ntions, while states that are difficult to reach from one another are far away.\n\nWe pre-train an ICVF model on the prior dataset and extract the visual encoder \u03beICVF(s), which\nencodes an image into a low-dimensional feature vector. These pre-trained encoder parameters are\nused to initialize the encoder for both the reward model r\u03b8(\u03be\u03b8(s)) and the RND network f\u03d5(\u03be\u03d5(s)).\nNote that these encoder parameters are not shared and not frozen, meaning that once training begins,\nthe encoder parameters for the reward model will be different from the encoder parameters for the\nRND model, and also diverge from the pre-trained initialization.\n\nAs shown in Figure 5, ICVF consistently improves the online learning efficiency across all image-\ninput tasks on top of optimistic reward labeling. It is worth noting that the encoder initialization\nfrom pre-training alone is not sufficient for the learning agent to succeed online (Na\u00efve + ICVF\nstill fails almost everywhere). On most tasks, without optimistic reward labeling, the agent simply\nmakes zero progress, further highlighting the effectiveness of optimistic labeling in aiding online\nexploration and learning.\n\nTo understand how the pre-trained representation initialization influences online exploration, we an-\nalyze its effect on the UCB reward estimate early in the training. Figure 6 shows the UCB reward", "md": "We now investigate whether we can improve exploration by basing our UCB reward estimates on\ntop of pre-trained representations acquired from the prior data. We chose to use representations\nfrom ICVF [Ghosh et al., 2023], a method that trains feature representations by pre-training gen-\neral successor value functions on the offline data. Value-aware pre-training is a natural choice for\ndownstream reward uncertainty quantification, because the pre-training encourages representations\nto obey the spatiality of the environment \u2013 that states close to one another have similar representa-\ntions, while states that are difficult to reach from one another are far away.\n\nWe pre-train an ICVF model on the prior dataset and extract the visual encoder \u03beICVF(s), which\nencodes an image into a low-dimensional feature vector. These pre-trained encoder parameters are\nused to initialize the encoder for both the reward model r\u03b8(\u03be\u03b8(s)) and the RND network f\u03d5(\u03be\u03d5(s)).\nNote that these encoder parameters are not shared and not frozen, meaning that once training begins,\nthe encoder parameters for the reward model will be different from the encoder parameters for the\nRND model, and also diverge from the pre-trained initialization.\n\nAs shown in Figure 5, ICVF consistently improves the online learning efficiency across all image-\ninput tasks on top of optimistic reward labeling. It is worth noting that the encoder initialization\nfrom pre-training alone is not sufficient for the learning agent to succeed online (Na\u00efve + ICVF\nstill fails almost everywhere). On most tasks, without optimistic reward labeling, the agent simply\nmakes zero progress, further highlighting the effectiveness of optimistic labeling in aiding online\nexploration and learning.\n\nTo understand how the pre-trained representation initialization influences online exploration, we an-\nalyze its effect on the UCB reward estimate early in the training. Figure 6 shows the UCB reward"}]}, {"page": 9, "text": "         Oracle              Ours + ICVF                Ours             Naive + ICVF                 Naive\n      Normalized Return\n                  Pick and Place              Grasp from Closed Drawer       Grasp from Blocked Drawer 1\n        0.5                               0.5                              0.5\n        0.00.0       0.1        0.2       0.00.0       0.1       0.2       0.00.0        0.1       0.2\n                                    Environment Steps (\u00d7106)\nFigure 5: Accelerating exploration with pre-trained representations on three visual-input COG tasks.\nOurs + ICVF uses optimistic reward labeling with a pre-trained representation initialization; Ours uses op-\ntimistic reward labeling without the pre-trained representation initialization. The same applies for Na\u00efve and\nNa\u00efve + ICVF. Overall, initializing the reward model and the RND network using pre-trained representations\ngreatly increases how quickly the model learns.\n   Normalized Reward                  At 25k Environment Steps\n           Ours + ICVF\n           Ours\n                                               Trajectory Steps\n        0        5        10       15        20       25        30       35        40       45       50\nFigure 6: The effect of pre-trained representations on the relabeled reward. We show the normalized\nrelabeled rewards (by standardizing it to have zero mean and unit variance) of an optimal prior trajectory from\nthe closed drawer task in the COG environment. Using pre-trained representations to learn the optimistic\nrewards leads to smoother rewards over the course of an optimal trajectory. See more examples in Appendix D.\nestimates on an expert trajectory of the Grasp from Closed Drawer task when the reward\nmodel and the RND model are initialized with the pre-trained representations (in blue) and with-\nout the pre-trained representations (in orange). When our algorithm is initialized with the ICVF\nrepresentations, the UCB reward estimates are increasing along the trajectory time step, labeling\nthe images in later part of the trajectory with high reward, forming a natural curriculum for the\nagent to explore towards the trajectory end. When our algorithm is initialized randomly without\nthe pre-trained representations, such a clear monotonic trend disappears. We hypothesize that this\nmonotonic trend of the reward labeling may account for the online sample efficiency improvement\nwith a better shaped reward.\n5.5   Does optimistic reward labeling help online learning by improving online exploration?\nWe have shown that optimistic reward labeling is effective in accelerating online learning, does it\nactually work because of better exploration? To answer this question, we evaluate the effect of our\nmethod on the state-coverage of the agent in the AntMaze domain. In this domain, the agent is\nrequired to reach a particular goal in the maze but has no prior knowledge of the goal, and thus\nrequired to explore different parts of the maze in the online phase in order to locate the goal. The\noptimal behavior that we expect the agent to perform is to quickly try out every possible part of the\nmaze until the goal is located, and then focus entirely on that goal to achieve 100% success rate.\nTo more quantitatively measure the quality of our exploration behavior, we divide up the 2-D maze\ninto a grid of square regions, and count the number of regions that the Ant agent has visited (using\nthe online replay buffer). This provides a measure for the amount of state coverage that the agent has\n                                                       9", "md": "# Document\n\n## Oracle Ours + ICVF Ours Naive + ICVF Naive\n\n|Normalized Return|Pick and Place|Grasp from Closed Drawer|Grasp from Blocked Drawer 1|\n|---|---|---|---|\n|0.5|0.5|0.5| |\n| |0.0|0.1|0.2|0.0|0.1|0.2|0.0|0.1|0.2|\n\nEnvironment Steps (\u00d710^6)\n\nFigure 5: Accelerating exploration with pre-trained representations on three visual-input COG tasks. Ours + ICVF uses optimistic reward labeling with a pre-trained representation initialization; Ours uses optimistic reward labeling without the pre-trained representation initialization. The same applies for Na\u00efve and Na\u00efve + ICVF. Overall, initializing the reward model and the RND network using pre-trained representations greatly increases how quickly the model learns.\n\n## Normalized Reward At 25k Environment Steps\n\nOurs + ICVF Ours\n\nTrajectory Steps\n\n0 5 10 15 20 25 30 35 40 45 50\n\nFigure 6: The effect of pre-trained representations on the relabeled reward. We show the normalized relabeled rewards (by standardizing it to have zero mean and unit variance) of an optimal prior trajectory from the closed drawer task in the COG environment. Using pre-trained representations to learn the optimistic rewards leads to smoother rewards over the course of an optimal trajectory. See more examples in Appendix D.\n\nestimates on an expert trajectory of the Grasp from Closed Drawer task when the reward model and the RND model are initialized with the pre-trained representations (in blue) and without the pre-trained representations (in orange). When our algorithm is initialized with the ICVF representations, the UCB reward estimates are increasing along the trajectory time step, labeling the images in later part of the trajectory with high reward, forming a natural curriculum for the agent to explore towards the trajectory end. When our algorithm is initialized randomly without the pre-trained representations, such a clear monotonic trend disappears. We hypothesize that this monotonic trend of the reward labeling may account for the online sample efficiency improvement with a better shaped reward.\n\n5.5 Does optimistic reward labeling help online learning by improving online exploration? We have shown that optimistic reward labeling is effective in accelerating online learning, does it actually work because of better exploration? To answer this question, we evaluate the effect of our method on the state-coverage of the agent in the AntMaze domain. In this domain, the agent is required to reach a particular goal in the maze but has no prior knowledge of the goal, and thus required to explore different parts of the maze in the online phase in order to locate the goal. The optimal behavior that we expect the agent to perform is to quickly try out every possible part of the maze until the goal is located, and then focus entirely on that goal to achieve 100% success rate. To more quantitatively measure the quality of our exploration behavior, we divide up the 2-D maze into a grid of square regions, and count the number of regions that the Ant agent has visited (using the online replay buffer). This provides a measure for the amount of state coverage that the agent has", "images": [{"name": "page-9-2.jpg", "height": 29, "width": 31, "x": 191, "y": 375}, {"name": "page-9-5.jpg", "height": 29, "width": 31, "x": 292, "y": 375}, {"name": "page-9-0.jpg", "height": 29, "width": 31, "x": 124, "y": 375}, {"name": "page-9-1.jpg", "height": 29, "width": 31, "x": 158, "y": 375}, {"name": "page-9-4.jpg", "height": 29, "width": 31, "x": 259, "y": 375}, {"name": "page-9-3.jpg", "height": 29, "width": 31, "x": 225, "y": 375}, {"name": "page-9-7.jpg", "height": 29, "width": 31, "x": 360, "y": 375}, {"name": "page-9-6.jpg", "height": 29, "width": 31, "x": 326, "y": 375}, {"name": "page-9-10.jpg", "height": 29, "width": 31, "x": 461, "y": 375}, {"name": "page-9-8.jpg", "height": 29, "width": 31, "x": 393, "y": 375}, {"name": "page-9-9.jpg", "height": 29, "width": 31, "x": 427, "y": 375}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Oracle Ours + ICVF Ours Naive + ICVF Naive", "md": "## Oracle Ours + ICVF Ours Naive + ICVF Naive"}, {"type": "table", "rows": [["Normalized Return", "Pick and Place", "Grasp from Closed Drawer", "Grasp from Blocked Drawer 1"], ["0.5", "0.5", "0.5", ""], ["", "0.0", "0.1", "0.2", "0.0", "0.1", "0.2", "0.0", "0.1", "0.2"]], "md": "|Normalized Return|Pick and Place|Grasp from Closed Drawer|Grasp from Blocked Drawer 1|\n|---|---|---|---|\n|0.5|0.5|0.5| |\n| |0.0|0.1|0.2|0.0|0.1|0.2|0.0|0.1|0.2|", "isPerfectTable": false, "csv": "\"Normalized Return\",\"Pick and Place\",\"Grasp from Closed Drawer\",\"Grasp from Blocked Drawer 1\"\n\"0.5\",\"0.5\",\"0.5\",\"\"\n\"\",\"0.0\",\"0.1\",\"0.2\",\"0.0\",\"0.1\",\"0.2\",\"0.0\",\"0.1\",\"0.2\""}, {"type": "text", "value": "Environment Steps (\u00d710^6)\n\nFigure 5: Accelerating exploration with pre-trained representations on three visual-input COG tasks. Ours + ICVF uses optimistic reward labeling with a pre-trained representation initialization; Ours uses optimistic reward labeling without the pre-trained representation initialization. The same applies for Na\u00efve and Na\u00efve + ICVF. Overall, initializing the reward model and the RND network using pre-trained representations greatly increases how quickly the model learns.", "md": "Environment Steps (\u00d710^6)\n\nFigure 5: Accelerating exploration with pre-trained representations on three visual-input COG tasks. Ours + ICVF uses optimistic reward labeling with a pre-trained representation initialization; Ours uses optimistic reward labeling without the pre-trained representation initialization. The same applies for Na\u00efve and Na\u00efve + ICVF. Overall, initializing the reward model and the RND network using pre-trained representations greatly increases how quickly the model learns."}, {"type": "heading", "lvl": 2, "value": "Normalized Reward At 25k Environment Steps", "md": "## Normalized Reward At 25k Environment Steps"}, {"type": "text", "value": "Ours + ICVF Ours\n\nTrajectory Steps\n\n0 5 10 15 20 25 30 35 40 45 50\n\nFigure 6: The effect of pre-trained representations on the relabeled reward. We show the normalized relabeled rewards (by standardizing it to have zero mean and unit variance) of an optimal prior trajectory from the closed drawer task in the COG environment. Using pre-trained representations to learn the optimistic rewards leads to smoother rewards over the course of an optimal trajectory. See more examples in Appendix D.\n\nestimates on an expert trajectory of the Grasp from Closed Drawer task when the reward model and the RND model are initialized with the pre-trained representations (in blue) and without the pre-trained representations (in orange). When our algorithm is initialized with the ICVF representations, the UCB reward estimates are increasing along the trajectory time step, labeling the images in later part of the trajectory with high reward, forming a natural curriculum for the agent to explore towards the trajectory end. When our algorithm is initialized randomly without the pre-trained representations, such a clear monotonic trend disappears. We hypothesize that this monotonic trend of the reward labeling may account for the online sample efficiency improvement with a better shaped reward.\n\n5.5 Does optimistic reward labeling help online learning by improving online exploration? We have shown that optimistic reward labeling is effective in accelerating online learning, does it actually work because of better exploration? To answer this question, we evaluate the effect of our method on the state-coverage of the agent in the AntMaze domain. In this domain, the agent is required to reach a particular goal in the maze but has no prior knowledge of the goal, and thus required to explore different parts of the maze in the online phase in order to locate the goal. The optimal behavior that we expect the agent to perform is to quickly try out every possible part of the maze until the goal is located, and then focus entirely on that goal to achieve 100% success rate. To more quantitatively measure the quality of our exploration behavior, we divide up the 2-D maze into a grid of square regions, and count the number of regions that the Ant agent has visited (using the online replay buffer). This provides a measure for the amount of state coverage that the agent has", "md": "Ours + ICVF Ours\n\nTrajectory Steps\n\n0 5 10 15 20 25 30 35 40 45 50\n\nFigure 6: The effect of pre-trained representations on the relabeled reward. We show the normalized relabeled rewards (by standardizing it to have zero mean and unit variance) of an optimal prior trajectory from the closed drawer task in the COG environment. Using pre-trained representations to learn the optimistic rewards leads to smoother rewards over the course of an optimal trajectory. See more examples in Appendix D.\n\nestimates on an expert trajectory of the Grasp from Closed Drawer task when the reward model and the RND model are initialized with the pre-trained representations (in blue) and without the pre-trained representations (in orange). When our algorithm is initialized with the ICVF representations, the UCB reward estimates are increasing along the trajectory time step, labeling the images in later part of the trajectory with high reward, forming a natural curriculum for the agent to explore towards the trajectory end. When our algorithm is initialized randomly without the pre-trained representations, such a clear monotonic trend disappears. We hypothesize that this monotonic trend of the reward labeling may account for the online sample efficiency improvement with a better shaped reward.\n\n5.5 Does optimistic reward labeling help online learning by improving online exploration? We have shown that optimistic reward labeling is effective in accelerating online learning, does it actually work because of better exploration? To answer this question, we evaluate the effect of our method on the state-coverage of the agent in the AntMaze domain. In this domain, the agent is required to reach a particular goal in the maze but has no prior knowledge of the goal, and thus required to explore different parts of the maze in the online phase in order to locate the goal. The optimal behavior that we expect the agent to perform is to quickly try out every possible part of the maze until the goal is located, and then focus entirely on that goal to achieve 100% success rate. To more quantitatively measure the quality of our exploration behavior, we divide up the 2-D maze into a grid of square regions, and count the number of regions that the Ant agent has visited (using the online replay buffer). This provides a measure for the amount of state coverage that the agent has"}]}, {"page": 10, "text": "                                 Ours              Naive               Online + RND\n       1.00        umaze-diverse                    medium-diverse                      large-diverse\n    Coverage\n       0.75\n       0.50\n       0.25\n       0.00 0.0      0.1       0.2      0.3   0.0       0.1      0.2      0.3   0.0       0.1      0.2      0.3\n                                  Environment Steps (\u00d7106)\nFigure 7: State coverage for 3 AntMaze tasks, estimated by counting the number of square regions that the\nant has visited, normalized by the total number of square regions that can be visited. Ours optimistically labels\nthe prior data, whereas Na\u00efve na\u00efvely labels the prior data without optimism. Optimistic labeling enables the\nRL agent to achieve a significantly higher coverage. Online RND also explores well in the maze, but with a\nmuch slower rate compared to our approach, highlighting the importance of leveraging unlabeled prior data for\nefficient online exploration. Full results in Appendix C.1.\nachieved in the environment, and the agent that explores better should achieve higher state coverage.\nFigure 7 shows how optimistic reward labeling affects the state coverage, and Figure 2 shows an\nexample visualization of the state-visitation for each method on antmaze-medium. Across all six\nAntMaze tasks, our approach achieves significantly higher state-coverage compared to na\u00efvely label\nwithout optimism both in the early stage of training and asymptotically compared to the baseline\nthat does not leverage the unlabeled prior data. In particular, we find that na\u00efvely labeling reward\nwith no optimism is doing poorly on larger mazes, highlighting the importance of optimistic reward\nlabeling for effective online exploration.\n5.6    How robust is our method in handling different offline data corruptions?\nTo further stress test the capability of our method in leveraging prior data, we corrupt one of the\nD4RL offline datasets, antmaze-large-diverse-v2, and test how our method performs un-\nder the corruption. In particular, we experimented with following three different kinds of corruptions\nand report the results in Figure 8.\nOrthogonal transitions: all the transitions that move in the up/right (towards the goal) are removed\nfrom the offline data. In this setting, stitching the transitions in the offline data would not lead to a\ntrajectory that goes from the start to the goal. This is a good test for the robustness of the algorithm\nfor exploration because the agent can not rely on the actions in the prior data and must explore to\ndiscover actions that move the ant in the correct direction.\nInsufficient coverage: all the transitions around the goals are removed from the offline data. This\nrequires the algorithm to actively explore so that it discovers the desired goal that it needs to reach.\n1% data: A random 1% of the transitions are kept in the offline data. This tests the capability of\nour method in handling the limited data regime.\nIn addition to the main comparisons above, we consider two modifications specifically for these\nexperiments to address the more challenging tasks.\nOurs + Online RND: This is an extension of our method, with an RND bonus added to the online\ndata in addition to the prior data. Similar to how we can extend our method with representation\nlearning, having exploration bonuses on the online data does not conflict with exploration bonuses\nadded to the offline data.\nNa\u00efve + Online RND: This is an extension of Na\u00efve Reward Labeling. An RND bonus added to the\nonline data, and the prior data is labeled with the unbiased reward estimate as before.\nDirectly using our method in the Insufficient Coverage setting yields zero success, but this is to be\nexpected as our algorithm does not incentivize exploration beyond the offline data. We find that\nthis can be patched by simply combining our method with an additional RND bonus on online\n                                                        10", "md": "```markdown\n| |Ours|Naive|Online + RND|\n|---|---|---|---|\n|1.00|umaze-diverse|medium-diverse|large-diverse|\n|Coverage| | | |\n|0.75| | | |\n|0.50| | | |\n|0.25| | | |\n|0.00| | | |\n| |0.0|0.1|0.2|0.3|0.0|0.1|0.2|0.3|0.0|0.1|0.2|0.3|\n\n```\n\n$$\n\\text{Figure 7: State coverage for 3 AntMaze tasks, estimated by counting the number of square regions that the ant has visited, normalized by the total number of square regions that can be visited. Ours optimistically labels the prior data, whereas Na\u00efve na\u00efvely labels the prior data without optimism. Optimistic labeling enables the RL agent to achieve a significantly higher coverage. Online RND also explores well in the maze, but with a much slower rate compared to our approach, highlighting the importance of leveraging unlabeled prior data for efficient online exploration. Full results in Appendix C.1.}\n$$\n\n$$\n\\text{achieved in the environment, and the agent that explores better should achieve higher state coverage.}\n$$\n\n$$\n\\text{Figure 7 shows how optimistic reward labeling affects the state coverage, and Figure 2 shows an example visualization of the state-visitation for each method on antmaze-medium. Across all six AntMaze tasks, our approach achieves significantly higher state-coverage compared to na\u00efvely label without optimism both in the early stage of training and asymptotically compared to the baseline that does not leverage the unlabeled prior data. In particular, we find that na\u00efvely labeling reward with no optimism is doing poorly on larger mazes, highlighting the importance of optimistic reward labeling for effective online exploration.}\n$$\n\n$$\n\\text{5.6 How robust is our method in handling different offline data corruptions?}\n$$\n\n$$\n\\text{To further stress test the capability of our method in leveraging prior data, we corrupt one of the D4RL offline datasets, antmaze-large-diverse-v2, and test how our method performs under the corruption. In particular, we experimented with following three different kinds of corruptions and report the results in Figure 8.}\n$$\n\n$$\n\\text{Orthogonal transitions: all the transitions that move in the up/right (towards the goal) are removed from the offline data. In this setting, stitching the transitions in the offline data would not lead to a trajectory that goes from the start to the goal. This is a good test for the robustness of the algorithm for exploration because the agent can not rely on the actions in the prior data and must explore to discover actions that move the ant in the correct direction.}\n$$\n\n$$\n\\text{Insufficient coverage: all the transitions around the goals are removed from the offline data. This requires the algorithm to actively explore so that it discovers the desired goal that it needs to reach.}\n$$\n\n$$\n\\text{1% data: A random 1% of the transitions are kept in the offline data. This tests the capability of our method in handling the limited data regime.}\n$$\n\n$$\n\\text{In addition to the main comparisons above, we consider two modifications specifically for these experiments to address the more challenging tasks.}\n$$\n\n$$\n\\text{Ours + Online RND: This is an extension of our method, with an RND bonus added to the online data in addition to the prior data. Similar to how we can extend our method with representation learning, having exploration bonuses on the online data does not conflict with exploration bonuses added to the offline data.}\n$$\n\n$$\n\\text{Na\u00efve + Online RND: This is an extension of Na\u00efve Reward Labeling. An RND bonus added to the online data, and the prior data is labeled with the unbiased reward estimate as before.}\n$$\n\n$$\n\\text{Directly using our method in the Insufficient Coverage setting yields zero success, but this is to be expected as our algorithm does not incentivize exploration beyond the offline data. We find that this can be patched by simply combining our method with an additional RND bonus on online.}\n$$", "images": [], "items": [{"type": "text", "value": "```markdown", "md": "```markdown"}, {"type": "table", "rows": [["", "Ours", "Naive", "Online + RND"], ["1.00", "umaze-diverse", "medium-diverse", "large-diverse"], ["Coverage", "", "", ""], ["0.75", "", "", ""], ["0.50", "", "", ""], ["0.25", "", "", ""], ["0.00", "", "", ""], ["", "0.0", "0.1", "0.2", "0.3", "0.0", "0.1", "0.2", "0.3", "0.0", "0.1", "0.2", "0.3"]], "md": "| |Ours|Naive|Online + RND|\n|---|---|---|---|\n|1.00|umaze-diverse|medium-diverse|large-diverse|\n|Coverage| | | |\n|0.75| | | |\n|0.50| | | |\n|0.25| | | |\n|0.00| | | |\n| |0.0|0.1|0.2|0.3|0.0|0.1|0.2|0.3|0.0|0.1|0.2|0.3|", "isPerfectTable": false, "csv": "\"\",\"Ours\",\"Naive\",\"Online + RND\"\n\"1.00\",\"umaze-diverse\",\"medium-diverse\",\"large-diverse\"\n\"Coverage\",\"\",\"\",\"\"\n\"0.75\",\"\",\"\",\"\"\n\"0.50\",\"\",\"\",\"\"\n\"0.25\",\"\",\"\",\"\"\n\"0.00\",\"\",\"\",\"\"\n\"\",\"0.0\",\"0.1\",\"0.2\",\"0.3\",\"0.0\",\"0.1\",\"0.2\",\"0.3\",\"0.0\",\"0.1\",\"0.2\",\"0.3\""}, {"type": "text", "value": "```\n\n$$\n\\text{Figure 7: State coverage for 3 AntMaze tasks, estimated by counting the number of square regions that the ant has visited, normalized by the total number of square regions that can be visited. Ours optimistically labels the prior data, whereas Na\u00efve na\u00efvely labels the prior data without optimism. Optimistic labeling enables the RL agent to achieve a significantly higher coverage. Online RND also explores well in the maze, but with a much slower rate compared to our approach, highlighting the importance of leveraging unlabeled prior data for efficient online exploration. Full results in Appendix C.1.}\n$$\n\n$$\n\\text{achieved in the environment, and the agent that explores better should achieve higher state coverage.}\n$$\n\n$$\n\\text{Figure 7 shows how optimistic reward labeling affects the state coverage, and Figure 2 shows an example visualization of the state-visitation for each method on antmaze-medium. Across all six AntMaze tasks, our approach achieves significantly higher state-coverage compared to na\u00efvely label without optimism both in the early stage of training and asymptotically compared to the baseline that does not leverage the unlabeled prior data. In particular, we find that na\u00efvely labeling reward with no optimism is doing poorly on larger mazes, highlighting the importance of optimistic reward labeling for effective online exploration.}\n$$\n\n$$\n\\text{5.6 How robust is our method in handling different offline data corruptions?}\n$$\n\n$$\n\\text{To further stress test the capability of our method in leveraging prior data, we corrupt one of the D4RL offline datasets, antmaze-large-diverse-v2, and test how our method performs under the corruption. In particular, we experimented with following three different kinds of corruptions and report the results in Figure 8.}\n$$\n\n$$\n\\text{Orthogonal transitions: all the transitions that move in the up/right (towards the goal) are removed from the offline data. In this setting, stitching the transitions in the offline data would not lead to a trajectory that goes from the start to the goal. This is a good test for the robustness of the algorithm for exploration because the agent can not rely on the actions in the prior data and must explore to discover actions that move the ant in the correct direction.}\n$$\n\n$$\n\\text{Insufficient coverage: all the transitions around the goals are removed from the offline data. This requires the algorithm to actively explore so that it discovers the desired goal that it needs to reach.}\n$$\n\n$$\n\\text{1% data: A random 1% of the transitions are kept in the offline data. This tests the capability of our method in handling the limited data regime.}\n$$\n\n$$\n\\text{In addition to the main comparisons above, we consider two modifications specifically for these experiments to address the more challenging tasks.}\n$$\n\n$$\n\\text{Ours + Online RND: This is an extension of our method, with an RND bonus added to the online data in addition to the prior data. Similar to how we can extend our method with representation learning, having exploration bonuses on the online data does not conflict with exploration bonuses added to the offline data.}\n$$\n\n$$\n\\text{Na\u00efve + Online RND: This is an extension of Na\u00efve Reward Labeling. An RND bonus added to the online data, and the prior data is labeled with the unbiased reward estimate as before.}\n$$\n\n$$\n\\text{Directly using our method in the Insufficient Coverage setting yields zero success, but this is to be expected as our algorithm does not incentivize exploration beyond the offline data. We find that this can be patched by simply combining our method with an additional RND bonus on online.}\n$$", "md": "```\n\n$$\n\\text{Figure 7: State coverage for 3 AntMaze tasks, estimated by counting the number of square regions that the ant has visited, normalized by the total number of square regions that can be visited. Ours optimistically labels the prior data, whereas Na\u00efve na\u00efvely labels the prior data without optimism. Optimistic labeling enables the RL agent to achieve a significantly higher coverage. Online RND also explores well in the maze, but with a much slower rate compared to our approach, highlighting the importance of leveraging unlabeled prior data for efficient online exploration. Full results in Appendix C.1.}\n$$\n\n$$\n\\text{achieved in the environment, and the agent that explores better should achieve higher state coverage.}\n$$\n\n$$\n\\text{Figure 7 shows how optimistic reward labeling affects the state coverage, and Figure 2 shows an example visualization of the state-visitation for each method on antmaze-medium. Across all six AntMaze tasks, our approach achieves significantly higher state-coverage compared to na\u00efvely label without optimism both in the early stage of training and asymptotically compared to the baseline that does not leverage the unlabeled prior data. In particular, we find that na\u00efvely labeling reward with no optimism is doing poorly on larger mazes, highlighting the importance of optimistic reward labeling for effective online exploration.}\n$$\n\n$$\n\\text{5.6 How robust is our method in handling different offline data corruptions?}\n$$\n\n$$\n\\text{To further stress test the capability of our method in leveraging prior data, we corrupt one of the D4RL offline datasets, antmaze-large-diverse-v2, and test how our method performs under the corruption. In particular, we experimented with following three different kinds of corruptions and report the results in Figure 8.}\n$$\n\n$$\n\\text{Orthogonal transitions: all the transitions that move in the up/right (towards the goal) are removed from the offline data. In this setting, stitching the transitions in the offline data would not lead to a trajectory that goes from the start to the goal. This is a good test for the robustness of the algorithm for exploration because the agent can not rely on the actions in the prior data and must explore to discover actions that move the ant in the correct direction.}\n$$\n\n$$\n\\text{Insufficient coverage: all the transitions around the goals are removed from the offline data. This requires the algorithm to actively explore so that it discovers the desired goal that it needs to reach.}\n$$\n\n$$\n\\text{1% data: A random 1% of the transitions are kept in the offline data. This tests the capability of our method in handling the limited data regime.}\n$$\n\n$$\n\\text{In addition to the main comparisons above, we consider two modifications specifically for these experiments to address the more challenging tasks.}\n$$\n\n$$\n\\text{Ours + Online RND: This is an extension of our method, with an RND bonus added to the online data in addition to the prior data. Similar to how we can extend our method with representation learning, having exploration bonuses on the online data does not conflict with exploration bonuses added to the offline data.}\n$$\n\n$$\n\\text{Na\u00efve + Online RND: This is an extension of Na\u00efve Reward Labeling. An RND bonus added to the online data, and the prior data is labeled with the unbiased reward estimate as before.}\n$$\n\n$$\n\\text{Directly using our method in the Insufficient Coverage setting yields zero success, but this is to be expected as our algorithm does not incentivize exploration beyond the offline data. We find that this can be patched by simply combining our method with an additional RND bonus on online.}\n$$"}]}, {"page": 11, "text": "                  Oracle              Ours + Online RND                  Online Only + RND                 Naive + Online RND\n                  Ours                Online Only                        Naive                             Naive + BC\n    Normalized Return All                   Orthogonal Transitions             Insufficient Coverage                      1% Data\n      1.00                              1.00                               1.00                               1.00\n      0.75                              0.75                               0.75                               0.75\n      0.50                              0.50                               0.50                               0.50\n      0.25                              0.25                               0.25                               0.25\n      0.00                              0.00                               0.00                               0.00\n         0.00 0.15 0.30 0.45 0.60 0.75      0.00 0.15 0.30 0.45 0.60 0.75     0.00 0.15 0.30 0.45 0.60 0.75      0.00 0.15 0.30 0.45 0.60 0.75\nFigure 8: The effect of different offline data corruptions on online learning efficiency. Top: The normal-\nized return on antmaze-large-diverse-v2 under three different offline data corruptions (\u201call\u201d means\nno data corruption). Middle: visualization of the coverage of the transitions in the offline data. Bottom: visu-\nalization of the direction of the transitions in the offline data.\ndata, allowing our method to learn even when data has poor coverage and no data near the goal is\nunavailable. Similarly, we have found that combining our method with online RND can also boost\nthe performance in the Orthogonal Transitions, where no transitions move in the direction of the\ngoal. For the 1% Data setting, our method can still largely match the oracle performance without\nthe need for an online RND bonus.\nIt is also worth noting that in the Orthogonal Transitions setting, our method achieves good perfor-\nmance whereas the oracle fails to succeed. The failures of the oracle are not unexpected because the\nprior data do not contain any single trajectories, or even trajectories stitched together from different\ntransitions, that can reach the goal. However, since our method succeeds, this indicates that explo-\nration bonuses applied only to the prior data can still be utilized, despite there being no forward\nconnection through the prior data. These additional results, while initially surprising even to us (es-\npecially the \u201corthogonal\u201d setting, which seems very difficult), strongly suggest that our method can\nlead to significant improvement even when the data is not very good \u2013 certainly such data would be\nwoefully inadequate for regular offline RL or na\u00efve policy initialization.\n6      Discussion\nWe showed how we can effectively leverage unlabeled prior data to improve online exploration by\nrunning a standard off-policy RL algorithm on the data relabeled with UCB reward estimates. In\npractice, the UCB estimates can be approximated by combining the prediction of a reward model\nand an RND network, both can be trained online with little additional computational cost. We\ndemonstrated the surprising effectiveness of this simple approach on a diverse set of domains. Our\ninstantiation of the optimistic reward labeling idea presents a number of avenues for future research.\nFirst, on the relocate Adroit task, we found that na\u00efvely fitting the reward model on the online\nreplay buffer without any regularization leads to poor performance (possibly due to catastrophic\noverfitting). While we have found a temporary workaround by periodically re-initializing the reward\nmodel, such a solution may seem ad-hoc and could be disruptive to the RL learning due to the sudden\nchange in the learning objective (which is shaped by the reward model). Next, the UCB estimates\nof the state-action can rapidly change, especially in the beginning of the training, which may cause\nlearning instability in the RL algorithm. There is no mechanism in our current algorithm to tackle\n                                                                      11", "md": "# Document\n\n## Oracle\n\n|Oracle|Ours + Online RND|Online Only + RND|Naive + Online RND|\n|---|---|---|---|\n|Ours|Online Only|Naive|Naive + BC|\n\n## Normalized Return\n\n| |Orthogonal Transitions|Insufficient Coverage|1% Data|\n|---|---|---|---|\n|1.00|1.00|1.00|1.00|\n|0.75|0.75|0.75|0.75|\n|0.50|0.50|0.50|0.50|\n|0.25|0.25|0.25|0.25|\n|0.00|0.00|0.00|0.00|\n\nFigure 8: The effect of different offline data corruptions on online learning efficiency. Top: The normalized return on antmaze-large-diverse-v2 under three different offline data corruptions (\u201call\u201d means no data corruption). Middle: visualization of the coverage of the transitions in the offline data. Bottom: visualization of the direction of the transitions in the offline data.\n\nData, allowing our method to learn even when data has poor coverage and no data near the goal is unavailable. Similarly, we have found that combining our method with online RND can also boost the performance in the Orthogonal Transitions, where no transitions move in the direction of the goal. For the 1% Data setting, our method can still largely match the oracle performance without the need for an online RND bonus.\n\nIt is also worth noting that in the Orthogonal Transitions setting, our method achieves good performance whereas the oracle fails to succeed. The failures of the oracle are not unexpected because the prior data do not contain any single trajectories, or even trajectories stitched together from different transitions, that can reach the goal. However, since our method succeeds, this indicates that exploration bonuses applied only to the prior data can still be utilized, despite there being no forward connection through the prior data. These additional results, while initially surprising even to us (especially the \u201corthogonal\u201d setting, which seems very difficult), strongly suggest that our method can lead to significant improvement even when the data is not very good \u2013 certainly such data would be woefully inadequate for regular offline RL or na\u00efve policy initialization.\n\n## Discussion\n\nWe showed how we can effectively leverage unlabeled prior data to improve online exploration by running a standard off-policy RL algorithm on the data relabeled with UCB reward estimates. In practice, the UCB estimates can be approximated by combining the prediction of a reward model and an RND network, both can be trained online with little additional computational cost. We demonstrated the surprising effectiveness of this simple approach on a diverse set of domains. Our instantiation of the optimistic reward labeling idea presents a number of avenues for future research.\n\nFirst, on the relocate Adroit task, we found that na\u00efvely fitting the reward model on the online replay buffer without any regularization leads to poor performance (possibly due to catastrophic overfitting). While we have found a temporary workaround by periodically re-initializing the reward model, such a solution may seem ad-hoc and could be disruptive to the RL learning due to the sudden change in the learning objective (which is shaped by the reward model). Next, the UCB estimates of the state-action can rapidly change, especially in the beginning of the training, which may cause learning instability in the RL algorithm. There is no mechanism in our current algorithm to tackle", "images": [{"name": "page-11-0.jpg", "height": 79, "width": 406, "x": 100, "y": 169}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Oracle", "md": "## Oracle"}, {"type": "table", "rows": [["Oracle", "Ours + Online RND", "Online Only + RND", "Naive + Online RND"], ["Ours", "Online Only", "Naive", "Naive + BC"]], "md": "|Oracle|Ours + Online RND|Online Only + RND|Naive + Online RND|\n|---|---|---|---|\n|Ours|Online Only|Naive|Naive + BC|", "isPerfectTable": true, "csv": "\"Oracle\",\"Ours + Online RND\",\"Online Only + RND\",\"Naive + Online RND\"\n\"Ours\",\"Online Only\",\"Naive\",\"Naive + BC\""}, {"type": "heading", "lvl": 2, "value": "Normalized Return", "md": "## Normalized Return"}, {"type": "table", "rows": [["", "Orthogonal Transitions", "Insufficient Coverage", "1% Data"], ["1.00", "1.00", "1.00", "1.00"], ["0.75", "0.75", "0.75", "0.75"], ["0.50", "0.50", "0.50", "0.50"], ["0.25", "0.25", "0.25", "0.25"], ["0.00", "0.00", "0.00", "0.00"]], "md": "| |Orthogonal Transitions|Insufficient Coverage|1% Data|\n|---|---|---|---|\n|1.00|1.00|1.00|1.00|\n|0.75|0.75|0.75|0.75|\n|0.50|0.50|0.50|0.50|\n|0.25|0.25|0.25|0.25|\n|0.00|0.00|0.00|0.00|", "isPerfectTable": true, "csv": "\"\",\"Orthogonal Transitions\",\"Insufficient Coverage\",\"1% Data\"\n\"1.00\",\"1.00\",\"1.00\",\"1.00\"\n\"0.75\",\"0.75\",\"0.75\",\"0.75\"\n\"0.50\",\"0.50\",\"0.50\",\"0.50\"\n\"0.25\",\"0.25\",\"0.25\",\"0.25\"\n\"0.00\",\"0.00\",\"0.00\",\"0.00\""}, {"type": "text", "value": "Figure 8: The effect of different offline data corruptions on online learning efficiency. Top: The normalized return on antmaze-large-diverse-v2 under three different offline data corruptions (\u201call\u201d means no data corruption). Middle: visualization of the coverage of the transitions in the offline data. Bottom: visualization of the direction of the transitions in the offline data.\n\nData, allowing our method to learn even when data has poor coverage and no data near the goal is unavailable. Similarly, we have found that combining our method with online RND can also boost the performance in the Orthogonal Transitions, where no transitions move in the direction of the goal. For the 1% Data setting, our method can still largely match the oracle performance without the need for an online RND bonus.\n\nIt is also worth noting that in the Orthogonal Transitions setting, our method achieves good performance whereas the oracle fails to succeed. The failures of the oracle are not unexpected because the prior data do not contain any single trajectories, or even trajectories stitched together from different transitions, that can reach the goal. However, since our method succeeds, this indicates that exploration bonuses applied only to the prior data can still be utilized, despite there being no forward connection through the prior data. These additional results, while initially surprising even to us (especially the \u201corthogonal\u201d setting, which seems very difficult), strongly suggest that our method can lead to significant improvement even when the data is not very good \u2013 certainly such data would be woefully inadequate for regular offline RL or na\u00efve policy initialization.", "md": "Figure 8: The effect of different offline data corruptions on online learning efficiency. Top: The normalized return on antmaze-large-diverse-v2 under three different offline data corruptions (\u201call\u201d means no data corruption). Middle: visualization of the coverage of the transitions in the offline data. Bottom: visualization of the direction of the transitions in the offline data.\n\nData, allowing our method to learn even when data has poor coverage and no data near the goal is unavailable. Similarly, we have found that combining our method with online RND can also boost the performance in the Orthogonal Transitions, where no transitions move in the direction of the goal. For the 1% Data setting, our method can still largely match the oracle performance without the need for an online RND bonus.\n\nIt is also worth noting that in the Orthogonal Transitions setting, our method achieves good performance whereas the oracle fails to succeed. The failures of the oracle are not unexpected because the prior data do not contain any single trajectories, or even trajectories stitched together from different transitions, that can reach the goal. However, since our method succeeds, this indicates that exploration bonuses applied only to the prior data can still be utilized, despite there being no forward connection through the prior data. These additional results, while initially surprising even to us (especially the \u201corthogonal\u201d setting, which seems very difficult), strongly suggest that our method can lead to significant improvement even when the data is not very good \u2013 certainly such data would be woefully inadequate for regular offline RL or na\u00efve policy initialization."}, {"type": "heading", "lvl": 2, "value": "Discussion", "md": "## Discussion"}, {"type": "text", "value": "We showed how we can effectively leverage unlabeled prior data to improve online exploration by running a standard off-policy RL algorithm on the data relabeled with UCB reward estimates. In practice, the UCB estimates can be approximated by combining the prediction of a reward model and an RND network, both can be trained online with little additional computational cost. We demonstrated the surprising effectiveness of this simple approach on a diverse set of domains. Our instantiation of the optimistic reward labeling idea presents a number of avenues for future research.\n\nFirst, on the relocate Adroit task, we found that na\u00efvely fitting the reward model on the online replay buffer without any regularization leads to poor performance (possibly due to catastrophic overfitting). While we have found a temporary workaround by periodically re-initializing the reward model, such a solution may seem ad-hoc and could be disruptive to the RL learning due to the sudden change in the learning objective (which is shaped by the reward model). Next, the UCB estimates of the state-action can rapidly change, especially in the beginning of the training, which may cause learning instability in the RL algorithm. There is no mechanism in our current algorithm to tackle", "md": "We showed how we can effectively leverage unlabeled prior data to improve online exploration by running a standard off-policy RL algorithm on the data relabeled with UCB reward estimates. In practice, the UCB estimates can be approximated by combining the prediction of a reward model and an RND network, both can be trained online with little additional computational cost. We demonstrated the surprising effectiveness of this simple approach on a diverse set of domains. Our instantiation of the optimistic reward labeling idea presents a number of avenues for future research.\n\nFirst, on the relocate Adroit task, we found that na\u00efvely fitting the reward model on the online replay buffer without any regularization leads to poor performance (possibly due to catastrophic overfitting). While we have found a temporary workaround by periodically re-initializing the reward model, such a solution may seem ad-hoc and could be disruptive to the RL learning due to the sudden change in the learning objective (which is shaped by the reward model). Next, the UCB estimates of the state-action can rapidly change, especially in the beginning of the training, which may cause learning instability in the RL algorithm. There is no mechanism in our current algorithm to tackle"}]}, {"page": 12, "text": "such a potential issue. Nevertheless, our work indicates that there exist simple ways of incorporating\nprior data for online RL agents even when no reward labels exist. Scaling these mechanisms to more\ncomplex prior datasets is an exciting direction towards the promise of RL agents that can leverage\ngeneral prior data.\nAcknowledgement. This work was partially done while QL was a visiting student researcher at\nFAIR, Meta. We would like to thank Seohong Park for providing his implementation of goal-\nconditioned IQL (GC-IQL) and pre-trained GC-IQL checkpoints (which were used in producing\nFigure 16). We would also like to thank Fangchen Liu for the acronym of the method. We would\nalso like to thank Laura Smith, Kuba Grudzien, Toru Lin, Oleg Rybkin, Aviral Kumar, Ahmed Touati\nfor discussion on the method and feedback on the early draft of the paper. We would also like to\nthank the members of the RAIL lab for insightful discussions on the paper. This research was par-\ntially supported by the Office of Naval Research under N00014-21-1-2838 and N00014-22-1-2773,\nand ARO W911NF-21-1-0097.\nReferences\nJoshua Achiam and S. Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement\n  learning. ArXiv, abs/1703.01732, 2017.\nAbien Fred Agarap.         Deep learning using rectified linear units (ReLU).             arXiv preprint\n  arXiv:1803.08375, 2018.\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Belle-\n  mare. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress.\n  In Neural Information Processing Systems, 2022.\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. OPAL: Offline\n  primitive discovery for accelerating offline reinforcement learning.          In International Confer-\n  ence on Learning Representations, 2021. URL https://openreview.net/forum?id=\n  V69LGwJ0lIN.\nAnurag Ajay, Abhishek Gupta, Dibya Ghosh, Sergey Levine, and Pulkit Agrawal. Distributionally\n  adaptive meta reinforcement learning. ArXiv, abs/2210.03104, 2022.\nMohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforce-\n  ment learning. In International Conference on Machine Learning, pages 263\u2013272. PMLR, 2017.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n  arXiv:1607.06450, 2016.\nPhilip J. Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learn-\n  ing with offline data. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,\n  Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on\n  Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1577\u20131594.\n  PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/ball23a.\n  html.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.\n  Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information\n  Processing Systems, pages 1471\u20131479, 2016.\nDavid Brandfonbrener, Stephen Tu, Avi Singh, Stefan Welker, Chad Boodoo, N. Matni, and Jacob\n  Varley. Visual backtracking teleoperation: A data collection protocol for offline image-based\n  reinforcement learning. ArXiv, abs/2210.02343, 2022.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network\n  distillation. arXiv preprint arXiv:1810.12894, 2018.\nChristoph Dann, Tor Lattimore, and Emma Brunskill.                  Unifying PAC and regret:          Uni-\n  form PAC bounds for episodic reinforcement learning.                In I. Guyon, U. Von Luxburg,\n  S. Bengio,     H. Wallach,      R. Fergus,     S. Vishwanathan,      and R. Garnett,      editors,   Ad-\n  vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,\n                                                    12", "md": "such a potential issue. Nevertheless, our work indicates that there exist simple ways of incorporating prior data for online RL agents even when no reward labels exist. Scaling these mechanisms to more complex prior datasets is an exciting direction towards the promise of RL agents that can leverage general prior data.\n\nAcknowledgement. This work was partially done while QL was a visiting student researcher at FAIR, Meta. We would like to thank Seohong Park for providing his implementation of goal-conditioned IQL (GC-IQL) and pre-trained GC-IQL checkpoints (which were used in producing Figure 16). We would also like to thank Fangchen Liu for the acronym of the method. We would also like to thank Laura Smith, Kuba Grudzien, Toru Lin, Oleg Rybkin, Aviral Kumar, Ahmed Touati for discussion on the method and feedback on the early draft of the paper. We would also like to thank the members of the RAIL lab for insightful discussions on the paper. This research was partially supported by the Office of Naval Research under N00014-21-1-2838 and N00014-22-1-2773, and ARO W911NF-21-1-0097.\n\n## References\n\n1. Joshua Achiam and S. Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learning. ArXiv, abs/1703.01732, 2017.\n2. Abien Fred Agarap. Deep learning using rectified linear units (ReLU). arXiv preprint arXiv:1803.08375, 2018.\n3. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. In Neural Information Processing Systems, 2022.\n4. Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. OPAL: Offline primitive discovery for accelerating offline reinforcement learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=V69LGwJ0lIN.\n5. Anurag Ajay, Abhishek Gupta, Dibya Ghosh, Sergey Levine, and Pulkit Agrawal. Distributionally adaptive meta reinforcement learning. ArXiv, abs/2210.03104, 2022.\n6. Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263\u2013272. PMLR, 2017.\n7. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n8. Philip J. Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1577\u20131594. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/ball23a.html.\n9. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471\u20131479, 2016.\n10. David Brandfonbrener, Stephen Tu, Avi Singh, Stefan Welker, Chad Boodoo, N. Matni, and Jacob Varley. Visual backtracking teleoperation: A data collection protocol for offline image-based reinforcement learning. ArXiv, abs/2210.02343, 2022.\n11. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\n12. Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 12", "images": [], "items": [{"type": "text", "value": "such a potential issue. Nevertheless, our work indicates that there exist simple ways of incorporating prior data for online RL agents even when no reward labels exist. Scaling these mechanisms to more complex prior datasets is an exciting direction towards the promise of RL agents that can leverage general prior data.\n\nAcknowledgement. This work was partially done while QL was a visiting student researcher at FAIR, Meta. We would like to thank Seohong Park for providing his implementation of goal-conditioned IQL (GC-IQL) and pre-trained GC-IQL checkpoints (which were used in producing Figure 16). We would also like to thank Fangchen Liu for the acronym of the method. We would also like to thank Laura Smith, Kuba Grudzien, Toru Lin, Oleg Rybkin, Aviral Kumar, Ahmed Touati for discussion on the method and feedback on the early draft of the paper. We would also like to thank the members of the RAIL lab for insightful discussions on the paper. This research was partially supported by the Office of Naval Research under N00014-21-1-2838 and N00014-22-1-2773, and ARO W911NF-21-1-0097.", "md": "such a potential issue. Nevertheless, our work indicates that there exist simple ways of incorporating prior data for online RL agents even when no reward labels exist. Scaling these mechanisms to more complex prior datasets is an exciting direction towards the promise of RL agents that can leverage general prior data.\n\nAcknowledgement. This work was partially done while QL was a visiting student researcher at FAIR, Meta. We would like to thank Seohong Park for providing his implementation of goal-conditioned IQL (GC-IQL) and pre-trained GC-IQL checkpoints (which were used in producing Figure 16). We would also like to thank Fangchen Liu for the acronym of the method. We would also like to thank Laura Smith, Kuba Grudzien, Toru Lin, Oleg Rybkin, Aviral Kumar, Ahmed Touati for discussion on the method and feedback on the early draft of the paper. We would also like to thank the members of the RAIL lab for insightful discussions on the paper. This research was partially supported by the Office of Naval Research under N00014-21-1-2838 and N00014-22-1-2773, and ARO W911NF-21-1-0097."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. Joshua Achiam and S. Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learning. ArXiv, abs/1703.01732, 2017.\n2. Abien Fred Agarap. Deep learning using rectified linear units (ReLU). arXiv preprint arXiv:1803.08375, 2018.\n3. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. In Neural Information Processing Systems, 2022.\n4. Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. OPAL: Offline primitive discovery for accelerating offline reinforcement learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=V69LGwJ0lIN.\n5. Anurag Ajay, Abhishek Gupta, Dibya Ghosh, Sergey Levine, and Pulkit Agrawal. Distributionally adaptive meta reinforcement learning. ArXiv, abs/2210.03104, 2022.\n6. Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263\u2013272. PMLR, 2017.\n7. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n8. Philip J. Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1577\u20131594. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/ball23a.html.\n9. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471\u20131479, 2016.\n10. David Brandfonbrener, Stephen Tu, Avi Singh, Stefan Welker, Chad Boodoo, N. Matni, and Jacob Varley. Visual backtracking teleoperation: A data collection protocol for offline image-based reinforcement learning. ArXiv, abs/2210.02343, 2022.\n11. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\n12. Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 12", "md": "1. Joshua Achiam and S. Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement learning. ArXiv, abs/1703.01732, 2017.\n2. Abien Fred Agarap. Deep learning using rectified linear units (ReLU). arXiv preprint arXiv:1803.08375, 2018.\n3. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. In Neural Information Processing Systems, 2022.\n4. Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. OPAL: Offline primitive discovery for accelerating offline reinforcement learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=V69LGwJ0lIN.\n5. Anurag Ajay, Abhishek Gupta, Dibya Ghosh, Sergey Levine, and Pulkit Agrawal. Distributionally adaptive meta reinforcement learning. ArXiv, abs/2210.03104, 2022.\n6. Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263\u2013272. PMLR, 2017.\n7. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n8. Philip J. Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1577\u20131594. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/ball23a.html.\n9. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471\u20131479, 2016.\n10. David Brandfonbrener, Stephen Tu, Avi Singh, Stefan Welker, Chad Boodoo, N. Matni, and Jacob Varley. Visual backtracking teleoperation: A data collection protocol for offline image-based reinforcement learning. ArXiv, abs/2210.02343, 2022.\n11. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\n12. Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 12"}]}, {"page": 13, "text": "   2017.    URL https://proceedings.neurips.cc/paper_files/paper/2017/\n   file/17d8da815fa21c57af9829fb0a869602-Paper.pdf.\nRon Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta reinforcement learning - identifiability\n   challenges and effective data collection strategies. In Neural Information Processing Systems,\n   2021.\nYan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and P. Abbeel. RL2: Fast\n   reinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016.\nJesse Farebrother,   Joshua Greaves,    Rishabh Agarwal,     Charline Le Lan,     Ross Goroshin,\n   Pablo Samuel Castro, and Marc G Bellemare. Proto-value networks: Scaling representation learn-\n   ing with auxiliary tasks. In The Eleventh International Conference on Learning Representations,\n   2023. URL https://openreview.net/forum?id=oGDKSt9JrZi.\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\n   data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\nDibya Ghosh, Chethan Anand Bhateja, and Sergey Levine. Reinforcement learning from passive\n   data via latent intentions. In International Conference on Machine Learning, pages 11321\u201311339.\n   PMLR, 2023.\nAbhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking re-\n   ward shaping: Understanding the benefits of reward engineering on sample complexity. Advances\n   in Neural Information Processing Systems, 35:15281\u201315295, 2022.\nBotao Hao, Rahul Jain, Dengwang Tang, and Zheng Wen. Bridging imitation and online reinforce-\n   ment learning: An optimistic tale. ArXiv, abs/2303.11369, 2023.\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:\n   Variational information maximizing exploration. In Advances in Neural Information Processing\n   Systems, pages 1109\u20131117, 2016.\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-\n   cient? Advances in neural information processing systems, 31, 2018.\nChi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for\n   reinforcement learning. In International Conference on Machine Learning, 2020.\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Q-\n   learning. In International Conference on Learning Representations, 2022. URL https://\n   openreview.net/forum?id=68n2s9ZJWF8.\nMichael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Ler-\n   rel Pinto, and Pieter Abbeel. URLB: Unsupervised reinforcement learning benchmark. arXiv\n   preprint arXiv:2110.15191, 2021.\nLisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhut-\n   dinov. Efficient exploration via state marginal matching. ArXiv, abs/1906.05274, 2019.\nSeunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online re-\n   inforcement learning via balanced replay and pessimistic Q-ensemble. In 5th Annual Conference\n   on Robot Learning, 2021. URL https://openreview.net/forum?id=AlJXhEI6J5W.\nQiyang Li, Yuexiang Zhai, Yi Ma, and Sergey Levine.        Understanding the complexity gains of\n   single-task RL with a curriculum. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\n   Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International\n   Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,\n   pages 20412\u201320451. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/\n   v202/li23as.html.\nRunji Lin, Ye Li, Xidong Feng, Zhaowei Zhang, Xian Hong Wu Fung, Haifeng Zhang, Jun Wang,\n   Yali Du, and Yaodong Yang. Contextual transformer for offline meta reinforcement learning.\n   ArXiv, abs/2211.08016, 2022.\n                                                13", "md": "# References\n\n## References\n\n- Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta reinforcement learning - identifiability\nchallenges and effective data collection strategies. In Neural Information Processing Systems, 2021.\n- Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and P. Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016.\n- Jesse Farebrother, Joshua Greaves, Rishabh Agarwal, Charline Le Lan, Ross Goroshin,\nPablo Samuel Castro, and Marc G Bellemare. Proto-value networks: Scaling representation learning\nwith auxiliary tasks. In The Eleventh International Conference on Learning Representations, 2023.\nURL Link.\n- Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n- Dibya Ghosh, Chethan Anand Bhateja, and Sergey Levine. Reinforcement learning from passive\ndata via latent intentions. In International Conference on Machine Learning, pages 11321\u201311339.\nPMLR, 2023.\n- Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity. Advances in Neural Information Processing Systems, 35:15281\u201315295, 2022.\n- Botao Hao, Rahul Jain, Dengwang Tang, and Zheng Wen. Bridging imitation and online reinforcement learning: An optimistic tale. ArXiv, abs/2303.11369, 2023.\n- Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109\u20131117, 2016.\n- Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient? Advances in neural information processing systems, 31, 2018.\n- Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning, 2020.\n- Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Q-learning. In International Conference on Learning Representations, 2022. URL Link.\n- Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. URLB: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191, 2021.\n- Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhutdinov. Efficient exploration via state marginal matching. ArXiv, abs/1906.05274, 2019.\n- Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online reinforcement learning via balanced replay and pessimistic Q-ensemble. In 5th Annual Conference on Robot Learning, 2021. URL Link.\n- Qiyang Li, Yuexiang Zhai, Yi Ma, and Sergey Levine. Understanding the complexity gains of single-task RL with a curriculum. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 20412\u201320451. PMLR, 23\u201329 Jul 2023. URL Link.\n- Runji Lin, Ye Li, Xidong Feng, Zhaowei Zhang, Xian Hong Wu Fung, Haifeng Zhang, Jun Wang, Yali Du, and Yaodong Yang. Contextual transformer for offline meta reinforcement learning. ArXiv, abs/2211.08016, 2022.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta reinforcement learning - identifiability\nchallenges and effective data collection strategies. In Neural Information Processing Systems, 2021.\n- Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and P. Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016.\n- Jesse Farebrother, Joshua Greaves, Rishabh Agarwal, Charline Le Lan, Ross Goroshin,\nPablo Samuel Castro, and Marc G Bellemare. Proto-value networks: Scaling representation learning\nwith auxiliary tasks. In The Eleventh International Conference on Learning Representations, 2023.\nURL Link.\n- Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n- Dibya Ghosh, Chethan Anand Bhateja, and Sergey Levine. Reinforcement learning from passive\ndata via latent intentions. In International Conference on Machine Learning, pages 11321\u201311339.\nPMLR, 2023.\n- Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity. Advances in Neural Information Processing Systems, 35:15281\u201315295, 2022.\n- Botao Hao, Rahul Jain, Dengwang Tang, and Zheng Wen. Bridging imitation and online reinforcement learning: An optimistic tale. ArXiv, abs/2303.11369, 2023.\n- Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109\u20131117, 2016.\n- Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient? Advances in neural information processing systems, 31, 2018.\n- Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning, 2020.\n- Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Q-learning. In International Conference on Learning Representations, 2022. URL Link.\n- Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. URLB: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191, 2021.\n- Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhutdinov. Efficient exploration via state marginal matching. ArXiv, abs/1906.05274, 2019.\n- Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online reinforcement learning via balanced replay and pessimistic Q-ensemble. In 5th Annual Conference on Robot Learning, 2021. URL Link.\n- Qiyang Li, Yuexiang Zhai, Yi Ma, and Sergey Levine. Understanding the complexity gains of single-task RL with a curriculum. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 20412\u201320451. PMLR, 23\u201329 Jul 2023. URL Link.\n- Runji Lin, Ye Li, Xidong Feng, Zhaowei Zhang, Xian Hong Wu Fung, Haifeng Zhang, Jun Wang, Yali Du, and Yaodong Yang. Contextual transformer for offline meta reinforcement learning. ArXiv, abs/2211.08016, 2022.", "md": "- Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta reinforcement learning - identifiability\nchallenges and effective data collection strategies. In Neural Information Processing Systems, 2021.\n- Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and P. Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016.\n- Jesse Farebrother, Joshua Greaves, Rishabh Agarwal, Charline Le Lan, Ross Goroshin,\nPablo Samuel Castro, and Marc G Bellemare. Proto-value networks: Scaling representation learning\nwith auxiliary tasks. In The Eleventh International Conference on Learning Representations, 2023.\nURL Link.\n- Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n- Dibya Ghosh, Chethan Anand Bhateja, and Sergey Levine. Reinforcement learning from passive\ndata via latent intentions. In International Conference on Machine Learning, pages 11321\u201311339.\nPMLR, 2023.\n- Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity. Advances in Neural Information Processing Systems, 35:15281\u201315295, 2022.\n- Botao Hao, Rahul Jain, Dengwang Tang, and Zheng Wen. Bridging imitation and online reinforcement learning: An optimistic tale. ArXiv, abs/2303.11369, 2023.\n- Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109\u20131117, 2016.\n- Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient? Advances in neural information processing systems, 31, 2018.\n- Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning, 2020.\n- Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Q-learning. In International Conference on Learning Representations, 2022. URL Link.\n- Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. URLB: Unsupervised reinforcement learning benchmark. arXiv preprint arXiv:2110.15191, 2021.\n- Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhutdinov. Efficient exploration via state marginal matching. ArXiv, abs/1906.05274, 2019.\n- Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online reinforcement learning via balanced replay and pessimistic Q-ensemble. In 5th Annual Conference on Robot Learning, 2021. URL Link.\n- Qiyang Li, Yuexiang Zhai, Yi Ma, and Sergey Levine. Understanding the complexity gains of single-task RL with a curriculum. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 20412\u201320451. PMLR, 23\u201329 Jul 2023. URL Link.\n- Runji Lin, Ye Li, Xidong Feng, Zhaowei Zhang, Xian Hong Wu Fung, Haifeng Zhang, Jun Wang, Yali Du, and Yaodong Yang. Contextual transformer for offline meta reinforcement learning. ArXiv, abs/2211.08016, 2022."}]}, {"page": 14, "text": "Toru Lin and Allan Jabri. MIMEx: Intrinsic rewards from masked input modeling. In Thirty-seventh\n  Conference on Neural Information Processing Systems, 2023. URL https://openreview.\n  net/forum?id=g1dMYenhe4.\nHao Liu and Pieter Abbeel.       Behavior from the void: Unsupervised active pre-training.        In\n  A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neu-\n  ral Information Processing Systems, 2021. URL https://openreview.net/forum?id=\n  fIn4wLS2XzU.\nYicheng Luo, Jackie Kay, Edward Grefenstette, and Marc Peter Deisenroth. Finetuning from offline\n  reinforcement learning: Challenges, trade-offs and practical solutions. ArXiv, abs/2303.17396,\n  2023.\nYecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy\n  Zhang. VIP: Towards universal visual reward and representation via value-implicit pre-training.\n  In The Eleventh International Conference on Learning Representations, 2023. URL https:\n  //openreview.net/forum?id=YJ7o2wetJ2.\nZhao Mandi, Pieter Abbeel, and Stephen James. On the effectiveness of fine-tuning versus meta-\n  reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\n  Cho, editors, Advances in Neural Information Processing Systems, 2022.          URL https://\n  openreview.net/forum?id=mux7gn3g_3.\nRussell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discover-\n  ing and achieving goals via world models. Advances in Neural Information Processing Systems,\n  34, 2021.\nAshvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and P. Abbeel. Overcoming\n  exploration in reinforcement learning with demonstrations. 2018 IEEE International Conference\n  on Robotics and Automation (ICRA), pages 6292\u20136299, 2017.\nAshvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. AWAC: Accelerating online\n  reinforcement learning with offline datasets, 2021.\nMitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral\n  Kumar, and Sergey Levine. Cal-QL: Calibrated offline RL pre-training for efficient online fine-\n  tuning. arXiv preprint arXiv:2303.05479, 2023.\nSoroush Nasiriany, Tian Gao, Ajay Mandlekar, and Yuke Zhu. Learning and retrieval from prior\n  data for skill-based imitation learning. In Conference on Robot Learning, 2022.\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\n  by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and\n  Pattern Recognition Workshops, pages 16\u201317, 2017.\nDeepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.\n  In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International\n  Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,\n  pages 5062\u20135071. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/\n  v97/pathak19a.html.\nKarl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned\n  skill priors. In Conference on robot learning, pages 188\u2013204. PMLR, 2021a.\nKarl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J Lim. Guided reinforcement learning with\n  learned skills. arXiv preprint arXiv:2107.10253, 2021b.\nVitchyr H. Pong, Ashvin Nair, Laura Smith, Catherine Huang, and Sergey Levine. Offline meta-\n  reinforcement learning with online self-supervision. In International Conference on Machine\n  Learning, 2021.\nIlija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell.\n  Real-world robot learning with masked visual pre-training. In 6th Annual Conference on Robot\n  Learning, 2022. URL https://openreview.net/forum?id=KWCZfuqshd.\n                                                 14", "md": "# References\n\n# References\n\n- Toru Lin and Allan Jabri. MIMEx: Intrinsic rewards from masked input modeling. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=g1dMYenhe4.\n- Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=fIn4wLS2XzU.\n- Yicheng Luo, Jackie Kay, Edward Grefenstette, and Marc Peter Deisenroth. Finetuning from offline\nreinforcement learning: Challenges, trade-offs and practical solutions. ArXiv, abs/2303.17396, 2023.\n- Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy\nZhang. VIP: Towards universal visual reward and representation via value-implicit pre-training.\nIn The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=YJ7o2wetJ2.\n- Zhao Mandi, Pieter Abbeel, and Stephen James. On the effectiveness of fine-tuning versus meta-reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=mux7gn3g_3.\n- Russell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discovering and achieving goals via world models. Advances in Neural Information Processing Systems, 34, 2021.\n- Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and P. Abbeel. Overcoming exploration in reinforcement learning with demonstrations. 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 6292\u20136299, 2017.\n- Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. AWAC: Accelerating online reinforcement learning with offline datasets, 2021.\n- Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-QL: Calibrated offline RL pre-training for efficient online fine-tuning. arXiv preprint arXiv:2303.05479, 2023.\n- Soroush Nasiriany, Tian Gao, Ajay Mandlekar, and Yuke Zhu. Learning and retrieval from prior data for skill-based imitation learning. In Conference on Robot Learning, 2022.\n- Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16\u201317, 2017.\n- Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5062\u20135071. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/pathak19a.html.\n- Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning, pages 188\u2013204. PMLR, 2021a.\n- Karl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J Lim. Guided reinforcement learning with learned skills. arXiv preprint arXiv:2107.10253, 2021b.\n- Vitchyr H. Pong, Ashvin Nair, Laura Smith, Catherine Huang, and Sergey Levine. Offline meta-reinforcement learning with online self-supervision. In International Conference on Machine Learning, 2021.\n- Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=KWCZfuqshd.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- Toru Lin and Allan Jabri. MIMEx: Intrinsic rewards from masked input modeling. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=g1dMYenhe4.\n- Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=fIn4wLS2XzU.\n- Yicheng Luo, Jackie Kay, Edward Grefenstette, and Marc Peter Deisenroth. Finetuning from offline\nreinforcement learning: Challenges, trade-offs and practical solutions. ArXiv, abs/2303.17396, 2023.\n- Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy\nZhang. VIP: Towards universal visual reward and representation via value-implicit pre-training.\nIn The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=YJ7o2wetJ2.\n- Zhao Mandi, Pieter Abbeel, and Stephen James. On the effectiveness of fine-tuning versus meta-reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=mux7gn3g_3.\n- Russell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discovering and achieving goals via world models. Advances in Neural Information Processing Systems, 34, 2021.\n- Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and P. Abbeel. Overcoming exploration in reinforcement learning with demonstrations. 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 6292\u20136299, 2017.\n- Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. AWAC: Accelerating online reinforcement learning with offline datasets, 2021.\n- Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-QL: Calibrated offline RL pre-training for efficient online fine-tuning. arXiv preprint arXiv:2303.05479, 2023.\n- Soroush Nasiriany, Tian Gao, Ajay Mandlekar, and Yuke Zhu. Learning and retrieval from prior data for skill-based imitation learning. In Conference on Robot Learning, 2022.\n- Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16\u201317, 2017.\n- Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5062\u20135071. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/pathak19a.html.\n- Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning, pages 188\u2013204. PMLR, 2021a.\n- Karl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J Lim. Guided reinforcement learning with learned skills. arXiv preprint arXiv:2107.10253, 2021b.\n- Vitchyr H. Pong, Ashvin Nair, Laura Smith, Catherine Huang, and Sergey Levine. Offline meta-reinforcement learning with online self-supervision. In International Conference on Machine Learning, 2021.\n- Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=KWCZfuqshd.", "md": "- Toru Lin and Allan Jabri. MIMEx: Intrinsic rewards from masked input modeling. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=g1dMYenhe4.\n- Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In\nA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=fIn4wLS2XzU.\n- Yicheng Luo, Jackie Kay, Edward Grefenstette, and Marc Peter Deisenroth. Finetuning from offline\nreinforcement learning: Challenges, trade-offs and practical solutions. ArXiv, abs/2303.17396, 2023.\n- Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy\nZhang. VIP: Towards universal visual reward and representation via value-implicit pre-training.\nIn The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=YJ7o2wetJ2.\n- Zhao Mandi, Pieter Abbeel, and Stephen James. On the effectiveness of fine-tuning versus meta-reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=mux7gn3g_3.\n- Russell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discovering and achieving goals via world models. Advances in Neural Information Processing Systems, 34, 2021.\n- Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and P. Abbeel. Overcoming exploration in reinforcement learning with demonstrations. 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 6292\u20136299, 2017.\n- Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. AWAC: Accelerating online reinforcement learning with offline datasets, 2021.\n- Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-QL: Calibrated offline RL pre-training for efficient online fine-tuning. arXiv preprint arXiv:2303.05479, 2023.\n- Soroush Nasiriany, Tian Gao, Ajay Mandlekar, and Yuke Zhu. Learning and retrieval from prior data for skill-based imitation learning. In Conference on Robot Learning, 2022.\n- Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16\u201317, 2017.\n- Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5062\u20135071. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/pathak19a.html.\n- Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning, pages 188\u2013204. PMLR, 2021a.\n- Karl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J Lim. Guided reinforcement learning with learned skills. arXiv preprint arXiv:2107.10253, 2021b.\n- Vitchyr H. Pong, Ashvin Nair, Laura Smith, Catherine Huang, and Sergey Levine. Offline meta-reinforcement learning with online self-supervision. In International Conference on Machine Learning, 2021.\n- Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=KWCZfuqshd."}]}, {"page": 15, "text": "Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy\n   meta-reinforcement learning via probabilistic context variables. In International conference on\n   machine learning, pages 5331\u20135340. PMLR, 2019.\nStefan Schaal. Learning by demonstration. In Encyclopedia of Machine Learning and Data Mining,\n   1996.\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.\n   Planning to explore via self-supervised world models. In International Conference on Machine\n   Learning, pages 8583\u20138592. PMLR, 2020.\nAvi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Par-\n   rot: Data-driven behavioral priors for reinforcement learning.     In International Conference\n   on Learning Representations, 2021a.       URL https://openreview.net/forum?id=\n   Ysuv-WOFeKR.\nAvi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Chaining\n   behaviors from data with model-free reinforcement learning. In Jens Kober, Fabio Ramos, and\n   Claire Tomlin, editors, Proceedings of the 2020 Conference on Robot Learning, volume 155 of\n   Proceedings of Machine Learning Research, pages 2162\u20132177. PMLR, 16\u201318 Nov 2021b. URL\n   https://proceedings.mlr.press/v155/singh21a.html.\nYuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hy-\n   brid RL: Using both offline and online data can make RL efficient. In The Eleventh International\n   Conference on Learning Representations, 2023. URL https://openreview.net/forum?\n   id=yyBis80iUuU.\nA. Srinivas, Michael Laskin, and P. Abbeel. CURL: Contrastive unsupervised representations for\n   reinforcement learning. In International Conference on Machine Learning, 2020.\nBradly C Stadie, Sergey Levine, and Pieter Abbeel.      Incentivizing exploration in reinforcement\n   learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\nAlexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for\n   markov decision processes. J. Comput. Syst. Sci., 74:1309\u20131331, 2008.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schul-\n   man, Filip DeTurck, and Pieter Abbeel. # Exploration: A study of count-based exploration for\n   deep reinforcement learning. In Advances in neural information processing systems, pages 2753\u2013\n   2762, 2017.\nDhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu,\n   Jonathan Schwarz, Guillaume Desjardins, Wojciech M. Czarnecki, Arun Ahuja, Yee Whye Teh,\n   and Nicolas Manfred Otto Heess. Behavior priors for efficient reinforcement learning. ArXiv,\n   abs/2010.14274, 2020.\nIkechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos\u00e9phine Simon, Matthew\n   Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, and Karol Hausman. Jump-start\n   reinforcement learning.   In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara En-\n   gelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International\n   Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,\n   pages 34556\u201334583. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/\n   v202/uchendu23a.html.\nMel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nico-\n   las Heess, Thomas Roth\u00f6rl, Thomas Lampe, and Martin Riedmiller.          Leveraging demonstra-\n   tions for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint\n   arXiv:1707.08817, 2017.\nRuosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement\n   learning with linear function approximation. Advances in neural information processing systems,\n   33:17816\u201317826, 2020.\n                                                15", "md": "# References\n\n# References\n\n- Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on machine learning, pages 5331\u20135340. PMLR, 2019.\n- Stefan Schaal. Learning by demonstration. In Encyclopedia of Machine Learning and Data Mining, 1996.\n- Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pages 8583\u20138592. PMLR, 2020.\n- Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven behavioral priors for reinforcement learning. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=Ysuv-WOFeKR.\n- Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Chaining behaviors from data with model-free reinforcement learning. In Jens Kober, Fabio Ramos, and Claire Tomlin, editors, Proceedings of the 2020 Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 2162\u20132177. PMLR, 16\u201318 Nov 2021b. URL https://proceedings.mlr.press/v155/singh21a.html.\n- Yuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid RL: Using both offline and online data can make RL efficient. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=yyBis80iUuU.\n- A. Srinivas, Michael Laskin, and P. Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, 2020.\n- Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\n- Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for Markov decision processes. J. Comput. Syst. Sci., 74:1309\u20131331, 2008.\n- Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # Exploration: A study of count-based exploration for deep reinforcement learning. In Advances in neural information processing systems, pages 2753\u20132762, 2017.\n- Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, Jonathan Schwarz, Guillaume Desjardins, Wojciech M. Czarnecki, Arun Ahuja, Yee Whye Teh, and Nicolas Manfred Otto Heess. Behavior priors for efficient reinforcement learning. ArXiv, abs/2010.14274, 2020.\n- Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos\u00e9phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, and Karol Hausman. Jump-start reinforcement learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 34556\u201334583. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/uchendu23a.html.\n- Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth\u00f6rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.\n- Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement learning with linear function approximation. Advances in neural information processing systems, 33:17816\u201317826, 2020.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on machine learning, pages 5331\u20135340. PMLR, 2019.\n- Stefan Schaal. Learning by demonstration. In Encyclopedia of Machine Learning and Data Mining, 1996.\n- Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pages 8583\u20138592. PMLR, 2020.\n- Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven behavioral priors for reinforcement learning. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=Ysuv-WOFeKR.\n- Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Chaining behaviors from data with model-free reinforcement learning. In Jens Kober, Fabio Ramos, and Claire Tomlin, editors, Proceedings of the 2020 Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 2162\u20132177. PMLR, 16\u201318 Nov 2021b. URL https://proceedings.mlr.press/v155/singh21a.html.\n- Yuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid RL: Using both offline and online data can make RL efficient. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=yyBis80iUuU.\n- A. Srinivas, Michael Laskin, and P. Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, 2020.\n- Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\n- Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for Markov decision processes. J. Comput. Syst. Sci., 74:1309\u20131331, 2008.\n- Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # Exploration: A study of count-based exploration for deep reinforcement learning. In Advances in neural information processing systems, pages 2753\u20132762, 2017.\n- Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, Jonathan Schwarz, Guillaume Desjardins, Wojciech M. Czarnecki, Arun Ahuja, Yee Whye Teh, and Nicolas Manfred Otto Heess. Behavior priors for efficient reinforcement learning. ArXiv, abs/2010.14274, 2020.\n- Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos\u00e9phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, and Karol Hausman. Jump-start reinforcement learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 34556\u201334583. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/uchendu23a.html.\n- Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth\u00f6rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.\n- Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement learning with linear function approximation. Advances in neural information processing systems, 33:17816\u201317826, 2020.", "md": "- Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International conference on machine learning, pages 5331\u20135340. PMLR, 2019.\n- Stefan Schaal. Learning by demonstration. In Encyclopedia of Machine Learning and Data Mining, 1996.\n- Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, pages 8583\u20138592. PMLR, 2020.\n- Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven behavioral priors for reinforcement learning. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=Ysuv-WOFeKR.\n- Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Chaining behaviors from data with model-free reinforcement learning. In Jens Kober, Fabio Ramos, and Claire Tomlin, editors, Proceedings of the 2020 Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 2162\u20132177. PMLR, 16\u201318 Nov 2021b. URL https://proceedings.mlr.press/v155/singh21a.html.\n- Yuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid RL: Using both offline and online data can make RL efficient. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=yyBis80iUuU.\n- A. Srinivas, Michael Laskin, and P. Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, 2020.\n- Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\n- Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for Markov decision processes. J. Comput. Syst. Sci., 74:1309\u20131331, 2008.\n- Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # Exploration: A study of count-based exploration for deep reinforcement learning. In Advances in neural information processing systems, pages 2753\u20132762, 2017.\n- Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, Jonathan Schwarz, Guillaume Desjardins, Wojciech M. Czarnecki, Arun Ahuja, Yee Whye Teh, and Nicolas Manfred Otto Heess. Behavior priors for efficient reinforcement learning. ArXiv, abs/2010.14274, 2020.\n- Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos\u00e9phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, and Karol Hausman. Jump-start reinforcement learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 34556\u201334583. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/uchendu23a.html.\n- Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth\u00f6rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.\n- Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement learning with linear function approximation. Advances in neural information processing systems, 33:17816\u201317826, 2020."}]}, {"page": 16, "text": "Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for\n   motor control. arXiv preprint arXiv:2203.06173, 2022.\nTengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging\n   sample-efficient offline and online reinforcement learning. In Neural Information Processing\n   Systems, 2021.\nMengjiao Yang and Ofir Nachum. Representation matters: offline pretraining for sequential decision\n   making. In International Conference on Machine Learning, pages 11784\u201311794. PMLR, 2021.\nDenis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, P. Abbeel, Alessandro Lazaric,\n   and Lerrel Pinto.  Don\u2019t change the algorithm, change the data: Exploratory data for offline\n   reinforcement learning. ArXiv, abs/2201.13425, 2022.\nTianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine.\n   How to leverage unlabeled data in offline reinforcement learning.      In Kamalika Chaudhuri,\n   Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceed-\n   ings of the 39th International Conference on Machine Learning, volume 162 of Proceedings\n   of Machine Learning Research, pages 25611\u201325635. PMLR, 17\u201323 Jul 2022. URL https:\n   //proceedings.mlr.press/v162/yu22c.html.\nHan Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy\n   learning for offline-to-online reinforcement learning. ArXiv, abs/2303.07693, 2023.\nZhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Learning sparse rewarded tasks from sub-\n   optimal demonstrations. ArXiv, abs/2004.00530, 2020.\nLuisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,\n   and Shimon Whiteson. VariBAD: A very good method for bayes-adaptive deep RL via meta-\n   learning. In International Conference on Learning Representations, 2019.\n                                                 16", "md": "# References\n\n## References\n\n- Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022.\n- Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. In Neural Information Processing Systems, 2021.\n- Mengjiao Yang and Ofir Nachum. Representation matters: offline pretraining for sequential decision making. In International Conference on Machine Learning, pages 11784\u201311794. PMLR, 2021.\n- Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, P. Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don\u2019t change the algorithm, change the data: Exploratory data for offline reinforcement learning. ArXiv, abs/2201.13425, 2022.\n- Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine. How to leverage unlabeled data in offline reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 25611\u201325635. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/yu22c.html.\n- Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy learning for offline-to-online reinforcement learning. ArXiv, abs/2303.07693, 2023.\n- Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Learning sparse rewarded tasks from sub-optimal demonstrations. ArXiv, abs/2004.00530, 2020.\n- Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. VariBAD: A very good method for bayes-adaptive deep RL via meta-learning. In International Conference on Learning Representations, 2019.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022.\n- Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. In Neural Information Processing Systems, 2021.\n- Mengjiao Yang and Ofir Nachum. Representation matters: offline pretraining for sequential decision making. In International Conference on Machine Learning, pages 11784\u201311794. PMLR, 2021.\n- Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, P. Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don\u2019t change the algorithm, change the data: Exploratory data for offline reinforcement learning. ArXiv, abs/2201.13425, 2022.\n- Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine. How to leverage unlabeled data in offline reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 25611\u201325635. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/yu22c.html.\n- Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy learning for offline-to-online reinforcement learning. ArXiv, abs/2303.07693, 2023.\n- Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Learning sparse rewarded tasks from sub-optimal demonstrations. ArXiv, abs/2004.00530, 2020.\n- Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. VariBAD: A very good method for bayes-adaptive deep RL via meta-learning. In International Conference on Learning Representations, 2019.", "md": "- Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022.\n- Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. In Neural Information Processing Systems, 2021.\n- Mengjiao Yang and Ofir Nachum. Representation matters: offline pretraining for sequential decision making. In International Conference on Machine Learning, pages 11784\u201311794. PMLR, 2021.\n- Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, P. Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don\u2019t change the algorithm, change the data: Exploratory data for offline reinforcement learning. ArXiv, abs/2201.13425, 2022.\n- Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine. How to leverage unlabeled data in offline reinforcement learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 25611\u201325635. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/yu22c.html.\n- Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy learning for offline-to-online reinforcement learning. ArXiv, abs/2303.07693, 2023.\n- Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Learning sparse rewarded tasks from sub-optimal demonstrations. ArXiv, abs/2004.00530, 2020.\n- Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. VariBAD: A very good method for bayes-adaptive deep RL via meta-learning. In International Conference on Learning Representations, 2019."}]}, {"page": 17, "text": "A     Experiment Setup Details\nOur codebase is based on the official RLPD codebase https://github.com/ikostrikov/\nrlpd with minor modifications. All configurations in our experiments use 10 seeds (20 seeds for\nall COG experiments) and we compute the standard error for the confidence interval in all our plots.\nA.1    Architecture Details\nRLPD extends a standard off-policy actor-critic algorithm which trains an actor network and a critic\nnetwork simultaneously. Our method requires training three additional networks: a reward predic-\ntion network, a termination prediction network, and an RND network. For state-based domains,\neach of the five networks is multilayer perceptron (MLP) with ReLU [Agarap, 2018] activation. For\nthe COG (image-based) domain, each network uses a pixel encoder followed by a MLP.\nPixel encoder architecture.      For the environments in the COG domain, we use the following\npixel encoder architecture, based on the pixel encoder implemented in RLPD\u2019s official codebase\n(located at https://github.com/ikostrikov/rlpd/blob/main/rlpd/networks/\nencoders/d4pg_encoder.py). Changes in orange are made to the observation size and num-\nber of frames stacked to match the dataset from the COG domain. Following the implementation\nfrom Ball et al. [2023], between the actor and the critic, the pixel encoder parameters are shared,\nand only the gradients through the critic loss objective update the encoder. The RND model, reward\nmodel, and the termination model each has its own pixel encoder with no parameter sharing.\n                       Parameter                                     Value\n                       Random Crop Padding                             4\n                       Number of Stacked Frames                        1\n                       Observation Size                            [48, 48, 3]\n                       CNN Features                            [32, 64, 128, 256]\n                       CNN Filter Size                                 3\n                       CNN Padding                                   Valid\n                       CNN Stride                                      2\n                       Encoder Output Latent Dimension                 50\n                                  Table 1: Pixel Encoder Architecture\nMLP architecture. In the state-based domains (AntMaze and Sparse Adroit), we use a 3-layer MLP\nwith hidden dimensions of 256. In the COG domain, we use a 2-layer MLP with hidden dimensions\nof 256, after a pixel encoder (Table 1). The RND network outputs a feature size of L = 256. The\nreward network output a single scalar. For the critic network, layer norm [Ba et al., 2016] is being\nadded to all the layers (of the MLP) with the learnable bias and scaling except the last layer.\nAction parameterization. In all of our experiments, we follow RLPD\u2019s actor design where the actor\noutputs a mean value \u00b5i and a log standard deviation value log \u03c3i for each action dimension, leading\nto a diagonal Gaussian distribution, N(\u00b5, diag(\u03c3)). This distribution is then transformed through\nthe Tanh function such that the range of the action falls in (\u22121, 1). The log standard deviation is\nclipped to be between \u221220 and 2, same as the default parameters in the RLPD repository.\nTermination prediction network. While predicting termination is trivial for the COG domain\nand the Sparse Adroit domain (the episode never terminates before the maximum episode length),\npredicting termination in AntMaze is just as difficult as predicting the reward (e.g., the episode\nterminates). To make our implementation generally applicable to any environments with or without\nterminations, we train a termination prediction network that takes in a state-action pair and outputs\na scalar (normalized through a Sigmoid function), which predicts the probability of the episode\nterminating at the current transition. Using the termination prediction network, the termination\nis also being labeled in the prior unlabeled data. This results in a slightly different TD back-up\nequation, which can be easily integrated in the RLPD algorithm:\n                         Q(s, a) \u2190   UCBR(s, a) + \u03b3(1 \u2212       \u02c6\n                                                             T(s, a)) \u00af\n                                                                      Q(s\u2032, a\u2032),\nwhere \u02c6r(s, a) is the labeled reward value and \u02c6 T(s, a) is the labeled termination prediction (with 1\nmeaning termination with a probability of 1). We left the discussion of the termination prediction\nnetwork out of the main body of the paper because it is a minor implementation detail. We present\nthe more detailed version of Algorithm 1 with the termination prediction as Algorithm 2.\n                                                   17", "md": "# Experiment Setup Details\n\n## Experiment Setup Details\n\nOur codebase is based on the official RLPD codebase https://github.com/ikostrikov/rlpd with minor modifications. All configurations in our experiments use 10 seeds (20 seeds for all COG experiments) and we compute the standard error for the confidence interval in all our plots.\n\n### Architecture Details\n\nRLPD extends a standard off-policy actor-critic algorithm which trains an actor network and a critic network simultaneously. Our method requires training three additional networks: a reward prediction network, a termination prediction network, and an RND network. For state-based domains, each of the five networks is multilayer perceptron (MLP) with ReLU activation. For the COG (image-based) domain, each network uses a pixel encoder followed by a MLP.\n\n#### Pixel encoder architecture\n\nFor the environments in the COG domain, we use the following pixel encoder architecture:\n\n|Parameter|Value|\n|---|---|\n|Random Crop Padding|4|\n|Number of Stacked Frames|1|\n|Observation Size|[48, 48, 3]|\n|CNN Features|[32, 64, 128, 256]|\n|CNN Filter Size|3|\n|CNN Padding|Valid|\n|CNN Stride|2|\n|Encoder Output Latent Dimension|50|\n\nTable 1: Pixel Encoder Architecture\n\nMLP architecture. In the state-based domains (AntMaze and Sparse Adroit), we use a 3-layer MLP with hidden dimensions of 256. In the COG domain, we use a 2-layer MLP with hidden dimensions of 256, after a pixel encoder (Table 1). The RND network outputs a feature size of L = 256. The reward network output a single scalar. For the critic network, layer norm is being added to all the layers (of the MLP) with the learnable bias and scaling except the last layer.\n\nAction parameterization. In all of our experiments, we follow RLPD\u2019s actor design where the actor outputs a mean value \u00b5i and a log standard deviation value log \u03c3i for each action dimension, leading to a diagonal Gaussian distribution, N(\u00b5, diag(\u03c3)). This distribution is then transformed through the Tanh function such that the range of the action falls in (-1, 1). The log standard deviation is clipped to be between -20 and 2, same as the default parameters in the RLPD repository.\n\nTermination prediction network. While predicting termination is trivial for the COG domain and the Sparse Adroit domain (the episode never terminates before the maximum episode length), predicting termination in AntMaze is just as difficult as predicting the reward (e.g., the episode terminates). To make our implementation generally applicable to any environments with or without terminations, we train a termination prediction network that takes in a state-action pair and outputs a scalar (normalized through a Sigmoid function), which predicts the probability of the episode terminating at the current transition.\n\nUsing the termination prediction network, the termination is also being labeled in the prior unlabeled data. This results in a slightly different TD back-up equation, which can be easily integrated in the RLPD algorithm:\n\n$$Q(s, a) \\leftarrow UCBR(s, a) + \\gamma(1 - \\hat{T}(s, a)) \\overline{Q}(s', a')$$\n\nWe left the discussion of the termination prediction network out of the main body of the paper because it is a minor implementation detail. We present the more detailed version of Algorithm 1 with the termination prediction as Algorithm 2.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Experiment Setup Details", "md": "# Experiment Setup Details"}, {"type": "heading", "lvl": 2, "value": "Experiment Setup Details", "md": "## Experiment Setup Details"}, {"type": "text", "value": "Our codebase is based on the official RLPD codebase https://github.com/ikostrikov/rlpd with minor modifications. All configurations in our experiments use 10 seeds (20 seeds for all COG experiments) and we compute the standard error for the confidence interval in all our plots.", "md": "Our codebase is based on the official RLPD codebase https://github.com/ikostrikov/rlpd with minor modifications. All configurations in our experiments use 10 seeds (20 seeds for all COG experiments) and we compute the standard error for the confidence interval in all our plots."}, {"type": "heading", "lvl": 3, "value": "Architecture Details", "md": "### Architecture Details"}, {"type": "text", "value": "RLPD extends a standard off-policy actor-critic algorithm which trains an actor network and a critic network simultaneously. Our method requires training three additional networks: a reward prediction network, a termination prediction network, and an RND network. For state-based domains, each of the five networks is multilayer perceptron (MLP) with ReLU activation. For the COG (image-based) domain, each network uses a pixel encoder followed by a MLP.", "md": "RLPD extends a standard off-policy actor-critic algorithm which trains an actor network and a critic network simultaneously. Our method requires training three additional networks: a reward prediction network, a termination prediction network, and an RND network. For state-based domains, each of the five networks is multilayer perceptron (MLP) with ReLU activation. For the COG (image-based) domain, each network uses a pixel encoder followed by a MLP."}, {"type": "heading", "lvl": 4, "value": "Pixel encoder architecture", "md": "#### Pixel encoder architecture"}, {"type": "text", "value": "For the environments in the COG domain, we use the following pixel encoder architecture:", "md": "For the environments in the COG domain, we use the following pixel encoder architecture:"}, {"type": "table", "rows": [["Parameter", "Value"], ["Random Crop Padding", "4"], ["Number of Stacked Frames", "1"], ["Observation Size", "[48, 48, 3]"], ["CNN Features", "[32, 64, 128, 256]"], ["CNN Filter Size", "3"], ["CNN Padding", "Valid"], ["CNN Stride", "2"], ["Encoder Output Latent Dimension", "50"]], "md": "|Parameter|Value|\n|---|---|\n|Random Crop Padding|4|\n|Number of Stacked Frames|1|\n|Observation Size|[48, 48, 3]|\n|CNN Features|[32, 64, 128, 256]|\n|CNN Filter Size|3|\n|CNN Padding|Valid|\n|CNN Stride|2|\n|Encoder Output Latent Dimension|50|", "isPerfectTable": true, "csv": "\"Parameter\",\"Value\"\n\"Random Crop Padding\",\"4\"\n\"Number of Stacked Frames\",\"1\"\n\"Observation Size\",\"[48, 48, 3]\"\n\"CNN Features\",\"[32, 64, 128, 256]\"\n\"CNN Filter Size\",\"3\"\n\"CNN Padding\",\"Valid\"\n\"CNN Stride\",\"2\"\n\"Encoder Output Latent Dimension\",\"50\""}, {"type": "text", "value": "Table 1: Pixel Encoder Architecture\n\nMLP architecture. In the state-based domains (AntMaze and Sparse Adroit), we use a 3-layer MLP with hidden dimensions of 256. In the COG domain, we use a 2-layer MLP with hidden dimensions of 256, after a pixel encoder (Table 1). The RND network outputs a feature size of L = 256. The reward network output a single scalar. For the critic network, layer norm is being added to all the layers (of the MLP) with the learnable bias and scaling except the last layer.\n\nAction parameterization. In all of our experiments, we follow RLPD\u2019s actor design where the actor outputs a mean value \u00b5i and a log standard deviation value log \u03c3i for each action dimension, leading to a diagonal Gaussian distribution, N(\u00b5, diag(\u03c3)). This distribution is then transformed through the Tanh function such that the range of the action falls in (-1, 1). The log standard deviation is clipped to be between -20 and 2, same as the default parameters in the RLPD repository.\n\nTermination prediction network. While predicting termination is trivial for the COG domain and the Sparse Adroit domain (the episode never terminates before the maximum episode length), predicting termination in AntMaze is just as difficult as predicting the reward (e.g., the episode terminates). To make our implementation generally applicable to any environments with or without terminations, we train a termination prediction network that takes in a state-action pair and outputs a scalar (normalized through a Sigmoid function), which predicts the probability of the episode terminating at the current transition.\n\nUsing the termination prediction network, the termination is also being labeled in the prior unlabeled data. This results in a slightly different TD back-up equation, which can be easily integrated in the RLPD algorithm:\n\n$$Q(s, a) \\leftarrow UCBR(s, a) + \\gamma(1 - \\hat{T}(s, a)) \\overline{Q}(s', a')$$\n\nWe left the discussion of the termination prediction network out of the main body of the paper because it is a minor implementation detail. We present the more detailed version of Algorithm 1 with the termination prediction as Algorithm 2.", "md": "Table 1: Pixel Encoder Architecture\n\nMLP architecture. In the state-based domains (AntMaze and Sparse Adroit), we use a 3-layer MLP with hidden dimensions of 256. In the COG domain, we use a 2-layer MLP with hidden dimensions of 256, after a pixel encoder (Table 1). The RND network outputs a feature size of L = 256. The reward network output a single scalar. For the critic network, layer norm is being added to all the layers (of the MLP) with the learnable bias and scaling except the last layer.\n\nAction parameterization. In all of our experiments, we follow RLPD\u2019s actor design where the actor outputs a mean value \u00b5i and a log standard deviation value log \u03c3i for each action dimension, leading to a diagonal Gaussian distribution, N(\u00b5, diag(\u03c3)). This distribution is then transformed through the Tanh function such that the range of the action falls in (-1, 1). The log standard deviation is clipped to be between -20 and 2, same as the default parameters in the RLPD repository.\n\nTermination prediction network. While predicting termination is trivial for the COG domain and the Sparse Adroit domain (the episode never terminates before the maximum episode length), predicting termination in AntMaze is just as difficult as predicting the reward (e.g., the episode terminates). To make our implementation generally applicable to any environments with or without terminations, we train a termination prediction network that takes in a state-action pair and outputs a scalar (normalized through a Sigmoid function), which predicts the probability of the episode terminating at the current transition.\n\nUsing the termination prediction network, the termination is also being labeled in the prior unlabeled data. This results in a slightly different TD back-up equation, which can be easily integrated in the RLPD algorithm:\n\n$$Q(s, a) \\leftarrow UCBR(s, a) + \\gamma(1 - \\hat{T}(s, a)) \\overline{Q}(s', a')$$\n\nWe left the discussion of the termination prediction network out of the main body of the paper because it is a minor implementation detail. We present the more detailed version of Algorithm 1 with the termination prediction as Algorithm 2."}]}, {"page": 18, "text": "Algorithm 2 EXPLORE (with termination estimates)\n 1: Input: prior unlabeled data Doffline\n 2: Initialize the UCB estimate of the reward function: UCBR(s, a)\n 3: Initialize an estimate of the termination function: \u02c6                     T (s, a)\n 4: Online replay buffer D \u2190                \u2205\n 5: Initialize off-policy RL algorithm with a policy \u03c0.\n 6: for each environment step do\n 7:        Collect (s, a, s\u2032, r) using the policy \u03c0 and add it to the replay buffer D.\n 8:        Update the UCB estimate UCBR(s, a) using the new transition (s, a, s\u2032, r).\n 9:        Relabel each transition (s, a, s\u2032) in Doffline with \u02c6                 r = UCBR(s, a), \u02c6          T = \u02c6  T  (s, a).\n10:        Run off-policy RL update on both D and the relabeled Doffline to improve the policy \u03c0.\n11: end for\nTraining of the UCB networks. The training of the reward prediction model, termination pre-\ndiction model and the RND model starts after 10000 environment steps for the AntMaze and the\nSparse Adroit domains and 5000 environment steps for the COG domain. The RND network takes\n1 gradient step on the current transition (snew, anew) according to the following objective:\n                                     L(\u03d5) = 1                                    f(snew, anew)\u22252       2,\n                                                  L\u2225f\u03d5(snew, anew) \u2212              \u00af\nwhere L is the number of output features of the RND network which is set to be 256. Both the\nreward model and the termination prediction model take the same number of gradient steps as the\nactor/critic (1 for COG and 20 for AntMaze and Sparse Adroit). The termination prediction model\noptimizes the following objective (the transition (s, a, r, T) is sampled from the online replay buffer\nD) with T \u2208        {0, 1} being the binary variable that indicates whether the episode terminates:\nL(\u03b8term) = \u2212E(s,a,T )\u223cD                T \u00b7 Log_Sigmoid( \u02c6         T\u03b8term(s, a)) + (1 \u2212          T  ) \u00b7 Log_Sigmoid(\u2212            \u02c6\n                                                                                                                               T\u03b8term(s, a)))  ,\nand the reward prediction model optimizes the following objective:\n                                            L(\u03b8) = E(s,a,r)\u223cD             (r\u03b8(s, a) \u2212      r)2    .\nModifications to UCB networks for the COG domain. For the COG domain, we used ICVFs to\ntest the incorporation of representation learning methods. We wanted to experiment with the formu-\nlation where the UCB network is learned on top of the encoder outputs of the ICVF representations,\nwhich depend only on states/observations s. That is, for some observation s and encoder \u03be(s), we\nwished to learn a UCB reward model r(\u03be(s)). However, this formulation does not incorporate ac-\ntions. Therefore, for all our experiments on the COG domain, the RND network, reward model,\nand termination prediction network all operate on observations only. More concretely, we make the\nfollowing replacements to our network architecture for the COG domains: for the RND networks\nwe replaced f\u03d5(s, a) with f\u03d5(s) and \u00af                 f(s, a) with \u00af    f(s), for the reward model, we replaced r\u03b8(s, a)\nwith r\u03b8(s), and for the termination prediction model, we replaced \u02c6                            T  (s, a) with \u02c6   T  (s).\nA.2      RLPD Hyperparameters\nMost of the hyperparameters we used are unchanged from the original RLPD hyperparameters, listed\nin Table 2. The online and offline batch sizes change when we run baselines that do not use offline\ndata. In that case, the offline batch size becomes 0, and the online batch size becomes 256, to keep\nthe combined batch size the same. The update the data ratio (UTD) was set to 1 in the COG domain.\nFor the other domains, we used the default value of 20. Action repeat is a hyperparameter that\ndetermines the number of times an action sampled from the policy is repeated in the environment.\nIt was originally 2 for the pixel-based hyperparameters in RLPD. We set it to 1 to match the prior\ndataset from the COG domains.\nA.3      Baseline Hyperparameters and Tuning Procedure Description\nOnline. For the online baseline, we don\u2019t use any offline data. The agent learns from scratch online.\n                                                                      18", "md": "# Algorithm 2 EXPLORE (with termination estimates)\n\n## Algorithm 2 EXPLORE (with termination estimates)\n\nInput: prior unlabeled data Doffline\n\n1. Initialize the UCB estimate of the reward function: UCBR(s, a)\n2. Initialize an estimate of the termination function: $\\hat{T}(s, a)$\n3. Online replay buffer D $\\leftarrow$ $\\emptyset$\n4. Initialize off-policy RL algorithm with a policy $\\pi$\n5. for each environment step do\n1. Collect (s, a, s', r) using the policy $\\pi$ and add it to the replay buffer D\n2. Update the UCB estimate UCBR(s, a) using the new transition (s, a, s', r)\n3. Relabel each transition (s, a, s') in Doffline with $\\hat{r} = UCBR(s, a)$, $\\hat{T} = \\hat{T}(s, a)$\n4. Run off-policy RL update on both D and the relabeled Doffline to improve the policy $\\pi$\n\nTraining of the UCB networks. The training of the reward prediction model, termination prediction model and the RND model starts after 10000 environment steps for the AntMaze and the Sparse Adroit domains and 5000 environment steps for the COG domain. The RND network takes 1 gradient step on the current transition (snew, anew) according to the following objective:\n\n$$L(\\phi) = \\frac{1}{L}||f(s_{new}, a_{new}) - \\overline{f}(\\phi(s_{new}, a_{new}))||^2_2$$\nwhere L is the number of output features of the RND network which is set to be 256. Both the reward model and the termination prediction model take the same number of gradient steps as the actor/critic (1 for COG and 20 for AntMaze and Sparse Adroit). The termination prediction model optimizes the following objective (the transition (s, a, r, T) is sampled from the online replay buffer D) with $$T \\in \\{0, 1\\}$$ being the binary variable that indicates whether the episode terminates:\n\n$$L(\\theta_{term}) = -E(s,a,T) \\sim D [T \\cdot \\text{Log\\_Sigmoid}(\\hat{T}\\theta_{term}(s, a)) + (1 - T) \\cdot \\text{Log\\_Sigmoid}(-\\hat{T}\\theta_{term}(s, a))]$$\nand the reward prediction model optimizes the following objective:\n\n$$L(\\theta) = E(s,a,r) \\sim D [(r\\theta(s, a) - r)^2]$$\nModifications to UCB networks for the COG domain. For the COG domain, we used ICVFs to test the incorporation of representation learning methods. We wanted to experiment with the formulation where the UCB network is learned on top of the encoder outputs of the ICVF representations, which depend only on states/observations s. That is, for some observation s and encoder $$\\xi(s)$$, we wished to learn a UCB reward model $$r(\\xi(s))$$. However, this formulation does not incorporate actions. Therefore, for all our experiments on the COG domain, the RND network, reward model, and termination prediction network all operate on observations only. More concretely, we make the following replacements to our network architecture for the COG domains: for the RND networks we replaced $$f_{\\phi}(s, a)$$ with $$f_{\\phi}(s)$$ and $$\\overline{f}(s, a)$$ with $$\\overline{f}(s)$$, for the reward model, we replaced $$r_{\\theta}(s, a)$$ with $$r_{\\theta}(s)$$, and for the termination prediction model, we replaced $$\\hat{T}(s, a)$$ with $$\\hat{T}(s)$$.\n\n### A.2 RLPD Hyperparameters\n\nMost of the hyperparameters we used are unchanged from the original RLPD hyperparameters, listed in Table 2. The online and offline batch sizes change when we run baselines that do not use offline data. In that case, the offline batch size becomes 0, and the online batch size becomes 256, to keep the combined batch size the same. The update the data ratio (UTD) was set to 1 in the COG domain. For the other domains, we used the default value of 20. Action repeat is a hyperparameter that determines the number of times an action sampled from the policy is repeated in the environment. It was originally 2 for the pixel-based hyperparameters in RLPD. We set it to 1 to match the prior dataset from the COG domains.\n\n### A.3 Baseline Hyperparameters and Tuning Procedure Description\n\nOnline. For the online baseline, we don\u2019t use any offline data. The agent learns from scratch online.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Algorithm 2 EXPLORE (with termination estimates)", "md": "# Algorithm 2 EXPLORE (with termination estimates)"}, {"type": "heading", "lvl": 2, "value": "Algorithm 2 EXPLORE (with termination estimates)", "md": "## Algorithm 2 EXPLORE (with termination estimates)"}, {"type": "text", "value": "Input: prior unlabeled data Doffline\n\n1. Initialize the UCB estimate of the reward function: UCBR(s, a)\n2. Initialize an estimate of the termination function: $\\hat{T}(s, a)$\n3. Online replay buffer D $\\leftarrow$ $\\emptyset$\n4. Initialize off-policy RL algorithm with a policy $\\pi$\n5. for each environment step do\n1. Collect (s, a, s', r) using the policy $\\pi$ and add it to the replay buffer D\n2. Update the UCB estimate UCBR(s, a) using the new transition (s, a, s', r)\n3. Relabel each transition (s, a, s') in Doffline with $\\hat{r} = UCBR(s, a)$, $\\hat{T} = \\hat{T}(s, a)$\n4. Run off-policy RL update on both D and the relabeled Doffline to improve the policy $\\pi$\n\nTraining of the UCB networks. The training of the reward prediction model, termination prediction model and the RND model starts after 10000 environment steps for the AntMaze and the Sparse Adroit domains and 5000 environment steps for the COG domain. The RND network takes 1 gradient step on the current transition (snew, anew) according to the following objective:\n\n$$L(\\phi) = \\frac{1}{L}||f(s_{new}, a_{new}) - \\overline{f}(\\phi(s_{new}, a_{new}))||^2_2$$\nwhere L is the number of output features of the RND network which is set to be 256. Both the reward model and the termination prediction model take the same number of gradient steps as the actor/critic (1 for COG and 20 for AntMaze and Sparse Adroit). The termination prediction model optimizes the following objective (the transition (s, a, r, T) is sampled from the online replay buffer D) with $$T \\in \\{0, 1\\}$$ being the binary variable that indicates whether the episode terminates:\n\n$$L(\\theta_{term}) = -E(s,a,T) \\sim D [T \\cdot \\text{Log\\_Sigmoid}(\\hat{T}\\theta_{term}(s, a)) + (1 - T) \\cdot \\text{Log\\_Sigmoid}(-\\hat{T}\\theta_{term}(s, a))]$$\nand the reward prediction model optimizes the following objective:\n\n$$L(\\theta) = E(s,a,r) \\sim D [(r\\theta(s, a) - r)^2]$$\nModifications to UCB networks for the COG domain. For the COG domain, we used ICVFs to test the incorporation of representation learning methods. We wanted to experiment with the formulation where the UCB network is learned on top of the encoder outputs of the ICVF representations, which depend only on states/observations s. That is, for some observation s and encoder $$\\xi(s)$$, we wished to learn a UCB reward model $$r(\\xi(s))$$. However, this formulation does not incorporate actions. Therefore, for all our experiments on the COG domain, the RND network, reward model, and termination prediction network all operate on observations only. More concretely, we make the following replacements to our network architecture for the COG domains: for the RND networks we replaced $$f_{\\phi}(s, a)$$ with $$f_{\\phi}(s)$$ and $$\\overline{f}(s, a)$$ with $$\\overline{f}(s)$$, for the reward model, we replaced $$r_{\\theta}(s, a)$$ with $$r_{\\theta}(s)$$, and for the termination prediction model, we replaced $$\\hat{T}(s, a)$$ with $$\\hat{T}(s)$$.", "md": "Input: prior unlabeled data Doffline\n\n1. Initialize the UCB estimate of the reward function: UCBR(s, a)\n2. Initialize an estimate of the termination function: $\\hat{T}(s, a)$\n3. Online replay buffer D $\\leftarrow$ $\\emptyset$\n4. Initialize off-policy RL algorithm with a policy $\\pi$\n5. for each environment step do\n1. Collect (s, a, s', r) using the policy $\\pi$ and add it to the replay buffer D\n2. Update the UCB estimate UCBR(s, a) using the new transition (s, a, s', r)\n3. Relabel each transition (s, a, s') in Doffline with $\\hat{r} = UCBR(s, a)$, $\\hat{T} = \\hat{T}(s, a)$\n4. Run off-policy RL update on both D and the relabeled Doffline to improve the policy $\\pi$\n\nTraining of the UCB networks. The training of the reward prediction model, termination prediction model and the RND model starts after 10000 environment steps for the AntMaze and the Sparse Adroit domains and 5000 environment steps for the COG domain. The RND network takes 1 gradient step on the current transition (snew, anew) according to the following objective:\n\n$$L(\\phi) = \\frac{1}{L}||f(s_{new}, a_{new}) - \\overline{f}(\\phi(s_{new}, a_{new}))||^2_2$$\nwhere L is the number of output features of the RND network which is set to be 256. Both the reward model and the termination prediction model take the same number of gradient steps as the actor/critic (1 for COG and 20 for AntMaze and Sparse Adroit). The termination prediction model optimizes the following objective (the transition (s, a, r, T) is sampled from the online replay buffer D) with $$T \\in \\{0, 1\\}$$ being the binary variable that indicates whether the episode terminates:\n\n$$L(\\theta_{term}) = -E(s,a,T) \\sim D [T \\cdot \\text{Log\\_Sigmoid}(\\hat{T}\\theta_{term}(s, a)) + (1 - T) \\cdot \\text{Log\\_Sigmoid}(-\\hat{T}\\theta_{term}(s, a))]$$\nand the reward prediction model optimizes the following objective:\n\n$$L(\\theta) = E(s,a,r) \\sim D [(r\\theta(s, a) - r)^2]$$\nModifications to UCB networks for the COG domain. For the COG domain, we used ICVFs to test the incorporation of representation learning methods. We wanted to experiment with the formulation where the UCB network is learned on top of the encoder outputs of the ICVF representations, which depend only on states/observations s. That is, for some observation s and encoder $$\\xi(s)$$, we wished to learn a UCB reward model $$r(\\xi(s))$$. However, this formulation does not incorporate actions. Therefore, for all our experiments on the COG domain, the RND network, reward model, and termination prediction network all operate on observations only. More concretely, we make the following replacements to our network architecture for the COG domains: for the RND networks we replaced $$f_{\\phi}(s, a)$$ with $$f_{\\phi}(s)$$ and $$\\overline{f}(s, a)$$ with $$\\overline{f}(s)$$, for the reward model, we replaced $$r_{\\theta}(s, a)$$ with $$r_{\\theta}(s)$$, and for the termination prediction model, we replaced $$\\hat{T}(s, a)$$ with $$\\hat{T}(s)$$."}, {"type": "heading", "lvl": 3, "value": "A.2 RLPD Hyperparameters", "md": "### A.2 RLPD Hyperparameters"}, {"type": "text", "value": "Most of the hyperparameters we used are unchanged from the original RLPD hyperparameters, listed in Table 2. The online and offline batch sizes change when we run baselines that do not use offline data. In that case, the offline batch size becomes 0, and the online batch size becomes 256, to keep the combined batch size the same. The update the data ratio (UTD) was set to 1 in the COG domain. For the other domains, we used the default value of 20. Action repeat is a hyperparameter that determines the number of times an action sampled from the policy is repeated in the environment. It was originally 2 for the pixel-based hyperparameters in RLPD. We set it to 1 to match the prior dataset from the COG domains.", "md": "Most of the hyperparameters we used are unchanged from the original RLPD hyperparameters, listed in Table 2. The online and offline batch sizes change when we run baselines that do not use offline data. In that case, the offline batch size becomes 0, and the online batch size becomes 256, to keep the combined batch size the same. The update the data ratio (UTD) was set to 1 in the COG domain. For the other domains, we used the default value of 20. Action repeat is a hyperparameter that determines the number of times an action sampled from the policy is repeated in the environment. It was originally 2 for the pixel-based hyperparameters in RLPD. We set it to 1 to match the prior dataset from the COG domains."}, {"type": "heading", "lvl": 3, "value": "A.3 Baseline Hyperparameters and Tuning Procedure Description", "md": "### A.3 Baseline Hyperparameters and Tuning Procedure Description"}, {"type": "text", "value": "Online. For the online baseline, we don\u2019t use any offline data. The agent learns from scratch online.", "md": "Online. For the online baseline, we don\u2019t use any offline data. The agent learns from scratch online."}]}, {"page": 19, "text": "       Parameter                                                                                    Value\n       Online batch size                                                                              128\n       Offline batch size                                                                             128\n       Discount factor \u03b3                                                                             0.99\n       Optimizer                                                                                    Adam\n       Learning rate                                                                              3 \u00d7 10\u22124\n       Critic ensemble size                                                                            10\n       Random critic target subset size                                    2 for Adroit and COG and 1 for AntMaze\n       Gradient Steps to Online Data Ratio (UTD)                                                       20\n       Network Width                                                                                  256\n       Initial Entropy Temperature                                                                    1.0\n       Target Entropy                                                                          \u2212   dim(A)/2\n       Entropy Backups                                                                               False\n       Start Training                                                               after 5000 environment steps\n                                               Table 2: RLPD hyperparameters.\nOnline + RND. For the online with RND baseline, we added an RND bonus on top of the ground\ntruth rewards in the online data. That is, given an online transition (s, a, r, s\u2032), and RND feature\nnetworks f\u03d5(s, a), \u00af       f(s, a), we set\n                                           \u02c6\n                                           r(s, a) \u2190      r + 1                       f(s, a)||2  2\n                                                                 L||f\u03d5(s, a) \u2212         \u00af\nand use the transition (s, a, \u02c6        r, s\u2032) in the online update. The RND training is done the same way as in\nour method where a gradient step is taken on every new transition collected.\nNa\u00efve Reward Labeling. The na\u00efve reward labeling baseline is a simple way to incorporate the\nunlabeled prior data by learning a reward model online and relabeling the rewards of the prior data.\nFor some prior transition (s, a, s\u2032), and a reward model r\u03b8 learned from online interactions, we set\nthe reward of this prior transition to be \u02c6                r(s, a) = r\u03b8(s, a), giving the relabeled offline transition\n(s, a, \u02c6r, s\u2032). Then, we take a batch of 50% offline data and 50% online data, and run RLPD as usual.\nNote that the only difference this baseline has with our method is that \u02c6                          r does not have an RND bonus\nadded to it.\nNa\u00efve + BC. Aside from performing the reward relabeling na\u00efvely as described in the previous\nsection, we also regularize the actor to more closely match the behavior in the prior data by adding\na behavior cloning loss to the actor loss (\u03b8actor is the parameter of the actor in RLPD):\n                   Lactor,bc(\u03b8actor) = Lactor(\u03b8actor) \u2212                 \u03b1bcE(s,a)\u223cDoffline [log \u03c0\u03b8actor(a|s)] ,\nwhere Lactor is the original loss function used in RLPD.\nTo find the best loss coefficient \u03b1bc, we perform a sweep over {0., 0.01, 0.1} for each domain. The\nbest coefficient is 0.01 for all the domains.\nMinR. The MinR baseline is a simpler version of Na\u00efve Reward Labeling. For all prior transitions\n(s, a, s\u2032), we set the reward \u02c6           r(s, a) = rmin, where rmin is the minimum reward for the task. For\nexample, for sparse 0-1 rewards, rmin = 0. Then, we run RLPD with this relabeled offline data.\nBC + JSRL. This baseline has a few additional hyperparameters. One controls the number of\ngradient steps to pre-train the behavior cloning guide policy on the unlabeled prior data, NJSRL.\nEach gradient step is taken according to the BC objective (\u03b8guide_policy is the parameters for the\nguide policy):\n                       LJSRL,bc(\u03b8guide_policy) = \u2212E(s,a)\u223cDoffline                      log \u03c0\u03b8guide_policy(a|s)        .\nThe other controls the probability of starting a trajectory by first rolling out the guide policy, \u03b2JSRL.\nFinally, in the course of a trajectory, we switch from the guide policy to the exploration policy\nwith probability 1 \u2212           \u03b3 where \u03b3 = 0.99 is the discount factor. To find the best hyper-parameters,\nwe perform a sweep over \u03b2JSRL \u2208                      {0.1, 0.5, 0.9} and NJSRL \u2208                {5000, 20000, 100000} for both\nAntMaze and Sparse Adroit. For AntMaze, the best parameter set is (\u03b2JSRL, NJSRL) = (0.9, 5000).\nFor Sparse Adroit, the best parmaeter set is (\u03b2JSRL, NJSRL) = (0.5, 100000). For COG domain,\n                                                                      19", "md": "# Document\n\n## Parameter Value\n\n|Parameter|Value|\n|---|---|\n|Online batch size|128|\n|Offline batch size|128|\n|Discount factor $\\gamma$|0.99|\n|Optimizer|Adam|\n|Learning rate|3 x 10-4|\n|Critic ensemble size|10|\n|Random critic target subset size|2 for Adroit and COG and 1 for AntMaze|\n|Gradient Steps to Online Data Ratio (UTD)|20|\n|Network Width|256|\n|Initial Entropy Temperature|1.0|\n|Target Entropy|$- \\frac{dim(A)}{2}$|\n|Entropy Backups|False|\n|Start Training|after 5000 environment steps|\n\n## Online + RND\n\nFor the online with RND baseline, we added an RND bonus on top of the ground truth rewards in the online data. That is, given an online transition (s, a, r, s\u2032), and RND feature networks f\u03d5(s, a), f(s, a), we set\n\n$$\\hat{r}(s, a) \\leftarrow r + 1 - \\frac{{||f(s, a) - f_{\\phi}(s, a)||^2}}{L||f(s, a)||^2}$$\nand use the transition (s, a, $$\\hat{r}$$, s\u2032) in the online update. The RND training is done the same way as in our method where a gradient step is taken on every new transition collected.\n\n## Na\u00efve Reward Labeling\n\nThe na\u00efve reward labeling baseline is a simple way to incorporate the unlabeled prior data by learning a reward model online and relabeling the rewards of the prior data. For some prior transition (s, a, s\u2032), and a reward model r\u03b8 learned from online interactions, we set the reward of this prior transition to be $$\\hat{r}(s, a) = r_{\\theta}(s, a)$$, giving the relabeled offline transition (s, a, $$\\hat{r}$$, s\u2032). Then, we take a batch of 50% offline data and 50% online data, and run RLPD as usual. Note that the only difference this baseline has with our method is that $$\\hat{r}$$ does not have an RND bonus added to it.\n\n## Na\u00efve + BC\n\nAside from performing the reward relabeling na\u00efvely as described in the previous section, we also regularize the actor to more closely match the behavior in the prior data by adding a behavior cloning loss to the actor loss ($$\\theta_{actor}$$ is the parameter of the actor in RLPD):\n\n$$L_{actor,bc}(\\theta_{actor}) = L_{actor}(\\theta_{actor}) - \\alpha_{bc}E(s,a) \\sim D_{offline} [log \\pi_{\\theta_{actor}}(a|s)]$$\nwhere $$L_{actor}$$ is the original loss function used in RLPD. To find the best loss coefficient $$\\alpha_{bc}$$, we perform a sweep over {0., 0.01, 0.1} for each domain. The best coefficient is 0.01 for all the domains.\n\n## MinR\n\nThe MinR baseline is a simpler version of Na\u00efve Reward Labeling. For all prior transitions (s, a, s\u2032), we set the reward $$\\hat{r}(s, a) = r_{min}$$, where $$r_{min}$$ is the minimum reward for the task. For example, for sparse 0-1 rewards, $$r_{min} = 0$$. Then, we run RLPD with this relabeled offline data.\n\n## BC + JSRL\n\nThis baseline has a few additional hyperparameters. One controls the number of gradient steps to pre-train the behavior cloning guide policy on the unlabeled prior data, NJSRL. Each gradient step is taken according to the BC objective ($$\\theta_{guide\\_policy}$$ is the parameters for the guide policy):\n\n$$L_{JSRL,bc}(\\theta_{guide\\_policy}) = -E(s,a) \\sim D_{offline} [log \\pi_{\\theta_{guide\\_policy}}(a|s)]$$\nThe other controls the probability of starting a trajectory by first rolling out the guide policy, $$\\beta_{JSRL}$$. Finally, in the course of a trajectory, we switch from the guide policy to the exploration policy with probability $$1 - \\gamma$$ where $$\\gamma = 0.99$$ is the discount factor. To find the best hyper-parameters, we perform a sweep over $$\\beta_{JSRL} \\in {0.1, 0.5, 0.9}$$ and $$N_{JSRL} \\in {5000, 20000, 100000}$$ for both AntMaze and Sparse Adroit. For AntMaze, the best parameter set is ($$\\beta_{JSRL}, N_{JSRL}$$) = (0.9, 5000). For Sparse Adroit, the best parameter set is ($$\\beta_{JSRL}, N_{JSRL}$$) = (0.5, 100000). For COG domain.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Parameter Value", "md": "## Parameter Value"}, {"type": "table", "rows": [["Parameter", "Value"], ["Online batch size", "128"], ["Offline batch size", "128"], ["Discount factor $\\gamma$", "0.99"], ["Optimizer", "Adam"], ["Learning rate", "3 x 10-4"], ["Critic ensemble size", "10"], ["Random critic target subset size", "2 for Adroit and COG and 1 for AntMaze"], ["Gradient Steps to Online Data Ratio (UTD)", "20"], ["Network Width", "256"], ["Initial Entropy Temperature", "1.0"], ["Target Entropy", "$- \\frac{dim(A)}{2}$"], ["Entropy Backups", "False"], ["Start Training", "after 5000 environment steps"]], "md": "|Parameter|Value|\n|---|---|\n|Online batch size|128|\n|Offline batch size|128|\n|Discount factor $\\gamma$|0.99|\n|Optimizer|Adam|\n|Learning rate|3 x 10-4|\n|Critic ensemble size|10|\n|Random critic target subset size|2 for Adroit and COG and 1 for AntMaze|\n|Gradient Steps to Online Data Ratio (UTD)|20|\n|Network Width|256|\n|Initial Entropy Temperature|1.0|\n|Target Entropy|$- \\frac{dim(A)}{2}$|\n|Entropy Backups|False|\n|Start Training|after 5000 environment steps|", "isPerfectTable": true, "csv": "\"Parameter\",\"Value\"\n\"Online batch size\",\"128\"\n\"Offline batch size\",\"128\"\n\"Discount factor $\\gamma$\",\"0.99\"\n\"Optimizer\",\"Adam\"\n\"Learning rate\",\"3 x 10-4\"\n\"Critic ensemble size\",\"10\"\n\"Random critic target subset size\",\"2 for Adroit and COG and 1 for AntMaze\"\n\"Gradient Steps to Online Data Ratio (UTD)\",\"20\"\n\"Network Width\",\"256\"\n\"Initial Entropy Temperature\",\"1.0\"\n\"Target Entropy\",\"$- \\frac{dim(A)}{2}$\"\n\"Entropy Backups\",\"False\"\n\"Start Training\",\"after 5000 environment steps\""}, {"type": "heading", "lvl": 2, "value": "Online + RND", "md": "## Online + RND"}, {"type": "text", "value": "For the online with RND baseline, we added an RND bonus on top of the ground truth rewards in the online data. That is, given an online transition (s, a, r, s\u2032), and RND feature networks f\u03d5(s, a), f(s, a), we set\n\n$$\\hat{r}(s, a) \\leftarrow r + 1 - \\frac{{||f(s, a) - f_{\\phi}(s, a)||^2}}{L||f(s, a)||^2}$$\nand use the transition (s, a, $$\\hat{r}$$, s\u2032) in the online update. The RND training is done the same way as in our method where a gradient step is taken on every new transition collected.", "md": "For the online with RND baseline, we added an RND bonus on top of the ground truth rewards in the online data. That is, given an online transition (s, a, r, s\u2032), and RND feature networks f\u03d5(s, a), f(s, a), we set\n\n$$\\hat{r}(s, a) \\leftarrow r + 1 - \\frac{{||f(s, a) - f_{\\phi}(s, a)||^2}}{L||f(s, a)||^2}$$\nand use the transition (s, a, $$\\hat{r}$$, s\u2032) in the online update. The RND training is done the same way as in our method where a gradient step is taken on every new transition collected."}, {"type": "heading", "lvl": 2, "value": "Na\u00efve Reward Labeling", "md": "## Na\u00efve Reward Labeling"}, {"type": "text", "value": "The na\u00efve reward labeling baseline is a simple way to incorporate the unlabeled prior data by learning a reward model online and relabeling the rewards of the prior data. For some prior transition (s, a, s\u2032), and a reward model r\u03b8 learned from online interactions, we set the reward of this prior transition to be $$\\hat{r}(s, a) = r_{\\theta}(s, a)$$, giving the relabeled offline transition (s, a, $$\\hat{r}$$, s\u2032). Then, we take a batch of 50% offline data and 50% online data, and run RLPD as usual. Note that the only difference this baseline has with our method is that $$\\hat{r}$$ does not have an RND bonus added to it.", "md": "The na\u00efve reward labeling baseline is a simple way to incorporate the unlabeled prior data by learning a reward model online and relabeling the rewards of the prior data. For some prior transition (s, a, s\u2032), and a reward model r\u03b8 learned from online interactions, we set the reward of this prior transition to be $$\\hat{r}(s, a) = r_{\\theta}(s, a)$$, giving the relabeled offline transition (s, a, $$\\hat{r}$$, s\u2032). Then, we take a batch of 50% offline data and 50% online data, and run RLPD as usual. Note that the only difference this baseline has with our method is that $$\\hat{r}$$ does not have an RND bonus added to it."}, {"type": "heading", "lvl": 2, "value": "Na\u00efve + BC", "md": "## Na\u00efve + BC"}, {"type": "text", "value": "Aside from performing the reward relabeling na\u00efvely as described in the previous section, we also regularize the actor to more closely match the behavior in the prior data by adding a behavior cloning loss to the actor loss ($$\\theta_{actor}$$ is the parameter of the actor in RLPD):\n\n$$L_{actor,bc}(\\theta_{actor}) = L_{actor}(\\theta_{actor}) - \\alpha_{bc}E(s,a) \\sim D_{offline} [log \\pi_{\\theta_{actor}}(a|s)]$$\nwhere $$L_{actor}$$ is the original loss function used in RLPD. To find the best loss coefficient $$\\alpha_{bc}$$, we perform a sweep over {0., 0.01, 0.1} for each domain. The best coefficient is 0.01 for all the domains.", "md": "Aside from performing the reward relabeling na\u00efvely as described in the previous section, we also regularize the actor to more closely match the behavior in the prior data by adding a behavior cloning loss to the actor loss ($$\\theta_{actor}$$ is the parameter of the actor in RLPD):\n\n$$L_{actor,bc}(\\theta_{actor}) = L_{actor}(\\theta_{actor}) - \\alpha_{bc}E(s,a) \\sim D_{offline} [log \\pi_{\\theta_{actor}}(a|s)]$$\nwhere $$L_{actor}$$ is the original loss function used in RLPD. To find the best loss coefficient $$\\alpha_{bc}$$, we perform a sweep over {0., 0.01, 0.1} for each domain. The best coefficient is 0.01 for all the domains."}, {"type": "heading", "lvl": 2, "value": "MinR", "md": "## MinR"}, {"type": "text", "value": "The MinR baseline is a simpler version of Na\u00efve Reward Labeling. For all prior transitions (s, a, s\u2032), we set the reward $$\\hat{r}(s, a) = r_{min}$$, where $$r_{min}$$ is the minimum reward for the task. For example, for sparse 0-1 rewards, $$r_{min} = 0$$. Then, we run RLPD with this relabeled offline data.", "md": "The MinR baseline is a simpler version of Na\u00efve Reward Labeling. For all prior transitions (s, a, s\u2032), we set the reward $$\\hat{r}(s, a) = r_{min}$$, where $$r_{min}$$ is the minimum reward for the task. For example, for sparse 0-1 rewards, $$r_{min} = 0$$. Then, we run RLPD with this relabeled offline data."}, {"type": "heading", "lvl": 2, "value": "BC + JSRL", "md": "## BC + JSRL"}, {"type": "text", "value": "This baseline has a few additional hyperparameters. One controls the number of gradient steps to pre-train the behavior cloning guide policy on the unlabeled prior data, NJSRL. Each gradient step is taken according to the BC objective ($$\\theta_{guide\\_policy}$$ is the parameters for the guide policy):\n\n$$L_{JSRL,bc}(\\theta_{guide\\_policy}) = -E(s,a) \\sim D_{offline} [log \\pi_{\\theta_{guide\\_policy}}(a|s)]$$\nThe other controls the probability of starting a trajectory by first rolling out the guide policy, $$\\beta_{JSRL}$$. Finally, in the course of a trajectory, we switch from the guide policy to the exploration policy with probability $$1 - \\gamma$$ where $$\\gamma = 0.99$$ is the discount factor. To find the best hyper-parameters, we perform a sweep over $$\\beta_{JSRL} \\in {0.1, 0.5, 0.9}$$ and $$N_{JSRL} \\in {5000, 20000, 100000}$$ for both AntMaze and Sparse Adroit. For AntMaze, the best parameter set is ($$\\beta_{JSRL}, N_{JSRL}$$) = (0.9, 5000). For Sparse Adroit, the best parameter set is ($$\\beta_{JSRL}, N_{JSRL}$$) = (0.5, 100000). For COG domain.", "md": "This baseline has a few additional hyperparameters. One controls the number of gradient steps to pre-train the behavior cloning guide policy on the unlabeled prior data, NJSRL. Each gradient step is taken according to the BC objective ($$\\theta_{guide\\_policy}$$ is the parameters for the guide policy):\n\n$$L_{JSRL,bc}(\\theta_{guide\\_policy}) = -E(s,a) \\sim D_{offline} [log \\pi_{\\theta_{guide\\_policy}}(a|s)]$$\nThe other controls the probability of starting a trajectory by first rolling out the guide policy, $$\\beta_{JSRL}$$. Finally, in the course of a trajectory, we switch from the guide policy to the exploration policy with probability $$1 - \\gamma$$ where $$\\gamma = 0.99$$ is the discount factor. To find the best hyper-parameters, we perform a sweep over $$\\beta_{JSRL} \\in {0.1, 0.5, 0.9}$$ and $$N_{JSRL} \\in {5000, 20000, 100000}$$ for both AntMaze and Sparse Adroit. For AntMaze, the best parameter set is ($$\\beta_{JSRL}, N_{JSRL}$$) = (0.9, 5000). For Sparse Adroit, the best parameter set is ($$\\beta_{JSRL}, N_{JSRL}$$) = (0.5, 100000). For COG domain."}]}, {"page": 20, "text": "none of the sweep runs succeeded. We just report the results of a randomly picked parameter set:\n(\u03b2JSRL, NJSRL) = (0.5, 100000). All the return evaluations are done solely on the exploration\npolicy (without rolling out the guide policy).\nOracle. This baseline assumes that the ground truth rewards for the online task are given in the\noffline data, and runs RLPD with that data.\nICVF training details. To evaluate reward functions and uncertainty estimation, we run our method\non top of a ICVF representation [Ghosh et al., 2023] pre-trained on the offline prior data. We use\nthe open-source implementation from the authors at https://github.com/dibyaghosh/\nicvf_release, using the COG encoder as the visual backbone and the default hyperparameters\notherwise from the open-source implementation. We pre-train the model for 0.75 \u00d7 105 timesteps,\nand use the final checkpoint as the encoder initialization for both RND and the reward model predic-\ntor. Notably, the representation is only ever used as an initialization, and is not frozen or regularized\nduring reward training.\n                                    Parameter               Value\n                                    Batch size               256\n                                    Discount factor \u03b3        0.99\n                                    Optimizer               Adam\n                                    Learning rate         3 \u00d7 10\u22124\n                                    MLP Width                256\n                                    MLP layers                2\n                                    Table 3: ICVF hyperparameters.\nB    Domain Details\nAntMaze (state)      In the D4RL AntMaze Fu et al. [2020] environment, an 8-DoF quadrupedal\n\u201cant\u201d agent navigates a maze from a fixed starting location and towards a fixed goal location. The\nreward is sparse: 0 around the goal and -1 elsewhere. The observation space is 29 dimensions,\nwith the first 2 dimensions indicating the position of the ant within the maze. The action space is 8\ndimensional. Trajectories terminate upon reaching the goal. We tested 6 total AntMaze tasks within\na total of 3 mazes of increasing size and complexity: umaze, umaze-diverse, medium-play,\nmedium-diverse, large-play, and large-diverse.\nThe offline data for this environment consists of trajectories with many different start and end po-\nsitions. The nature of these positions depends on the type of environment. In the play AntMaze\nenvironments, the start and end positions are handpicked whereas in the diverse AntMaze en-\nvironments, they are randomly chosen. Since the actual task contains only a single fixed start and\ngoal, the offline data does not always correspond to the online task. The coverage of the offline data\nis relatively complete over the entire maze.\nAdroit (state)    The Adroit environment Nair et al. [2021] we used consisted of a 24-DoF hand\nrobot agent and 3 dexterous manipulation tasks: pen-spinning, door-opening, and ball relocation.\nThe data distribution for the Adroit environments is relatively narrow. The prior data for these\nenvironments consists of expert trajectories from human demonstrations, as well as many behavior-\ncloned trajectories on the expert data. As the prior data here is closer to the target task distribution\ncompared to other environments, these experiments in Adroit allow us to compare the performance\nof our method more critically against our baselines that are less exploration focused.\nCOG (vision)      The three environments and datasets from COG come from the repo: https://\ngithub.com/avisingh599/cog. In the COG environments Singh et al. [2021b], the actor is\na simulated WidowX robot arm. The observation space is image-based with dimensions (48, 48, 3).\nThe rewards are sparse, with +1 reward when the task is completed, and 0 reward otherwise. The\nhorizon is infinite, but for exploration and evaluation, we set a max path length to end the episode.\nThe first environment we call Pick and Place. For this environment, the arm is placed in front\nof a small object and tray. The task is to pick up / grasp the object and place it in the tray, where\n                                                   20", "md": "```markdown\nnone of the sweep runs succeeded. We just report the results of a randomly picked parameter set:\n($$\\beta_{JSRL}$$, $$N_{JSRL}$$) = (0.5, 100000). All the return evaluations are done solely on the exploration\npolicy (without rolling out the guide policy).\nOracle. This baseline assumes that the ground truth rewards for the online task are given in the\noffline data, and runs RLPD with that data.\nICVF training details. To evaluate reward functions and uncertainty estimation, we run our method\non top of a ICVF representation [Ghosh et al., 2023] pre-trained on the offline prior data. We use\nthe open-source implementation from the authors at https://github.com/dibyaghosh/\nicvf_release, using the COG encoder as the visual backbone and the default hyperparameters\notherwise from the open-source implementation. We pre-train the model for 0.75 \u00d7 10^5 timesteps,\nand use the final checkpoint as the encoder initialization for both RND and the reward model predictor. Notably, the representation is only ever used as an initialization, and is not frozen or regularized\nduring reward training.\n\n| Parameter         | Value      |\n|-------------------|------------|\n| Batch size        | 256        |\n| Discount factor $$\\gamma$$ | 0.99 |\n| Optimizer         | Adam       |\n| Learning rate     | 3 \u00d7 10^-4  |\n| MLP Width         | 256        |\n| MLP layers        | 2          |\n\nTable 3: ICVF hyperparameters.\n\n### Domain Details\n**AntMaze (state)**\nIn the D4RL AntMaze Fu et al. [2020] environment, an 8-DoF quadrupedal\n\"ant\" agent navigates a maze from a fixed starting location and towards a fixed goal location. The\nreward is sparse: 0 around the goal and -1 elsewhere. The observation space is 29 dimensions,\nwith the first 2 dimensions indicating the position of the ant within the maze. The action space is 8\ndimensional. Trajectories terminate upon reaching the goal. We tested 6 total AntMaze tasks within\na total of 3 mazes of increasing size and complexity: umaze, umaze-diverse, medium-play,\nmedium-diverse, large-play, and large-diverse.\nThe offline data for this environment consists of trajectories with many different start and end positions. The nature of these positions depends on the type of environment. In the play AntMaze environments, the start and end positions are handpicked whereas in the diverse AntMaze environments, they are randomly chosen. Since the actual task contains only a single fixed start and goal, the offline data does not always correspond to the online task. The coverage of the offline data is relatively complete over the entire maze.\n\n**Adroit (state)**\nThe Adroit environment Nair et al. [2021] we used consisted of a 24-DoF hand\nrobot agent and 3 dexterous manipulation tasks: pen-spinning, door-opening, and ball relocation.\nThe data distribution for the Adroit environments is relatively narrow. The prior data for these environments consists of expert trajectories from human demonstrations, as well as many behavior-cloned trajectories on the expert data. As the prior data here is closer to the target task distribution compared to other environments, these experiments in Adroit allow us to compare the performance of our method more critically against our baselines that are less exploration focused.\n\n**COG (vision)**\nThe three environments and datasets from COG come from the repo: https://\ngithub.com/avisingh599/cog. In the COG environments Singh et al. [2021b], the actor is\na simulated WidowX robot arm. The observation space is image-based with dimensions (48, 48, 3).\nThe rewards are sparse, with +1 reward when the task is completed, and 0 reward otherwise. The\nhorizon is infinite, but for exploration and evaluation, we set a max path length to end the episode.\nThe first environment we call Pick and Place. For this environment, the arm is placed in front\nof a small object and tray. The task is to pick up / grasp the object and place it in the tray, where\n20\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nnone of the sweep runs succeeded. We just report the results of a randomly picked parameter set:\n($$\\beta_{JSRL}$$, $$N_{JSRL}$$) = (0.5, 100000). All the return evaluations are done solely on the exploration\npolicy (without rolling out the guide policy).\nOracle. This baseline assumes that the ground truth rewards for the online task are given in the\noffline data, and runs RLPD with that data.\nICVF training details. To evaluate reward functions and uncertainty estimation, we run our method\non top of a ICVF representation [Ghosh et al., 2023] pre-trained on the offline prior data. We use\nthe open-source implementation from the authors at https://github.com/dibyaghosh/\nicvf_release, using the COG encoder as the visual backbone and the default hyperparameters\notherwise from the open-source implementation. We pre-train the model for 0.75 \u00d7 10^5 timesteps,\nand use the final checkpoint as the encoder initialization for both RND and the reward model predictor. Notably, the representation is only ever used as an initialization, and is not frozen or regularized\nduring reward training.", "md": "```markdown\nnone of the sweep runs succeeded. We just report the results of a randomly picked parameter set:\n($$\\beta_{JSRL}$$, $$N_{JSRL}$$) = (0.5, 100000). All the return evaluations are done solely on the exploration\npolicy (without rolling out the guide policy).\nOracle. This baseline assumes that the ground truth rewards for the online task are given in the\noffline data, and runs RLPD with that data.\nICVF training details. To evaluate reward functions and uncertainty estimation, we run our method\non top of a ICVF representation [Ghosh et al., 2023] pre-trained on the offline prior data. We use\nthe open-source implementation from the authors at https://github.com/dibyaghosh/\nicvf_release, using the COG encoder as the visual backbone and the default hyperparameters\notherwise from the open-source implementation. We pre-train the model for 0.75 \u00d7 10^5 timesteps,\nand use the final checkpoint as the encoder initialization for both RND and the reward model predictor. Notably, the representation is only ever used as an initialization, and is not frozen or regularized\nduring reward training."}, {"type": "table", "rows": [["Parameter", "Value"], ["Batch size", "256"], ["Discount factor $$\\gamma$$", "0.99"], ["Optimizer", "Adam"], ["Learning rate", "3 \u00d7 10^-4"], ["MLP Width", "256"], ["MLP layers", "2"]], "md": "| Parameter         | Value      |\n|-------------------|------------|\n| Batch size        | 256        |\n| Discount factor $$\\gamma$$ | 0.99 |\n| Optimizer         | Adam       |\n| Learning rate     | 3 \u00d7 10^-4  |\n| MLP Width         | 256        |\n| MLP layers        | 2          |", "isPerfectTable": true, "csv": "\"Parameter\",\"Value\"\n\"Batch size\",\"256\"\n\"Discount factor $$\\gamma$$\",\"0.99\"\n\"Optimizer\",\"Adam\"\n\"Learning rate\",\"3 \u00d7 10^-4\"\n\"MLP Width\",\"256\"\n\"MLP layers\",\"2\""}, {"type": "text", "value": "Table 3: ICVF hyperparameters.", "md": "Table 3: ICVF hyperparameters."}, {"type": "heading", "lvl": 3, "value": "Domain Details", "md": "### Domain Details"}, {"type": "text", "value": "**AntMaze (state)**\nIn the D4RL AntMaze Fu et al. [2020] environment, an 8-DoF quadrupedal\n\"ant\" agent navigates a maze from a fixed starting location and towards a fixed goal location. The\nreward is sparse: 0 around the goal and -1 elsewhere. The observation space is 29 dimensions,\nwith the first 2 dimensions indicating the position of the ant within the maze. The action space is 8\ndimensional. Trajectories terminate upon reaching the goal. We tested 6 total AntMaze tasks within\na total of 3 mazes of increasing size and complexity: umaze, umaze-diverse, medium-play,\nmedium-diverse, large-play, and large-diverse.\nThe offline data for this environment consists of trajectories with many different start and end positions. The nature of these positions depends on the type of environment. In the play AntMaze environments, the start and end positions are handpicked whereas in the diverse AntMaze environments, they are randomly chosen. Since the actual task contains only a single fixed start and goal, the offline data does not always correspond to the online task. The coverage of the offline data is relatively complete over the entire maze.\n\n**Adroit (state)**\nThe Adroit environment Nair et al. [2021] we used consisted of a 24-DoF hand\nrobot agent and 3 dexterous manipulation tasks: pen-spinning, door-opening, and ball relocation.\nThe data distribution for the Adroit environments is relatively narrow. The prior data for these environments consists of expert trajectories from human demonstrations, as well as many behavior-cloned trajectories on the expert data. As the prior data here is closer to the target task distribution compared to other environments, these experiments in Adroit allow us to compare the performance of our method more critically against our baselines that are less exploration focused.\n\n**COG (vision)**\nThe three environments and datasets from COG come from the repo: https://\ngithub.com/avisingh599/cog. In the COG environments Singh et al. [2021b], the actor is\na simulated WidowX robot arm. The observation space is image-based with dimensions (48, 48, 3).\nThe rewards are sparse, with +1 reward when the task is completed, and 0 reward otherwise. The\nhorizon is infinite, but for exploration and evaluation, we set a max path length to end the episode.\nThe first environment we call Pick and Place. For this environment, the arm is placed in front\nof a small object and tray. The task is to pick up / grasp the object and place it in the tray, where\n20\n```", "md": "**AntMaze (state)**\nIn the D4RL AntMaze Fu et al. [2020] environment, an 8-DoF quadrupedal\n\"ant\" agent navigates a maze from a fixed starting location and towards a fixed goal location. The\nreward is sparse: 0 around the goal and -1 elsewhere. The observation space is 29 dimensions,\nwith the first 2 dimensions indicating the position of the ant within the maze. The action space is 8\ndimensional. Trajectories terminate upon reaching the goal. We tested 6 total AntMaze tasks within\na total of 3 mazes of increasing size and complexity: umaze, umaze-diverse, medium-play,\nmedium-diverse, large-play, and large-diverse.\nThe offline data for this environment consists of trajectories with many different start and end positions. The nature of these positions depends on the type of environment. In the play AntMaze environments, the start and end positions are handpicked whereas in the diverse AntMaze environments, they are randomly chosen. Since the actual task contains only a single fixed start and goal, the offline data does not always correspond to the online task. The coverage of the offline data is relatively complete over the entire maze.\n\n**Adroit (state)**\nThe Adroit environment Nair et al. [2021] we used consisted of a 24-DoF hand\nrobot agent and 3 dexterous manipulation tasks: pen-spinning, door-opening, and ball relocation.\nThe data distribution for the Adroit environments is relatively narrow. The prior data for these environments consists of expert trajectories from human demonstrations, as well as many behavior-cloned trajectories on the expert data. As the prior data here is closer to the target task distribution compared to other environments, these experiments in Adroit allow us to compare the performance of our method more critically against our baselines that are less exploration focused.\n\n**COG (vision)**\nThe three environments and datasets from COG come from the repo: https://\ngithub.com/avisingh599/cog. In the COG environments Singh et al. [2021b], the actor is\na simulated WidowX robot arm. The observation space is image-based with dimensions (48, 48, 3).\nThe rewards are sparse, with +1 reward when the task is completed, and 0 reward otherwise. The\nhorizon is infinite, but for exploration and evaluation, we set a max path length to end the episode.\nThe first environment we call Pick and Place. For this environment, the arm is placed in front\nof a small object and tray. The task is to pick up / grasp the object and place it in the tray, where\n20\n```"}]}, {"page": 21, "text": "                     Oracle           Online                    MinR             Naive + BC\n                     Ours             Online + RND              Naive            BC + JSRL\n                Pick and Place              Grasp from Closed Drawer         Grasp from Blocked Drawer 1\n   Normalized Return\n     0.5                                0.5                                0.5\n     0.00.0        0.1       0.2        0.00.0        0.1       0.2        0.00.0       0.1        0.2\n                                      Environment Steps (\u00d7106)\nFigure 9: The performance on 3 COG tasks with a larger offline dataset. BC+JSRL is able to match the oracle\nperformance. In the main body of the paper (Figure 4), we are using a dataset that is roughly 1% in size.\nsuccess occurs when the object is in the tray. The object is initialized at different locations on the\nground. The maximum episode length is 40.\nThe offline dataset for the Pick and Place consists of trajectories of grasping attempts and\nseparate trajectories of placing attempts. The data for these environments comes from a scripted\npolicy that achieves roughly 40% success at grasping the object and roughly 90% success at placing\nthe object into the tray. There are 100 trajectories of grasping attempts and 50 trajectories of placing\nattempts. There may be overlaps in states in the grasping and placing data, but there is no trajectory\nthat completes the entire pick and place task. For our RLPD baseline, where the rewards from the\nprior dataset are considered, we set a reward of +1 when the object is successfully placed in the tray,\nand 0 for all other transitions.\nWe also ran experiments on two drawer environments from COG. The main interaction in these\nenvironments is between the WidowX robot arm and a two tiered drawer. The first setup we call\nGrasp from Closed Drawer. In this setup, the bottom drawer is closed, and an object is\ninside the bottom drawer. The task is to open the bottom drawer and pick out the object. The\nsecond setup we call Grasp from Blocked Drawer 1. In this setup, the bottom drawer is\nclosed and blocked by an open upper drawer. The task is to close the upper drawer, open the bottom\ndrawer, and pick out the object. For both drawer environments, a success occurs when the object is\ntaken out of the bottom drawer. The maximum episode length is 50 for Grasp from Closed\nDrawer, and 80 for Grasp from Blocked Drawer 1.\nThe offline dataset we used for the drawer tasks consists of trajectories of drawer opening attempts\nand separate trajectories of taking the object out of an open drawer. For example, in the Grasp\nfrom Closed Drawer task, the drawer opening attempts consist of trajectories of opening the\nbottom drawer. Meanwhile, for the Grasp from Blocked Drawer 1 task, the drawer open-\ning attempts consist of trajectories of closing the top drawer and opening the bottom drawer. For\nboth tasks, the trajectories of taking the object out of an open drawer are the same. Similar to Pick\nand Place, these trajectories were generated using a randomized scripted policy as described in\nSingh et al. [2021b] that has about a 40-50% success rate at opening the drawer and about a 70%\nsuccess rate at taking the object out of the drawer. Like the Pick and Place task, there is no\ntrajectory that completes the entire task, but there may be overlaps in states. For our RLPD baseline,\nwe set a reward of +1 when the object has been successfully taken out of the bottom drawer, and 0\nfor all other transitions. There are 25 trajectories of drawer opening attempts and 25 trajectories of\nobject-taking attempts for the two drawer tasks.\nInitially, we experimented with a larger offline dataset with 100 times more trajectories for these\nthree tasks and we have found that the BC+JSRL baseline to closely match the oracle performance\nand our method did not really an edge over that baseline (see Figure 9). This is perhaps to be\nexpected as BC+JSRL puts a strong prior on following the offline data action by behavior cloning,\nand when offline data are good demonstration data. On the other hand, our method does not assume\nthat the offline dataset is of good quality, and does not try to closely follow the actions. Consequently,\n                                                      21", "md": "|Oracle|Online|MinR|Naive + BC|\n|---|---|---|---|\n|Ours|Online + RND|Naive|BC + JSRL|\n|Pick and Place|Grasp from Closed Drawer| |Grasp from Blocked Drawer 1|\n\nNormalized Return\n\n|0.5|0.5|0.5|\n|---|---|---|\n|0.0|0.1|0.2|0.0|0.1|0.2|0.0|0.1|0.2|\n\nEnvironment Steps (\u00d710^6)\n\nFigure 9: The performance on 3 COG tasks with a larger offline dataset. BC+JSRL is able to match the oracle performance. In the main body of the paper (Figure 4), we are using a dataset that is roughly 1% in size.\n\nSuccess occurs when the object is in the tray. The object is initialized at different locations on the ground. The maximum episode length is 40.\n\nThe offline dataset for the Pick and Place consists of trajectories of grasping attempts and separate trajectories of placing attempts. The data for these environments comes from a scripted policy that achieves roughly 40% success at grasping the object and roughly 90% success at placing the object into the tray. There are 100 trajectories of grasping attempts and 50 trajectories of placing attempts. There may be overlaps in states in the grasping and placing data, but there is no trajectory that completes the entire pick and place task. For our RLPD baseline, where the rewards from the prior dataset are considered, we set a reward of +1 when the object is successfully placed in the tray, and 0 for all other transitions.\n\nWe also ran experiments on two drawer environments from COG. The main interaction in these environments is between the WidowX robot arm and a two-tiered drawer. The first setup we call Grasp from Closed Drawer. In this setup, the bottom drawer is closed, and an object is inside the bottom drawer. The task is to open the bottom drawer and pick out the object. The second setup we call Grasp from Blocked Drawer 1. In this setup, the bottom drawer is closed and blocked by an open upper drawer. The task is to close the upper drawer, open the bottom drawer, and pick out the object. For both drawer environments, a success occurs when the object is taken out of the bottom drawer. The maximum episode length is 50 for Grasp from Closed Drawer, and 80 for Grasp from Blocked Drawer 1.\n\nThe offline dataset we used for the drawer tasks consists of trajectories of drawer opening attempts and separate trajectories of taking the object out of an open drawer. For example, in the Grasp from Closed Drawer task, the drawer opening attempts consist of trajectories of opening the bottom drawer. Meanwhile, for the Grasp from Blocked Drawer 1 task, the drawer opening attempts consist of trajectories of closing the top drawer and opening the bottom drawer. For both tasks, the trajectories of taking the object out of an open drawer are the same. Similar to Pick and Place, these trajectories were generated using a randomized scripted policy as described in Singh et al. [2021b] that has about a 40-50% success rate at opening the drawer and about a 70% success rate at taking the object out of the drawer. Like the Pick and Place task, there is no trajectory that completes the entire task, but there may be overlaps in states. For our RLPD baseline, we set a reward of +1 when the object has been successfully taken out of the bottom drawer, and 0 for all other transitions. There are 25 trajectories of drawer opening attempts and 25 trajectories of object-taking attempts for the two drawer tasks.\n\nInitially, we experimented with a larger offline dataset with 100 times more trajectories for these three tasks and we have found that the BC+JSRL baseline to closely match the oracle performance and our method did not really have an edge over that baseline (see Figure 9). This is perhaps to be expected as BC+JSRL puts a strong prior on following the offline data action by behavior cloning, and when offline data are good demonstration data. On the other hand, our method does not assume that the offline dataset is of good quality, and does not try to closely follow the actions. Consequently,", "images": [], "items": [{"type": "table", "rows": [["Oracle", "Online", "MinR", "Naive + BC"], ["Ours", "Online + RND", "Naive", "BC + JSRL"], ["Pick and Place", "Grasp from Closed Drawer", "", "Grasp from Blocked Drawer 1"]], "md": "|Oracle|Online|MinR|Naive + BC|\n|---|---|---|---|\n|Ours|Online + RND|Naive|BC + JSRL|\n|Pick and Place|Grasp from Closed Drawer| |Grasp from Blocked Drawer 1|", "isPerfectTable": true, "csv": "\"Oracle\",\"Online\",\"MinR\",\"Naive + BC\"\n\"Ours\",\"Online + RND\",\"Naive\",\"BC + JSRL\"\n\"Pick and Place\",\"Grasp from Closed Drawer\",\"\",\"Grasp from Blocked Drawer 1\""}, {"type": "text", "value": "Normalized Return", "md": "Normalized Return"}, {"type": "table", "rows": [["0.5", "0.5", "0.5"], ["0.0", "0.1", "0.2", "0.0", "0.1", "0.2", "0.0", "0.1", "0.2"]], "md": "|0.5|0.5|0.5|\n|---|---|---|\n|0.0|0.1|0.2|0.0|0.1|0.2|0.0|0.1|0.2|", "isPerfectTable": false, "csv": "\"0.5\",\"0.5\",\"0.5\"\n\"0.0\",\"0.1\",\"0.2\",\"0.0\",\"0.1\",\"0.2\",\"0.0\",\"0.1\",\"0.2\""}, {"type": "text", "value": "Environment Steps (\u00d710^6)\n\nFigure 9: The performance on 3 COG tasks with a larger offline dataset. BC+JSRL is able to match the oracle performance. In the main body of the paper (Figure 4), we are using a dataset that is roughly 1% in size.\n\nSuccess occurs when the object is in the tray. The object is initialized at different locations on the ground. The maximum episode length is 40.\n\nThe offline dataset for the Pick and Place consists of trajectories of grasping attempts and separate trajectories of placing attempts. The data for these environments comes from a scripted policy that achieves roughly 40% success at grasping the object and roughly 90% success at placing the object into the tray. There are 100 trajectories of grasping attempts and 50 trajectories of placing attempts. There may be overlaps in states in the grasping and placing data, but there is no trajectory that completes the entire pick and place task. For our RLPD baseline, where the rewards from the prior dataset are considered, we set a reward of +1 when the object is successfully placed in the tray, and 0 for all other transitions.\n\nWe also ran experiments on two drawer environments from COG. The main interaction in these environments is between the WidowX robot arm and a two-tiered drawer. The first setup we call Grasp from Closed Drawer. In this setup, the bottom drawer is closed, and an object is inside the bottom drawer. The task is to open the bottom drawer and pick out the object. The second setup we call Grasp from Blocked Drawer 1. In this setup, the bottom drawer is closed and blocked by an open upper drawer. The task is to close the upper drawer, open the bottom drawer, and pick out the object. For both drawer environments, a success occurs when the object is taken out of the bottom drawer. The maximum episode length is 50 for Grasp from Closed Drawer, and 80 for Grasp from Blocked Drawer 1.\n\nThe offline dataset we used for the drawer tasks consists of trajectories of drawer opening attempts and separate trajectories of taking the object out of an open drawer. For example, in the Grasp from Closed Drawer task, the drawer opening attempts consist of trajectories of opening the bottom drawer. Meanwhile, for the Grasp from Blocked Drawer 1 task, the drawer opening attempts consist of trajectories of closing the top drawer and opening the bottom drawer. For both tasks, the trajectories of taking the object out of an open drawer are the same. Similar to Pick and Place, these trajectories were generated using a randomized scripted policy as described in Singh et al. [2021b] that has about a 40-50% success rate at opening the drawer and about a 70% success rate at taking the object out of the drawer. Like the Pick and Place task, there is no trajectory that completes the entire task, but there may be overlaps in states. For our RLPD baseline, we set a reward of +1 when the object has been successfully taken out of the bottom drawer, and 0 for all other transitions. There are 25 trajectories of drawer opening attempts and 25 trajectories of object-taking attempts for the two drawer tasks.\n\nInitially, we experimented with a larger offline dataset with 100 times more trajectories for these three tasks and we have found that the BC+JSRL baseline to closely match the oracle performance and our method did not really have an edge over that baseline (see Figure 9). This is perhaps to be expected as BC+JSRL puts a strong prior on following the offline data action by behavior cloning, and when offline data are good demonstration data. On the other hand, our method does not assume that the offline dataset is of good quality, and does not try to closely follow the actions. Consequently,", "md": "Environment Steps (\u00d710^6)\n\nFigure 9: The performance on 3 COG tasks with a larger offline dataset. BC+JSRL is able to match the oracle performance. In the main body of the paper (Figure 4), we are using a dataset that is roughly 1% in size.\n\nSuccess occurs when the object is in the tray. The object is initialized at different locations on the ground. The maximum episode length is 40.\n\nThe offline dataset for the Pick and Place consists of trajectories of grasping attempts and separate trajectories of placing attempts. The data for these environments comes from a scripted policy that achieves roughly 40% success at grasping the object and roughly 90% success at placing the object into the tray. There are 100 trajectories of grasping attempts and 50 trajectories of placing attempts. There may be overlaps in states in the grasping and placing data, but there is no trajectory that completes the entire pick and place task. For our RLPD baseline, where the rewards from the prior dataset are considered, we set a reward of +1 when the object is successfully placed in the tray, and 0 for all other transitions.\n\nWe also ran experiments on two drawer environments from COG. The main interaction in these environments is between the WidowX robot arm and a two-tiered drawer. The first setup we call Grasp from Closed Drawer. In this setup, the bottom drawer is closed, and an object is inside the bottom drawer. The task is to open the bottom drawer and pick out the object. The second setup we call Grasp from Blocked Drawer 1. In this setup, the bottom drawer is closed and blocked by an open upper drawer. The task is to close the upper drawer, open the bottom drawer, and pick out the object. For both drawer environments, a success occurs when the object is taken out of the bottom drawer. The maximum episode length is 50 for Grasp from Closed Drawer, and 80 for Grasp from Blocked Drawer 1.\n\nThe offline dataset we used for the drawer tasks consists of trajectories of drawer opening attempts and separate trajectories of taking the object out of an open drawer. For example, in the Grasp from Closed Drawer task, the drawer opening attempts consist of trajectories of opening the bottom drawer. Meanwhile, for the Grasp from Blocked Drawer 1 task, the drawer opening attempts consist of trajectories of closing the top drawer and opening the bottom drawer. For both tasks, the trajectories of taking the object out of an open drawer are the same. Similar to Pick and Place, these trajectories were generated using a randomized scripted policy as described in Singh et al. [2021b] that has about a 40-50% success rate at opening the drawer and about a 70% success rate at taking the object out of the drawer. Like the Pick and Place task, there is no trajectory that completes the entire task, but there may be overlaps in states. For our RLPD baseline, we set a reward of +1 when the object has been successfully taken out of the bottom drawer, and 0 for all other transitions. There are 25 trajectories of drawer opening attempts and 25 trajectories of object-taking attempts for the two drawer tasks.\n\nInitially, we experimented with a larger offline dataset with 100 times more trajectories for these three tasks and we have found that the BC+JSRL baseline to closely match the oracle performance and our method did not really have an edge over that baseline (see Figure 9). This is perhaps to be expected as BC+JSRL puts a strong prior on following the offline data action by behavior cloning, and when offline data are good demonstration data. On the other hand, our method does not assume that the offline dataset is of good quality, and does not try to closely follow the actions. Consequently,"}]}, {"page": 22, "text": "our method is able to perform well more robustly across different settings compared to BC+JSRL\nas shown in our main results (Figure 4).\nC     Additional Results\nC.1     Full AntMaze results.\nWe include the complete set of AntMaze results in this section. Figure 10 shows the normalized\nreturn, which is the percentage success rate of the ant reaching the desired goal. Figure 11 shows\nthe coverage metric, which is an estimate of how much percentage of the free space that the ant has\nexplored in the maze.\n                          Oracle             Online                     MinR             Naive + BC\n                          Ours               Online + RND               Naive            BC + JSRL\n                  1.00      umaze-diverse                medium-diverse                 large-diverse\n                  0.75\n                Normalized Return\n                  0.50\n                  0.25\n                  0.00\n                     0.0      0.1     0.2     0.3  0.0      0.1     0.2     0.3  0.0     0.1      0.2     0.3\n                  1.00          umaze                      medium-play                    large-play\n                  0.75\n                  0.50\n                  0.25\n                  0.00\n                     0.0      0.1     0.2     0.3  0.0      0.1     0.2     0.3  0.0     0.1      0.2     0.3\n                                       Environment Steps (\u00d7106)\nFigure 10: The success rate for the 6 AntMaze tasks. Every method except Oracle has no access to the\nreward function. Our method consistently performs well and nearly matches the oracle performance.\n                          Oracle             Online                     MinR             Naive + BC\n                          Ours               Online + RND               Naive            BC + JSRL\n                 1.00       umaze-diverse                medium-diverse                 large-diverse\n                 0.75\n                 0.50\n                 0.25\n                 0.00\n               Coverage\n                     0.0     0.1      0.2     0.3  0.0     0.1      0.2     0.3  0.0     0.1     0.2      0.3\n                 1.00           umaze                     medium-play                     large-play\n                 0.75\n                 0.50\n                 0.25\n                 0.000.0     0.1      0.2     0.3  0.0     0.1      0.2     0.3  0.0     0.1     0.2      0.3\n                                           Environment Steps (\u00d7106)\nFigure 11: State coverage for the 6 AntMaze tasks, estimated by counting the number of square regions that\nthe ant has visited, normalized by the total number of square regions that can be visited.\n                                                            22", "md": "# Additional Results\n\n## Full AntMaze results\n\nWe include the complete set of AntMaze results in this section. Figure 10 shows the normalized return, which is the percentage success rate of the ant reaching the desired goal. Figure 11 shows the coverage metric, which is an estimate of how much percentage of the free space that the ant has explored in the maze.\n\n| |Oracle|Online|MinR|Naive + BC|\n|---|---|---|---|---|\n|1.00|umaze-diverse|medium-diverse| |large-diverse|\n|0.75|Normalized Return| | | |\n|0.50| | | | |\n|0.25| | | | |\n|0.00| | | | |\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Oracle & Online & MinR & Naive + BC \\\\\n\\hline\n1.00 & umaze-diverse & medium-diverse & large-diverse \\\\\n0.75 & \\text{Normalized Return} & & & \\\\\n0.50 & & & & \\\\\n0.25 & & & & \\\\\n0.00 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 10: The success rate for the 6 AntMaze tasks. Every method except Oracle has no access to the reward function. Our method consistently performs well and nearly matches the oracle performance.\n\n| |Oracle|Online|MinR|Naive + BC|\n|---|---|---|---|---|\n|1.00|umaze-diverse|medium-diverse| |large-diverse|\n|0.75|Coverage| | | |\n|0.50| | | | |\n|0.25| | | | |\n|0.00| | | | |\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Oracle & Online & MinR & Naive + BC \\\\\n\\hline\n1.00 & umaze-diverse & medium-diverse & large-diverse \\\\\n0.75 & \\text{Coverage} & & & \\\\\n0.50 & & & & \\\\\n0.25 & & & & \\\\\n0.00 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 11: State coverage for the 6 AntMaze tasks, estimated by counting the number of square regions that the ant has visited, normalized by the total number of square regions that can be visited.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Results", "md": "# Additional Results"}, {"type": "heading", "lvl": 2, "value": "Full AntMaze results", "md": "## Full AntMaze results"}, {"type": "text", "value": "We include the complete set of AntMaze results in this section. Figure 10 shows the normalized return, which is the percentage success rate of the ant reaching the desired goal. Figure 11 shows the coverage metric, which is an estimate of how much percentage of the free space that the ant has explored in the maze.", "md": "We include the complete set of AntMaze results in this section. Figure 10 shows the normalized return, which is the percentage success rate of the ant reaching the desired goal. Figure 11 shows the coverage metric, which is an estimate of how much percentage of the free space that the ant has explored in the maze."}, {"type": "table", "rows": [["", "Oracle", "Online", "MinR", "Naive + BC"], ["1.00", "umaze-diverse", "medium-diverse", "", "large-diverse"], ["0.75", "Normalized Return", "", "", ""], ["0.50", "", "", "", ""], ["0.25", "", "", "", ""], ["0.00", "", "", "", ""]], "md": "| |Oracle|Online|MinR|Naive + BC|\n|---|---|---|---|---|\n|1.00|umaze-diverse|medium-diverse| |large-diverse|\n|0.75|Normalized Return| | | |\n|0.50| | | | |\n|0.25| | | | |\n|0.00| | | | |", "isPerfectTable": true, "csv": "\"\",\"Oracle\",\"Online\",\"MinR\",\"Naive + BC\"\n\"1.00\",\"umaze-diverse\",\"medium-diverse\",\"\",\"large-diverse\"\n\"0.75\",\"Normalized Return\",\"\",\"\",\"\"\n\"0.50\",\"\",\"\",\"\",\"\"\n\"0.25\",\"\",\"\",\"\",\"\"\n\"0.00\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Oracle & Online & MinR & Naive + BC \\\\\n\\hline\n1.00 & umaze-diverse & medium-diverse & large-diverse \\\\\n0.75 & \\text{Normalized Return} & & & \\\\\n0.50 & & & & \\\\\n0.25 & & & & \\\\\n0.00 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 10: The success rate for the 6 AntMaze tasks. Every method except Oracle has no access to the reward function. Our method consistently performs well and nearly matches the oracle performance.", "md": "$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Oracle & Online & MinR & Naive + BC \\\\\n\\hline\n1.00 & umaze-diverse & medium-diverse & large-diverse \\\\\n0.75 & \\text{Normalized Return} & & & \\\\\n0.50 & & & & \\\\\n0.25 & & & & \\\\\n0.00 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 10: The success rate for the 6 AntMaze tasks. Every method except Oracle has no access to the reward function. Our method consistently performs well and nearly matches the oracle performance."}, {"type": "table", "rows": [["", "Oracle", "Online", "MinR", "Naive + BC"], ["1.00", "umaze-diverse", "medium-diverse", "", "large-diverse"], ["0.75", "Coverage", "", "", ""], ["0.50", "", "", "", ""], ["0.25", "", "", "", ""], ["0.00", "", "", "", ""]], "md": "| |Oracle|Online|MinR|Naive + BC|\n|---|---|---|---|---|\n|1.00|umaze-diverse|medium-diverse| |large-diverse|\n|0.75|Coverage| | | |\n|0.50| | | | |\n|0.25| | | | |\n|0.00| | | | |", "isPerfectTable": true, "csv": "\"\",\"Oracle\",\"Online\",\"MinR\",\"Naive + BC\"\n\"1.00\",\"umaze-diverse\",\"medium-diverse\",\"\",\"large-diverse\"\n\"0.75\",\"Coverage\",\"\",\"\",\"\"\n\"0.50\",\"\",\"\",\"\",\"\"\n\"0.25\",\"\",\"\",\"\",\"\"\n\"0.00\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Oracle & Online & MinR & Naive + BC \\\\\n\\hline\n1.00 & umaze-diverse & medium-diverse & large-diverse \\\\\n0.75 & \\text{Coverage} & & & \\\\\n0.50 & & & & \\\\\n0.25 & & & & \\\\\n0.00 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 11: State coverage for the 6 AntMaze tasks, estimated by counting the number of square regions that the ant has visited, normalized by the total number of square regions that can be visited.", "md": "$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Oracle & Online & MinR & Naive + BC \\\\\n\\hline\n1.00 & umaze-diverse & medium-diverse & large-diverse \\\\\n0.75 & \\text{Coverage} & & & \\\\\n0.50 & & & & \\\\\n0.25 & & & & \\\\\n0.00 & & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 11: State coverage for the 6 AntMaze tasks, estimated by counting the number of square regions that the ant has visited, normalized by the total number of square regions that can be visited."}]}, {"page": 23, "text": "C.2      Full Adroit Results\nWe include the complete set of Adroit results in this section. Figure 12 shows the normalized return\nfor each method on each of the three sparse Adroit tasks. Figure 13 studies the effect of periodic\nresetting the reward and the RND networks on the hardest task, relocate. Periodic resetting helps\na lot on the relocate task. We hypothesize that it is because the periodic resetting procedure\nmight have alleviated the overfitting issue of the reward function, allowing it to continuously adapt\nto incoming transitions.\n                                                                                                              Oracle                 Naive\n                                                                                                              Ours                   Naive + Reset\n                                                                                                              Ours + Reset\n              Oracle            Online                     MinR             Naive + BC                               relocate-binary-v0\n              Ours              Online + RND               Naive            BC + JSRL                            1.0\n    Normalized Return                                                                                          Normalized Return\n     1.0       pen-binary-v0                 door-binary-v0              relocate-binary-v0                      0.8\n     0.8                                                                                                         0.6\n     0.6\n     0.4                                                                                                         0.4\n     0.2                                                                                                         0.2\n     0.00.0   0.2    0.4   0.6   0.8  0.0   0.2   0.4    0.6   0.8  0.0   0.2   0.4   0.6    0.8                 0.00.0 0.2 0.4 0.6 0.8\n                          Environment Steps (\u00d7106)                                                                 Environment Steps (\u00d7106)\nFigure 12: The normalized return for the 3 sparse Adroit tasks.                                       Figure 13: Periodically resetting\nEvery method except Oracle has no access to the reward function. Na\u00efve                                the reward model is crucial for\nreward modeling without optimistic relabeling is sufficient for all Adroit                            success in relocate. We exper-\ntasks. Periodically resets (every 1000 environment steps) are used for                                iment with different reset intervals\nthe relocate task. See Figure 13 for the reset ablation study.                                        and have found that resetting the\n                                                                                                      reward model after every 20K gra-\n                                                                                                      dient steps (1K environment steps)\n                                                                                                      to work well.\nC.3      Full COG results\n                            Oracle                  Online                             MinR                  Naive + BC\n                            Ours                    Online + RND                       Naive                 BC + JSRL\n    Normalized ReturnPick and Place                        Grasp from Closed Drawer                     Grasp from Blocked Drawer 1\n       0.5                                           0.5                                            0.5\n       0.0 0.0           0.1            0.2          0.0  0.0           0.1           0.2           0.0 0.0            0.1           0.2\n                                                   Environment Steps (\u00d7106)\n                                 Figure 14: The performance on 3 COG tasks without ICVF.\nC.4      Implicit-Q-learning (IQL) as the backbone RL algorithm.\nConceptually, our proposed method, ExPLORe, is not specific to the selected RL method, and can be\ncombined with any RL methods that take in both offline and online transitions. Other than RLPD, we\nexperimented with another RL algorithm, IQL [Kostrikov et al., 2022], in the offline to online fine-\ntuning setting where we run IQL on an online replay buffer prepended with the reward-relabelled\noffline data. In our experiments (Figure 15), we have found the use of the RND bonus in the offline\ndata to be essential for accelerating learning in both medium datasets (without RND bonus, none of\nthe runs succeed).\n                                                                         23", "md": "# Document\n\n## C.2 Full Adroit Results\n\nWe include the complete set of Adroit results in this section. Figure 12 shows the normalized return for each method on each of the three sparse Adroit tasks. Figure 13 studies the effect of periodic resetting the reward and the RND networks on the hardest task, relocate. Periodic resetting helps a lot on the relocate task. We hypothesize that it is because the periodic resetting procedure might have alleviated the overfitting issue of the reward function, allowing it to continuously adapt to incoming transitions.\n\n| |Oracle|Naive|\n|---|---|---|\n| | |Ours|Naive + Reset|\n|Oracle|Online|MinR|Naive + BC|\n|Ours|Online + RND|Naive|BC + JSRL|\n\nFigure 12: The normalized return for the 3 sparse Adroit tasks. Every method except Oracle has no access to the reward function. Na\u00efve reward modeling without optimistic relabeling is sufficient for all Adroit tasks. Periodically resets (every 1000 environment steps) are used for the relocate task. See Figure 13 for the reset ablation study.\n\nFigure 13: Periodically resetting the reward model is crucial for success in relocate. We experiment with different reset intervals and have found that resetting the reward model after every 20K gradient steps (1K environment steps) to work well.\n\n## C.3 Full COG results\n\n| |Oracle|Online|MinR|Naive + BC|\n|---|---|---|---|---|\n| |Ours|Online + RND|Naive|BC + JSRL|\n\nFigure 14: The performance on 3 COG tasks without ICVF.\n\n## C.4 Implicit-Q-learning (IQL) as the backbone RL algorithm\n\nConceptually, our proposed method, ExPLORe, is not specific to the selected RL method, and can be combined with any RL methods that take in both offline and online transitions. Other than RLPD, we experimented with another RL algorithm, IQL [Kostrikov et al., 2022], in the offline to online fine-tuning setting where we run IQL on an online replay buffer prepended with the reward-relabelled offline data. In our experiments (Figure 15), we have found the use of the RND bonus in the offline data to be essential for accelerating learning in both medium datasets (without RND bonus, none of the runs succeed).\n\n23", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "C.2 Full Adroit Results", "md": "## C.2 Full Adroit Results"}, {"type": "text", "value": "We include the complete set of Adroit results in this section. Figure 12 shows the normalized return for each method on each of the three sparse Adroit tasks. Figure 13 studies the effect of periodic resetting the reward and the RND networks on the hardest task, relocate. Periodic resetting helps a lot on the relocate task. We hypothesize that it is because the periodic resetting procedure might have alleviated the overfitting issue of the reward function, allowing it to continuously adapt to incoming transitions.", "md": "We include the complete set of Adroit results in this section. Figure 12 shows the normalized return for each method on each of the three sparse Adroit tasks. Figure 13 studies the effect of periodic resetting the reward and the RND networks on the hardest task, relocate. Periodic resetting helps a lot on the relocate task. We hypothesize that it is because the periodic resetting procedure might have alleviated the overfitting issue of the reward function, allowing it to continuously adapt to incoming transitions."}, {"type": "table", "rows": [["", "Oracle", "Naive"], ["", "", "Ours", "Naive + Reset"], ["Oracle", "Online", "MinR", "Naive + BC"], ["Ours", "Online + RND", "Naive", "BC + JSRL"]], "md": "| |Oracle|Naive|\n|---|---|---|\n| | |Ours|Naive + Reset|\n|Oracle|Online|MinR|Naive + BC|\n|Ours|Online + RND|Naive|BC + JSRL|", "isPerfectTable": false, "csv": "\"\",\"Oracle\",\"Naive\"\n\"\",\"\",\"Ours\",\"Naive + Reset\"\n\"Oracle\",\"Online\",\"MinR\",\"Naive + BC\"\n\"Ours\",\"Online + RND\",\"Naive\",\"BC + JSRL\""}, {"type": "text", "value": "Figure 12: The normalized return for the 3 sparse Adroit tasks. Every method except Oracle has no access to the reward function. Na\u00efve reward modeling without optimistic relabeling is sufficient for all Adroit tasks. Periodically resets (every 1000 environment steps) are used for the relocate task. See Figure 13 for the reset ablation study.\n\nFigure 13: Periodically resetting the reward model is crucial for success in relocate. We experiment with different reset intervals and have found that resetting the reward model after every 20K gradient steps (1K environment steps) to work well.", "md": "Figure 12: The normalized return for the 3 sparse Adroit tasks. Every method except Oracle has no access to the reward function. Na\u00efve reward modeling without optimistic relabeling is sufficient for all Adroit tasks. Periodically resets (every 1000 environment steps) are used for the relocate task. See Figure 13 for the reset ablation study.\n\nFigure 13: Periodically resetting the reward model is crucial for success in relocate. We experiment with different reset intervals and have found that resetting the reward model after every 20K gradient steps (1K environment steps) to work well."}, {"type": "heading", "lvl": 2, "value": "C.3 Full COG results", "md": "## C.3 Full COG results"}, {"type": "table", "rows": [["", "Oracle", "Online", "MinR", "Naive + BC"], ["", "Ours", "Online + RND", "Naive", "BC + JSRL"]], "md": "| |Oracle|Online|MinR|Naive + BC|\n|---|---|---|---|---|\n| |Ours|Online + RND|Naive|BC + JSRL|", "isPerfectTable": true, "csv": "\"\",\"Oracle\",\"Online\",\"MinR\",\"Naive + BC\"\n\"\",\"Ours\",\"Online + RND\",\"Naive\",\"BC + JSRL\""}, {"type": "text", "value": "Figure 14: The performance on 3 COG tasks without ICVF.", "md": "Figure 14: The performance on 3 COG tasks without ICVF."}, {"type": "heading", "lvl": 2, "value": "C.4 Implicit-Q-learning (IQL) as the backbone RL algorithm", "md": "## C.4 Implicit-Q-learning (IQL) as the backbone RL algorithm"}, {"type": "text", "value": "Conceptually, our proposed method, ExPLORe, is not specific to the selected RL method, and can be combined with any RL methods that take in both offline and online transitions. Other than RLPD, we experimented with another RL algorithm, IQL [Kostrikov et al., 2022], in the offline to online fine-tuning setting where we run IQL on an online replay buffer prepended with the reward-relabelled offline data. In our experiments (Figure 15), we have found the use of the RND bonus in the offline data to be essential for accelerating learning in both medium datasets (without RND bonus, none of the runs succeed).\n\n23", "md": "Conceptually, our proposed method, ExPLORe, is not specific to the selected RL method, and can be combined with any RL methods that take in both offline and online transitions. Other than RLPD, we experimented with another RL algorithm, IQL [Kostrikov et al., 2022], in the offline to online fine-tuning setting where we run IQL on an online replay buffer prepended with the reward-relabelled offline data. In our experiments (Figure 15), we have found the use of the RND bonus in the offline data to be essential for accelerating learning in both medium datasets (without RND bonus, none of the runs succeed).\n\n23"}]}, {"page": 24, "text": "                                    Offline IQL Oracle            Ours           Naive\n   Normalized Return\n     1.00      umaze             1.00  umaze-diverse         1.00   medium-play           1.00 medium-diverse\n     0.75                        0.75                        0.75                         0.75\n     0.50                        0.50                        0.50                         0.50\n     0.25                        0.25                        0.25                         0.25\n     0.00                        0.00                        0.00                         0.00\n        0.000.250.500.751.00        0.000.250.500.751.00         0.000.250.500.751.00        0.000.250.500.751.00\nFigure 15: Results on 4 AntMaze datasets. The oracle offline IQL numbers are directly taken from\nthe original paper. It uses the offline dataset with ground truth reward. The other curves do not have\naccess to the ground truth reward in the offline dataset. 10 seeds are used for each curve.\nC.5     Pre-trained goal-conditioned (GC)-IQL skills.\nTo get a sense of how well skill-discovery may perform, we perform an additional empirical study in\nantmaze-large-diverse-v2 with a set of unsupervised, pre-trained skills. In particular, we\nspecify the skills to be a set of pre-trained goal-conditioned (GC) policies from the offline data where\nthe 2-D goal coordinate specifies the skill. We chose this more manually defined skill set with more\ninductive biases to demonstrate an approximate upper-bound on an approach that discovers skills in\nan unsupervised fashion.\nWe use the offline data to pre-train goal-conditioned policies using GC-IQL on the offline dataset. In\nthe online phase, we perform skill-inference on this set of skills by using RLPD to train a high-level\npolicy that outputs actions corresponding to 2D goal coordinate in the maze for the underlying skill\nto reach. While this combination leads to increased coverage during early exploration, we found\nthat skill inference using RLPD eventually gets stuck if reward is not found and ceases to explore\nfurther, unless additional online RND bonuses are applied. In Figure 16, we plot the performance of\nthis skill inference comparison (5 seeds for skill-inference, 3 for skill pre-training, and ten seeds for\nthe rest) \u2013 while inference in the space of skills leads to much faster exploration than with standard\nonline RND, we found the best variant of skill-discovery to be slightly worse than our method (GC-\nIQL+Online RLPD + Online RND vs. Ours).\nIn summary, we found that while skill-learning and online skill inference does lead to faster explo-\nration over online-only exploration, it still lags in performance compared to our approach. We note\nthat our method has several advantages over a skill-learning baseline, such as not needing a separate\npre-training phase, and not needing to specify or discover a separate skill set.\n                    Oracle           Online Only                  GC-IQL + Online RLPD\n                    Ours             Online Only + RND            GC-IQL + Online RLPD + Online RND\n                1.00    Normalized Score                                    Coverage\n                0.75                                         0.75\n                0.50                                         0.50\n                0.25                                         0.25\n                0.00  0.0        0.1       0.2        0.3                    0.1       0.2       0.3\nFigure 16: Results on AntMaze-Large-Diverse using pre-trained goal-conditioned IQL on offline\ndata. The x-axis is the number of environment steps \u00d7106. The y-axis is the normalized return (left)\nand the coverage percentage (right). The pre-trained IQL agent achieves around 50% success rate\nby using the correct goal.\n                                                          24", "md": "# OCR Text Results\n\n## Results on 4 AntMaze datasets\n\nFigure 15: Results on 4 AntMaze datasets. The oracle offline IQL numbers are directly taken from\nthe original paper. It uses the offline dataset with ground truth reward. The other curves do not have\naccess to the ground truth reward in the offline dataset. 10 seeds are used for each curve.\n\n| |Oracle Offline IQL|Ours|Naive|\n|---|---|---|---|\n|1.00|umaze|1.00 umaze-diverse|1.00 medium-play|1.00 medium-diverse|\n|0.75| |0.75| |0.75|\n|0.50| |0.50| |0.50|\n|0.25| |0.25| |0.25|\n|0.00| |0.00| |0.00|\n\n## Pre-trained goal-conditioned (GC)-IQL skills\n\nTo get a sense of how well skill-discovery may perform, we perform an additional empirical study in\nantmaze-large-diverse-v2 with a set of unsupervised, pre-trained skills. In particular, we\nspecify the skills to be a set of pre-trained goal-conditioned (GC) policies from the offline data where\nthe 2-D goal coordinate specifies the skill. We chose this more manually defined skill set with more\ninductive biases to demonstrate an approximate upper-bound on an approach that discovers skills in\nan unsupervised fashion.\n\nWe use the offline data to pre-train goal-conditioned policies using GC-IQL on the offline dataset. In\nthe online phase, we perform skill-inference on this set of skills by using RLPD to train a high-level\npolicy that outputs actions corresponding to 2D goal coordinate in the maze for the underlying skill\nto reach. While this combination leads to increased coverage during early exploration, we found\nthat skill inference using RLPD eventually gets stuck if reward is not found and ceases to explore\nfurther, unless additional online RND bonuses are applied.\n\nIn Figure 16, we plot the performance of this skill inference comparison (5 seeds for skill-inference, 3 for skill pre-training, and ten seeds for\nthe rest) \u2013 while inference in the space of skills leads to much faster exploration than with standard\nonline RND, we found the best variant of skill-discovery to be slightly worse than our method (GC-\nIQL+Online RLPD + Online RND vs. Ours).\n\nIn summary, we found that while skill-learning and online skill inference does lead to faster exploration over online-only exploration, it still lags in performance compared to our approach. We note\nthat our method has several advantages over a skill-learning baseline, such as not needing a separate\npre-training phase, and not needing to specify or discover a separate skill set.\n\n## Results on AntMaze-Large-Diverse using pre-trained goal-conditioned IQL on offline data\n\nFigure 16: Results on AntMaze-Large-Diverse using pre-trained goal-conditioned IQL on offline\ndata. The x-axis is the number of environment steps \u00d710^6. The y-axis is the normalized return (left)\nand the coverage percentage (right). The pre-trained IQL agent achieves around 50% success rate\nby using the correct goal.\n\n| |Oracle|Online Only|GC-IQL + Online RLPD|\n|---|---|---|---|\n|1.00|Normalized Score| |Coverage|\n|0.75| |0.75| |\n|0.50| |0.50| |\n|0.25| |0.25| |\n|0.00|0.0 0.1 0.2 0.3|0.1 0.2 0.3| |", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text Results", "md": "# OCR Text Results"}, {"type": "heading", "lvl": 2, "value": "Results on 4 AntMaze datasets", "md": "## Results on 4 AntMaze datasets"}, {"type": "text", "value": "Figure 15: Results on 4 AntMaze datasets. The oracle offline IQL numbers are directly taken from\nthe original paper. It uses the offline dataset with ground truth reward. The other curves do not have\naccess to the ground truth reward in the offline dataset. 10 seeds are used for each curve.", "md": "Figure 15: Results on 4 AntMaze datasets. The oracle offline IQL numbers are directly taken from\nthe original paper. It uses the offline dataset with ground truth reward. The other curves do not have\naccess to the ground truth reward in the offline dataset. 10 seeds are used for each curve."}, {"type": "table", "rows": [["", "Oracle Offline IQL", "Ours", "Naive"], ["1.00", "umaze", "1.00 umaze-diverse", "1.00 medium-play", "1.00 medium-diverse"], ["0.75", "", "0.75", "", "0.75"], ["0.50", "", "0.50", "", "0.50"], ["0.25", "", "0.25", "", "0.25"], ["0.00", "", "0.00", "", "0.00"]], "md": "| |Oracle Offline IQL|Ours|Naive|\n|---|---|---|---|\n|1.00|umaze|1.00 umaze-diverse|1.00 medium-play|1.00 medium-diverse|\n|0.75| |0.75| |0.75|\n|0.50| |0.50| |0.50|\n|0.25| |0.25| |0.25|\n|0.00| |0.00| |0.00|", "isPerfectTable": false, "csv": "\"\",\"Oracle Offline IQL\",\"Ours\",\"Naive\"\n\"1.00\",\"umaze\",\"1.00 umaze-diverse\",\"1.00 medium-play\",\"1.00 medium-diverse\"\n\"0.75\",\"\",\"0.75\",\"\",\"0.75\"\n\"0.50\",\"\",\"0.50\",\"\",\"0.50\"\n\"0.25\",\"\",\"0.25\",\"\",\"0.25\"\n\"0.00\",\"\",\"0.00\",\"\",\"0.00\""}, {"type": "heading", "lvl": 2, "value": "Pre-trained goal-conditioned (GC)-IQL skills", "md": "## Pre-trained goal-conditioned (GC)-IQL skills"}, {"type": "text", "value": "To get a sense of how well skill-discovery may perform, we perform an additional empirical study in\nantmaze-large-diverse-v2 with a set of unsupervised, pre-trained skills. In particular, we\nspecify the skills to be a set of pre-trained goal-conditioned (GC) policies from the offline data where\nthe 2-D goal coordinate specifies the skill. We chose this more manually defined skill set with more\ninductive biases to demonstrate an approximate upper-bound on an approach that discovers skills in\nan unsupervised fashion.\n\nWe use the offline data to pre-train goal-conditioned policies using GC-IQL on the offline dataset. In\nthe online phase, we perform skill-inference on this set of skills by using RLPD to train a high-level\npolicy that outputs actions corresponding to 2D goal coordinate in the maze for the underlying skill\nto reach. While this combination leads to increased coverage during early exploration, we found\nthat skill inference using RLPD eventually gets stuck if reward is not found and ceases to explore\nfurther, unless additional online RND bonuses are applied.\n\nIn Figure 16, we plot the performance of this skill inference comparison (5 seeds for skill-inference, 3 for skill pre-training, and ten seeds for\nthe rest) \u2013 while inference in the space of skills leads to much faster exploration than with standard\nonline RND, we found the best variant of skill-discovery to be slightly worse than our method (GC-\nIQL+Online RLPD + Online RND vs. Ours).\n\nIn summary, we found that while skill-learning and online skill inference does lead to faster exploration over online-only exploration, it still lags in performance compared to our approach. We note\nthat our method has several advantages over a skill-learning baseline, such as not needing a separate\npre-training phase, and not needing to specify or discover a separate skill set.", "md": "To get a sense of how well skill-discovery may perform, we perform an additional empirical study in\nantmaze-large-diverse-v2 with a set of unsupervised, pre-trained skills. In particular, we\nspecify the skills to be a set of pre-trained goal-conditioned (GC) policies from the offline data where\nthe 2-D goal coordinate specifies the skill. We chose this more manually defined skill set with more\ninductive biases to demonstrate an approximate upper-bound on an approach that discovers skills in\nan unsupervised fashion.\n\nWe use the offline data to pre-train goal-conditioned policies using GC-IQL on the offline dataset. In\nthe online phase, we perform skill-inference on this set of skills by using RLPD to train a high-level\npolicy that outputs actions corresponding to 2D goal coordinate in the maze for the underlying skill\nto reach. While this combination leads to increased coverage during early exploration, we found\nthat skill inference using RLPD eventually gets stuck if reward is not found and ceases to explore\nfurther, unless additional online RND bonuses are applied.\n\nIn Figure 16, we plot the performance of this skill inference comparison (5 seeds for skill-inference, 3 for skill pre-training, and ten seeds for\nthe rest) \u2013 while inference in the space of skills leads to much faster exploration than with standard\nonline RND, we found the best variant of skill-discovery to be slightly worse than our method (GC-\nIQL+Online RLPD + Online RND vs. Ours).\n\nIn summary, we found that while skill-learning and online skill inference does lead to faster exploration over online-only exploration, it still lags in performance compared to our approach. We note\nthat our method has several advantages over a skill-learning baseline, such as not needing a separate\npre-training phase, and not needing to specify or discover a separate skill set."}, {"type": "heading", "lvl": 2, "value": "Results on AntMaze-Large-Diverse using pre-trained goal-conditioned IQL on offline data", "md": "## Results on AntMaze-Large-Diverse using pre-trained goal-conditioned IQL on offline data"}, {"type": "text", "value": "Figure 16: Results on AntMaze-Large-Diverse using pre-trained goal-conditioned IQL on offline\ndata. The x-axis is the number of environment steps \u00d710^6. The y-axis is the normalized return (left)\nand the coverage percentage (right). The pre-trained IQL agent achieves around 50% success rate\nby using the correct goal.", "md": "Figure 16: Results on AntMaze-Large-Diverse using pre-trained goal-conditioned IQL on offline\ndata. The x-axis is the number of environment steps \u00d710^6. The y-axis is the normalized return (left)\nand the coverage percentage (right). The pre-trained IQL agent achieves around 50% success rate\nby using the correct goal."}, {"type": "table", "rows": [["", "Oracle", "Online Only", "GC-IQL + Online RLPD"], ["1.00", "Normalized Score", "", "Coverage"], ["0.75", "", "0.75", ""], ["0.50", "", "0.50", ""], ["0.25", "", "0.25", ""], ["0.00", "0.0 0.1 0.2 0.3", "0.1 0.2 0.3", ""]], "md": "| |Oracle|Online Only|GC-IQL + Online RLPD|\n|---|---|---|---|\n|1.00|Normalized Score| |Coverage|\n|0.75| |0.75| |\n|0.50| |0.50| |\n|0.25| |0.25| |\n|0.00|0.0 0.1 0.2 0.3|0.1 0.2 0.3| |", "isPerfectTable": true, "csv": "\"\",\"Oracle\",\"Online Only\",\"GC-IQL + Online RLPD\"\n\"1.00\",\"Normalized Score\",\"\",\"Coverage\"\n\"0.75\",\"\",\"0.75\",\"\"\n\"0.50\",\"\",\"0.50\",\"\"\n\"0.25\",\"\",\"0.25\",\"\"\n\"0.00\",\"0.0 0.1 0.2 0.3\",\"0.1 0.2 0.3\",\"\""}]}, {"page": 25, "text": "D          Visualization Details for Figure 6\nSince the prior data on COG contains no full trajectories that complete the task, to generate Fig-\nure 6, a successful trajectory on the Grasp from Closed Drawer environment was made by\ncombining a successful trajectory opening the drawer and a successful trajectory taking the object\nout of the drawer from the prior dataset. Afterward, we evaluate the reward model on this successful\ntrajectory for 20 training seeds and normalize each to have zero mean and unit variance.\nBelow, we provide the same visualization as in Figure 6, but on other successful trajectories com-\nbined from the prior data on Grasp from Closed Drawer and observe a similar effect.\n             Normalized Reward             At 25k Environment Steps                                            Normalized Reward              At 25k Environment Steps\n                    Ours + ICVF                                                                                       Ours + ICVF\n                    Ours                                                                                              Ours\n                                                   Trajectory Steps                                                                                  Trajectory Steps\n                 0        5        10       15       20       25        30       35       40       45              0       5       10      15       20      25      30     35       40      45      50\n             Normalized Reward             At 25k Environment Steps                                            Normalized Reward              At 25k Environment Steps\n                    Ours + ICVF                                                                                                                                                                Ours + ICVF\n                    Ours                                                                                                                                                                       Ours\n                                                   Trajectory Steps                                                                                  Trajectory Steps\n                 0     5     10     15    20    25     30    35     40    45    50     55    60     65             0      5      10      15     20     25      30     35     40     45      50     55\n             Normalized Reward             At 25k Environment Steps                                            Normalized Reward              At 25k Environment Steps\n                    Ours + ICVF                                                                                       Ours + ICVF\n                    Ours                                                                                              Ours\n                                                   Trajectory Steps                                                                                  Trajectory Steps\n                 0     5     10    15    20    25    30    35    40     45    50    55    60    65                 0     5     10    15     20    25    30     35    40   45     50    55    60     65\n             Normalized Reward             At 25k Environment Steps                                            Normalized Reward              At 25k Environment Steps\n                                                                                              Ours + ICVF             Ours + ICVF\n                                                                                              Ours                    Ours\n                                                   Trajectory Steps                                                                                  Trajectory Steps\n                 0     5    10   15    20    25    30   35    40    45    50   55    60    65   70                 0      5      10     15     20     25     30      35    40     45     50     55\n            Normalized Reward              At 25k Environment Steps                                           Normalized Reward              At 25k Environment Steps\n                                                                                             Ours + ICVF             Ours + ICVF\n                                                                                             Ours                    Ours\n                                                   Trajectory Steps                                                                                  Trajectory Steps\n                0       5     10     15      20     25     30      35     40     45      50     55                0       5     10     15     20      25     30     35    40     45     50      55\nE          Compute Description\nWe use Tesla V100 GPU for running the experiments. Each AntMaze experiment (300,000 envi-\nronment steps) takes around 2 GPU hours. Each Adroit experiment (1 million environment steps)\ntakes around 6 GPU hours. Each COG experiment (250,000 environment steps) takes around 3\nGPU hours. To reproduce all the results in the main body of the paper, it takes around 10 (seeds) \u00d7\n6 (tasks) \u00d7 6 (methods) \u00d7 2 (AntMaze) + 10 (seeds) \u00d7 3 (tasks) \u00d7 6 (methods) \u00d7 6 (Adroit) + 20\n(seeds) \u00d7 3 (tasks) \u00d7 6 (method) \u00d7 3 (COG) = 2880 GPU hours.\n                                                                                                          25", "md": "## Visualization Details for Figure 6\n\nSince the prior data on COG contains no full trajectories that complete the task, to generate Figure 6, a successful trajectory on the Grasp from Closed Drawer environment was made by combining a successful trajectory opening the drawer and a successful trajectory taking the object out of the drawer from the prior dataset. Afterward, we evaluate the reward model on this successful trajectory for 20 training seeds and normalize each to have zero mean and unit variance.\n\nBelow, we provide the same visualization as in Figure 6, but on other successful trajectories combined from the prior data on Grasp from Closed Drawer and observe a similar effect.\n\n|Normalized Reward at 25k Environment Steps|Normalized Reward at 25k Environment Steps|\n|---|---|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55|\n\n## Compute Description\n\nWe use Tesla V100 GPU for running the experiments. Each AntMaze experiment (300,000 environment steps) takes around 2 GPU hours. Each Adroit experiment (1 million environment steps) takes around 6 GPU hours. Each COG experiment (250,000 environment steps) takes around 3 GPU hours. To reproduce all the results in the main body of the paper, it takes around 10 (seeds) \u00d7 6 (tasks) \u00d7 6 (methods) \u00d7 2 (AntMaze) + 10 (seeds) \u00d7 3 (tasks) \u00d7 6 (methods) \u00d7 6 (Adroit) + 20 (seeds) \u00d7 3 (tasks) \u00d7 6 (method) \u00d7 3 (COG) = 2880 GPU hours.", "images": [{"name": "page-25-23.jpg", "height": 13, "width": 27, "x": 328, "y": 297}, {"name": "page-25-21.jpg", "height": 13, "width": 167, "x": 133, "y": 297}, {"name": "page-25-25.jpg", "height": 13, "width": 27, "x": 382, "y": 297}, {"name": "page-25-24.jpg", "height": 13, "width": 27, "x": 355, "y": 297}, {"name": "page-25-27.jpg", "height": 13, "width": 27, "x": 423, "y": 297}, {"name": "page-25-28.jpg", "height": 13, "width": 27, "x": 450, "y": 297}, {"name": "page-25-29.jpg", "height": 13, "width": 160, "x": 133, "y": 358}, {"name": "page-25-33.jpg", "height": 13, "width": 105, "x": 367, "y": 418}, {"name": "page-25-32.jpg", "height": 13, "width": 53, "x": 314, "y": 418}, {"name": "page-25-36.jpg", "height": 13, "width": 40, "x": 199, "y": 479}, {"name": "page-25-30.jpg", "height": 13, "width": 164, "x": 314, "y": 358}, {"name": "page-25-31.jpg", "height": 13, "width": 161, "x": 133, "y": 418}, {"name": "page-25-34.jpg", "height": 13, "width": 27, "x": 132, "y": 479}, {"name": "page-25-35.jpg", "height": 13, "width": 40, "x": 159, "y": 479}, {"name": "page-25-37.jpg", "height": 13, "width": 40, "x": 239, "y": 479}, {"name": "page-25-39.jpg", "height": 13, "width": 92, "x": 313, "y": 479}, {"name": "page-25-40.jpg", "height": 13, "width": 66, "x": 405, "y": 479}], "items": [{"type": "heading", "lvl": 2, "value": "Visualization Details for Figure 6", "md": "## Visualization Details for Figure 6"}, {"type": "text", "value": "Since the prior data on COG contains no full trajectories that complete the task, to generate Figure 6, a successful trajectory on the Grasp from Closed Drawer environment was made by combining a successful trajectory opening the drawer and a successful trajectory taking the object out of the drawer from the prior dataset. Afterward, we evaluate the reward model on this successful trajectory for 20 training seeds and normalize each to have zero mean and unit variance.\n\nBelow, we provide the same visualization as in Figure 6, but on other successful trajectories combined from the prior data on Grasp from Closed Drawer and observe a similar effect.", "md": "Since the prior data on COG contains no full trajectories that complete the task, to generate Figure 6, a successful trajectory on the Grasp from Closed Drawer environment was made by combining a successful trajectory opening the drawer and a successful trajectory taking the object out of the drawer from the prior dataset. Afterward, we evaluate the reward model on this successful trajectory for 20 training seeds and normalize each to have zero mean and unit variance.\n\nBelow, we provide the same visualization as in Figure 6, but on other successful trajectories combined from the prior data on Grasp from Closed Drawer and observe a similar effect."}, {"type": "table", "rows": [["Normalized Reward at 25k Environment Steps", "Normalized Reward at 25k Environment Steps"], ["Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45", "Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50"], ["Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65", "Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55"], ["Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65", "Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65"], ["Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70", "Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55"], ["Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55", "Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55"]], "md": "|Normalized Reward at 25k Environment Steps|Normalized Reward at 25k Environment Steps|\n|---|---|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55|\n|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55|Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55|", "isPerfectTable": true, "csv": "\"Normalized Reward at 25k Environment Steps\",\"Normalized Reward at 25k Environment Steps\"\n\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45\",\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50\"\n\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65\",\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55\"\n\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65\",\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65\"\n\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70\",\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55\"\n\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55\",\"Ours + ICVF Ours Trajectory Steps 0 5 10 15 20 25 30 35 40 45 50 55\""}, {"type": "heading", "lvl": 2, "value": "Compute Description", "md": "## Compute Description"}, {"type": "text", "value": "We use Tesla V100 GPU for running the experiments. Each AntMaze experiment (300,000 environment steps) takes around 2 GPU hours. Each Adroit experiment (1 million environment steps) takes around 6 GPU hours. Each COG experiment (250,000 environment steps) takes around 3 GPU hours. To reproduce all the results in the main body of the paper, it takes around 10 (seeds) \u00d7 6 (tasks) \u00d7 6 (methods) \u00d7 2 (AntMaze) + 10 (seeds) \u00d7 3 (tasks) \u00d7 6 (methods) \u00d7 6 (Adroit) + 20 (seeds) \u00d7 3 (tasks) \u00d7 6 (method) \u00d7 3 (COG) = 2880 GPU hours.", "md": "We use Tesla V100 GPU for running the experiments. Each AntMaze experiment (300,000 environment steps) takes around 2 GPU hours. Each Adroit experiment (1 million environment steps) takes around 6 GPU hours. Each COG experiment (250,000 environment steps) takes around 3 GPU hours. To reproduce all the results in the main body of the paper, it takes around 10 (seeds) \u00d7 6 (tasks) \u00d7 6 (methods) \u00d7 2 (AntMaze) + 10 (seeds) \u00d7 3 (tasks) \u00d7 6 (methods) \u00d7 6 (Adroit) + 20 (seeds) \u00d7 3 (tasks) \u00d7 6 (method) \u00d7 3 (COG) = 2880 GPU hours."}]}], "job_id": "1df0ab25-98a8-4cdd-9fcf-f25b2cb042c3", "file_path": "./corpus/2311.05067.pdf"}