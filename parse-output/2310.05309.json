{"pages": [{"page": 1, "text": "         Optimizing Solution-Samplers for Combinatorial Problems:\n                       The Landscape of Policy-Gradient Methods\n                   Constantine Caramanis*                  Dimitris Fotakis \u2020           Alkis Kalavasis \u2021\n                University of Texas at Austin                     NTUA                   Yale University\narXiv:2310.05309v2  [cs.LG]  7 Nov 2023\n                      Vasilis Kontonis \u00a7                                 Christos Tzamos\u00b6\n               University of Texas at Austin              UOA & University of Wisconsin-Madison\n                                                   November 8, 2023\n                                                          Abstract\n                  Deep Neural Networks and Reinforcement Learning methods have empirically shown\n              great promise in tackling challenging combinatorial problems. In those methods a deep neu-\n              ral network is used as a solution generator which is then trained by gradient-based methods\n              (e.g., policy gradient) to successively obtain better solution distributions. In this work we\n              introduce a novel theoretical framework for analyzing the effectiveness of such methods. We\n              ask whether there exist generative models that (i) are expressive enough to generate approxi-\n              mately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number\n              of parameters; (iii) their optimization landscape is benign in the sense that it does not contain\n              sub-optimal stationary points. Our main contribution is a positive answer to this question.\n              Our result holds for a broad class of combinatorial problems including Max- and Min-Cut,\n              Max-k-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. As a\n              byproduct of our analysis we introduce a novel regularization process over vanilla gradient\n              descent and provide theoretical and experimental evidence that it helps address vanishing-\n              gradient issues and escape bad stationary points.\n        1    Introduction\n        Gradient descent has proven remarkably effective for diverse optimization problems in neural\n        networks. From the early days of neural networks, this has motivated their use for combinatorial\n        optimization [HT85, Smi99, VFJ15, BPL+16]. More recently, an approach by [BPL+16], where\n        a neural network is used to generate (sample) solutions for the combinatorial problem. The\n        parameters of the neural network thus parameterize the space of distributions. This allows one\n        to perform gradient steps in this distribution space. In several interesting settings, including\n           *\n           constantine@utexas.edu\n           \u2020\n           fotakis@cs.ntua.gr\n           \u2021\n           alvertos.kalavasis@yale.edu\n           \u00a7\n          \u00b6vkonton@gmail.com\n           tzamos@wisc.edu\n                                                              1", "md": "# Optimizing Solution-Samplers for Combinatorial Problems\n\n# Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods\n\nConstantine Caramanis* - University of Texas at Austin\n\nDimitris Fotakis \u2020 - NTUA\n\nAlkis Kalavasis \u2021 - Yale University\n\nVasilis Kontonis \u00a7 - University of Texas at Austin\n\nChristos Tzamos\u00b6 - UOA & University of Wisconsin-Madison\n\narXiv:2310.05309v2 [cs.LG] 7 Nov 2023\n\nNovember 8, 2023\n\n## Abstract\n\nDeep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. As a byproduct of our analysis we introduce a novel regularization process over vanilla gradient descent and provide theoretical and experimental evidence that it helps address vanishing-gradient issues and escape bad stationary points.\n\n### Introduction\n\nGradient descent has proven remarkably effective for diverse optimization problems in neural networks. From the early days of neural networks, this has motivated their use for combinatorial optimization [HT85, Smi99, VFJ15, BPL+16]. More recently, an approach by [BPL+16], where a neural network is used to generate (sample) solutions for the combinatorial problem. The parameters of the neural network thus parameterize the space of distributions. This allows one to perform gradient steps in this distribution space. In several interesting settings, including", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Optimizing Solution-Samplers for Combinatorial Problems", "md": "# Optimizing Solution-Samplers for Combinatorial Problems"}, {"type": "heading", "lvl": 1, "value": "Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods", "md": "# Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods"}, {"type": "text", "value": "Constantine Caramanis* - University of Texas at Austin\n\nDimitris Fotakis \u2020 - NTUA\n\nAlkis Kalavasis \u2021 - Yale University\n\nVasilis Kontonis \u00a7 - University of Texas at Austin\n\nChristos Tzamos\u00b6 - UOA & University of Wisconsin-Madison\n\narXiv:2310.05309v2 [cs.LG] 7 Nov 2023\n\nNovember 8, 2023", "md": "Constantine Caramanis* - University of Texas at Austin\n\nDimitris Fotakis \u2020 - NTUA\n\nAlkis Kalavasis \u2021 - Yale University\n\nVasilis Kontonis \u00a7 - University of Texas at Austin\n\nChristos Tzamos\u00b6 - UOA & University of Wisconsin-Madison\n\narXiv:2310.05309v2 [cs.LG] 7 Nov 2023\n\nNovember 8, 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. As a byproduct of our analysis we introduce a novel regularization process over vanilla gradient descent and provide theoretical and experimental evidence that it helps address vanishing-gradient issues and escape bad stationary points.", "md": "Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. As a byproduct of our analysis we introduce a novel regularization process over vanilla gradient descent and provide theoretical and experimental evidence that it helps address vanishing-gradient issues and escape bad stationary points."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "Gradient descent has proven remarkably effective for diverse optimization problems in neural networks. From the early days of neural networks, this has motivated their use for combinatorial optimization [HT85, Smi99, VFJ15, BPL+16]. More recently, an approach by [BPL+16], where a neural network is used to generate (sample) solutions for the combinatorial problem. The parameters of the neural network thus parameterize the space of distributions. This allows one to perform gradient steps in this distribution space. In several interesting settings, including", "md": "Gradient descent has proven remarkably effective for diverse optimization problems in neural networks. From the early days of neural networks, this has motivated their use for combinatorial optimization [HT85, Smi99, VFJ15, BPL+16]. More recently, an approach by [BPL+16], where a neural network is used to generate (sample) solutions for the combinatorial problem. The parameters of the neural network thus parameterize the space of distributions. This allows one to perform gradient steps in this distribution space. In several interesting settings, including"}]}, {"page": 2, "text": "the Traveling Salesman Problem, they have shown that this approach works remarkably well.\nGiven the widespread application but also the notorious difficulty of combinatorial optimization\n[GLS12, PS98, S+03, Sch05, CLS+95], approaches that provide a more general solution framework\nare appealing.\n    This is the point of departure of this paper. We investigate whether gradient descent can\nsucceed in a general setting that encompasses the problems studied in [BPL+16]. This requires a\nparameterization of distributions over solutions with a \u201cnice\u201d optimization landscape (intuitively,\nthat gradient descent does not get stuck in local minima or points of vanishing gradient) and that\nhas a polynomial number of parameters. Satisfying both requirements simultaneously is non-\ntrivial. As we show precisely below, a simple lifting to the exponential-size probability simplex\non all solutions guarantees convexity; and, on the other hand, compressed parameterizations with\n\u201cbad\u201d optimization landscapes are also easy to come by (we give a natural example for Max-\nCut in Remark 1). Hence, we seek to understand the parametric complexity of gradient-based\nmethods, i.e., how many parameters suffice for a benign optimization landscape in the sense that\nit does not contain \u201cbad\u201d stationary points.\n    We thus theoretically investigate whether there exist solution generators with a tractable\nnumber of parameters that are also efficiently optimizable, i.e., gradient descent requires a small\nnumber of steps to reach a near-optimal solution. We provide a positive answer under general\nassumptions and specialize our results for several classes of hard and easy combinatorial opti-\nmization problems, including Max-Cut and Min-Cut, Max-k-CSP, Maximum-Weighted-Bipartite-\nMatching and Traveling Salesman. We remark that a key difference between (computationally)\neasy and hard problems is not the ability to find a compressed and efficiently optimizable genera-\ntive model but rather the ability to efficiently draw samples from the parameterized distributions.\n1.1   Our Framework\nWe introduce a theoretical framework for analyzing the effectiveness of gradient-based methods\non the optimization of solution generators in combinatorial optimization, inspired by [BPL+16].\n    Let I be a collection of instances of a combinatorial problem with common solution space\nS and L(\u00b7; I) : S \u2192    R be the cost function associated with an instance I \u2208         I, i.e., L(s; I) is the\ncost of solution s given the instance I. For example, for the Max-Cut problem the collection of\ninstances I corresponds to all graphs with n nodes, the solution space S consists of all subsets of\nnodes, and the loss L(s; I) is equal to (minus) the weight of the cut (s, [n] \\ s) corresponding to\nthe subset of nodes s \u2208   S (our goal is to minimize L).\nDefinition 1 (Solution Cost Oracle). For a given instance I we assume that we have access to an oracle\nO(\u00b7; I) to the cost of any given solution s \u2208 S, i.e., O(s; I) = L(s; I).\n    The above oracle is standard in combinatorial optimization and query-efficient algorithms are\nprovided for various problems [RSW17, GPRW19, LSZ21, AEG+22, PRW22]. We remark that the\ngoal of this work is not to design algorithms that solve combinatorial problems using access to the\nsolution cost oracle (as the aforementioned works do). This paper focuses on landscape design:\nthe algorithm is fixed, namely (stochastic) gradient descent; the question is how to design a\ngenerative model that has a small number of parameters and the induced optimization landscape\nallows gradient-based methods to converge to the optimal solution without getting trapped at\nlocal minima or vanishing gradient points.\n                                                     2", "md": "the Traveling Salesman Problem, they have shown that this approach works remarkably well.\nGiven the widespread application but also the notorious difficulty of combinatorial optimization\n[GLS12, PS98, S+03, Sch05, CLS+95], approaches that provide a more general solution framework\nare appealing.\n\nThis is the point of departure of this paper. We investigate whether gradient descent can\nsucceed in a general setting that encompasses the problems studied in [BPL+16]. This requires a\nparameterization of distributions over solutions with a \u201cnice\u201d optimization landscape (intuitively,\nthat gradient descent does not get stuck in local minima or points of vanishing gradient) and that\nhas a polynomial number of parameters. Satisfying both requirements simultaneously is non-\ntrivial. As we show precisely below, a simple lifting to the exponential-size probability simplex\non all solutions guarantees convexity; and, on the other hand, compressed parameterizations with\n\u201cbad\u201d optimization landscapes are also easy to come by (we give a natural example for Max-\nCut in Remark 1). Hence, we seek to understand the parametric complexity of gradient-based\nmethods, i.e., how many parameters suffice for a benign optimization landscape in the sense that\nit does not contain \u201cbad\u201d stationary points.\n\nWe thus theoretically investigate whether there exist solution generators with a tractable\nnumber of parameters that are also efficiently optimizable, i.e., gradient descent requires a small\nnumber of steps to reach a near-optimal solution. We provide a positive answer under general\nassumptions and specialize our results for several classes of hard and easy combinatorial opti-\nmization problems, including Max-Cut and Min-Cut, Max-k-CSP, Maximum-Weighted-Bipartite-\nMatching and Traveling Salesman. We remark that a key difference between (computationally)\neasy and hard problems is not the ability to find a compressed and efficiently optimizable genera-\ntive model but rather the ability to efficiently draw samples from the parameterized distributions.\n\n## Our Framework\n\nWe introduce a theoretical framework for analyzing the effectiveness of gradient-based methods\non the optimization of solution generators in combinatorial optimization, inspired by [BPL+16].\n\nLet \\( I \\) be a collection of instances of a combinatorial problem with common solution space\n\\( S \\) and \\( L(\\cdot; I) : S \\rightarrow \\mathbb{R} \\) be the cost function associated with an instance \\( I \\in I \\), i.e., \\( L(s; I) \\) is the\ncost of solution \\( s \\) given the instance \\( I \\). For example, for the Max-Cut problem the collection of\ninstances \\( I \\) corresponds to all graphs with \\( n \\) nodes, the solution space \\( S \\) consists of all subsets of\nnodes, and the loss \\( L(s; I) \\) is equal to (minus) the weight of the cut \\( (s, [n] \\backslash s) \\) corresponding to\nthe subset of nodes \\( s \\in S \\) (our goal is to minimize \\( L \\)).\n\n### Definition 1 (Solution Cost Oracle)\n\nFor a given instance \\( I \\) we assume that we have access to an oracle\n\\( O(\\cdot; I) \\) to the cost of any given solution \\( s \\in S \\), i.e., \\( O(s; I) = L(s; I) \\).\n\nThe above oracle is standard in combinatorial optimization and query-efficient algorithms are\nprovided for various problems [RSW17, GPRW19, LSZ21, AEG+22, PRW22]. We remark that the\ngoal of this work is not to design algorithms that solve combinatorial problems using access to the\nsolution cost oracle (as the aforementioned works do). This paper focuses on landscape design:\nthe algorithm is fixed, namely (stochastic) gradient descent; the question is how to design a\ngenerative model that has a small number of parameters and the induced optimization landscape\nallows gradient-based methods to converge to the optimal solution without getting trapped at\nlocal minima or vanishing gradient points.", "images": [], "items": [{"type": "text", "value": "the Traveling Salesman Problem, they have shown that this approach works remarkably well.\nGiven the widespread application but also the notorious difficulty of combinatorial optimization\n[GLS12, PS98, S+03, Sch05, CLS+95], approaches that provide a more general solution framework\nare appealing.\n\nThis is the point of departure of this paper. We investigate whether gradient descent can\nsucceed in a general setting that encompasses the problems studied in [BPL+16]. This requires a\nparameterization of distributions over solutions with a \u201cnice\u201d optimization landscape (intuitively,\nthat gradient descent does not get stuck in local minima or points of vanishing gradient) and that\nhas a polynomial number of parameters. Satisfying both requirements simultaneously is non-\ntrivial. As we show precisely below, a simple lifting to the exponential-size probability simplex\non all solutions guarantees convexity; and, on the other hand, compressed parameterizations with\n\u201cbad\u201d optimization landscapes are also easy to come by (we give a natural example for Max-\nCut in Remark 1). Hence, we seek to understand the parametric complexity of gradient-based\nmethods, i.e., how many parameters suffice for a benign optimization landscape in the sense that\nit does not contain \u201cbad\u201d stationary points.\n\nWe thus theoretically investigate whether there exist solution generators with a tractable\nnumber of parameters that are also efficiently optimizable, i.e., gradient descent requires a small\nnumber of steps to reach a near-optimal solution. We provide a positive answer under general\nassumptions and specialize our results for several classes of hard and easy combinatorial opti-\nmization problems, including Max-Cut and Min-Cut, Max-k-CSP, Maximum-Weighted-Bipartite-\nMatching and Traveling Salesman. We remark that a key difference between (computationally)\neasy and hard problems is not the ability to find a compressed and efficiently optimizable genera-\ntive model but rather the ability to efficiently draw samples from the parameterized distributions.", "md": "the Traveling Salesman Problem, they have shown that this approach works remarkably well.\nGiven the widespread application but also the notorious difficulty of combinatorial optimization\n[GLS12, PS98, S+03, Sch05, CLS+95], approaches that provide a more general solution framework\nare appealing.\n\nThis is the point of departure of this paper. We investigate whether gradient descent can\nsucceed in a general setting that encompasses the problems studied in [BPL+16]. This requires a\nparameterization of distributions over solutions with a \u201cnice\u201d optimization landscape (intuitively,\nthat gradient descent does not get stuck in local minima or points of vanishing gradient) and that\nhas a polynomial number of parameters. Satisfying both requirements simultaneously is non-\ntrivial. As we show precisely below, a simple lifting to the exponential-size probability simplex\non all solutions guarantees convexity; and, on the other hand, compressed parameterizations with\n\u201cbad\u201d optimization landscapes are also easy to come by (we give a natural example for Max-\nCut in Remark 1). Hence, we seek to understand the parametric complexity of gradient-based\nmethods, i.e., how many parameters suffice for a benign optimization landscape in the sense that\nit does not contain \u201cbad\u201d stationary points.\n\nWe thus theoretically investigate whether there exist solution generators with a tractable\nnumber of parameters that are also efficiently optimizable, i.e., gradient descent requires a small\nnumber of steps to reach a near-optimal solution. We provide a positive answer under general\nassumptions and specialize our results for several classes of hard and easy combinatorial opti-\nmization problems, including Max-Cut and Min-Cut, Max-k-CSP, Maximum-Weighted-Bipartite-\nMatching and Traveling Salesman. We remark that a key difference between (computationally)\neasy and hard problems is not the ability to find a compressed and efficiently optimizable genera-\ntive model but rather the ability to efficiently draw samples from the parameterized distributions."}, {"type": "heading", "lvl": 2, "value": "Our Framework", "md": "## Our Framework"}, {"type": "text", "value": "We introduce a theoretical framework for analyzing the effectiveness of gradient-based methods\non the optimization of solution generators in combinatorial optimization, inspired by [BPL+16].\n\nLet \\( I \\) be a collection of instances of a combinatorial problem with common solution space\n\\( S \\) and \\( L(\\cdot; I) : S \\rightarrow \\mathbb{R} \\) be the cost function associated with an instance \\( I \\in I \\), i.e., \\( L(s; I) \\) is the\ncost of solution \\( s \\) given the instance \\( I \\). For example, for the Max-Cut problem the collection of\ninstances \\( I \\) corresponds to all graphs with \\( n \\) nodes, the solution space \\( S \\) consists of all subsets of\nnodes, and the loss \\( L(s; I) \\) is equal to (minus) the weight of the cut \\( (s, [n] \\backslash s) \\) corresponding to\nthe subset of nodes \\( s \\in S \\) (our goal is to minimize \\( L \\)).", "md": "We introduce a theoretical framework for analyzing the effectiveness of gradient-based methods\non the optimization of solution generators in combinatorial optimization, inspired by [BPL+16].\n\nLet \\( I \\) be a collection of instances of a combinatorial problem with common solution space\n\\( S \\) and \\( L(\\cdot; I) : S \\rightarrow \\mathbb{R} \\) be the cost function associated with an instance \\( I \\in I \\), i.e., \\( L(s; I) \\) is the\ncost of solution \\( s \\) given the instance \\( I \\). For example, for the Max-Cut problem the collection of\ninstances \\( I \\) corresponds to all graphs with \\( n \\) nodes, the solution space \\( S \\) consists of all subsets of\nnodes, and the loss \\( L(s; I) \\) is equal to (minus) the weight of the cut \\( (s, [n] \\backslash s) \\) corresponding to\nthe subset of nodes \\( s \\in S \\) (our goal is to minimize \\( L \\))."}, {"type": "heading", "lvl": 3, "value": "Definition 1 (Solution Cost Oracle)", "md": "### Definition 1 (Solution Cost Oracle)"}, {"type": "text", "value": "For a given instance \\( I \\) we assume that we have access to an oracle\n\\( O(\\cdot; I) \\) to the cost of any given solution \\( s \\in S \\), i.e., \\( O(s; I) = L(s; I) \\).\n\nThe above oracle is standard in combinatorial optimization and query-efficient algorithms are\nprovided for various problems [RSW17, GPRW19, LSZ21, AEG+22, PRW22]. We remark that the\ngoal of this work is not to design algorithms that solve combinatorial problems using access to the\nsolution cost oracle (as the aforementioned works do). This paper focuses on landscape design:\nthe algorithm is fixed, namely (stochastic) gradient descent; the question is how to design a\ngenerative model that has a small number of parameters and the induced optimization landscape\nallows gradient-based methods to converge to the optimal solution without getting trapped at\nlocal minima or vanishing gradient points.", "md": "For a given instance \\( I \\) we assume that we have access to an oracle\n\\( O(\\cdot; I) \\) to the cost of any given solution \\( s \\in S \\), i.e., \\( O(s; I) = L(s; I) \\).\n\nThe above oracle is standard in combinatorial optimization and query-efficient algorithms are\nprovided for various problems [RSW17, GPRW19, LSZ21, AEG+22, PRW22]. We remark that the\ngoal of this work is not to design algorithms that solve combinatorial problems using access to the\nsolution cost oracle (as the aforementioned works do). This paper focuses on landscape design:\nthe algorithm is fixed, namely (stochastic) gradient descent; the question is how to design a\ngenerative model that has a small number of parameters and the induced optimization landscape\nallows gradient-based methods to converge to the optimal solution without getting trapped at\nlocal minima or vanishing gradient points."}]}, {"page": 3, "text": "       Let R be some prior distribution over the instance space I and W be the space of parameters\nof the model.               We now define the class of solution generators.                                             The solution generator p(w)\nwith parameter w \u2208                     W takes as input an instance I and generates a random solution s in S.\nTo distinguish between the output, the input, and the parameter of the solution generator, we\nuse the notation p(\u00b7; I; w) to denote the distribution over solutions and p(s; I; w) to denote the\nprobability of an individual solution s \u2208                                    S. We denote by P = {p(w) : w \u2208                                      W} the above\nparametric class of solution generators. For some parameter w, the loss corresponding to the\nsolutions sampled by p(\u00b7; I; w) is equal to L(w) = E                                                     E                                                           (1)\n                                                           I\u223cR[LI(w)] , LI(w) =                     s\u223cp(\u00b7;I;w)[L(s; I)] .\nOur goal (which was the empirical focus of [BPL+16]) is to optimize the parameter w \u2208                                                                           W in\norder to find a sampler p(\u00b7; I; w) whose loss L(w) is close to the expected optimal value opt:\n                                                                opt = E              min                   .                                                         (2)\n                                                                            I\u223cR       s\u2208S L(s; I)\nAs we have already mentioned, we focus on gradient descent dynamics: the policy gradient\nmethod [Kak01] expresses the gradient of L as follows\n                                           \u2207wL(w) = E                       E\n                                                                I\u223cR    s\u223cp(\u00b7;I;w)[L(s; I) \u2207w log p(s; I; w)] ,\nand updates the parameter w using the gradient descent update. Observe that a (stochastic)\npolicy gradient update can be implemented using only access to a solution cost oracle of Defini-\ntion 1.\nSolution Generators.                       In [BPL+16] the authors used neural networks as parametric solution\ngenerators for the TSP problem. They provided empirical evidence that optimizing the param-\neters of the neural network using the policy gradient method results to samplers that generate\nvery good solutions for (Euclidean) TSP instances. Parameterizing the solution generators using\nneural networks essentially compresses the description of distributions over solutions (the full pa-\nrameterization would require assigning a parameter to every solution-instance pair (s, I)). Since\nfor most combinatorial problems the size of the solution space is exponentially large (compared\nto the description of the instance), it is crucial that for such methods to succeed the parameteriza-\ntion must be compressed in the sense that the description of the parameter space W is polynomial\nin the size of the description of the instance family I. Apart from having a tractable number of\nparameters, it is important that the optimization objective corresponding to the parametric class P\ncan provably be optimized using some first-order method in polynomial (in the size of the input)\niterations.\n       We collect these desiderata in the following definition. We denote by [I] the description size\nof I, i.e., the number of bits required to identify any element of I. For instance, if I is the space\nof unweighted graphs with at most n nodes, [I] = O(n2).\nDefinition 2 (Complete, Compressed and Efficiently Optimizable Solution Generator). Fix a prior\nR over I, a family of solution generators P = {p(w) : w \u2208                                              W}, a loss function L as in Equation (1)\nand some \u03f5 > 0.\n                                                                                     3", "md": "# Math Equations and Text\n\nLet \\( R \\) be some prior distribution over the instance space \\( I \\) and \\( W \\) be the space of parameters\nof the model. We now define the class of solution generators. The solution generator \\( p(w) \\)\nwith parameter \\( w \\in W \\) takes as input an instance \\( I \\) and generates a random solution \\( s \\) in \\( S \\).\nTo distinguish between the output, the input, and the parameter of the solution generator, we\nuse the notation \\( p(\\cdot; I; w) \\) to denote the distribution over solutions and \\( p(s; I; w) \\) to denote the\nprobability of an individual solution \\( s \\in S \\). We denote by \\( P = \\{ p(w) : w \\in W \\} \\) the above\nparametric class of solution generators. For some parameter \\( w \\), the loss corresponding to the\nsolutions sampled by \\( p(\\cdot; I; w) \\) is equal to \\( L(w) = E [ E (1) I \\sim R [L_I(w)] , L_I(w) = s \\sim p(\\cdot;I;w)[L(s; I)] \\).\n\nOur goal (which was the empirical focus of [BPL+16]) is to optimize the parameter \\( w \\in W \\) in\norder to find a sampler \\( p(\\cdot; I; w) \\) whose loss \\( L(w) \\) is close to the expected optimal value opt:\n\\( opt = E [ min (2) I \\sim R s \\in S L(s; I) \\).\n\nAs we have already mentioned, we focus on gradient descent dynamics: the policy gradient\nmethod [Kak01] expresses the gradient of \\( L \\) as follows\n\\( \\nabla_w L(w) = E [ E I \\sim R s \\sim p(\\cdot;I;w)[L(s; I) \\nabla_w \\log p(s; I; w)] \\),\nand updates the parameter \\( w \\) using the gradient descent update. Observe that a (stochastic)\npolicy gradient update can be implemented using only access to a solution cost oracle of Definition 1.\n\nSolution Generators. In [BPL+16] the authors used neural networks as parametric solution\ngenerators for the TSP problem. They provided empirical evidence that optimizing the parameters of the neural network using the policy gradient method results in samplers that generate very good solutions for (Euclidean) TSP instances. Parameterizing the solution generators using neural networks essentially compresses the description of distributions over solutions (the full parameterization would require assigning a parameter to every solution-instance pair \\( (s, I) \\)). Since for most combinatorial problems the size of the solution space is exponentially large (compared to the description of the instance), it is crucial that for such methods to succeed the parameterization must be compressed in the sense that the description of the parameter space \\( W \\) is polynomial in the size of the description of the instance family \\( I \\). Apart from having a tractable number of parameters, it is important that the optimization objective corresponding to the parametric class \\( P \\) can provably be optimized using some first-order method in polynomial (in the size of the input) iterations.\n\nWe collect these desiderata in the following definition. We denote by \\( [I] \\) the description size\nof \\( I \\), i.e., the number of bits required to identify any element of \\( I \\). For instance, if \\( I \\) is the space\nof unweighted graphs with at most \\( n \\) nodes, \\( [I] = O(n^2) \\).\n\nDefinition 2 (Complete, Compressed and Efficiently Optimizable Solution Generator). Fix a prior\n\\( R \\) over \\( I \\), a family of solution generators \\( P = \\{ p(w) : w \\in W \\} \\), a loss function \\( L \\) as in Equation (1)\nand some \\( \\epsilon > 0 \\).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Let \\( R \\) be some prior distribution over the instance space \\( I \\) and \\( W \\) be the space of parameters\nof the model. We now define the class of solution generators. The solution generator \\( p(w) \\)\nwith parameter \\( w \\in W \\) takes as input an instance \\( I \\) and generates a random solution \\( s \\) in \\( S \\).\nTo distinguish between the output, the input, and the parameter of the solution generator, we\nuse the notation \\( p(\\cdot; I; w) \\) to denote the distribution over solutions and \\( p(s; I; w) \\) to denote the\nprobability of an individual solution \\( s \\in S \\). We denote by \\( P = \\{ p(w) : w \\in W \\} \\) the above\nparametric class of solution generators. For some parameter \\( w \\), the loss corresponding to the\nsolutions sampled by \\( p(\\cdot; I; w) \\) is equal to \\( L(w) = E [ E (1) I \\sim R [L_I(w)] , L_I(w) = s \\sim p(\\cdot;I;w)[L(s; I)] \\).\n\nOur goal (which was the empirical focus of [BPL+16]) is to optimize the parameter \\( w \\in W \\) in\norder to find a sampler \\( p(\\cdot; I; w) \\) whose loss \\( L(w) \\) is close to the expected optimal value opt:\n\\( opt = E [ min (2) I \\sim R s \\in S L(s; I) \\).\n\nAs we have already mentioned, we focus on gradient descent dynamics: the policy gradient\nmethod [Kak01] expresses the gradient of \\( L \\) as follows\n\\( \\nabla_w L(w) = E [ E I \\sim R s \\sim p(\\cdot;I;w)[L(s; I) \\nabla_w \\log p(s; I; w)] \\),\nand updates the parameter \\( w \\) using the gradient descent update. Observe that a (stochastic)\npolicy gradient update can be implemented using only access to a solution cost oracle of Definition 1.\n\nSolution Generators. In [BPL+16] the authors used neural networks as parametric solution\ngenerators for the TSP problem. They provided empirical evidence that optimizing the parameters of the neural network using the policy gradient method results in samplers that generate very good solutions for (Euclidean) TSP instances. Parameterizing the solution generators using neural networks essentially compresses the description of distributions over solutions (the full parameterization would require assigning a parameter to every solution-instance pair \\( (s, I) \\)). Since for most combinatorial problems the size of the solution space is exponentially large (compared to the description of the instance), it is crucial that for such methods to succeed the parameterization must be compressed in the sense that the description of the parameter space \\( W \\) is polynomial in the size of the description of the instance family \\( I \\). Apart from having a tractable number of parameters, it is important that the optimization objective corresponding to the parametric class \\( P \\) can provably be optimized using some first-order method in polynomial (in the size of the input) iterations.\n\nWe collect these desiderata in the following definition. We denote by \\( [I] \\) the description size\nof \\( I \\), i.e., the number of bits required to identify any element of \\( I \\). For instance, if \\( I \\) is the space\nof unweighted graphs with at most \\( n \\) nodes, \\( [I] = O(n^2) \\).\n\nDefinition 2 (Complete, Compressed and Efficiently Optimizable Solution Generator). Fix a prior\n\\( R \\) over \\( I \\), a family of solution generators \\( P = \\{ p(w) : w \\in W \\} \\), a loss function \\( L \\) as in Equation (1)\nand some \\( \\epsilon > 0 \\).", "md": "Let \\( R \\) be some prior distribution over the instance space \\( I \\) and \\( W \\) be the space of parameters\nof the model. We now define the class of solution generators. The solution generator \\( p(w) \\)\nwith parameter \\( w \\in W \\) takes as input an instance \\( I \\) and generates a random solution \\( s \\) in \\( S \\).\nTo distinguish between the output, the input, and the parameter of the solution generator, we\nuse the notation \\( p(\\cdot; I; w) \\) to denote the distribution over solutions and \\( p(s; I; w) \\) to denote the\nprobability of an individual solution \\( s \\in S \\). We denote by \\( P = \\{ p(w) : w \\in W \\} \\) the above\nparametric class of solution generators. For some parameter \\( w \\), the loss corresponding to the\nsolutions sampled by \\( p(\\cdot; I; w) \\) is equal to \\( L(w) = E [ E (1) I \\sim R [L_I(w)] , L_I(w) = s \\sim p(\\cdot;I;w)[L(s; I)] \\).\n\nOur goal (which was the empirical focus of [BPL+16]) is to optimize the parameter \\( w \\in W \\) in\norder to find a sampler \\( p(\\cdot; I; w) \\) whose loss \\( L(w) \\) is close to the expected optimal value opt:\n\\( opt = E [ min (2) I \\sim R s \\in S L(s; I) \\).\n\nAs we have already mentioned, we focus on gradient descent dynamics: the policy gradient\nmethod [Kak01] expresses the gradient of \\( L \\) as follows\n\\( \\nabla_w L(w) = E [ E I \\sim R s \\sim p(\\cdot;I;w)[L(s; I) \\nabla_w \\log p(s; I; w)] \\),\nand updates the parameter \\( w \\) using the gradient descent update. Observe that a (stochastic)\npolicy gradient update can be implemented using only access to a solution cost oracle of Definition 1.\n\nSolution Generators. In [BPL+16] the authors used neural networks as parametric solution\ngenerators for the TSP problem. They provided empirical evidence that optimizing the parameters of the neural network using the policy gradient method results in samplers that generate very good solutions for (Euclidean) TSP instances. Parameterizing the solution generators using neural networks essentially compresses the description of distributions over solutions (the full parameterization would require assigning a parameter to every solution-instance pair \\( (s, I) \\)). Since for most combinatorial problems the size of the solution space is exponentially large (compared to the description of the instance), it is crucial that for such methods to succeed the parameterization must be compressed in the sense that the description of the parameter space \\( W \\) is polynomial in the size of the description of the instance family \\( I \\). Apart from having a tractable number of parameters, it is important that the optimization objective corresponding to the parametric class \\( P \\) can provably be optimized using some first-order method in polynomial (in the size of the input) iterations.\n\nWe collect these desiderata in the following definition. We denote by \\( [I] \\) the description size\nof \\( I \\), i.e., the number of bits required to identify any element of \\( I \\). For instance, if \\( I \\) is the space\nof unweighted graphs with at most \\( n \\) nodes, \\( [I] = O(n^2) \\).\n\nDefinition 2 (Complete, Compressed and Efficiently Optimizable Solution Generator). Fix a prior\n\\( R \\) over \\( I \\), a family of solution generators \\( P = \\{ p(w) : w \\in W \\} \\), a loss function \\( L \\) as in Equation (1)\nand some \\( \\epsilon > 0 \\)."}]}, {"page": 4, "text": "    1. We say that P is complete if there exists some w \u2208         W such that L(w) \u2264       opt + \u03b5, where opt is\n       defined in (2).\n    2. We say that P is compressed if the description size of the parameter space W is polynomial in [I]\n       and in log(1/\u03b5).\n    3. We say that P is efficiently optimizable if there exists a first-order method applied on the objective\n       L such that after T = poly([W], 1/\u03b5) many updates on the parameter vectors, finds an (at most)\n       \u03f5-sub-optimal vector   w, i.e., L(w) \u2264   L(w) + \u03f5 .\nRemark 1. We remark that constructing parametric families that are complete and compressed, complete\nand efficiently optimizable, or compressed and efficiently optimizable (i.e., satisfying any pair of assump-\ntions of Question 1 but not all 3) is usually a much easier task. Consider, for example, the Max-Cut\nproblem on a fixed (unweighted) graph with n nodes. Note that I has description size O(n2). Solutions of\nthe Max-Cut for a graph with n nodes are represented by vertices on the binary hypercube {\u00b11}n (coor-\ndinate i dictates the side of the cut that we put node i). One may consider the full parameterization of all\ndistributions over the hypercube. It is not hard to see that this is a complete and efficiently optimiz-\nable family (the optimization landscape corresponds to optimizing a linear objective). However, it is not\ncompressed, since it requires 2n parameters. On the other extreme, considering a product distribution\nover coordinates, i.e., we set the value of node i to be +1 with probability pi and \u22121 with 1 \u2212            pi gives\na complete and compressed family. However, as we show in Appendix B, the landscape of this com-\npressed parameterization suffers from highly sub-optimal local minima and therefore, it is not efficiently\noptimizable.\n     Therefore, in this work we investigate whether it is possible to have all 3 desiderata of Defi-\nnition 2 at the same time.\nQuestion 1. Are there complete, compressed, and efficiently optimizable solution generators (i.e., satisfy-\ning Definition 2) for challenging combinatorial tasks?\n1.2    Our Results\nOur Contributions.         Before we present our results formally, we summarize the contributions of\nthis work.\n    \u2022 Our main contribution is a positive answer (Theorem 1) to Question 1 under general as-\n       sumptions that capture many combinatorial tasks.                We identify a set of conditions (see\n       Assumption 1) that allow us to design a family of solution generators that are complete,\n       compressed and efficiently optimizable.\n    \u2022 The conditions are motivated by obstacles that are important for any approach of this\n       nature. This includes solutions that escape to infinity, and also parts of the landscape with\n       vanishing gradient. See the discussion in Section 3 and Figure 1.\n    \u2022 We specialize our framework to several important combinatorial problems, some of which\n       are NP-hard, and others tractable: Max-Cut, Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-\n       Matching, and the Traveling Salesman Problem.\n                                                         4", "md": "# Complete, Compressed, and Efficiently Optimizable Solution Generators\n\n# Complete, Compressed, and Efficiently Optimizable Solution Generators\n\n1. We say that P is complete if there exists some w \u2208 W such that L(w) \u2264 opt + \u03b5, where opt is defined in (2).\n\n2. We say that P is compressed if the description size of the parameter space W is polynomial in [I] and in log(1/\u03b5).\n\n3. We say that P is efficiently optimizable if there exists a first-order method applied on the objective L such that after T = poly([W], 1/\u03b5) many updates on the parameter vectors, finds an (at most) \u03f5-sub-optimal vector w, i.e., L(w) \u2264 L(w) + \u03f5.\n\nRemark 1. We remark that constructing parametric families that are complete and compressed, complete and efficiently optimizable, or compressed and efficiently optimizable (i.e., satisfying any pair of assumptions of Question 1 but not all 3) is usually a much easier task. Consider, for example, the Max-Cut problem on a fixed (unweighted) graph with n nodes. Note that I has description size O(n^2). Solutions of the Max-Cut for a graph with n nodes are represented by vertices on the binary hypercube {\u00b11}^n (coordinate i dictates the side of the cut that we put node i). One may consider the full parameterization of all distributions over the hypercube. It is not hard to see that this is a complete and efficiently optimizable family (the optimization landscape corresponds to optimizing a linear objective). However, it is not compressed, since it requires 2n parameters. On the other extreme, considering a product distribution over coordinates, i.e., we set the value of node i to be +1 with probability p_i and -1 with 1 - p_i gives a complete and compressed family. However, as we show in Appendix B, the landscape of this compressed parameterization suffers from highly sub-optimal local minima and therefore, it is not efficiently optimizable.\n\nTherefore, in this work we investigate whether it is possible to have all 3 desiderata of Definition 2 at the same time.\n\nQuestion 1. Are there complete, compressed, and efficiently optimizable solution generators (i.e., satisfying Definition 2) for challenging combinatorial tasks?\n\n## 1.2 Our Results\n\nOur Contributions. Before we present our results formally, we summarize the contributions of this work.\n\n- Our main contribution is a positive answer (Theorem 1) to Question 1 under general assumptions that capture many combinatorial tasks. We identify a set of conditions (see Assumption 1) that allow us to design a family of solution generators that are complete, compressed and efficiently optimizable.\n- The conditions are motivated by obstacles that are important for any approach of this nature. This includes solutions that escape to infinity, and also parts of the landscape with vanishing gradient. See the discussion in Section 3 and Figure 1.\n- We specialize our framework to several important combinatorial problems, some of which are NP-hard, and others tractable: Max-Cut, Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Complete, Compressed, and Efficiently Optimizable Solution Generators", "md": "# Complete, Compressed, and Efficiently Optimizable Solution Generators"}, {"type": "heading", "lvl": 1, "value": "Complete, Compressed, and Efficiently Optimizable Solution Generators", "md": "# Complete, Compressed, and Efficiently Optimizable Solution Generators"}, {"type": "text", "value": "1. We say that P is complete if there exists some w \u2208 W such that L(w) \u2264 opt + \u03b5, where opt is defined in (2).\n\n2. We say that P is compressed if the description size of the parameter space W is polynomial in [I] and in log(1/\u03b5).\n\n3. We say that P is efficiently optimizable if there exists a first-order method applied on the objective L such that after T = poly([W], 1/\u03b5) many updates on the parameter vectors, finds an (at most) \u03f5-sub-optimal vector w, i.e., L(w) \u2264 L(w) + \u03f5.\n\nRemark 1. We remark that constructing parametric families that are complete and compressed, complete and efficiently optimizable, or compressed and efficiently optimizable (i.e., satisfying any pair of assumptions of Question 1 but not all 3) is usually a much easier task. Consider, for example, the Max-Cut problem on a fixed (unweighted) graph with n nodes. Note that I has description size O(n^2). Solutions of the Max-Cut for a graph with n nodes are represented by vertices on the binary hypercube {\u00b11}^n (coordinate i dictates the side of the cut that we put node i). One may consider the full parameterization of all distributions over the hypercube. It is not hard to see that this is a complete and efficiently optimizable family (the optimization landscape corresponds to optimizing a linear objective). However, it is not compressed, since it requires 2n parameters. On the other extreme, considering a product distribution over coordinates, i.e., we set the value of node i to be +1 with probability p_i and -1 with 1 - p_i gives a complete and compressed family. However, as we show in Appendix B, the landscape of this compressed parameterization suffers from highly sub-optimal local minima and therefore, it is not efficiently optimizable.\n\nTherefore, in this work we investigate whether it is possible to have all 3 desiderata of Definition 2 at the same time.\n\nQuestion 1. Are there complete, compressed, and efficiently optimizable solution generators (i.e., satisfying Definition 2) for challenging combinatorial tasks?", "md": "1. We say that P is complete if there exists some w \u2208 W such that L(w) \u2264 opt + \u03b5, where opt is defined in (2).\n\n2. We say that P is compressed if the description size of the parameter space W is polynomial in [I] and in log(1/\u03b5).\n\n3. We say that P is efficiently optimizable if there exists a first-order method applied on the objective L such that after T = poly([W], 1/\u03b5) many updates on the parameter vectors, finds an (at most) \u03f5-sub-optimal vector w, i.e., L(w) \u2264 L(w) + \u03f5.\n\nRemark 1. We remark that constructing parametric families that are complete and compressed, complete and efficiently optimizable, or compressed and efficiently optimizable (i.e., satisfying any pair of assumptions of Question 1 but not all 3) is usually a much easier task. Consider, for example, the Max-Cut problem on a fixed (unweighted) graph with n nodes. Note that I has description size O(n^2). Solutions of the Max-Cut for a graph with n nodes are represented by vertices on the binary hypercube {\u00b11}^n (coordinate i dictates the side of the cut that we put node i). One may consider the full parameterization of all distributions over the hypercube. It is not hard to see that this is a complete and efficiently optimizable family (the optimization landscape corresponds to optimizing a linear objective). However, it is not compressed, since it requires 2n parameters. On the other extreme, considering a product distribution over coordinates, i.e., we set the value of node i to be +1 with probability p_i and -1 with 1 - p_i gives a complete and compressed family. However, as we show in Appendix B, the landscape of this compressed parameterization suffers from highly sub-optimal local minima and therefore, it is not efficiently optimizable.\n\nTherefore, in this work we investigate whether it is possible to have all 3 desiderata of Definition 2 at the same time.\n\nQuestion 1. Are there complete, compressed, and efficiently optimizable solution generators (i.e., satisfying Definition 2) for challenging combinatorial tasks?"}, {"type": "heading", "lvl": 2, "value": "1.2 Our Results", "md": "## 1.2 Our Results"}, {"type": "text", "value": "Our Contributions. Before we present our results formally, we summarize the contributions of this work.\n\n- Our main contribution is a positive answer (Theorem 1) to Question 1 under general assumptions that capture many combinatorial tasks. We identify a set of conditions (see Assumption 1) that allow us to design a family of solution generators that are complete, compressed and efficiently optimizable.\n- The conditions are motivated by obstacles that are important for any approach of this nature. This includes solutions that escape to infinity, and also parts of the landscape with vanishing gradient. See the discussion in Section 3 and Figure 1.\n- We specialize our framework to several important combinatorial problems, some of which are NP-hard, and others tractable: Max-Cut, Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem.", "md": "Our Contributions. Before we present our results formally, we summarize the contributions of this work.\n\n- Our main contribution is a positive answer (Theorem 1) to Question 1 under general assumptions that capture many combinatorial tasks. We identify a set of conditions (see Assumption 1) that allow us to design a family of solution generators that are complete, compressed and efficiently optimizable.\n- The conditions are motivated by obstacles that are important for any approach of this nature. This includes solutions that escape to infinity, and also parts of the landscape with vanishing gradient. See the discussion in Section 3 and Figure 1.\n- We specialize our framework to several important combinatorial problems, some of which are NP-hard, and others tractable: Max-Cut, Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem."}]}, {"page": 5, "text": "   \u2022 Finally, we investigate experimentally the effect of the entropy regularizer and the fast/slow\n      mixture scheme that we introduced (see Section 3) and provide evidence that it leads to\n      better solution generators.\n    We begin with the formal presentation of our assumptions on the feature mappings of the\ninstances and solutions and on the structure of cost function of the combinatorial problem.\nAssumption 1 (Structured Feature Mappings). Let S be the solution space and I be the instance space.\nThere exist feature mappings \u03c8S : S \u2192       X, for the solutions, and, \u03c8I : I \u2192       Z, for the instances, where\nX, Z are Euclidean vector spaces of dimension nX and nZ, such that:\n   1. (Bounded Feature Spaces) The feature and instance mappings are bounded, i.e., there exist DS, DI >\n      0 such that \u2225\u03c8S(s)\u22252 \u2264      DS, for all s \u2208 S and \u2225\u03c8I(I)\u22252 \u2264      DI, for all I \u2208  I.\n   2. (Bilinear Cost Oracle) The cost of a solution s under instance I can be expressed as a bilinear\n      function of the corresponding feature vector \u03c8S(s) and instance vector \u03c8I(I), i.e., the solution\n      oracle can be expressed as O(s, I) = \u03c8I(I)\u22a4M\u03c8S(s) for any s \u2208           S, I \u2208  I, for some matrix M with\n      \u2225M\u2225F \u2264     C.\n   3. (Variance Preserving Features) There exists \u03b1 > 0 such that Var            s\u223cU(S)[v \u00b7 \u03c8S(s)] \u2265    \u03b1\u2225v\u22252 2 for\n      any v \u2208   X, where U(S) is the uniform distribution over the solution space S.\n   4. (Bounded Dimensions/Diameters) The feature dimensions nX, nZ, and the diameter bounds\n      DS, DI, C are bounded above by a polynomial of the description size of the instance space I. The\n      variance lower bound \u03b1 is bounded below by 1/poly([I]).\nRemark 2 (Boundedness and Bilinear Cost Assumptions). We remark that Items 1, 4 are simply\nboundedness assumptions for the corresponding feauture mappings and usually follow easily assuming\nthat we consider reasonable feature mappings.         At a high-level, the assumption that the solution is a\nbilinear function of the solution and instance features (Item 2) prescribes that \u201cgood\u201d feature mappings\nshould enable a simple (i.e., bilinear) expression for the cost function. In the sequel we see that this is\nsatisfied by natural feature mappings for important classes of combinatorial problems.\nRemark 3 (Variance Preservation Assumption). In Item 3 (variance preservation) we require that\nthe solution feature mapping has variance along every direction, i.e., the feature vectors corresponding to\nthe solutions must be \u201cspread-out\u201d when the underlying solution generator is the uniform distribution.\nAs we show, this assumption is crucial so that the gradients of the resulting optimization objective are\nnot-vanishing, allowing for its efficient optimization.\n    We mention that various important combinatorial problems satisfy Assumption 1. For in-\nstance, Assumption 1 is satisfied by Max-Cut, Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-\nMatching and Traveling Salesman. We refer the reader to the upcoming Section 2 for an explicit\ndescription of the structured feature mappings for these problems. Having discussed Assump-\ntion 1, we are ready to state our main abstract result which resolves Question 1.\nTheorem 1. Consider a combinatorial problem with instance space I that satisfies Assumption 1. For\nany prior R over I and \u03f5 > 0, there exists a family of solution generators P = {p(w) : w \u2208               W} with\nparameter space W that is complete, compressed and, efficiently optimizable.\n                                                         5", "md": "# Math Equations and Text\n\nFinally, we investigate experimentally the effect of the entropy regularizer and the fast/slow mixture scheme that we introduced (see Section 3) and provide evidence that it leads to better solution generators.\n\nWe begin with the formal presentation of our assumptions on the feature mappings of the instances and solutions and on the structure of the cost function of the combinatorial problem.\n\nAssumption 1 (Structured Feature Mappings). Let S be the solution space and I be the instance space. There exist feature mappings $$\\psi_S : S \\rightarrow X$$, for the solutions, and $$\\psi_I : I \\rightarrow Z$$, for the instances, where X, Z are Euclidean vector spaces of dimension nX and nZ, such that:\n\n1. (Bounded Feature Spaces) The feature and instance mappings are bounded, i.e., there exist DS, DI &gt; 0 such that $\\|\\psi_S(s)\\|_2 \\leq DS$, for all s \u2208 S and $\\|\\psi_I(I)\\|_2 \\leq DI$, for all I \u2208 I.\n2. (Bilinear Cost Oracle) The cost of a solution s under instance I can be expressed as a bilinear function of the corresponding feature vector $\\psi_S(s)$ and instance vector $\\psi_I(I)$, i.e., the solution oracle can be expressed as $O(s, I) = \\psi_I(I)^T M \\psi_S(s)$ for any s \u2208 S, I \u2208 I, for some matrix M with $\\|M\\|_F \\leq C$.\n3. (Variance Preserving Features) There exists \u03b1 &gt; 0 such that $Var_{s \\sim U(S)}[v \\cdot \\psi_S(s)] \\geq \\alpha \\|v\\|_2^2$ for any v \u2208 X, where U(S) is the uniform distribution over the solution space S.\n4. (Bounded Dimensions/Diameters) The feature dimensions nX, nZ, and the diameter bounds DS, DI, C are bounded above by a polynomial of the description size of the instance space I. The variance lower bound \u03b1 is bounded below by 1/poly([I]).\n\nRemark 2 (Boundedness and Bilinear Cost Assumptions). We remark that Items 1, 4 are simply boundedness assumptions for the corresponding feature mappings and usually follow easily assuming that we consider reasonable feature mappings. At a high-level, the assumption that the solution is a bilinear function of the solution and instance features (Item 2) prescribes that \u201cgood\u201d feature mappings should enable a simple (i.e., bilinear) expression for the cost function. In the sequel we see that this is satisfied by natural feature mappings for important classes of combinatorial problems.\n\nRemark 3 (Variance Preservation Assumption). In Item 3 (variance preservation) we require that the solution feature mapping has variance along every direction, i.e., the feature vectors corresponding to the solutions must be \u201cspread-out\u201d when the underlying solution generator is the uniform distribution. As we show, this assumption is crucial so that the gradients of the resulting optimization objective are not-vanishing, allowing for its efficient optimization.\n\nWe mention that various important combinatorial problems satisfy Assumption 1. For instance, Assumption 1 is satisfied by Max-Cut, Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching and Traveling Salesman. We refer the reader to the upcoming Section 2 for an explicit description of the structured feature mappings for these problems. Having discussed Assumption 1, we are ready to state our main abstract result which resolves Question 1.\n\nTheorem 1. Consider a combinatorial problem with instance space I that satisfies Assumption 1. For any prior R over I and \u03f5 &gt; 0, there exists a family of solution generators P = {p(w) : w \u2208 W} with parameter space W that is complete, compressed and, efficiently optimizable.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Finally, we investigate experimentally the effect of the entropy regularizer and the fast/slow mixture scheme that we introduced (see Section 3) and provide evidence that it leads to better solution generators.\n\nWe begin with the formal presentation of our assumptions on the feature mappings of the instances and solutions and on the structure of the cost function of the combinatorial problem.\n\nAssumption 1 (Structured Feature Mappings). Let S be the solution space and I be the instance space. There exist feature mappings $$\\psi_S : S \\rightarrow X$$, for the solutions, and $$\\psi_I : I \\rightarrow Z$$, for the instances, where X, Z are Euclidean vector spaces of dimension nX and nZ, such that:\n\n1. (Bounded Feature Spaces) The feature and instance mappings are bounded, i.e., there exist DS, DI &gt; 0 such that $\\|\\psi_S(s)\\|_2 \\leq DS$, for all s \u2208 S and $\\|\\psi_I(I)\\|_2 \\leq DI$, for all I \u2208 I.\n2. (Bilinear Cost Oracle) The cost of a solution s under instance I can be expressed as a bilinear function of the corresponding feature vector $\\psi_S(s)$ and instance vector $\\psi_I(I)$, i.e., the solution oracle can be expressed as $O(s, I) = \\psi_I(I)^T M \\psi_S(s)$ for any s \u2208 S, I \u2208 I, for some matrix M with $\\|M\\|_F \\leq C$.\n3. (Variance Preserving Features) There exists \u03b1 &gt; 0 such that $Var_{s \\sim U(S)}[v \\cdot \\psi_S(s)] \\geq \\alpha \\|v\\|_2^2$ for any v \u2208 X, where U(S) is the uniform distribution over the solution space S.\n4. (Bounded Dimensions/Diameters) The feature dimensions nX, nZ, and the diameter bounds DS, DI, C are bounded above by a polynomial of the description size of the instance space I. The variance lower bound \u03b1 is bounded below by 1/poly([I]).\n\nRemark 2 (Boundedness and Bilinear Cost Assumptions). We remark that Items 1, 4 are simply boundedness assumptions for the corresponding feature mappings and usually follow easily assuming that we consider reasonable feature mappings. At a high-level, the assumption that the solution is a bilinear function of the solution and instance features (Item 2) prescribes that \u201cgood\u201d feature mappings should enable a simple (i.e., bilinear) expression for the cost function. In the sequel we see that this is satisfied by natural feature mappings for important classes of combinatorial problems.\n\nRemark 3 (Variance Preservation Assumption). In Item 3 (variance preservation) we require that the solution feature mapping has variance along every direction, i.e., the feature vectors corresponding to the solutions must be \u201cspread-out\u201d when the underlying solution generator is the uniform distribution. As we show, this assumption is crucial so that the gradients of the resulting optimization objective are not-vanishing, allowing for its efficient optimization.\n\nWe mention that various important combinatorial problems satisfy Assumption 1. For instance, Assumption 1 is satisfied by Max-Cut, Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching and Traveling Salesman. We refer the reader to the upcoming Section 2 for an explicit description of the structured feature mappings for these problems. Having discussed Assumption 1, we are ready to state our main abstract result which resolves Question 1.\n\nTheorem 1. Consider a combinatorial problem with instance space I that satisfies Assumption 1. For any prior R over I and \u03f5 &gt; 0, there exists a family of solution generators P = {p(w) : w \u2208 W} with parameter space W that is complete, compressed and, efficiently optimizable.", "md": "Finally, we investigate experimentally the effect of the entropy regularizer and the fast/slow mixture scheme that we introduced (see Section 3) and provide evidence that it leads to better solution generators.\n\nWe begin with the formal presentation of our assumptions on the feature mappings of the instances and solutions and on the structure of the cost function of the combinatorial problem.\n\nAssumption 1 (Structured Feature Mappings). Let S be the solution space and I be the instance space. There exist feature mappings $$\\psi_S : S \\rightarrow X$$, for the solutions, and $$\\psi_I : I \\rightarrow Z$$, for the instances, where X, Z are Euclidean vector spaces of dimension nX and nZ, such that:\n\n1. (Bounded Feature Spaces) The feature and instance mappings are bounded, i.e., there exist DS, DI &gt; 0 such that $\\|\\psi_S(s)\\|_2 \\leq DS$, for all s \u2208 S and $\\|\\psi_I(I)\\|_2 \\leq DI$, for all I \u2208 I.\n2. (Bilinear Cost Oracle) The cost of a solution s under instance I can be expressed as a bilinear function of the corresponding feature vector $\\psi_S(s)$ and instance vector $\\psi_I(I)$, i.e., the solution oracle can be expressed as $O(s, I) = \\psi_I(I)^T M \\psi_S(s)$ for any s \u2208 S, I \u2208 I, for some matrix M with $\\|M\\|_F \\leq C$.\n3. (Variance Preserving Features) There exists \u03b1 &gt; 0 such that $Var_{s \\sim U(S)}[v \\cdot \\psi_S(s)] \\geq \\alpha \\|v\\|_2^2$ for any v \u2208 X, where U(S) is the uniform distribution over the solution space S.\n4. (Bounded Dimensions/Diameters) The feature dimensions nX, nZ, and the diameter bounds DS, DI, C are bounded above by a polynomial of the description size of the instance space I. The variance lower bound \u03b1 is bounded below by 1/poly([I]).\n\nRemark 2 (Boundedness and Bilinear Cost Assumptions). We remark that Items 1, 4 are simply boundedness assumptions for the corresponding feature mappings and usually follow easily assuming that we consider reasonable feature mappings. At a high-level, the assumption that the solution is a bilinear function of the solution and instance features (Item 2) prescribes that \u201cgood\u201d feature mappings should enable a simple (i.e., bilinear) expression for the cost function. In the sequel we see that this is satisfied by natural feature mappings for important classes of combinatorial problems.\n\nRemark 3 (Variance Preservation Assumption). In Item 3 (variance preservation) we require that the solution feature mapping has variance along every direction, i.e., the feature vectors corresponding to the solutions must be \u201cspread-out\u201d when the underlying solution generator is the uniform distribution. As we show, this assumption is crucial so that the gradients of the resulting optimization objective are not-vanishing, allowing for its efficient optimization.\n\nWe mention that various important combinatorial problems satisfy Assumption 1. For instance, Assumption 1 is satisfied by Max-Cut, Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching and Traveling Salesman. We refer the reader to the upcoming Section 2 for an explicit description of the structured feature mappings for these problems. Having discussed Assumption 1, we are ready to state our main abstract result which resolves Question 1.\n\nTheorem 1. Consider a combinatorial problem with instance space I that satisfies Assumption 1. For any prior R over I and \u03f5 &gt; 0, there exists a family of solution generators P = {p(w) : w \u2208 W} with parameter space W that is complete, compressed and, efficiently optimizable."}]}, {"page": 6, "text": "    A sketch behind the design of the family P can be found in Section 3 and Section 4.\nRemark 4 (Computational Barriers in Sampling). We note that the families of generative models (a.k.a.,\nsolution generators) that we provide have polynomial parameter complexity and are optimizable in a small\nnumber of steps using gradient-based methods. Hence, in a small number of iterations, gradient-based\nmethods converge to distributions whose mass is concentrated on nearly optimal solutions. This holds,\nas we show, even for challenging (NP-hard) combinatorial problems. Our results do not, however, prove\nP = NP, as it may be computationally hard to sample from our generative models. We remark that\nwhile such approaches are in theory hard, such models seem to perform remarkably well experimentally\nwhere sampling is based on Langevin dynamics techniques [SE20, SSDK+20]. Though as our theory\npredicts, and simulations support, landscape problems seem to be a direct impediment even to obtain good\napproximate solutions.\nRemark 5 (Neural Networks as Solution Samplers). A natural question would be whether our results\ncan be extended to the case where neural networks are (efficient) solution samplers, as in [BPL+16].\nUnfortunately, a benign landscape result for neural network solution generators most likely cannot exist.\nIt is well-known that end-to-end theoretical guarantees for training neural networks are out of reach since\nthe corresponding optimization tasks are provably computationally intractable, see, e.g., [CGKM22] and\nthe references therein.\n    Finally, we would like to mention an interesting aspect of Assumption 1.                    Given a com-\nbinatorial problem, Assumption 1 essentially asks for the design of feature mappings for the\nsolutions and the instances that satisfy desiderata such as boundedness and variance preserva-\ntion. Max-Cut, Min-Cut, TSP and Max-k-CSP and other problems satisfy Assumption 1 because\nwe managed to design appropriate (problem-specific) feature mappings that satisfy the require-\nments of Assumption 1. There are interesting combinatorial problems for which we do not know\nhow to design such good feature mappings. For instance, the \"natural\" feature mapping for the\nSatisfiability problem (SAT) (similar to the one we used for Max-k-CSPs) would require feature\ndimension exponential in the size of the instance (we need all possible monomials of n variables\nand degree at most n) and therefore, would violate Item 4 of Assumption 1.\n1.3    Related Work\nNeural Combinatorial Optimization.              Tackling combinatorial optimization problems consti-\ntutes one of the most fundamental tasks of theoretical computer science [GLS12, PS98, S+03,\nSch05, CLS+95] and various approaches have been studied for these problems such as local search\nmethods, branch-and-bound algorithms and meta-heuristics such as genetic algorithms and sim-\nulated annealing. Starting from the seminal work of [HT85], researchers apply neural networks\n[Smi99, VFJ15, BPL+16] to solve combinatorial optimization tasks. In particular, researchers have\nexplored the power of machine learning, reinforcement learning and deep learning methods for\nsolving combinatorial optimization problems [BPL+16, YW20, LZ09, DCL+18, BLP21, MSIB21,\nNOST18, SHM+16, MKS+13, SSS+17, ER18, KVHW18, ZCH+20, CCK+21, MGH+19, GCF+19,\nKLMS19].\n    The use of neural networks in combinatorial problems is extensive [SLB+18, JLB19, GCF+19,\nYGS20, MSIB21, BPL+16, KDZ+17, YP19, CT19, YBV19, KCK+20, KCY+21, DAT20, NJS+20,\n                                                        6", "md": "# Document\n\nA sketch behind the design of the family P can be found in Section 3 and Section 4.\n\nRemark 4 (Computational Barriers in Sampling). We note that the families of generative models (a.k.a., solution generators) that we provide have polynomial parameter complexity and are optimizable in a small number of steps using gradient-based methods. Hence, in a small number of iterations, gradient-based methods converge to distributions whose mass is concentrated on nearly optimal solutions. This holds, as we show, even for challenging (NP-hard) combinatorial problems. Our results do not, however, prove P = NP, as it may be computationally hard to sample from our generative models. We remark that while such approaches are in theory hard, such models seem to perform remarkably well experimentally where sampling is based on Langevin dynamics techniques [SE20, SSDK+20]. Though as our theory predicts, and simulations support, landscape problems seem to be a direct impediment even to obtain good approximate solutions.\n\nRemark 5 (Neural Networks as Solution Samplers). A natural question would be whether our results can be extended to the case where neural networks are (efficient) solution samplers, as in [BPL+16]. Unfortunately, a benign landscape result for neural network solution generators most likely cannot exist. It is well-known that end-to-end theoretical guarantees for training neural networks are out of reach since the corresponding optimization tasks are provably computationally intractable, see, e.g., [CGKM22] and the references therein.\n\nFinally, we would like to mention an interesting aspect of Assumption 1. Given a combinatorial problem, Assumption 1 essentially asks for the design of feature mappings for the solutions and the instances that satisfy desiderata such as boundedness and variance preservation. Max-Cut, Min-Cut, TSP and Max-k-CSP and other problems satisfy Assumption 1 because we managed to design appropriate (problem-specific) feature mappings that satisfy the requirements of Assumption 1. There are interesting combinatorial problems for which we do not know how to design such good feature mappings. For instance, the \"natural\" feature mapping for the Satisfiability problem (SAT) (similar to the one we used for Max-k-CSPs) would require feature dimension exponential in the size of the instance (we need all possible monomials of n variables and degree at most n) and therefore, would violate Item 4 of Assumption 1.\n\n### 1.3 Related Work\n\nNeural Combinatorial Optimization. Tackling combinatorial optimization problems constitutes one of the most fundamental tasks of theoretical computer science [GLS12, PS98, S+03, Sch05, CLS+95] and various approaches have been studied for these problems such as local search methods, branch-and-bound algorithms and meta-heuristics such as genetic algorithms and simulated annealing. Starting from the seminal work of [HT85], researchers apply neural networks [Smi99, VFJ15, BPL+16] to solve combinatorial optimization tasks. In particular, researchers have explored the power of machine learning, reinforcement learning and deep learning methods for solving combinatorial optimization problems [BPL+16, YW20, LZ09, DCL+18, BLP21, MSIB21, NOST18, SHM+16, MKS+13, SSS+17, ER18, KVHW18, ZCH+20, CCK+21, MGH+19, GCF+19, KLMS19].\n\nThe use of neural networks in combinatorial problems is extensive [SLB+18, JLB19, GCF+19, YGS20, MSIB21, BPL+16, KDZ+17, YP19, CT19, YBV19, KCK+20, KCY+21, DAT20, NJS+20.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "A sketch behind the design of the family P can be found in Section 3 and Section 4.\n\nRemark 4 (Computational Barriers in Sampling). We note that the families of generative models (a.k.a., solution generators) that we provide have polynomial parameter complexity and are optimizable in a small number of steps using gradient-based methods. Hence, in a small number of iterations, gradient-based methods converge to distributions whose mass is concentrated on nearly optimal solutions. This holds, as we show, even for challenging (NP-hard) combinatorial problems. Our results do not, however, prove P = NP, as it may be computationally hard to sample from our generative models. We remark that while such approaches are in theory hard, such models seem to perform remarkably well experimentally where sampling is based on Langevin dynamics techniques [SE20, SSDK+20]. Though as our theory predicts, and simulations support, landscape problems seem to be a direct impediment even to obtain good approximate solutions.\n\nRemark 5 (Neural Networks as Solution Samplers). A natural question would be whether our results can be extended to the case where neural networks are (efficient) solution samplers, as in [BPL+16]. Unfortunately, a benign landscape result for neural network solution generators most likely cannot exist. It is well-known that end-to-end theoretical guarantees for training neural networks are out of reach since the corresponding optimization tasks are provably computationally intractable, see, e.g., [CGKM22] and the references therein.\n\nFinally, we would like to mention an interesting aspect of Assumption 1. Given a combinatorial problem, Assumption 1 essentially asks for the design of feature mappings for the solutions and the instances that satisfy desiderata such as boundedness and variance preservation. Max-Cut, Min-Cut, TSP and Max-k-CSP and other problems satisfy Assumption 1 because we managed to design appropriate (problem-specific) feature mappings that satisfy the requirements of Assumption 1. There are interesting combinatorial problems for which we do not know how to design such good feature mappings. For instance, the \"natural\" feature mapping for the Satisfiability problem (SAT) (similar to the one we used for Max-k-CSPs) would require feature dimension exponential in the size of the instance (we need all possible monomials of n variables and degree at most n) and therefore, would violate Item 4 of Assumption 1.", "md": "A sketch behind the design of the family P can be found in Section 3 and Section 4.\n\nRemark 4 (Computational Barriers in Sampling). We note that the families of generative models (a.k.a., solution generators) that we provide have polynomial parameter complexity and are optimizable in a small number of steps using gradient-based methods. Hence, in a small number of iterations, gradient-based methods converge to distributions whose mass is concentrated on nearly optimal solutions. This holds, as we show, even for challenging (NP-hard) combinatorial problems. Our results do not, however, prove P = NP, as it may be computationally hard to sample from our generative models. We remark that while such approaches are in theory hard, such models seem to perform remarkably well experimentally where sampling is based on Langevin dynamics techniques [SE20, SSDK+20]. Though as our theory predicts, and simulations support, landscape problems seem to be a direct impediment even to obtain good approximate solutions.\n\nRemark 5 (Neural Networks as Solution Samplers). A natural question would be whether our results can be extended to the case where neural networks are (efficient) solution samplers, as in [BPL+16]. Unfortunately, a benign landscape result for neural network solution generators most likely cannot exist. It is well-known that end-to-end theoretical guarantees for training neural networks are out of reach since the corresponding optimization tasks are provably computationally intractable, see, e.g., [CGKM22] and the references therein.\n\nFinally, we would like to mention an interesting aspect of Assumption 1. Given a combinatorial problem, Assumption 1 essentially asks for the design of feature mappings for the solutions and the instances that satisfy desiderata such as boundedness and variance preservation. Max-Cut, Min-Cut, TSP and Max-k-CSP and other problems satisfy Assumption 1 because we managed to design appropriate (problem-specific) feature mappings that satisfy the requirements of Assumption 1. There are interesting combinatorial problems for which we do not know how to design such good feature mappings. For instance, the \"natural\" feature mapping for the Satisfiability problem (SAT) (similar to the one we used for Max-k-CSPs) would require feature dimension exponential in the size of the instance (we need all possible monomials of n variables and degree at most n) and therefore, would violate Item 4 of Assumption 1."}, {"type": "heading", "lvl": 3, "value": "1.3 Related Work", "md": "### 1.3 Related Work"}, {"type": "text", "value": "Neural Combinatorial Optimization. Tackling combinatorial optimization problems constitutes one of the most fundamental tasks of theoretical computer science [GLS12, PS98, S+03, Sch05, CLS+95] and various approaches have been studied for these problems such as local search methods, branch-and-bound algorithms and meta-heuristics such as genetic algorithms and simulated annealing. Starting from the seminal work of [HT85], researchers apply neural networks [Smi99, VFJ15, BPL+16] to solve combinatorial optimization tasks. In particular, researchers have explored the power of machine learning, reinforcement learning and deep learning methods for solving combinatorial optimization problems [BPL+16, YW20, LZ09, DCL+18, BLP21, MSIB21, NOST18, SHM+16, MKS+13, SSS+17, ER18, KVHW18, ZCH+20, CCK+21, MGH+19, GCF+19, KLMS19].\n\nThe use of neural networks in combinatorial problems is extensive [SLB+18, JLB19, GCF+19, YGS20, MSIB21, BPL+16, KDZ+17, YP19, CT19, YBV19, KCK+20, KCY+21, DAT20, NJS+20.", "md": "Neural Combinatorial Optimization. Tackling combinatorial optimization problems constitutes one of the most fundamental tasks of theoretical computer science [GLS12, PS98, S+03, Sch05, CLS+95] and various approaches have been studied for these problems such as local search methods, branch-and-bound algorithms and meta-heuristics such as genetic algorithms and simulated annealing. Starting from the seminal work of [HT85], researchers apply neural networks [Smi99, VFJ15, BPL+16] to solve combinatorial optimization tasks. In particular, researchers have explored the power of machine learning, reinforcement learning and deep learning methods for solving combinatorial optimization problems [BPL+16, YW20, LZ09, DCL+18, BLP21, MSIB21, NOST18, SHM+16, MKS+13, SSS+17, ER18, KVHW18, ZCH+20, CCK+21, MGH+19, GCF+19, KLMS19].\n\nThe use of neural networks in combinatorial problems is extensive [SLB+18, JLB19, GCF+19, YGS20, MSIB21, BPL+16, KDZ+17, YP19, CT19, YBV19, KCK+20, KCY+21, DAT20, NJS+20."}]}, {"page": 7, "text": "TRWG21, AMW18, KL20, Jeg22, SBK22, ART23] and various papers aim to understand the the-\noretical ability of neural networks to solve such problems [HS23b, HS23a, Gam23]. Our paper\nbuilds on the framework of the influential experimental work of [BPL+16] to tackle combina-\ntorial optimization problems such as TSP using neural networks and reinforcement learning.\n[KP+21] uses an entropy maximization scheme in order to generate diversified candidate solu-\ntions. This experimental heuristic is quite close to our theoretical idea for entropy regularization.\nIn our work, entropy regularization allows us to design quasar-convex landscapes and the fast/s-\nlow mixing scheme to obtain diversification of solutions. Among other related applied works,\n[KCK+20, KPP22] study the use of Transformer architectures combined with the Reinforce algo-\nrithm employing symmetries (i.e., the existence of multiple optimal solutions of a CO problem)\nimproving the generalization capability of Deep RL NCO and [MLC+21] studies Transformer\narchitectures and aims to learn improvement heuristics for routing problems using RL.\nGradient Descent Dynamics.        Our work provides theoretical understanding on the gradient-\ndescent landscape arising in NCO problems. Similar questions regarding the dynamics of gra-\ndient descent have been studied in prior work concerning neural networks; for instance, [AS20]\nand [AKM+21] fix the algorithm (SGD on neural networks) and aim to understand the power\nof this approach (which function classes can be learned). Various other works study gradient\ndescent dynamics in neural networks. We refer to [AS18, AS20, ABAB+21, MYSSS21, BEG+22,\nDLS22, ABA22, AAM22, BBSS22, ABAM23, AKM+21, EGK+23] (and the references therein) for\na small sample of this line of research.\n2    Combinatorial Applications\nWe now consider concrete combinatorial problems and show that there exist appropriate and\nnatural feature mappings for the solutions and instances that satisfy Assumption 1; so Theo-\nrem 1 is applicable for any such combinatorial task. For a more detailed treatment, we refer to\nAppendix G.\nMin-Cut and Max-Cut.        Min-Cut (resp. Max-Cut) are central graph combinatorial problems\nwhere the task is to split the nodes of the graph in two subsets so that the number of edges from\none subset to the other (edges of the cut) is minimized (resp. maximized). Given a graph G with\n n nodes represented by its Laplacian matrix LG = D \u2212     A, where D is the diagonal degree matrix\nand A is the adjacency matrix of the graph, the goal in the Min-Cut (resp. Max-Cut) problem is\nto find a solution vector s \u2208 {\u00b11}n so that s\u22a4LGs/4 is minimized (resp. maximized) [Spi07].\n    We first show that there exist natural feature mappings so that the cost of every solution s\nunder any instance/graph G is a bilinear function of the feature vectors, see Item 2 of Assump-\ntion 1. We consider the correlation-based feature mapping \u03c8S(s) = (ss\u22a4)\u266d           \u2208  Rn2, where by\n (\u00b7)\u266dwe denote the vectorization/flattening operation and the Laplacian for the instance (graph),\n \u03c8I(G) = (LG)\u266d   \u2208 Rn2. Then simply setting the matrix M to be the identity I \u2208    Rn2\u00d7n2 the cost of\nany solution s can be expressed as the bilinear function \u03c8I(G)\u22a4M\u03c8S(s) = (L\u266d      G)\u22a4(ss\u22a4)\u266d  = s\u22a4LGs.\nWe observe that for (unweighted) graphs with n nodes the description size of the family of all\ninstances I is roughly O(n2), and therefore the dimensions of the feature mappings \u03c8S, \u03c8I are\nclearly polynomial in the description size of I. Moreover, considering unweighted graphs, it\n                                                  7", "md": "# Combinatorial Optimization and Neural Networks\n\n## Combinatorial Optimization and Neural Networks\n\nVarious research papers aim to understand the theoretical ability of neural networks to solve combinatorial optimization problems. Our paper builds on the framework of influential experimental work to tackle problems such as the Traveling Salesman Problem (TSP) using neural networks and reinforcement learning. Entropy regularization is used to design landscapes and obtain diversified solutions. Other works study the use of Transformer architectures combined with the Reinforce algorithm to improve generalization capability in Deep RL NCO.\n\nGradient Descent Dynamics: Our work provides theoretical understanding of the gradient-descent landscape in NCO problems. Similar studies have been done on the dynamics of gradient descent in neural networks, aiming to understand the power of this approach and which function classes can be learned.\n\nCombinatorial Applications: Concrete combinatorial problems like Min-Cut and Max-Cut are discussed. These problems involve splitting graph nodes into subsets to minimize or maximize the number of edges between subsets. Feature mappings are shown to exist for solutions and instances, making Theorem 1 applicable for any combinatorial task.\n\n### Min-Cut and Max-Cut\n\nMin-Cut and Max-Cut are central graph combinatorial problems where the goal is to split graph nodes into subsets to minimize or maximize the number of edges between subsets. The Laplacian matrix LG = D - A is used to represent a graph G with n nodes. The cost of a solution vector s in Min-Cut or Max-Cut is expressed as s\u1d40LGs/4.\n\nWe show that there are natural feature mappings where the cost of every solution s under any graph G is a bilinear function of the feature vectors. The cost of any solution s can be expressed as the bilinear function s\u1d40LGs. The dimensions of the feature mappings \u03c8S, \u03c8I are polynomial in the description size of I for unweighted graphs with n nodes.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Combinatorial Optimization and Neural Networks", "md": "# Combinatorial Optimization and Neural Networks"}, {"type": "heading", "lvl": 2, "value": "Combinatorial Optimization and Neural Networks", "md": "## Combinatorial Optimization and Neural Networks"}, {"type": "text", "value": "Various research papers aim to understand the theoretical ability of neural networks to solve combinatorial optimization problems. Our paper builds on the framework of influential experimental work to tackle problems such as the Traveling Salesman Problem (TSP) using neural networks and reinforcement learning. Entropy regularization is used to design landscapes and obtain diversified solutions. Other works study the use of Transformer architectures combined with the Reinforce algorithm to improve generalization capability in Deep RL NCO.\n\nGradient Descent Dynamics: Our work provides theoretical understanding of the gradient-descent landscape in NCO problems. Similar studies have been done on the dynamics of gradient descent in neural networks, aiming to understand the power of this approach and which function classes can be learned.\n\nCombinatorial Applications: Concrete combinatorial problems like Min-Cut and Max-Cut are discussed. These problems involve splitting graph nodes into subsets to minimize or maximize the number of edges between subsets. Feature mappings are shown to exist for solutions and instances, making Theorem 1 applicable for any combinatorial task.", "md": "Various research papers aim to understand the theoretical ability of neural networks to solve combinatorial optimization problems. Our paper builds on the framework of influential experimental work to tackle problems such as the Traveling Salesman Problem (TSP) using neural networks and reinforcement learning. Entropy regularization is used to design landscapes and obtain diversified solutions. Other works study the use of Transformer architectures combined with the Reinforce algorithm to improve generalization capability in Deep RL NCO.\n\nGradient Descent Dynamics: Our work provides theoretical understanding of the gradient-descent landscape in NCO problems. Similar studies have been done on the dynamics of gradient descent in neural networks, aiming to understand the power of this approach and which function classes can be learned.\n\nCombinatorial Applications: Concrete combinatorial problems like Min-Cut and Max-Cut are discussed. These problems involve splitting graph nodes into subsets to minimize or maximize the number of edges between subsets. Feature mappings are shown to exist for solutions and instances, making Theorem 1 applicable for any combinatorial task."}, {"type": "heading", "lvl": 3, "value": "Min-Cut and Max-Cut", "md": "### Min-Cut and Max-Cut"}, {"type": "text", "value": "Min-Cut and Max-Cut are central graph combinatorial problems where the goal is to split graph nodes into subsets to minimize or maximize the number of edges between subsets. The Laplacian matrix LG = D - A is used to represent a graph G with n nodes. The cost of a solution vector s in Min-Cut or Max-Cut is expressed as s\u1d40LGs/4.\n\nWe show that there are natural feature mappings where the cost of every solution s under any graph G is a bilinear function of the feature vectors. The cost of any solution s can be expressed as the bilinear function s\u1d40LGs. The dimensions of the feature mappings \u03c8S, \u03c8I are polynomial in the description size of I for unweighted graphs with n nodes.", "md": "Min-Cut and Max-Cut are central graph combinatorial problems where the goal is to split graph nodes into subsets to minimize or maximize the number of edges between subsets. The Laplacian matrix LG = D - A is used to represent a graph G with n nodes. The cost of a solution vector s in Min-Cut or Max-Cut is expressed as s\u1d40LGs/4.\n\nWe show that there are natural feature mappings where the cost of every solution s under any graph G is a bilinear function of the feature vectors. The cost of any solution s can be expressed as the bilinear function s\u1d40LGs. The dimensions of the feature mappings \u03c8S, \u03c8I are polynomial in the description size of I for unweighted graphs with n nodes."}]}, {"page": 8, "text": "holds that \u2225\u03c8I(G)\u22252, \u2225\u03c8S(s)\u22252, \u2225M\u2225F \u2264        poly(n). Therefore, the constants DS, DI, C are polyno-\nmial in the description size of the instance family.\n    It remains to show that our solution feature mapping satisfies the variance preservation as-\nsumption, i.e., Item 3 in Assumption 1. A uniformly random solution vector s \u2208            {\u00b11}n is sam-\npled by setting each si = 1 with probability 1/2 independently. In that case, we have E[v \u00b7 x] = 0\nand therefore Var(v \u00b7 x) = E[(v \u00b7 x)2] = \u2211i\u0338=j vivj E[xixj] = \u2211i v2   i = \u2225v\u22252 2, since, by the indepen-\ndence of xi, xj, the cross-terms of the sum vanish. We observe that the same hold true for the\nMax-Cut problem and therefore, structured feature mappings exist for Max-Cut as well (where\nL(s; G) = \u2212s\u22a4LGs). We shortly mention that there also exist structured feature mappings for\nMax-k-CSP. We refer to Theorem 4 for further details.\nRemark 6 (Partial Instance Information/Instance Context). We remark that Assumption 1 allows for\nthe \u201cinstance\u201d I to only contain partial information about the actual cost function. For example, consider\nthe setting where each sampled instance is an unweighted graph G but the cost oracle takes the form\nO(G, s) = (LG)\u266dM(ss\u22a4)\u266d      for a matrix Mij = ai when i = j and Mij = 0 otherwise. This cost function\nmodels having a unknown weight function, i.e., the weight of edge i of G is ai if edge i exists in the\nobserved instance G, on the edges of the observed unweighted graph G, that the algorithm has to learn in\norder to be able to find the minimum or maximum cut. For simplicity, in what follows, we will continue\nreferring to I as the instance even though it may only contain partial information about the cost function\nof the underlying combinatorial problem.\nMaximum-Weight-Bipartite-Matching and TSP.              Maximum-Weight-Bipartite-Matching (MWBP)\nis another graph problem that, given a bipartite graph G with n nodes and m edges, asks for the\nmaximum-weight matching. The feature vector corresponding to a matching can be represented\nas a binary matrix R \u2208    {0, 1}n\u00d7n with \u2211j Rij = 1 for all i and \u2211i Rij = 1 for all j, i.e., R is a per-\nmutation matrix. Therefore, for a candidate matching s, we set \u03c8S(s) to be the matrix R defined\nabove. Moreover, the feature vector of the graph is the (negative flattened) adjacency matrix E\u266d.\nThe cost oracle is then L(R; E) = \u2211ij EijMijRij perhaps for an unknown weight matrix Mij (see\nRemark 6). For the Traveling Salesman Problem (TSP) the feature vector is again a matrix R with\nthe additional constraint that R has to represent a single cycle (a tour over all cities). The cost\nfunction for TSP is again L(R; E) = \u2211ij EijMijRij. One can check that those representations of\nthe instance and solution satisfy the assumptions of Items 1 and 4. Showing that the variance of\nthose representations has a polynomial lower bound is more subtle and we refer the reader to\nthe Supplementary Material.\n    We shortly mention that the solution generators for Min-Cut and Maximum-Weight-Bipartite-\nMatching are also efficiently samplable.\n3    Optimization Landscape\nExponential Families as Solution Generators.          A natural candidate to construct our family of\nsolution generators is to consider the distribution that assigns to each solution s \u2208     S and instance\nI \u2208  I mass proportional to its score exp(\u2212\u03c4L(s; I)) = exp(\u2212\u03c4\u03c8I(I)\u22a4M\u03c8S(s)) = exp(\u2212\u03c4z\u22a4Mx)\nfor some \u201ctemperature\u201d parameter \u03c4, where \u03c8I and \u03c8S are the feature mappings promised to\n                                                     8", "md": "holds that $$\\|\\psi_I(G)\\|^2, \\|\\psi_S(s)\\|^2, \\|M\\|_F \\leq \\text{poly}(n)$$. Therefore, the constants D_S, D_I, C are polynomial in the description size of the instance family.\n\nIt remains to show that our solution feature mapping satisfies the variance preservation assumption, i.e., Item 3 in Assumption 1. A uniformly random solution vector $$s \\in \\{ \\pm 1 \\}^n$$ is sampled by setting each $$s_i = 1$$ with probability 1/2 independently. In that case, we have $$E[v \\cdot x] = 0$$ and therefore $$\\text{Var}(v \\cdot x) = E[(v \\cdot x)^2] = \\sum_{i \\neq j} v_i v_j E[x_i x_j] = \\sum_i v_i^2 = \\|v\\|^2_2$$, since, by the independence of $$x_i, x_j$$, the cross-terms of the sum vanish. We observe that the same hold true for the Max-Cut problem and therefore, structured feature mappings exist for Max-Cut as well (where $$L(s; G) = -s^{\\top}LGs$$). We shortly mention that there also exist structured feature mappings for Max-k-CSP. We refer to Theorem 4 for further details.\n\nRemark 6 (Partial Instance Information/Instance Context). We remark that Assumption 1 allows for the \u201cinstance\u201d I to only contain partial information about the actual cost function. For example, consider the setting where each sampled instance is an unweighted graph G but the cost oracle takes the form $$O(G, s) = (LG)^{\\flat}M(ss^{\\top})^{\\flat}$$ for a matrix $$M_{ij} = a_i$$ when $$i = j$$ and $$M_{ij} = 0$$ otherwise. This cost function models having an unknown weight function, i.e., the weight of edge i of G is $$a_i$$ if edge i exists in the observed instance G, on the edges of the observed unweighted graph G, that the algorithm has to learn in order to be able to find the minimum or maximum cut. For simplicity, in what follows, we will continue referring to I as the instance even though it may only contain partial information about the cost function of the underlying combinatorial problem.\n\nMaximum-Weight-Bipartite-Matching and TSP. Maximum-Weight-Bipartite-Matching (MWBP) is another graph problem that, given a bipartite graph G with n nodes and m edges, asks for the maximum-weight matching. The feature vector corresponding to a matching can be represented as a binary matrix $$R \\in \\{0, 1\\}^{n \\times n}$$ with $$\\sum_j R_{ij} = 1$$ for all i and $$\\sum_i R_{ij} = 1$$ for all j, i.e., R is a permutation matrix. Therefore, for a candidate matching s, we set $$\\psi_S(s)$$ to be the matrix R defined above. Moreover, the feature vector of the graph is the (negative flattened) adjacency matrix $$E^{\\flat}$$. The cost oracle is then $$L(R; E) = \\sum_{ij} E_{ij}M_{ij}R_{ij}$$ perhaps for an unknown weight matrix $$M_{ij}$$ (see Remark 6). For the Traveling Salesman Problem (TSP) the feature vector is again a matrix R with the additional constraint that R has to represent a single cycle (a tour over all cities). The cost function for TSP is again $$L(R; E) = \\sum_{ij} E_{ij}M_{ij}R_{ij}$$. One can check that those representations of the instance and solution satisfy the assumptions of Items 1 and 4. Showing that the variance of those representations has a polynomial lower bound is more subtle and we refer the reader to the Supplementary Material.\n\nWe shortly mention that the solution generators for Min-Cut and Maximum-Weight-Bipartite-Matching are also efficiently samplable.\n\n### Optimization Landscape\n\nExponential Families as Solution Generators. A natural candidate to construct our family of solution generators is to consider the distribution that assigns to each solution $$s \\in S$$ and instance $$I \\in I$$ mass proportional to its score $$\\exp(-\\tau L(s; I)) = \\exp(-\\tau \\psi_I(I)^{\\top}M\\psi_S(s)) = \\exp(-\\tau z^{\\top}Mx$$ for some \u201ctemperature\u201d parameter $$\\tau$$, where $$\\psi_I$$ and $$\\psi_S$$ are the feature mappings promised to", "images": [], "items": [{"type": "text", "value": "holds that $$\\|\\psi_I(G)\\|^2, \\|\\psi_S(s)\\|^2, \\|M\\|_F \\leq \\text{poly}(n)$$. Therefore, the constants D_S, D_I, C are polynomial in the description size of the instance family.\n\nIt remains to show that our solution feature mapping satisfies the variance preservation assumption, i.e., Item 3 in Assumption 1. A uniformly random solution vector $$s \\in \\{ \\pm 1 \\}^n$$ is sampled by setting each $$s_i = 1$$ with probability 1/2 independently. In that case, we have $$E[v \\cdot x] = 0$$ and therefore $$\\text{Var}(v \\cdot x) = E[(v \\cdot x)^2] = \\sum_{i \\neq j} v_i v_j E[x_i x_j] = \\sum_i v_i^2 = \\|v\\|^2_2$$, since, by the independence of $$x_i, x_j$$, the cross-terms of the sum vanish. We observe that the same hold true for the Max-Cut problem and therefore, structured feature mappings exist for Max-Cut as well (where $$L(s; G) = -s^{\\top}LGs$$). We shortly mention that there also exist structured feature mappings for Max-k-CSP. We refer to Theorem 4 for further details.\n\nRemark 6 (Partial Instance Information/Instance Context). We remark that Assumption 1 allows for the \u201cinstance\u201d I to only contain partial information about the actual cost function. For example, consider the setting where each sampled instance is an unweighted graph G but the cost oracle takes the form $$O(G, s) = (LG)^{\\flat}M(ss^{\\top})^{\\flat}$$ for a matrix $$M_{ij} = a_i$$ when $$i = j$$ and $$M_{ij} = 0$$ otherwise. This cost function models having an unknown weight function, i.e., the weight of edge i of G is $$a_i$$ if edge i exists in the observed instance G, on the edges of the observed unweighted graph G, that the algorithm has to learn in order to be able to find the minimum or maximum cut. For simplicity, in what follows, we will continue referring to I as the instance even though it may only contain partial information about the cost function of the underlying combinatorial problem.\n\nMaximum-Weight-Bipartite-Matching and TSP. Maximum-Weight-Bipartite-Matching (MWBP) is another graph problem that, given a bipartite graph G with n nodes and m edges, asks for the maximum-weight matching. The feature vector corresponding to a matching can be represented as a binary matrix $$R \\in \\{0, 1\\}^{n \\times n}$$ with $$\\sum_j R_{ij} = 1$$ for all i and $$\\sum_i R_{ij} = 1$$ for all j, i.e., R is a permutation matrix. Therefore, for a candidate matching s, we set $$\\psi_S(s)$$ to be the matrix R defined above. Moreover, the feature vector of the graph is the (negative flattened) adjacency matrix $$E^{\\flat}$$. The cost oracle is then $$L(R; E) = \\sum_{ij} E_{ij}M_{ij}R_{ij}$$ perhaps for an unknown weight matrix $$M_{ij}$$ (see Remark 6). For the Traveling Salesman Problem (TSP) the feature vector is again a matrix R with the additional constraint that R has to represent a single cycle (a tour over all cities). The cost function for TSP is again $$L(R; E) = \\sum_{ij} E_{ij}M_{ij}R_{ij}$$. One can check that those representations of the instance and solution satisfy the assumptions of Items 1 and 4. Showing that the variance of those representations has a polynomial lower bound is more subtle and we refer the reader to the Supplementary Material.\n\nWe shortly mention that the solution generators for Min-Cut and Maximum-Weight-Bipartite-Matching are also efficiently samplable.", "md": "holds that $$\\|\\psi_I(G)\\|^2, \\|\\psi_S(s)\\|^2, \\|M\\|_F \\leq \\text{poly}(n)$$. Therefore, the constants D_S, D_I, C are polynomial in the description size of the instance family.\n\nIt remains to show that our solution feature mapping satisfies the variance preservation assumption, i.e., Item 3 in Assumption 1. A uniformly random solution vector $$s \\in \\{ \\pm 1 \\}^n$$ is sampled by setting each $$s_i = 1$$ with probability 1/2 independently. In that case, we have $$E[v \\cdot x] = 0$$ and therefore $$\\text{Var}(v \\cdot x) = E[(v \\cdot x)^2] = \\sum_{i \\neq j} v_i v_j E[x_i x_j] = \\sum_i v_i^2 = \\|v\\|^2_2$$, since, by the independence of $$x_i, x_j$$, the cross-terms of the sum vanish. We observe that the same hold true for the Max-Cut problem and therefore, structured feature mappings exist for Max-Cut as well (where $$L(s; G) = -s^{\\top}LGs$$). We shortly mention that there also exist structured feature mappings for Max-k-CSP. We refer to Theorem 4 for further details.\n\nRemark 6 (Partial Instance Information/Instance Context). We remark that Assumption 1 allows for the \u201cinstance\u201d I to only contain partial information about the actual cost function. For example, consider the setting where each sampled instance is an unweighted graph G but the cost oracle takes the form $$O(G, s) = (LG)^{\\flat}M(ss^{\\top})^{\\flat}$$ for a matrix $$M_{ij} = a_i$$ when $$i = j$$ and $$M_{ij} = 0$$ otherwise. This cost function models having an unknown weight function, i.e., the weight of edge i of G is $$a_i$$ if edge i exists in the observed instance G, on the edges of the observed unweighted graph G, that the algorithm has to learn in order to be able to find the minimum or maximum cut. For simplicity, in what follows, we will continue referring to I as the instance even though it may only contain partial information about the cost function of the underlying combinatorial problem.\n\nMaximum-Weight-Bipartite-Matching and TSP. Maximum-Weight-Bipartite-Matching (MWBP) is another graph problem that, given a bipartite graph G with n nodes and m edges, asks for the maximum-weight matching. The feature vector corresponding to a matching can be represented as a binary matrix $$R \\in \\{0, 1\\}^{n \\times n}$$ with $$\\sum_j R_{ij} = 1$$ for all i and $$\\sum_i R_{ij} = 1$$ for all j, i.e., R is a permutation matrix. Therefore, for a candidate matching s, we set $$\\psi_S(s)$$ to be the matrix R defined above. Moreover, the feature vector of the graph is the (negative flattened) adjacency matrix $$E^{\\flat}$$. The cost oracle is then $$L(R; E) = \\sum_{ij} E_{ij}M_{ij}R_{ij}$$ perhaps for an unknown weight matrix $$M_{ij}$$ (see Remark 6). For the Traveling Salesman Problem (TSP) the feature vector is again a matrix R with the additional constraint that R has to represent a single cycle (a tour over all cities). The cost function for TSP is again $$L(R; E) = \\sum_{ij} E_{ij}M_{ij}R_{ij}$$. One can check that those representations of the instance and solution satisfy the assumptions of Items 1 and 4. Showing that the variance of those representations has a polynomial lower bound is more subtle and we refer the reader to the Supplementary Material.\n\nWe shortly mention that the solution generators for Min-Cut and Maximum-Weight-Bipartite-Matching are also efficiently samplable."}, {"type": "heading", "lvl": 3, "value": "Optimization Landscape", "md": "### Optimization Landscape"}, {"type": "text", "value": "Exponential Families as Solution Generators. A natural candidate to construct our family of solution generators is to consider the distribution that assigns to each solution $$s \\in S$$ and instance $$I \\in I$$ mass proportional to its score $$\\exp(-\\tau L(s; I)) = \\exp(-\\tau \\psi_I(I)^{\\top}M\\psi_S(s)) = \\exp(-\\tau z^{\\top}Mx$$ for some \u201ctemperature\u201d parameter $$\\tau$$, where $$\\psi_I$$ and $$\\psi_S$$ are the feature mappings promised to", "md": "Exponential Families as Solution Generators. A natural candidate to construct our family of solution generators is to consider the distribution that assigns to each solution $$s \\in S$$ and instance $$I \\in I$$ mass proportional to its score $$\\exp(-\\tau L(s; I)) = \\exp(-\\tau \\psi_I(I)^{\\top}M\\psi_S(s)) = \\exp(-\\tau z^{\\top}Mx$$ for some \u201ctemperature\u201d parameter $$\\tau$$, where $$\\psi_I$$ and $$\\psi_S$$ are the feature mappings promised to"}]}, {"page": 9, "text": "       Figure 1: In the left plot, we show the landscape of the \u201cvanilla\u201d objective of Equation (1) for\n       the feature domain X = {(1, 0), (2, 2), (0, 2)} and linear cost oracle c \u00b7 x for c = (\u22123, \u22123). We see\n       that the \u201cvanilla\u201d objective is minimized at the direction of \u2212c, i.e., along the direction \u03c4(1, 1)\n       for \u03c4 \u2192    +\u221e. We observe the two issues described in Section 3, i.e., that the true minimizer\n       is a point at infinity, and that gradients vanish so gradient descent may get trapped in sub-\n       optimal solutions, (e.g., in the upper-right corner if initialized in the top corner). In the middle\n       plot, we show the landscape of the entropy-regularized objective of Equation (3) that makes\n       the minimizer finite and brings it closer to the origin. Observe that even if a gradient iteration\n       is initialized in the top corner it will eventually converge to the minimizer; however the rate\n       of convergence may be very slow. The right plot corresponds to the loss objective where we\n       combine a mixture of exponential families as solution generator, as in Equation (5), and the\n       entropy regularization approach.             We observe that we are able to obtain a benign (quasar-\n       convex) landscape via the entropy regularization while the mixture-generator guarantees non-\n       vanishing gradients.\nexist due to Assumption 1, z = \u03c8I(I), and, x = \u03c8S(s). Note that as long as \u03c4 \u2192                                       +\u221e, this\ndistribution tends to concentrate on solutions that achieve small loss.\nRemark 7. To construct the above solution sampler one could artificially query specific solutions to the\ncost oracle of Definition 1 and try to learn the cost matrix M. However, we remark that our goal (see\nDefinition 2) is to show that we can train a parametric family via gradient-based methods so that it\ngenerates (approximately) optimal solutions and not to simply learn the cost matrix M via some other\nmethod and then use it to generate good solutions.\nObstacle I: Minimizers at Infinity.                One could naturally consider the parametric family \u03d5(x; z; W) \u221d\nexp(z\u22a4Wx) (note that with W = \u2212\u03c4M, we recover the distribution of the previous paragraph)\nand try to perform gradient-based methods on the loss (recall that L(x; z) = z\u22a4Mx)1\n                                              L(W) = E               E                                                        (1)\n                                                           z\u223cR  x\u223c\u03d5(\u00b7;z;W)[z\u22a4Mx] .\nThe question is whether gradient updates on the parameter W eventually converge to a matrix W\nwhose associated distribution \u03d5(W) generates near-optimal solutions (note that the matrix \u2212\u03c4M\nwith \u03c4 \u2192      +\u221e    is such a solution). After computing the gradient of L, we observe that\n   1                               \u2207WL(W) \u00b7 M = Varz\u223cR,x\u223c\u03d5(\u00b7;z;W)[z\u22a4Mx] \u2265                      0 ,\n     We note that we overload the notation and assume that our distributions generate directly the featurizations z\n(resp. x) of I (resp. s).\n                                                                  9", "md": "# Math Equations and Text\n\n## Figure 1:\n\nIn the left plot, we show the landscape of the \u201cvanilla\u201d objective of Equation (1) for the feature domain X = {(1, 0), (2, 2), (0, 2)} and linear cost oracle c \u00b7 x for c = (-3, -3). We see that the \u201cvanilla\u201d objective is minimized at the direction of -c, i.e., along the direction $$\\tau(1, 1)$$ for $$\\tau \\rightarrow +\\infty$$. We observe the two issues described in Section 3, i.e., that the true minimizer is a point at infinity, and that gradients vanish so gradient descent may get trapped in sub-optimal solutions, (e.g., in the upper-right corner if initialized in the top corner). In the middle plot, we show the landscape of the entropy-regularized objective of Equation (3) that makes the minimizer finite and brings it closer to the origin. Observe that even if a gradient iteration is initialized in the top corner it will eventually converge to the minimizer; however the rate of convergence may be very slow. The right plot corresponds to the loss objective where we combine a mixture of exponential families as solution generator, as in Equation (5), and the entropy regularization approach. We observe that we are able to obtain a benign (quasar-convex) landscape via the entropy regularization while the mixture-generator guarantees non-vanishing gradients.\n\nExist due to Assumption 1, z = \u03c8I(I), and, x = \u03c8S(s). Note that as long as $$\\tau \\rightarrow +\\infty$$, this distribution tends to concentrate on solutions that achieve small loss.\n\n### Remark 7:\n\nTo construct the above solution sampler one could artificially query specific solutions to the cost oracle of Definition 1 and try to learn the cost matrix M. However, we remark that our goal (see Definition 2) is to show that we can train a parametric family via gradient-based methods so that it generates (approximately) optimal solutions and not to simply learn the cost matrix M via some other method and then use it to generate good solutions.\n\n### Obstacle I: Minimizers at Infinity.\n\nOne could naturally consider the parametric family $$\\phi(x; z; W) \\propto e^{z^TWx}$$ (note that with W = -\u03c4M, we recover the distribution of the previous paragraph) and try to perform gradient-based methods on the loss (recall that $$L(x; z) = z^TMx$$):\n\n$$L(W) = E[E[z \\sim R, x \\sim \\phi(\\cdot; z; W)[z^TMx]]$$\n\nThe question is whether gradient updates on the parameter W eventually converge to a matrix W whose associated distribution $$\\phi(W)$$ generates near-optimal solutions (note that the matrix -\u03c4M with $$\\tau \\rightarrow +\\infty$$ is such a solution). After computing the gradient of L, we observe that:\n\n$$\\nabla_W L(W) \\cdot M = Var[z \\sim R, x \\sim \\phi(\\cdot; z; W)[z^TMx] \\geq 0$$\n\nWe note that we overload the notation and assume that our distributions generate directly the featurizations z (resp. x) of I (resp. s).", "images": [{"name": "page-9-0.jpg", "height": 91, "width": 141, "x": 85, "y": 76}, {"name": "page-9-1.jpg", "height": 95, "width": 141, "x": 228, "y": 72}, {"name": "page-9-2.jpg", "height": 94, "width": 141, "x": 372, "y": 74}], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Figure 1:", "md": "## Figure 1:"}, {"type": "text", "value": "In the left plot, we show the landscape of the \u201cvanilla\u201d objective of Equation (1) for the feature domain X = {(1, 0), (2, 2), (0, 2)} and linear cost oracle c \u00b7 x for c = (-3, -3). We see that the \u201cvanilla\u201d objective is minimized at the direction of -c, i.e., along the direction $$\\tau(1, 1)$$ for $$\\tau \\rightarrow +\\infty$$. We observe the two issues described in Section 3, i.e., that the true minimizer is a point at infinity, and that gradients vanish so gradient descent may get trapped in sub-optimal solutions, (e.g., in the upper-right corner if initialized in the top corner). In the middle plot, we show the landscape of the entropy-regularized objective of Equation (3) that makes the minimizer finite and brings it closer to the origin. Observe that even if a gradient iteration is initialized in the top corner it will eventually converge to the minimizer; however the rate of convergence may be very slow. The right plot corresponds to the loss objective where we combine a mixture of exponential families as solution generator, as in Equation (5), and the entropy regularization approach. We observe that we are able to obtain a benign (quasar-convex) landscape via the entropy regularization while the mixture-generator guarantees non-vanishing gradients.\n\nExist due to Assumption 1, z = \u03c8I(I), and, x = \u03c8S(s). Note that as long as $$\\tau \\rightarrow +\\infty$$, this distribution tends to concentrate on solutions that achieve small loss.", "md": "In the left plot, we show the landscape of the \u201cvanilla\u201d objective of Equation (1) for the feature domain X = {(1, 0), (2, 2), (0, 2)} and linear cost oracle c \u00b7 x for c = (-3, -3). We see that the \u201cvanilla\u201d objective is minimized at the direction of -c, i.e., along the direction $$\\tau(1, 1)$$ for $$\\tau \\rightarrow +\\infty$$. We observe the two issues described in Section 3, i.e., that the true minimizer is a point at infinity, and that gradients vanish so gradient descent may get trapped in sub-optimal solutions, (e.g., in the upper-right corner if initialized in the top corner). In the middle plot, we show the landscape of the entropy-regularized objective of Equation (3) that makes the minimizer finite and brings it closer to the origin. Observe that even if a gradient iteration is initialized in the top corner it will eventually converge to the minimizer; however the rate of convergence may be very slow. The right plot corresponds to the loss objective where we combine a mixture of exponential families as solution generator, as in Equation (5), and the entropy regularization approach. We observe that we are able to obtain a benign (quasar-convex) landscape via the entropy regularization while the mixture-generator guarantees non-vanishing gradients.\n\nExist due to Assumption 1, z = \u03c8I(I), and, x = \u03c8S(s). Note that as long as $$\\tau \\rightarrow +\\infty$$, this distribution tends to concentrate on solutions that achieve small loss."}, {"type": "heading", "lvl": 3, "value": "Remark 7:", "md": "### Remark 7:"}, {"type": "text", "value": "To construct the above solution sampler one could artificially query specific solutions to the cost oracle of Definition 1 and try to learn the cost matrix M. However, we remark that our goal (see Definition 2) is to show that we can train a parametric family via gradient-based methods so that it generates (approximately) optimal solutions and not to simply learn the cost matrix M via some other method and then use it to generate good solutions.", "md": "To construct the above solution sampler one could artificially query specific solutions to the cost oracle of Definition 1 and try to learn the cost matrix M. However, we remark that our goal (see Definition 2) is to show that we can train a parametric family via gradient-based methods so that it generates (approximately) optimal solutions and not to simply learn the cost matrix M via some other method and then use it to generate good solutions."}, {"type": "heading", "lvl": 3, "value": "Obstacle I: Minimizers at Infinity.", "md": "### Obstacle I: Minimizers at Infinity."}, {"type": "text", "value": "One could naturally consider the parametric family $$\\phi(x; z; W) \\propto e^{z^TWx}$$ (note that with W = -\u03c4M, we recover the distribution of the previous paragraph) and try to perform gradient-based methods on the loss (recall that $$L(x; z) = z^TMx$$):\n\n$$L(W) = E[E[z \\sim R, x \\sim \\phi(\\cdot; z; W)[z^TMx]]$$\n\nThe question is whether gradient updates on the parameter W eventually converge to a matrix W whose associated distribution $$\\phi(W)$$ generates near-optimal solutions (note that the matrix -\u03c4M with $$\\tau \\rightarrow +\\infty$$ is such a solution). After computing the gradient of L, we observe that:\n\n$$\\nabla_W L(W) \\cdot M = Var[z \\sim R, x \\sim \\phi(\\cdot; z; W)[z^TMx] \\geq 0$$\n\nWe note that we overload the notation and assume that our distributions generate directly the featurizations z (resp. x) of I (resp. s).", "md": "One could naturally consider the parametric family $$\\phi(x; z; W) \\propto e^{z^TWx}$$ (note that with W = -\u03c4M, we recover the distribution of the previous paragraph) and try to perform gradient-based methods on the loss (recall that $$L(x; z) = z^TMx$$):\n\n$$L(W) = E[E[z \\sim R, x \\sim \\phi(\\cdot; z; W)[z^TMx]]$$\n\nThe question is whether gradient updates on the parameter W eventually converge to a matrix W whose associated distribution $$\\phi(W)$$ generates near-optimal solutions (note that the matrix -\u03c4M with $$\\tau \\rightarrow +\\infty$$ is such a solution). After computing the gradient of L, we observe that:\n\n$$\\nabla_W L(W) \\cdot M = Var[z \\sim R, x \\sim \\phi(\\cdot; z; W)[z^TMx] \\geq 0$$\n\nWe note that we overload the notation and assume that our distributions generate directly the featurizations z (resp. x) of I (resp. s)."}]}, {"page": 10, "text": "where the inner product between two matrices A \u00b7 B is the trace Tr(A\u22a4B) = \u2211i,j AijBij. This\nmeans that the gradient field of L always has a contribution to the direction of M. Nevertheless\nthe actual minimizer is at infinity, i.e., it corresponds to the point W = \u2212\u03c4M when \u03c4 \u2192                     +\u221e.\nWhile the correlation with the optimal point is positive (which is encouraging), having such con-\ntribution to this direction is not a sufficient condition for actually reaching W. The objective has\nvanishing gradients at infinity and gradient descent may get trapped in sub-optimal stationary\npoints, see the left plot in Figure 1.\nSolution I: Quasar Convexity via Entropy Regularization.                  Our plan is to try and make the\nobjective landscape more benign by adding an entropy-regularizer. Instead of trying to make\nthe objective convex (which may be too much to ask in the first place) we are able obtain a much\nbetter landscape with a finite global minimizer and a gradient field that guides gradient descent to the\nminimizer. Those properties are described by the so-called class of \u201cquasar-convex\u201d functions.\nQuasar convexity (or weak quasi-convexity [HMR16]) is a well-studied notion in optimization\n[HMR16, HSS20, LV16, ZMB+17, HLSS15] and can be considered as a high-dimensional general-\nization of unimodality.\nDefinition 3 (Quasar Convexity [HMR16, HSS20]). Let \u03b3 \u2208                 (0, 1] and let x be a minimizer of the\ndifferentiable function f : Rn \u2192    R. The function f is \u03b3-quasar-convex with respect to x on a domain\nD \u2286   Rn if for all x \u2208 D, \u2207f (x) \u00b7 (x \u2212    x) \u2265  \u03b3( f (x) \u2212 f (x)).\n    In the above definition, notice that the main property that we need to establish is that the\ngradient field of our objective correlates positively with the direction W \u2212               W, where W is its\nminimizer. We denote by H : W \u2192           R the negative entropy of \u03d5(W), i.e.,\n                                   H(W) = E           E                                                       (2)\nand consider the regularized objective        z\u223cR x\u223c\u03d5(\u00b7;z;W)[log \u03d5(x; z; W)] ,\n                                         L\u03bb(W) = L(W) + \u03bbH(W) ,                                               (3)\nfor some \u03bb > 0. We show (follows from Lemma 4) that the gradient-field of the regularized\nobjective indeed \u201cpoints\u201d towards a finite minimizer (the matrix W = \u2212M/\u03bb):\n                                   \u2207WL\u03bb(W) \u00b7 (W + M/\u03bb) =\n                                                Var[z\u22a4(W + M/\u03bb)x] \u2265         0 ,                               (4)\nwhere the randomness is over z \u223c           R, x \u223c    \u03d5(\u00b7; z; W). Observe that now the minimizer of L\u03bb\nis the point \u2212M/\u03bb, which for \u03bb = poly(\u03f5, \u03b1, 1/C, 1/DS, 1/DI) (these are the parameters of\nAssumption 1) is promised to yield a solution sampler that generates \u03f5-sub-optimal solutions (see\nalso Proposition 2 and Appendix C). Having the property of Equation (4) suffices for showing\nthat a gradient descent iteration (with an appropriately small step-size) will eventually converge\nto the minimizer.\n                                                       10", "md": "where the inner product between two matrices $$A \\cdot B$$ is the trace $$\\text{Tr}(A^\\top B) = \\sum_{i,j} A_{ij}B_{ij}$$. This means that the gradient field of L always has a contribution to the direction of M. Nevertheless, the actual minimizer is at infinity, i.e., it corresponds to the point $$W = -\\tau M$$ when $$\\tau \\rightarrow +\\infty$$. While the correlation with the optimal point is positive (which is encouraging), having such contribution to this direction is not a sufficient condition for actually reaching W. The objective has vanishing gradients at infinity and gradient descent may get trapped in sub-optimal stationary points, see the left plot in Figure 1.\n\nSolution I: Quasar Convexity via Entropy Regularization. Our plan is to try and make the objective landscape more benign by adding an entropy-regularizer. Instead of trying to make the objective convex (which may be too much to ask in the first place), we are able to obtain a much better landscape with a finite global minimizer and a gradient field that guides gradient descent to the minimizer. Those properties are described by the so-called class of \u201cquasar-convex\u201d functions.\n\nQuasar convexity (or weak quasi-convexity [HMR16]) is a well-studied notion in optimization [HMR16, HSS20, LV16, ZMB+17, HLSS15] and can be considered as a high-dimensional generalization of unimodality.\n\nDefinition 3 (Quasar Convexity [HMR16, HSS20]). Let $$\\gamma \\in (0, 1]$$ and let x be a minimizer of the differentiable function $$f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$$. The function f is $$\\gamma$$-quasar-convex with respect to x on a domain $$D \\subseteq \\mathbb{R}^n$$ if for all $$x \\in D$$, $$\\nabla f(x) \\cdot (x - x) \\geq \\gamma(f(x) - f(x))$$.\n\nIn the above definition, notice that the main property that we need to establish is that the gradient field of our objective correlates positively with the direction $$W - W$$, where W is its minimizer. We denote by $$H : W \\rightarrow \\mathbb{R}$$ the negative entropy of $$\\phi(W)$$, i.e.,\n\n$$H(W) = E[E]$$\n\nand consider the regularized objective\n\n$$L_\\lambda(W) = L(W) + \\lambda H(W)$$\n\nfor some $$\\lambda > 0$$. We show (follows from Lemma 4) that the gradient-field of the regularized objective indeed \u201cpoints\u201d towards a finite minimizer (the matrix $$W = -M/\\lambda$$):\n\n$$\\nabla W L_\\lambda(W) \\cdot (W + M/\\lambda) = \\text{Var}[z^\\top(W + M/\\lambda)x] \\geq 0$$\n\nwhere the randomness is over $$z \\sim R$$, $$x \\sim \\phi(\\cdot; z; W)$$. Observe that now the minimizer of $$L_\\lambda$$ is the point $$-M/\\lambda$$, which for $$\\lambda = \\text{poly}(\\epsilon, \\alpha, 1/C, 1/DS, 1/DI)$$ (these are the parameters of Assumption 1) is promised to yield a solution sampler that generates $$\\epsilon$$-sub-optimal solutions (see also Proposition 2 and Appendix C). Having the property of Equation (4) suffices for showing that a gradient descent iteration (with an appropriately small step-size) will eventually converge to the minimizer.\n\n10", "images": [], "items": [{"type": "text", "value": "where the inner product between two matrices $$A \\cdot B$$ is the trace $$\\text{Tr}(A^\\top B) = \\sum_{i,j} A_{ij}B_{ij}$$. This means that the gradient field of L always has a contribution to the direction of M. Nevertheless, the actual minimizer is at infinity, i.e., it corresponds to the point $$W = -\\tau M$$ when $$\\tau \\rightarrow +\\infty$$. While the correlation with the optimal point is positive (which is encouraging), having such contribution to this direction is not a sufficient condition for actually reaching W. The objective has vanishing gradients at infinity and gradient descent may get trapped in sub-optimal stationary points, see the left plot in Figure 1.\n\nSolution I: Quasar Convexity via Entropy Regularization. Our plan is to try and make the objective landscape more benign by adding an entropy-regularizer. Instead of trying to make the objective convex (which may be too much to ask in the first place), we are able to obtain a much better landscape with a finite global minimizer and a gradient field that guides gradient descent to the minimizer. Those properties are described by the so-called class of \u201cquasar-convex\u201d functions.\n\nQuasar convexity (or weak quasi-convexity [HMR16]) is a well-studied notion in optimization [HMR16, HSS20, LV16, ZMB+17, HLSS15] and can be considered as a high-dimensional generalization of unimodality.\n\nDefinition 3 (Quasar Convexity [HMR16, HSS20]). Let $$\\gamma \\in (0, 1]$$ and let x be a minimizer of the differentiable function $$f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$$. The function f is $$\\gamma$$-quasar-convex with respect to x on a domain $$D \\subseteq \\mathbb{R}^n$$ if for all $$x \\in D$$, $$\\nabla f(x) \\cdot (x - x) \\geq \\gamma(f(x) - f(x))$$.\n\nIn the above definition, notice that the main property that we need to establish is that the gradient field of our objective correlates positively with the direction $$W - W$$, where W is its minimizer. We denote by $$H : W \\rightarrow \\mathbb{R}$$ the negative entropy of $$\\phi(W)$$, i.e.,\n\n$$H(W) = E[E]$$\n\nand consider the regularized objective\n\n$$L_\\lambda(W) = L(W) + \\lambda H(W)$$\n\nfor some $$\\lambda > 0$$. We show (follows from Lemma 4) that the gradient-field of the regularized objective indeed \u201cpoints\u201d towards a finite minimizer (the matrix $$W = -M/\\lambda$$):\n\n$$\\nabla W L_\\lambda(W) \\cdot (W + M/\\lambda) = \\text{Var}[z^\\top(W + M/\\lambda)x] \\geq 0$$\n\nwhere the randomness is over $$z \\sim R$$, $$x \\sim \\phi(\\cdot; z; W)$$. Observe that now the minimizer of $$L_\\lambda$$ is the point $$-M/\\lambda$$, which for $$\\lambda = \\text{poly}(\\epsilon, \\alpha, 1/C, 1/DS, 1/DI)$$ (these are the parameters of Assumption 1) is promised to yield a solution sampler that generates $$\\epsilon$$-sub-optimal solutions (see also Proposition 2 and Appendix C). Having the property of Equation (4) suffices for showing that a gradient descent iteration (with an appropriately small step-size) will eventually converge to the minimizer.\n\n10", "md": "where the inner product between two matrices $$A \\cdot B$$ is the trace $$\\text{Tr}(A^\\top B) = \\sum_{i,j} A_{ij}B_{ij}$$. This means that the gradient field of L always has a contribution to the direction of M. Nevertheless, the actual minimizer is at infinity, i.e., it corresponds to the point $$W = -\\tau M$$ when $$\\tau \\rightarrow +\\infty$$. While the correlation with the optimal point is positive (which is encouraging), having such contribution to this direction is not a sufficient condition for actually reaching W. The objective has vanishing gradients at infinity and gradient descent may get trapped in sub-optimal stationary points, see the left plot in Figure 1.\n\nSolution I: Quasar Convexity via Entropy Regularization. Our plan is to try and make the objective landscape more benign by adding an entropy-regularizer. Instead of trying to make the objective convex (which may be too much to ask in the first place), we are able to obtain a much better landscape with a finite global minimizer and a gradient field that guides gradient descent to the minimizer. Those properties are described by the so-called class of \u201cquasar-convex\u201d functions.\n\nQuasar convexity (or weak quasi-convexity [HMR16]) is a well-studied notion in optimization [HMR16, HSS20, LV16, ZMB+17, HLSS15] and can be considered as a high-dimensional generalization of unimodality.\n\nDefinition 3 (Quasar Convexity [HMR16, HSS20]). Let $$\\gamma \\in (0, 1]$$ and let x be a minimizer of the differentiable function $$f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$$. The function f is $$\\gamma$$-quasar-convex with respect to x on a domain $$D \\subseteq \\mathbb{R}^n$$ if for all $$x \\in D$$, $$\\nabla f(x) \\cdot (x - x) \\geq \\gamma(f(x) - f(x))$$.\n\nIn the above definition, notice that the main property that we need to establish is that the gradient field of our objective correlates positively with the direction $$W - W$$, where W is its minimizer. We denote by $$H : W \\rightarrow \\mathbb{R}$$ the negative entropy of $$\\phi(W)$$, i.e.,\n\n$$H(W) = E[E]$$\n\nand consider the regularized objective\n\n$$L_\\lambda(W) = L(W) + \\lambda H(W)$$\n\nfor some $$\\lambda > 0$$. We show (follows from Lemma 4) that the gradient-field of the regularized objective indeed \u201cpoints\u201d towards a finite minimizer (the matrix $$W = -M/\\lambda$$):\n\n$$\\nabla W L_\\lambda(W) \\cdot (W + M/\\lambda) = \\text{Var}[z^\\top(W + M/\\lambda)x] \\geq 0$$\n\nwhere the randomness is over $$z \\sim R$$, $$x \\sim \\phi(\\cdot; z; W)$$. Observe that now the minimizer of $$L_\\lambda$$ is the point $$-M/\\lambda$$, which for $$\\lambda = \\text{poly}(\\epsilon, \\alpha, 1/C, 1/DS, 1/DI)$$ (these are the parameters of Assumption 1) is promised to yield a solution sampler that generates $$\\epsilon$$-sub-optimal solutions (see also Proposition 2 and Appendix C). Having the property of Equation (4) suffices for showing that a gradient descent iteration (with an appropriately small step-size) will eventually converge to the minimizer.\n\n10"}]}, {"page": 11, "text": "Obstacle II: Vanishing Gradients.                                   While we have established that the gradient field of the\nregularized objective \u201cpoints\u201d towards the right direction, the regularized objective still suffers\nfrom vanishing gradients, see the middle plot in Figure 1. In other words, \u03b3 in the definition of\nquasar convexity (Definition 3) may be exponentially small, as it is proportional to the variance of\nthe random variable z\u22a4(W + M/\u03bb)x, see Equation (4). As we see in the middle plot of Figure 1,\nthe main issue is the vanishing gradient when W gets closer to the minimizer \u2212M/\u03bb (towards\nthe front-corner). For simplicity, consider the variance along the direction of W, i.e., Var[z\u22a4Wx]\nand recall that x is generated by the density exp(z\u22a4Wx)/(\u2211x\u2208X exp(z\u22a4Wx)). When \u2225W\u22252 \u2192                                                               +\u221e\nwe observe that the value z\u22a4Wx concentrates exponentially fast to maxx\u2208X z\u22a4Wx (think of the\nconvergence of the soft-max to the max function). Therefore, the variance Var[z\u22a4Wx] may vanish\nexponentially fast making the convergence of gradient descent slow.\nSolution II: Non-Vanishing Gradients via Fast/Slow Mixture Generators.                                                         We propose a fix to\nthe vanishing gradients issue by using a mixture of exponential families as a solution generator.\nWe define the family of solution generators P = {p(W) : W \u2208                                                     W} to be\n                                              P = {(1 \u2212           \u03b2\u22c6)\u03d5(W) + \u03b2\u22c6\u03d5(\u03c1\u22c6W) : W \u2208                           W} ,                             (5)\nfor a (fixed) mixing parameter \u03b2\u22c6                             and a (fixed) temperature parameter \u03c1\u22c6. The main idea is to\nhave the first component of the mixture to converge fast to the optimal solution (to \u2212M/\u03bb) while\nthe second \u201cslow\u201d component that has parameter \u03c1\u22c6W stays closer to the uniform distribution\nover solutions that guarantees non-trivial variance (and therefore non-vanishing gradients).\n       More precisely, taking \u03c1\u22c6                      to be sufficiently small, the distribution \u03d5(\u03c1\u22c6W) is almost uniform\nover the solution space \u03c8S(S). Therefore, in Equation (4), the almost uniform distribution com-\nponent of the mixture will add to the variance and allow us to show a lower bound. This is\nwhere Item 3 of Assumption 1 comes into play and gives us the desired non-trivial variance\nlower bound under the uniform distribution. We view this fast/slow mixture technique as an\ninteresting insight of our work: we use the \u201cfast\u201d component (the one with parameter W) to\nactually reach the optimal solution \u2212M/\u03bb and and we use the \u201cslow\u201d component (the one with\nparameter \u03c1\u22c6W that essentially generates random solutions) to preserve a non-trivial variance\nlower bound during optimization.\n4       A Complete, Compressed and Efficiently Optimizable Sampler\nIn this section, we discuss the main results that imply Theorem 1: the family P of Equation (5) is\ncomplete, compressed and efficiently optimizable (for some choice of \u03b2\u22c6, \u03c1\u22c6                                                   and W).\nCompleteness.                  First, we show that the family of solution generators of Equation (5) is complete.\nFor the proof, we refer to Proposition 2 in Appendix C. At a high-level, we to pick \u03b2\u22c6, \u03c1\u22c6                                                             to\nbe of order poly(\u03f5, \u03b1, 1/C, 1/DS, 1/DI). This yields that the matrix W = \u2212M/\u03bb is such that\nL(W) \u2264          opt + \u03f5, where M is the matrix of Item 2 in Assumption 1 and \u03bb is poly(\u03f5/[I]). To give\nsome intuition about this choice of matrix, let us see how L(W) behaves. By definition, we have\nthat                                                      L(W) = E                     E           z\u22a4Mx          ,\n                                                                          z\u223cR    x\u223cp(\u00b7;z;W)\n                                                                                    11", "md": "Obstacle II: Vanishing Gradients.\n\nWhile we have established that the gradient field of the regularized objective \"points\" towards the right direction, the regularized objective still suffers from vanishing gradients, see the middle plot in Figure 1. In other words, \u03b3 in the definition of quasar convexity (Definition 3) may be exponentially small, as it is proportional to the variance of the random variable \\(z^{\\top}(W + M/\\lambda)x\\), see Equation (4). As we see in the middle plot of Figure 1, the main issue is the vanishing gradient when W gets closer to the minimizer \\(-M/\\lambda\\) (towards the front-corner). For simplicity, consider the variance along the direction of W, i.e., Var\\([z^{\\top}Wx]\\) and recall that x is generated by the density \\(\\frac{exp(z^{\\top}Wx)}{\\sum_{x\\in X} exp(z^{\\top}Wx)}\\). When \\(\\|W\\|_2 \\rightarrow +\\infty\\), we observe that the value \\(z^{\\top}Wx\\) concentrates exponentially fast to \\(\\max_{x\\in X} z^{\\top}Wx\\) (think of the convergence of the soft-max to the max function). Therefore, the variance Var\\([z^{\\top}Wx]\\) may vanish exponentially fast making the convergence of gradient descent slow.\n\nSolution II: Non-Vanishing Gradients via Fast/Slow Mixture Generators.\n\nWe propose a fix to the vanishing gradients issue by using a mixture of exponential families as a solution generator. We define the family of solution generators \\(P = \\{p(W) : W \\in \\mathcal{W}\\}\\) to be\n\n\\[\nP = \\{(1 - \\beta^*)\\phi(W) + \\beta^*\\phi(\\rho^*W) : W \\in \\mathcal{W}\\} , \\quad (5)\n\\]\n\nfor a (fixed) mixing parameter \\(\\beta^*\\) and a (fixed) temperature parameter \\(\\rho^*\\). The main idea is to have the first component of the mixture to converge fast to the optimal solution (to \\(-M/\\lambda\\)) while the second \"slow\" component that has parameter \\(\\rho^*W\\) stays closer to the uniform distribution over solutions that guarantees non-trivial variance (and therefore non-vanishing gradients).\n\nMore precisely, taking \\(\\rho^*\\) to be sufficiently small, the distribution \\(\\phi(\\rho^*W)\\) is almost uniform over the solution space \\(\\mathcal{S}(S)\\). Therefore, in Equation (4), the almost uniform distribution component of the mixture will add to the variance and allow us to show a lower bound. This is where Item 3 of Assumption 1 comes into play and gives us the desired non-trivial variance lower bound under the uniform distribution. We view this fast/slow mixture technique as an interesting insight of our work: we use the \"fast\" component (the one with parameter W) to actually reach the optimal solution \\(-M/\\lambda\\) and we use the \"slow\" component (the one with parameter \\(\\rho^*W\\) that essentially generates random solutions) to preserve a non-trivial variance lower bound during optimization.\n\nA Complete, Compressed and Efficiently Optimizable Sampler\n\nIn this section, we discuss the main results that imply Theorem 1: the family \\(P\\) of Equation (5) is complete, compressed and efficiently optimizable (for some choice of \\(\\beta^*, \\rho^*\\) and \\(\\mathcal{W}\\).\n\nCompleteness.\n\nFirst, we show that the family of solution generators of Equation (5) is complete. For the proof, we refer to Proposition 2 in Appendix C. At a high-level, we to pick \\(\\beta^*, \\rho^*\\) to be of order poly(\\(\\epsilon, \\alpha, 1/C, 1/DS, 1/DI\\)). This yields that the matrix \\(W = -M/\\lambda\\) is such that \\(L(W) \\leq \\text{opt} + \\epsilon\\), where \\(M\\) is the matrix of Item 2 in Assumption 1 and \\(\\lambda\\) is poly(\\(\\epsilon/[I]\\)). To give some intuition about this choice of matrix, let us see how \\(L(W)\\) behaves. By definition, we have that \\(L(W) = E[E[z^{\\top}Mx]\\), \\(z \\sim R, x \\sim p(\\cdot; z; W)\\).", "images": [], "items": [{"type": "text", "value": "Obstacle II: Vanishing Gradients.\n\nWhile we have established that the gradient field of the regularized objective \"points\" towards the right direction, the regularized objective still suffers from vanishing gradients, see the middle plot in Figure 1. In other words, \u03b3 in the definition of quasar convexity (Definition 3) may be exponentially small, as it is proportional to the variance of the random variable \\(z^{\\top}(W + M/\\lambda)x\\), see Equation (4). As we see in the middle plot of Figure 1, the main issue is the vanishing gradient when W gets closer to the minimizer \\(-M/\\lambda\\) (towards the front-corner). For simplicity, consider the variance along the direction of W, i.e., Var\\([z^{\\top}Wx]\\) and recall that x is generated by the density \\(\\frac{exp(z^{\\top}Wx)}{\\sum_{x\\in X} exp(z^{\\top}Wx)}\\). When \\(\\|W\\|_2 \\rightarrow +\\infty\\), we observe that the value \\(z^{\\top}Wx\\) concentrates exponentially fast to \\(\\max_{x\\in X} z^{\\top}Wx\\) (think of the convergence of the soft-max to the max function). Therefore, the variance Var\\([z^{\\top}Wx]\\) may vanish exponentially fast making the convergence of gradient descent slow.\n\nSolution II: Non-Vanishing Gradients via Fast/Slow Mixture Generators.\n\nWe propose a fix to the vanishing gradients issue by using a mixture of exponential families as a solution generator. We define the family of solution generators \\(P = \\{p(W) : W \\in \\mathcal{W}\\}\\) to be\n\n\\[\nP = \\{(1 - \\beta^*)\\phi(W) + \\beta^*\\phi(\\rho^*W) : W \\in \\mathcal{W}\\} , \\quad (5)\n\\]\n\nfor a (fixed) mixing parameter \\(\\beta^*\\) and a (fixed) temperature parameter \\(\\rho^*\\). The main idea is to have the first component of the mixture to converge fast to the optimal solution (to \\(-M/\\lambda\\)) while the second \"slow\" component that has parameter \\(\\rho^*W\\) stays closer to the uniform distribution over solutions that guarantees non-trivial variance (and therefore non-vanishing gradients).\n\nMore precisely, taking \\(\\rho^*\\) to be sufficiently small, the distribution \\(\\phi(\\rho^*W)\\) is almost uniform over the solution space \\(\\mathcal{S}(S)\\). Therefore, in Equation (4), the almost uniform distribution component of the mixture will add to the variance and allow us to show a lower bound. This is where Item 3 of Assumption 1 comes into play and gives us the desired non-trivial variance lower bound under the uniform distribution. We view this fast/slow mixture technique as an interesting insight of our work: we use the \"fast\" component (the one with parameter W) to actually reach the optimal solution \\(-M/\\lambda\\) and we use the \"slow\" component (the one with parameter \\(\\rho^*W\\) that essentially generates random solutions) to preserve a non-trivial variance lower bound during optimization.\n\nA Complete, Compressed and Efficiently Optimizable Sampler\n\nIn this section, we discuss the main results that imply Theorem 1: the family \\(P\\) of Equation (5) is complete, compressed and efficiently optimizable (for some choice of \\(\\beta^*, \\rho^*\\) and \\(\\mathcal{W}\\).\n\nCompleteness.\n\nFirst, we show that the family of solution generators of Equation (5) is complete. For the proof, we refer to Proposition 2 in Appendix C. At a high-level, we to pick \\(\\beta^*, \\rho^*\\) to be of order poly(\\(\\epsilon, \\alpha, 1/C, 1/DS, 1/DI\\)). This yields that the matrix \\(W = -M/\\lambda\\) is such that \\(L(W) \\leq \\text{opt} + \\epsilon\\), where \\(M\\) is the matrix of Item 2 in Assumption 1 and \\(\\lambda\\) is poly(\\(\\epsilon/[I]\\)). To give some intuition about this choice of matrix, let us see how \\(L(W)\\) behaves. By definition, we have that \\(L(W) = E[E[z^{\\top}Mx]\\), \\(z \\sim R, x \\sim p(\\cdot; z; W)\\).", "md": "Obstacle II: Vanishing Gradients.\n\nWhile we have established that the gradient field of the regularized objective \"points\" towards the right direction, the regularized objective still suffers from vanishing gradients, see the middle plot in Figure 1. In other words, \u03b3 in the definition of quasar convexity (Definition 3) may be exponentially small, as it is proportional to the variance of the random variable \\(z^{\\top}(W + M/\\lambda)x\\), see Equation (4). As we see in the middle plot of Figure 1, the main issue is the vanishing gradient when W gets closer to the minimizer \\(-M/\\lambda\\) (towards the front-corner). For simplicity, consider the variance along the direction of W, i.e., Var\\([z^{\\top}Wx]\\) and recall that x is generated by the density \\(\\frac{exp(z^{\\top}Wx)}{\\sum_{x\\in X} exp(z^{\\top}Wx)}\\). When \\(\\|W\\|_2 \\rightarrow +\\infty\\), we observe that the value \\(z^{\\top}Wx\\) concentrates exponentially fast to \\(\\max_{x\\in X} z^{\\top}Wx\\) (think of the convergence of the soft-max to the max function). Therefore, the variance Var\\([z^{\\top}Wx]\\) may vanish exponentially fast making the convergence of gradient descent slow.\n\nSolution II: Non-Vanishing Gradients via Fast/Slow Mixture Generators.\n\nWe propose a fix to the vanishing gradients issue by using a mixture of exponential families as a solution generator. We define the family of solution generators \\(P = \\{p(W) : W \\in \\mathcal{W}\\}\\) to be\n\n\\[\nP = \\{(1 - \\beta^*)\\phi(W) + \\beta^*\\phi(\\rho^*W) : W \\in \\mathcal{W}\\} , \\quad (5)\n\\]\n\nfor a (fixed) mixing parameter \\(\\beta^*\\) and a (fixed) temperature parameter \\(\\rho^*\\). The main idea is to have the first component of the mixture to converge fast to the optimal solution (to \\(-M/\\lambda\\)) while the second \"slow\" component that has parameter \\(\\rho^*W\\) stays closer to the uniform distribution over solutions that guarantees non-trivial variance (and therefore non-vanishing gradients).\n\nMore precisely, taking \\(\\rho^*\\) to be sufficiently small, the distribution \\(\\phi(\\rho^*W)\\) is almost uniform over the solution space \\(\\mathcal{S}(S)\\). Therefore, in Equation (4), the almost uniform distribution component of the mixture will add to the variance and allow us to show a lower bound. This is where Item 3 of Assumption 1 comes into play and gives us the desired non-trivial variance lower bound under the uniform distribution. We view this fast/slow mixture technique as an interesting insight of our work: we use the \"fast\" component (the one with parameter W) to actually reach the optimal solution \\(-M/\\lambda\\) and we use the \"slow\" component (the one with parameter \\(\\rho^*W\\) that essentially generates random solutions) to preserve a non-trivial variance lower bound during optimization.\n\nA Complete, Compressed and Efficiently Optimizable Sampler\n\nIn this section, we discuss the main results that imply Theorem 1: the family \\(P\\) of Equation (5) is complete, compressed and efficiently optimizable (for some choice of \\(\\beta^*, \\rho^*\\) and \\(\\mathcal{W}\\).\n\nCompleteness.\n\nFirst, we show that the family of solution generators of Equation (5) is complete. For the proof, we refer to Proposition 2 in Appendix C. At a high-level, we to pick \\(\\beta^*, \\rho^*\\) to be of order poly(\\(\\epsilon, \\alpha, 1/C, 1/DS, 1/DI\\)). This yields that the matrix \\(W = -M/\\lambda\\) is such that \\(L(W) \\leq \\text{opt} + \\epsilon\\), where \\(M\\) is the matrix of Item 2 in Assumption 1 and \\(\\lambda\\) is poly(\\(\\epsilon/[I]\\)). To give some intuition about this choice of matrix, let us see how \\(L(W)\\) behaves. By definition, we have that \\(L(W) = E[E[z^{\\top}Mx]\\), \\(z \\sim R, x \\sim p(\\cdot; z; W)\\)."}]}, {"page": 12, "text": "where the distribution p belongs to the family of Equation (5), i.e., p(W) = (1 \u2212                \u03b2\u22c6)\u03d5(W) +\n\u03b2\u22c6\u03d5(\u03c1\u22c6W). Since the mixing weight \u03b2\u22c6          is small, we have that p(W) is approximately equal to\n\u03d5(W). This means that our solution generator draws samples from the distribution whose mass\nat x given instance z is proportional to exp(\u2212z\u22a4Mx/\u03bb) and, since \u03bb > 0 is very small, the\ndistribution concentrates to solutions x that tend to minimize the objective z\u22a4Mx. This is the\nreason why W = \u2212M/\u03bb is close to opt in the sense that L(W) \u2264               opt + \u03f5.\nCompression.       As a second step, we show (in Proposition 3, see Appendix D) that P is a com-\npressed family of solution generators. This result follows immediately from the structure of\nEquation (5) (observe that W has nX nZ parameters) and the boundedness of W = \u2212M/\u03bb.\nEfficiently Optimizable.       The proof of this result essentially corresponds to the discussion pro-\nvided in Section 3. Our main structural result shows that the landscape of the regularized objec-\ntive with the fast/slow mixture solution-generator is quasar convex. More precisely, we consider\nthe following objective:\n                                L\u03bb(W) = E           E                                                      (1)\n                                           z\u223cR  x\u223cp(\u00b7;z;W)[z\u22a4Mx] + \u03bbR(W) ,\nwhere p(W) belongs in the family P of Equation (5) and R is a weighted sum of two negative\nentropy regularizers (to be in accordance with the mixture structure of P), i.e., R(W) = (1 \u2212\n\u03b2\u22c6)H(W) + \u03b2\u22c6/\u03c1\u22c6H(\u03c1\u22c6W). Our main structural results follows (for the proof, see Appendix E.1).\nProposition 1 (Quasar Convexity). Consider \u03f5 > 0 and a prior R over I. Assume that Assumption 1\nholds. The function L\u03bb of Equation (1) with domain W is poly(\u03f5, \u03b1, 1/C, 1/DS, 1/DI)-quasar convex\nwith respect to \u2212M/\u03bb on the domain W.\n    Since \u03c1\u22c6  is small (by Proposition 2), H(\u03c1\u22c6W) is essentially constant and close in value to the\nnegative entropy of the uniform distribution. Hence, the effect of R(W) during optimization is\nessentially the same as that of H(W) (since \u03b2\u22c6        is close to 0). We show that L\u03bb is quasar convex\nwith a non-trivial parameter \u03b3 (see Proposition 1). We can then apply (in a black-box manner)\nthe convergence results from [HMR16] to optimize it using projected SGD. We show that SGD\nfinds a weight matrix     W such that the solution generator p(        W) generates solutions achieving\nactual loss L close to that of the near optimal matrix W = \u2212M/\u03bb, i.e., L(             W) \u2264   L(W) + \u03f5. For\nfurther details, see Appendix E.3.\n5    Experimental Evaluation\nIn this section, we investigate experimentally the effect of our main theoretical contributions, the\nentropy regularizer (see Equation (2)) and the fast/slow mixture scheme (see Equation (5)). We\ntry to find the Max-Cut of a fixed graph G, i.e., the support of the prior R is a single graph.\nSimilarly to our theoretical results, our sampler is of the form escore(s;w), where s \u2208       {\u22121, 1}n (here\nn is the number of nodes in the graph) is a candidate solution of the Max-Cut problem. For the\nscore function we use a simple linear layer (left plot of Figure 2) and a 3-layer ReLU network\n(right plot of Figure 2).\n    In this work, we focus on instances where the number of nodes n is small (say n = 15). In\nsuch instances, we can explicitly compute the density function and work with an exact sampler.\n                                                      12", "md": "# Math Equations and Text\n\nwhere the distribution p belongs to the family of Equation (5), i.e., $$p(W) = (1 - \\beta^*)\\phi(W) + \\beta^*\\phi(\\rho^*W)$$. Since the mixing weight $$\\beta^*$$ is small, we have that $$p(W)$$ is approximately equal to $$\\phi(W)$$. This means that our solution generator draws samples from the distribution whose mass at x given instance z is proportional to $$\\exp(-z^TMx/\\lambda)$$ and, since $$\\lambda > 0$$ is very small, the distribution concentrates to solutions x that tend to minimize the objective $$z^TMx$$. This is the reason why $$W = -M/\\lambda$$ is close to opt in the sense that $$L(W) \\leq opt + \\epsilon$$.\n\nCompression. As a second step, we show (in Proposition 3, see Appendix D) that P is a compressed family of solution generators. This result follows immediately from the structure of Equation (5) (observe that W has $$n \\times nZ$$ parameters) and the boundedness of $$W = -M/\\lambda$$.\n\nEfficiently Optimizable. The proof of this result essentially corresponds to the discussion provided in Section 3. Our main structural result shows that the landscape of the regularized objective with the fast/slow mixture solution-generator is quasar convex. More precisely, we consider the following objective:\n\n$$L_{\\lambda}(W) = E_z \\sim R, x \\sim p(\\cdot; z; W)[z^TMx] + \\lambda R(W)$$\n\nwhere $$p(W)$$ belongs in the family P of Equation (5) and R is a weighted sum of two negative entropy regularizers (to be in accordance with the mixture structure of P), i.e., $$R(W) = (1 - \\beta^*)H(W) + \\beta^*/\\rho^*H(\\rho^*W)$$. Our main structural results follows (for the proof, see Appendix E.1).\n\nProposition 1 (Quasar Convexity). Consider $$\\epsilon > 0$$ and a prior R over I. Assume that Assumption 1 holds. The function $$L_{\\lambda}$$ of Equation (1) with domain W is poly($$\\epsilon, \\alpha, 1/C, 1/DS, 1/DI$$)-quasar convex with respect to $$-M/\\lambda$$ on the domain W.\n\nSince $$\\rho^*$$ is small (by Proposition 2), $$H(\\rho^*W)$$ is essentially constant and close in value to the negative entropy of the uniform distribution. Hence, the effect of $$R(W)$$ during optimization is essentially the same as that of $$H(W)$$ (since $$\\beta^*$$ is close to 0). We show that $$L_{\\lambda}$$ is quasar convex with a non-trivial parameter $$\\gamma$$ (see Proposition 1). We can then apply (in a black-box manner) the convergence results from [HMR16] to optimize it using projected SGD. We show that SGD finds a weight matrix W such that the solution generator $$p(W)$$ generates solutions achieving actual loss L close to that of the near optimal matrix $$W = -M/\\lambda$$, i.e., $$L(W) \\leq L(W) + \\epsilon$$. For further details, see Appendix E.3.\n\n## 5 Experimental Evaluation\n\nIn this section, we investigate experimentally the effect of our main theoretical contributions, the entropy regularizer (see Equation (2)) and the fast/slow mixture scheme (see Equation (5)). We try to find the Max-Cut of a fixed graph G, i.e., the support of the prior R is a single graph. Similarly to our theoretical results, our sampler is of the form escore(s;w), where $$s \\in \\{-1, 1\\}^n$$ (here n is the number of nodes in the graph) is a candidate solution of the Max-Cut problem. For the score function we use a simple linear layer (left plot of Figure 2) and a 3-layer ReLU network (right plot of Figure 2).\n\nIn this work, we focus on instances where the number of nodes n is small (say n = 15). In such instances, we can explicitly compute the density function and work with an exact sampler.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "where the distribution p belongs to the family of Equation (5), i.e., $$p(W) = (1 - \\beta^*)\\phi(W) + \\beta^*\\phi(\\rho^*W)$$. Since the mixing weight $$\\beta^*$$ is small, we have that $$p(W)$$ is approximately equal to $$\\phi(W)$$. This means that our solution generator draws samples from the distribution whose mass at x given instance z is proportional to $$\\exp(-z^TMx/\\lambda)$$ and, since $$\\lambda > 0$$ is very small, the distribution concentrates to solutions x that tend to minimize the objective $$z^TMx$$. This is the reason why $$W = -M/\\lambda$$ is close to opt in the sense that $$L(W) \\leq opt + \\epsilon$$.\n\nCompression. As a second step, we show (in Proposition 3, see Appendix D) that P is a compressed family of solution generators. This result follows immediately from the structure of Equation (5) (observe that W has $$n \\times nZ$$ parameters) and the boundedness of $$W = -M/\\lambda$$.\n\nEfficiently Optimizable. The proof of this result essentially corresponds to the discussion provided in Section 3. Our main structural result shows that the landscape of the regularized objective with the fast/slow mixture solution-generator is quasar convex. More precisely, we consider the following objective:\n\n$$L_{\\lambda}(W) = E_z \\sim R, x \\sim p(\\cdot; z; W)[z^TMx] + \\lambda R(W)$$\n\nwhere $$p(W)$$ belongs in the family P of Equation (5) and R is a weighted sum of two negative entropy regularizers (to be in accordance with the mixture structure of P), i.e., $$R(W) = (1 - \\beta^*)H(W) + \\beta^*/\\rho^*H(\\rho^*W)$$. Our main structural results follows (for the proof, see Appendix E.1).\n\nProposition 1 (Quasar Convexity). Consider $$\\epsilon > 0$$ and a prior R over I. Assume that Assumption 1 holds. The function $$L_{\\lambda}$$ of Equation (1) with domain W is poly($$\\epsilon, \\alpha, 1/C, 1/DS, 1/DI$$)-quasar convex with respect to $$-M/\\lambda$$ on the domain W.\n\nSince $$\\rho^*$$ is small (by Proposition 2), $$H(\\rho^*W)$$ is essentially constant and close in value to the negative entropy of the uniform distribution. Hence, the effect of $$R(W)$$ during optimization is essentially the same as that of $$H(W)$$ (since $$\\beta^*$$ is close to 0). We show that $$L_{\\lambda}$$ is quasar convex with a non-trivial parameter $$\\gamma$$ (see Proposition 1). We can then apply (in a black-box manner) the convergence results from [HMR16] to optimize it using projected SGD. We show that SGD finds a weight matrix W such that the solution generator $$p(W)$$ generates solutions achieving actual loss L close to that of the near optimal matrix $$W = -M/\\lambda$$, i.e., $$L(W) \\leq L(W) + \\epsilon$$. For further details, see Appendix E.3.", "md": "where the distribution p belongs to the family of Equation (5), i.e., $$p(W) = (1 - \\beta^*)\\phi(W) + \\beta^*\\phi(\\rho^*W)$$. Since the mixing weight $$\\beta^*$$ is small, we have that $$p(W)$$ is approximately equal to $$\\phi(W)$$. This means that our solution generator draws samples from the distribution whose mass at x given instance z is proportional to $$\\exp(-z^TMx/\\lambda)$$ and, since $$\\lambda > 0$$ is very small, the distribution concentrates to solutions x that tend to minimize the objective $$z^TMx$$. This is the reason why $$W = -M/\\lambda$$ is close to opt in the sense that $$L(W) \\leq opt + \\epsilon$$.\n\nCompression. As a second step, we show (in Proposition 3, see Appendix D) that P is a compressed family of solution generators. This result follows immediately from the structure of Equation (5) (observe that W has $$n \\times nZ$$ parameters) and the boundedness of $$W = -M/\\lambda$$.\n\nEfficiently Optimizable. The proof of this result essentially corresponds to the discussion provided in Section 3. Our main structural result shows that the landscape of the regularized objective with the fast/slow mixture solution-generator is quasar convex. More precisely, we consider the following objective:\n\n$$L_{\\lambda}(W) = E_z \\sim R, x \\sim p(\\cdot; z; W)[z^TMx] + \\lambda R(W)$$\n\nwhere $$p(W)$$ belongs in the family P of Equation (5) and R is a weighted sum of two negative entropy regularizers (to be in accordance with the mixture structure of P), i.e., $$R(W) = (1 - \\beta^*)H(W) + \\beta^*/\\rho^*H(\\rho^*W)$$. Our main structural results follows (for the proof, see Appendix E.1).\n\nProposition 1 (Quasar Convexity). Consider $$\\epsilon > 0$$ and a prior R over I. Assume that Assumption 1 holds. The function $$L_{\\lambda}$$ of Equation (1) with domain W is poly($$\\epsilon, \\alpha, 1/C, 1/DS, 1/DI$$)-quasar convex with respect to $$-M/\\lambda$$ on the domain W.\n\nSince $$\\rho^*$$ is small (by Proposition 2), $$H(\\rho^*W)$$ is essentially constant and close in value to the negative entropy of the uniform distribution. Hence, the effect of $$R(W)$$ during optimization is essentially the same as that of $$H(W)$$ (since $$\\beta^*$$ is close to 0). We show that $$L_{\\lambda}$$ is quasar convex with a non-trivial parameter $$\\gamma$$ (see Proposition 1). We can then apply (in a black-box manner) the convergence results from [HMR16] to optimize it using projected SGD. We show that SGD finds a weight matrix W such that the solution generator $$p(W)$$ generates solutions achieving actual loss L close to that of the near optimal matrix $$W = -M/\\lambda$$, i.e., $$L(W) \\leq L(W) + \\epsilon$$. For further details, see Appendix E.3."}, {"type": "heading", "lvl": 2, "value": "5 Experimental Evaluation", "md": "## 5 Experimental Evaluation"}, {"type": "text", "value": "In this section, we investigate experimentally the effect of our main theoretical contributions, the entropy regularizer (see Equation (2)) and the fast/slow mixture scheme (see Equation (5)). We try to find the Max-Cut of a fixed graph G, i.e., the support of the prior R is a single graph. Similarly to our theoretical results, our sampler is of the form escore(s;w), where $$s \\in \\{-1, 1\\}^n$$ (here n is the number of nodes in the graph) is a candidate solution of the Max-Cut problem. For the score function we use a simple linear layer (left plot of Figure 2) and a 3-layer ReLU network (right plot of Figure 2).\n\nIn this work, we focus on instances where the number of nodes n is small (say n = 15). In such instances, we can explicitly compute the density function and work with an exact sampler.", "md": "In this section, we investigate experimentally the effect of our main theoretical contributions, the entropy regularizer (see Equation (2)) and the fast/slow mixture scheme (see Equation (5)). We try to find the Max-Cut of a fixed graph G, i.e., the support of the prior R is a single graph. Similarly to our theoretical results, our sampler is of the form escore(s;w), where $$s \\in \\{-1, 1\\}^n$$ (here n is the number of nodes in the graph) is a candidate solution of the Max-Cut problem. For the score function we use a simple linear layer (left plot of Figure 2) and a 3-layer ReLU network (right plot of Figure 2).\n\nIn this work, we focus on instances where the number of nodes n is small (say n = 15). In such instances, we can explicitly compute the density function and work with an exact sampler."}]}, {"page": 13, "text": "We generate 100 random G(n, p) (Erd\u02dd       os\u2013R\u00e9nyi) graphs with n = 15 nodes and p = 0.5 and train\nsolution generators using both the \u201dvanilla\u201d loss L and the entropy-regularized loss L\u03bb with the\nfast/slow mixture scheme. We perform 600 iterations and, for the entropy regularization, we\nprogressively decrease the regularization weight, starting from 10, and dividing it by 2 every 60\niterations. Out of the 100 trials we found that our proposed objective was always able to find the\noptimal cut while the model trained with the vanilla loss was able to find it for approximately\n65% of the graphs (for 65 out of 100 using the linear network and for 66 using the ReLU network).\n     Hence, our experiments demonstrate that while the unregularized objective is often \u201cstuck\u201d\nat sub-optimal solutions \u2013 and this happens even for very small instances (n =15 nodes) \u2013 of the\nMax-Cut problem, the objective motivated by our theoretical results is able to find the optimal\nsolutions. For further details, see Appendix I. We leave further experimental evaluation of our\napproach as future work.\n                Rcouianzec Lor                                Rcouianzec Lor\n      Figure 2: Plot of the Max-Cut value trajectory of the \u201cvanilla\u201d objective and entropy-regularized\n      objective with the slow/fast mixture scheme. We remark that we plot the value of the cut of\n      each iteration (and not the value of the regularized-loss). On the horizontal axis we plot the\n      number of iterations and on the vertical axis we plot the achieved value of the cut. Both graphs\n      used were random G(n, p) graphs generated with n = 15 nodes and edge probability p = 0.5.\n      For the left plot we used a linear network (the same exponential family as the one used in our\n      theoretical results). For the right plot we used a simple 3-Layer ReLU network to generate the\n      scores. We observe that the \u201dvanilla\u201d loss gets stuck on sub-optimal solutions.\n6    Conclusion\nNeural networks have proven to be extraordinarily flexible, and their promise for combinatorial\noptimization appears to be significant. Yet as our work demonstrates, while gradient methods\nare powerful, without a favorable landscape, they are destined to fail. We show what it takes to\ndesign such a favorable optimization landscape.\n     At the same time, our work raises various interesting research questions regarding algorith-\nmic implications. An intriguing direction has to do with efficiently samplable generative models.\nIn this paper we have focused on the number of parameters that we have to optimize and on\nthe optimization landscape of the corresponding objective, i.e., whether \u201cfollowing\u201d its gradient\nfield leads to optimal solutions. Apart from these properties it is important and interesting to\n                                                     13", "md": "We generate 100 random $$G(n, p)$$ (Erd\u0151s\u2013R\u00e9nyi) graphs with $$n = 15$$ nodes and $$p = 0.5$$ and train solution generators using both the \"vanilla\" loss $$L$$ and the entropy-regularized loss $$L_{\\lambda}$$ with the fast/slow mixture scheme. We perform 600 iterations and, for the entropy regularization, we progressively decrease the regularization weight, starting from 10, and dividing it by 2 every 60 iterations. Out of the 100 trials we found that our proposed objective was always able to find the optimal cut while the model trained with the vanilla loss was able to find it for approximately 65% of the graphs (for 65 out of 100 using the linear network and for 66 using the ReLU network).\n\nHence, our experiments demonstrate that while the unregularized objective is often \"stuck\" at sub-optimal solutions \u2013 and this happens even for very small instances ($$n = 15$$ nodes) \u2013 of the Max-Cut problem, the objective motivated by our theoretical results is able to find the optimal solutions. For further details, see Appendix I. We leave further experimental evaluation of our approach as future work.\n\n**Figure 2:** Plot of the Max-Cut value trajectory of the \"vanilla\" objective and entropy-regularized objective with the slow/fast mixture scheme. We remark that we plot the value of the cut of each iteration (and not the value of the regularized-loss). On the horizontal axis we plot the number of iterations and on the vertical axis we plot the achieved value of the cut. Both graphs used were random $$G(n, p)$$ graphs generated with $$n = 15$$ nodes and edge probability $$p = 0.5$$. For the left plot we used a linear network (the same exponential family as the one used in our theoretical results). For the right plot we used a simple 3-Layer ReLU network to generate the scores. We observe that the \"vanilla\" loss gets stuck on sub-optimal solutions.\n\n**Conclusion**\nNeural networks have proven to be extraordinarily flexible, and their promise for combinatorial optimization appears to be significant. Yet as our work demonstrates, while gradient methods are powerful, without a favorable landscape, they are destined to fail. We show what it takes to design such a favorable optimization landscape.\n\nAt the same time, our work raises various interesting research questions regarding algorithmic implications. An intriguing direction has to do with efficiently samplable generative models. In this paper we have focused on the number of parameters that we have to optimize and on the optimization landscape of the corresponding objective, i.e., whether \"following\" its gradient field leads to optimal solutions. Apart from these properties it is important and interesting to", "images": [{"name": "page-13-1.jpg", "height": 149, "width": 196, "x": 300, "y": 258}, {"name": "page-13-0.jpg", "height": 149, "width": 196, "x": 101, "y": 258}], "items": [{"type": "text", "value": "We generate 100 random $$G(n, p)$$ (Erd\u0151s\u2013R\u00e9nyi) graphs with $$n = 15$$ nodes and $$p = 0.5$$ and train solution generators using both the \"vanilla\" loss $$L$$ and the entropy-regularized loss $$L_{\\lambda}$$ with the fast/slow mixture scheme. We perform 600 iterations and, for the entropy regularization, we progressively decrease the regularization weight, starting from 10, and dividing it by 2 every 60 iterations. Out of the 100 trials we found that our proposed objective was always able to find the optimal cut while the model trained with the vanilla loss was able to find it for approximately 65% of the graphs (for 65 out of 100 using the linear network and for 66 using the ReLU network).\n\nHence, our experiments demonstrate that while the unregularized objective is often \"stuck\" at sub-optimal solutions \u2013 and this happens even for very small instances ($$n = 15$$ nodes) \u2013 of the Max-Cut problem, the objective motivated by our theoretical results is able to find the optimal solutions. For further details, see Appendix I. We leave further experimental evaluation of our approach as future work.\n\n**Figure 2:** Plot of the Max-Cut value trajectory of the \"vanilla\" objective and entropy-regularized objective with the slow/fast mixture scheme. We remark that we plot the value of the cut of each iteration (and not the value of the regularized-loss). On the horizontal axis we plot the number of iterations and on the vertical axis we plot the achieved value of the cut. Both graphs used were random $$G(n, p)$$ graphs generated with $$n = 15$$ nodes and edge probability $$p = 0.5$$. For the left plot we used a linear network (the same exponential family as the one used in our theoretical results). For the right plot we used a simple 3-Layer ReLU network to generate the scores. We observe that the \"vanilla\" loss gets stuck on sub-optimal solutions.\n\n**Conclusion**\nNeural networks have proven to be extraordinarily flexible, and their promise for combinatorial optimization appears to be significant. Yet as our work demonstrates, while gradient methods are powerful, without a favorable landscape, they are destined to fail. We show what it takes to design such a favorable optimization landscape.\n\nAt the same time, our work raises various interesting research questions regarding algorithmic implications. An intriguing direction has to do with efficiently samplable generative models. In this paper we have focused on the number of parameters that we have to optimize and on the optimization landscape of the corresponding objective, i.e., whether \"following\" its gradient field leads to optimal solutions. Apart from these properties it is important and interesting to", "md": "We generate 100 random $$G(n, p)$$ (Erd\u0151s\u2013R\u00e9nyi) graphs with $$n = 15$$ nodes and $$p = 0.5$$ and train solution generators using both the \"vanilla\" loss $$L$$ and the entropy-regularized loss $$L_{\\lambda}$$ with the fast/slow mixture scheme. We perform 600 iterations and, for the entropy regularization, we progressively decrease the regularization weight, starting from 10, and dividing it by 2 every 60 iterations. Out of the 100 trials we found that our proposed objective was always able to find the optimal cut while the model trained with the vanilla loss was able to find it for approximately 65% of the graphs (for 65 out of 100 using the linear network and for 66 using the ReLU network).\n\nHence, our experiments demonstrate that while the unregularized objective is often \"stuck\" at sub-optimal solutions \u2013 and this happens even for very small instances ($$n = 15$$ nodes) \u2013 of the Max-Cut problem, the objective motivated by our theoretical results is able to find the optimal solutions. For further details, see Appendix I. We leave further experimental evaluation of our approach as future work.\n\n**Figure 2:** Plot of the Max-Cut value trajectory of the \"vanilla\" objective and entropy-regularized objective with the slow/fast mixture scheme. We remark that we plot the value of the cut of each iteration (and not the value of the regularized-loss). On the horizontal axis we plot the number of iterations and on the vertical axis we plot the achieved value of the cut. Both graphs used were random $$G(n, p)$$ graphs generated with $$n = 15$$ nodes and edge probability $$p = 0.5$$. For the left plot we used a linear network (the same exponential family as the one used in our theoretical results). For the right plot we used a simple 3-Layer ReLU network to generate the scores. We observe that the \"vanilla\" loss gets stuck on sub-optimal solutions.\n\n**Conclusion**\nNeural networks have proven to be extraordinarily flexible, and their promise for combinatorial optimization appears to be significant. Yet as our work demonstrates, while gradient methods are powerful, without a favorable landscape, they are destined to fail. We show what it takes to design such a favorable optimization landscape.\n\nAt the same time, our work raises various interesting research questions regarding algorithmic implications. An intriguing direction has to do with efficiently samplable generative models. In this paper we have focused on the number of parameters that we have to optimize and on the optimization landscape of the corresponding objective, i.e., whether \"following\" its gradient field leads to optimal solutions. Apart from these properties it is important and interesting to"}]}, {"page": 14, "text": "have parametric classes that can efficiently generate samples (in terms of computation). Under\nstandard computational complexity assumptions, it is not possible to design solution generators\nwhich are both efficiently optimizable and samplable for challenging combinatorial problems.\nAn interesting direction is to relax our goal to generating approximately optimal solutions:\nOpen Question 1. Are there complete, compressed, efficiently optimizable and samplable solution gener-\nators that obtain non-trivial approximation guarantees for challenging combinatorial tasks?\nReferences\n[AAM22]      Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz.                The merged-\n             staircase property: a necessary and nearly sufficient condition for sgd learning of\n             sparse functions on two-layer neural networks.       In Conference on Learning Theory,\n             pages 4782\u20134887. PMLR, 2022. 7\n[ABA22]      Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning:\n             quantifying the cost of symmetry. arXiv preprint arXiv:2208.03113, 2022. 7\n[ABAB+21] Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj\n             Nagaraj. The staircase property: How hierarchical structure can guide deep learning.\n             Advances in Neural Information Processing Systems, 34:26989\u201327002, 2021. 7\n[ABAM23]     Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on\n             neural networks: leap complexity and saddle-to-saddle dynamics.           arXiv preprint\n             arXiv:2302.11055, 2023. 7\n[AEG+22]     Simon Apers, Yuval Efron, Pawel Gawrychowski, Troy Lee, Sagnik Mukhopadhyay,\n             and Danupon Nanongkai. Cut query algorithms with star contraction. arXiv preprint\n             arXiv:2201.05674, 2022. 2\n[AKM+21]     Emmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro.\n             On the power of differentiable learning versus pac and sq learning.         Advances in\n             Neural Information Processing Systems, 34:24340\u201324351, 2021. 7\n[AMW18]      Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-\n             sat: An unsupervised differentiable approach. In International Conference on Learning\n             Representations, 2018. 7\n[ART23]      Maria Chiara Angelini and Federico Ricci-Tersenghi. Modern graph neural networks\n             do worse than classical greedy algorithms in solving combinatorial optimization\n             problems like maximum independent set.          Nature Machine Intelligence, 5(1):29\u201331,\n             2023. 7\n[AS18]       Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. arXiv\n             preprint arXiv:1812.06369, 2018. 7\n[AS20]       Emmanuel Abbe and Colin Sandon. On the universality of deep learning. Advances\n             in Neural Information Processing Systems, 33:20061\u201320072, 2020. 7\n                                                 14", "md": "# Document\n\nhave parametric classes that can efficiently generate samples (in terms of computation). Under standard computational complexity assumptions, it is not possible to design solution generators which are both efficiently optimizable and samplable for challenging combinatorial problems. An interesting direction is to relax our goal to generating approximately optimal solutions:\n\nOpen Question 1. Are there complete, compressed, efficiently optimizable and samplable solution generators that obtain non-trivial approximation guarantees for challenging combinatorial tasks?\n\n### References\n\n|[AAM22]|Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782\u20134887. PMLR, 2022.|\n|---|---|\n|[ABA22]|Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning: quantifying the cost of symmetry. arXiv preprint arXiv:2208.03113, 2022.|\n|[ABAB+21]|Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. Advances in Neural Information Processing Systems, 34:26989\u201327002, 2021.|\n|[ABAM23]|Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. arXiv preprint arXiv:2302.11055, 2023.|\n|[AEG+22]|Simon Apers, Yuval Efron, Pawel Gawrychowski, Troy Lee, Sagnik Mukhopadhyay, and Danupon Nanongkai. Cut query algorithms with star contraction. arXiv preprint arXiv:2201.05674, 2022.|\n|[AKM+21]|Emmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro. On the power of differentiable learning versus pac and sq learning. Advances in Neural Information Processing Systems, 34:24340\u201324351, 2021.|\n|[AMW18]|Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-sat: An unsupervised differentiable approach. In International Conference on Learning Representations, 2018.|\n|[ART23]|Maria Chiara Angelini and Federico Ricci-Tersenghi. Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set. Nature Machine Intelligence, 5(1):29\u201331, 2023.|\n|[AS18]|Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. arXiv preprint arXiv:1812.06369, 2018.|\n|[AS20]|Emmanuel Abbe and Colin Sandon. On the universality of deep learning. Advances in Neural Information Processing Systems, 33:20061\u201320072, 2020.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "have parametric classes that can efficiently generate samples (in terms of computation). Under standard computational complexity assumptions, it is not possible to design solution generators which are both efficiently optimizable and samplable for challenging combinatorial problems. An interesting direction is to relax our goal to generating approximately optimal solutions:\n\nOpen Question 1. Are there complete, compressed, efficiently optimizable and samplable solution generators that obtain non-trivial approximation guarantees for challenging combinatorial tasks?", "md": "have parametric classes that can efficiently generate samples (in terms of computation). Under standard computational complexity assumptions, it is not possible to design solution generators which are both efficiently optimizable and samplable for challenging combinatorial problems. An interesting direction is to relax our goal to generating approximately optimal solutions:\n\nOpen Question 1. Are there complete, compressed, efficiently optimizable and samplable solution generators that obtain non-trivial approximation guarantees for challenging combinatorial tasks?"}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "table", "rows": [["[AAM22]", "Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782\u20134887. PMLR, 2022."], ["[ABA22]", "Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning: quantifying the cost of symmetry. arXiv preprint arXiv:2208.03113, 2022."], ["[ABAB+21]", "Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. Advances in Neural Information Processing Systems, 34:26989\u201327002, 2021."], ["[ABAM23]", "Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. arXiv preprint arXiv:2302.11055, 2023."], ["[AEG+22]", "Simon Apers, Yuval Efron, Pawel Gawrychowski, Troy Lee, Sagnik Mukhopadhyay, and Danupon Nanongkai. Cut query algorithms with star contraction. arXiv preprint arXiv:2201.05674, 2022."], ["[AKM+21]", "Emmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro. On the power of differentiable learning versus pac and sq learning. Advances in Neural Information Processing Systems, 34:24340\u201324351, 2021."], ["[AMW18]", "Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-sat: An unsupervised differentiable approach. In International Conference on Learning Representations, 2018."], ["[ART23]", "Maria Chiara Angelini and Federico Ricci-Tersenghi. Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set. Nature Machine Intelligence, 5(1):29\u201331, 2023."], ["[AS18]", "Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. arXiv preprint arXiv:1812.06369, 2018."], ["[AS20]", "Emmanuel Abbe and Colin Sandon. On the universality of deep learning. Advances in Neural Information Processing Systems, 33:20061\u201320072, 2020."]], "md": "|[AAM22]|Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782\u20134887. PMLR, 2022.|\n|---|---|\n|[ABA22]|Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning: quantifying the cost of symmetry. arXiv preprint arXiv:2208.03113, 2022.|\n|[ABAB+21]|Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. Advances in Neural Information Processing Systems, 34:26989\u201327002, 2021.|\n|[ABAM23]|Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. arXiv preprint arXiv:2302.11055, 2023.|\n|[AEG+22]|Simon Apers, Yuval Efron, Pawel Gawrychowski, Troy Lee, Sagnik Mukhopadhyay, and Danupon Nanongkai. Cut query algorithms with star contraction. arXiv preprint arXiv:2201.05674, 2022.|\n|[AKM+21]|Emmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro. On the power of differentiable learning versus pac and sq learning. Advances in Neural Information Processing Systems, 34:24340\u201324351, 2021.|\n|[AMW18]|Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-sat: An unsupervised differentiable approach. In International Conference on Learning Representations, 2018.|\n|[ART23]|Maria Chiara Angelini and Federico Ricci-Tersenghi. Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set. Nature Machine Intelligence, 5(1):29\u201331, 2023.|\n|[AS18]|Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. arXiv preprint arXiv:1812.06369, 2018.|\n|[AS20]|Emmanuel Abbe and Colin Sandon. On the universality of deep learning. Advances in Neural Information Processing Systems, 33:20061\u201320072, 2020.|", "isPerfectTable": true, "csv": "\"[AAM22]\",\"Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782\u20134887. PMLR, 2022.\"\n\"[ABA22]\",\"Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning: quantifying the cost of symmetry. arXiv preprint arXiv:2208.03113, 2022.\"\n\"[ABAB+21]\",\"Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. Advances in Neural Information Processing Systems, 34:26989\u201327002, 2021.\"\n\"[ABAM23]\",\"Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. arXiv preprint arXiv:2302.11055, 2023.\"\n\"[AEG+22]\",\"Simon Apers, Yuval Efron, Pawel Gawrychowski, Troy Lee, Sagnik Mukhopadhyay, and Danupon Nanongkai. Cut query algorithms with star contraction. arXiv preprint arXiv:2201.05674, 2022.\"\n\"[AKM+21]\",\"Emmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro. On the power of differentiable learning versus pac and sq learning. Advances in Neural Information Processing Systems, 34:24340\u201324351, 2021.\"\n\"[AMW18]\",\"Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-sat: An unsupervised differentiable approach. In International Conference on Learning Representations, 2018.\"\n\"[ART23]\",\"Maria Chiara Angelini and Federico Ricci-Tersenghi. Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set. Nature Machine Intelligence, 5(1):29\u201331, 2023.\"\n\"[AS18]\",\"Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. arXiv preprint arXiv:1812.06369, 2018.\"\n\"[AS20]\",\"Emmanuel Abbe and Colin Sandon. On the universality of deep learning. Advances in Neural Information Processing Systems, 33:20061\u201320072, 2020.\""}]}, {"page": 15, "text": "[BBSS22]      Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-\n              index models with shallow neural networks. Advances in Neural Information Process-\n              ing Systems, 35:9768\u20139783, 2022. 7\n[BEG+22]      Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and\n              Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the com-\n              putational limit. arXiv preprint arXiv:2207.08799, 2022. 7\n[BLP21]       Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combina-\n              torial optimization: a methodological tour d\u2019horizon. European Journal of Operational\n              Research, 290(2):405\u2013421, 2021. 6\n[BPL+16]      Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio.\n              Neural combinatorial optimization with reinforcement learning.                   arXiv preprint\n              arXiv:1611.09940, 2016. 1, 2, 3, 6, 7\n[CCK+21]      Quentin Cappart, Didier Ch\u00e9telat, Elias Khalil, Andrea Lodi, Christopher Morris,\n              and Petar Veli\u02c7  ckovi\u00b4c. Combinatorial optimization and reasoning with graph neural\n              networks. arXiv preprint arXiv:2102.09544, 2021. 6\n[CGG+19]      Zongchen Chen, Andreas Galanis, Leslie Ann Goldberg, Will Perkins, James Stewart,\n              and Eric Vigoda. Fast algorithms at low temperatures via markov chains. arXiv\n              preprint arXiv:1901.06653, 2019. 34\n[CGKM22] Sitan Chen, Aravind Gollakota, Adam Klivans, and Raghu Meka. Hardness of noise-\n              free learning for two-hidden-layer neural networks. Advances in Neural Information\n              Processing Systems, 35:10709\u201310724, 2022. 6\n[CLS+95]      William Cook, L\u00e1szl\u00f3 Lov\u00e1sz, Paul D Seymour, et al.                Combinatorial optimization:\n              papers from the DIMACS Special Year, volume 20. American Mathematical Soc., 1995.\n              2, 6\n[CLV22]       Zongchen Chen, Kuikui Liu, and Eric Vigoda. Spectral independence via stability\n              and applications to holant-type problems. In 2021 IEEE 62nd Annual Symposium on\n              Foundations of Computer Science (FOCS), pages 149\u2013160. IEEE, 2022. 34\n[COLMS22] Amin Coja-Oghlan, Philipp Loick, Bal\u00e1zs F Mezei, and Gregory B Sorkin. The ising\n              antiferromagnet and max cut on random regular graphs. SIAM Journal on Discrete\n              Mathematics, 36(2):1306\u20131342, 2022. 33\n[CT19]        Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combi-\n              natorial optimization. Advances in Neural Information Processing Systems, 32, 2019.\n              7\n[CZ22]        Xiaoyu Chen and Xinyuan Zhang. A near-linear time sampler for the ising model.\n              arXiv preprint arXiv:2207.09391, 2022. 34\n[d\u2019A08]       Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM\n              Journal on Optimization, 19(3):1171\u20131183, 2008. 35\n                                                      15", "md": "# References\n\n# References\n\n|[BBSS22]|Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. Advances in Neural Information Processing Systems, 35:9768\u20139783, 2022. 7|\n|---|---|\n|[BEG+22]|Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. arXiv preprint arXiv:2207.08799, 2022. 7|\n|[BLP21]|Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d\u2019horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021. 6|\n|[BPL+16]|Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016. 1, 2, 3, 6, 7|\n|[CCK+21]|Quentin Cappart, Didier Ch\u00e9telat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Veli\u02c7 ckovi\u00b4c. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021. 6|\n|[CGG+19]|Zongchen Chen, Andreas Galanis, Leslie Ann Goldberg, Will Perkins, James Stewart, and Eric Vigoda. Fast algorithms at low temperatures via markov chains. arXiv preprint arXiv:1901.06653, 2019. 34|\n|[CGKM22]|Sitan Chen, Aravind Gollakota, Adam Klivans, and Raghu Meka. Hardness of noise-free learning for two-hidden-layer neural networks. Advances in Neural Information Processing Systems, 35:10709\u201310724, 2022. 6|\n|[CLS+95]|William Cook, L\u00e1szl\u00f3 Lov\u00e1sz, Paul D Seymour, et al. Combinatorial optimization: papers from the DIMACS Special Year, volume 20. American Mathematical Soc., 1995. 2, 6|\n|[CLV22]|Zongchen Chen, Kuikui Liu, and Eric Vigoda. Spectral independence via stability and applications to holant-type problems. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 149\u2013160. IEEE, 2022. 34|\n|[COLMS22]|Amin Coja-Oghlan, Philipp Loick, Bal\u00e1zs F Mezei, and Gregory B Sorkin. The ising antiferromagnet and max cut on random regular graphs. SIAM Journal on Discrete Mathematics, 36(2):1306\u20131342, 2022. 33|\n|[CT19]|Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. Advances in Neural Information Processing Systems, 32, 2019. 7|\n|[CZ22]|Xiaoyu Chen and Xinyuan Zhang. A near-linear time sampler for the ising model. arXiv preprint arXiv:2207.09391, 2022. 34|\n|[d\u2019A08]|Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization, 19(3):1171\u20131183, 2008. 35|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "table", "rows": [["[BBSS22]", "Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. Advances in Neural Information Processing Systems, 35:9768\u20139783, 2022. 7"], ["[BEG+22]", "Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. arXiv preprint arXiv:2207.08799, 2022. 7"], ["[BLP21]", "Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d\u2019horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021. 6"], ["[BPL+16]", "Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016. 1, 2, 3, 6, 7"], ["[CCK+21]", "Quentin Cappart, Didier Ch\u00e9telat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Veli\u02c7 ckovi\u00b4c. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021. 6"], ["[CGG+19]", "Zongchen Chen, Andreas Galanis, Leslie Ann Goldberg, Will Perkins, James Stewart, and Eric Vigoda. Fast algorithms at low temperatures via markov chains. arXiv preprint arXiv:1901.06653, 2019. 34"], ["[CGKM22]", "Sitan Chen, Aravind Gollakota, Adam Klivans, and Raghu Meka. Hardness of noise-free learning for two-hidden-layer neural networks. Advances in Neural Information Processing Systems, 35:10709\u201310724, 2022. 6"], ["[CLS+95]", "William Cook, L\u00e1szl\u00f3 Lov\u00e1sz, Paul D Seymour, et al. Combinatorial optimization: papers from the DIMACS Special Year, volume 20. American Mathematical Soc., 1995. 2, 6"], ["[CLV22]", "Zongchen Chen, Kuikui Liu, and Eric Vigoda. Spectral independence via stability and applications to holant-type problems. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 149\u2013160. IEEE, 2022. 34"], ["[COLMS22]", "Amin Coja-Oghlan, Philipp Loick, Bal\u00e1zs F Mezei, and Gregory B Sorkin. The ising antiferromagnet and max cut on random regular graphs. SIAM Journal on Discrete Mathematics, 36(2):1306\u20131342, 2022. 33"], ["[CT19]", "Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. Advances in Neural Information Processing Systems, 32, 2019. 7"], ["[CZ22]", "Xiaoyu Chen and Xinyuan Zhang. A near-linear time sampler for the ising model. arXiv preprint arXiv:2207.09391, 2022. 34"], ["[d\u2019A08]", "Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization, 19(3):1171\u20131183, 2008. 35"]], "md": "|[BBSS22]|Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. Advances in Neural Information Processing Systems, 35:9768\u20139783, 2022. 7|\n|---|---|\n|[BEG+22]|Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. arXiv preprint arXiv:2207.08799, 2022. 7|\n|[BLP21]|Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d\u2019horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021. 6|\n|[BPL+16]|Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016. 1, 2, 3, 6, 7|\n|[CCK+21]|Quentin Cappart, Didier Ch\u00e9telat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Veli\u02c7 ckovi\u00b4c. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021. 6|\n|[CGG+19]|Zongchen Chen, Andreas Galanis, Leslie Ann Goldberg, Will Perkins, James Stewart, and Eric Vigoda. Fast algorithms at low temperatures via markov chains. arXiv preprint arXiv:1901.06653, 2019. 34|\n|[CGKM22]|Sitan Chen, Aravind Gollakota, Adam Klivans, and Raghu Meka. Hardness of noise-free learning for two-hidden-layer neural networks. Advances in Neural Information Processing Systems, 35:10709\u201310724, 2022. 6|\n|[CLS+95]|William Cook, L\u00e1szl\u00f3 Lov\u00e1sz, Paul D Seymour, et al. Combinatorial optimization: papers from the DIMACS Special Year, volume 20. American Mathematical Soc., 1995. 2, 6|\n|[CLV22]|Zongchen Chen, Kuikui Liu, and Eric Vigoda. Spectral independence via stability and applications to holant-type problems. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 149\u2013160. IEEE, 2022. 34|\n|[COLMS22]|Amin Coja-Oghlan, Philipp Loick, Bal\u00e1zs F Mezei, and Gregory B Sorkin. The ising antiferromagnet and max cut on random regular graphs. SIAM Journal on Discrete Mathematics, 36(2):1306\u20131342, 2022. 33|\n|[CT19]|Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. Advances in Neural Information Processing Systems, 32, 2019. 7|\n|[CZ22]|Xiaoyu Chen and Xinyuan Zhang. A near-linear time sampler for the ising model. arXiv preprint arXiv:2207.09391, 2022. 34|\n|[d\u2019A08]|Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization, 19(3):1171\u20131183, 2008. 35|", "isPerfectTable": true, "csv": "\"[BBSS22]\",\"Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. Advances in Neural Information Processing Systems, 35:9768\u20139783, 2022. 7\"\n\"[BEG+22]\",\"Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. arXiv preprint arXiv:2207.08799, 2022. 7\"\n\"[BLP21]\",\"Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d\u2019horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021. 6\"\n\"[BPL+16]\",\"Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016. 1, 2, 3, 6, 7\"\n\"[CCK+21]\",\"Quentin Cappart, Didier Ch\u00e9telat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Veli\u02c7 ckovi\u00b4c. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021. 6\"\n\"[CGG+19]\",\"Zongchen Chen, Andreas Galanis, Leslie Ann Goldberg, Will Perkins, James Stewart, and Eric Vigoda. Fast algorithms at low temperatures via markov chains. arXiv preprint arXiv:1901.06653, 2019. 34\"\n\"[CGKM22]\",\"Sitan Chen, Aravind Gollakota, Adam Klivans, and Raghu Meka. Hardness of noise-free learning for two-hidden-layer neural networks. Advances in Neural Information Processing Systems, 35:10709\u201310724, 2022. 6\"\n\"[CLS+95]\",\"William Cook, L\u00e1szl\u00f3 Lov\u00e1sz, Paul D Seymour, et al. Combinatorial optimization: papers from the DIMACS Special Year, volume 20. American Mathematical Soc., 1995. 2, 6\"\n\"[CLV22]\",\"Zongchen Chen, Kuikui Liu, and Eric Vigoda. Spectral independence via stability and applications to holant-type problems. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 149\u2013160. IEEE, 2022. 34\"\n\"[COLMS22]\",\"Amin Coja-Oghlan, Philipp Loick, Bal\u00e1zs F Mezei, and Gregory B Sorkin. The ising antiferromagnet and max cut on random regular graphs. SIAM Journal on Discrete Mathematics, 36(2):1306\u20131342, 2022. 33\"\n\"[CT19]\",\"Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. Advances in Neural Information Processing Systems, 32, 2019. 7\"\n\"[CZ22]\",\"Xiaoyu Chen and Xinyuan Zhang. A near-linear time sampler for the ising model. arXiv preprint arXiv:2207.09391, 2022. 34\"\n\"[d\u2019A08]\",\"Alexandre d\u2019Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Optimization, 19(3):1171\u20131183, 2008. 35\""}]}, {"page": 16, "text": "[DAT20]   Arthur Delarue, Ross Anderson, and Christian Tjandraatmadja.                Reinforcement\n          learning with combinatorial actions: An application to vehicle routing. Advances\n          in Neural Information Processing Systems, 33:609\u2013620, 2020. 7\n[DCL+18]  Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-\n          Martin Rousseau. Learning heuristics for the tsp by policy gradient. In International\n          conference on the integration of constraint programming, artificial intelligence, and opera-\n          tions research, pages 170\u2013181. Springer, 2018. 6\n[DLS22]   Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi.              Neural networks can\n          learn representations with gradient descent. In Conference on Learning Theory, pages\n          5413\u20135452. PMLR, 2022. 7\n[dPS97]   JC Angl\u00e8s d\u2019Auriac, M Preissmann, and A Seb\u00f6. Optimal cuts in graphs and statis-\n          tical mechanics. Mathematical and Computer Modelling, 26(8-10):1\u201311, 1997. 34\n[Edm65]   Jack Edmonds. Maximum matching and a polyhedron with 0, 1-vertices. Journal of\n          research of the National Bureau of Standards B, 69(125-130):55\u201356, 1965. 36\n[EGK+23]  Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.\n          Pareto frontiers in neural feature learning: Data, compute, width, and luck. arXiv\n          preprint arXiv:2309.03800, 2023. 7\n[ER18]    Patrick Emami and Sanjay Ranka. Learning permutations with sinkhorn policy gra-\n          dient. arXiv preprint arXiv:1805.07010, 2018. 6\n[Gam23]   David Gamarnik. Barriers for the performance of graph neural networks (gnn) in\n          discrete random structures. a comment on \\cite {schuetz2022combinatorial},\\cite\n          {angelini2023modern},\\cite {schuetz2023reply}.           arXiv preprint arXiv:2306.02555,\n          2023. 7\n[GCF+19]  Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi.\n          Exact combinatorial optimization with graph convolutional neural networks. Ad-\n          vances in Neural Information Processing Systems, 32, 2019. 6, 7\n[GLS12]   Martin Gr\u00f6tschel, L\u00e1szl\u00f3 Lov\u00e1sz, and Alexander Schrijver. Geometric algorithms and\n          combinatorial optimization, volume 2. Springer Science & Business Media, 2012. 2, 6\n[GPRW19]  Andrei Graur, Tristan Pollner, Vidhya Ramaswamy, and S Matthew Weinberg.\n          New query lower bounds for submodular function minimization.                 arXiv preprint\n          arXiv:1911.06889, 2019. 2\n[HLSS15]  Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz.            Beyond convexity: Stochastic\n          quasi-convex optimization. Advances in neural information processing systems, 28, 2015.\n          10\n[HMR16]   Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dy-\n          namical systems. arXiv preprint arXiv:1609.05191, 2016. 10, 12, 21, 29, 30\n                                                16", "md": "# References\n\n# References\n\n|[DAT20]|Arthur Delarue, Ross Anderson, and Christian Tjandraatmadja. Reinforcement learning with combinatorial actions: An application to vehicle routing. Advances in Neural Information Processing Systems, 33:609\u2013620, 2020.|\n|---|---|\n|[DCL+18]|Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau. Learning heuristics for the tsp by policy gradient. In International conference on the integration of constraint programming, artificial intelligence, and operations research, pages 170\u2013181. Springer, 2018.|\n|[DLS22]|Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413\u20135452. PMLR, 2022.|\n|[dPS97]|JC Angl\u00e8s d\u2019Auriac, M Preissmann, and A Seb\u00f6. Optimal cuts in graphs and statistical mechanics. Mathematical and Computer Modelling, 26(8-10):1\u201311, 1997.|\n|[Edm65]|Jack Edmonds. Maximum matching and a polyhedron with 0, 1-vertices. Journal of research of the National Bureau of Standards B, 69(125-130):55\u201356, 1965.|\n|[EGK+23]|Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Pareto frontiers in neural feature learning: Data, compute, width, and luck. arXiv preprint arXiv:2309.03800, 2023.|\n|[ER18]|Patrick Emami and Sanjay Ranka. Learning permutations with sinkhorn policy gradient. arXiv preprint arXiv:1805.07010, 2018.|\n|[Gam23]|David Gamarnik. Barriers for the performance of graph neural networks (gnn) in discrete random structures. a comment on \\cite {schuetz2022combinatorial},\\cite {angelini2023modern},\\cite {schuetz2023reply}. arXiv preprint arXiv:2306.02555, 2023.|\n|[GCF+19]|Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019.|\n|[GLS12]|Martin Gr\u00f6tschel, L\u00e1szl\u00f3 Lov\u00e1sz, and Alexander Schrijver. Geometric algorithms and combinatorial optimization, volume 2. Springer Science & Business Media, 2012.|\n|[GPRW19]|Andrei Graur, Tristan Pollner, Vidhya Ramaswamy, and S Matthew Weinberg. New query lower bounds for submodular function minimization. arXiv preprint arXiv:1911.06889, 2019.|\n|[HLSS15]|Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. Advances in neural information processing systems, 28, 2015.|\n|[HMR16]|Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv preprint arXiv:1609.05191, 2016.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "table", "rows": [["[DAT20]", "Arthur Delarue, Ross Anderson, and Christian Tjandraatmadja. Reinforcement learning with combinatorial actions: An application to vehicle routing. Advances in Neural Information Processing Systems, 33:609\u2013620, 2020."], ["[DCL+18]", "Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau. Learning heuristics for the tsp by policy gradient. In International conference on the integration of constraint programming, artificial intelligence, and operations research, pages 170\u2013181. Springer, 2018."], ["[DLS22]", "Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413\u20135452. PMLR, 2022."], ["[dPS97]", "JC Angl\u00e8s d\u2019Auriac, M Preissmann, and A Seb\u00f6. Optimal cuts in graphs and statistical mechanics. Mathematical and Computer Modelling, 26(8-10):1\u201311, 1997."], ["[Edm65]", "Jack Edmonds. Maximum matching and a polyhedron with 0, 1-vertices. Journal of research of the National Bureau of Standards B, 69(125-130):55\u201356, 1965."], ["[EGK+23]", "Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Pareto frontiers in neural feature learning: Data, compute, width, and luck. arXiv preprint arXiv:2309.03800, 2023."], ["[ER18]", "Patrick Emami and Sanjay Ranka. Learning permutations with sinkhorn policy gradient. arXiv preprint arXiv:1805.07010, 2018."], ["[Gam23]", "David Gamarnik. Barriers for the performance of graph neural networks (gnn) in discrete random structures. a comment on \\cite {schuetz2022combinatorial},\\cite {angelini2023modern},\\cite {schuetz2023reply}. arXiv preprint arXiv:2306.02555, 2023."], ["[GCF+19]", "Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019."], ["[GLS12]", "Martin Gr\u00f6tschel, L\u00e1szl\u00f3 Lov\u00e1sz, and Alexander Schrijver. Geometric algorithms and combinatorial optimization, volume 2. Springer Science & Business Media, 2012."], ["[GPRW19]", "Andrei Graur, Tristan Pollner, Vidhya Ramaswamy, and S Matthew Weinberg. New query lower bounds for submodular function minimization. arXiv preprint arXiv:1911.06889, 2019."], ["[HLSS15]", "Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. Advances in neural information processing systems, 28, 2015."], ["[HMR16]", "Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv preprint arXiv:1609.05191, 2016."]], "md": "|[DAT20]|Arthur Delarue, Ross Anderson, and Christian Tjandraatmadja. Reinforcement learning with combinatorial actions: An application to vehicle routing. Advances in Neural Information Processing Systems, 33:609\u2013620, 2020.|\n|---|---|\n|[DCL+18]|Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau. Learning heuristics for the tsp by policy gradient. In International conference on the integration of constraint programming, artificial intelligence, and operations research, pages 170\u2013181. Springer, 2018.|\n|[DLS22]|Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413\u20135452. PMLR, 2022.|\n|[dPS97]|JC Angl\u00e8s d\u2019Auriac, M Preissmann, and A Seb\u00f6. Optimal cuts in graphs and statistical mechanics. Mathematical and Computer Modelling, 26(8-10):1\u201311, 1997.|\n|[Edm65]|Jack Edmonds. Maximum matching and a polyhedron with 0, 1-vertices. Journal of research of the National Bureau of Standards B, 69(125-130):55\u201356, 1965.|\n|[EGK+23]|Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Pareto frontiers in neural feature learning: Data, compute, width, and luck. arXiv preprint arXiv:2309.03800, 2023.|\n|[ER18]|Patrick Emami and Sanjay Ranka. Learning permutations with sinkhorn policy gradient. arXiv preprint arXiv:1805.07010, 2018.|\n|[Gam23]|David Gamarnik. Barriers for the performance of graph neural networks (gnn) in discrete random structures. a comment on \\cite {schuetz2022combinatorial},\\cite {angelini2023modern},\\cite {schuetz2023reply}. arXiv preprint arXiv:2306.02555, 2023.|\n|[GCF+19]|Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019.|\n|[GLS12]|Martin Gr\u00f6tschel, L\u00e1szl\u00f3 Lov\u00e1sz, and Alexander Schrijver. Geometric algorithms and combinatorial optimization, volume 2. Springer Science & Business Media, 2012.|\n|[GPRW19]|Andrei Graur, Tristan Pollner, Vidhya Ramaswamy, and S Matthew Weinberg. New query lower bounds for submodular function minimization. arXiv preprint arXiv:1911.06889, 2019.|\n|[HLSS15]|Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. Advances in neural information processing systems, 28, 2015.|\n|[HMR16]|Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv preprint arXiv:1609.05191, 2016.|", "isPerfectTable": true, "csv": "\"[DAT20]\",\"Arthur Delarue, Ross Anderson, and Christian Tjandraatmadja. Reinforcement learning with combinatorial actions: An application to vehicle routing. Advances in Neural Information Processing Systems, 33:609\u2013620, 2020.\"\n\"[DCL+18]\",\"Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau. Learning heuristics for the tsp by policy gradient. In International conference on the integration of constraint programming, artificial intelligence, and operations research, pages 170\u2013181. Springer, 2018.\"\n\"[DLS22]\",\"Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413\u20135452. PMLR, 2022.\"\n\"[dPS97]\",\"JC Angl\u00e8s d\u2019Auriac, M Preissmann, and A Seb\u00f6. Optimal cuts in graphs and statistical mechanics. Mathematical and Computer Modelling, 26(8-10):1\u201311, 1997.\"\n\"[Edm65]\",\"Jack Edmonds. Maximum matching and a polyhedron with 0, 1-vertices. Journal of research of the National Bureau of Standards B, 69(125-130):55\u201356, 1965.\"\n\"[EGK+23]\",\"Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Pareto frontiers in neural feature learning: Data, compute, width, and luck. arXiv preprint arXiv:2309.03800, 2023.\"\n\"[ER18]\",\"Patrick Emami and Sanjay Ranka. Learning permutations with sinkhorn policy gradient. arXiv preprint arXiv:1805.07010, 2018.\"\n\"[Gam23]\",\"David Gamarnik. Barriers for the performance of graph neural networks (gnn) in discrete random structures. a comment on \\cite {schuetz2022combinatorial},\\cite {angelini2023modern},\\cite {schuetz2023reply}. arXiv preprint arXiv:2306.02555, 2023.\"\n\"[GCF+19]\",\"Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019.\"\n\"[GLS12]\",\"Martin Gr\u00f6tschel, L\u00e1szl\u00f3 Lov\u00e1sz, and Alexander Schrijver. Geometric algorithms and combinatorial optimization, volume 2. Springer Science & Business Media, 2012.\"\n\"[GPRW19]\",\"Andrei Graur, Tristan Pollner, Vidhya Ramaswamy, and S Matthew Weinberg. New query lower bounds for submodular function minimization. arXiv preprint arXiv:1911.06889, 2019.\"\n\"[HLSS15]\",\"Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. Advances in neural information processing systems, 28, 2015.\"\n\"[HMR16]\",\"Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. arXiv preprint arXiv:1609.05191, 2016.\""}]}, {"page": 17, "text": "[HS23a]   Christoph Hertrich and Leon Sering. Relu neural networks of polynomial size for\n          exact maximum flow computation. In International Conference on Integer Programming\n          and Combinatorial Optimization, pages 187\u2013202. Springer, 2023. 7\n[HS23b]   Christoph Hertrich and Martin Skutella. Provably good solutions to the knapsack\n          problem via neural networks of bounded size. INFORMS Journal on Computing, 2023.\n          7\n[HSS20]   Oliver Hinder, Aaron Sidford, and Nimit Sohoni. Near-optimal methods for min-\n          imizing star-convex functions and beyond. In Conference on learning theory, pages\n          1894\u20131938. PMLR, 2020. 10\n[HT85]    John J Hopfield and David W Tank. \u201cneural\u201d computation of decisions in optimiza-\n          tion problems. Biological cybernetics, 52(3):141\u2013152, 1985. 1, 6\n[Jeg22]   Stefanie Jegelka. Theory of graph neural networks: Representation and learning.\n          arXiv preprint arXiv:2204.07697, 2022. 7\n[Jer03]   Mark Jerrum. Counting, sampling and integrating: algorithms and complexity. Springer\n          Science & Business Media, 2003. 38\n[JLB19]   Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph con-\n          volutional network technique for the travelling salesman problem. arXiv preprint\n          arXiv:1906.01227, 2019. 7\n[JS93]    Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for\n          the ising model. SIAM Journal on computing, 22(5):1087\u20131116, 1993. 34\n[JSV04]   Mark Jerrum, Alistair Sinclair, and Eric Vigoda. A polynomial-time approximation\n          algorithm for the permanent of a matrix with nonnegative entries. Journal of the ACM\n          (JACM), 51(4):671\u2013697, 2004. 38\n[Kak01]   Sham M Kakade. A natural policy gradient. Advances in neural information processing\n          systems, 14, 2001. 3\n[KCK+20]  Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and\n          Seungjai Min. Pomo: Policy optimization with multiple optima for reinforcement\n          learning. Advances in Neural Information Processing Systems, 33:21188\u201321198, 2020. 7\n[KCY+21]  Yeong-Dae Kwon, Jinho Choo, Iljoo Yoon, Minah Park, Duwon Park, and Youngjune\n          Gwon. Matrix encoding networks for neural combinatorial optimization. Advances\n          in Neural Information Processing Systems, 34:5138\u20135149, 2021. 7\n[KDZ+17]  Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combi-\n          natorial optimization algorithms over graphs. Advances in neural information process-\n          ing systems, 30, 2017. 7\n[KL20]    Nikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learn-\n          ing framework for combinatorial optimization on graphs. Advances in Neural Infor-\n          mation Processing Systems, 33:6659\u20136672, 2020. 7\n                                               17", "md": "# References\n\n# List of References\n\n|Reference|Authors|Title|Publication Details|Page|\n|---|---|---|---|---|\n|[HS23a]|Christoph Hertrich and Leon Sering|Relu neural networks of polynomial size for exact maximum flow computation|International Conference on Integer Programming and Combinatorial Optimization, Springer, 2023|7|\n|[HS23b]|Christoph Hertrich and Martin Skutella|Provably good solutions to the knapsack problem via neural networks of bounded size|INFORMS Journal on Computing, 2023|7|\n|[HSS20]|Oliver Hinder, Aaron Sidford, and Nimit Sohoni|Near-optimal methods for minimizing star-convex functions and beyond|Conference on learning theory, PMLR, 2020|10|\n|[HT85]|John J Hopfield and David W Tank|\"neural\" computation of decisions in optimization problems|Biological cybernetics, 52(3):141\u2013152, 1985|1, 6|\n|[Jeg22]|Stefanie Jegelka|Theory of graph neural networks: Representation and learning|arXiv preprint arXiv:2204.07697, 2022|7|\n|[Jer03]|Mark Jerrum|Counting, sampling and integrating: algorithms and complexity|Springer Science & Business Media, 2003|38|\n|[JLB19]|Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson|An efficient graph convolutional network technique for the travelling salesman problem|arXiv preprint arXiv:1906.01227, 2019|7|\n|[JS93]|Mark Jerrum and Alistair Sinclair|Polynomial-time approximation algorithms for the Ising model|SIAM Journal on computing, 22(5):1087\u20131116, 1993|34|\n|[JSV04]|Mark Jerrum, Alistair Sinclair, and Eric Vigoda|A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries|Journal of the ACM (JACM), 51(4):671\u2013697, 2004|38|\n|[Kak01]|Sham M Kakade|A natural policy gradient|Advances in neural information processing systems, 14, 2001|3|\n|[KCK+20]|Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min|Pomo: Policy optimization with multiple optima for reinforcement learning|Advances in Neural Information Processing Systems, 33:21188\u201321198, 2020|7|\n|[KCY+21]|Yeong-Dae Kwon, Jinho Choo, Iljoo Yoon, Minah Park, Duwon Park, and Youngjune Gwon|Matrix encoding networks for neural combinatorial optimization|Advances in Neural Information Processing Systems, 34:5138\u20135149, 2021|7|\n|[KDZ+17]|Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song|Learning combinatorial optimization algorithms over graphs|Advances in neural information processing systems, 30, 2017|7|\n|[KL20]|Nikolaos Karalias and Andreas Loukas|Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs|Advances in Neural Information Processing Systems, 33:6659\u20136672, 2020|7|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["Reference", "Authors", "Title", "Publication Details", "Page"], ["[HS23a]", "Christoph Hertrich and Leon Sering", "Relu neural networks of polynomial size for exact maximum flow computation", "International Conference on Integer Programming and Combinatorial Optimization, Springer, 2023", "7"], ["[HS23b]", "Christoph Hertrich and Martin Skutella", "Provably good solutions to the knapsack problem via neural networks of bounded size", "INFORMS Journal on Computing, 2023", "7"], ["[HSS20]", "Oliver Hinder, Aaron Sidford, and Nimit Sohoni", "Near-optimal methods for minimizing star-convex functions and beyond", "Conference on learning theory, PMLR, 2020", "10"], ["[HT85]", "John J Hopfield and David W Tank", "\"neural\" computation of decisions in optimization problems", "Biological cybernetics, 52(3):141\u2013152, 1985", "1, 6"], ["[Jeg22]", "Stefanie Jegelka", "Theory of graph neural networks: Representation and learning", "arXiv preprint arXiv:2204.07697, 2022", "7"], ["[Jer03]", "Mark Jerrum", "Counting, sampling and integrating: algorithms and complexity", "Springer Science & Business Media, 2003", "38"], ["[JLB19]", "Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson", "An efficient graph convolutional network technique for the travelling salesman problem", "arXiv preprint arXiv:1906.01227, 2019", "7"], ["[JS93]", "Mark Jerrum and Alistair Sinclair", "Polynomial-time approximation algorithms for the Ising model", "SIAM Journal on computing, 22(5):1087\u20131116, 1993", "34"], ["[JSV04]", "Mark Jerrum, Alistair Sinclair, and Eric Vigoda", "A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries", "Journal of the ACM (JACM), 51(4):671\u2013697, 2004", "38"], ["[Kak01]", "Sham M Kakade", "A natural policy gradient", "Advances in neural information processing systems, 14, 2001", "3"], ["[KCK+20]", "Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min", "Pomo: Policy optimization with multiple optima for reinforcement learning", "Advances in Neural Information Processing Systems, 33:21188\u201321198, 2020", "7"], ["[KCY+21]", "Yeong-Dae Kwon, Jinho Choo, Iljoo Yoon, Minah Park, Duwon Park, and Youngjune Gwon", "Matrix encoding networks for neural combinatorial optimization", "Advances in Neural Information Processing Systems, 34:5138\u20135149, 2021", "7"], ["[KDZ+17]", "Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song", "Learning combinatorial optimization algorithms over graphs", "Advances in neural information processing systems, 30, 2017", "7"], ["[KL20]", "Nikolaos Karalias and Andreas Loukas", "Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs", "Advances in Neural Information Processing Systems, 33:6659\u20136672, 2020", "7"]], "md": "|Reference|Authors|Title|Publication Details|Page|\n|---|---|---|---|---|\n|[HS23a]|Christoph Hertrich and Leon Sering|Relu neural networks of polynomial size for exact maximum flow computation|International Conference on Integer Programming and Combinatorial Optimization, Springer, 2023|7|\n|[HS23b]|Christoph Hertrich and Martin Skutella|Provably good solutions to the knapsack problem via neural networks of bounded size|INFORMS Journal on Computing, 2023|7|\n|[HSS20]|Oliver Hinder, Aaron Sidford, and Nimit Sohoni|Near-optimal methods for minimizing star-convex functions and beyond|Conference on learning theory, PMLR, 2020|10|\n|[HT85]|John J Hopfield and David W Tank|\"neural\" computation of decisions in optimization problems|Biological cybernetics, 52(3):141\u2013152, 1985|1, 6|\n|[Jeg22]|Stefanie Jegelka|Theory of graph neural networks: Representation and learning|arXiv preprint arXiv:2204.07697, 2022|7|\n|[Jer03]|Mark Jerrum|Counting, sampling and integrating: algorithms and complexity|Springer Science & Business Media, 2003|38|\n|[JLB19]|Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson|An efficient graph convolutional network technique for the travelling salesman problem|arXiv preprint arXiv:1906.01227, 2019|7|\n|[JS93]|Mark Jerrum and Alistair Sinclair|Polynomial-time approximation algorithms for the Ising model|SIAM Journal on computing, 22(5):1087\u20131116, 1993|34|\n|[JSV04]|Mark Jerrum, Alistair Sinclair, and Eric Vigoda|A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries|Journal of the ACM (JACM), 51(4):671\u2013697, 2004|38|\n|[Kak01]|Sham M Kakade|A natural policy gradient|Advances in neural information processing systems, 14, 2001|3|\n|[KCK+20]|Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min|Pomo: Policy optimization with multiple optima for reinforcement learning|Advances in Neural Information Processing Systems, 33:21188\u201321198, 2020|7|\n|[KCY+21]|Yeong-Dae Kwon, Jinho Choo, Iljoo Yoon, Minah Park, Duwon Park, and Youngjune Gwon|Matrix encoding networks for neural combinatorial optimization|Advances in Neural Information Processing Systems, 34:5138\u20135149, 2021|7|\n|[KDZ+17]|Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song|Learning combinatorial optimization algorithms over graphs|Advances in neural information processing systems, 30, 2017|7|\n|[KL20]|Nikolaos Karalias and Andreas Loukas|Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs|Advances in Neural Information Processing Systems, 33:6659\u20136672, 2020|7|", "isPerfectTable": true, "csv": "\"Reference\",\"Authors\",\"Title\",\"Publication Details\",\"Page\"\n\"[HS23a]\",\"Christoph Hertrich and Leon Sering\",\"Relu neural networks of polynomial size for exact maximum flow computation\",\"International Conference on Integer Programming and Combinatorial Optimization, Springer, 2023\",\"7\"\n\"[HS23b]\",\"Christoph Hertrich and Martin Skutella\",\"Provably good solutions to the knapsack problem via neural networks of bounded size\",\"INFORMS Journal on Computing, 2023\",\"7\"\n\"[HSS20]\",\"Oliver Hinder, Aaron Sidford, and Nimit Sohoni\",\"Near-optimal methods for minimizing star-convex functions and beyond\",\"Conference on learning theory, PMLR, 2020\",\"10\"\n\"[HT85]\",\"John J Hopfield and David W Tank\",\"\"\"neural\"\" computation of decisions in optimization problems\",\"Biological cybernetics, 52(3):141\u2013152, 1985\",\"1, 6\"\n\"[Jeg22]\",\"Stefanie Jegelka\",\"Theory of graph neural networks: Representation and learning\",\"arXiv preprint arXiv:2204.07697, 2022\",\"7\"\n\"[Jer03]\",\"Mark Jerrum\",\"Counting, sampling and integrating: algorithms and complexity\",\"Springer Science & Business Media, 2003\",\"38\"\n\"[JLB19]\",\"Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson\",\"An efficient graph convolutional network technique for the travelling salesman problem\",\"arXiv preprint arXiv:1906.01227, 2019\",\"7\"\n\"[JS93]\",\"Mark Jerrum and Alistair Sinclair\",\"Polynomial-time approximation algorithms for the Ising model\",\"SIAM Journal on computing, 22(5):1087\u20131116, 1993\",\"34\"\n\"[JSV04]\",\"Mark Jerrum, Alistair Sinclair, and Eric Vigoda\",\"A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries\",\"Journal of the ACM (JACM), 51(4):671\u2013697, 2004\",\"38\"\n\"[Kak01]\",\"Sham M Kakade\",\"A natural policy gradient\",\"Advances in neural information processing systems, 14, 2001\",\"3\"\n\"[KCK+20]\",\"Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min\",\"Pomo: Policy optimization with multiple optima for reinforcement learning\",\"Advances in Neural Information Processing Systems, 33:21188\u201321198, 2020\",\"7\"\n\"[KCY+21]\",\"Yeong-Dae Kwon, Jinho Choo, Iljoo Yoon, Minah Park, Duwon Park, and Youngjune Gwon\",\"Matrix encoding networks for neural combinatorial optimization\",\"Advances in Neural Information Processing Systems, 34:5138\u20135149, 2021\",\"7\"\n\"[KDZ+17]\",\"Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song\",\"Learning combinatorial optimization algorithms over graphs\",\"Advances in neural information processing systems, 30, 2017\",\"7\"\n\"[KL20]\",\"Nikolaos Karalias and Andreas Loukas\",\"Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs\",\"Advances in Neural Information Processing Systems, 33:6659\u20136672, 2020\",\"7\""}]}, {"page": 18, "text": "[KLMS19]      Weiwei Kong, Christopher Liaw, Aranyak Mehta, and D Sivakumar. A new dog\n              learns old tricks: Rl finds classic optimization algorithms. In International conference\n              on learning representations, 2019. 6\n[KP+21]       Minsu Kim, Jinkyoo Park, et al. Learning collaborative policies to solve np-hard\n              routing problems. Advances in Neural Information Processing Systems, 34:10418\u201310430,\n              2021. 7\n[KPP22]       Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym-nco: Leveraging symmetricity\n              for neural combinatorial optimization. arXiv preprint arXiv:2205.13209, 2022. 7\n[KVHW18] Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing\n              problems! arXiv preprint arXiv:1803.08475, 2018. 6\n[LSS19]       Jingcheng Liu, Alistair Sinclair, and Piyush Srivastava. The ising partition function:\n              Zeros and deterministic approximation. Journal of Statistical Physics, 174(2):287\u2013315,\n              2019. 34\n[LSZ21]       Troy Lee, Miklos Santha, and Shengyu Zhang. Quantum algorithms for graph prob-\n              lems with cut queries. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete\n              Algorithms (SODA), pages 939\u2013958. SIAM, 2021. 2\n[LV16]        Jasper CH Lee and Paul Valiant. Optimizing star-convex functions. In 2016 IEEE\n              57th Annual Symposium on Foundations of Computer Science (FOCS), pages 603\u2013614.\n              IEEE, 2016. 10\n[LZ09]        Fei Liu and Guangzhou Zeng. Study of genetic algorithm with reinforcement learn-\n              ing to solve the tsp. Expert Systems with Applications, 36(3):6995\u20137001, 2009. 6\n[MGH+19] Qiang Ma, Suwen Ge, Danyang He, Darshan Thaker, and Iddo Drori. Combinatorial\n              optimization by graph pointer networks and hierarchical reinforcement learning.\n              arXiv preprint arXiv:1911.04936, 2019. 6\n[MKS+13]      Volodymyr Mnih,       Koray Kavukcuoglu,        David Silver,    Alex Graves,      Ioannis\n              Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep rein-\n              forcement learning. arXiv preprint arXiv:1312.5602, 2013. 6\n[MLC+21]      Yining Ma, Jingwen Li, Zhiguang Cao, Wen Song, Le Zhang, Zhenghua Chen, and\n              Jing Tang. Learning to iteratively solve routing problems with dual-aspect collabo-\n              rative transformer. Advances in Neural Information Processing Systems, 34:11096\u201311107,\n              2021. 7\n[MSIB21]      Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforce-\n              ment learning for combinatorial optimization: A survey. Computers & Operations\n              Research, 134:105400, 2021. 6, 7\n[MYSSS21] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. The connec-\n              tion between approximation, depth separation and learnability in neural networks.\n              In Conference on Learning Theory, pages 3265\u20133295. PMLR, 2021. 7\n                                                   18", "md": "# References\n\n# List of References\n\n## [KLMS19] Weiwei Kong, Christopher Liaw, Aranyak Mehta, and D Sivakumar. A new dog learns old tricks: Rl finds classic optimization algorithms. In International conference on learning representations, 2019. 6\n\n## [KP+21] Minsu Kim, Jinkyoo Park, et al. Learning collaborative policies to solve np-hard routing problems. Advances in Neural Information Processing Systems, 34:10418\u201310430, 2021. 7\n\n## [KPP22] Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym-nco: Leveraging symmetricity for neural combinatorial optimization. arXiv preprint arXiv:2205.13209, 2022. 7\n\n## [KVHW18] Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475, 2018. 6\n\n## [LSS19] Jingcheng Liu, Alistair Sinclair, and Piyush Srivastava. The ising partition function: Zeros and deterministic approximation. Journal of Statistical Physics, 174(2):287\u2013315, 2019. 34\n\n## [LSZ21] Troy Lee, Miklos Santha, and Shengyu Zhang. Quantum algorithms for graph problems with cut queries. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 939\u2013958. SIAM, 2021. 2\n\n## [LV16] Jasper CH Lee and Paul Valiant. Optimizing star-convex functions. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 603\u2013614. IEEE, 2016. 10\n\n## [LZ09] Fei Liu and Guangzhou Zeng. Study of genetic algorithm with reinforcement learning to solve the tsp. Expert Systems with Applications, 36(3):6995\u20137001, 2009. 6\n\n## [MGH+19] Qiang Ma, Suwen Ge, Danyang He, Darshan Thaker, and Iddo Drori. Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning. arXiv preprint arXiv:1911.04936, 2019. 6\n\n## [MKS+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 6\n\n## [MLC+21] Yining Ma, Jingwen Li, Zhiguang Cao, Wen Song, Le Zhang, Zhenghua Chen, and Jing Tang. Learning to iteratively solve routing problems with dual-aspect collaborative transformer. Advances in Neural Information Processing Systems, 34:11096\u201311107, 2021. 7\n\n## [MSIB21] Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial optimization: A survey. Computers & Operations Research, 134:105400, 2021. 6, 7\n\n## [MYSSS21] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. The connection between approximation, depth separation and learnability in neural networks. In Conference on Learning Theory, pages 3265\u20133295. PMLR, 2021. 7", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "heading", "lvl": 2, "value": "[KLMS19] Weiwei Kong, Christopher Liaw, Aranyak Mehta, and D Sivakumar. A new dog learns old tricks: Rl finds classic optimization algorithms. In International conference on learning representations, 2019. 6", "md": "## [KLMS19] Weiwei Kong, Christopher Liaw, Aranyak Mehta, and D Sivakumar. A new dog learns old tricks: Rl finds classic optimization algorithms. In International conference on learning representations, 2019. 6"}, {"type": "heading", "lvl": 2, "value": "[KP+21] Minsu Kim, Jinkyoo Park, et al. Learning collaborative policies to solve np-hard routing problems. Advances in Neural Information Processing Systems, 34:10418\u201310430, 2021. 7", "md": "## [KP+21] Minsu Kim, Jinkyoo Park, et al. Learning collaborative policies to solve np-hard routing problems. Advances in Neural Information Processing Systems, 34:10418\u201310430, 2021. 7"}, {"type": "heading", "lvl": 2, "value": "[KPP22] Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym-nco: Leveraging symmetricity for neural combinatorial optimization. arXiv preprint arXiv:2205.13209, 2022. 7", "md": "## [KPP22] Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym-nco: Leveraging symmetricity for neural combinatorial optimization. arXiv preprint arXiv:2205.13209, 2022. 7"}, {"type": "heading", "lvl": 2, "value": "[KVHW18] Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475, 2018. 6", "md": "## [KVHW18] Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475, 2018. 6"}, {"type": "heading", "lvl": 2, "value": "[LSS19] Jingcheng Liu, Alistair Sinclair, and Piyush Srivastava. The ising partition function: Zeros and deterministic approximation. Journal of Statistical Physics, 174(2):287\u2013315, 2019. 34", "md": "## [LSS19] Jingcheng Liu, Alistair Sinclair, and Piyush Srivastava. The ising partition function: Zeros and deterministic approximation. Journal of Statistical Physics, 174(2):287\u2013315, 2019. 34"}, {"type": "heading", "lvl": 2, "value": "[LSZ21] Troy Lee, Miklos Santha, and Shengyu Zhang. Quantum algorithms for graph problems with cut queries. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 939\u2013958. SIAM, 2021. 2", "md": "## [LSZ21] Troy Lee, Miklos Santha, and Shengyu Zhang. Quantum algorithms for graph problems with cut queries. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 939\u2013958. SIAM, 2021. 2"}, {"type": "heading", "lvl": 2, "value": "[LV16] Jasper CH Lee and Paul Valiant. Optimizing star-convex functions. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 603\u2013614. IEEE, 2016. 10", "md": "## [LV16] Jasper CH Lee and Paul Valiant. Optimizing star-convex functions. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 603\u2013614. IEEE, 2016. 10"}, {"type": "heading", "lvl": 2, "value": "[LZ09] Fei Liu and Guangzhou Zeng. Study of genetic algorithm with reinforcement learning to solve the tsp. Expert Systems with Applications, 36(3):6995\u20137001, 2009. 6", "md": "## [LZ09] Fei Liu and Guangzhou Zeng. Study of genetic algorithm with reinforcement learning to solve the tsp. Expert Systems with Applications, 36(3):6995\u20137001, 2009. 6"}, {"type": "heading", "lvl": 2, "value": "[MGH+19] Qiang Ma, Suwen Ge, Danyang He, Darshan Thaker, and Iddo Drori. Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning. arXiv preprint arXiv:1911.04936, 2019. 6", "md": "## [MGH+19] Qiang Ma, Suwen Ge, Danyang He, Darshan Thaker, and Iddo Drori. Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning. arXiv preprint arXiv:1911.04936, 2019. 6"}, {"type": "heading", "lvl": 2, "value": "[MKS+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 6", "md": "## [MKS+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 6"}, {"type": "heading", "lvl": 2, "value": "[MLC+21] Yining Ma, Jingwen Li, Zhiguang Cao, Wen Song, Le Zhang, Zhenghua Chen, and Jing Tang. Learning to iteratively solve routing problems with dual-aspect collaborative transformer. Advances in Neural Information Processing Systems, 34:11096\u201311107, 2021. 7", "md": "## [MLC+21] Yining Ma, Jingwen Li, Zhiguang Cao, Wen Song, Le Zhang, Zhenghua Chen, and Jing Tang. Learning to iteratively solve routing problems with dual-aspect collaborative transformer. Advances in Neural Information Processing Systems, 34:11096\u201311107, 2021. 7"}, {"type": "heading", "lvl": 2, "value": "[MSIB21] Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial optimization: A survey. Computers & Operations Research, 134:105400, 2021. 6, 7", "md": "## [MSIB21] Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial optimization: A survey. Computers & Operations Research, 134:105400, 2021. 6, 7"}, {"type": "heading", "lvl": 2, "value": "[MYSSS21] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. The connection between approximation, depth separation and learnability in neural networks. In Conference on Learning Theory, pages 3265\u20133295. PMLR, 2021. 7", "md": "## [MYSSS21] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. The connection between approximation, depth separation and learnability in neural networks. In Conference on Learning Theory, pages 3265\u20133295. PMLR, 2021. 7"}]}, {"page": 19, "text": "[NJS+20]  Yatin Nandwani, Deepanshu Jindal, Parag Singla, et al. Neural learning of one-\n          of-many solutions for combinatorial problems in structured output spaces. arXiv\n          preprint arXiv:2008.11990, 2020. 7\n[NOST18]  Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Tak\u00e1c. Re-\n          inforcement learning for solving the vehicle routing problem.         Advances in neural\n          information processing systems, 31, 2018. 6\n[PRW22]   Orestis Plevrakis, Seyoon Ragavan, and S Matthew Weinberg.            On the cut-query\n          complexity of approximating max-cut. arXiv preprint arXiv:2211.04506, 2022. 2\n[PS98]    Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algo-\n          rithms and complexity. Courier Corporation, 1998. 2, 6\n[RSW17]   Aviad Rubinstein, Tselil Schramm, and S Matthew Weinberg. Computing exact min-\n          imum cuts without knowing the graph. arXiv preprint arXiv:1711.03165, 2017. 2\n[S+03]    Alexander Schrijver et al.    Combinatorial optimization: polyhedra and efficiency, vol-\n          ume 24. Springer, 2003. 2, 6\n[SBK22]   Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial opti-\n          mization with physics-inspired graph neural networks. Nature Machine Intelligence,\n          4(4):367\u2013377, 2022. 7\n[Sch05]   Alexander Schrijver. On the history of combinatorial optimization (till 1960). Hand-\n          books in operations research and management science, 12:1\u201368, 2005. 2, 6\n[SE20]    Yang Song and Stefano Ermon. Improved techniques for training score-based gener-\n          ative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.\n          6\n[SHM+16]  David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\n          Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam,\n          Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree\n          search. nature, 529(7587):484\u2013489, 2016. 6\n[Sin12]   Alistair Sinclair. Algorithms for random generation and counting: a Markov chain ap-\n          proach. Springer Science & Business Media, 2012. 38, 41, 42\n[SLB+18]  Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura,\n          and David L Dill. Learning a sat solver from single-bit supervision. arXiv preprint\n          arXiv:1802.03685, 2018. 7\n[Smi99]   Kate A Smith. Neural networks for combinatorial optimization: a review of more\n          than a decade of research. Informs journal on Computing, 11(1):15\u201334, 1999. 1, 6\n[Spi07]   Daniel A Spielman. Spectral graph theory and its applications. In 48th Annual IEEE\n          Symposium on Foundations of Computer Science (FOCS\u201907), pages 29\u201338. IEEE, 2007. 7\n                                               19", "md": "# References\n\n# References\n\n|[NJS+20]|Yatin Nandwani, Deepanshu Jindal, Parag Singla, et al. Neural learning of one-of-many solutions for combinatorial problems in structured output spaces. arXiv preprint arXiv:2008.11990, 2020.|\n|---|---|\n|[NOST18]|Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Tak\u00e1c. Reinforcement learning for solving the vehicle routing problem. Advances in neural information processing systems, 31, 2018.|\n|[PRW22]|Orestis Plevrakis, Seyoon Ragavan, and S Matthew Weinberg. On the cut-query complexity of approximating max-cut. arXiv preprint arXiv:2211.04506, 2022.|\n|[PS98]|Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity. Courier Corporation, 1998.|\n|[RSW17]|Aviad Rubinstein, Tselil Schramm, and S Matthew Weinberg. Computing exact minimum cuts without knowing the graph. arXiv preprint arXiv:1711.03165, 2017.|\n|[S+03]|Alexander Schrijver et al. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer, 2003.|\n|[SBK22]|Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics-inspired graph neural networks. Nature Machine Intelligence, 4(4):367\u2013377, 2022.|\n|[Sch05]|Alexander Schrijver. On the history of combinatorial optimization (till 1960). Handbooks in operations research and management science, 12:1\u201368, 2005.|\n|[SE20]|Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.|\n|[SHM+16]|David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.|\n|[Sin12]|Alistair Sinclair. Algorithms for random generation and counting: a Markov chain approach. Springer Science & Business Media, 2012.|\n|[SLB+18]|Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.|\n|[Smi99]|Kate A Smith. Neural networks for combinatorial optimization: a review of more than a decade of research. Informs journal on Computing, 11(1):15\u201334, 1999.|\n|[Spi07]|Daniel A Spielman. Spectral graph theory and its applications. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907), pages 29\u201338. IEEE, 2007.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "table", "rows": [["[NJS+20]", "Yatin Nandwani, Deepanshu Jindal, Parag Singla, et al. Neural learning of one-of-many solutions for combinatorial problems in structured output spaces. arXiv preprint arXiv:2008.11990, 2020."], ["[NOST18]", "Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Tak\u00e1c. Reinforcement learning for solving the vehicle routing problem. Advances in neural information processing systems, 31, 2018."], ["[PRW22]", "Orestis Plevrakis, Seyoon Ragavan, and S Matthew Weinberg. On the cut-query complexity of approximating max-cut. arXiv preprint arXiv:2211.04506, 2022."], ["[PS98]", "Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity. Courier Corporation, 1998."], ["[RSW17]", "Aviad Rubinstein, Tselil Schramm, and S Matthew Weinberg. Computing exact minimum cuts without knowing the graph. arXiv preprint arXiv:1711.03165, 2017."], ["[S+03]", "Alexander Schrijver et al. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer, 2003."], ["[SBK22]", "Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics-inspired graph neural networks. Nature Machine Intelligence, 4(4):367\u2013377, 2022."], ["[Sch05]", "Alexander Schrijver. On the history of combinatorial optimization (till 1960). Handbooks in operations research and management science, 12:1\u201368, 2005."], ["[SE20]", "Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020."], ["[SHM+16]", "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016."], ["[Sin12]", "Alistair Sinclair. Algorithms for random generation and counting: a Markov chain approach. Springer Science & Business Media, 2012."], ["[SLB+18]", "Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018."], ["[Smi99]", "Kate A Smith. Neural networks for combinatorial optimization: a review of more than a decade of research. Informs journal on Computing, 11(1):15\u201334, 1999."], ["[Spi07]", "Daniel A Spielman. Spectral graph theory and its applications. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907), pages 29\u201338. IEEE, 2007."]], "md": "|[NJS+20]|Yatin Nandwani, Deepanshu Jindal, Parag Singla, et al. Neural learning of one-of-many solutions for combinatorial problems in structured output spaces. arXiv preprint arXiv:2008.11990, 2020.|\n|---|---|\n|[NOST18]|Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Tak\u00e1c. Reinforcement learning for solving the vehicle routing problem. Advances in neural information processing systems, 31, 2018.|\n|[PRW22]|Orestis Plevrakis, Seyoon Ragavan, and S Matthew Weinberg. On the cut-query complexity of approximating max-cut. arXiv preprint arXiv:2211.04506, 2022.|\n|[PS98]|Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity. Courier Corporation, 1998.|\n|[RSW17]|Aviad Rubinstein, Tselil Schramm, and S Matthew Weinberg. Computing exact minimum cuts without knowing the graph. arXiv preprint arXiv:1711.03165, 2017.|\n|[S+03]|Alexander Schrijver et al. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer, 2003.|\n|[SBK22]|Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics-inspired graph neural networks. Nature Machine Intelligence, 4(4):367\u2013377, 2022.|\n|[Sch05]|Alexander Schrijver. On the history of combinatorial optimization (till 1960). Handbooks in operations research and management science, 12:1\u201368, 2005.|\n|[SE20]|Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.|\n|[SHM+16]|David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.|\n|[Sin12]|Alistair Sinclair. Algorithms for random generation and counting: a Markov chain approach. Springer Science & Business Media, 2012.|\n|[SLB+18]|Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.|\n|[Smi99]|Kate A Smith. Neural networks for combinatorial optimization: a review of more than a decade of research. Informs journal on Computing, 11(1):15\u201334, 1999.|\n|[Spi07]|Daniel A Spielman. Spectral graph theory and its applications. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907), pages 29\u201338. IEEE, 2007.|", "isPerfectTable": true, "csv": "\"[NJS+20]\",\"Yatin Nandwani, Deepanshu Jindal, Parag Singla, et al. Neural learning of one-of-many solutions for combinatorial problems in structured output spaces. arXiv preprint arXiv:2008.11990, 2020.\"\n\"[NOST18]\",\"Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Tak\u00e1c. Reinforcement learning for solving the vehicle routing problem. Advances in neural information processing systems, 31, 2018.\"\n\"[PRW22]\",\"Orestis Plevrakis, Seyoon Ragavan, and S Matthew Weinberg. On the cut-query complexity of approximating max-cut. arXiv preprint arXiv:2211.04506, 2022.\"\n\"[PS98]\",\"Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity. Courier Corporation, 1998.\"\n\"[RSW17]\",\"Aviad Rubinstein, Tselil Schramm, and S Matthew Weinberg. Computing exact minimum cuts without knowing the graph. arXiv preprint arXiv:1711.03165, 2017.\"\n\"[S+03]\",\"Alexander Schrijver et al. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer, 2003.\"\n\"[SBK22]\",\"Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics-inspired graph neural networks. Nature Machine Intelligence, 4(4):367\u2013377, 2022.\"\n\"[Sch05]\",\"Alexander Schrijver. On the history of combinatorial optimization (till 1960). Handbooks in operations research and management science, 12:1\u201368, 2005.\"\n\"[SE20]\",\"Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438\u201312448, 2020.\"\n\"[SHM+16]\",\"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.\"\n\"[Sin12]\",\"Alistair Sinclair. Algorithms for random generation and counting: a Markov chain approach. Springer Science & Business Media, 2012.\"\n\"[SLB+18]\",\"Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.\"\n\"[Smi99]\",\"Kate A Smith. Neural networks for combinatorial optimization: a review of more than a decade of research. Informs journal on Computing, 11(1):15\u201334, 1999.\"\n\"[Spi07]\",\"Daniel A Spielman. Spectral graph theory and its applications. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201907), pages 29\u201338. IEEE, 2007.\""}]}, {"page": 20, "text": "[SSDK+20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano\n             Ermon, and Ben Poole. Score-based generative modeling through stochastic differ-\n             ential equations. arXiv preprint arXiv:2011.13456, 2020. 6\n[SSS+17]     David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang,\n             Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mas-\n             tering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.\n             6\n[TRWG21]     Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural net-\n             works for maximum constraint satisfaction. Frontiers in artificial intelligence, 3:580607,\n             2021. 7\n[VFJ15]      Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in\n             neural information processing systems, 28, 2015. 1, 6\n[YBV19]      Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of\n             graph neural networks on random instances of max-cut. In Wavelets and Sparsity\n             XVIII, volume 11138, pages 242\u2013251. SPIE, 2019. 7\n[YGS20]      Gal Yehuda, Moshe Gabel, and Assaf Schuster. It\u2019s not what machines can learn, it\u2019s\n             what we cannot teach. In International conference on machine learning, pages 10831\u2013\n             10841. PMLR, 2020. 7\n[YP19]       Emre Yolcu and Barnab\u00e1s P\u00f3czos. Learning local search heuristics for boolean satis-\n             fiability. Advances in Neural Information Processing Systems, 32, 2019. 7\n[YW20]       Yunhao Yang and Andrew Whinston. A survey on reinforcement learning for com-\n             binatorial optimization. arXiv preprint arXiv:2008.12248, 2020. 6\n[ZCH+20]     Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,\n             Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review\n             of methods and applications. AI Open, 1:57\u201381, 2020. 6\n[ZMB+17]     Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and\n             Peter W Glynn.      Stochastic mirror descent in variationally coherent optimization\n             problems. Advances in Neural Information Processing Systems, 30, 2017. 10\n                                                   20", "md": "# References\n\n# References\n\n- [SSDK+20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\n- [SSS+17] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.\n- [TRWG21] Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in artificial intelligence, 3:580607, 2021.\n- [VFJ15] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural information processing systems, 28, 2015.\n- [YBV19] Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max-cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242\u2013251. SPIE, 2019.\n- [YGS20] Gal Yehuda, Moshe Gabel, and Assaf Schuster. It\u2019s not what machines can learn, it\u2019s what we cannot teach. In International conference on machine learning, pages 10831\u201310841. PMLR, 2020.\n- [YP19] Emre Yolcu and Barnab\u00e1s P\u00f3czos. Learning local search heuristics for boolean satisfiability. Advances in Neural Information Processing Systems, 32, 2019.\n- [YW20] Yunhao Yang and Andrew Whinston. A survey on reinforcement learning for combinatorial optimization. arXiv preprint arXiv:2008.12248, 2020.\n- [ZCH+20] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57\u201381, 2020.\n- [ZMB+17] Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W Glynn. Stochastic mirror descent in variationally coherent optimization problems. Advances in Neural Information Processing Systems, 30, 2017.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- [SSDK+20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\n- [SSS+17] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.\n- [TRWG21] Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in artificial intelligence, 3:580607, 2021.\n- [VFJ15] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural information processing systems, 28, 2015.\n- [YBV19] Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max-cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242\u2013251. SPIE, 2019.\n- [YGS20] Gal Yehuda, Moshe Gabel, and Assaf Schuster. It\u2019s not what machines can learn, it\u2019s what we cannot teach. In International conference on machine learning, pages 10831\u201310841. PMLR, 2020.\n- [YP19] Emre Yolcu and Barnab\u00e1s P\u00f3czos. Learning local search heuristics for boolean satisfiability. Advances in Neural Information Processing Systems, 32, 2019.\n- [YW20] Yunhao Yang and Andrew Whinston. A survey on reinforcement learning for combinatorial optimization. arXiv preprint arXiv:2008.12248, 2020.\n- [ZCH+20] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57\u201381, 2020.\n- [ZMB+17] Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W Glynn. Stochastic mirror descent in variationally coherent optimization problems. Advances in Neural Information Processing Systems, 30, 2017.", "md": "- [SSDK+20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\n- [SSS+17] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.\n- [TRWG21] Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in artificial intelligence, 3:580607, 2021.\n- [VFJ15] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural information processing systems, 28, 2015.\n- [YBV19] Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max-cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242\u2013251. SPIE, 2019.\n- [YGS20] Gal Yehuda, Moshe Gabel, and Assaf Schuster. It\u2019s not what machines can learn, it\u2019s what we cannot teach. In International conference on machine learning, pages 10831\u201310841. PMLR, 2020.\n- [YP19] Emre Yolcu and Barnab\u00e1s P\u00f3czos. Learning local search heuristics for boolean satisfiability. Advances in Neural Information Processing Systems, 32, 2019.\n- [YW20] Yunhao Yang and Andrew Whinston. A survey on reinforcement learning for combinatorial optimization. arXiv preprint arXiv:2008.12248, 2020.\n- [ZCH+20] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57\u201381, 2020.\n- [ZMB+17] Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W Glynn. Stochastic mirror descent in variationally coherent optimization problems. Advances in Neural Information Processing Systems, 30, 2017."}]}, {"page": 21, "text": "A          Preliminaries and Notation\nThis lemma is a useful tool for quasar convex functions.\nLemma 1 ([HMR16]). Suppose that the functions f1, . . . , fn are individually \u03b3-quasar convex in X with\nrespect to a common global minimum x. Then for non-negative weights a1, . . . , an, the linear combination\nf = \u2211i\u2208[n] ai fi is also \u03b3-quasar convex with respect to x in X.\n        In the proofs, we use the following notation: for a matrix W and vectors x, z, we let\n                                                                        \u03d5(x; z; W) =                       exp(z\u22a4Wz)                                                                                       (1)\nbe a probability mass function over X and we overload the notation as                                \u2211y\u2208X exp(z\u22a4Wy)\n                                                                            \u03d5(x; w) =                    exp(w \u00b7 x)                                                                                        (2)\n                                                                                                   \u2211y\u2208X exp(w \u00b7 y) .\nB          The Proof of Remark 1\nProof. Let x1, . . . , xn be the variables of the Max-Cut problem of interest and S = {\u22121, 1}n be\nthe solution space. Consider P to be the collection of product distributions over S, i.e., for any                                    1+s i                   1\u2212si\np \u2208       P, it holds that, for any s \u2208                                    S, Prx\u223cp[x = s] = \u220fi\u2208[n] pi                                  2   (1 \u2212       pi)      2 . Let us consider the\ncube [\u03f5, 1 \u2212               \u03f5]n. This family is complete since the O(\u03f5)-sub-optimal solution of I belongs to P\nand is compressed since the description size is poly(n, log(1/\u03f5)). We show that in this setting\nthere exist bad stationary points. Let LG be the Laplacian matrix of the input graph. For some\nproduct distribution p \u2208                               P, it holds that\n                   L(p) = \u2212                  E                                                                                         \u2207pL(p) = \u22124LG(2p \u2212                                1) ,\n                                         x\u223cp(\u00b7)[x\u22a4LGx] = \u2212(2p \u2212                                1)\u22a4LG(2p \u2212                 1) ,\nwhere LG is zero in the diagonal and equal to the Laplacian otherwise. Let us consider a vertex\nof the cube p \u2208                     [\u03f5, 1 \u2212        \u03f5]n which is highly and strictly sub-optimal, i.e., any single change of a\nnode would strictly improve the number of edges in the cut and the score attained in p is very\nlarge compared to minx\u2208S \u2212x\u22a4LGx. For any i \u2208                                                             [n], we show that\n                                                                        (\u2207L(p) \u00b7 ei)((2p \u2212                        1) \u00b7 ei) < 0 .\nThis means that if pi is large (i.e., 1 \u2212                                          \u03f5), then the i-th coordinate of the gradient of L(p) should\nbe negative since this would imply that the negative gradient would preserve pi to the right\nboundary. Similarly for the case where pi is small. This means that this point is a stationary\npoint and is highly sub-optimal by assumption.\n        Let P (resp. N) be the set of indices in [n] where p takes the value 1 \u2212                                                                                       \u03f5 (resp. \u03f5). For any\ni \u2208     [n], let N (i) be its neighborhood in G. Let us consider i \u2208                                                                       P. We have that (2p \u2212                              1) \u00b7 ei > 0\nand so it suffices to show that\n                                                                                  (LG(2p \u2212              1)) \u00b7 ei > 0 ,\n                                                                                                       21", "md": "## A Preliminaries and Notation\n\nThis lemma is a useful tool for quasar convex functions.\n\nLemma 1 ([HMR16]). Suppose that the functions f1, . . . , fn are individually \u03b3-quasar convex in X with\nrespect to a common global minimum x. Then for non-negative weights a1, . . . , an, the linear combination\nf = $$\\sum_{i\\in[n]} a_i f_i$$ is also \u03b3-quasar convex with respect to x in X.\n\nIn the proofs, we use the following notation: for a matrix W and vectors x, z, we let\n\n$$\\phi(x; z; W) = \\exp(z^\\top Wz) \\quad (1)$$\n\nbe a probability mass function over X and we overload the notation as\n\n$$\\phi(x; w) = \\exp(w \\cdot x) \\quad (2)$$\n\n## B The Proof of Remark 1\n\nProof. Let x1, . . . , xn be the variables of the Max-Cut problem of interest and S = {\u22121, 1}n be\nthe solution space. Consider P to be the collection of product distributions over S, i.e., for any\n\np \u2208 P, it holds that, for any s \u2208 S, $$Pr_{x\\sim p}[x = s] = \\prod_{i\\in[n]} p_i^{1+s_i} (1 - p_i)^{1-s_i}$$.\n\nLet us consider the cube [\u03f5, 1 - \u03f5]n. This family is complete since the O(\u03f5)-sub-optimal solution of I belongs to P\nand is compressed since the description size is poly(n, log(1/\u03f5)). We show that in this setting\nthere exist bad stationary points. Let LG be the Laplacian matrix of the input graph. For some\nproduct distribution p \u2208 P, it holds that\n\n$$L(p) = -E[\\nabla p L(p)] = -4LG(2p - 1)$$,\n\nwhere LG is zero in the diagonal and equal to the Laplacian otherwise. Let us consider a vertex\nof the cube p \u2208 [\u03f5, 1 - \u03f5]n which is highly and strictly sub-optimal, i.e., any single change of a\nnode would strictly improve the number of edges in the cut and the score attained in p is very\nlarge compared to $$\\min_{x\\in S} -x^\\top LGx$$. For any i \u2208 [n], we show that\n\n$$(\\nabla L(p) \\cdot e_i)((2p - 1) \\cdot e_i) < 0$$.\n\nThis means that if pi is large (i.e., 1 - \u03f5), then the i-th coordinate of the gradient of L(p) should\nbe negative since this would imply that the negative gradient would preserve pi to the right\nboundary. Similarly for the case where pi is small. This means that this point is a stationary\npoint and is highly sub-optimal by assumption.\n\nLet P (resp. N) be the set of indices in [n] where p takes the value 1 - \u03f5 (resp. \u03f5). For any\ni \u2208 [n], let N(i) be its neighborhood in G. Let us consider i \u2208 P. We have that (2p - 1) \u00b7 ei > 0\nand so it suffices to show that\n\n$$(LG(2p - 1)) \\cdot e_i > 0$$.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "A Preliminaries and Notation", "md": "## A Preliminaries and Notation"}, {"type": "text", "value": "This lemma is a useful tool for quasar convex functions.\n\nLemma 1 ([HMR16]). Suppose that the functions f1, . . . , fn are individually \u03b3-quasar convex in X with\nrespect to a common global minimum x. Then for non-negative weights a1, . . . , an, the linear combination\nf = $$\\sum_{i\\in[n]} a_i f_i$$ is also \u03b3-quasar convex with respect to x in X.\n\nIn the proofs, we use the following notation: for a matrix W and vectors x, z, we let\n\n$$\\phi(x; z; W) = \\exp(z^\\top Wz) \\quad (1)$$\n\nbe a probability mass function over X and we overload the notation as\n\n$$\\phi(x; w) = \\exp(w \\cdot x) \\quad (2)$$", "md": "This lemma is a useful tool for quasar convex functions.\n\nLemma 1 ([HMR16]). Suppose that the functions f1, . . . , fn are individually \u03b3-quasar convex in X with\nrespect to a common global minimum x. Then for non-negative weights a1, . . . , an, the linear combination\nf = $$\\sum_{i\\in[n]} a_i f_i$$ is also \u03b3-quasar convex with respect to x in X.\n\nIn the proofs, we use the following notation: for a matrix W and vectors x, z, we let\n\n$$\\phi(x; z; W) = \\exp(z^\\top Wz) \\quad (1)$$\n\nbe a probability mass function over X and we overload the notation as\n\n$$\\phi(x; w) = \\exp(w \\cdot x) \\quad (2)$$"}, {"type": "heading", "lvl": 2, "value": "B The Proof of Remark 1", "md": "## B The Proof of Remark 1"}, {"type": "text", "value": "Proof. Let x1, . . . , xn be the variables of the Max-Cut problem of interest and S = {\u22121, 1}n be\nthe solution space. Consider P to be the collection of product distributions over S, i.e., for any\n\np \u2208 P, it holds that, for any s \u2208 S, $$Pr_{x\\sim p}[x = s] = \\prod_{i\\in[n]} p_i^{1+s_i} (1 - p_i)^{1-s_i}$$.\n\nLet us consider the cube [\u03f5, 1 - \u03f5]n. This family is complete since the O(\u03f5)-sub-optimal solution of I belongs to P\nand is compressed since the description size is poly(n, log(1/\u03f5)). We show that in this setting\nthere exist bad stationary points. Let LG be the Laplacian matrix of the input graph. For some\nproduct distribution p \u2208 P, it holds that\n\n$$L(p) = -E[\\nabla p L(p)] = -4LG(2p - 1)$$,\n\nwhere LG is zero in the diagonal and equal to the Laplacian otherwise. Let us consider a vertex\nof the cube p \u2208 [\u03f5, 1 - \u03f5]n which is highly and strictly sub-optimal, i.e., any single change of a\nnode would strictly improve the number of edges in the cut and the score attained in p is very\nlarge compared to $$\\min_{x\\in S} -x^\\top LGx$$. For any i \u2208 [n], we show that\n\n$$(\\nabla L(p) \\cdot e_i)((2p - 1) \\cdot e_i) < 0$$.\n\nThis means that if pi is large (i.e., 1 - \u03f5), then the i-th coordinate of the gradient of L(p) should\nbe negative since this would imply that the negative gradient would preserve pi to the right\nboundary. Similarly for the case where pi is small. This means that this point is a stationary\npoint and is highly sub-optimal by assumption.\n\nLet P (resp. N) be the set of indices in [n] where p takes the value 1 - \u03f5 (resp. \u03f5). For any\ni \u2208 [n], let N(i) be its neighborhood in G. Let us consider i \u2208 P. We have that (2p - 1) \u00b7 ei > 0\nand so it suffices to show that\n\n$$(LG(2p - 1)) \\cdot e_i > 0$$.", "md": "Proof. Let x1, . . . , xn be the variables of the Max-Cut problem of interest and S = {\u22121, 1}n be\nthe solution space. Consider P to be the collection of product distributions over S, i.e., for any\n\np \u2208 P, it holds that, for any s \u2208 S, $$Pr_{x\\sim p}[x = s] = \\prod_{i\\in[n]} p_i^{1+s_i} (1 - p_i)^{1-s_i}$$.\n\nLet us consider the cube [\u03f5, 1 - \u03f5]n. This family is complete since the O(\u03f5)-sub-optimal solution of I belongs to P\nand is compressed since the description size is poly(n, log(1/\u03f5)). We show that in this setting\nthere exist bad stationary points. Let LG be the Laplacian matrix of the input graph. For some\nproduct distribution p \u2208 P, it holds that\n\n$$L(p) = -E[\\nabla p L(p)] = -4LG(2p - 1)$$,\n\nwhere LG is zero in the diagonal and equal to the Laplacian otherwise. Let us consider a vertex\nof the cube p \u2208 [\u03f5, 1 - \u03f5]n which is highly and strictly sub-optimal, i.e., any single change of a\nnode would strictly improve the number of edges in the cut and the score attained in p is very\nlarge compared to $$\\min_{x\\in S} -x^\\top LGx$$. For any i \u2208 [n], we show that\n\n$$(\\nabla L(p) \\cdot e_i)((2p - 1) \\cdot e_i) < 0$$.\n\nThis means that if pi is large (i.e., 1 - \u03f5), then the i-th coordinate of the gradient of L(p) should\nbe negative since this would imply that the negative gradient would preserve pi to the right\nboundary. Similarly for the case where pi is small. This means that this point is a stationary\npoint and is highly sub-optimal by assumption.\n\nLet P (resp. N) be the set of indices in [n] where p takes the value 1 - \u03f5 (resp. \u03f5). For any\ni \u2208 [n], let N(i) be its neighborhood in G. Let us consider i \u2208 P. We have that (2p - 1) \u00b7 ei > 0\nand so it suffices to show that\n\n$$(LG(2p - 1)) \\cdot e_i > 0$$."}]}, {"page": 22, "text": "which corresponds to showing that\n                         j\u2208N\u2211(i)\u2229PLG(i, j)(1 \u2212   2\u03f5) +  j\u2208N\u2211(i)\u2229N LG(i, j)(2\u03f5 \u2212  1) > 0 ,\nand so we would like to have\n                                        \u2211   LG(i, j) \u2212  \u2211   LG(i, j) > 0 .\n                                        j\u2208P             j\u2208N\nNote that this is true for any i \u2208    [n] since the current solution is a strict local optimum. The same\nholds if i \u2208  N.\nC     Completeness\nProposition 2 (Completeness). Consider \u03f5 > 0 and a prior R over I. Assume that Assumption 1\nholds. There exist \u03b2\u22c6, \u03c1\u22c6  \u2208  (0, 1) and W such that the family of solution generators P of Equation (5) is\ncomplete.\nProof. Assume that O(s, I) = \u03c8I(I)\u22a4M\u03c8S(s) and let z = \u03c8I(I) and x = \u03c8S(s). Moreover, let\n\u03b1, C, DS, DI be the parameters promised by Assumption 1.                   Let us consider the family P =\n{p(W) : W \u2208     W} with                                ez\u22a4Wx               ez\u22a4\u03c1\u22c6Wx\n                           p(x; z; W) = (1 \u2212    \u03b2\u22c6) \u2211y\u2208X ez\u22a4Wy + \u03b2\u22c6\u2211y\u2208X ez\u22a4\u03c1\u22c6Wy ,\nwhere the mixing weight \u03b2\u22c6        \u2208  (0, 1) and the inverse temperate \u03c1\u22c6       are to be decided. Recall that\n                          L(W) = E  z\u223cR      E                  z\u223cR      E\n                                         x\u223cp(\u00b7;z;W)[L(x; z)] = E     x\u223cp(\u00b7;z;W)[z\u22a4Mx] .\nLet us pick the parameter matrix W = \u2212M/\u03bb. Let us now fix a z \u2208                   \u03c8I(I). For the given matrix\nM, we can consider the finite set of values V obtained by the quadratic forms {z\u22a4Mx}x\u2208\u03c8S(S). We\nfurther cluster these values so that they have distance at least \u03f5 between each other. We consider\nthe level sets Ci where C1 is the subset of S with minimum value v1(= v1(z)) \u2208                       V, C2 is the\nsubset with the second smallest v2(= v2(z)) \u2208           V, etc. For fixed z \u2208    \u03c8I(I), we have that\n                 E                                    E                              E\n           x\u223cp(\u00b7;z;\u2212M/\u03bb)[z\u22a4Mx] = (1 \u2212       \u03b2\u22c6) x\u223c\u03d5(\u00b7;z;\u2212M/\u03bb)[z\u22a4Mx] + \u03b2\u22c6      x\u223c\u03d5(\u00b7;z;\u2212\u03c1\u22c6M/\u03bb)[z\u22a4Mx] ,\nwhere \u03d5 comes from (1). We note that\n                                       Pr                           |Ci|e\u2212vi/\u03bb\n                                 x\u223c\u03d5(\u00b7;z;\u2212M/\u03bb)[z\u22a4Mx \u2208      Ci] =  \u2211j |Cj|e\u2212vj/\u03bb .\nWe claim that, by letting \u03bb \u2192      0, the above measure concentrates uniformly on C1. The worst case\nscenario is when |C2| = |S| \u2212      |C1| and v2 = v1 + \u03f5. Then we have that\n                             Pr                            |C2|/|C1|e(\u2212v2+v1)/\u03bb\n                       x\u223c\u03d5(\u00b7;z;\u2212M/\u03bb)[z\u22a4Mx \u2208      C2] =   1 + |C2|/|C1|e(\u2212v2+v1)/\u03bb \u2264      \u03b4 ,\n                                                        22", "md": "which corresponds to showing that\n$$\n\\sum_{j\\in N}\\sum(i)\\cap PLG(i, j)(1 - 2\\epsilon) + \\sum_{j\\in N}\\sum(i)\\cap N LG(i, j)(2\\epsilon - 1) > 0,\n$$\nand so we would like to have\n$$\n\\sum_{j\\in P} LG(i, j) - \\sum_{j\\in N} LG(i, j) > 0.\n$$\nNote that this is true for any $i \\in [n]$ since the current solution is a strict local optimum. The same holds if $i \\in N$.\n\n### Completeness\n**Proposition 2 (Completeness):** Consider $\\epsilon > 0$ and a prior $R$ over $I$. Assume that Assumption 1 holds. There exist $\\beta^*, \\rho^* \\in (0, 1)$ and $W$ such that the family of solution generators $P$ of Equation (5) is complete.\n\n**Proof:** Assume that $O(s, I) = \\psi_I(I)^\\top M \\psi_S(s)$ and let $z = \\psi_I(I)$ and $x = \\psi_S(s)$. Moreover, let $\\alpha, C, DS, DI$ be the parameters promised by Assumption 1. Let us consider the family $P = \\{p(W) : W \\in W\\}$ with\n$$\np(x; z; W) = (1 - \\beta^*) \\sum_{y\\in X} e^{z^\\top Wy} + \\beta^* \\sum_{y\\in X} e^{z^\\top \\rho^* Wy},\n$$\nwhere the mixing weight $\\beta^* \\in (0, 1)$ and the inverse temperature $\\rho^*$ are to be decided. Recall that\n$$\nL(W) = E_{z\\sim R} E_{z\\sim R} E_{x\\sim p(\\cdot; z; W)}[L(x; z)] = E_{x\\sim p(\\cdot; z; W)}[z^\\top Mx].\n$$\nLet us pick the parameter matrix $W = -M/\\lambda$. Let us now fix a $z \\in \\psi_I(I)$. For the given matrix $M$, we can consider the finite set of values $V$ obtained by the quadratic forms $\\{z^\\top Mx\\}_{x\\in \\psi_S(S)}$. We further cluster these values so that they have distance at least $\\epsilon$ between each other. We consider the level sets $C_i$ where $C_1$ is the subset of $S$ with minimum value $v_1(= v_1(z)) \\in V$, $C_2$ is the subset with the second smallest $v_2(= v_2(z)) \\in V$, etc. For fixed $z \\in \\psi_I(I)$, we have that\n$$\nE_{x\\sim p(\\cdot; z; -M/\\lambda)}[z^\\top Mx] = (1 - \\beta^*) E_{x\\sim \\phi(\\cdot; z; -M/\\lambda)}[z^\\top Mx] + \\beta^* E_{x\\sim \\phi(\\cdot; z; -\\rho^*M/\\lambda)}[z^\\top Mx],\n$$\nwhere $\\phi$ comes from (1). We note that\n$$\n\\Pr\\left\\{x\\sim \\phi(\\cdot; z; -M/\\lambda)[z^\\top Mx \\in C_i]\\right\\} = \\sum_j |C_j|e^{-v_j/\\lambda}.\n$$\nWe claim that, by letting $\\lambda \\to 0$, the above measure concentrates uniformly on $C_1$. The worst-case scenario is when $|C_2| = |S| - |C_1|$ and $v_2 = v_1 + \\epsilon$. Then we have that\n$$\n\\Pr\\left\\{x\\sim \\phi(\\cdot; z; -M/\\lambda)[z^\\top Mx \\in C_2]\\right\\} = 1 + \\frac{|C_2|}{|C_1|}e^{(-v_2+v_1)/\\lambda} \\leq \\delta.\n$$", "images": [], "items": [{"type": "text", "value": "which corresponds to showing that\n$$\n\\sum_{j\\in N}\\sum(i)\\cap PLG(i, j)(1 - 2\\epsilon) + \\sum_{j\\in N}\\sum(i)\\cap N LG(i, j)(2\\epsilon - 1) > 0,\n$$\nand so we would like to have\n$$\n\\sum_{j\\in P} LG(i, j) - \\sum_{j\\in N} LG(i, j) > 0.\n$$\nNote that this is true for any $i \\in [n]$ since the current solution is a strict local optimum. The same holds if $i \\in N$.", "md": "which corresponds to showing that\n$$\n\\sum_{j\\in N}\\sum(i)\\cap PLG(i, j)(1 - 2\\epsilon) + \\sum_{j\\in N}\\sum(i)\\cap N LG(i, j)(2\\epsilon - 1) > 0,\n$$\nand so we would like to have\n$$\n\\sum_{j\\in P} LG(i, j) - \\sum_{j\\in N} LG(i, j) > 0.\n$$\nNote that this is true for any $i \\in [n]$ since the current solution is a strict local optimum. The same holds if $i \\in N$."}, {"type": "heading", "lvl": 3, "value": "Completeness", "md": "### Completeness"}, {"type": "text", "value": "**Proposition 2 (Completeness):** Consider $\\epsilon > 0$ and a prior $R$ over $I$. Assume that Assumption 1 holds. There exist $\\beta^*, \\rho^* \\in (0, 1)$ and $W$ such that the family of solution generators $P$ of Equation (5) is complete.\n\n**Proof:** Assume that $O(s, I) = \\psi_I(I)^\\top M \\psi_S(s)$ and let $z = \\psi_I(I)$ and $x = \\psi_S(s)$. Moreover, let $\\alpha, C, DS, DI$ be the parameters promised by Assumption 1. Let us consider the family $P = \\{p(W) : W \\in W\\}$ with\n$$\np(x; z; W) = (1 - \\beta^*) \\sum_{y\\in X} e^{z^\\top Wy} + \\beta^* \\sum_{y\\in X} e^{z^\\top \\rho^* Wy},\n$$\nwhere the mixing weight $\\beta^* \\in (0, 1)$ and the inverse temperature $\\rho^*$ are to be decided. Recall that\n$$\nL(W) = E_{z\\sim R} E_{z\\sim R} E_{x\\sim p(\\cdot; z; W)}[L(x; z)] = E_{x\\sim p(\\cdot; z; W)}[z^\\top Mx].\n$$\nLet us pick the parameter matrix $W = -M/\\lambda$. Let us now fix a $z \\in \\psi_I(I)$. For the given matrix $M$, we can consider the finite set of values $V$ obtained by the quadratic forms $\\{z^\\top Mx\\}_{x\\in \\psi_S(S)}$. We further cluster these values so that they have distance at least $\\epsilon$ between each other. We consider the level sets $C_i$ where $C_1$ is the subset of $S$ with minimum value $v_1(= v_1(z)) \\in V$, $C_2$ is the subset with the second smallest $v_2(= v_2(z)) \\in V$, etc. For fixed $z \\in \\psi_I(I)$, we have that\n$$\nE_{x\\sim p(\\cdot; z; -M/\\lambda)}[z^\\top Mx] = (1 - \\beta^*) E_{x\\sim \\phi(\\cdot; z; -M/\\lambda)}[z^\\top Mx] + \\beta^* E_{x\\sim \\phi(\\cdot; z; -\\rho^*M/\\lambda)}[z^\\top Mx],\n$$\nwhere $\\phi$ comes from (1). We note that\n$$\n\\Pr\\left\\{x\\sim \\phi(\\cdot; z; -M/\\lambda)[z^\\top Mx \\in C_i]\\right\\} = \\sum_j |C_j|e^{-v_j/\\lambda}.\n$$\nWe claim that, by letting $\\lambda \\to 0$, the above measure concentrates uniformly on $C_1$. The worst-case scenario is when $|C_2| = |S| - |C_1|$ and $v_2 = v_1 + \\epsilon$. Then we have that\n$$\n\\Pr\\left\\{x\\sim \\phi(\\cdot; z; -M/\\lambda)[z^\\top Mx \\in C_2]\\right\\} = 1 + \\frac{|C_2|}{|C_1|}e^{(-v_2+v_1)/\\lambda} \\leq \\delta.\n$$", "md": "**Proposition 2 (Completeness):** Consider $\\epsilon > 0$ and a prior $R$ over $I$. Assume that Assumption 1 holds. There exist $\\beta^*, \\rho^* \\in (0, 1)$ and $W$ such that the family of solution generators $P$ of Equation (5) is complete.\n\n**Proof:** Assume that $O(s, I) = \\psi_I(I)^\\top M \\psi_S(s)$ and let $z = \\psi_I(I)$ and $x = \\psi_S(s)$. Moreover, let $\\alpha, C, DS, DI$ be the parameters promised by Assumption 1. Let us consider the family $P = \\{p(W) : W \\in W\\}$ with\n$$\np(x; z; W) = (1 - \\beta^*) \\sum_{y\\in X} e^{z^\\top Wy} + \\beta^* \\sum_{y\\in X} e^{z^\\top \\rho^* Wy},\n$$\nwhere the mixing weight $\\beta^* \\in (0, 1)$ and the inverse temperature $\\rho^*$ are to be decided. Recall that\n$$\nL(W) = E_{z\\sim R} E_{z\\sim R} E_{x\\sim p(\\cdot; z; W)}[L(x; z)] = E_{x\\sim p(\\cdot; z; W)}[z^\\top Mx].\n$$\nLet us pick the parameter matrix $W = -M/\\lambda$. Let us now fix a $z \\in \\psi_I(I)$. For the given matrix $M$, we can consider the finite set of values $V$ obtained by the quadratic forms $\\{z^\\top Mx\\}_{x\\in \\psi_S(S)}$. We further cluster these values so that they have distance at least $\\epsilon$ between each other. We consider the level sets $C_i$ where $C_1$ is the subset of $S$ with minimum value $v_1(= v_1(z)) \\in V$, $C_2$ is the subset with the second smallest $v_2(= v_2(z)) \\in V$, etc. For fixed $z \\in \\psi_I(I)$, we have that\n$$\nE_{x\\sim p(\\cdot; z; -M/\\lambda)}[z^\\top Mx] = (1 - \\beta^*) E_{x\\sim \\phi(\\cdot; z; -M/\\lambda)}[z^\\top Mx] + \\beta^* E_{x\\sim \\phi(\\cdot; z; -\\rho^*M/\\lambda)}[z^\\top Mx],\n$$\nwhere $\\phi$ comes from (1). We note that\n$$\n\\Pr\\left\\{x\\sim \\phi(\\cdot; z; -M/\\lambda)[z^\\top Mx \\in C_i]\\right\\} = \\sum_j |C_j|e^{-v_j/\\lambda}.\n$$\nWe claim that, by letting $\\lambda \\to 0$, the above measure concentrates uniformly on $C_1$. The worst-case scenario is when $|C_2| = |S| - |C_1|$ and $v_2 = v_1 + \\epsilon$. Then we have that\n$$\n\\Pr\\left\\{x\\sim \\phi(\\cdot; z; -M/\\lambda)[z^\\top Mx \\in C_2]\\right\\} = 1 + \\frac{|C_2|}{|C_1|}e^{(-v_2+v_1)/\\lambda} \\leq \\delta.\n$$"}]}, {"page": 23, "text": "when 1/\u03bb > log(|\u03c8S(S)|/\u03b4)/\u03f5, since in the worst case |C2|/|C1| = \u2126(|\u03c8S(S)|). Using this choice\nof \u03bb and taking expectation over z, we get that\n E         E                                     (1 \u2212 \u03b4) min                          + \u03b2\u22c6          E\nz\u223cR                                       z\u223cR\n     x\u223cp(\u00b7;z;\u2212M/\u03bb)[z\u22a4Mx] \u2264      (1\u2212   \u03b2\u22c6) E              x\u2208\u03c8S(S) L(x; z) + \u03b4v2(z)           x\u223c\u03d5(\u00b7;z;\u2212\u03c1\u22c6M/\u03bb)[z\u22a4Mx] .\nFirst, we remark that by taking \u03c1\u22c6         = poly(\u03b1, 1/C, 1/DS, 1/DI), the last term in the right-hand\nside of the above expression can be replaced by the expected score of an almost-uniform solu-\ntion (see Lemma 3 and Proposition 8), which is at most poly(DS, DI, C)2\u2212|\u03c8S(S)| (and which is\nessentially negligible). Finally, one can pick \u03b2\u22c6, \u03b4 = poly(\u03f5, 1/C, 1/DS, 1/DI) so that\n                   L(\u2212M/\u03bb) = E      z\u223cR        E                    z\u223cR     min              + \u03f5 .\n                                         x\u223cp(\u00b7;z;\u2212M/\u03bb)[z\u22a4Mx] \u2264        E    x\u2208\u03c8S(S) L(x; z)\nThis implies that P is complete by letting W = \u2212M/\u03bb \u2208                 W. This means that one can take W be\na ball centered at 0 with radius (of \u03f5-sub-optimality) to be of order at least B = \u2225M\u2225F/\u03bb.\nD     Compression\nProposition 3 (Compression). Consider \u03f5 > 0 and a prior R over I. Assume that Assumption 1\nholds. There exist \u03b2\u22c6, \u03c1\u22c6   \u2208  (0, 1) and W such that the family of solution generators P of Equation (5) is\ncompressed.\nProof. We have that the bit complexity to represent the mixing weight \u03b2\u22c6               is polylog(DS, DI, C, 1/\u03f5)\nand the description size of W is polynomial in [I] and in log(1/\u03f5). This follows from Assump-\ntion 1 since the feature dimensions nX and nZ are poly([I]) and W is a ball centered at 0 with\nradius O(B), where B = \u2225M\u2225F/\u03bb \u2264             C/\u03bb, which are also poly([I]/\u03f5).\nE     Efficiently Optimizable\nProposition 4 (Efficiently Optimizable). Consider \u03f5 > 0 and a prior R over I. Assume that Assump-\ntion 1 holds. There exist \u03b2\u22c6, \u03c1\u22c6   \u2208  (0, 1) and W such that family of solution generators P of Equation (5)\nis efficiently optimizable using Projected SGD, where the projection set is W.\n    The proof of this proposition is essentially decomposed into two parts: first, we show that\nthe entropy-reularized loss of Equation (2) is quasar convex and then apply the projected SGD\nalgorithm to L\u03bb.\n    Recall that H(W) = Ez\u223cR Ex\u223c\u03d5(\u00b7;z;W)[log \u03d5(x; z; W)]. Let R be a weighted sum (to be in accor-\ndance with the mixture structure of P) of negative entropy regularizers\n                                    R(W) = (1 \u2212     \u03b2\u22c6)H(W) + \u03b2\u22c6   \u03c1\u22c6  H(\u03c1\u22c6W) ,                                   (1)\nwhere \u03b2\u22c6, \u03c1\u22c6    are the fixed parameters of P (recall Equation (5)). We define the regularized loss\nwhere                                     L\u03bb(W) = L(W) + \u03bbR(W) ,                                                  (2)\n                                L(W) = E            E                  p(W) \u2208    P .\n                                           z\u223cR  x\u223cp(\u00b7;z;W)[L(z; x)] ,\n                                                         23", "md": "When $$\\frac{1}{\\lambda} > \\frac{\\log(|\\psi_S(S)|/\\delta)}{\\epsilon}$$, since in the worst case $$\\frac{|C2|}{|C1|} = \\Omega(|\\psi_S(S)|)$$. Using this choice of $$\\lambda$$ and taking expectation over $$z$$, we get that\n\n$$\n\\begin{align*}\nE[z\\sim R, x\\sim p(\\cdot;z;-M/\\lambda)][z^{\\top}Mx] &\\leq (1-\\beta^*)E[x\\in\\psi_S(S)]L(x;z) + \\delta v2(z) x\\sim\\phi(\\cdot;z;-\\rho^*M/\\lambda)[z^{\\top}Mx].\n\\end{align*}\n$$\n\nFirst, we remark that by taking $$\\rho^* = \\text{poly}(\\alpha, 1/C, 1/DS, 1/DI)$$, the last term in the right-hand side of the above expression can be replaced by the expected score of an almost-uniform solution (see Lemma 3 and Proposition 8), which is at most $$\\text{poly}(DS, DI, C)^2-|\\psi_S(S)|$$ (and which is essentially negligible). Finally, one can pick $$\\beta^*, \\delta = \\text{poly}(\\epsilon, 1/C, 1/DS, 1/DI)$$ so that\n\n$$\n\\begin{align*}\nL(-M/\\lambda) &= E[z\\sim R]E[z\\sim R]\\min + \\epsilon.\n\\end{align*}\n$$\n\nThis implies that $$P$$ is complete by letting $$W = -M/\\lambda \\in W$$. This means that one can take $$W$$ to be a ball centered at 0 with radius (of $$\\epsilon$$-sub-optimality) to be of order at least $$B = \\|M\\|_F/\\lambda$$.\n\n## Compression\n\nProposition 3 (Compression). Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds. There exist $$\\beta^*, \\rho^* \\in (0, 1)$$ and $$W$$ such that the family of solution generators $$P$$ of Equation (5) is compressed.\n\nProof. We have that the bit complexity to represent the mixing weight $$\\beta^*$$ is $$\\text{polylog}(DS, DI, C, 1/\\epsilon)$$ and the description size of $$W$$ is polynomial in $$[I]$$ and in $$\\log(1/\\epsilon)$$. This follows from Assumption 1 since the feature dimensions $$nX$$ and $$nZ$$ are $$\\text{poly}([I])$$ and $$W$$ is a ball centered at 0 with radius $$O(B)$$, where $$B = \\|M\\|_F/\\lambda \\leq C/\\lambda$$, which are also $$\\text{poly}([I]/\\epsilon)$$.\n\n## Efficiently Optimizable\n\nProposition 4 (Efficiently Optimizable). Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds. There exist $$\\beta^*, \\rho^* \\in (0, 1)$$ and $$W$$ such that family of solution generators $$P$$ of Equation (5) is efficiently optimizable using Projected SGD, where the projection set is $$W$$.\n\nThe proof of this proposition is essentially decomposed into two parts: first, we show that the entropy-regularized loss of Equation (2) is quasar convex and then apply the projected SGD algorithm to $$L_{\\lambda}$$.\n\nRecall that $$H(W) = E_{z\\sim R}E_{x\\sim\\phi(\\cdot;z;W)}[\\log\\phi(x; z; W)]$$. Let $$R$$ be a weighted sum (to be in accordance with the mixture structure of $$P$$) of negative entropy regularizers\n\n$$\n\\begin{align*}\nR(W) &= (1-\\beta^*)H(W) + \\beta^*\\rho^*H(\\rho^*W), &(1)\n\\end{align*}\n$$\n\nwhere $$\\beta^*, \\rho^*$$ are the fixed parameters of $$P$$ (recall Equation (5)). We define the regularized loss where $$L_{\\lambda}(W) = L(W) + \\lambda R(W)$$,\n\n$$\n\\begin{align*}\nL(W) &= E_{z\\sim R}E_{x\\sim p(\\cdot;z;W)}[L(z; x)], &(2)\n\\end{align*}\n$$\n\nwhere $$23$$", "images": [], "items": [{"type": "text", "value": "When $$\\frac{1}{\\lambda} > \\frac{\\log(|\\psi_S(S)|/\\delta)}{\\epsilon}$$, since in the worst case $$\\frac{|C2|}{|C1|} = \\Omega(|\\psi_S(S)|)$$. Using this choice of $$\\lambda$$ and taking expectation over $$z$$, we get that\n\n$$\n\\begin{align*}\nE[z\\sim R, x\\sim p(\\cdot;z;-M/\\lambda)][z^{\\top}Mx] &\\leq (1-\\beta^*)E[x\\in\\psi_S(S)]L(x;z) + \\delta v2(z) x\\sim\\phi(\\cdot;z;-\\rho^*M/\\lambda)[z^{\\top}Mx].\n\\end{align*}\n$$\n\nFirst, we remark that by taking $$\\rho^* = \\text{poly}(\\alpha, 1/C, 1/DS, 1/DI)$$, the last term in the right-hand side of the above expression can be replaced by the expected score of an almost-uniform solution (see Lemma 3 and Proposition 8), which is at most $$\\text{poly}(DS, DI, C)^2-|\\psi_S(S)|$$ (and which is essentially negligible). Finally, one can pick $$\\beta^*, \\delta = \\text{poly}(\\epsilon, 1/C, 1/DS, 1/DI)$$ so that\n\n$$\n\\begin{align*}\nL(-M/\\lambda) &= E[z\\sim R]E[z\\sim R]\\min + \\epsilon.\n\\end{align*}\n$$\n\nThis implies that $$P$$ is complete by letting $$W = -M/\\lambda \\in W$$. This means that one can take $$W$$ to be a ball centered at 0 with radius (of $$\\epsilon$$-sub-optimality) to be of order at least $$B = \\|M\\|_F/\\lambda$$.", "md": "When $$\\frac{1}{\\lambda} > \\frac{\\log(|\\psi_S(S)|/\\delta)}{\\epsilon}$$, since in the worst case $$\\frac{|C2|}{|C1|} = \\Omega(|\\psi_S(S)|)$$. Using this choice of $$\\lambda$$ and taking expectation over $$z$$, we get that\n\n$$\n\\begin{align*}\nE[z\\sim R, x\\sim p(\\cdot;z;-M/\\lambda)][z^{\\top}Mx] &\\leq (1-\\beta^*)E[x\\in\\psi_S(S)]L(x;z) + \\delta v2(z) x\\sim\\phi(\\cdot;z;-\\rho^*M/\\lambda)[z^{\\top}Mx].\n\\end{align*}\n$$\n\nFirst, we remark that by taking $$\\rho^* = \\text{poly}(\\alpha, 1/C, 1/DS, 1/DI)$$, the last term in the right-hand side of the above expression can be replaced by the expected score of an almost-uniform solution (see Lemma 3 and Proposition 8), which is at most $$\\text{poly}(DS, DI, C)^2-|\\psi_S(S)|$$ (and which is essentially negligible). Finally, one can pick $$\\beta^*, \\delta = \\text{poly}(\\epsilon, 1/C, 1/DS, 1/DI)$$ so that\n\n$$\n\\begin{align*}\nL(-M/\\lambda) &= E[z\\sim R]E[z\\sim R]\\min + \\epsilon.\n\\end{align*}\n$$\n\nThis implies that $$P$$ is complete by letting $$W = -M/\\lambda \\in W$$. This means that one can take $$W$$ to be a ball centered at 0 with radius (of $$\\epsilon$$-sub-optimality) to be of order at least $$B = \\|M\\|_F/\\lambda$$."}, {"type": "heading", "lvl": 2, "value": "Compression", "md": "## Compression"}, {"type": "text", "value": "Proposition 3 (Compression). Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds. There exist $$\\beta^*, \\rho^* \\in (0, 1)$$ and $$W$$ such that the family of solution generators $$P$$ of Equation (5) is compressed.\n\nProof. We have that the bit complexity to represent the mixing weight $$\\beta^*$$ is $$\\text{polylog}(DS, DI, C, 1/\\epsilon)$$ and the description size of $$W$$ is polynomial in $$[I]$$ and in $$\\log(1/\\epsilon)$$. This follows from Assumption 1 since the feature dimensions $$nX$$ and $$nZ$$ are $$\\text{poly}([I])$$ and $$W$$ is a ball centered at 0 with radius $$O(B)$$, where $$B = \\|M\\|_F/\\lambda \\leq C/\\lambda$$, which are also $$\\text{poly}([I]/\\epsilon)$$.", "md": "Proposition 3 (Compression). Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds. There exist $$\\beta^*, \\rho^* \\in (0, 1)$$ and $$W$$ such that the family of solution generators $$P$$ of Equation (5) is compressed.\n\nProof. We have that the bit complexity to represent the mixing weight $$\\beta^*$$ is $$\\text{polylog}(DS, DI, C, 1/\\epsilon)$$ and the description size of $$W$$ is polynomial in $$[I]$$ and in $$\\log(1/\\epsilon)$$. This follows from Assumption 1 since the feature dimensions $$nX$$ and $$nZ$$ are $$\\text{poly}([I])$$ and $$W$$ is a ball centered at 0 with radius $$O(B)$$, where $$B = \\|M\\|_F/\\lambda \\leq C/\\lambda$$, which are also $$\\text{poly}([I]/\\epsilon)$$."}, {"type": "heading", "lvl": 2, "value": "Efficiently Optimizable", "md": "## Efficiently Optimizable"}, {"type": "text", "value": "Proposition 4 (Efficiently Optimizable). Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds. There exist $$\\beta^*, \\rho^* \\in (0, 1)$$ and $$W$$ such that family of solution generators $$P$$ of Equation (5) is efficiently optimizable using Projected SGD, where the projection set is $$W$$.\n\nThe proof of this proposition is essentially decomposed into two parts: first, we show that the entropy-regularized loss of Equation (2) is quasar convex and then apply the projected SGD algorithm to $$L_{\\lambda}$$.\n\nRecall that $$H(W) = E_{z\\sim R}E_{x\\sim\\phi(\\cdot;z;W)}[\\log\\phi(x; z; W)]$$. Let $$R$$ be a weighted sum (to be in accordance with the mixture structure of $$P$$) of negative entropy regularizers\n\n$$\n\\begin{align*}\nR(W) &= (1-\\beta^*)H(W) + \\beta^*\\rho^*H(\\rho^*W), &(1)\n\\end{align*}\n$$\n\nwhere $$\\beta^*, \\rho^*$$ are the fixed parameters of $$P$$ (recall Equation (5)). We define the regularized loss where $$L_{\\lambda}(W) = L(W) + \\lambda R(W)$$,\n\n$$\n\\begin{align*}\nL(W) &= E_{z\\sim R}E_{x\\sim p(\\cdot;z;W)}[L(z; x)], &(2)\n\\end{align*}\n$$\n\nwhere $$23$$", "md": "Proposition 4 (Efficiently Optimizable). Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds. There exist $$\\beta^*, \\rho^* \\in (0, 1)$$ and $$W$$ such that family of solution generators $$P$$ of Equation (5) is efficiently optimizable using Projected SGD, where the projection set is $$W$$.\n\nThe proof of this proposition is essentially decomposed into two parts: first, we show that the entropy-regularized loss of Equation (2) is quasar convex and then apply the projected SGD algorithm to $$L_{\\lambda}$$.\n\nRecall that $$H(W) = E_{z\\sim R}E_{x\\sim\\phi(\\cdot;z;W)}[\\log\\phi(x; z; W)]$$. Let $$R$$ be a weighted sum (to be in accordance with the mixture structure of $$P$$) of negative entropy regularizers\n\n$$\n\\begin{align*}\nR(W) &= (1-\\beta^*)H(W) + \\beta^*\\rho^*H(\\rho^*W), &(1)\n\\end{align*}\n$$\n\nwhere $$\\beta^*, \\rho^*$$ are the fixed parameters of $$P$$ (recall Equation (5)). We define the regularized loss where $$L_{\\lambda}(W) = L(W) + \\lambda R(W)$$,\n\n$$\n\\begin{align*}\nL(W) &= E_{z\\sim R}E_{x\\sim p(\\cdot;z;W)}[L(z; x)], &(2)\n\\end{align*}\n$$\n\nwhere $$23$$"}]}, {"page": 24, "text": "E.1        Quasar Convexity of the Regularized Loss\nIn this section, we show that L\u03bb of Equation (2) is quasar convex. We restate Proposition 1.\nProposition 5 (Quasar Convexity). Assume that Assumption 1 holds. The function L\u03bb of Equation (2)\nwith domain W is poly(C, DS, DI, 1/\u03f5, 1/\u03b1)-quasar convex with respect to \u2212M/\u03bb on the domain W.\nProof. We can write the loss L\u03bb as       L\u03bb(W) = E         z\u223cR[L\u03bb,z(W)] = E             z\u223cR [Lz(W) + \u03bbRz(W)] ,\nwhere the mappings Lz and Rz are instance-specific (i.e., we have fixed z). We can make use\nof Lemma 1, which states that linear combinations of quasar convex (with the same minimizer)\nremain quasar convex. Hence, since the functions L\u03bb,z have the same minimizer \u2212M/\u03bb, it suffices\nto show quasar convexity for a particular fixed instance mapping, i.e., it sufffices to show that\nthe function\n                                                            L\u03bb,z(W) = Lz(W) + \u03bbRz(W)\nis quasar convex. Recall that W is a matrix of dimension nZ \u00d7 nX. To deal with the function\nL\u03bb,z, we consider the simpler function that maps vectors instead of matrices to real numbers. For\nsome vector c, let Lvec            \u03bb     : RnX \u2192        R be\n                                                      Lvec                     E                                                                            (3)\n                                                         \u03bb (w) =          x\u223cp(\u00b7;w)[c \u00b7 x] + \u03bbRvec(w) ,\nwhere for any vector w \u2208                         RnX, we define the probability distribution \u03d5(\u00b7; w) over the solution\nspace X = \u03c8S(S) with probability mass function\n                                                                                           ew\u00b7x\nWe then define                                                     \u03d5(x; w) =          \u2211y\u2208X ew\u00b7y .\n                                                   p(\u00b7; w) = (1 \u2212            \u03b2\u22c6)\u03d5(\u00b7; w) + \u03b2\u22c6\u03d5(\u00b7; \u03c1\u22c6w) ,\nand Rvec(w) = (1 \u2212                   \u03b2\u22c6)H(w) + \u03b2\u22c6          \u03c1\u22c6 H(\u03c1\u22c6w) (this is a essentially a weighted sum of regularizers,\nneeded to simplify the proof) with H(w) = E                                       x\u223c\u03d5(\u00b7;w) log \u03d5(x, w). These quantities are essentially\nthe fixed-instance analogues of Equations (5) and (2). The crucial observation is that by taking\nc = z\u22a4M and applying the chain rule we have that     \u2207WL\u03bb,z(W) = z \u00b7                   \u2207wLvec   \u03bb (z\u22a4W)          \u22a4    .                                     (4)\nThis means that the gradient of the fixed-instance objective L\u03bb,z is a matrix of dimension nZ \u00d7 nX\nthat is equal to the outer product of the instance featurization z and the gradient of the simpler\nfunction Lvec       w    evaluated at z\u22a4W. Let us now return on showing that L\u03bb,z is quasar convex. To\nthis end, we observe that\n                                \u2207WL\u03bb,z(W) \u00b7                W + M             = \u2207wLvec     \u03bb (z\u22a4W) \u00b7             z\u22a4W + z\u22a4           M       .\n                                                                     \u03bb                                                             \u03bb\nThis means that, since z is fixed, it suffices to show that the function Lvec                                                   \u03bb     is quasar convex. We\nprovide the next key proposition that deals with issue. This result is one the main technical\naspects of this work and its proof can be found in Appendix E.2.\nspace. In the following, intuitively X is the post-featurization instance space and Z is the parameter\n                                                                                    24", "md": "## E.1 Quasar Convexity of the Regularized Loss\n\nIn this section, we show that $$L_{\\lambda}$$ of Equation (2) is quasar convex. We restate Proposition 1.\n\nProposition 5 (Quasar Convexity). Assume that Assumption 1 holds. The function $$L_{\\lambda}$$ of Equation (2) with domain W is poly(C, DS, DI, 1/\u03f5, 1/\u03b1)-quasar convex with respect to \u2212M/\u03bb on the domain W.\n\nProof. We can write the loss $$L_{\\lambda}$$ as $$L_{\\lambda}(W) = E[z \\sim R[L_{\\lambda,z}(W)] = E[z \\sim R [L_z(W) + \\lambda R_z(W)]$$, where the mappings L_z and R_z are instance-specific (i.e., we have fixed z). We can make use of Lemma 1, which states that linear combinations of quasar convex (with the same minimizer) remain quasar convex. Hence, since the functions $$L_{\\lambda,z}$$ have the same minimizer \u2212M/\u03bb, it suffices to show quasar convexity for a particular fixed instance mapping, i.e., it suffices to show that the function\n\n$$L_{\\lambda,z}(W) = L_z(W) + \\lambda R_z(W)$$\n\nis quasar convex. Recall that W is a matrix of dimension nZ \u00d7 nX. To deal with the function $$L_{\\lambda,z}$$, we consider the simpler function that maps vectors instead of matrices to real numbers. For some vector c, let $$L_{\\text{vec}}^{\\lambda} : \\mathbb{R}^{nX} \\rightarrow \\mathbb{R}$$ be\n\n$$\nL_{\\text{vec}}^{\\lambda}(w) = E[x \\sim p(\\cdot;w)][c \\cdot x] + \\lambda R_{\\text{vec}}(w),\n$$\nwhere for any vector w \u2208 $$\\mathbb{R}^{nX}$$, we define the probability distribution $$\\phi(\\cdot; w)$$ over the solution space X = $$\\psi_S(S)$$ with probability mass function\n\n$$\n\\phi(x; w) = \\sum_{y \\in X} e^{w \\cdot y}.\n$$\nWe then define $$p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^* \\phi(\\cdot; \\rho^* w)$$, and $$R_{\\text{vec}}(w) = (1 - \\beta^*)H(w) + \\beta^* \\rho^* H(\\rho^* w)$$ (this is essentially a weighted sum of regularizers, needed to simplify the proof) with $$H(w) = E[x \\sim \\phi(\\cdot;w)] \\log \\phi(x, w)$$. These quantities are essentially the fixed-instance analogues of Equations (5) and (2). The crucial observation is that by taking c = $$z^{\\top}M$$ and applying the chain rule we have that $$\\nabla_W L_{\\lambda,z}(W) = z \\cdot \\nabla_w L_{\\text{vec}}^{\\lambda}(z^{\\top}W)^{\\top}$$. This means that the gradient of the fixed-instance objective $$L_{\\lambda,z}$$ is a matrix of dimension nZ \u00d7 nX that is equal to the outer product of the instance featurization z and the gradient of the simpler function $$L_{\\text{vec}}^{\\lambda}$$ evaluated at $$z^{\\top}W$$. Let us now return on showing that $$L_{\\lambda,z}$$ is quasar convex. To this end, we observe that\n\n$$\n\\nabla_W L_{\\lambda,z}(W) \\cdot (W + M) = \\nabla_w L_{\\text{vec}}^{\\lambda}(z^{\\top}W) \\cdot (z^{\\top}W + z^{\\top}M).\n$$\nThis means that, since z is fixed, it suffices to show that the function $$L_{\\text{vec}}^{\\lambda}$$ is quasar convex. We provide the next key proposition that deals with this issue. This result is one of the main technical aspects of this work and its proof can be found in Appendix E.2.\n\nspace. In the following, intuitively X is the post-featurization instance space and Z is the parameter", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "E.1 Quasar Convexity of the Regularized Loss", "md": "## E.1 Quasar Convexity of the Regularized Loss"}, {"type": "text", "value": "In this section, we show that $$L_{\\lambda}$$ of Equation (2) is quasar convex. We restate Proposition 1.\n\nProposition 5 (Quasar Convexity). Assume that Assumption 1 holds. The function $$L_{\\lambda}$$ of Equation (2) with domain W is poly(C, DS, DI, 1/\u03f5, 1/\u03b1)-quasar convex with respect to \u2212M/\u03bb on the domain W.\n\nProof. We can write the loss $$L_{\\lambda}$$ as $$L_{\\lambda}(W) = E[z \\sim R[L_{\\lambda,z}(W)] = E[z \\sim R [L_z(W) + \\lambda R_z(W)]$$, where the mappings L_z and R_z are instance-specific (i.e., we have fixed z). We can make use of Lemma 1, which states that linear combinations of quasar convex (with the same minimizer) remain quasar convex. Hence, since the functions $$L_{\\lambda,z}$$ have the same minimizer \u2212M/\u03bb, it suffices to show quasar convexity for a particular fixed instance mapping, i.e., it suffices to show that the function\n\n$$L_{\\lambda,z}(W) = L_z(W) + \\lambda R_z(W)$$\n\nis quasar convex. Recall that W is a matrix of dimension nZ \u00d7 nX. To deal with the function $$L_{\\lambda,z}$$, we consider the simpler function that maps vectors instead of matrices to real numbers. For some vector c, let $$L_{\\text{vec}}^{\\lambda} : \\mathbb{R}^{nX} \\rightarrow \\mathbb{R}$$ be\n\n$$\nL_{\\text{vec}}^{\\lambda}(w) = E[x \\sim p(\\cdot;w)][c \\cdot x] + \\lambda R_{\\text{vec}}(w),\n$$\nwhere for any vector w \u2208 $$\\mathbb{R}^{nX}$$, we define the probability distribution $$\\phi(\\cdot; w)$$ over the solution space X = $$\\psi_S(S)$$ with probability mass function\n\n$$\n\\phi(x; w) = \\sum_{y \\in X} e^{w \\cdot y}.\n$$\nWe then define $$p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^* \\phi(\\cdot; \\rho^* w)$$, and $$R_{\\text{vec}}(w) = (1 - \\beta^*)H(w) + \\beta^* \\rho^* H(\\rho^* w)$$ (this is essentially a weighted sum of regularizers, needed to simplify the proof) with $$H(w) = E[x \\sim \\phi(\\cdot;w)] \\log \\phi(x, w)$$. These quantities are essentially the fixed-instance analogues of Equations (5) and (2). The crucial observation is that by taking c = $$z^{\\top}M$$ and applying the chain rule we have that $$\\nabla_W L_{\\lambda,z}(W) = z \\cdot \\nabla_w L_{\\text{vec}}^{\\lambda}(z^{\\top}W)^{\\top}$$. This means that the gradient of the fixed-instance objective $$L_{\\lambda,z}$$ is a matrix of dimension nZ \u00d7 nX that is equal to the outer product of the instance featurization z and the gradient of the simpler function $$L_{\\text{vec}}^{\\lambda}$$ evaluated at $$z^{\\top}W$$. Let us now return on showing that $$L_{\\lambda,z}$$ is quasar convex. To this end, we observe that\n\n$$\n\\nabla_W L_{\\lambda,z}(W) \\cdot (W + M) = \\nabla_w L_{\\text{vec}}^{\\lambda}(z^{\\top}W) \\cdot (z^{\\top}W + z^{\\top}M).\n$$\nThis means that, since z is fixed, it suffices to show that the function $$L_{\\text{vec}}^{\\lambda}$$ is quasar convex. We provide the next key proposition that deals with this issue. This result is one of the main technical aspects of this work and its proof can be found in Appendix E.2.\n\nspace. In the following, intuitively X is the post-featurization instance space and Z is the parameter", "md": "In this section, we show that $$L_{\\lambda}$$ of Equation (2) is quasar convex. We restate Proposition 1.\n\nProposition 5 (Quasar Convexity). Assume that Assumption 1 holds. The function $$L_{\\lambda}$$ of Equation (2) with domain W is poly(C, DS, DI, 1/\u03f5, 1/\u03b1)-quasar convex with respect to \u2212M/\u03bb on the domain W.\n\nProof. We can write the loss $$L_{\\lambda}$$ as $$L_{\\lambda}(W) = E[z \\sim R[L_{\\lambda,z}(W)] = E[z \\sim R [L_z(W) + \\lambda R_z(W)]$$, where the mappings L_z and R_z are instance-specific (i.e., we have fixed z). We can make use of Lemma 1, which states that linear combinations of quasar convex (with the same minimizer) remain quasar convex. Hence, since the functions $$L_{\\lambda,z}$$ have the same minimizer \u2212M/\u03bb, it suffices to show quasar convexity for a particular fixed instance mapping, i.e., it suffices to show that the function\n\n$$L_{\\lambda,z}(W) = L_z(W) + \\lambda R_z(W)$$\n\nis quasar convex. Recall that W is a matrix of dimension nZ \u00d7 nX. To deal with the function $$L_{\\lambda,z}$$, we consider the simpler function that maps vectors instead of matrices to real numbers. For some vector c, let $$L_{\\text{vec}}^{\\lambda} : \\mathbb{R}^{nX} \\rightarrow \\mathbb{R}$$ be\n\n$$\nL_{\\text{vec}}^{\\lambda}(w) = E[x \\sim p(\\cdot;w)][c \\cdot x] + \\lambda R_{\\text{vec}}(w),\n$$\nwhere for any vector w \u2208 $$\\mathbb{R}^{nX}$$, we define the probability distribution $$\\phi(\\cdot; w)$$ over the solution space X = $$\\psi_S(S)$$ with probability mass function\n\n$$\n\\phi(x; w) = \\sum_{y \\in X} e^{w \\cdot y}.\n$$\nWe then define $$p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^* \\phi(\\cdot; \\rho^* w)$$, and $$R_{\\text{vec}}(w) = (1 - \\beta^*)H(w) + \\beta^* \\rho^* H(\\rho^* w)$$ (this is essentially a weighted sum of regularizers, needed to simplify the proof) with $$H(w) = E[x \\sim \\phi(\\cdot;w)] \\log \\phi(x, w)$$. These quantities are essentially the fixed-instance analogues of Equations (5) and (2). The crucial observation is that by taking c = $$z^{\\top}M$$ and applying the chain rule we have that $$\\nabla_W L_{\\lambda,z}(W) = z \\cdot \\nabla_w L_{\\text{vec}}^{\\lambda}(z^{\\top}W)^{\\top}$$. This means that the gradient of the fixed-instance objective $$L_{\\lambda,z}$$ is a matrix of dimension nZ \u00d7 nX that is equal to the outer product of the instance featurization z and the gradient of the simpler function $$L_{\\text{vec}}^{\\lambda}$$ evaluated at $$z^{\\top}W$$. Let us now return on showing that $$L_{\\lambda,z}$$ is quasar convex. To this end, we observe that\n\n$$\n\\nabla_W L_{\\lambda,z}(W) \\cdot (W + M) = \\nabla_w L_{\\text{vec}}^{\\lambda}(z^{\\top}W) \\cdot (z^{\\top}W + z^{\\top}M).\n$$\nThis means that, since z is fixed, it suffices to show that the function $$L_{\\text{vec}}^{\\lambda}$$ is quasar convex. We provide the next key proposition that deals with this issue. This result is one of the main technical aspects of this work and its proof can be found in Appendix E.2.\n\nspace. In the following, intuitively X is the post-featurization instance space and Z is the parameter"}]}, {"page": 25, "text": "Proposition 6. Consider \u03f5, \u03bb > 0. Let \u2225c\u22252 \u2264                 C1. Let Z be an open ball centered at 0 with diameter\n2C1/\u03bb. Let X be a space of diameter D and let Varx\u223cU(X )[v \u00b7 x] \u2265                   \u03b1\u2225v\u22252 2 for any v \u2208     Z. The function\nLvec\nZ.\u03bb (w) = Ex\u223cp(\u00b7;w)[c \u00b7 x] + \u03bbRvec(w) is poly(1/C1, 1/D, \u03f5, \u03b1)-quasar convex with respect to \u2212c/\u03bb on\n     We can apply the above result with c = z\u22a4M, w = z\u22a4W, D = DS and C1 = DIC. These give\nthat the quasar convexity parameter \u03b3 is of order \u03b3 = poly(\u03f5, \u03b1, 1/C, 1/DS, 1/DI). Since we have\nthat Lvec\n        \u03bb (z\u22a4W) = L\u03bb,z(W), we get that\n                          \u2207WL\u03bb,z(W) \u00b7 (W + M/\u03bb) \u2265               \u03b3(L\u03bb,z(W) \u2212       L\u03bb,z(\u2212M/\u03bb)) .\nThis implies that L\u03bb,z is \u03b3-quasar convex with respect to the minimizer \u2212M/\u03bb and completes\nthe proof using Lemma 1.\nE.2     The Proof of Proposition 6\nLet us consider Lvec    \u03bb   to be a real-valued differentiable function defined on Z. Let w, \u2212c/\u03bb \u2208                          Z\nand let L be the line segment between them with L \u2208                      Z. The mean value theorem implies that\nthere exists w\u2032 \u2208      L such that\n            Lvec\n              \u03bb (w) \u2212     Lvec\n                            \u03bb (\u2212c/\u03bb) = \u2207wLvec       \u03bb (w\u2032) \u00b7 (w + c/\u03bb) \u2264         \u2225\u2207Lvec\u03bb (w\u2032)\u22252\u2225w + c/\u03bb\u22252 .\nNow we have that Lvec      \u03bb   has bounded gradient (see Lemma 2) and so we get that\n                 \u2225\u2207wLvec \u03bb (w\u2032)\u22252 \u2264      D2\u2225c + \u03bbw\u2032\u22252 = D2\u03bb\u2225w\u2032 + c/\u03bb\u22252 \u2264                 D2\u03bb\u2225w + c/\u03bb\u22252 ,\nsince w\u2032 \u2208     L. This implies that\n                  Lvec                                                  2 \u2264   1     \u03bb (w) \u00b7 (w + c/\u03bb) ,\n                     \u03bb (w) \u2212    Lvec\n                                   \u03bb (\u2212c/\u03bb) \u2264       D2\u03bb\u2225w + c/\u03bb\u22252            \u03b3\u2207Lvec\nwhere 1/\u03b3 = poly(C1,D) \u03f53\u03b12   . The last inequality is an application of the correlation lower bound (see\nLemma 3).\n     In the above proof, we used two key lemmas: a bound for the norm of the gradient and a\nlower bound for the correlation. In the upcoming subsections, we prove these two results.\nE.2.1    Bounded Gradient Lemma and Proof\nLemma 2 (Bounded Gradient Norm of Lvec                 \u03bb ). Consider \u03f5, \u03bb > 0. Let Z be the domain of Lvec          \u03bb   of (3).\nLet X be a space of diameter D. For any w \u2208             Z, it holds that\nProof. We have that                      \u2225\u2207wLvec \u03bb (w)\u22252 \u2264      O(D2)\u2225c + \u03bbw\u22252 .\nwhere                                  \u2207wLvec\u03bb (w) = (1 \u2212       \u03b2\u22c6)Gw + \u03b2\u22c6\u03c1\u22c6G\u03c1\u22c6w ,\n                       Gw =       E                                 E                          E\n                               x\u223c\u03d5(\u00b7;w)[((c + \u03bbw) \u00b7 x)x] \u2212       x\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x]    x\u223c\u03d5(\u00b7;w)[x] ,\n                                                              25", "md": "Proposition 6. Consider \u03f5, \u03bb &gt; 0. Let $$\\|c\\|_2 \\leq C1$$. Let Z be an open ball centered at 0 with diameter 2C1/\u03bb. Let X be a space of diameter D and let $$Var_{x\\sim U(X)}[v \\cdot x] \\geq \\alpha\\|v\\|_2^2$$ for any $$v \\in Z$$. The function $$L_{vec}^{Z,\\lambda}(w) = E_{x\\sim p(\\cdot;w)}[c \\cdot x] + \\lambda R_{vec}(w)$$ is poly(1/C1, 1/D, \u03f5, \u03b1)-quasar convex with respect to $$-\\frac{c}{\\lambda}$$ on\n\nWe can apply the above result with c = $$z^\\top M$$, w = $$z^\\top W$$, D = DS and C1 = DIC. These give that the quasar convexity parameter \u03b3 is of order \u03b3 = poly(\u03f5, \u03b1, 1/C, 1/DS, 1/DI). Since we have that $$L_{vec}^{\\lambda}(z^\\top W) = L_{\\lambda,z}(W)$$, we get that\n\n$$\n\\nabla W L_{\\lambda,z}(W) \\cdot (W + M/\\lambda) \\geq \\gamma(L_{\\lambda,z}(W) - L_{\\lambda,z}(-M/\\lambda)) .\n$$\nThis implies that $$L_{\\lambda,z}$$ is \u03b3-quasar convex with respect to the minimizer $$-\\frac{M}{\\lambda}$$ and completes the proof using Lemma 1.\n\nE.2 The Proof of Proposition 6\n\nLet us consider $$L_{vec}^{\\lambda}$$ to be a real-valued differentiable function defined on Z. Let w, $$-\\frac{c}{\\lambda} \\in Z$$ and let L be the line segment between them with $$L \\in Z$$. The mean value theorem implies that there exists w' \u2208 L such that\n\n$$\nL_{vec}^{\\lambda}(w) - L_{vec}^{\\lambda}(-\\frac{c}{\\lambda}) = \\nabla w L_{vec}^{\\lambda}(w') \\cdot (w + \\frac{c}{\\lambda}) \\leq \\|\\nabla L_{vec}^{\\lambda}(w')\\|^2\\|w + \\frac{c}{\\lambda}\\|^2 .\n$$\nNow we have that $$L_{vec}^{\\lambda}$$ has bounded gradient (see Lemma 2) and so we get that\n\n$$\n\\|\\nabla w L_{vec}^{\\lambda}(w')\\|^2 \\leq D^2\\|c + \\lambda w'\\|^2 = D^2\\lambda\\|w' + \\frac{c}{\\lambda}\\|^2 \\leq D^2\\lambda\\|w + \\frac{c}{\\lambda}\\|^2 ,\n$$\nsince w' \u2208 L. This implies that\n\n$$\nL_{vec}^{\\lambda}(w) - L_{vec}^{\\lambda}(-\\frac{c}{\\lambda}) \\leq D^2\\lambda\\|w + \\frac{c}{\\lambda}\\|^2 \\leq \\frac{1}{\\gamma}\\nabla L_{vec}^{\\lambda}\n$$\nwhere $$\\frac{1}{\\gamma} = poly(C1,D) \\epsilon^3\\alpha^2$$. The last inequality is an application of the correlation lower bound (see Lemma 3).\n\nIn the above proof, we used two key lemmas: a bound for the norm of the gradient and a lower bound for the correlation. In the upcoming subsections, we prove these two results.\n\nE.2.1 Bounded Gradient Lemma and Proof\n\nLemma 2 (Bounded Gradient Norm of $$L_{vec}^{\\lambda}$$). Consider \u03f5, \u03bb &gt; 0. Let Z be the domain of $$L_{vec}^{\\lambda}$$ of (3). Let X be a space of diameter D. For any w \u2208 Z, it holds that\n\nProof. We have that $$\\|\\nabla w L_{vec}^{\\lambda}(w)\\|^2 \\leq O(D^2)\\|c + \\lambda w\\|^2$$.\n\nwhere $$\\nabla w L_{vec}^{\\lambda}(w) = (1 - \\beta^*)Gw + \\beta^*\\rho^*G\\rho^*w$$,\n\n$$Gw = E[x\\sim\\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x\\sim\\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] - E[x\\sim\\phi(\\cdot;w)[x]]$$,\n\n25", "images": [], "items": [{"type": "text", "value": "Proposition 6. Consider \u03f5, \u03bb &gt; 0. Let $$\\|c\\|_2 \\leq C1$$. Let Z be an open ball centered at 0 with diameter 2C1/\u03bb. Let X be a space of diameter D and let $$Var_{x\\sim U(X)}[v \\cdot x] \\geq \\alpha\\|v\\|_2^2$$ for any $$v \\in Z$$. The function $$L_{vec}^{Z,\\lambda}(w) = E_{x\\sim p(\\cdot;w)}[c \\cdot x] + \\lambda R_{vec}(w)$$ is poly(1/C1, 1/D, \u03f5, \u03b1)-quasar convex with respect to $$-\\frac{c}{\\lambda}$$ on\n\nWe can apply the above result with c = $$z^\\top M$$, w = $$z^\\top W$$, D = DS and C1 = DIC. These give that the quasar convexity parameter \u03b3 is of order \u03b3 = poly(\u03f5, \u03b1, 1/C, 1/DS, 1/DI). Since we have that $$L_{vec}^{\\lambda}(z^\\top W) = L_{\\lambda,z}(W)$$, we get that\n\n$$\n\\nabla W L_{\\lambda,z}(W) \\cdot (W + M/\\lambda) \\geq \\gamma(L_{\\lambda,z}(W) - L_{\\lambda,z}(-M/\\lambda)) .\n$$\nThis implies that $$L_{\\lambda,z}$$ is \u03b3-quasar convex with respect to the minimizer $$-\\frac{M}{\\lambda}$$ and completes the proof using Lemma 1.\n\nE.2 The Proof of Proposition 6\n\nLet us consider $$L_{vec}^{\\lambda}$$ to be a real-valued differentiable function defined on Z. Let w, $$-\\frac{c}{\\lambda} \\in Z$$ and let L be the line segment between them with $$L \\in Z$$. The mean value theorem implies that there exists w' \u2208 L such that\n\n$$\nL_{vec}^{\\lambda}(w) - L_{vec}^{\\lambda}(-\\frac{c}{\\lambda}) = \\nabla w L_{vec}^{\\lambda}(w') \\cdot (w + \\frac{c}{\\lambda}) \\leq \\|\\nabla L_{vec}^{\\lambda}(w')\\|^2\\|w + \\frac{c}{\\lambda}\\|^2 .\n$$\nNow we have that $$L_{vec}^{\\lambda}$$ has bounded gradient (see Lemma 2) and so we get that\n\n$$\n\\|\\nabla w L_{vec}^{\\lambda}(w')\\|^2 \\leq D^2\\|c + \\lambda w'\\|^2 = D^2\\lambda\\|w' + \\frac{c}{\\lambda}\\|^2 \\leq D^2\\lambda\\|w + \\frac{c}{\\lambda}\\|^2 ,\n$$\nsince w' \u2208 L. This implies that\n\n$$\nL_{vec}^{\\lambda}(w) - L_{vec}^{\\lambda}(-\\frac{c}{\\lambda}) \\leq D^2\\lambda\\|w + \\frac{c}{\\lambda}\\|^2 \\leq \\frac{1}{\\gamma}\\nabla L_{vec}^{\\lambda}\n$$\nwhere $$\\frac{1}{\\gamma} = poly(C1,D) \\epsilon^3\\alpha^2$$. The last inequality is an application of the correlation lower bound (see Lemma 3).\n\nIn the above proof, we used two key lemmas: a bound for the norm of the gradient and a lower bound for the correlation. In the upcoming subsections, we prove these two results.\n\nE.2.1 Bounded Gradient Lemma and Proof\n\nLemma 2 (Bounded Gradient Norm of $$L_{vec}^{\\lambda}$$). Consider \u03f5, \u03bb &gt; 0. Let Z be the domain of $$L_{vec}^{\\lambda}$$ of (3). Let X be a space of diameter D. For any w \u2208 Z, it holds that\n\nProof. We have that $$\\|\\nabla w L_{vec}^{\\lambda}(w)\\|^2 \\leq O(D^2)\\|c + \\lambda w\\|^2$$.\n\nwhere $$\\nabla w L_{vec}^{\\lambda}(w) = (1 - \\beta^*)Gw + \\beta^*\\rho^*G\\rho^*w$$,\n\n$$Gw = E[x\\sim\\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x\\sim\\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] - E[x\\sim\\phi(\\cdot;w)[x]]$$,\n\n25", "md": "Proposition 6. Consider \u03f5, \u03bb &gt; 0. Let $$\\|c\\|_2 \\leq C1$$. Let Z be an open ball centered at 0 with diameter 2C1/\u03bb. Let X be a space of diameter D and let $$Var_{x\\sim U(X)}[v \\cdot x] \\geq \\alpha\\|v\\|_2^2$$ for any $$v \\in Z$$. The function $$L_{vec}^{Z,\\lambda}(w) = E_{x\\sim p(\\cdot;w)}[c \\cdot x] + \\lambda R_{vec}(w)$$ is poly(1/C1, 1/D, \u03f5, \u03b1)-quasar convex with respect to $$-\\frac{c}{\\lambda}$$ on\n\nWe can apply the above result with c = $$z^\\top M$$, w = $$z^\\top W$$, D = DS and C1 = DIC. These give that the quasar convexity parameter \u03b3 is of order \u03b3 = poly(\u03f5, \u03b1, 1/C, 1/DS, 1/DI). Since we have that $$L_{vec}^{\\lambda}(z^\\top W) = L_{\\lambda,z}(W)$$, we get that\n\n$$\n\\nabla W L_{\\lambda,z}(W) \\cdot (W + M/\\lambda) \\geq \\gamma(L_{\\lambda,z}(W) - L_{\\lambda,z}(-M/\\lambda)) .\n$$\nThis implies that $$L_{\\lambda,z}$$ is \u03b3-quasar convex with respect to the minimizer $$-\\frac{M}{\\lambda}$$ and completes the proof using Lemma 1.\n\nE.2 The Proof of Proposition 6\n\nLet us consider $$L_{vec}^{\\lambda}$$ to be a real-valued differentiable function defined on Z. Let w, $$-\\frac{c}{\\lambda} \\in Z$$ and let L be the line segment between them with $$L \\in Z$$. The mean value theorem implies that there exists w' \u2208 L such that\n\n$$\nL_{vec}^{\\lambda}(w) - L_{vec}^{\\lambda}(-\\frac{c}{\\lambda}) = \\nabla w L_{vec}^{\\lambda}(w') \\cdot (w + \\frac{c}{\\lambda}) \\leq \\|\\nabla L_{vec}^{\\lambda}(w')\\|^2\\|w + \\frac{c}{\\lambda}\\|^2 .\n$$\nNow we have that $$L_{vec}^{\\lambda}$$ has bounded gradient (see Lemma 2) and so we get that\n\n$$\n\\|\\nabla w L_{vec}^{\\lambda}(w')\\|^2 \\leq D^2\\|c + \\lambda w'\\|^2 = D^2\\lambda\\|w' + \\frac{c}{\\lambda}\\|^2 \\leq D^2\\lambda\\|w + \\frac{c}{\\lambda}\\|^2 ,\n$$\nsince w' \u2208 L. This implies that\n\n$$\nL_{vec}^{\\lambda}(w) - L_{vec}^{\\lambda}(-\\frac{c}{\\lambda}) \\leq D^2\\lambda\\|w + \\frac{c}{\\lambda}\\|^2 \\leq \\frac{1}{\\gamma}\\nabla L_{vec}^{\\lambda}\n$$\nwhere $$\\frac{1}{\\gamma} = poly(C1,D) \\epsilon^3\\alpha^2$$. The last inequality is an application of the correlation lower bound (see Lemma 3).\n\nIn the above proof, we used two key lemmas: a bound for the norm of the gradient and a lower bound for the correlation. In the upcoming subsections, we prove these two results.\n\nE.2.1 Bounded Gradient Lemma and Proof\n\nLemma 2 (Bounded Gradient Norm of $$L_{vec}^{\\lambda}$$). Consider \u03f5, \u03bb &gt; 0. Let Z be the domain of $$L_{vec}^{\\lambda}$$ of (3). Let X be a space of diameter D. For any w \u2208 Z, it holds that\n\nProof. We have that $$\\|\\nabla w L_{vec}^{\\lambda}(w)\\|^2 \\leq O(D^2)\\|c + \\lambda w\\|^2$$.\n\nwhere $$\\nabla w L_{vec}^{\\lambda}(w) = (1 - \\beta^*)Gw + \\beta^*\\rho^*G\\rho^*w$$,\n\n$$Gw = E[x\\sim\\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x\\sim\\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] - E[x\\sim\\phi(\\cdot;w)[x]]$$,\n\n25"}]}, {"page": 26, "text": "and\n                         G\u03c1\u22c6w =              E                                               E                                      E\n                                      x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[((c + \u03bbw) \u00b7 x)x] \u2212                    x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x]               x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[x] .\nNote that since x \u2208                 X , it holds that \u2225x\u22252 \u2264                  D. Hence\n                                                 \u2225Gw\u22252 =            sup       |v \u00b7 Gw| \u2264         2D2\u2225c + \u03bbw\u22252 .\nMoreover, we have that                                            v:\u2225v\u22252=1\nThis means that                                                \u2225G\u03c1\u22c6w\u22252 \u2264           2D2\u2225c + \u03bbw\u22252 .\n             \u2225\u2207wLvec    \u03bb (w)\u22252 \u2264           2(1 \u2212      \u03b2\u22c6)\u2225c + \u03bbw\u22252D2 + 2\u03b2\u22c6\u03c1\u22c6\u2225c + \u03bbw\u22252D2 = O(D2)\u2225c + \u03bbw\u22252 .\nE.2.2        Correlation Lower Bound Lemma and Proof\nThe following lemma is the second ingredient in order to show Proposition 6.\nLemma 3 (Correlation Lower Bound for Lvec                                    \u03bb ). Let \u03bb > 0. Let \u2225c\u22252 \u2264                         C1. Let Z be an open ball\ncentered at 0 with diameter B = 2C1/\u03bb. Let X be a space of diameter D. Assume that w \u2208                                                                          Z and\nVar    x\u223cU(X )[(c + \u03bbw) \u00b7 x] \u2265                 \u03b1\u2225c + \u03bbw\u22252         2 for some \u03b1 > 0. Then, for any \u03b2\u22c6                         \u2208   (0, 1), there exists \u03c1\u22c6            > 0\nsuch that it holds that\n                                       \u2207wLvec   \u03bb (w) \u00b7 (c + \u03bbw) = \u2126                     \u03b2\u22c6\u03b12/(BD3)\u2225c + \u03bbw\u22252                   2    ,\nwhere Lvec    \u03bb    is the regularized loss of Proposition 6, \u03c1\u22c6                          is the scale in the second component of the mixture\nof (5) and \u03b2\u22c6          \u2208   (0, 1) is the mixture weight.\n       First, in Lemma 4 and Lemma 5, we give a formula for the desired correlation \u2207wLvec                                                                    \u03bb (w) \u00b7\n(c + \u03bbw) and, then we can provide a proof for Lemma 3 by lower bounding this formula.\nLemma 4 (Correlation with Regularization). Consider the function g(w) = E                                                           x\u223c\u03d5(\u00b7;w)[c \u00b7 x] + \u03bbH(w),\nwhere H is the negative entropy regularizer. Then it holds that\n                                             \u2207wg(w) \u00b7 (c + \u03bbw) = Varx\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x] .\nProof. Let us consider the following objective function:\n                                g(w) =             E                                       \u03d5(x; w) =               exp(w \u00b7 x)\n                                              x\u223c\u03d5(\u00b7;w)[c \u00b7 x] + \u03bbH(w) ,                                       \u2211y\u2208X exp(w \u00b7 y) ,\nwhere H is the negative entropy regularizer, i.e.,\n                              H(w) =               E                                       E                                \u2211     ew\u00b7y       .\nThe gradient of g with respect to w \u2208         x\u223c\u03d5(\u00b7;w) [log \u03d5(x; w)]W=is equal to     x\u223c\u03d5(\u00b7;w)[w \u00b7 x] \u2212          log       y\u2208X\n                                      \u2207wg(w) =                 E\n                                                           x\u223c\u03d5(\u00b7;w)[(c \u00b7 x)\u2207w log \u03d5(x; w)] + \u03bb\u2207wH(w) .\n                                                                                    26", "md": "and\n\n$$\nG\\rho^*w = E[x \\sim \\phi(\\cdot;\\rho^*w)[((c + \\lambda w) \\cdot x)x] - x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x] - x \\sim \\phi(\\cdot;\\rho^*w)[x] .\n$$\nNote that since \\( x \\in X \\), it holds that \\( \\|x\\|_2 \\leq D \\). Hence\n\n$$\n\\|Gw\\|_2 = \\sup |v \\cdot Gw| \\leq 2D^2\\|c + \\lambda w\\|_2 .\n$$\nMoreover, we have that\n\n$$\nv:\\|v\\|_2=1\n$$\nThis means that\n\n$$\n\\|G\\rho^*w\\|_2 \\leq 2D^2\\|c + \\lambda w\\|_2 .\n$$\n$$\n\\|\\nabla w L_{vec}^{\\lambda}(w)\\|_2 \\leq 2(1 - \\beta^*)\\|c + \\lambda w\\|_2D^2 + 2\\beta^*\\rho^*\\|c + \\lambda w\\|_2D^2 = O(D^2)\\|c + \\lambda w\\|_2 .\n$$\n## E.2.2 Correlation Lower Bound Lemma and Proof\n\nThe following lemma is the second ingredient in order to show Proposition 6.\n\nLemma 3 (Correlation Lower Bound for \\( L_{vec}^{\\lambda} \\)). Let \\( \\lambda > 0 \\). Let \\( \\|c\\|_2 \\leq C1 \\). Let Z be an open ball centered at 0 with diameter B = 2C1/\\lambda. Let X be a space of diameter D. Assume that \\( w \\in Z \\) and \\( Var x \\sim U(X)[(c + \\lambda w) \\cdot x] \\geq \\alpha\\|c + \\lambda w\\|_2^2 \\) for some \\( \\alpha > 0 \\). Then, for any \\( \\beta^* \\in (0, 1) \\), there exists \\( \\rho^* > 0 \\) such that it holds that\n\n$$\n\\nabla w L_{vec}^{\\lambda}(w) \\cdot (c + \\lambda w) = \\Omega \\beta^*\\alpha^2/(BD^3)\\|c + \\lambda w\\|_2^2 ,\n$$\nwhere \\( L_{vec}^{\\lambda} \\) is the regularized loss of Proposition 6, \\( \\rho^* \\) is the scale in the second component of the mixture of (5) and \\( \\beta^* \\in (0, 1) \\) is the mixture weight.\n\nFirst, in Lemma 4 and Lemma 5, we give a formula for the desired correlation \\( \\nabla w L_{vec}^{\\lambda}(w) \\cdot (c + \\lambda w) \\) and, then we can provide a proof for Lemma 3 by lower bounding this formula.\n\nLemma 4 (Correlation with Regularization). Consider the function \\( g(w) = E[x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda H(w)] \\), where H is the negative entropy regularizer. Then it holds that\n\n$$\n\\nabla wg(w) \\cdot (c + \\lambda w) = Varx \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] .\n$$\nProof. Let us consider the following objective function:\n\n$$\ng(w) = E[\\phi(x; w) = exp(w \\cdot x) x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda H(w) , \\sum_{y \\in X} exp(w \\cdot y) ,\n$$\nwhere H is the negative entropy regularizer, i.e.,\n\n$$\nH(w) = E[E \\sum_{y} e^{w \\cdot y} .\n$$\nThe gradient of g with respect to \\( w \\in x \\sim \\phi(\\cdot;w) [log \\phi(x; w)] \\) is equal to \\( x \\sim \\phi(\\cdot;w)[w \\cdot x] - log y \\in X \\)\n\n$$\n\\nabla wg(w) = E[x \\sim \\phi(\\cdot;w)[(c \\cdot x)\\nabla log \\phi(x; w)] + \\lambda \\nabla H(w) .\n$$\n26", "images": [], "items": [{"type": "text", "value": "and\n\n$$\nG\\rho^*w = E[x \\sim \\phi(\\cdot;\\rho^*w)[((c + \\lambda w) \\cdot x)x] - x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x] - x \\sim \\phi(\\cdot;\\rho^*w)[x] .\n$$\nNote that since \\( x \\in X \\), it holds that \\( \\|x\\|_2 \\leq D \\). Hence\n\n$$\n\\|Gw\\|_2 = \\sup |v \\cdot Gw| \\leq 2D^2\\|c + \\lambda w\\|_2 .\n$$\nMoreover, we have that\n\n$$\nv:\\|v\\|_2=1\n$$\nThis means that\n\n$$\n\\|G\\rho^*w\\|_2 \\leq 2D^2\\|c + \\lambda w\\|_2 .\n$$\n$$\n\\|\\nabla w L_{vec}^{\\lambda}(w)\\|_2 \\leq 2(1 - \\beta^*)\\|c + \\lambda w\\|_2D^2 + 2\\beta^*\\rho^*\\|c + \\lambda w\\|_2D^2 = O(D^2)\\|c + \\lambda w\\|_2 .\n$$", "md": "and\n\n$$\nG\\rho^*w = E[x \\sim \\phi(\\cdot;\\rho^*w)[((c + \\lambda w) \\cdot x)x] - x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x] - x \\sim \\phi(\\cdot;\\rho^*w)[x] .\n$$\nNote that since \\( x \\in X \\), it holds that \\( \\|x\\|_2 \\leq D \\). Hence\n\n$$\n\\|Gw\\|_2 = \\sup |v \\cdot Gw| \\leq 2D^2\\|c + \\lambda w\\|_2 .\n$$\nMoreover, we have that\n\n$$\nv:\\|v\\|_2=1\n$$\nThis means that\n\n$$\n\\|G\\rho^*w\\|_2 \\leq 2D^2\\|c + \\lambda w\\|_2 .\n$$\n$$\n\\|\\nabla w L_{vec}^{\\lambda}(w)\\|_2 \\leq 2(1 - \\beta^*)\\|c + \\lambda w\\|_2D^2 + 2\\beta^*\\rho^*\\|c + \\lambda w\\|_2D^2 = O(D^2)\\|c + \\lambda w\\|_2 .\n$$"}, {"type": "heading", "lvl": 2, "value": "E.2.2 Correlation Lower Bound Lemma and Proof", "md": "## E.2.2 Correlation Lower Bound Lemma and Proof"}, {"type": "text", "value": "The following lemma is the second ingredient in order to show Proposition 6.\n\nLemma 3 (Correlation Lower Bound for \\( L_{vec}^{\\lambda} \\)). Let \\( \\lambda > 0 \\). Let \\( \\|c\\|_2 \\leq C1 \\). Let Z be an open ball centered at 0 with diameter B = 2C1/\\lambda. Let X be a space of diameter D. Assume that \\( w \\in Z \\) and \\( Var x \\sim U(X)[(c + \\lambda w) \\cdot x] \\geq \\alpha\\|c + \\lambda w\\|_2^2 \\) for some \\( \\alpha > 0 \\). Then, for any \\( \\beta^* \\in (0, 1) \\), there exists \\( \\rho^* > 0 \\) such that it holds that\n\n$$\n\\nabla w L_{vec}^{\\lambda}(w) \\cdot (c + \\lambda w) = \\Omega \\beta^*\\alpha^2/(BD^3)\\|c + \\lambda w\\|_2^2 ,\n$$\nwhere \\( L_{vec}^{\\lambda} \\) is the regularized loss of Proposition 6, \\( \\rho^* \\) is the scale in the second component of the mixture of (5) and \\( \\beta^* \\in (0, 1) \\) is the mixture weight.\n\nFirst, in Lemma 4 and Lemma 5, we give a formula for the desired correlation \\( \\nabla w L_{vec}^{\\lambda}(w) \\cdot (c + \\lambda w) \\) and, then we can provide a proof for Lemma 3 by lower bounding this formula.\n\nLemma 4 (Correlation with Regularization). Consider the function \\( g(w) = E[x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda H(w)] \\), where H is the negative entropy regularizer. Then it holds that\n\n$$\n\\nabla wg(w) \\cdot (c + \\lambda w) = Varx \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] .\n$$\nProof. Let us consider the following objective function:\n\n$$\ng(w) = E[\\phi(x; w) = exp(w \\cdot x) x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda H(w) , \\sum_{y \\in X} exp(w \\cdot y) ,\n$$\nwhere H is the negative entropy regularizer, i.e.,\n\n$$\nH(w) = E[E \\sum_{y} e^{w \\cdot y} .\n$$\nThe gradient of g with respect to \\( w \\in x \\sim \\phi(\\cdot;w) [log \\phi(x; w)] \\) is equal to \\( x \\sim \\phi(\\cdot;w)[w \\cdot x] - log y \\in X \\)\n\n$$\n\\nabla wg(w) = E[x \\sim \\phi(\\cdot;w)[(c \\cdot x)\\nabla log \\phi(x; w)] + \\lambda \\nabla H(w) .\n$$\n26", "md": "The following lemma is the second ingredient in order to show Proposition 6.\n\nLemma 3 (Correlation Lower Bound for \\( L_{vec}^{\\lambda} \\)). Let \\( \\lambda > 0 \\). Let \\( \\|c\\|_2 \\leq C1 \\). Let Z be an open ball centered at 0 with diameter B = 2C1/\\lambda. Let X be a space of diameter D. Assume that \\( w \\in Z \\) and \\( Var x \\sim U(X)[(c + \\lambda w) \\cdot x] \\geq \\alpha\\|c + \\lambda w\\|_2^2 \\) for some \\( \\alpha > 0 \\). Then, for any \\( \\beta^* \\in (0, 1) \\), there exists \\( \\rho^* > 0 \\) such that it holds that\n\n$$\n\\nabla w L_{vec}^{\\lambda}(w) \\cdot (c + \\lambda w) = \\Omega \\beta^*\\alpha^2/(BD^3)\\|c + \\lambda w\\|_2^2 ,\n$$\nwhere \\( L_{vec}^{\\lambda} \\) is the regularized loss of Proposition 6, \\( \\rho^* \\) is the scale in the second component of the mixture of (5) and \\( \\beta^* \\in (0, 1) \\) is the mixture weight.\n\nFirst, in Lemma 4 and Lemma 5, we give a formula for the desired correlation \\( \\nabla w L_{vec}^{\\lambda}(w) \\cdot (c + \\lambda w) \\) and, then we can provide a proof for Lemma 3 by lower bounding this formula.\n\nLemma 4 (Correlation with Regularization). Consider the function \\( g(w) = E[x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda H(w)] \\), where H is the negative entropy regularizer. Then it holds that\n\n$$\n\\nabla wg(w) \\cdot (c + \\lambda w) = Varx \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] .\n$$\nProof. Let us consider the following objective function:\n\n$$\ng(w) = E[\\phi(x; w) = exp(w \\cdot x) x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda H(w) , \\sum_{y \\in X} exp(w \\cdot y) ,\n$$\nwhere H is the negative entropy regularizer, i.e.,\n\n$$\nH(w) = E[E \\sum_{y} e^{w \\cdot y} .\n$$\nThe gradient of g with respect to \\( w \\in x \\sim \\phi(\\cdot;w) [log \\phi(x; w)] \\) is equal to \\( x \\sim \\phi(\\cdot;w)[w \\cdot x] - log y \\in X \\)\n\n$$\n\\nabla wg(w) = E[x \\sim \\phi(\\cdot;w)[(c \\cdot x)\\nabla log \\phi(x; w)] + \\lambda \\nabla H(w) .\n$$\n26"}]}, {"page": 27, "text": "It holds that                   \u2207w log \u03d5(x; w) = \u2207w                      w \u00b7 x \u2212      log \u2211        ew\u00b7y       = x \u2212           E\nand                                                                                         y\u2208X                           x\u223c\u03d5(\u00b7;w)[x] ,\n\u2207H(w) =                 E                     E                                                      E                      E\nSo, we get that    x\u223c\u03d5(\u00b7;w)[x] +         x\u223c\u03d5(\u00b7;w)[(w \u00b7 x)\u2207w log \u03d5(x; w)] \u2212                       x\u223c\u03d5(\u00b7;w)[x] =         x\u223c\u03d5(\u00b7;w)[(w \u00b7 x)\u2207w log \u03d5(x; w)] .\n                          \u2207wg(w) =                 E                                             E                                   E\nNote that                                     x\u223c\u03d5(\u00b7;w)[((c + \u03bbw) \u00b7 x)x] \u2212                   x\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x]              x\u223c\u03d5(\u00b7;w)[x] .\n                                             \u2207wg(w) \u00b7 (c + \u03bbw) = Varx\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x] .\nLemma 5 (Gradient with Regularization and Mixing). For any \u03f5 > 0, for the family of solution gen-\nerators P = {p(\u00b7; w) = (1 \u2212                      \u03b2\u22c6)\u03d5(\u00b7; w) + \u03b2\u22c6\u03d5(\u00b7; \u03c1\u22c6w) : w \u2208                        W} and the objective Lvec             \u03bb     of Equation (3),\nit holds that\n      \u2207wLvec   \u03bb (w) \u00b7 (c + \u03bbw) = (1 \u2212                    \u03b2\u22c6) Varx\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x] + \u03b2\u22c6\u03c1\u22c6                          Varx\u223c\u03d5(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x] ,\nfor any \u03bb > 0.\nProof. Let us first consider the scaled parameter \u03c1w \u2208                                           W for some \u03c1 > 0. Then it holds that\n\u2207w           E                           E          (c \u00b7 x)      \u03c1x \u2212           E                     = \u03c1         E          (c \u00b7 x)       x \u2212          E               .\n       x\u223c\u03d5(\u00b7;\u03c1w)[c \u00b7 x] =          x\u223c\u03d5(\u00b7;\u03c1w)                               x\u223c\u03d5(\u00b7;\u03c1w)[\u03c1x]                     x\u223c\u03d5(\u00b7;\u03c1w)                            x\u223c\u03d5(\u00b7;\u03c1w)[x]\nMoreover, the negative entropy regularizer at \u03c1w is\n                                               H(\u03c1w) =                E                                          e(\u03c1w)\u00b7y .\nIt holds that                                                    x\u223c\u03d5(\u00b7;\u03c1w)[(\u03c1w) \u00b7 x] \u2212             log \u2211  y\u2208X\n\u2207wH(\u03c1w) =                    E                                                                     E                                  E                        E           .\n                        x\u223c\u03d5(\u00b7;\u03c1w)[(\u03c1w \u00b7 x)\u2207w log \u03d5(x; \u03c1w)] = \u03c12                               x\u223c\u03d5(\u00b7;\u03c1w)[(w \u00b7 x)x] \u2212              x\u223c\u03d5(\u00b7;\u03c1w)[w \u00b7 x]         x\u223c\u03d5(\u00b7;\u03c1w)[x]\nWe consider the objective function Lvec                           \u03bb    to be defined as follows: first, we take\n                                                   p(\u00b7; w) = (1 \u2212            \u03b2\u22c6)\u03d5(\u00b7; w) + \u03b2\u22c6\u03d5(\u00b7; \u03c1\u22c6w) ,\ni.e., p(\u00b7; w) is the mixture of the probability measures \u03d5(\u00b7; w) and \u03d5(\u00b7; \u03c1\u22c6w) with weights 1 \u2212\n\u03b2\u22c6    and \u03b2\u22c6        respectively for some scale \u03c1\u22c6                          > 0. Moreover, we take Rvec(w) = (1 \u2212                                       \u03b2\u22c6)H(w) +\n \u03b2\u22c6\n \u03c1\u22c6 H(\u03c1\u22c6w). Then we define our regularized loss Lvec  Lvec                     E         \u03bb    to be\n                                                         \u03bb (w) =          x\u223cp(\u00b7;w)[c \u00b7 x] + \u03bbRvec(w) .\n                                                                                    27", "md": "# Math Equations\n\nIt holds that $$\\nabla_w \\log \\phi(x; w) = \\nabla_w (w \\cdot x - \\log \\sum_{y \\in X} e^{w \\cdot y}) = x - E$$ and $$y \\sim X, x \\sim \\phi(\\cdot; w)[x],$$\n\n$$\\nabla H(w) = E[E[E[E]]],$$\n\nSo, we get that $$x \\sim \\phi(\\cdot; w)[x] + x \\sim \\phi(\\cdot; w)[(w \\cdot x) \\nabla_w \\log \\phi(x; w)] - x \\sim \\phi(\\cdot; w)[x] = x \\sim \\phi(\\cdot; w)[(w \\cdot x) \\nabla_w \\log \\phi(x; w)].$$\n\n$$\\nabla_w g(w) = E[E[E]],$$\n\nNote that $$x \\sim \\phi(\\cdot; w)[((c + \\lambda w) \\cdot x)x] - x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x] - x \\sim \\phi(\\cdot; w)[x].$$\n\n$$\\nabla_w g(w) \\cdot (c + \\lambda w) = \\text{Var} x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x].$$\n\nLemma 5 (Gradient with Regularization and Mixing). For any $$\\epsilon > 0$$, for the family of solution generators $$P = \\{p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^*\\phi(\\cdot; \\rho^*w) : w \\in W\\}$$ and the objective $$L_{\\text{vec}}^{\\lambda}$$ of Equation (3),\n\nit holds that\n\n$$\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var} x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x] + \\beta^* \\rho^* \\text{Var} x \\sim \\phi(\\cdot; \\rho^*w)[(c + \\lambda w) \\cdot x],$$\n\nfor any $$\\lambda > 0.$$\n\nProof. Let us first consider the scaled parameter $$\\rho w \\in W$$ for some $$\\rho > 0$$. Then it holds that\n\n$$\\nabla_w E[E(c \\cdot x) \\rho x - E[E] = \\rho E(c \\cdot x) x - E[E].$$\n\n$$x \\sim \\phi(\\cdot; \\rho w)[c \\cdot x] = x \\sim \\phi(\\cdot; \\rho w) x \\sim \\phi(\\cdot; \\rho w)[\\rho x] x \\sim \\phi(\\cdot; \\rho w) x \\sim \\phi(\\cdot; \\rho w)[x].$$\n\nMoreover, the negative entropy regularizer at $$\\rho w$$ is\n\n$$H(\\rho w) = E[e(\\rho w) \\cdot y].$$\n\nIt holds that\n\n$$x \\sim \\phi(\\cdot; \\rho w)[(\\rho w) \\cdot x] - \\log \\sum_{y \\in X}$$\n\n$$\\nabla_w H(\\rho w) = E[E[E[E]]] = \\rho^2 x \\sim \\phi(\\cdot; \\rho w)[(w \\cdot x) x] - x \\sim \\phi(\\cdot; \\rho w)[w \\cdot x] x \\sim \\phi(\\cdot; \\rho w)[x].$$\n\nWe consider the objective function $$L_{\\text{vec}}^{\\lambda}$$ to be defined as follows: first, we take\n\n$$p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^*\\phi(\\cdot; \\rho^*w),$$\n\ni.e., $$p(\\cdot; w)$$ is the mixture of the probability measures $$\\phi(\\cdot; w)$$ and $$\\phi(\\cdot; \\rho^*w)$$ with weights $$1 - \\beta^*$$ and $$\\beta^*$$ respectively for some scale $$\\rho^* > 0$$. Moreover, we take $$R_{\\text{vec}}(w) = (1 - \\beta^*)H(w) + \\beta^* \\rho^* H(\\rho^*w)$$. Then we define our regularized loss $$L_{\\text{vec}}^{\\lambda}$$ to be\n\n$$\\lambda(w) = x \\sim p(\\cdot;w)[c \\cdot x] + \\lambda R_{\\text{vec}}(w).$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "It holds that $$\\nabla_w \\log \\phi(x; w) = \\nabla_w (w \\cdot x - \\log \\sum_{y \\in X} e^{w \\cdot y}) = x - E$$ and $$y \\sim X, x \\sim \\phi(\\cdot; w)[x],$$\n\n$$\\nabla H(w) = E[E[E[E]]],$$\n\nSo, we get that $$x \\sim \\phi(\\cdot; w)[x] + x \\sim \\phi(\\cdot; w)[(w \\cdot x) \\nabla_w \\log \\phi(x; w)] - x \\sim \\phi(\\cdot; w)[x] = x \\sim \\phi(\\cdot; w)[(w \\cdot x) \\nabla_w \\log \\phi(x; w)].$$\n\n$$\\nabla_w g(w) = E[E[E]],$$\n\nNote that $$x \\sim \\phi(\\cdot; w)[((c + \\lambda w) \\cdot x)x] - x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x] - x \\sim \\phi(\\cdot; w)[x].$$\n\n$$\\nabla_w g(w) \\cdot (c + \\lambda w) = \\text{Var} x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x].$$\n\nLemma 5 (Gradient with Regularization and Mixing). For any $$\\epsilon > 0$$, for the family of solution generators $$P = \\{p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^*\\phi(\\cdot; \\rho^*w) : w \\in W\\}$$ and the objective $$L_{\\text{vec}}^{\\lambda}$$ of Equation (3),\n\nit holds that\n\n$$\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var} x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x] + \\beta^* \\rho^* \\text{Var} x \\sim \\phi(\\cdot; \\rho^*w)[(c + \\lambda w) \\cdot x],$$\n\nfor any $$\\lambda > 0.$$\n\nProof. Let us first consider the scaled parameter $$\\rho w \\in W$$ for some $$\\rho > 0$$. Then it holds that\n\n$$\\nabla_w E[E(c \\cdot x) \\rho x - E[E] = \\rho E(c \\cdot x) x - E[E].$$\n\n$$x \\sim \\phi(\\cdot; \\rho w)[c \\cdot x] = x \\sim \\phi(\\cdot; \\rho w) x \\sim \\phi(\\cdot; \\rho w)[\\rho x] x \\sim \\phi(\\cdot; \\rho w) x \\sim \\phi(\\cdot; \\rho w)[x].$$\n\nMoreover, the negative entropy regularizer at $$\\rho w$$ is\n\n$$H(\\rho w) = E[e(\\rho w) \\cdot y].$$\n\nIt holds that\n\n$$x \\sim \\phi(\\cdot; \\rho w)[(\\rho w) \\cdot x] - \\log \\sum_{y \\in X}$$\n\n$$\\nabla_w H(\\rho w) = E[E[E[E]]] = \\rho^2 x \\sim \\phi(\\cdot; \\rho w)[(w \\cdot x) x] - x \\sim \\phi(\\cdot; \\rho w)[w \\cdot x] x \\sim \\phi(\\cdot; \\rho w)[x].$$\n\nWe consider the objective function $$L_{\\text{vec}}^{\\lambda}$$ to be defined as follows: first, we take\n\n$$p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^*\\phi(\\cdot; \\rho^*w),$$\n\ni.e., $$p(\\cdot; w)$$ is the mixture of the probability measures $$\\phi(\\cdot; w)$$ and $$\\phi(\\cdot; \\rho^*w)$$ with weights $$1 - \\beta^*$$ and $$\\beta^*$$ respectively for some scale $$\\rho^* > 0$$. Moreover, we take $$R_{\\text{vec}}(w) = (1 - \\beta^*)H(w) + \\beta^* \\rho^* H(\\rho^*w)$$. Then we define our regularized loss $$L_{\\text{vec}}^{\\lambda}$$ to be\n\n$$\\lambda(w) = x \\sim p(\\cdot;w)[c \\cdot x] + \\lambda R_{\\text{vec}}(w).$$", "md": "It holds that $$\\nabla_w \\log \\phi(x; w) = \\nabla_w (w \\cdot x - \\log \\sum_{y \\in X} e^{w \\cdot y}) = x - E$$ and $$y \\sim X, x \\sim \\phi(\\cdot; w)[x],$$\n\n$$\\nabla H(w) = E[E[E[E]]],$$\n\nSo, we get that $$x \\sim \\phi(\\cdot; w)[x] + x \\sim \\phi(\\cdot; w)[(w \\cdot x) \\nabla_w \\log \\phi(x; w)] - x \\sim \\phi(\\cdot; w)[x] = x \\sim \\phi(\\cdot; w)[(w \\cdot x) \\nabla_w \\log \\phi(x; w)].$$\n\n$$\\nabla_w g(w) = E[E[E]],$$\n\nNote that $$x \\sim \\phi(\\cdot; w)[((c + \\lambda w) \\cdot x)x] - x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x] - x \\sim \\phi(\\cdot; w)[x].$$\n\n$$\\nabla_w g(w) \\cdot (c + \\lambda w) = \\text{Var} x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x].$$\n\nLemma 5 (Gradient with Regularization and Mixing). For any $$\\epsilon > 0$$, for the family of solution generators $$P = \\{p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^*\\phi(\\cdot; \\rho^*w) : w \\in W\\}$$ and the objective $$L_{\\text{vec}}^{\\lambda}$$ of Equation (3),\n\nit holds that\n\n$$\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var} x \\sim \\phi(\\cdot; w)[(c + \\lambda w) \\cdot x] + \\beta^* \\rho^* \\text{Var} x \\sim \\phi(\\cdot; \\rho^*w)[(c + \\lambda w) \\cdot x],$$\n\nfor any $$\\lambda > 0.$$\n\nProof. Let us first consider the scaled parameter $$\\rho w \\in W$$ for some $$\\rho > 0$$. Then it holds that\n\n$$\\nabla_w E[E(c \\cdot x) \\rho x - E[E] = \\rho E(c \\cdot x) x - E[E].$$\n\n$$x \\sim \\phi(\\cdot; \\rho w)[c \\cdot x] = x \\sim \\phi(\\cdot; \\rho w) x \\sim \\phi(\\cdot; \\rho w)[\\rho x] x \\sim \\phi(\\cdot; \\rho w) x \\sim \\phi(\\cdot; \\rho w)[x].$$\n\nMoreover, the negative entropy regularizer at $$\\rho w$$ is\n\n$$H(\\rho w) = E[e(\\rho w) \\cdot y].$$\n\nIt holds that\n\n$$x \\sim \\phi(\\cdot; \\rho w)[(\\rho w) \\cdot x] - \\log \\sum_{y \\in X}$$\n\n$$\\nabla_w H(\\rho w) = E[E[E[E]]] = \\rho^2 x \\sim \\phi(\\cdot; \\rho w)[(w \\cdot x) x] - x \\sim \\phi(\\cdot; \\rho w)[w \\cdot x] x \\sim \\phi(\\cdot; \\rho w)[x].$$\n\nWe consider the objective function $$L_{\\text{vec}}^{\\lambda}$$ to be defined as follows: first, we take\n\n$$p(\\cdot; w) = (1 - \\beta^*)\\phi(\\cdot; w) + \\beta^*\\phi(\\cdot; \\rho^*w),$$\n\ni.e., $$p(\\cdot; w)$$ is the mixture of the probability measures $$\\phi(\\cdot; w)$$ and $$\\phi(\\cdot; \\rho^*w)$$ with weights $$1 - \\beta^*$$ and $$\\beta^*$$ respectively for some scale $$\\rho^* > 0$$. Moreover, we take $$R_{\\text{vec}}(w) = (1 - \\beta^*)H(w) + \\beta^* \\rho^* H(\\rho^*w)$$. Then we define our regularized loss $$L_{\\text{vec}}^{\\lambda}$$ to be\n\n$$\\lambda(w) = x \\sim p(\\cdot;w)[c \\cdot x] + \\lambda R_{\\text{vec}}(w).$$"}]}, {"page": 28, "text": "Using Lemma 4 and the above calculations, we have that\n\u2207wLvec   \u03bb (w) = (1 \u2212             \u03b2\u22c6)\u2207w             E                                                                        E                         \u03c1\u22c6  \u2207wH(\u03c1\u22c6w)\n                                               x\u223c\u03d5(\u00b7;w)[c \u00b7 x] + \u03bb(1 \u2212              \u03b2\u22c6)\u2207wH(w) + \u03b2\u22c6\u2207                    x\u223c\u03d5(\u00b7;\u03c1\u22c6x)[c \u00b7 x] + \u03bb \u03b2\u22c6\n                     = (1 \u2212       \u03b2\u22c6)            E                                            E                                   E               +\n                                            x\u223c\u03d5(\u00b7;w)[((c + \u03bbw) \u00b7 x)x] \u2212                   x\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x]              x\u223c\u03d5(\u00b7;w)[x]\n                     + \u03b2\u22c6\u03c1\u22c6                E                                                E                                      E                .\nThe above calculations yield         x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[((c + \u03bbw) \u00b7 x)x] \u2212                   x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x]               x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[x]\n      \u2207wLvec   \u03bb (w) \u00b7 (c + \u03bbw) = (1 \u2212                    \u03b2\u22c6) Varx\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x] + \u03b2\u22c6\u03c1\u22c6                          Varx\u223c\u03d5(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x] ,\nand this concludes the proof.\n       The above correlation being positive intuitively means that performing gradient descent to\nLvec     gives that the parameter w converges to \u2212c/\u03bb, the point that achieves completeness for that\n   \u03bb\nobjective.\n       However, to obtain fast convergence, we need to show that the above correlation is non-\ntrivial. This means that our goal in order to prove Lemma 3 is to provide a lower bound for the\nabove quantity, i.e., it suffices to give a non-trivial lower bound for the variance of the random\nvariable (c + \u03bbw) \u00b7 x with respect to the probability measure \u03d5(\u00b7; \u03c1\u22c6w). It is important to note that\nin the above statement we did not fix the value of \u03c1\u22c6. We can now make use of Proposition 8.\nIntuitively, by taking the scale parameter appearing in the mixture \u03c1\u22c6                                                      to be sufficiently small, we\ncan manage to provide a lower bound for the variance of (c + \u03bbw) \u00b7 x with respect to the almost\nuniform measure, i.e, the second summand of the above right-hand side expression has signifi-\ncant contribution. We remark that \u03c1 corresponds to the inverse temperature parameter. Hence,\nour previous analysis essentially implies that policy gradient on combinatorial optimization po-\ntentially works if the variance Var                            x\u223c\u03d5(\u00b7;\u03c1w)[(c + \u03bbw) \u00b7 x] is non-vanishing at high temperatures\n1/\u03c1.\nThe proof of Lemma 3. Recall that                                                          ew\u00b7x\n                                                                   \u03d5(x; w) =          \u2211y\u2208X ew\u00b7y .\nLet also D be the diameter of X and B the diameter of Z. Recall from Lemma 5 that we have that\n      \u2207wLvec   \u03bb (w) \u00b7 (c + \u03bbw) = (1 \u2212                    \u03b2\u22c6) Varx\u223cp(\u00b7;w)[(c + \u03bbw) \u00b7 x] + \u03b2\u22c6\u03c1\u22c6                          Varx\u223cp(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x] ,\nwhere the scale parameter \u03c1\u22c6                         > 0 is to be decided. This means that\n                                     \u2207wLvec   \u03bb (w) \u00b7 (c + \u03bbw) \u2265                 \u03b2\u22c6\u03c1\u22c6    Varx\u223c\u03d5(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x] .\nOur goal is now to apply Proposition 8 in order to lower bound the above variance. Applying\nProposition 8 for \u00b5 \u2190                    \u03d5(\u00b7; \u03c1\u22c6w), c \u2190            c + \u03bbw \u2208          Z and, so for some absolute constant C0, we can\npick\n                                                                          \u03c1\u22c6   = C0     BD3\u03b1.\n                                                                                    28", "md": "# Math Equations\n\nUsing Lemma 4 and the above calculations, we have that\n\n$$\n\\begin{align*}\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) &= (1 - \\beta^*)\\nabla_w E[\\rho^* \\nabla_w H(\\rho^*w) x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda(1 - \\beta^*)\\nabla_w H(w) + \\beta^*\\nabla x \\sim \\phi(\\cdot;\\rho^*x)[c \\cdot x] + \\lambda \\beta^* \\\\\n&= (1 - \\beta^*) E[x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] E[x \\sim \\phi(\\cdot;w)[x] + \\beta^*\\rho^* E[x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] E[x \\sim \\phi(\\cdot;w)[x] \\\\\n&+ \\beta^*\\rho^* Var[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* Var[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n\\end{align*}\n$$\nThe above calculations yield\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var}[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n$$\nand this concludes the proof.\n\nThe above correlation being positive intuitively means that performing gradient descent to $L_{\\text{vec}}^{\\lambda}$ gives that the parameter $w$ converges to $-\\frac{c}{\\lambda}$, the point that achieves completeness for that objective.\n\nHowever, to obtain fast convergence, we need to show that the above correlation is non-trivial. This means that our goal in order to prove Lemma 3 is to provide a lower bound for the above quantity, i.e., it suffices to give a non-trivial lower bound for the variance of the random variable $(c + \\lambda w) \\cdot x$ with respect to the probability measure $\\phi(\\cdot; \\rho^*w)$. It is important to note that in the above statement we did not fix the value of $\\rho^*$. We can now make use of Proposition 8.\n\nIntuitively, by taking the scale parameter appearing in the mixture $\\rho^*$ to be sufficiently small, we can manage to provide a lower bound for the variance of $(c + \\lambda w) \\cdot x$ with respect to the almost uniform measure, i.e., the second summand of the above right-hand side expression has significant contribution. We remark that $\\rho$ corresponds to the inverse temperature parameter. Hence, our previous analysis essentially implies that policy gradient on combinatorial optimization potentially works if the variance $\\text{Var}[x \\sim \\phi(\\cdot;\\rho w)[(c + \\lambda w) \\cdot x]$ is non-vanishing at high temperatures $\\frac{1}{\\rho}$.\n\nThe proof of Lemma 3. Recall that $\\phi(x; w) = \\sum_{y \\in X} e^{w \\cdot y}$. Let also $D$ be the diameter of $X$ and $B$ the diameter of $Z$. Recall from Lemma 5 that we have that\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var}[x \\sim p(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\text{Var}[x \\sim p(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n$$\nwhere the scale parameter $\\rho^* > 0$ is to be decided. This means that\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) \\geq \\beta^*\\rho^* \\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x].\n$$\nOur goal is now to apply Proposition 8 in order to lower bound the above variance. Applying Proposition 8 for $\\mu \\leftarrow \\phi(\\cdot; \\rho^*w), c \\leftarrow c + \\lambda w \\in Z$ and, so for some absolute constant $C_0$, we can pick\n\n$$\n\\rho^* = C_0 BD^{3\\alpha}.\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Using Lemma 4 and the above calculations, we have that\n\n$$\n\\begin{align*}\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) &= (1 - \\beta^*)\\nabla_w E[\\rho^* \\nabla_w H(\\rho^*w) x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda(1 - \\beta^*)\\nabla_w H(w) + \\beta^*\\nabla x \\sim \\phi(\\cdot;\\rho^*x)[c \\cdot x] + \\lambda \\beta^* \\\\\n&= (1 - \\beta^*) E[x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] E[x \\sim \\phi(\\cdot;w)[x] + \\beta^*\\rho^* E[x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] E[x \\sim \\phi(\\cdot;w)[x] \\\\\n&+ \\beta^*\\rho^* Var[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* Var[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n\\end{align*}\n$$\nThe above calculations yield\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var}[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n$$\nand this concludes the proof.\n\nThe above correlation being positive intuitively means that performing gradient descent to $L_{\\text{vec}}^{\\lambda}$ gives that the parameter $w$ converges to $-\\frac{c}{\\lambda}$, the point that achieves completeness for that objective.\n\nHowever, to obtain fast convergence, we need to show that the above correlation is non-trivial. This means that our goal in order to prove Lemma 3 is to provide a lower bound for the above quantity, i.e., it suffices to give a non-trivial lower bound for the variance of the random variable $(c + \\lambda w) \\cdot x$ with respect to the probability measure $\\phi(\\cdot; \\rho^*w)$. It is important to note that in the above statement we did not fix the value of $\\rho^*$. We can now make use of Proposition 8.\n\nIntuitively, by taking the scale parameter appearing in the mixture $\\rho^*$ to be sufficiently small, we can manage to provide a lower bound for the variance of $(c + \\lambda w) \\cdot x$ with respect to the almost uniform measure, i.e., the second summand of the above right-hand side expression has significant contribution. We remark that $\\rho$ corresponds to the inverse temperature parameter. Hence, our previous analysis essentially implies that policy gradient on combinatorial optimization potentially works if the variance $\\text{Var}[x \\sim \\phi(\\cdot;\\rho w)[(c + \\lambda w) \\cdot x]$ is non-vanishing at high temperatures $\\frac{1}{\\rho}$.\n\nThe proof of Lemma 3. Recall that $\\phi(x; w) = \\sum_{y \\in X} e^{w \\cdot y}$. Let also $D$ be the diameter of $X$ and $B$ the diameter of $Z$. Recall from Lemma 5 that we have that\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var}[x \\sim p(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\text{Var}[x \\sim p(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n$$\nwhere the scale parameter $\\rho^* > 0$ is to be decided. This means that\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) \\geq \\beta^*\\rho^* \\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x].\n$$\nOur goal is now to apply Proposition 8 in order to lower bound the above variance. Applying Proposition 8 for $\\mu \\leftarrow \\phi(\\cdot; \\rho^*w), c \\leftarrow c + \\lambda w \\in Z$ and, so for some absolute constant $C_0$, we can pick\n\n$$\n\\rho^* = C_0 BD^{3\\alpha}.\n$$", "md": "Using Lemma 4 and the above calculations, we have that\n\n$$\n\\begin{align*}\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) &= (1 - \\beta^*)\\nabla_w E[\\rho^* \\nabla_w H(\\rho^*w) x \\sim \\phi(\\cdot;w)[c \\cdot x] + \\lambda(1 - \\beta^*)\\nabla_w H(w) + \\beta^*\\nabla x \\sim \\phi(\\cdot;\\rho^*x)[c \\cdot x] + \\lambda \\beta^* \\\\\n&= (1 - \\beta^*) E[x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] E[x \\sim \\phi(\\cdot;w)[x] + \\beta^*\\rho^* E[x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] - E[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] E[x \\sim \\phi(\\cdot;w)[x] \\\\\n&+ \\beta^*\\rho^* Var[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* Var[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n\\end{align*}\n$$\nThe above calculations yield\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var}[x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n$$\nand this concludes the proof.\n\nThe above correlation being positive intuitively means that performing gradient descent to $L_{\\text{vec}}^{\\lambda}$ gives that the parameter $w$ converges to $-\\frac{c}{\\lambda}$, the point that achieves completeness for that objective.\n\nHowever, to obtain fast convergence, we need to show that the above correlation is non-trivial. This means that our goal in order to prove Lemma 3 is to provide a lower bound for the above quantity, i.e., it suffices to give a non-trivial lower bound for the variance of the random variable $(c + \\lambda w) \\cdot x$ with respect to the probability measure $\\phi(\\cdot; \\rho^*w)$. It is important to note that in the above statement we did not fix the value of $\\rho^*$. We can now make use of Proposition 8.\n\nIntuitively, by taking the scale parameter appearing in the mixture $\\rho^*$ to be sufficiently small, we can manage to provide a lower bound for the variance of $(c + \\lambda w) \\cdot x$ with respect to the almost uniform measure, i.e., the second summand of the above right-hand side expression has significant contribution. We remark that $\\rho$ corresponds to the inverse temperature parameter. Hence, our previous analysis essentially implies that policy gradient on combinatorial optimization potentially works if the variance $\\text{Var}[x \\sim \\phi(\\cdot;\\rho w)[(c + \\lambda w) \\cdot x]$ is non-vanishing at high temperatures $\\frac{1}{\\rho}$.\n\nThe proof of Lemma 3. Recall that $\\phi(x; w) = \\sum_{y \\in X} e^{w \\cdot y}$. Let also $D$ be the diameter of $X$ and $B$ the diameter of $Z$. Recall from Lemma 5 that we have that\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) = (1 - \\beta^*) \\text{Var}[x \\sim p(\\cdot;w)[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\text{Var}[x \\sim p(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x],\n$$\nwhere the scale parameter $\\rho^* > 0$ is to be decided. This means that\n\n$$\n\\nabla_w L_{\\text{vec}}^{\\lambda}(w) \\cdot (c + \\lambda w) \\geq \\beta^*\\rho^* \\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x].\n$$\nOur goal is now to apply Proposition 8 in order to lower bound the above variance. Applying Proposition 8 for $\\mu \\leftarrow \\phi(\\cdot; \\rho^*w), c \\leftarrow c + \\lambda w \\in Z$ and, so for some absolute constant $C_0$, we can pick\n\n$$\n\\rho^* = C_0 BD^{3\\alpha}.\n$$"}]}, {"page": 29, "text": "Thus, we have that\nThis implies the desired result since          Var   x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x] \u2265                  \u2126(\u03b1\u2225c + \u03bbw\u22252            2) .\n                                         \u2207wLvec   \u03bb (w) \u00b7 (c + \u03bbw) \u2265                 C0\u03b2\u22c6\u03b12/(BD3)\u2225c + \u03bbw\u22252                     2 .\nE.3        Convergence for Quasar Convex Functions\nThe fact that L\u03bb is quasar convex with respect to \u2212M/\u03bb implies that projected SGD converges\nto that point in a small number if steps and hence the family P is efficiently optimizable. The\nanalysis is standard (see e.g., [HMR16]). For completeness a proof can be found in Appendix E.3.\nProposition 7 (Convergence). Consider \u03f5 > 0 and a prior R over I. Assume that Assumption 1 holds\nwith parameters C, DS, DI, \u03b1. Let W1, . . . , WT be the updates of the SGD algorithm with projection set W\nperformed on L\u03bb of Equation (2) with appropriate step size and parameter \u03bb. Then, for the non-regularized\nobjective L, it holds that                                   E\nwhen T \u2265          poly(1/\u03f5, 1/\u03b1, C, DS, DI, \u2225W0 + M/\u03bb\u2225F).t\u223cU([T])[L(Wt)] \u2264              L(\u2212M/\u03bb) + \u03f5 ,\n       Our next goal is to use Proposition 1 and show that standard projected SGD on the objective\nL\u03bb converges in a polynomial number of steps. The intuition behind this result is that since the\ncorrelation between \u2207L\u03bb(W) and the direction M + \u03bbW is positive and non-trivial, the gradient\nfield drives the optimization method towards the point \u2212M/\u03bb.\nProof. Consider the sequence of matrices W1, . . . , Wt, . . . , WT generated by applying PSGD on L\u03bb\nwith step size \u03b7 (to be decided) and initial parameter vector W0 \u2208                                                  W. We have that L\u03bb is \u03b3-quasar\nconvex and is also O(\u0393)-weakly smooth2 since we now show that it is \u0393-smooth.\nLemma 6. L\u03bb is poly(DS, DI, C)-smooth.\nProof. We have that\n                            \u2225\u2207L\u03bb(W)\u22252            F = \u2225       E                          F \u2264       E          2\u2225\u2207wLvec     \u03bb (z\u22a4W)\u22252         F .\n                                                           z\u223cR[\u2207L\u03bb,z(W)]\u22252                      z\u223cR \u2225z\u22252\nIt suffices to show that Lvec                \u03bb    is smooth. Recall that\n            \u2207wLvec  \u03bb (w) = (1 \u2212              \u03b2\u22c6)           E                                             E                                   E              +\n                                                       x\u223c\u03d5(\u00b7;w)[((c + \u03bbw) \u00b7 x)x] \u2212                   x\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x]              x\u223c\u03d5(\u00b7;w)[x]\n                                + \u03b2\u22c6\u03c1\u22c6                E                                                E                                      E                 .\nThis means that                                 x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[((c + \u03bbw) \u00b7 x)x] \u2212                    x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x]               x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[x]\n                                      \u2225\u22072   wLvec\u03bb (w)\u22252      F \u2264    (1 \u2212     \u03b2\u22c6)(A1 + A2) + \u03b2\u22c6\u03c1\u22c6(A3 + A4) ,\n     2As mentioned in [HMR16], a function f is \u0393-weakly smooth if for any point \u03b8, \u2225\u2207                                                f (\u03b8)\u22252 \u2264      \u0393( f (\u03b8) \u2212    f (\u03b8\u22c6)).\nMoreover, a function f that is \u0393-smooth (in the sense \u2225\u22072 f \u2225                            \u2264   \u0393), is also O(\u0393)-weakly smooth.\n                                                                                    29", "md": "Thus, we have that\nThis implies the desired result since $$\\text{Var } x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x] \\geq \\Omega(\\alpha \\|c + \\lambda w\\|_2^2)$$.\n$$\\nabla_w L_{\\text{vec}}^\\lambda(w) \\cdot (c + \\lambda w) \\geq \\frac{C_0\\beta^* \\alpha^2}{(BD^3)\\|c + \\lambda w\\|_2^2}$$.\n\n### Convergence for Quasar Convex Functions\nThe fact that $$L_\\lambda$$ is quasar convex with respect to $$-\\frac{M}{\\lambda}$$ implies that projected SGD converges to that point in a small number of steps and hence the family P is efficiently optimizable. The analysis is standard (see e.g., [HMR16]). For completeness, a proof can be found in Appendix E.3.\n\n**Proposition 7 (Convergence):** Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds with parameters $$C, DS, DI, \\alpha$$. Let $$W_1, ..., W_T$$ be the updates of the SGD algorithm with projection set $$W$$ performed on $$L_\\lambda$$ of Equation (2) with appropriate step size and parameter $$\\lambda$$. Then, for the non-regularized objective $$L$$, it holds that\n$$\\mathbb{E}_{t \\sim U([T])}[L(W_t)] \\leq L(-M/\\lambda) + \\epsilon$$ when $$T \\geq \\text{poly}(1/\\epsilon, 1/\\alpha, C, DS, DI, \\|W_0 + M/\\lambda\\|_F)$$.\n\nOur next goal is to use Proposition 1 and show that standard projected SGD on the objective $$L_\\lambda$$ converges in a polynomial number of steps. The intuition behind this result is that since the correlation between $$\\nabla L_\\lambda(W)$$ and the direction $$M + \\lambda W$$ is positive and non-trivial, the gradient field drives the optimization method towards the point $$-\\frac{M}{\\lambda}$$.\n\n**Proof:** Consider the sequence of matrices $$W_1, ..., W_t, ..., W_T$$ generated by applying PSGD on $$L_\\lambda$$ with step size $$\\eta$$ (to be decided) and initial parameter vector $$W_0 \\in W$$. We have that $$L_\\lambda$$ is $$\\gamma$$-quasar convex and is also $$O(\\Gamma)$$-weakly smooth since we now show that it is $$\\Gamma$$-smooth.\n\n**Lemma 6:** $$L_\\lambda$$ is poly($$DS, DI, C$$)-smooth.\n\n**Proof:** We have that\n$$\\| \\nabla L_\\lambda(W) \\|_F^2 = \\| \\mathbb{E}_{z \\sim R}[\\nabla L_{\\lambda,z}(W)] \\|_2^2 \\leq \\mathbb{E}_{z \\sim R} \\| \\nabla_w L_{\\text{vec}}^\\lambda(z^TW) \\|_F^2$$.\n\nIt suffices to show that $$L_{\\text{vec}}^\\lambda$$ is smooth. Recall that\n$$\\nabla_w L_{\\text{vec}}^\\lambda(w) = (1 - \\beta^*) \\mathbb{E}_{x \\sim \\phi(\\cdot;w)}[((c + \\lambda w) \\cdot x)x] - \\mathbb{E}_{x \\sim \\phi(\\cdot;w)}[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[\\cdot]$$.\n\nThis means that\n$$\\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[((c + \\lambda w) \\cdot x)x] - \\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[(c + \\lambda w) \\cdot x] \\leq \\left(1 - \\beta^*\\right)(A1 + A2) + \\beta^*\\rho^*(A3 + A4)$$.\n\nAs mentioned in [HMR16], a function $$f$$ is $$\\Gamma$$-weakly smooth if for any point $$\\theta$$, $$\\| \\nabla f(\\theta) \\|_2 \\leq \\Gamma(f(\\theta) - f(\\theta^*))$$. Moreover, a function $$f$$ that is $$\\Gamma$$-smooth (in the sense $$\\| \\nabla^2 f \\| \\leq \\Gamma$$) is also $$O(\\Gamma)$$-weakly smooth.", "images": [], "items": [{"type": "text", "value": "Thus, we have that\nThis implies the desired result since $$\\text{Var } x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x] \\geq \\Omega(\\alpha \\|c + \\lambda w\\|_2^2)$$.\n$$\\nabla_w L_{\\text{vec}}^\\lambda(w) \\cdot (c + \\lambda w) \\geq \\frac{C_0\\beta^* \\alpha^2}{(BD^3)\\|c + \\lambda w\\|_2^2}$$.", "md": "Thus, we have that\nThis implies the desired result since $$\\text{Var } x \\sim \\phi(\\cdot;\\rho^*w)[(c + \\lambda w) \\cdot x] \\geq \\Omega(\\alpha \\|c + \\lambda w\\|_2^2)$$.\n$$\\nabla_w L_{\\text{vec}}^\\lambda(w) \\cdot (c + \\lambda w) \\geq \\frac{C_0\\beta^* \\alpha^2}{(BD^3)\\|c + \\lambda w\\|_2^2}$$."}, {"type": "heading", "lvl": 3, "value": "Convergence for Quasar Convex Functions", "md": "### Convergence for Quasar Convex Functions"}, {"type": "text", "value": "The fact that $$L_\\lambda$$ is quasar convex with respect to $$-\\frac{M}{\\lambda}$$ implies that projected SGD converges to that point in a small number of steps and hence the family P is efficiently optimizable. The analysis is standard (see e.g., [HMR16]). For completeness, a proof can be found in Appendix E.3.\n\n**Proposition 7 (Convergence):** Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds with parameters $$C, DS, DI, \\alpha$$. Let $$W_1, ..., W_T$$ be the updates of the SGD algorithm with projection set $$W$$ performed on $$L_\\lambda$$ of Equation (2) with appropriate step size and parameter $$\\lambda$$. Then, for the non-regularized objective $$L$$, it holds that\n$$\\mathbb{E}_{t \\sim U([T])}[L(W_t)] \\leq L(-M/\\lambda) + \\epsilon$$ when $$T \\geq \\text{poly}(1/\\epsilon, 1/\\alpha, C, DS, DI, \\|W_0 + M/\\lambda\\|_F)$$.\n\nOur next goal is to use Proposition 1 and show that standard projected SGD on the objective $$L_\\lambda$$ converges in a polynomial number of steps. The intuition behind this result is that since the correlation between $$\\nabla L_\\lambda(W)$$ and the direction $$M + \\lambda W$$ is positive and non-trivial, the gradient field drives the optimization method towards the point $$-\\frac{M}{\\lambda}$$.\n\n**Proof:** Consider the sequence of matrices $$W_1, ..., W_t, ..., W_T$$ generated by applying PSGD on $$L_\\lambda$$ with step size $$\\eta$$ (to be decided) and initial parameter vector $$W_0 \\in W$$. We have that $$L_\\lambda$$ is $$\\gamma$$-quasar convex and is also $$O(\\Gamma)$$-weakly smooth since we now show that it is $$\\Gamma$$-smooth.\n\n**Lemma 6:** $$L_\\lambda$$ is poly($$DS, DI, C$$)-smooth.\n\n**Proof:** We have that\n$$\\| \\nabla L_\\lambda(W) \\|_F^2 = \\| \\mathbb{E}_{z \\sim R}[\\nabla L_{\\lambda,z}(W)] \\|_2^2 \\leq \\mathbb{E}_{z \\sim R} \\| \\nabla_w L_{\\text{vec}}^\\lambda(z^TW) \\|_F^2$$.\n\nIt suffices to show that $$L_{\\text{vec}}^\\lambda$$ is smooth. Recall that\n$$\\nabla_w L_{\\text{vec}}^\\lambda(w) = (1 - \\beta^*) \\mathbb{E}_{x \\sim \\phi(\\cdot;w)}[((c + \\lambda w) \\cdot x)x] - \\mathbb{E}_{x \\sim \\phi(\\cdot;w)}[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[\\cdot]$$.\n\nThis means that\n$$\\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[((c + \\lambda w) \\cdot x)x] - \\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[(c + \\lambda w) \\cdot x] \\leq \\left(1 - \\beta^*\\right)(A1 + A2) + \\beta^*\\rho^*(A3 + A4)$$.\n\nAs mentioned in [HMR16], a function $$f$$ is $$\\Gamma$$-weakly smooth if for any point $$\\theta$$, $$\\| \\nabla f(\\theta) \\|_2 \\leq \\Gamma(f(\\theta) - f(\\theta^*))$$. Moreover, a function $$f$$ that is $$\\Gamma$$-smooth (in the sense $$\\| \\nabla^2 f \\| \\leq \\Gamma$$) is also $$O(\\Gamma)$$-weakly smooth.", "md": "The fact that $$L_\\lambda$$ is quasar convex with respect to $$-\\frac{M}{\\lambda}$$ implies that projected SGD converges to that point in a small number of steps and hence the family P is efficiently optimizable. The analysis is standard (see e.g., [HMR16]). For completeness, a proof can be found in Appendix E.3.\n\n**Proposition 7 (Convergence):** Consider $$\\epsilon > 0$$ and a prior $$R$$ over $$I$$. Assume that Assumption 1 holds with parameters $$C, DS, DI, \\alpha$$. Let $$W_1, ..., W_T$$ be the updates of the SGD algorithm with projection set $$W$$ performed on $$L_\\lambda$$ of Equation (2) with appropriate step size and parameter $$\\lambda$$. Then, for the non-regularized objective $$L$$, it holds that\n$$\\mathbb{E}_{t \\sim U([T])}[L(W_t)] \\leq L(-M/\\lambda) + \\epsilon$$ when $$T \\geq \\text{poly}(1/\\epsilon, 1/\\alpha, C, DS, DI, \\|W_0 + M/\\lambda\\|_F)$$.\n\nOur next goal is to use Proposition 1 and show that standard projected SGD on the objective $$L_\\lambda$$ converges in a polynomial number of steps. The intuition behind this result is that since the correlation between $$\\nabla L_\\lambda(W)$$ and the direction $$M + \\lambda W$$ is positive and non-trivial, the gradient field drives the optimization method towards the point $$-\\frac{M}{\\lambda}$$.\n\n**Proof:** Consider the sequence of matrices $$W_1, ..., W_t, ..., W_T$$ generated by applying PSGD on $$L_\\lambda$$ with step size $$\\eta$$ (to be decided) and initial parameter vector $$W_0 \\in W$$. We have that $$L_\\lambda$$ is $$\\gamma$$-quasar convex and is also $$O(\\Gamma)$$-weakly smooth since we now show that it is $$\\Gamma$$-smooth.\n\n**Lemma 6:** $$L_\\lambda$$ is poly($$DS, DI, C$$)-smooth.\n\n**Proof:** We have that\n$$\\| \\nabla L_\\lambda(W) \\|_F^2 = \\| \\mathbb{E}_{z \\sim R}[\\nabla L_{\\lambda,z}(W)] \\|_2^2 \\leq \\mathbb{E}_{z \\sim R} \\| \\nabla_w L_{\\text{vec}}^\\lambda(z^TW) \\|_F^2$$.\n\nIt suffices to show that $$L_{\\text{vec}}^\\lambda$$ is smooth. Recall that\n$$\\nabla_w L_{\\text{vec}}^\\lambda(w) = (1 - \\beta^*) \\mathbb{E}_{x \\sim \\phi(\\cdot;w)}[((c + \\lambda w) \\cdot x)x] - \\mathbb{E}_{x \\sim \\phi(\\cdot;w)}[(c + \\lambda w) \\cdot x] + \\beta^*\\rho^* \\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[\\cdot]$$.\n\nThis means that\n$$\\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[((c + \\lambda w) \\cdot x)x] - \\mathbb{E}_{x \\sim \\phi(\\cdot;\\rho^*w)}[(c + \\lambda w) \\cdot x] \\leq \\left(1 - \\beta^*\\right)(A1 + A2) + \\beta^*\\rho^*(A3 + A4)$$.\n\nAs mentioned in [HMR16], a function $$f$$ is $$\\Gamma$$-weakly smooth if for any point $$\\theta$$, $$\\| \\nabla f(\\theta) \\|_2 \\leq \\Gamma(f(\\theta) - f(\\theta^*))$$. Moreover, a function $$f$$ that is $$\\Gamma$$-smooth (in the sense $$\\| \\nabla^2 f \\| \\leq \\Gamma$$) is also $$O(\\Gamma)$$-weakly smooth."}]}, {"page": 30, "text": "where\n            A1 =       \u2207w           E                                     2   ,   A2 =       \u2207w           E                                   E             2  ,\n                               x\u223c\u03d5(\u00b7;w)[((c + \u03bbw) \u00b7 x)x]                  F                          x\u223c\u03d5(\u00b7;w)[(c + \u03bbw) \u00b7 x]              x\u223c\u03d5(\u00b7;w)[x]        F\n        A3 =       \u2207w            E                                       2  ,   A4 =        \u2207w            E                                      E              2  .\n                           x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[((c + \u03bbw) \u00b7 x)x]                   F                          x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[(c + \u03bbw) \u00b7 x]              x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[x]         F\nStandard computation of these values yields that, since DS and DI are bounds to x and z respec-\ntively, we have that L\u03bb is smooth with parameter poly(DS, DI, C).\n       Let V be the variance of the unbiased estimator used for \u2207WL\u03bb(W). We can apply the next\nresult of [HMR16].\nLemma 7 ([HMR16]). Suppose the objective function f is \u03b3-weakly quasi convex and \u0393-weakly smooth,\nand let r(\u00b7) be an unbiased estimator for \u2207                          f (\u03b8) with variance V. Moreover, suppose the global minimum \u03b8\nbelongs to W, and the initial point \u03b80 satisfies \u2225\u03b80 \u2212                                 \u03b8\u22252 \u2264      R. Then projected stochastic gradient descent\nwith a proper learning rate returns \u03b8T in T iterations with expected error\n                                                                                                    \u0393R2         \u221a  V\n                                                    E                                               \u03b32T    , R  \u221a           .\n                                                t\u223cU([T]) f (\u03b8t) \u2212          f (\u03b8) \u2264     max                   \u03b3     T\n       We apply the above result to L\u03bb in order to find matrices W1, ..., WT that achieve good loss\non average compared to \u2212M/\u03bb. Moreover, using a batch SGD update, we can take V to be\nalso polynomial in the crucial parameters of the problem. We note that one can adapt the above\nconvergence proof and show that the actual loss L (and not the loss L\u03bb) are close after sufficiently\nmany iterations (as indicated by the above lemma). We know that the Frobenius norm of the\ngradient of L(W) is at most of order O(D2                                  ICD2    S). We can apply the mean value theorem in high\ndimensions (by taking W to be an open ball of radius O(B)) and this yields that the difference\nbetween the values of L(WT) and L(\u2212M/\u03bb) is at most D2                                                       ICD2    S\u2225Wt + M/\u03bb\u22252              F. However, the\nright-hand side is upper bounded by the correlation between \u2207L\u03bb(Wt) and Wt + M/\u03bb. Hence,\nwe can still use this correlation as a potential in order to minimize L. This implies that the desired\nconvergence guarantee holds as long as T \u2265                                      poly(1/\u03f5, 1/\u03b1, C, DS, DI, \u2225W0 + M/\u03bb\u2225F) .\nF       Deferred Proofs: Variance under Almost Uniform Distributions\nThis section is a technical section that states some properties of exponential families. We use\nsome standard notation, such as w and x, for the statements and the proofs but we underline\nthat these symbols do not correspond to the notation in the main body of the paper.\n       We consider the parameter space \u0398 and for any parameter w \u2208                                                        \u0398, we define the probability\ndistribution \u03d5(\u00b7; w) over a space X with density                                           ew\u00b7x\n                                                                   \u03d5(x; w) =          \u2211y\u2208X ew\u00b7y .\nIn this section, our goal is to relate the variance of c \u00b7 x under the measure \u03d5(\u00b7; 0) (uniform case)\nand \u03d5(\u00b7; \u03c1\u22c6w) for some w \u2208                         W and some sufficiently small \u03c1\u22c6                              (almost uniform case). The main\nresult of this section follows.\n                                                                                    30", "md": "where\n\n$$\n\\begin{align*}\nA1 & = \\nabla_{w} E_{2}, & A2 & = \\nabla_{w} E_{E^{2}}, \\\\\n& x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] & & x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] & & x \\sim \\phi(\\cdot;w)[x] \\\\\nA3 & = \\nabla_{w} E_{2}, & A4 & = \\nabla_{w} E_{E^{2}}, \\\\\n& x \\sim \\phi(\\cdot;\\rho^{\\star}w)[((c + \\lambda w) \\cdot x)x] & & x \\sim \\phi(\\cdot;\\rho^{\\star}w)[(c + \\lambda w) \\cdot x] & & x \\sim \\phi(\\cdot;\\rho^{\\star}w)[x]\n\\end{align*}\n$$\nStandard computation of these values yields that, since DS and DI are bounds to x and z respectively, we have that L\u03bb is smooth with parameter poly(DS, DI, C).\n\nLet V be the variance of the unbiased estimator used for $\\nabla WL_{\\lambda}(W)$. We can apply the next result of [HMR16].\n\nLemma 7 ([HMR16]). Suppose the objective function f is $\\gamma$-weakly quasi convex and $\\Gamma$-weakly smooth, and let $r(\\cdot)$ be an unbiased estimator for $\\nabla f(\\theta)$ with variance V. Moreover, suppose the global minimum $\\theta$ belongs to W, and the initial point $\\theta_{0}$ satisfies $\\|\\theta_{0} - \\theta\\|_{2} \\leq R$. Then projected stochastic gradient descent with a proper learning rate returns $\\theta_{T}$ in T iterations with expected error\n\n$$\nE_{t \\sim U([T])}[f(\\theta_{t}) - f(\\theta)] \\leq \\max\\left(\\frac{\\Gamma R^{2}}{\\gamma^{2}T}, R\\sqrt{\\frac{V}{T}}\\right).\n$$\nWe apply the above result to L\u03bb in order to find matrices $W_{1}, ..., W_{T}$ that achieve good loss on average compared to $-M/\\lambda$. Moreover, using a batch SGD update, we can take V to be also polynomial in the crucial parameters of the problem. We note that one can adapt the above convergence proof and show that the actual loss L (and not the loss L\u03bb) are close after sufficiently many iterations (as indicated by the above lemma). We know that the Frobenius norm of the gradient of L(W) is at most of order $O(D^{2}_{I}CD^{2}_{S})$. We can apply the mean value theorem in high dimensions (by taking W to be an open ball of radius $O(B)$) and this yields that the difference between the values of L(W_{T}) and L(-M/\\lambda) is at most $D^{2}_{I}CD^{2}_{S}\\|W_{t} + M/\\lambda\\|_{2}$. However, the right-hand side is upper bounded by the correlation between $\\nabla L_{\\lambda}(W_{t})$ and $W_{t} + M/\\lambda$. Hence, we can still use this correlation as a potential in order to minimize L. This implies that the desired convergence guarantee holds as long as $T \\geq \\text{poly}(1/\\epsilon, 1/\\alpha, C, DS, DI, \\|W_{0} + M/\\lambda\\|_{F})$.\n\nDeferred Proofs: Variance under Almost Uniform Distributions\n\nThis section is a technical section that states some properties of exponential families. We use some standard notation, such as w and x, for the statements and the proofs but we underline that these symbols do not correspond to the notation in the main body of the paper.\n\nWe consider the parameter space $\\Theta$ and for any parameter $w \\in \\Theta$, we define the probability distribution $\\phi(\\cdot; w)$ over a space X with density $\\phi(x; w) = \\frac{e^{w \\cdot x}}{\\sum_{y \\in X} e^{w \\cdot y}}$.\n\nIn this section, our goal is to relate the variance of $c \\cdot x$ under the measure $\\phi(\\cdot; 0)$ (uniform case) and $\\phi(\\cdot; \\rho^{\\star}w)$ for some $w \\in W$ and some sufficiently small $\\rho^{\\star}$ (almost uniform case). The main result of this section follows.\n\n30", "images": [], "items": [{"type": "text", "value": "where\n\n$$\n\\begin{align*}\nA1 & = \\nabla_{w} E_{2}, & A2 & = \\nabla_{w} E_{E^{2}}, \\\\\n& x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] & & x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] & & x \\sim \\phi(\\cdot;w)[x] \\\\\nA3 & = \\nabla_{w} E_{2}, & A4 & = \\nabla_{w} E_{E^{2}}, \\\\\n& x \\sim \\phi(\\cdot;\\rho^{\\star}w)[((c + \\lambda w) \\cdot x)x] & & x \\sim \\phi(\\cdot;\\rho^{\\star}w)[(c + \\lambda w) \\cdot x] & & x \\sim \\phi(\\cdot;\\rho^{\\star}w)[x]\n\\end{align*}\n$$\nStandard computation of these values yields that, since DS and DI are bounds to x and z respectively, we have that L\u03bb is smooth with parameter poly(DS, DI, C).\n\nLet V be the variance of the unbiased estimator used for $\\nabla WL_{\\lambda}(W)$. We can apply the next result of [HMR16].\n\nLemma 7 ([HMR16]). Suppose the objective function f is $\\gamma$-weakly quasi convex and $\\Gamma$-weakly smooth, and let $r(\\cdot)$ be an unbiased estimator for $\\nabla f(\\theta)$ with variance V. Moreover, suppose the global minimum $\\theta$ belongs to W, and the initial point $\\theta_{0}$ satisfies $\\|\\theta_{0} - \\theta\\|_{2} \\leq R$. Then projected stochastic gradient descent with a proper learning rate returns $\\theta_{T}$ in T iterations with expected error\n\n$$\nE_{t \\sim U([T])}[f(\\theta_{t}) - f(\\theta)] \\leq \\max\\left(\\frac{\\Gamma R^{2}}{\\gamma^{2}T}, R\\sqrt{\\frac{V}{T}}\\right).\n$$\nWe apply the above result to L\u03bb in order to find matrices $W_{1}, ..., W_{T}$ that achieve good loss on average compared to $-M/\\lambda$. Moreover, using a batch SGD update, we can take V to be also polynomial in the crucial parameters of the problem. We note that one can adapt the above convergence proof and show that the actual loss L (and not the loss L\u03bb) are close after sufficiently many iterations (as indicated by the above lemma). We know that the Frobenius norm of the gradient of L(W) is at most of order $O(D^{2}_{I}CD^{2}_{S})$. We can apply the mean value theorem in high dimensions (by taking W to be an open ball of radius $O(B)$) and this yields that the difference between the values of L(W_{T}) and L(-M/\\lambda) is at most $D^{2}_{I}CD^{2}_{S}\\|W_{t} + M/\\lambda\\|_{2}$. However, the right-hand side is upper bounded by the correlation between $\\nabla L_{\\lambda}(W_{t})$ and $W_{t} + M/\\lambda$. Hence, we can still use this correlation as a potential in order to minimize L. This implies that the desired convergence guarantee holds as long as $T \\geq \\text{poly}(1/\\epsilon, 1/\\alpha, C, DS, DI, \\|W_{0} + M/\\lambda\\|_{F})$.\n\nDeferred Proofs: Variance under Almost Uniform Distributions\n\nThis section is a technical section that states some properties of exponential families. We use some standard notation, such as w and x, for the statements and the proofs but we underline that these symbols do not correspond to the notation in the main body of the paper.\n\nWe consider the parameter space $\\Theta$ and for any parameter $w \\in \\Theta$, we define the probability distribution $\\phi(\\cdot; w)$ over a space X with density $\\phi(x; w) = \\frac{e^{w \\cdot x}}{\\sum_{y \\in X} e^{w \\cdot y}}$.\n\nIn this section, our goal is to relate the variance of $c \\cdot x$ under the measure $\\phi(\\cdot; 0)$ (uniform case) and $\\phi(\\cdot; \\rho^{\\star}w)$ for some $w \\in W$ and some sufficiently small $\\rho^{\\star}$ (almost uniform case). The main result of this section follows.\n\n30", "md": "where\n\n$$\n\\begin{align*}\nA1 & = \\nabla_{w} E_{2}, & A2 & = \\nabla_{w} E_{E^{2}}, \\\\\n& x \\sim \\phi(\\cdot;w)[((c + \\lambda w) \\cdot x)x] & & x \\sim \\phi(\\cdot;w)[(c + \\lambda w) \\cdot x] & & x \\sim \\phi(\\cdot;w)[x] \\\\\nA3 & = \\nabla_{w} E_{2}, & A4 & = \\nabla_{w} E_{E^{2}}, \\\\\n& x \\sim \\phi(\\cdot;\\rho^{\\star}w)[((c + \\lambda w) \\cdot x)x] & & x \\sim \\phi(\\cdot;\\rho^{\\star}w)[(c + \\lambda w) \\cdot x] & & x \\sim \\phi(\\cdot;\\rho^{\\star}w)[x]\n\\end{align*}\n$$\nStandard computation of these values yields that, since DS and DI are bounds to x and z respectively, we have that L\u03bb is smooth with parameter poly(DS, DI, C).\n\nLet V be the variance of the unbiased estimator used for $\\nabla WL_{\\lambda}(W)$. We can apply the next result of [HMR16].\n\nLemma 7 ([HMR16]). Suppose the objective function f is $\\gamma$-weakly quasi convex and $\\Gamma$-weakly smooth, and let $r(\\cdot)$ be an unbiased estimator for $\\nabla f(\\theta)$ with variance V. Moreover, suppose the global minimum $\\theta$ belongs to W, and the initial point $\\theta_{0}$ satisfies $\\|\\theta_{0} - \\theta\\|_{2} \\leq R$. Then projected stochastic gradient descent with a proper learning rate returns $\\theta_{T}$ in T iterations with expected error\n\n$$\nE_{t \\sim U([T])}[f(\\theta_{t}) - f(\\theta)] \\leq \\max\\left(\\frac{\\Gamma R^{2}}{\\gamma^{2}T}, R\\sqrt{\\frac{V}{T}}\\right).\n$$\nWe apply the above result to L\u03bb in order to find matrices $W_{1}, ..., W_{T}$ that achieve good loss on average compared to $-M/\\lambda$. Moreover, using a batch SGD update, we can take V to be also polynomial in the crucial parameters of the problem. We note that one can adapt the above convergence proof and show that the actual loss L (and not the loss L\u03bb) are close after sufficiently many iterations (as indicated by the above lemma). We know that the Frobenius norm of the gradient of L(W) is at most of order $O(D^{2}_{I}CD^{2}_{S})$. We can apply the mean value theorem in high dimensions (by taking W to be an open ball of radius $O(B)$) and this yields that the difference between the values of L(W_{T}) and L(-M/\\lambda) is at most $D^{2}_{I}CD^{2}_{S}\\|W_{t} + M/\\lambda\\|_{2}$. However, the right-hand side is upper bounded by the correlation between $\\nabla L_{\\lambda}(W_{t})$ and $W_{t} + M/\\lambda$. Hence, we can still use this correlation as a potential in order to minimize L. This implies that the desired convergence guarantee holds as long as $T \\geq \\text{poly}(1/\\epsilon, 1/\\alpha, C, DS, DI, \\|W_{0} + M/\\lambda\\|_{F})$.\n\nDeferred Proofs: Variance under Almost Uniform Distributions\n\nThis section is a technical section that states some properties of exponential families. We use some standard notation, such as w and x, for the statements and the proofs but we underline that these symbols do not correspond to the notation in the main body of the paper.\n\nWe consider the parameter space $\\Theta$ and for any parameter $w \\in \\Theta$, we define the probability distribution $\\phi(\\cdot; w)$ over a space X with density $\\phi(x; w) = \\frac{e^{w \\cdot x}}{\\sum_{y \\in X} e^{w \\cdot y}}$.\n\nIn this section, our goal is to relate the variance of $c \\cdot x$ under the measure $\\phi(\\cdot; 0)$ (uniform case) and $\\phi(\\cdot; \\rho^{\\star}w)$ for some $w \\in W$ and some sufficiently small $\\rho^{\\star}$ (almost uniform case). The main result of this section follows.\n\n30"}]}, {"page": 31, "text": "Proposition 8 (Variance Lower Bound Under Almost Uniform Distributions). Assume that the\nvariance of c \u00b7 x under the uniform distribution over X , whose diameter is D, is lower bounded by\n\u03b1\u2225c\u22252    2. Moreover assume that w \u2208                           \u0398 with \u2225w\u22252 \u2264                 B. Then, setting \u03c1\u22c6                 = O(\u03b1/(BD3)), it holds\nthat Var      x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[c \u00b7 x] = \u2126(\u03b1\u2225c\u22252                 2).\n       We first provide a general abstract lemma that relates the variance of the uniform distribution\nU over X to the variance of an almost uniform probability measure \u00b5. For simplicity, we denote\nthe uniform distribution over X with U = U(X ).\nLemma 8. Let w \u2208                  \u0398 and x \u2208           X with \u2225x\u22252 \u2264              D. Consider the uniform probability measure U over X\nand let \u00b5 over X be such that there exist \u03f51, \u03f52 > 0 with:\n      \u2022    Ex\u223cU[x] \u2212           Ex\u223c\u00b5[x]         2 \u2264    \u03f51, and,\n      \u2022 w\u22a4       Ex\u223c\u00b5[xx\u22a4]w \u2265              w\u22a4    Ex\u223cU[xx\u22a4]w \u2212               \u03f52\u2225w\u22252     2.\nThen it holds that Varx\u223c\u00b5[w \u00b7 x] \u2265                         Varx\u223cU[w \u00b7 x] \u2212             3 max{\u03f52      1, \u03f51D, \u03f52}\u2225w\u22252          2.\nProof. We have that                          Varx\u223c\u00b5[w \u00b7 x] = E                   (w \u00b7 x)2        \u2212        E              2    .\n                                                                         x\u223c\u00b5                            x\u223c\u00b5[w \u00b7 x]\nWe first deal with upper-bounding the square of the first moment. Note that\n                                 w \u00b7       E                             \u2264   \u2225w\u22252          E                             \u2264   \u03f51\u2225w\u22252 .\n                                          x\u223c\u00b5[x] \u2212         E                             x\u223c\u00b5[x] \u2212         E\n                                                         x\u223cU[x]                                         x\u223cU[x]        2\nLet us take \u03f5 > 0 (with \u03f5 < \u03f51) for simplicity to be such that                                             Ex\u223c\u00b5[w \u00b7 x]         2 = (Ex\u223cU[w \u00b7 x] + \u03f5\u2225w\u22252)2.\nThis means that\n                                    E              2    \u2264         E               2   + 2\u03f5\u2225w\u22252             E                + \u03f52\u2225w\u22252        2\n                                  x\u223c\u00b5[w \u00b7 x]                    x\u223cU[w \u00b7 x]                               x\u223cU[w \u00b7 x]\n                                                        \u2264         E               2   + 2\u03f5D\u2225w\u22252          2 + \u03f52\u2225w\u22252       2 .\nNext we lower-bound the second moment. It holds that            x\u223cU[w \u00b7 x]\n                                    E     (w \u00b7 x)2         = w\u22a4        E      xx\u22a4       w \u2265       E                                   2 ,\nfor some \u03f52 > 0. This means that   x\u223c\u00b5                                x\u223c\u00b5                       x\u223cU[(w \u00b7 x)2] \u2212            \u03f52\u2225w\u22252\n                Varx\u223c\u00b5(w \u00b7 x) \u2265                E     (w \u00b7 x)2         \u2212   \u03f52\u2225w\u22252     2 \u2212        E               2    \u2212   2\u03f5D\u2225w\u22252       2 \u2212    \u03f52\u2225w\u22252     2 .\nHence,                                       x\u223cU                                              x\u223cU[w \u00b7 x]\n                                    Varx\u223c\u00b5[w \u00b7 x] \u2265              Varx\u223cU[w \u00b7 x] \u2212             3 max{\u03f52, \u03f52        1, \u03f51D}\u2225w\u22252        2 .\n       Our next goal is to relate \u03d5(\u00b7; \u03c1\u22c6w) with the uniform measure \u03d5(\u00b7; 0). According to the above\ngeneral lemma, we have to relate the first and second moments of \u03d5(\u00b7; \u03c1\u22c6w) with the ones of the\nuniform distribution U = \u03d5(\u00b7; 0).                                                   31", "md": "Proposition 8 (Variance Lower Bound Under Almost Uniform Distributions). Assume that the\nvariance of $$c \\cdot x$$ under the uniform distribution over X, whose diameter is D, is lower bounded by\n$$\\alpha \\|c\\|_2^2$$. Moreover assume that $$w \\in \\Theta$$ with $$\\|w\\|_2 \\leq B$$. Then, setting $$\\rho^* = O\\left(\\frac{\\alpha}{BD^3}\\right)$$, it holds\nthat $$\\text{Var}_{x \\sim \\phi(\\cdot;\\rho^*w)}[c \\cdot x] = \\Omega(\\alpha \\|c\\|_2^2)$$.\n\nWe first provide a general abstract lemma that relates the variance of the uniform distribution\nU over X to the variance of an almost uniform probability measure $$\\mu$$. For simplicity, we denote\nthe uniform distribution over X with $$U = U(X)$$.\nLemma 8. Let $$w \\in \\Theta$$ and $$x \\in X$$ with $$\\|x\\|_2 \\leq D$$. Consider the uniform probability measure U over X\nand let $$\\mu$$ over X be such that there exist $$\\epsilon_1, \\epsilon_2 > 0$$ with:\n\n- $$\\mathbb{E}_{x \\sim U}[x] - \\mathbb{E}_{x \\sim \\mu}[x]^2 \\leq \\epsilon_1$$, and,\n\n- $$w^\\top \\mathbb{E}_{x \\sim \\mu}[xx^\\top]w \\geq w^\\top \\mathbb{E}_{x \\sim U}[xx^\\top]w - \\epsilon_2\\|w\\|_2^2$$.\n\nThen it holds that $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] \\geq \\text{Var}_{x \\sim U}[w \\cdot x] - 3 \\max\\{\\epsilon_2, \\epsilon_1D, \\epsilon_2\\}\\|w\\|_2^2$$.\nProof. We have that $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] = \\mathbb{E}[(w \\cdot x)^2] - \\frac{(\\mathbb{E}[x])^2}{\\mathbb{E}[w \\cdot x]}$$.\n\nWe first deal with upper-bounding the square of the first moment. Note that\n\n$$w \\cdot \\mathbb{E} \\leq \\|w\\|_2 \\mathbb{E} \\leq \\epsilon_1\\|w\\|_2$$.\n\nLet us take $$\\epsilon > 0$$ (with $$\\epsilon < \\epsilon_1$$) for simplicity to be such that $$\\mathbb{E}_{x \\sim \\mu}[w \\cdot x]^2 = (\\mathbb{E}_{x \\sim U}[w \\cdot x] + \\epsilon\\|w\\|_2)^2$$.\n\nThis means that\n\n$$\\mathbb{E}^2 \\leq \\mathbb{E}^2 + 2\\epsilon\\|w\\|_2 \\mathbb{E} + \\epsilon^2\\|w\\|_2^2$$\n\n$$\\leq \\mathbb{E}^2 + 2\\epsilon D\\|w\\|_2^2 + \\epsilon^2\\|w\\|_2^2$$.\n\nNext we lower-bound the second moment. It holds that $$\\mathbb{E}[(w \\cdot x)^2] = w^\\top \\mathbb{E}[xx^\\top]w \\geq \\mathbb{E}[x \\cdot x]$$, for some $$\\epsilon_2 > 0$$. This means that $$\\text{Var}_{x \\sim \\mu}(w \\cdot x) \\geq \\mathbb{E}[(w \\cdot x)^2] - \\epsilon_2\\|w\\|_2^2 - \\mathbb{E}^2 - 2\\epsilon D\\|w\\|_2^2 - \\epsilon_2\\|w\\|_2^2$$.\n\nHence, $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] \\geq \\text{Var}_{x \\sim U}[w \\cdot x] - 3 \\max\\{\\epsilon_2, \\epsilon_2, \\epsilon_1D\\}\\|w\\|_2^2$$.\n\nOur next goal is to relate $$\\phi(\\cdot; \\rho^*w)$$ with the uniform measure $$\\phi(\\cdot; 0)$$. According to the above\ngeneral lemma, we have to relate the first and second moments of $$\\phi(\\cdot; \\rho^*w)$$ with the ones of the\nuniform distribution $$U = \\phi(\\cdot; 0)$$. 31", "images": [], "items": [{"type": "text", "value": "Proposition 8 (Variance Lower Bound Under Almost Uniform Distributions). Assume that the\nvariance of $$c \\cdot x$$ under the uniform distribution over X, whose diameter is D, is lower bounded by\n$$\\alpha \\|c\\|_2^2$$. Moreover assume that $$w \\in \\Theta$$ with $$\\|w\\|_2 \\leq B$$. Then, setting $$\\rho^* = O\\left(\\frac{\\alpha}{BD^3}\\right)$$, it holds\nthat $$\\text{Var}_{x \\sim \\phi(\\cdot;\\rho^*w)}[c \\cdot x] = \\Omega(\\alpha \\|c\\|_2^2)$$.\n\nWe first provide a general abstract lemma that relates the variance of the uniform distribution\nU over X to the variance of an almost uniform probability measure $$\\mu$$. For simplicity, we denote\nthe uniform distribution over X with $$U = U(X)$$.\nLemma 8. Let $$w \\in \\Theta$$ and $$x \\in X$$ with $$\\|x\\|_2 \\leq D$$. Consider the uniform probability measure U over X\nand let $$\\mu$$ over X be such that there exist $$\\epsilon_1, \\epsilon_2 > 0$$ with:\n\n- $$\\mathbb{E}_{x \\sim U}[x] - \\mathbb{E}_{x \\sim \\mu}[x]^2 \\leq \\epsilon_1$$, and,\n\n- $$w^\\top \\mathbb{E}_{x \\sim \\mu}[xx^\\top]w \\geq w^\\top \\mathbb{E}_{x \\sim U}[xx^\\top]w - \\epsilon_2\\|w\\|_2^2$$.\n\nThen it holds that $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] \\geq \\text{Var}_{x \\sim U}[w \\cdot x] - 3 \\max\\{\\epsilon_2, \\epsilon_1D, \\epsilon_2\\}\\|w\\|_2^2$$.\nProof. We have that $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] = \\mathbb{E}[(w \\cdot x)^2] - \\frac{(\\mathbb{E}[x])^2}{\\mathbb{E}[w \\cdot x]}$$.\n\nWe first deal with upper-bounding the square of the first moment. Note that\n\n$$w \\cdot \\mathbb{E} \\leq \\|w\\|_2 \\mathbb{E} \\leq \\epsilon_1\\|w\\|_2$$.\n\nLet us take $$\\epsilon > 0$$ (with $$\\epsilon < \\epsilon_1$$) for simplicity to be such that $$\\mathbb{E}_{x \\sim \\mu}[w \\cdot x]^2 = (\\mathbb{E}_{x \\sim U}[w \\cdot x] + \\epsilon\\|w\\|_2)^2$$.\n\nThis means that\n\n$$\\mathbb{E}^2 \\leq \\mathbb{E}^2 + 2\\epsilon\\|w\\|_2 \\mathbb{E} + \\epsilon^2\\|w\\|_2^2$$\n\n$$\\leq \\mathbb{E}^2 + 2\\epsilon D\\|w\\|_2^2 + \\epsilon^2\\|w\\|_2^2$$.\n\nNext we lower-bound the second moment. It holds that $$\\mathbb{E}[(w \\cdot x)^2] = w^\\top \\mathbb{E}[xx^\\top]w \\geq \\mathbb{E}[x \\cdot x]$$, for some $$\\epsilon_2 > 0$$. This means that $$\\text{Var}_{x \\sim \\mu}(w \\cdot x) \\geq \\mathbb{E}[(w \\cdot x)^2] - \\epsilon_2\\|w\\|_2^2 - \\mathbb{E}^2 - 2\\epsilon D\\|w\\|_2^2 - \\epsilon_2\\|w\\|_2^2$$.\n\nHence, $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] \\geq \\text{Var}_{x \\sim U}[w \\cdot x] - 3 \\max\\{\\epsilon_2, \\epsilon_2, \\epsilon_1D\\}\\|w\\|_2^2$$.\n\nOur next goal is to relate $$\\phi(\\cdot; \\rho^*w)$$ with the uniform measure $$\\phi(\\cdot; 0)$$. According to the above\ngeneral lemma, we have to relate the first and second moments of $$\\phi(\\cdot; \\rho^*w)$$ with the ones of the\nuniform distribution $$U = \\phi(\\cdot; 0)$$. 31", "md": "Proposition 8 (Variance Lower Bound Under Almost Uniform Distributions). Assume that the\nvariance of $$c \\cdot x$$ under the uniform distribution over X, whose diameter is D, is lower bounded by\n$$\\alpha \\|c\\|_2^2$$. Moreover assume that $$w \\in \\Theta$$ with $$\\|w\\|_2 \\leq B$$. Then, setting $$\\rho^* = O\\left(\\frac{\\alpha}{BD^3}\\right)$$, it holds\nthat $$\\text{Var}_{x \\sim \\phi(\\cdot;\\rho^*w)}[c \\cdot x] = \\Omega(\\alpha \\|c\\|_2^2)$$.\n\nWe first provide a general abstract lemma that relates the variance of the uniform distribution\nU over X to the variance of an almost uniform probability measure $$\\mu$$. For simplicity, we denote\nthe uniform distribution over X with $$U = U(X)$$.\nLemma 8. Let $$w \\in \\Theta$$ and $$x \\in X$$ with $$\\|x\\|_2 \\leq D$$. Consider the uniform probability measure U over X\nand let $$\\mu$$ over X be such that there exist $$\\epsilon_1, \\epsilon_2 > 0$$ with:\n\n- $$\\mathbb{E}_{x \\sim U}[x] - \\mathbb{E}_{x \\sim \\mu}[x]^2 \\leq \\epsilon_1$$, and,\n\n- $$w^\\top \\mathbb{E}_{x \\sim \\mu}[xx^\\top]w \\geq w^\\top \\mathbb{E}_{x \\sim U}[xx^\\top]w - \\epsilon_2\\|w\\|_2^2$$.\n\nThen it holds that $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] \\geq \\text{Var}_{x \\sim U}[w \\cdot x] - 3 \\max\\{\\epsilon_2, \\epsilon_1D, \\epsilon_2\\}\\|w\\|_2^2$$.\nProof. We have that $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] = \\mathbb{E}[(w \\cdot x)^2] - \\frac{(\\mathbb{E}[x])^2}{\\mathbb{E}[w \\cdot x]}$$.\n\nWe first deal with upper-bounding the square of the first moment. Note that\n\n$$w \\cdot \\mathbb{E} \\leq \\|w\\|_2 \\mathbb{E} \\leq \\epsilon_1\\|w\\|_2$$.\n\nLet us take $$\\epsilon > 0$$ (with $$\\epsilon < \\epsilon_1$$) for simplicity to be such that $$\\mathbb{E}_{x \\sim \\mu}[w \\cdot x]^2 = (\\mathbb{E}_{x \\sim U}[w \\cdot x] + \\epsilon\\|w\\|_2)^2$$.\n\nThis means that\n\n$$\\mathbb{E}^2 \\leq \\mathbb{E}^2 + 2\\epsilon\\|w\\|_2 \\mathbb{E} + \\epsilon^2\\|w\\|_2^2$$\n\n$$\\leq \\mathbb{E}^2 + 2\\epsilon D\\|w\\|_2^2 + \\epsilon^2\\|w\\|_2^2$$.\n\nNext we lower-bound the second moment. It holds that $$\\mathbb{E}[(w \\cdot x)^2] = w^\\top \\mathbb{E}[xx^\\top]w \\geq \\mathbb{E}[x \\cdot x]$$, for some $$\\epsilon_2 > 0$$. This means that $$\\text{Var}_{x \\sim \\mu}(w \\cdot x) \\geq \\mathbb{E}[(w \\cdot x)^2] - \\epsilon_2\\|w\\|_2^2 - \\mathbb{E}^2 - 2\\epsilon D\\|w\\|_2^2 - \\epsilon_2\\|w\\|_2^2$$.\n\nHence, $$\\text{Var}_{x \\sim \\mu}[w \\cdot x] \\geq \\text{Var}_{x \\sim U}[w \\cdot x] - 3 \\max\\{\\epsilon_2, \\epsilon_2, \\epsilon_1D\\}\\|w\\|_2^2$$.\n\nOur next goal is to relate $$\\phi(\\cdot; \\rho^*w)$$ with the uniform measure $$\\phi(\\cdot; 0)$$. According to the above\ngeneral lemma, we have to relate the first and second moments of $$\\phi(\\cdot; \\rho^*w)$$ with the ones of the\nuniform distribution $$U = \\phi(\\cdot; 0)$$. 31"}]}, {"page": 32, "text": "The Proof of Proposition 8. Our goal is to apply Lemma 8. First, let us set\n                                                                            fv(\u03c1) =                E\nfor any unit vector v \u2208                         \u0398. Then it holds that                        x\u223c\u03d5(\u00b7;\u03c1w)[v \u00b7 x] ,\n                                                    E                         E                     =       sup         | fv(0) \u2212         fv(\u03c1\u22c6)| .\n                                               x\u223c\u03d5(\u00b7;0)[x] \u2212           x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[x]            2        v:\u2225v\u22252=1\nUsing the mean value theorem in [0, \u03c1\u22c6] for any unit vector v, we have that there exists a \u03be = \u03bev \u2208\n(0, \u03c1\u22c6) such that\n                                                                     | fv(0) \u2212         fv(\u03c1\u22c6)| = \u03c1\u22c6              | f \u2032\nIt suffices to upper bound f \u2032                          v(\u03be) for any unit vector v and \u03be \u2208                          v(\u03be)| .  (0, \u03c1\u22c6). Let us compute f \u2032                       v. We have\nthat\n           d fv                                     e\u03c1(wx)                            E                                                 E                          E\n           d\u03c1 =           S(v \u00b7 x) d     d\u03c1      S e\u03c1(wy)dy          dx =       x\u223c\u03d5(\u00b7;\u03c1w)[(v \u00b7 x)(w \u00b7 x)] \u2212                       x\u223c\u03d5(\u00b7;\u03c1w)[v \u00b7 x]           x\u223c\u03d5(\u00b7;\u03c1w)[w \u00b7 x] .\nSince x \u2208           X , we have that                               sup            sup        | f \u2032\n                                                                v:\u2225v\u22252=1       \u03be\u2208(0,\u03c1\u22c6)         v(\u03be)| \u2264         2\u2225w\u22252D2 .\nThis gives that                                                 E                         E                    \u2264    2\u03c1\u22c6\u2225w\u22252D2 .\n                                                          x\u223c\u03d5(\u00b7;0)[x] \u2212            x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[x]            2\nWe then continue with controlling the second moment: it suffices to find \u03f52 such that for any\nv \u2208     \u0398, it holds                               v\u22a4           E                                         E                                     2 .\n                                                        x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[xx\u22a4]v \u2265                   v\u22a4    x\u223c\u03d5(\u00b7;0)[xx\u22a4]v \u2212                 \u03f52\u2225v\u22252\nLet us set gv(\u03c1) = E                     x\u223c\u03d5(\u00b7;\u03c1w)[(v \u00b7 x)2] for any vector v \u2208                                  \u0398. We have that\nwhere \u03be \u2208             (0, \u03c1\u22c6). It holds that                         |gv(0) \u2212          gv(\u03c1\u22c6)| = \u03c1\u22c6|g\u2032              v(\u03be)| ,\n                dgv       =            E                                                   E                                E                       \u2264     2\u2225v\u22252\u2225w\u22252D3 .\n                 d\u03c1              x\u223c\u03d5(\u00b7;\u03c1w)[(v \u00b7 x)2(w \u00b7 x)] \u2212                       x\u223c\u03d5(\u00b7;\u03c1w)[(v \u00b7 x)2]               x\u223c\u03d5(\u00b7;\u03c1w)[w \u00b7 x]\nThis gives that for any v \u2208              v\u22a4           E\u0398, it holds                             E                                                        2 .\n                                               x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[xx\u22a4]v \u2265                   v\u22a4    x\u223c\u03d5(\u00b7;0)[xx\u22a4]v \u2212                2\u03c1\u22c6\u2225w\u22252D3\u2225v\u22252\nNote that the above holds for v = c too. Lemma 8 gives us that\n                                   Var     x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[c \u00b7 x] \u2265               Varx\u223c\u03d5(\u00b7;0)[c \u00b7 x] \u2212                 3 max{\u03f52, \u03f52          1, \u03f51D}\u2225c\u22252           2 ,\nwhere \u03f51 = 2\u03c1\u22c6BD2 and \u03f52 = 2\u03c1\u22c6BD3. This implies that by picking                       \u03c1\u22c6   = C0       BD3\u03b1\nfor some universal constant C0, we get that\n                                                                   Var    x\u223c\u03d5(\u00b7;\u03c1\u22c6w)[c \u00b7 x] = \u2126(\u03b1\u2225c\u22252                       2) .\n       In this section, we considered \u03c1\u22c6                                   as indicated by the above Proposition 8.\n                                                                                                32", "md": "# Math Equations\n\n## The Proof of Proposition 8\n\nOur goal is to apply Lemma 8. First, let us set\n\n$$f_v(\\rho) = E$$\n\nfor any unit vector \\(v \\in \\Theta\\). Then it holds that\n\n$$E\\left[\\frac{x \\sim \\phi(\\cdot;0)[v \\cdot x]}{E[x \\sim \\phi(\\cdot;\\rho^*w)[x]] - E[x \\sim \\phi(\\cdot;0)[x]]}\\right] = \\sup_{\\|v\\|_2=1} |f_v(0) - f_v(\\rho^*)|.$$\n\nUsing the mean value theorem in \\([0, \\rho^*]\\) for any unit vector \\(v\\), we have that there exists a \\(\\xi = \\xi_v \\in (0, \\rho^*)\\) such that\n\n$$|f_v(0) - f_v(\\rho^*)| = \\rho^* |f'(v, \\xi)|.$$\n\nIt suffices to upper bound \\(f'(v, \\xi)\\) for any unit vector \\(v\\) and \\(\\xi \\in (0, \\rho^*)\\). Let us compute \\(f'_v\\). We have that\n\n$$\\frac{d f_v}{d \\rho} = E\\left[S(v \\cdot x) \\frac{d}{d \\rho} S e^{\\rho(wx)}\\right] = E\\left[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)(w \\cdot x)] - E[x \\sim \\phi(\\cdot;\\rho w)[v \\cdot x] E[x \\sim \\phi(\\cdot;\\rho w)[w \\cdot x]]\\right].$$\n\nSince \\(x \\in X\\), we have that\n\n$$\\sup_{\\|v\\|_2=1} \\sup_{\\xi \\in (0,\\rho^*)} |f'(v, \\xi)| \\leq 2\\|w\\|_2 D^2.$$\n\nThis gives that\n\n$$\\frac{E\\left[E[x \\sim \\phi(\\cdot;0)[x] - E[x \\sim \\phi(\\cdot;\\rho^*w)[x]]^2\\right]}{2} \\leq 2\\rho^* \\|w\\|_2 D^2.$$\n\nWe then continue with controlling the second moment: it suffices to find \\(\\epsilon^2\\) such that for any \\(v \\in \\Theta\\), it holds\n\n$$v^T E[x \\sim \\phi(\\cdot;\\rho^*w)[xx^T]v \\geq v^T E[x \\sim \\phi(\\cdot;0)[xx^T]v - \\epsilon^2 \\|v\\|_2^2.$$\n\nLet us set \\(g_v(\\rho) = E[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2]\\) for any vector \\(v \\in \\Theta\\). We have that\n\nwhere \\(\\xi \\in (0, \\rho^*)\\). It holds that\n\n$$|g_v(0) - g_v(\\rho^*)| = \\rho^*|g'(v, \\xi)|,$$\n\n$$\\frac{d g_v}{d \\rho} = E\\left[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2(w \\cdot x)] - E[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2] E[x \\sim \\phi(\\cdot;\\rho w)[w \\cdot x]]\\right].$$\n\nThis gives that for any \\(v \\in \\Theta\\), it holds\n\n$$v^T E[xx^T]v \\geq v^T E[xx^T]v - 2\\rho^* \\|w\\|_2 D^3 \\|v\\|_2.$$\n\nNote that the above holds for \\(v = c\\) too. Lemma 8 gives us that\n\n$$\\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[c \\cdot x]] \\geq \\text{Var}[x \\sim \\phi(\\cdot;0)[c \\cdot x]] - 3 \\max\\{\\epsilon^2, \\epsilon^2_1, \\epsilon^1 D\\} \\|c\\|_2^2,$$\n\nwhere \\(\\epsilon^1 = 2\\rho^*BD^2\\) and \\(\\epsilon^2 = 2\\rho^*BD^3\\). This implies that by picking \\(\\rho^* = C_0 BD^3\\alpha\\) for some universal constant \\(C_0\\), we get that\n\n$$\\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[c \\cdot x]] = \\Omega(\\alpha \\|c\\|_2^2).$$\n\nIn this section, we considered \\(\\rho^*\\) as indicated by the above Proposition 8.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "heading", "lvl": 2, "value": "The Proof of Proposition 8", "md": "## The Proof of Proposition 8"}, {"type": "text", "value": "Our goal is to apply Lemma 8. First, let us set\n\n$$f_v(\\rho) = E$$\n\nfor any unit vector \\(v \\in \\Theta\\). Then it holds that\n\n$$E\\left[\\frac{x \\sim \\phi(\\cdot;0)[v \\cdot x]}{E[x \\sim \\phi(\\cdot;\\rho^*w)[x]] - E[x \\sim \\phi(\\cdot;0)[x]]}\\right] = \\sup_{\\|v\\|_2=1} |f_v(0) - f_v(\\rho^*)|.$$\n\nUsing the mean value theorem in \\([0, \\rho^*]\\) for any unit vector \\(v\\), we have that there exists a \\(\\xi = \\xi_v \\in (0, \\rho^*)\\) such that\n\n$$|f_v(0) - f_v(\\rho^*)| = \\rho^* |f'(v, \\xi)|.$$\n\nIt suffices to upper bound \\(f'(v, \\xi)\\) for any unit vector \\(v\\) and \\(\\xi \\in (0, \\rho^*)\\). Let us compute \\(f'_v\\). We have that\n\n$$\\frac{d f_v}{d \\rho} = E\\left[S(v \\cdot x) \\frac{d}{d \\rho} S e^{\\rho(wx)}\\right] = E\\left[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)(w \\cdot x)] - E[x \\sim \\phi(\\cdot;\\rho w)[v \\cdot x] E[x \\sim \\phi(\\cdot;\\rho w)[w \\cdot x]]\\right].$$\n\nSince \\(x \\in X\\), we have that\n\n$$\\sup_{\\|v\\|_2=1} \\sup_{\\xi \\in (0,\\rho^*)} |f'(v, \\xi)| \\leq 2\\|w\\|_2 D^2.$$\n\nThis gives that\n\n$$\\frac{E\\left[E[x \\sim \\phi(\\cdot;0)[x] - E[x \\sim \\phi(\\cdot;\\rho^*w)[x]]^2\\right]}{2} \\leq 2\\rho^* \\|w\\|_2 D^2.$$\n\nWe then continue with controlling the second moment: it suffices to find \\(\\epsilon^2\\) such that for any \\(v \\in \\Theta\\), it holds\n\n$$v^T E[x \\sim \\phi(\\cdot;\\rho^*w)[xx^T]v \\geq v^T E[x \\sim \\phi(\\cdot;0)[xx^T]v - \\epsilon^2 \\|v\\|_2^2.$$\n\nLet us set \\(g_v(\\rho) = E[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2]\\) for any vector \\(v \\in \\Theta\\). We have that\n\nwhere \\(\\xi \\in (0, \\rho^*)\\). It holds that\n\n$$|g_v(0) - g_v(\\rho^*)| = \\rho^*|g'(v, \\xi)|,$$\n\n$$\\frac{d g_v}{d \\rho} = E\\left[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2(w \\cdot x)] - E[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2] E[x \\sim \\phi(\\cdot;\\rho w)[w \\cdot x]]\\right].$$\n\nThis gives that for any \\(v \\in \\Theta\\), it holds\n\n$$v^T E[xx^T]v \\geq v^T E[xx^T]v - 2\\rho^* \\|w\\|_2 D^3 \\|v\\|_2.$$\n\nNote that the above holds for \\(v = c\\) too. Lemma 8 gives us that\n\n$$\\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[c \\cdot x]] \\geq \\text{Var}[x \\sim \\phi(\\cdot;0)[c \\cdot x]] - 3 \\max\\{\\epsilon^2, \\epsilon^2_1, \\epsilon^1 D\\} \\|c\\|_2^2,$$\n\nwhere \\(\\epsilon^1 = 2\\rho^*BD^2\\) and \\(\\epsilon^2 = 2\\rho^*BD^3\\). This implies that by picking \\(\\rho^* = C_0 BD^3\\alpha\\) for some universal constant \\(C_0\\), we get that\n\n$$\\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[c \\cdot x]] = \\Omega(\\alpha \\|c\\|_2^2).$$\n\nIn this section, we considered \\(\\rho^*\\) as indicated by the above Proposition 8.", "md": "Our goal is to apply Lemma 8. First, let us set\n\n$$f_v(\\rho) = E$$\n\nfor any unit vector \\(v \\in \\Theta\\). Then it holds that\n\n$$E\\left[\\frac{x \\sim \\phi(\\cdot;0)[v \\cdot x]}{E[x \\sim \\phi(\\cdot;\\rho^*w)[x]] - E[x \\sim \\phi(\\cdot;0)[x]]}\\right] = \\sup_{\\|v\\|_2=1} |f_v(0) - f_v(\\rho^*)|.$$\n\nUsing the mean value theorem in \\([0, \\rho^*]\\) for any unit vector \\(v\\), we have that there exists a \\(\\xi = \\xi_v \\in (0, \\rho^*)\\) such that\n\n$$|f_v(0) - f_v(\\rho^*)| = \\rho^* |f'(v, \\xi)|.$$\n\nIt suffices to upper bound \\(f'(v, \\xi)\\) for any unit vector \\(v\\) and \\(\\xi \\in (0, \\rho^*)\\). Let us compute \\(f'_v\\). We have that\n\n$$\\frac{d f_v}{d \\rho} = E\\left[S(v \\cdot x) \\frac{d}{d \\rho} S e^{\\rho(wx)}\\right] = E\\left[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)(w \\cdot x)] - E[x \\sim \\phi(\\cdot;\\rho w)[v \\cdot x] E[x \\sim \\phi(\\cdot;\\rho w)[w \\cdot x]]\\right].$$\n\nSince \\(x \\in X\\), we have that\n\n$$\\sup_{\\|v\\|_2=1} \\sup_{\\xi \\in (0,\\rho^*)} |f'(v, \\xi)| \\leq 2\\|w\\|_2 D^2.$$\n\nThis gives that\n\n$$\\frac{E\\left[E[x \\sim \\phi(\\cdot;0)[x] - E[x \\sim \\phi(\\cdot;\\rho^*w)[x]]^2\\right]}{2} \\leq 2\\rho^* \\|w\\|_2 D^2.$$\n\nWe then continue with controlling the second moment: it suffices to find \\(\\epsilon^2\\) such that for any \\(v \\in \\Theta\\), it holds\n\n$$v^T E[x \\sim \\phi(\\cdot;\\rho^*w)[xx^T]v \\geq v^T E[x \\sim \\phi(\\cdot;0)[xx^T]v - \\epsilon^2 \\|v\\|_2^2.$$\n\nLet us set \\(g_v(\\rho) = E[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2]\\) for any vector \\(v \\in \\Theta\\). We have that\n\nwhere \\(\\xi \\in (0, \\rho^*)\\). It holds that\n\n$$|g_v(0) - g_v(\\rho^*)| = \\rho^*|g'(v, \\xi)|,$$\n\n$$\\frac{d g_v}{d \\rho} = E\\left[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2(w \\cdot x)] - E[x \\sim \\phi(\\cdot;\\rho w)[(v \\cdot x)^2] E[x \\sim \\phi(\\cdot;\\rho w)[w \\cdot x]]\\right].$$\n\nThis gives that for any \\(v \\in \\Theta\\), it holds\n\n$$v^T E[xx^T]v \\geq v^T E[xx^T]v - 2\\rho^* \\|w\\|_2 D^3 \\|v\\|_2.$$\n\nNote that the above holds for \\(v = c\\) too. Lemma 8 gives us that\n\n$$\\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[c \\cdot x]] \\geq \\text{Var}[x \\sim \\phi(\\cdot;0)[c \\cdot x]] - 3 \\max\\{\\epsilon^2, \\epsilon^2_1, \\epsilon^1 D\\} \\|c\\|_2^2,$$\n\nwhere \\(\\epsilon^1 = 2\\rho^*BD^2\\) and \\(\\epsilon^2 = 2\\rho^*BD^3\\). This implies that by picking \\(\\rho^* = C_0 BD^3\\alpha\\) for some universal constant \\(C_0\\), we get that\n\n$$\\text{Var}[x \\sim \\phi(\\cdot;\\rho^*w)[c \\cdot x]] = \\Omega(\\alpha \\|c\\|_2^2).$$\n\nIn this section, we considered \\(\\rho^*\\) as indicated by the above Proposition 8."}]}, {"page": 33, "text": "G        Applications to Combinatorial Problems\nIn this section we provide a series of combinatorial applications of our theoretical framework\n(Theorem 1). In particular, for each one of the following combinatorial problems (that prov-\nably satisfy Assumption 1), it suffices to specify the feature mappings \u03c8S, \u03c8I and compute the\nparameters C, DS, DI, \u03b1.\nG.1       Maximum Cut, Maximum Flow and Max-k-CSPs\nWe first provide a general lemma for the variance of \u201dlinear tensors\u201d under the uniform measure.\nLemma 9 (Variance Lower Bound Under Uniform). Let n, k \u2208                                            N. For any w \u2208           R(n  k), it holds that\n                                            Var   x\u223cU({\u22121,1}n)[w \u00b7 x\u2297k] =                     \u2211          w2 S .\n                                                                                      \u2205\u0338=S\u2286[n]:|S|\u2264k\nProof. For any w \u2208 Var         R(n  k), it holds that                            (w \u00b7 x\u2297k)2        \u2212           E\n                         x\u223cU({\u22121,1}n)[w \u00b7 x\u2297k] =                     E                                 x\u223cU({\u22121,1}n)[w \u00b7 x\u2297k]2 .\n                                                             x\u223cU({\u22121,1}n)\nNote that w \u2208           R(n k) can be written as w = (w\u2205, w\u2212\u2205) where w\u2205                               corresponds to the constant term\nof the Fourier expansion and w\u2212\u2205                        = (wS)\u2205\u0338=S\u2286[n]:|S|\u2264k is the vector of the remaining coordinates.\nThe Fourier expansion implies that\n                                                  Var   x\u223cU({\u22121,1}n)[w \u00b7 x\u2297k] = \u2225w\u2212\u2205\u22252                 2 ,\nwhich yields the desired equality for the variance.\nG.1.1       Maximum Cut\nLet us consider a graph with n nodes and weighted adjacency matrix A with non-negative\nweights. Maximum cut is naturally associated with the Ising model and, intuitively, our approach\ndoes not yield an efficient algorithm for solving Max-Cut since we cannot efficiently sample from\nthe Ising model in general. To provide some further intuition, consider a single-parameter Ising\n                                                                                                1+xixj\nmodel for G = (V, E) with Hamiltonian HG(x) = \u2211(i,j)\u2208E                                             2    . Then the partition function is\nequal to ZG(\u03b2) = \u2211x\u2208{\u22121,1}V exp(\u03b2HG(x)). Note that when \u03b2 > 0, the Gibbs measure favours\nconfigurations with alligned spins (ferromagnetic case) and when \u03b2 < 0, the measure favours\nconfigurations with opposite spins (anti-ferromagnetic case). The antiferromagnetic Ising model\nappears to be more challenging. According to physicists the main reason is that its Boltzmann\ndistribution is prone to a complicated type of long-range correlation known as \u2018replica symmetry\nbreaking\u2019 [COLMS22]. From the TCS viewpoint, observe that as \u03b2 goes to \u2212\u221e, the mass of the\nGibbs distribution shifts to spin configurations with more edges joining vertices with opposite\nspins and concentrates on the maximum cuts of the graph. Hence, being able to efficiently ap-\nproximate the log-partition function for general Ising models, would lead to solving the Max-Cut\nproblem.\n                                                                            33", "md": "# Applications to Combinatorial Problems\n\n# Applications to Combinatorial Problems\n\nIn this section we provide a series of combinatorial applications of our theoretical framework (Theorem 1). In particular, for each one of the following combinatorial problems (that provably satisfy Assumption 1), it suffices to specify the feature mappings $$\\psi_S$$, $$\\psi_I$$ and compute the parameters $$C$$, $$D_S$$, $$D_I$$, $$\\alpha$$.\n\n## Maximum Cut, Maximum Flow and Max-k-CSPs\n\nWe first provide a general lemma for the variance of \"linear tensors\" under the uniform measure.\n\n### Lemma 9 (Variance Lower Bound Under Uniform)\n\nLet $$n, k \\in \\mathbb{N}$$. For any $$w \\in \\mathbb{R}^{n \\times k}$$, it holds that\n\n$$\\text{Var}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\sum_{\\emptyset \\neq S \\subseteq [n]: |S| \\leq k} w^2_S$$.\n\nProof: For any $$w \\in \\mathbb{R}^{n \\times k}$$, it holds that\n\n$$\\left(w \\cdot x \\otimes k\\right)^2 - \\mathbb{E}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\mathbb{E}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k]^2$$.\n\nNote that $$w \\in \\mathbb{R}^{n \\times k}$$ can be written as $$w = (w_{\\emptyset}, w_{-\\emptyset})$$ where $$w_{\\emptyset}$$ corresponds to the constant term of the Fourier expansion and $$w_{-\\emptyset} = (w_S)_{\\emptyset \\neq S \\subseteq [n]: |S| \\leq k}$$ is the vector of the remaining coordinates.\n\nThe Fourier expansion implies that\n\n$$\\text{Var}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\|w_{-\\emptyset}\\|^2_2$$,\n\nwhich yields the desired equality for the variance.\n\n### Maximum Cut\n\nLet us consider a graph with $$n$$ nodes and weighted adjacency matrix $$A$$ with non-negative weights. Maximum cut is naturally associated with the Ising model and, intuitively, our approach does not yield an efficient algorithm for solving Max-Cut since we cannot efficiently sample from the Ising model in general. To provide some further intuition, consider a single-parameter Ising model for $$G = (V, E)$$ with Hamiltonian $$H_G(x) = \\sum_{(i,j) \\in E} \\frac{1+x_i x_j}{2}$$. Then the partition function is equal to $$Z_G(\\beta) = \\sum_{x \\in \\{-1,1\\}^V} \\exp(\\beta H_G(x))$$. Note that when $$\\beta > 0$$, the Gibbs measure favours configurations with aligned spins (ferromagnetic case) and when $$\\beta < 0$$, the measure favours configurations with opposite spins (anti-ferromagnetic case). The antiferromagnetic Ising model appears to be more challenging. According to physicists the main reason is that its Boltzmann distribution is prone to a complicated type of long-range correlation known as 'replica symmetry breaking' [COLMS22]. From the TCS viewpoint, observe that as $$\\beta$$ goes to $$-\\infty$$, the mass of the Gibbs distribution shifts to spin configurations with more edges joining vertices with opposite spins and concentrates on the maximum cuts of the graph. Hence, being able to efficiently approximate the log-partition function for general Ising models, would lead to solving the Max-Cut problem.\n\nPage number: 33", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Applications to Combinatorial Problems", "md": "# Applications to Combinatorial Problems"}, {"type": "heading", "lvl": 1, "value": "Applications to Combinatorial Problems", "md": "# Applications to Combinatorial Problems"}, {"type": "text", "value": "In this section we provide a series of combinatorial applications of our theoretical framework (Theorem 1). In particular, for each one of the following combinatorial problems (that provably satisfy Assumption 1), it suffices to specify the feature mappings $$\\psi_S$$, $$\\psi_I$$ and compute the parameters $$C$$, $$D_S$$, $$D_I$$, $$\\alpha$$.", "md": "In this section we provide a series of combinatorial applications of our theoretical framework (Theorem 1). In particular, for each one of the following combinatorial problems (that provably satisfy Assumption 1), it suffices to specify the feature mappings $$\\psi_S$$, $$\\psi_I$$ and compute the parameters $$C$$, $$D_S$$, $$D_I$$, $$\\alpha$$."}, {"type": "heading", "lvl": 2, "value": "Maximum Cut, Maximum Flow and Max-k-CSPs", "md": "## Maximum Cut, Maximum Flow and Max-k-CSPs"}, {"type": "text", "value": "We first provide a general lemma for the variance of \"linear tensors\" under the uniform measure.", "md": "We first provide a general lemma for the variance of \"linear tensors\" under the uniform measure."}, {"type": "heading", "lvl": 3, "value": "Lemma 9 (Variance Lower Bound Under Uniform)", "md": "### Lemma 9 (Variance Lower Bound Under Uniform)"}, {"type": "text", "value": "Let $$n, k \\in \\mathbb{N}$$. For any $$w \\in \\mathbb{R}^{n \\times k}$$, it holds that\n\n$$\\text{Var}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\sum_{\\emptyset \\neq S \\subseteq [n]: |S| \\leq k} w^2_S$$.\n\nProof: For any $$w \\in \\mathbb{R}^{n \\times k}$$, it holds that\n\n$$\\left(w \\cdot x \\otimes k\\right)^2 - \\mathbb{E}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\mathbb{E}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k]^2$$.\n\nNote that $$w \\in \\mathbb{R}^{n \\times k}$$ can be written as $$w = (w_{\\emptyset}, w_{-\\emptyset})$$ where $$w_{\\emptyset}$$ corresponds to the constant term of the Fourier expansion and $$w_{-\\emptyset} = (w_S)_{\\emptyset \\neq S \\subseteq [n]: |S| \\leq k}$$ is the vector of the remaining coordinates.\n\nThe Fourier expansion implies that\n\n$$\\text{Var}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\|w_{-\\emptyset}\\|^2_2$$,\n\nwhich yields the desired equality for the variance.", "md": "Let $$n, k \\in \\mathbb{N}$$. For any $$w \\in \\mathbb{R}^{n \\times k}$$, it holds that\n\n$$\\text{Var}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\sum_{\\emptyset \\neq S \\subseteq [n]: |S| \\leq k} w^2_S$$.\n\nProof: For any $$w \\in \\mathbb{R}^{n \\times k}$$, it holds that\n\n$$\\left(w \\cdot x \\otimes k\\right)^2 - \\mathbb{E}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\mathbb{E}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k]^2$$.\n\nNote that $$w \\in \\mathbb{R}^{n \\times k}$$ can be written as $$w = (w_{\\emptyset}, w_{-\\emptyset})$$ where $$w_{\\emptyset}$$ corresponds to the constant term of the Fourier expansion and $$w_{-\\emptyset} = (w_S)_{\\emptyset \\neq S \\subseteq [n]: |S| \\leq k}$$ is the vector of the remaining coordinates.\n\nThe Fourier expansion implies that\n\n$$\\text{Var}_{x \\sim U(\\{-1,1\\}^n)}[w \\cdot x \\otimes k] = \\|w_{-\\emptyset}\\|^2_2$$,\n\nwhich yields the desired equality for the variance."}, {"type": "heading", "lvl": 3, "value": "Maximum Cut", "md": "### Maximum Cut"}, {"type": "text", "value": "Let us consider a graph with $$n$$ nodes and weighted adjacency matrix $$A$$ with non-negative weights. Maximum cut is naturally associated with the Ising model and, intuitively, our approach does not yield an efficient algorithm for solving Max-Cut since we cannot efficiently sample from the Ising model in general. To provide some further intuition, consider a single-parameter Ising model for $$G = (V, E)$$ with Hamiltonian $$H_G(x) = \\sum_{(i,j) \\in E} \\frac{1+x_i x_j}{2}$$. Then the partition function is equal to $$Z_G(\\beta) = \\sum_{x \\in \\{-1,1\\}^V} \\exp(\\beta H_G(x))$$. Note that when $$\\beta > 0$$, the Gibbs measure favours configurations with aligned spins (ferromagnetic case) and when $$\\beta < 0$$, the measure favours configurations with opposite spins (anti-ferromagnetic case). The antiferromagnetic Ising model appears to be more challenging. According to physicists the main reason is that its Boltzmann distribution is prone to a complicated type of long-range correlation known as 'replica symmetry breaking' [COLMS22]. From the TCS viewpoint, observe that as $$\\beta$$ goes to $$-\\infty$$, the mass of the Gibbs distribution shifts to spin configurations with more edges joining vertices with opposite spins and concentrates on the maximum cuts of the graph. Hence, being able to efficiently approximate the log-partition function for general Ising models, would lead to solving the Max-Cut problem.\n\nPage number: 33", "md": "Let us consider a graph with $$n$$ nodes and weighted adjacency matrix $$A$$ with non-negative weights. Maximum cut is naturally associated with the Ising model and, intuitively, our approach does not yield an efficient algorithm for solving Max-Cut since we cannot efficiently sample from the Ising model in general. To provide some further intuition, consider a single-parameter Ising model for $$G = (V, E)$$ with Hamiltonian $$H_G(x) = \\sum_{(i,j) \\in E} \\frac{1+x_i x_j}{2}$$. Then the partition function is equal to $$Z_G(\\beta) = \\sum_{x \\in \\{-1,1\\}^V} \\exp(\\beta H_G(x))$$. Note that when $$\\beta > 0$$, the Gibbs measure favours configurations with aligned spins (ferromagnetic case) and when $$\\beta < 0$$, the measure favours configurations with opposite spins (anti-ferromagnetic case). The antiferromagnetic Ising model appears to be more challenging. According to physicists the main reason is that its Boltzmann distribution is prone to a complicated type of long-range correlation known as 'replica symmetry breaking' [COLMS22]. From the TCS viewpoint, observe that as $$\\beta$$ goes to $$-\\infty$$, the mass of the Gibbs distribution shifts to spin configurations with more edges joining vertices with opposite spins and concentrates on the maximum cuts of the graph. Hence, being able to efficiently approximate the log-partition function for general Ising models, would lead to solving the Max-Cut problem.\n\nPage number: 33"}]}, {"page": 34, "text": "Theorem 2 (Max-Cut has a Compressed and Efficiently Optimizable Solution Generator). Con-\nsider a prior over Max-Cut instances with n nodes. For any \u03f5 > 0, there exists a solution generator\nP = {p(w) : w \u2208                 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5) and\nL + \u03bbR : W  \u2192                  R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5)\nsteps for some \u03bb > 0.\nProof of Theorem 2. It suffices to show that Max-Cut satisfies Assumption 1. Consider an input\ngraph G with n nodes and Laplacian matrix LG. Then\n                                           MAXCUT = 1              max                                min\n                                                             4  s\u2208{\u22121,1}n s\u22a4LGs = 1             4  s\u2208{\u22121,1}n \u2212s\u22a4LGs .\nWe show that there exist feature mappings so that the cost of every solution s under any in-\nstance/graph G is a bilinear function of the feature vectors (cf. Item 2 of Assumption 1). We\nconsider the correlation-based feature mapping \u03c8S(s) = (ss\u22a4)\u266d                                                          \u2208   Rn2, where by (\u00b7)\u266d                  we de-\nnote the vectorization/flattening operation and the negative Laplacian for the instance (graph),\n\u03c8I(G) = (\u2212LG)\u266d                  \u2208   Rn2. Then simply setting the matrix M to be the identity I \u2208                                                 Rn2\u00d7n2 the cost\nof any solution s can be expressed as the bilinear function \u03c8I(G)\u22a4M\u03c8S(s) = (\u2212L\u266d                                                                       G)\u22a4(ssT)\u266d          =\n\u2212s\u22a4LGs. We observe that (for unweighted graphs) with n nodes the bit-complexity of the family\nof all instances I is roughly O(n2), and therefore the dimensions of the \u03c8S, \u03c8I feature mappings\nare clearly polynomial in the bit-complexity of I. Moreover, considering unweighted graphs, it\nholds \u2225\u03c8I(G)\u22252, \u2225\u03c8S(s)\u22252, \u2225M\u2225F \u2264                               poly(n). Therefore, the constants DS, DI, C are polynomial in\nthe bit-complexity of the instance family.\n       It remains to show that our solution feature mapping satisfy the variance preservation as-\nsumption. For any v, we have that Var                                  s\u223cU(S)[v \u00b7 \u03c8S(s)] = Vars\u223cU({\u22121,1}n)[v \u00b7 (ss\u22a4)\u266d] = \u2126(\u2225v\u22252                                        2),\nusing Lemma 9 with k = 2, since c\u2205                               = 0 with loss of generality.\nG.1.2         Minimum Cut/Maximum Flow\nLet us again consider a graph with n nodes and Laplacian matrix LG.                                                                   It is known that the\nminimum cut problem is solvable in polynomial time when all the weights are positive. From\nthe discussion of the maximum cut case, we can intuitively relate minimum cut with positive\nweights to the ferromagnetic Ising setting [dPS97]. We remark that we can consider the ferro-\nmagnetic parameter space Wfer = R(n                                 2)\n                                                                   \u22650 and get the variance lower bound from Lemma 9. We\nconstraint projected SGD in Wfer. This means that during any step of SGD our algorithm has\nto sample from a mixture of ferromagnetic models with known mixture weights. The state of\nthe art approximate sampling algorithm from ferromagnetic Ising models achieves the following\nperformance, improving on prior work [JS93, LSS19, CLV22, CGG+19].\nProposition 9 (Theorem 1.1 of [CZ22]). Let \u03b4\u03b2, \u03b4\u03bb \u2208                                        (0, 1) be constants and \u00b5 be the Gibbs distribution\nof the ferromagnetic Ising model specified by graph G = (V, E), |V| = n, |E| = m, parameters \u03b2 \u2208\n[1 + \u03b4\u03b2, +\u221e)m and external field \u03bb \u2208                             [0, 1 \u2212     \u03b4\u03bb]n. There exists an algorithm that samples X satisfying\nTV(X, \u00b5) \u2264            \u03f5 for any given parameter \u03f5 \u2208                     (0, 1) within running time\n                                                                     m     log n\u03f5   34 O\u03b4\u03b2,\u03b4\u03bb(1)       .", "md": "# Max-Cut Theorem\n\n## Theorem 2 (Max-Cut has a Compressed and Efficiently Optimizable Solution Generator)\n\nConsider a prior over Max-Cut instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0.\n\n### Proof of Theorem 2:\n\nIt suffices to show that Max-Cut satisfies Assumption 1. Consider an input graph G with n nodes and Laplacian matrix LG. Then\n\n$$\n\\text{MAXCUT} = \\frac{1}{4} \\max_{s\\in\\{-1,1\\}^n} s^T LGs = \\frac{1}{4} \\min_{s\\in\\{-1,1\\}^n} -s^T LGs .\n$$\nWe show that there exist feature mappings so that the cost of every solution s under any instance/graph G is a bilinear function of the feature vectors (cf. Item 2 of Assumption 1). We consider the correlation-based feature mapping $$\\psi_S(s) = (ss^T)^\\flat \\in \\mathbb{R}^{n^2}$$, where by $(\\cdot)^\\flat$ we denote the vectorization/flattening operation and the negative Laplacian for the instance (graph), $$\\psi_I(G) = (-LG)^\\flat \\in \\mathbb{R}^{n^2}$$. Then simply setting the matrix M to be the identity $I \\in \\mathbb{R}^{n^2\\times n^2}$ the cost of any solution s can be expressed as the bilinear function $$\\psi_I(G)^T M \\psi_S(s) = (-L^\\flat G)^T(ss^T)^\\flat = -s^T LGs$$. We observe that (for unweighted graphs) with n nodes the bit-complexity of the family of all instances I is roughly O(n^2), and therefore the dimensions of the $\\psi_S, \\psi_I$ feature mappings are clearly polynomial in the bit-complexity of I. Moreover, considering unweighted graphs, it holds $\\|\\psi_I(G)\\|_2, \\|\\psi_S(s)\\|_2, \\|M\\|_F \\leq \\text{poly}(n)$. Therefore, the constants D_S, D_I, C are polynomial in the bit-complexity of the instance family.\n\nIt remains to show that our solution feature mapping satisfy the variance preservation assumption. For any v, we have that $$\\text{Var}_{s\\sim U(S)}[v \\cdot \\psi_S(s)] = \\text{Var}_{s\\sim U(\\{-1,1\\}^n)}[v \\cdot (ss^T)^\\flat] = \\Omega(\\|v\\|_2^2)$$, using Lemma 9 with k = 2, since $c_\\emptyset = 0$ with loss of generality.\n\n### Minimum Cut/Maximum Flow\n\nLet us again consider a graph with n nodes and Laplacian matrix LG. It is known that the minimum cut problem is solvable in polynomial time when all the weights are positive. From the discussion of the maximum cut case, we can intuitively relate minimum cut with positive weights to the ferromagnetic Ising setting [dPS97]. We remark that we can consider the ferromagnetic parameter space $W_{\\text{fer}} = \\mathbb{R}^{n \\choose 2}_{\\geq 0}$ and get the variance lower bound from Lemma 9. We constraint projected SGD in $W_{\\text{fer}}$. This means that during any step of SGD our algorithm has to sample from a mixture of ferromagnetic models with known mixture weights. The state of the art approximate sampling algorithm from ferromagnetic Ising models achieves the following performance, improving on prior work [JS93, LSS19, CLV22, CGG+19].\n\n#### Proposition 9 (Theorem 1.1 of [CZ22]):\n\nLet $\\delta\\beta, \\delta\\lambda \\in (0, 1)$ be constants and $\\mu$ be the Gibbs distribution of the ferromagnetic Ising model specified by graph $G = (V, E)$, $|V| = n$, $|E| = m$, parameters $\\beta \\in [1 + \\delta\\beta, +\\infty)^m$ and external field $\\lambda \\in [0, 1 - \\delta\\lambda]^n$. There exists an algorithm that samples X satisfying $TV(X, \\mu) \\leq \\epsilon$ for any given parameter $\\epsilon \\in (0, 1)$ within running time $$m \\log n \\epsilon \\frac{3}{4} O_{\\delta\\beta,\\delta\\lambda}(1)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Max-Cut Theorem", "md": "# Max-Cut Theorem"}, {"type": "heading", "lvl": 2, "value": "Theorem 2 (Max-Cut has a Compressed and Efficiently Optimizable Solution Generator)", "md": "## Theorem 2 (Max-Cut has a Compressed and Efficiently Optimizable Solution Generator)"}, {"type": "text", "value": "Consider a prior over Max-Cut instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0.", "md": "Consider a prior over Max-Cut instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0."}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 2:", "md": "### Proof of Theorem 2:"}, {"type": "text", "value": "It suffices to show that Max-Cut satisfies Assumption 1. Consider an input graph G with n nodes and Laplacian matrix LG. Then\n\n$$\n\\text{MAXCUT} = \\frac{1}{4} \\max_{s\\in\\{-1,1\\}^n} s^T LGs = \\frac{1}{4} \\min_{s\\in\\{-1,1\\}^n} -s^T LGs .\n$$\nWe show that there exist feature mappings so that the cost of every solution s under any instance/graph G is a bilinear function of the feature vectors (cf. Item 2 of Assumption 1). We consider the correlation-based feature mapping $$\\psi_S(s) = (ss^T)^\\flat \\in \\mathbb{R}^{n^2}$$, where by $(\\cdot)^\\flat$ we denote the vectorization/flattening operation and the negative Laplacian for the instance (graph), $$\\psi_I(G) = (-LG)^\\flat \\in \\mathbb{R}^{n^2}$$. Then simply setting the matrix M to be the identity $I \\in \\mathbb{R}^{n^2\\times n^2}$ the cost of any solution s can be expressed as the bilinear function $$\\psi_I(G)^T M \\psi_S(s) = (-L^\\flat G)^T(ss^T)^\\flat = -s^T LGs$$. We observe that (for unweighted graphs) with n nodes the bit-complexity of the family of all instances I is roughly O(n^2), and therefore the dimensions of the $\\psi_S, \\psi_I$ feature mappings are clearly polynomial in the bit-complexity of I. Moreover, considering unweighted graphs, it holds $\\|\\psi_I(G)\\|_2, \\|\\psi_S(s)\\|_2, \\|M\\|_F \\leq \\text{poly}(n)$. Therefore, the constants D_S, D_I, C are polynomial in the bit-complexity of the instance family.\n\nIt remains to show that our solution feature mapping satisfy the variance preservation assumption. For any v, we have that $$\\text{Var}_{s\\sim U(S)}[v \\cdot \\psi_S(s)] = \\text{Var}_{s\\sim U(\\{-1,1\\}^n)}[v \\cdot (ss^T)^\\flat] = \\Omega(\\|v\\|_2^2)$$, using Lemma 9 with k = 2, since $c_\\emptyset = 0$ with loss of generality.", "md": "It suffices to show that Max-Cut satisfies Assumption 1. Consider an input graph G with n nodes and Laplacian matrix LG. Then\n\n$$\n\\text{MAXCUT} = \\frac{1}{4} \\max_{s\\in\\{-1,1\\}^n} s^T LGs = \\frac{1}{4} \\min_{s\\in\\{-1,1\\}^n} -s^T LGs .\n$$\nWe show that there exist feature mappings so that the cost of every solution s under any instance/graph G is a bilinear function of the feature vectors (cf. Item 2 of Assumption 1). We consider the correlation-based feature mapping $$\\psi_S(s) = (ss^T)^\\flat \\in \\mathbb{R}^{n^2}$$, where by $(\\cdot)^\\flat$ we denote the vectorization/flattening operation and the negative Laplacian for the instance (graph), $$\\psi_I(G) = (-LG)^\\flat \\in \\mathbb{R}^{n^2}$$. Then simply setting the matrix M to be the identity $I \\in \\mathbb{R}^{n^2\\times n^2}$ the cost of any solution s can be expressed as the bilinear function $$\\psi_I(G)^T M \\psi_S(s) = (-L^\\flat G)^T(ss^T)^\\flat = -s^T LGs$$. We observe that (for unweighted graphs) with n nodes the bit-complexity of the family of all instances I is roughly O(n^2), and therefore the dimensions of the $\\psi_S, \\psi_I$ feature mappings are clearly polynomial in the bit-complexity of I. Moreover, considering unweighted graphs, it holds $\\|\\psi_I(G)\\|_2, \\|\\psi_S(s)\\|_2, \\|M\\|_F \\leq \\text{poly}(n)$. Therefore, the constants D_S, D_I, C are polynomial in the bit-complexity of the instance family.\n\nIt remains to show that our solution feature mapping satisfy the variance preservation assumption. For any v, we have that $$\\text{Var}_{s\\sim U(S)}[v \\cdot \\psi_S(s)] = \\text{Var}_{s\\sim U(\\{-1,1\\}^n)}[v \\cdot (ss^T)^\\flat] = \\Omega(\\|v\\|_2^2)$$, using Lemma 9 with k = 2, since $c_\\emptyset = 0$ with loss of generality."}, {"type": "heading", "lvl": 3, "value": "Minimum Cut/Maximum Flow", "md": "### Minimum Cut/Maximum Flow"}, {"type": "text", "value": "Let us again consider a graph with n nodes and Laplacian matrix LG. It is known that the minimum cut problem is solvable in polynomial time when all the weights are positive. From the discussion of the maximum cut case, we can intuitively relate minimum cut with positive weights to the ferromagnetic Ising setting [dPS97]. We remark that we can consider the ferromagnetic parameter space $W_{\\text{fer}} = \\mathbb{R}^{n \\choose 2}_{\\geq 0}$ and get the variance lower bound from Lemma 9. We constraint projected SGD in $W_{\\text{fer}}$. This means that during any step of SGD our algorithm has to sample from a mixture of ferromagnetic models with known mixture weights. The state of the art approximate sampling algorithm from ferromagnetic Ising models achieves the following performance, improving on prior work [JS93, LSS19, CLV22, CGG+19].", "md": "Let us again consider a graph with n nodes and Laplacian matrix LG. It is known that the minimum cut problem is solvable in polynomial time when all the weights are positive. From the discussion of the maximum cut case, we can intuitively relate minimum cut with positive weights to the ferromagnetic Ising setting [dPS97]. We remark that we can consider the ferromagnetic parameter space $W_{\\text{fer}} = \\mathbb{R}^{n \\choose 2}_{\\geq 0}$ and get the variance lower bound from Lemma 9. We constraint projected SGD in $W_{\\text{fer}}$. This means that during any step of SGD our algorithm has to sample from a mixture of ferromagnetic models with known mixture weights. The state of the art approximate sampling algorithm from ferromagnetic Ising models achieves the following performance, improving on prior work [JS93, LSS19, CLV22, CGG+19]."}, {"type": "heading", "lvl": 4, "value": "Proposition 9 (Theorem 1.1 of [CZ22]):", "md": "#### Proposition 9 (Theorem 1.1 of [CZ22]):"}, {"type": "text", "value": "Let $\\delta\\beta, \\delta\\lambda \\in (0, 1)$ be constants and $\\mu$ be the Gibbs distribution of the ferromagnetic Ising model specified by graph $G = (V, E)$, $|V| = n$, $|E| = m$, parameters $\\beta \\in [1 + \\delta\\beta, +\\infty)^m$ and external field $\\lambda \\in [0, 1 - \\delta\\lambda]^n$. There exists an algorithm that samples X satisfying $TV(X, \\mu) \\leq \\epsilon$ for any given parameter $\\epsilon \\in (0, 1)$ within running time $$m \\log n \\epsilon \\frac{3}{4} O_{\\delta\\beta,\\delta\\lambda}(1)$$.", "md": "Let $\\delta\\beta, \\delta\\lambda \\in (0, 1)$ be constants and $\\mu$ be the Gibbs distribution of the ferromagnetic Ising model specified by graph $G = (V, E)$, $|V| = n$, $|E| = m$, parameters $\\beta \\in [1 + \\delta\\beta, +\\infty)^m$ and external field $\\lambda \\in [0, 1 - \\delta\\lambda]^n$. There exists an algorithm that samples X satisfying $TV(X, \\mu) \\leq \\epsilon$ for any given parameter $\\epsilon \\in (0, 1)$ within running time $$m \\log n \\epsilon \\frac{3}{4} O_{\\delta\\beta,\\delta\\lambda}(1)$$."}]}, {"page": 35, "text": "     This algorithm can handle general instances and it only takes a near-linear running time\nwhen parameters are bounded away from the all-ones vector. Our goal is to sample from a\nmixture of two such ferromagnetic Ising models which can be done efficiently. For simplicity, we\nnext restrict ourselves to the unweighted case.\nTheorem 3 (Min-Cut has a Compressed, Efficiently Optimizable and Samplable Solution Gener-\nator). Consider a prior over Min-Cut instances with n nodes. For any \u03f5 > 0, there exists a solution gener-\nator P = {p(w) : w \u2208            W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5),\nL + \u03bbR : W  \u2192            R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5)\nsteps for some \u03bb > 0 and efficiently samplable in poly(n, 1/\u03f5) steps.\nProof. We have that\n                                                 MINCUT = 1           min\n                                                                4  x\u2208{\u22121,1}n x\u22a4LGx .\nThe analysis (i.e., the selection of the feature mappings) is similar to the one of Theorem 2 with\nthe sole difference that the parameter space is constrained to be Wfer and \u03c8I(G) = (LG)\u266d. We note\nthat Proposition 9 is applicable during the optimization steps. Having an efficient approximate\nsampler for solutions of Min-Cut, it holds that the runtime of the projected SGD algorithm is\npoly(n, 1/\u03f5).        We note that during the execution of the algorithm we do not have access to\nperfectly unbiased samples from mixture of ferromagnetic Ising models. However, we remark\nthat SGD is robust to that inaccuracy in the stochastic oracle. For further details, we refer e.g., to\n[d\u2019A08]).\nG.1.3      Max-k-CSPs\nIn this problem, we are given a set of variables {xu}u\u2208U where |U| = n and a set of Boolean\npredicates P. Each variable xu takes values in {\u22121, 1}. Each predicate depends on at most k\nvariables. For instance, Max-Cut is a Max-2-CSP. Our goal is to assign values to variables so as\nto maximize the number of satisfied constraints (i.e., predicates equal to 1). Let us fix a predicate\nh \u2208   P, i.e., a Boolean function h : {\u22121, 1}n \u2192                    {0, 1} which is a k-junta. Using standard Fourier\nanalysis, the number of satisfied predicates for the assignment x \u2208                               {\u22121, 1}n is\n                                                          |P|\n                                               F(x) =     \u2211        \u2211        hj(S) \u220f u\u2208S  xu ,\n                                                          j=1 S\u2286[n],|S|\u2264k\nwhere  hj(S) is the Fourier coefficient of the predicate hj at S.\nTheorem 4 (Max-k-CSPs have a Compressed and Efficiently Optimizable Solution Generator).\nConsider a prior over Max-k-CSP instances with n variables, where k \u2208                               N can be considered constant\ncompared to n. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208                                         W} such that\nP is complete, compressed with description O(nk)polylog(1/\u03f5) and L + \u03bbR : W  \u2192                                          R is efficiently\noptimizable via projected stochastic gradient descent in poly(nk, 1/\u03f5) steps for some \u03bb > 0.\nProof. Any instance of Max-k-CSP is a list of predicates (i.e., Boolean functions) and our goal is\nto maximize the number of satisfied predicated with a single assignment s \u2208                                     {\u22121, 1}n. We show\nthat there exist feature mappings so that the cost of every solution s under any instance/predi-\ncates list P is a bilinear function of the feature vectors (cf. Item 2 of Assumption 1). We consider\n                                                                    35", "md": "# Math Equations and Text\n\nThis algorithm can handle general instances and it only takes a near-linear running time when parameters are bounded away from the all-ones vector. Our goal is to sample from a mixture of two such ferromagnetic Ising models which can be done efficiently. For simplicity, we next restrict ourselves to the unweighted case.\n\nTheorem 3 (Min-Cut has a Compressed, Efficiently Optimizable and Samplable Solution Generator). Consider a prior over Min-Cut instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5), L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0 and efficiently samplable in poly(n, 1/\u03f5) steps.\n\nProof. We have that\n\n$$\n\\text{MINCUT} = 1 - \\min_{x\\in\\{-1,1\\}^n} x^{\\top}LGx .\n$$\nThe analysis (i.e., the selection of the feature mappings) is similar to the one of Theorem 2 with the sole difference that the parameter space is constrained to be Wfer and \u03c8I(G) = (LG)\u266d. We note that Proposition 9 is applicable during the optimization steps. Having an efficient approximate sampler for solutions of Min-Cut, it holds that the runtime of the projected SGD algorithm is poly(n, 1/\u03f5). We note that during the execution of the algorithm we do not have access to perfectly unbiased samples from a mixture of ferromagnetic Ising models. However, we remark that SGD is robust to that inaccuracy in the stochastic oracle. For further details, we refer e.g., to [d\u2019A08]).\n\n### Max-k-CSPs\n\nIn this problem, we are given a set of variables {xu}u\u2208U where |U| = n and a set of Boolean predicates P. Each variable xu takes values in {-1, 1}. Each predicate depends on at most k variables. For instance, Max-Cut is a Max-2-CSP. Our goal is to assign values to variables so as to maximize the number of satisfied constraints (i.e., predicates equal to 1). Let us fix a predicate h \u2208 P, i.e., a Boolean function h : {-1, 1}^n \u2192 {0, 1} which is a k-junta. Using standard Fourier analysis, the number of satisfied predicates for the assignment x \u2208 {-1, 1}^n is\n\n$$\nF(x) = \\sum_{j=1}^{|P|} \\sum_{S\\subseteq[n],|S|\\leq k} h_j(S) \\prod_{u\\in S} x_u ,\n$$\nwhere h_j(S) is the Fourier coefficient of the predicate h_j at S.\n\nTheorem 4 (Max-k-CSPs have a Compressed and Efficiently Optimizable Solution Generator). Consider a prior over Max-k-CSP instances with n variables, where k \u2208 N can be considered constant compared to n. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description O(nk)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(nk, 1/\u03f5) steps for some \u03bb > 0.\n\nProof. Any instance of Max-k-CSP is a list of predicates (i.e., Boolean functions) and our goal is to maximize the number of satisfied predicates with a single assignment s \u2208 {-1, 1}^n. We show that there exist feature mappings so that the cost of every solution s under any instance/predicates list P is a bilinear function of the feature vectors (cf. Item 2 of Assumption 1). We consider\n\n35", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "This algorithm can handle general instances and it only takes a near-linear running time when parameters are bounded away from the all-ones vector. Our goal is to sample from a mixture of two such ferromagnetic Ising models which can be done efficiently. For simplicity, we next restrict ourselves to the unweighted case.\n\nTheorem 3 (Min-Cut has a Compressed, Efficiently Optimizable and Samplable Solution Generator). Consider a prior over Min-Cut instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5), L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0 and efficiently samplable in poly(n, 1/\u03f5) steps.\n\nProof. We have that\n\n$$\n\\text{MINCUT} = 1 - \\min_{x\\in\\{-1,1\\}^n} x^{\\top}LGx .\n$$\nThe analysis (i.e., the selection of the feature mappings) is similar to the one of Theorem 2 with the sole difference that the parameter space is constrained to be Wfer and \u03c8I(G) = (LG)\u266d. We note that Proposition 9 is applicable during the optimization steps. Having an efficient approximate sampler for solutions of Min-Cut, it holds that the runtime of the projected SGD algorithm is poly(n, 1/\u03f5). We note that during the execution of the algorithm we do not have access to perfectly unbiased samples from a mixture of ferromagnetic Ising models. However, we remark that SGD is robust to that inaccuracy in the stochastic oracle. For further details, we refer e.g., to [d\u2019A08]).", "md": "This algorithm can handle general instances and it only takes a near-linear running time when parameters are bounded away from the all-ones vector. Our goal is to sample from a mixture of two such ferromagnetic Ising models which can be done efficiently. For simplicity, we next restrict ourselves to the unweighted case.\n\nTheorem 3 (Min-Cut has a Compressed, Efficiently Optimizable and Samplable Solution Generator). Consider a prior over Min-Cut instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5), L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0 and efficiently samplable in poly(n, 1/\u03f5) steps.\n\nProof. We have that\n\n$$\n\\text{MINCUT} = 1 - \\min_{x\\in\\{-1,1\\}^n} x^{\\top}LGx .\n$$\nThe analysis (i.e., the selection of the feature mappings) is similar to the one of Theorem 2 with the sole difference that the parameter space is constrained to be Wfer and \u03c8I(G) = (LG)\u266d. We note that Proposition 9 is applicable during the optimization steps. Having an efficient approximate sampler for solutions of Min-Cut, it holds that the runtime of the projected SGD algorithm is poly(n, 1/\u03f5). We note that during the execution of the algorithm we do not have access to perfectly unbiased samples from a mixture of ferromagnetic Ising models. However, we remark that SGD is robust to that inaccuracy in the stochastic oracle. For further details, we refer e.g., to [d\u2019A08])."}, {"type": "heading", "lvl": 3, "value": "Max-k-CSPs", "md": "### Max-k-CSPs"}, {"type": "text", "value": "In this problem, we are given a set of variables {xu}u\u2208U where |U| = n and a set of Boolean predicates P. Each variable xu takes values in {-1, 1}. Each predicate depends on at most k variables. For instance, Max-Cut is a Max-2-CSP. Our goal is to assign values to variables so as to maximize the number of satisfied constraints (i.e., predicates equal to 1). Let us fix a predicate h \u2208 P, i.e., a Boolean function h : {-1, 1}^n \u2192 {0, 1} which is a k-junta. Using standard Fourier analysis, the number of satisfied predicates for the assignment x \u2208 {-1, 1}^n is\n\n$$\nF(x) = \\sum_{j=1}^{|P|} \\sum_{S\\subseteq[n],|S|\\leq k} h_j(S) \\prod_{u\\in S} x_u ,\n$$\nwhere h_j(S) is the Fourier coefficient of the predicate h_j at S.\n\nTheorem 4 (Max-k-CSPs have a Compressed and Efficiently Optimizable Solution Generator). Consider a prior over Max-k-CSP instances with n variables, where k \u2208 N can be considered constant compared to n. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description O(nk)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(nk, 1/\u03f5) steps for some \u03bb > 0.\n\nProof. Any instance of Max-k-CSP is a list of predicates (i.e., Boolean functions) and our goal is to maximize the number of satisfied predicates with a single assignment s \u2208 {-1, 1}^n. We show that there exist feature mappings so that the cost of every solution s under any instance/predicates list P is a bilinear function of the feature vectors (cf. Item 2 of Assumption 1). We consider\n\n35", "md": "In this problem, we are given a set of variables {xu}u\u2208U where |U| = n and a set of Boolean predicates P. Each variable xu takes values in {-1, 1}. Each predicate depends on at most k variables. For instance, Max-Cut is a Max-2-CSP. Our goal is to assign values to variables so as to maximize the number of satisfied constraints (i.e., predicates equal to 1). Let us fix a predicate h \u2208 P, i.e., a Boolean function h : {-1, 1}^n \u2192 {0, 1} which is a k-junta. Using standard Fourier analysis, the number of satisfied predicates for the assignment x \u2208 {-1, 1}^n is\n\n$$\nF(x) = \\sum_{j=1}^{|P|} \\sum_{S\\subseteq[n],|S|\\leq k} h_j(S) \\prod_{u\\in S} x_u ,\n$$\nwhere h_j(S) is the Fourier coefficient of the predicate h_j at S.\n\nTheorem 4 (Max-k-CSPs have a Compressed and Efficiently Optimizable Solution Generator). Consider a prior over Max-k-CSP instances with n variables, where k \u2208 N can be considered constant compared to n. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description O(nk)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(nk, 1/\u03f5) steps for some \u03bb > 0.\n\nProof. Any instance of Max-k-CSP is a list of predicates (i.e., Boolean functions) and our goal is to maximize the number of satisfied predicates with a single assignment s \u2208 {-1, 1}^n. We show that there exist feature mappings so that the cost of every solution s under any instance/predicates list P is a bilinear function of the feature vectors (cf. Item 2 of Assumption 1). We consider\n\n35"}]}, {"page": 36, "text": "the order k correlation-based feature mappings \u03c8S(s) = (s\u2297k)\u266d           \u2208  Rnk, where by (\u00b7)\u266d   we denote the\nflattening operation of the order k tensor, and, \u03c8I(P) = \u03c8I(h1, . . . , h|P|) = \u2212       \u2211|P|   hj(S))S\u2286[n],|S|\u2264k)\u22a4  \u2208\nRnk                                                                                       j=1((\n    , where ( hj(S))S\u2286[n],|S|\u2264k is a vector of size nk with the Fourier coeffients of the j-th pred-\nicate.  We take \u03c8I being the coordinate-wise sum of these coefficients.                  The setting the ma-\ntrix M to be the identity matrix I \u2208           Rnk\u00d7nk, we get that the cost of any solution s can be\nexpressed as the bilinear function \u03c8I(P)\u22a4M\u03c8S(s) = \u2212               \u2211|P|               hj(S) \u220fu\u2208S xu. For any\n                                                                     j=1 \u2211S\u2286[n],|S|\u2264k\nh : {\u22121, 1}n \u2192     {0, 1}, we get that the description size of any  h(S) is poly(n, k) and so the di-\nmensions of the \u03c8S, \u03c8I feature mappings are polynomial in the description size of I. Moreover,\nwe get that \u2225\u03c8I(P)\u2225, \u2225\u03c8S(s)\u2225, \u2225M\u2225        \u2264  poly(nk). Hence, the constants DS, DI, C are polynomial in\nthe description size of the instance family. Finally, we have that for any v, Var           s\u223cU(S)[v \u00b7 \u03c8S(s)] =\nVar s\u223cU({\u22121,1}n)[v \u00b7 s\u2297k] = \u2126(\u2225v\u22252   2), using Lemma 9, assuming that v\u2205          is 0 without loss of gener-\nality. This implies the result.\nG.2    Bipartite Matching and TSP\nG.2.1    Maximum Weight Bipartite Matching\nIn Maximum Weight Bipartite Matching (MWBM) there exists a complete bipartite graph (A, B)\nwith |A| = |B| = n (the assumptions that the graph is complete and balanced is without loss of\ngenerality) with weight matrix W where W(i, j) indicates the value of the edge (i, j), i \u2208             A, j \u2208  B\nand the goal is to match the vertices in order to maximize the value.                   Hence the goal is to\nmaximize L(\u03a0) = W \u00b7 \u03a0 over all permutation matrices. By the structure of the problem some\nmaximum weight matching is a perfect matching. Furthermore, by negating the weights of the\nedges we can state the problem as the following minimization problem: given a bipartite graph\n(A, B) and weight matrix W \u2208        (R \u222a  {\u221e})n\u00d7n, find a perfect matching M with minimum weight.\nOne of the fundamental results in combinatorial optimization is the polynomial-time blossom\nalgorithm for computing minimum-weight perfect matchings by [Edm65].\n    We begin this section by showing a variance lower bound under the uniform distribution\nover the permutation group.\nLemma 10 (Variance Lower Bound). Let U(Sn) be the uniform distribution over n \u00d7 n permutation\nmatrices. For any matrix W \u2208     Rn\u00d7n, with \u2211i Wij = 0 and \u2211j Wij = 0 we have\n                                                                      F\n                                        Var \u03a0\u223cU(Sn)[W \u00b7 \u03a0] = \u2225W\u22252n \u2212  1 .\n                                                       36", "md": "# Math Equations and Text\n\nthe order k correlation-based feature mappings $$\\psi_S(s) = (s\\otimes k)^\\flat \\in \\mathbb{R}^{nk}$$, where by $$(\\cdot)^\\flat$$ we denote the flattening operation of the order k tensor, and, $$\\psi_I(P) = \\psi_I(h_1, \\ldots, h|P|) = -\\sum_{|P|} h_j(S))S\\subseteq[n],|S|\\leq k)^\\top \\in \\mathbb{R}^{nk}$$, where $$(h_j(S))S\\subseteq[n],|S|\\leq k$$ is a vector of size nk with the Fourier coefficients of the j-th predicate. We take $$\\psi_I$$ being the coordinate-wise sum of these coefficients. The setting the matrix M to be the identity matrix $$I \\in \\mathbb{R}^{nk\\times nk}$$, we get that the cost of any solution s can be expressed as the bilinear function $$\\psi_I(P)^\\top M\\psi_S(s) = -\\sum_{|P|} h_j(S) \\prod_{u\\in S} x_u$$. For any $$h : \\{-1, 1\\}^n \\rightarrow \\{0, 1\\}$$, we get that the description size of any $$h(S)$$ is poly(n, k) and so the dimensions of the $$\\psi_S, \\psi_I$$ feature mappings are polynomial in the description size of I. Moreover, we get that $$\\|\\psi_I(P)\\|, \\|\\psi_S(s)\\|, \\|M\\| \\leq \\text{poly}(nk)$$. Hence, the constants DS, DI, C are polynomial in the description size of the instance family. Finally, we have that for any v, $$\\text{Var} s\\sim U(S)[v \\cdot \\psi_S(s)] = \\text{Var} s\\sim U(\\{-1,1\\}^n)[v \\cdot s\\otimes k] = \\Omega(\\|v\\|^2_2)$$, using Lemma 9, assuming that $$v_\\emptyset$$ is 0 without loss of generality. This implies the result.\n\n## Bipartite Matching and TSP\n\n### Maximum Weight Bipartite Matching\n\nIn Maximum Weight Bipartite Matching (MWBM) there exists a complete bipartite graph (A, B) with $$|A| = |B| = n$$ (the assumptions that the graph is complete and balanced is without loss of generality) with weight matrix W where $$W(i, j)$$ indicates the value of the edge (i, j), $$i \\in A, j \\in B$$ and the goal is to match the vertices in order to maximize the value. Hence the goal is to maximize $$L(\\Pi) = W \\cdot \\Pi$$ over all permutation matrices. By the structure of the problem some maximum weight matching is a perfect matching. Furthermore, by negating the weights of the edges we can state the problem as the following minimization problem: given a bipartite graph (A, B) and weight matrix $$W \\in (\\mathbb{R} \\cup \\{\\infty\\})^{n\\times n}$$, find a perfect matching M with minimum weight. One of the fundamental results in combinatorial optimization is the polynomial-time blossom algorithm for computing minimum-weight perfect matchings by [Edm65].\n\nWe begin this section by showing a variance lower bound under the uniform distribution over the permutation group.\n\nLemma 10 (Variance Lower Bound). Let $$U(S_n)$$ be the uniform distribution over $$n \\times n$$ permutation matrices. For any matrix $$W \\in \\mathbb{R}^{n\\times n}$$, with $$\\sum_i W_{ij} = 0$$ and $$\\sum_j W_{ij} = 0$$ we have\n\n$$\\text{Var} \\Pi\\sim U(S_n)[W \\cdot \\Pi] = \\|W\\|^2n - 1.$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "the order k correlation-based feature mappings $$\\psi_S(s) = (s\\otimes k)^\\flat \\in \\mathbb{R}^{nk}$$, where by $$(\\cdot)^\\flat$$ we denote the flattening operation of the order k tensor, and, $$\\psi_I(P) = \\psi_I(h_1, \\ldots, h|P|) = -\\sum_{|P|} h_j(S))S\\subseteq[n],|S|\\leq k)^\\top \\in \\mathbb{R}^{nk}$$, where $$(h_j(S))S\\subseteq[n],|S|\\leq k$$ is a vector of size nk with the Fourier coefficients of the j-th predicate. We take $$\\psi_I$$ being the coordinate-wise sum of these coefficients. The setting the matrix M to be the identity matrix $$I \\in \\mathbb{R}^{nk\\times nk}$$, we get that the cost of any solution s can be expressed as the bilinear function $$\\psi_I(P)^\\top M\\psi_S(s) = -\\sum_{|P|} h_j(S) \\prod_{u\\in S} x_u$$. For any $$h : \\{-1, 1\\}^n \\rightarrow \\{0, 1\\}$$, we get that the description size of any $$h(S)$$ is poly(n, k) and so the dimensions of the $$\\psi_S, \\psi_I$$ feature mappings are polynomial in the description size of I. Moreover, we get that $$\\|\\psi_I(P)\\|, \\|\\psi_S(s)\\|, \\|M\\| \\leq \\text{poly}(nk)$$. Hence, the constants DS, DI, C are polynomial in the description size of the instance family. Finally, we have that for any v, $$\\text{Var} s\\sim U(S)[v \\cdot \\psi_S(s)] = \\text{Var} s\\sim U(\\{-1,1\\}^n)[v \\cdot s\\otimes k] = \\Omega(\\|v\\|^2_2)$$, using Lemma 9, assuming that $$v_\\emptyset$$ is 0 without loss of generality. This implies the result.", "md": "the order k correlation-based feature mappings $$\\psi_S(s) = (s\\otimes k)^\\flat \\in \\mathbb{R}^{nk}$$, where by $$(\\cdot)^\\flat$$ we denote the flattening operation of the order k tensor, and, $$\\psi_I(P) = \\psi_I(h_1, \\ldots, h|P|) = -\\sum_{|P|} h_j(S))S\\subseteq[n],|S|\\leq k)^\\top \\in \\mathbb{R}^{nk}$$, where $$(h_j(S))S\\subseteq[n],|S|\\leq k$$ is a vector of size nk with the Fourier coefficients of the j-th predicate. We take $$\\psi_I$$ being the coordinate-wise sum of these coefficients. The setting the matrix M to be the identity matrix $$I \\in \\mathbb{R}^{nk\\times nk}$$, we get that the cost of any solution s can be expressed as the bilinear function $$\\psi_I(P)^\\top M\\psi_S(s) = -\\sum_{|P|} h_j(S) \\prod_{u\\in S} x_u$$. For any $$h : \\{-1, 1\\}^n \\rightarrow \\{0, 1\\}$$, we get that the description size of any $$h(S)$$ is poly(n, k) and so the dimensions of the $$\\psi_S, \\psi_I$$ feature mappings are polynomial in the description size of I. Moreover, we get that $$\\|\\psi_I(P)\\|, \\|\\psi_S(s)\\|, \\|M\\| \\leq \\text{poly}(nk)$$. Hence, the constants DS, DI, C are polynomial in the description size of the instance family. Finally, we have that for any v, $$\\text{Var} s\\sim U(S)[v \\cdot \\psi_S(s)] = \\text{Var} s\\sim U(\\{-1,1\\}^n)[v \\cdot s\\otimes k] = \\Omega(\\|v\\|^2_2)$$, using Lemma 9, assuming that $$v_\\emptyset$$ is 0 without loss of generality. This implies the result."}, {"type": "heading", "lvl": 2, "value": "Bipartite Matching and TSP", "md": "## Bipartite Matching and TSP"}, {"type": "heading", "lvl": 3, "value": "Maximum Weight Bipartite Matching", "md": "### Maximum Weight Bipartite Matching"}, {"type": "text", "value": "In Maximum Weight Bipartite Matching (MWBM) there exists a complete bipartite graph (A, B) with $$|A| = |B| = n$$ (the assumptions that the graph is complete and balanced is without loss of generality) with weight matrix W where $$W(i, j)$$ indicates the value of the edge (i, j), $$i \\in A, j \\in B$$ and the goal is to match the vertices in order to maximize the value. Hence the goal is to maximize $$L(\\Pi) = W \\cdot \\Pi$$ over all permutation matrices. By the structure of the problem some maximum weight matching is a perfect matching. Furthermore, by negating the weights of the edges we can state the problem as the following minimization problem: given a bipartite graph (A, B) and weight matrix $$W \\in (\\mathbb{R} \\cup \\{\\infty\\})^{n\\times n}$$, find a perfect matching M with minimum weight. One of the fundamental results in combinatorial optimization is the polynomial-time blossom algorithm for computing minimum-weight perfect matchings by [Edm65].\n\nWe begin this section by showing a variance lower bound under the uniform distribution over the permutation group.\n\nLemma 10 (Variance Lower Bound). Let $$U(S_n)$$ be the uniform distribution over $$n \\times n$$ permutation matrices. For any matrix $$W \\in \\mathbb{R}^{n\\times n}$$, with $$\\sum_i W_{ij} = 0$$ and $$\\sum_j W_{ij} = 0$$ we have\n\n$$\\text{Var} \\Pi\\sim U(S_n)[W \\cdot \\Pi] = \\|W\\|^2n - 1.$$", "md": "In Maximum Weight Bipartite Matching (MWBM) there exists a complete bipartite graph (A, B) with $$|A| = |B| = n$$ (the assumptions that the graph is complete and balanced is without loss of generality) with weight matrix W where $$W(i, j)$$ indicates the value of the edge (i, j), $$i \\in A, j \\in B$$ and the goal is to match the vertices in order to maximize the value. Hence the goal is to maximize $$L(\\Pi) = W \\cdot \\Pi$$ over all permutation matrices. By the structure of the problem some maximum weight matching is a perfect matching. Furthermore, by negating the weights of the edges we can state the problem as the following minimization problem: given a bipartite graph (A, B) and weight matrix $$W \\in (\\mathbb{R} \\cup \\{\\infty\\})^{n\\times n}$$, find a perfect matching M with minimum weight. One of the fundamental results in combinatorial optimization is the polynomial-time blossom algorithm for computing minimum-weight perfect matchings by [Edm65].\n\nWe begin this section by showing a variance lower bound under the uniform distribution over the permutation group.\n\nLemma 10 (Variance Lower Bound). Let $$U(S_n)$$ be the uniform distribution over $$n \\times n$$ permutation matrices. For any matrix $$W \\in \\mathbb{R}^{n\\times n}$$, with $$\\sum_i W_{ij} = 0$$ and $$\\sum_j W_{ij} = 0$$ we have\n\n$$\\text{Var} \\Pi\\sim U(S_n)[W \\cdot \\Pi] = \\|W\\|^2n - 1.$$"}]}, {"page": 37, "text": "Proof. We have that E                        \u03a0\u223cU(Sn)[\u03a0ij] = 1/n and E\u03a0\u223cU(Sn)[\u03a0ij\u03a0ab] = 1{i\u0338=a\u2227j\u0338=b}                                           n(n\u22121)         + 1{i=a,j=b}  n       . We have\n              Var     \u03a0\u223cU(Sn)[W \u00b7 \u03a0] =                           E                                            E                        2\n                                                           \u03a0\u223cU(Sn)[(W \u00b7 \u03a0)2] \u2212                          \u03a0\u223cU(Sn)[W \u00b7 \u03a0]\n                                                     = \u2211            WabWij             1{i \u0338= a, j \u0338= b}               + 1{i = a, j = b}                       \u2212        \u2211     Wij    2\n                                                           i,j,a,b                            n(n \u2212        1)                              n                             i,j    n\n                                                     = 1             W2  ij + \u2211           WijWab         1{i \u0338= a, j \u0338= b}\n                                                           n \u2211  i,j               i,j,a,b                       n(n \u2212        1)\n                                                     = \u2225W\u22252          F   + \u2211          WijWab         1{i \u0338= a, j \u0338= b}               ,\n                                                                n            i,j,a,b                        n(n \u2212        1)\nwhere to obtain the third equality we used our assumption that \u2211ij Wij = 0. We observe that, by\nour assumption that \u2211b Wab = 0 for all a it holds \u2211b\u0338=j Wab = \u2212Waj and therefore, we have\n                               \u2211 b   Wab1{i \u0338= a, j \u0338= b} = 1{i \u0338= a}                                \u2211 b   Wab1{j \u0338= b} = \u22121{i \u0338= a}Waj .\nSimilarly, using the fact that \u2211a\u0338=i Waj = \u2212Wij we obtain that\nTherefore, using the above identity, we have that \u2211ab   Wab1{i \u0338= a, j \u0338= b} = \u2211                       a    \u2212Waj1{i \u0338= a} = Wij .\n                                              \u2211      WijWab         1{i \u0338= a, j \u0338= b}                = \u2211              W2   ij                \u2225W\u22252      F\n                                             i,j,a,b                       n(n \u2212        1)                 i,j   n(n \u2212        1) =        n(n \u2212        1) .\nCombining the above we obtain the claimed identity.\nRemark 8. We note that in MWBM the conditions \u2211i Wij = 0 and \u2211j Wij = 0 are without loss of\ngenerality.\n        We next claim that there exists an efficient algorithm for (approximately) sampling such\npermutation matrices.\nLemma 11 (Efficient Sampling). There exists an algorithm that generates approximate samples from\nthe Gibbs distribution p(\u00b7; W) with parameter W over the symmetric group, i.e., p(\u03a0; W) \u221d                                                                                             exp(W \u00b7\n\u03a0)1{\u03a0 \u2208              Sn}, in poly(n) time.\nProof. This lemma essentially requires approximating the permanent of a weighted matrix, since\nthis would imply that one has an approximation of the partition function. Essentially, our goal is\nto generate a random variable X that is \u03f5-close in statistical distance to the probability measure\nNote that the partition function is                               p(\u03a0; W) \u221d              exp(W \u00b7 \u03a0)1{\u03a0 \u2208                        Sn} .\n                                              Z(W) = \u2211           \u03a0\u2208Sn     eW\u00b7\u03a0 = \u2211         \u03a0\u2208Sn \u220f   (i,j)  eWij\u03a0ij = \u2211        \u03c3\u2208Sn \u220f  i\u2208[n]    Ai,\u03c3(i) ,\nwhere A is a non-negative real matrix with entries Aij = exp(Wij). Note that in the third equality,\nwe used the isomorphism between permutations and permutation matrices. Hence, Z(W) is\nexactly the permanent of the matrix A.\n                                                                                                   37", "md": "Proof. We have that $$E[\\Pi \\sim U(S_n)[\\Pi_{ij}]] = \\frac{1}{n}$$ and $$E[\\Pi \\sim U(S_n)[\\Pi_{ij}\\Pi_{ab}]] = 1\\{i \\neq a \\land j \\neq b\\} \\frac{n(n-1)}{n} + 1\\{i = a, j = b\\} \\frac{n}{n}$$. We have\n\n$$\n\\begin{align*}\n&Var[\\Pi \\sim U(S_n)[W \\cdot \\Pi]] \\\\\n&= E[\\Pi \\sim U(S_n)[(W \\cdot \\Pi)^2]] - E[\\Pi \\sim U(S_n)[W \\cdot \\Pi]]^2 \\\\\n&= \\sum_{i,j,a,b} W_{ab}W_{ij} 1\\{i \\neq a, j \\neq b\\} + 1\\{i = a, j = b\\} - \\sum_{i,j} W_{ij}^2 \\\\\n&= \\frac{1}{n} \\sum_{i,j} W_{ij}^2 + \\sum_{i,j,a,b} W_{ij}W_{ab} 1\\{i \\neq a, j \\neq b\\}\n\\end{align*}\n$$\nwhere to obtain the third equality we used our assumption that $$\\sum_{ij} W_{ij} = 0$$. We observe that, by our assumption that $$\\sum_{b} W_{ab} = 0$$ for all $$a$$ it holds $$\\sum_{b \\neq j} W_{ab} = -W_{aj}$$ and therefore, we have\n\n$$\n\\begin{align*}\n&\\sum_{b} W_{ab} 1\\{i \\neq a, j \\neq b\\} = 1\\{i \\neq a\\} \\\\\n&\\sum_{b} W_{ab} 1\\{j \\neq b\\} = -1\\{i \\neq a\\}W_{aj}\n\\end{align*}\n$$\nSimilarly, using the fact that $$\\sum_{a \\neq i} W_{aj} = -W_{ij}$$ we obtain that\n\n$$\n\\begin{align*}\n&\\sum_{ab} W_{ab} 1\\{i \\neq a, j \\neq b\\} = \\sum_{a} -W_{aj} 1\\{i \\neq a\\} = W_{ij} \\\\\n&\\sum_{ijab} W_{ij}W_{ab} 1\\{i \\neq a, j \\neq b\\} = \\sum_{ij} W_{ij}^2 \\frac{\\|W\\|^2_F}{n(n-1)} = \\frac{n(n-1)}\n\\end{align*}\n$$\nCombining the above we obtain the claimed identity.\n\nRemark 8. We note that in MWBM the conditions $$\\sum_{i} W_{ij} = 0$$ and $$\\sum_{j} W_{ij} = 0$$ are without loss of generality.\n\nWe next claim that there exists an efficient algorithm for (approximately) sampling such permutation matrices.\n\nLemma 11 (Efficient Sampling). There exists an algorithm that generates approximate samples from the Gibbs distribution $$p(\\cdot; W)$$ with parameter $$W$$ over the symmetric group, i.e., $$p(\\Pi; W) \\propto \\exp(W \\cdot \\Pi)1\\{\\Pi \\in S_n\\}$$, in poly(n) time.\n\nProof. This lemma essentially requires approximating the permanent of a weighted matrix, since this would imply that one has an approximation of the partition function. Essentially, our goal is to generate a random variable X that is $$\\epsilon$$-close in statistical distance to the probability measure\n\nNote that the partition function is $$Z(W) = \\sum_{\\Pi \\in S_n} e^{W \\cdot \\Pi} = \\sum_{\\Pi \\in S_n} \\prod_{(i,j)} e^{W_{ij}\\Pi_{ij}} = \\sum_{\\sigma \\in S_n} \\prod_{i \\in [n]} A_{i,\\sigma(i)}$$, where A is a non-negative real matrix with entries $$A_{ij} = \\exp(W_{ij})$$. Note that in the third equality, we used the isomorphism between permutations and permutation matrices. Hence, $$Z(W)$$ is exactly the permanent of the matrix A.\n\n37", "images": [], "items": [{"type": "text", "value": "Proof. We have that $$E[\\Pi \\sim U(S_n)[\\Pi_{ij}]] = \\frac{1}{n}$$ and $$E[\\Pi \\sim U(S_n)[\\Pi_{ij}\\Pi_{ab}]] = 1\\{i \\neq a \\land j \\neq b\\} \\frac{n(n-1)}{n} + 1\\{i = a, j = b\\} \\frac{n}{n}$$. We have\n\n$$\n\\begin{align*}\n&Var[\\Pi \\sim U(S_n)[W \\cdot \\Pi]] \\\\\n&= E[\\Pi \\sim U(S_n)[(W \\cdot \\Pi)^2]] - E[\\Pi \\sim U(S_n)[W \\cdot \\Pi]]^2 \\\\\n&= \\sum_{i,j,a,b} W_{ab}W_{ij} 1\\{i \\neq a, j \\neq b\\} + 1\\{i = a, j = b\\} - \\sum_{i,j} W_{ij}^2 \\\\\n&= \\frac{1}{n} \\sum_{i,j} W_{ij}^2 + \\sum_{i,j,a,b} W_{ij}W_{ab} 1\\{i \\neq a, j \\neq b\\}\n\\end{align*}\n$$\nwhere to obtain the third equality we used our assumption that $$\\sum_{ij} W_{ij} = 0$$. We observe that, by our assumption that $$\\sum_{b} W_{ab} = 0$$ for all $$a$$ it holds $$\\sum_{b \\neq j} W_{ab} = -W_{aj}$$ and therefore, we have\n\n$$\n\\begin{align*}\n&\\sum_{b} W_{ab} 1\\{i \\neq a, j \\neq b\\} = 1\\{i \\neq a\\} \\\\\n&\\sum_{b} W_{ab} 1\\{j \\neq b\\} = -1\\{i \\neq a\\}W_{aj}\n\\end{align*}\n$$\nSimilarly, using the fact that $$\\sum_{a \\neq i} W_{aj} = -W_{ij}$$ we obtain that\n\n$$\n\\begin{align*}\n&\\sum_{ab} W_{ab} 1\\{i \\neq a, j \\neq b\\} = \\sum_{a} -W_{aj} 1\\{i \\neq a\\} = W_{ij} \\\\\n&\\sum_{ijab} W_{ij}W_{ab} 1\\{i \\neq a, j \\neq b\\} = \\sum_{ij} W_{ij}^2 \\frac{\\|W\\|^2_F}{n(n-1)} = \\frac{n(n-1)}\n\\end{align*}\n$$\nCombining the above we obtain the claimed identity.\n\nRemark 8. We note that in MWBM the conditions $$\\sum_{i} W_{ij} = 0$$ and $$\\sum_{j} W_{ij} = 0$$ are without loss of generality.\n\nWe next claim that there exists an efficient algorithm for (approximately) sampling such permutation matrices.\n\nLemma 11 (Efficient Sampling). There exists an algorithm that generates approximate samples from the Gibbs distribution $$p(\\cdot; W)$$ with parameter $$W$$ over the symmetric group, i.e., $$p(\\Pi; W) \\propto \\exp(W \\cdot \\Pi)1\\{\\Pi \\in S_n\\}$$, in poly(n) time.\n\nProof. This lemma essentially requires approximating the permanent of a weighted matrix, since this would imply that one has an approximation of the partition function. Essentially, our goal is to generate a random variable X that is $$\\epsilon$$-close in statistical distance to the probability measure\n\nNote that the partition function is $$Z(W) = \\sum_{\\Pi \\in S_n} e^{W \\cdot \\Pi} = \\sum_{\\Pi \\in S_n} \\prod_{(i,j)} e^{W_{ij}\\Pi_{ij}} = \\sum_{\\sigma \\in S_n} \\prod_{i \\in [n]} A_{i,\\sigma(i)}$$, where A is a non-negative real matrix with entries $$A_{ij} = \\exp(W_{ij})$$. Note that in the third equality, we used the isomorphism between permutations and permutation matrices. Hence, $$Z(W)$$ is exactly the permanent of the matrix A.\n\n37", "md": "Proof. We have that $$E[\\Pi \\sim U(S_n)[\\Pi_{ij}]] = \\frac{1}{n}$$ and $$E[\\Pi \\sim U(S_n)[\\Pi_{ij}\\Pi_{ab}]] = 1\\{i \\neq a \\land j \\neq b\\} \\frac{n(n-1)}{n} + 1\\{i = a, j = b\\} \\frac{n}{n}$$. We have\n\n$$\n\\begin{align*}\n&Var[\\Pi \\sim U(S_n)[W \\cdot \\Pi]] \\\\\n&= E[\\Pi \\sim U(S_n)[(W \\cdot \\Pi)^2]] - E[\\Pi \\sim U(S_n)[W \\cdot \\Pi]]^2 \\\\\n&= \\sum_{i,j,a,b} W_{ab}W_{ij} 1\\{i \\neq a, j \\neq b\\} + 1\\{i = a, j = b\\} - \\sum_{i,j} W_{ij}^2 \\\\\n&= \\frac{1}{n} \\sum_{i,j} W_{ij}^2 + \\sum_{i,j,a,b} W_{ij}W_{ab} 1\\{i \\neq a, j \\neq b\\}\n\\end{align*}\n$$\nwhere to obtain the third equality we used our assumption that $$\\sum_{ij} W_{ij} = 0$$. We observe that, by our assumption that $$\\sum_{b} W_{ab} = 0$$ for all $$a$$ it holds $$\\sum_{b \\neq j} W_{ab} = -W_{aj}$$ and therefore, we have\n\n$$\n\\begin{align*}\n&\\sum_{b} W_{ab} 1\\{i \\neq a, j \\neq b\\} = 1\\{i \\neq a\\} \\\\\n&\\sum_{b} W_{ab} 1\\{j \\neq b\\} = -1\\{i \\neq a\\}W_{aj}\n\\end{align*}\n$$\nSimilarly, using the fact that $$\\sum_{a \\neq i} W_{aj} = -W_{ij}$$ we obtain that\n\n$$\n\\begin{align*}\n&\\sum_{ab} W_{ab} 1\\{i \\neq a, j \\neq b\\} = \\sum_{a} -W_{aj} 1\\{i \\neq a\\} = W_{ij} \\\\\n&\\sum_{ijab} W_{ij}W_{ab} 1\\{i \\neq a, j \\neq b\\} = \\sum_{ij} W_{ij}^2 \\frac{\\|W\\|^2_F}{n(n-1)} = \\frac{n(n-1)}\n\\end{align*}\n$$\nCombining the above we obtain the claimed identity.\n\nRemark 8. We note that in MWBM the conditions $$\\sum_{i} W_{ij} = 0$$ and $$\\sum_{j} W_{ij} = 0$$ are without loss of generality.\n\nWe next claim that there exists an efficient algorithm for (approximately) sampling such permutation matrices.\n\nLemma 11 (Efficient Sampling). There exists an algorithm that generates approximate samples from the Gibbs distribution $$p(\\cdot; W)$$ with parameter $$W$$ over the symmetric group, i.e., $$p(\\Pi; W) \\propto \\exp(W \\cdot \\Pi)1\\{\\Pi \\in S_n\\}$$, in poly(n) time.\n\nProof. This lemma essentially requires approximating the permanent of a weighted matrix, since this would imply that one has an approximation of the partition function. Essentially, our goal is to generate a random variable X that is $$\\epsilon$$-close in statistical distance to the probability measure\n\nNote that the partition function is $$Z(W) = \\sum_{\\Pi \\in S_n} e^{W \\cdot \\Pi} = \\sum_{\\Pi \\in S_n} \\prod_{(i,j)} e^{W_{ij}\\Pi_{ij}} = \\sum_{\\sigma \\in S_n} \\prod_{i \\in [n]} A_{i,\\sigma(i)}$$, where A is a non-negative real matrix with entries $$A_{ij} = \\exp(W_{ij})$$. Note that in the third equality, we used the isomorphism between permutations and permutation matrices. Hence, $$Z(W)$$ is exactly the permanent of the matrix A.\n\n37"}]}, {"page": 38, "text": "Proposition 10 ([JSV04]). There exists a fully polynomial randomized approximation scheme for the\npermanent of an arbitrary n \u00d7 n matrix A with non-negative entries.\n     To conclude the proof of the lemma, we need the following standard result.\nProposition 11 (See Appendix H and [Sin12, Jer03]). For self-reducible problems, fully polynomial\napproximate integration and fully polynomial approximate sampling are equivalent.\n     This concludes the proof since weighted matchings are self-reducible (see Appendix H).\n     The above lemma establishes our goal:\nTheorem 5 (MWBM has a Compressed, Efficiently Optimizable and Samplable Solution Gener-\nator). Consider a prior over MWBM instances with n nodes. For any \u03f5 > 0, there exists a solution genera-\ntor P = {p(w) : w \u2208     W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5),L +\n\u03bbR : W  \u2192    R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for\nsome \u03bb > 0 and efficiently samplable in poly(n, 1/\u03f5) steps.\nProof. Consider an input graph G with n nodes and adjacency matrix E. The feature vector\ncorresponding to a matching can be represented as a binary matrix \u03a0 \u2208            {0, 1}n\u00d7n with \u2211j \u03a0ij = 1\nfor all i and \u2211i \u03a0ij = 1 for all j, i.e., \u03a0 is a permutation matrix. Then\n                                    MWBM = max    E \u00b7 \u03a0 = min\n                                             \u03a0\u2208Sn          \u03a0\u2208Sn \u2212E \u00b7 \u03a0 .\nTherefore, for a candidate matching s, we set \u03c8S(s) to be the matrix \u03a0 defined above. Moreover,\nthe feature vector of the graph is the negative (flattened) adjacency matrix \u2212E\u266d. The cost oracle is\nthen L(R; E) = \u2212\u2211ij EijMijRij perhaps for an unknown weight matrix Mij (see Remark 6). This\nmeans that the dimensions of the feature mappings \u03c8S, \u03c8I are polynomial in the bit complexity of\nI. Moreover, we get that \u2225\u03c8I(I)\u2225F, \u2225\u03c8S(s)\u2225F, \u2225M\u2225F \u2264          poly(n). We can employ Lemma 10 to get\nthe variance lower bound under the uniform probability distribution in the subspace induced by\nthe matrices satisfying Lemma 10, i.e., the matrices of the parameter space (see Remark 9). Finally,\n(approximate) sampling from our solution generators can be done efficiently using Lemma 11\nand hence (noisy) projected SGD will have a runtime of order poly(n, 1/\u03f5) (as in the case of\nMin-Cut).\n     We close this section with a remark about Item 3 of Assumption 1.\nRemark 9. We note that Item 3 of Assumption 1 can be weakened. We use our variance lower bound\nin order to handle inner products of the form w \u00b7 x where w will lie in the parameter space and x is the\nfeaturization of a solution that lies in some space X. Hence it is possible that w lies in a low-dimensional\nsubspace of X. For our optimization purposes, it suffices to provide variance lower bounds only in the\nsubspace where w lies into.\nG.2.2    Travelling Salesman Problem\nLet us consider a weighted clique Kn with n vertices and weight matrix W \u2208               Rn\u00d7n. A solution\nto the TSP instance W is a sequence \u03c0 : [n] \u2192           [n] of the n elements indicating the TSP tour\n(\u03c0(1), \u03c0(2), . . . , \u03c0(n), \u03c0(1)) and suffers a cost\n                                            n\u22121\n                                   L(\u03c0) =   \u2211  W  \u03c0(i),\u03c0(i+1) + W\u03c0(n),\u03c0(1) .\n                                            i=1\n                                                     38", "md": "Proposition 10 ($$[JSV04]$$). There exists a fully polynomial randomized approximation scheme for the permanent of an arbitrary n \u00d7 n matrix A with non-negative entries.\n\nTo conclude the proof of the lemma, we need the following standard result.\n\nProposition 11 (See Appendix H and $$[Sin12, Jer03]$$). For self-reducible problems, fully polynomial approximate integration and fully polynomial approximate sampling are equivalent.\n\nThis concludes the proof since weighted matchings are self-reducible (see Appendix H).\n\nThe above lemma establishes our goal:\n\nTheorem 5 (MWBM has a Compressed, Efficiently Optimizable and Samplable Solution Generator). Consider a prior over MWBM instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5),L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0 and efficiently samplable in poly(n, 1/\u03f5) steps.\n\nProof. Consider an input graph G with n nodes and adjacency matrix E. The feature vector corresponding to a matching can be represented as a binary matrix \u03a0 \u2208 {0, 1}n\u00d7n with \u2211j \u03a0ij = 1 for all i and \u2211i \u03a0ij = 1 for all j, i.e., \u03a0 is a permutation matrix. Then\n\n$$\n\\text{MWBM} = \\max_{\\Pi \\in S_n} E \\cdot \\Pi = \\min_{\\Pi \\in S_n} -E \\cdot \\Pi .\n$$\nTherefore, for a candidate matching s, we set \u03c8S(s) to be the matrix \u03a0 defined above. Moreover, the feature vector of the graph is the negative (flattened) adjacency matrix \u2212E\u266d. The cost oracle is then L(R; E) = -\u2211_{ij} E_{ij}M_{ij}R_{ij} perhaps for an unknown weight matrix M_{ij} (see Remark 6). This means that the dimensions of the feature mappings \u03c8S, \u03c8I are polynomial in the bit complexity of I. Moreover, we get that ||\u03c8I(I)||_F, ||\u03c8S(s)||_F, ||M||_F \u2264 poly(n). We can employ Lemma 10 to get the variance lower bound under the uniform probability distribution in the subspace induced by the matrices satisfying Lemma 10, i.e., the matrices of the parameter space (see Remark 9). Finally, (approximate) sampling from our solution generators can be done efficiently using Lemma 11 and hence (noisy) projected SGD will have a runtime of order poly(n, 1/\u03f5) (as in the case of Min-Cut).\n\nWe close this section with a remark about Item 3 of Assumption 1.\n\nRemark 9. We note that Item 3 of Assumption 1 can be weakened. We use our variance lower bound in order to handle inner products of the form w \u00b7 x where w will lie in the parameter space and x is the featurization of a solution that lies in some space X. Hence it is possible that w lies in a low-dimensional subspace of X. For our optimization purposes, it suffices to provide variance lower bounds only in the subspace where w lies into.\n\nG.2.2 Travelling Salesman Problem\n\nLet us consider a weighted clique K_n with n vertices and weight matrix W \u2208 R^{n\u00d7n}. A solution to the TSP instance W is a sequence \u03c0 : [n] \u2192 [n] of the n elements indicating the TSP tour (\u03c0(1), \u03c0(2), ..., \u03c0(n), \u03c0(1)) and suffers a cost\n\n$$\nL(\\pi) = \\sum_{i=1}^{n-1} W_{\\pi(i),\\pi(i+1)} + W_{\\pi(n),\\pi(1)} .\n$$\n38", "images": [], "items": [{"type": "text", "value": "Proposition 10 ($$[JSV04]$$). There exists a fully polynomial randomized approximation scheme for the permanent of an arbitrary n \u00d7 n matrix A with non-negative entries.\n\nTo conclude the proof of the lemma, we need the following standard result.\n\nProposition 11 (See Appendix H and $$[Sin12, Jer03]$$). For self-reducible problems, fully polynomial approximate integration and fully polynomial approximate sampling are equivalent.\n\nThis concludes the proof since weighted matchings are self-reducible (see Appendix H).\n\nThe above lemma establishes our goal:\n\nTheorem 5 (MWBM has a Compressed, Efficiently Optimizable and Samplable Solution Generator). Consider a prior over MWBM instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5),L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0 and efficiently samplable in poly(n, 1/\u03f5) steps.\n\nProof. Consider an input graph G with n nodes and adjacency matrix E. The feature vector corresponding to a matching can be represented as a binary matrix \u03a0 \u2208 {0, 1}n\u00d7n with \u2211j \u03a0ij = 1 for all i and \u2211i \u03a0ij = 1 for all j, i.e., \u03a0 is a permutation matrix. Then\n\n$$\n\\text{MWBM} = \\max_{\\Pi \\in S_n} E \\cdot \\Pi = \\min_{\\Pi \\in S_n} -E \\cdot \\Pi .\n$$\nTherefore, for a candidate matching s, we set \u03c8S(s) to be the matrix \u03a0 defined above. Moreover, the feature vector of the graph is the negative (flattened) adjacency matrix \u2212E\u266d. The cost oracle is then L(R; E) = -\u2211_{ij} E_{ij}M_{ij}R_{ij} perhaps for an unknown weight matrix M_{ij} (see Remark 6). This means that the dimensions of the feature mappings \u03c8S, \u03c8I are polynomial in the bit complexity of I. Moreover, we get that ||\u03c8I(I)||_F, ||\u03c8S(s)||_F, ||M||_F \u2264 poly(n). We can employ Lemma 10 to get the variance lower bound under the uniform probability distribution in the subspace induced by the matrices satisfying Lemma 10, i.e., the matrices of the parameter space (see Remark 9). Finally, (approximate) sampling from our solution generators can be done efficiently using Lemma 11 and hence (noisy) projected SGD will have a runtime of order poly(n, 1/\u03f5) (as in the case of Min-Cut).\n\nWe close this section with a remark about Item 3 of Assumption 1.\n\nRemark 9. We note that Item 3 of Assumption 1 can be weakened. We use our variance lower bound in order to handle inner products of the form w \u00b7 x where w will lie in the parameter space and x is the featurization of a solution that lies in some space X. Hence it is possible that w lies in a low-dimensional subspace of X. For our optimization purposes, it suffices to provide variance lower bounds only in the subspace where w lies into.\n\nG.2.2 Travelling Salesman Problem\n\nLet us consider a weighted clique K_n with n vertices and weight matrix W \u2208 R^{n\u00d7n}. A solution to the TSP instance W is a sequence \u03c0 : [n] \u2192 [n] of the n elements indicating the TSP tour (\u03c0(1), \u03c0(2), ..., \u03c0(n), \u03c0(1)) and suffers a cost\n\n$$\nL(\\pi) = \\sum_{i=1}^{n-1} W_{\\pi(i),\\pi(i+1)} + W_{\\pi(n),\\pi(1)} .\n$$\n38", "md": "Proposition 10 ($$[JSV04]$$). There exists a fully polynomial randomized approximation scheme for the permanent of an arbitrary n \u00d7 n matrix A with non-negative entries.\n\nTo conclude the proof of the lemma, we need the following standard result.\n\nProposition 11 (See Appendix H and $$[Sin12, Jer03]$$). For self-reducible problems, fully polynomial approximate integration and fully polynomial approximate sampling are equivalent.\n\nThis concludes the proof since weighted matchings are self-reducible (see Appendix H).\n\nThe above lemma establishes our goal:\n\nTheorem 5 (MWBM has a Compressed, Efficiently Optimizable and Samplable Solution Generator). Consider a prior over MWBM instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5),L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0 and efficiently samplable in poly(n, 1/\u03f5) steps.\n\nProof. Consider an input graph G with n nodes and adjacency matrix E. The feature vector corresponding to a matching can be represented as a binary matrix \u03a0 \u2208 {0, 1}n\u00d7n with \u2211j \u03a0ij = 1 for all i and \u2211i \u03a0ij = 1 for all j, i.e., \u03a0 is a permutation matrix. Then\n\n$$\n\\text{MWBM} = \\max_{\\Pi \\in S_n} E \\cdot \\Pi = \\min_{\\Pi \\in S_n} -E \\cdot \\Pi .\n$$\nTherefore, for a candidate matching s, we set \u03c8S(s) to be the matrix \u03a0 defined above. Moreover, the feature vector of the graph is the negative (flattened) adjacency matrix \u2212E\u266d. The cost oracle is then L(R; E) = -\u2211_{ij} E_{ij}M_{ij}R_{ij} perhaps for an unknown weight matrix M_{ij} (see Remark 6). This means that the dimensions of the feature mappings \u03c8S, \u03c8I are polynomial in the bit complexity of I. Moreover, we get that ||\u03c8I(I)||_F, ||\u03c8S(s)||_F, ||M||_F \u2264 poly(n). We can employ Lemma 10 to get the variance lower bound under the uniform probability distribution in the subspace induced by the matrices satisfying Lemma 10, i.e., the matrices of the parameter space (see Remark 9). Finally, (approximate) sampling from our solution generators can be done efficiently using Lemma 11 and hence (noisy) projected SGD will have a runtime of order poly(n, 1/\u03f5) (as in the case of Min-Cut).\n\nWe close this section with a remark about Item 3 of Assumption 1.\n\nRemark 9. We note that Item 3 of Assumption 1 can be weakened. We use our variance lower bound in order to handle inner products of the form w \u00b7 x where w will lie in the parameter space and x is the featurization of a solution that lies in some space X. Hence it is possible that w lies in a low-dimensional subspace of X. For our optimization purposes, it suffices to provide variance lower bounds only in the subspace where w lies into.\n\nG.2.2 Travelling Salesman Problem\n\nLet us consider a weighted clique K_n with n vertices and weight matrix W \u2208 R^{n\u00d7n}. A solution to the TSP instance W is a sequence \u03c0 : [n] \u2192 [n] of the n elements indicating the TSP tour (\u03c0(1), \u03c0(2), ..., \u03c0(n), \u03c0(1)) and suffers a cost\n\n$$\nL(\\pi) = \\sum_{i=1}^{n-1} W_{\\pi(i),\\pi(i+1)} + W_{\\pi(n),\\pi(1)} .\n$$\n38"}]}, {"page": 39, "text": "Crucially, the allowed sequences are a proper subset of all possible permutations. For instance,\nthe permutations with fixed points or small cycles are not allowed. In particular, the solution\nspace of TSP corresponds to the set of cyclic permutations with no trivial cycles, i.e., containing\nan n-cycle. Clearly, the number of n-cycles is (n \u2212                                                               1)!. The goal is to find a tour of minimum\ncost. Our first goal is to write the cost objective as a linear function of the weight matrix W\nand the feasible solutions, which correspond to cyclic permutations. To this end, we can think\nof each cyclic permutation \u03c0 as a cyclic permutation matrix \u03a0 \u2208                                                                                     {0, 1}n\u00d7n. Then, the desired\nlinearization is given by L(\u03a0) = W \u00b7 \u03a0 (for a fixed graph instance).\n        Our next task is to provide a Gibbs measure that generates random cyclic permutations. Let\nC  n be the space of n \u00d7 n cyclic permutation matrices. Then we have that the tour \u03a0 is drawn\nfrom\n                                                                     pW(\u03a0) = exp(W \u00b7 \u03a0)1{\u03a0 \u2208  \u2211\u03a0\u2032\u2208Cn exp(W \u00b7 \u03a0\u2032)                   Cn}       ,\nwhere W is the weight matrix. The following key lemma provides guarantees for the performance\nof our approach to TSP. This lemma allows us to show that the number of optimization steps is\npoly(n, 1/\u03f5).\nLemma 12 (Variance Lower Bound). Let U(Cn) be the uniform distribution over n \u00d7 n cyclic permu-\ntation matrices. For any matrix W \u2208                                        Rn\u00d7n, with \u2211i Wij = 0 and \u2211j Wij = 0 we have   \u2225W\u22252      F\n                                                                 Var    \u03a0\u223cU(Cn)[W \u00b7 \u03a0] \u2265                        (n \u2212       1)(n \u2212          2) .\nProof. The first step is to compute some standard statistics about cyclic permutations (see Lemma 13).\nLemma 13 and the analysis of Lemma 10 gives us that Var                                                                         \u03a0\u223cU(Cn)[W \u00b7 \u03a0] is equal to\n            \u2225W\u22252      F                 WijWab              1{i \u0338= a = j \u0338= b}                    + 1{j \u0338= b = i \u0338= a}                        + 1{i \u0338= a \u0338= j \u0338= b \u0338= i}                 .\nLet us set  n \u2212      1 + \u2211     i,j,a,b                         (n \u2212       1)(n \u2212         2)                (n \u2212      1)(n \u2212          2)                    (n \u2212      1)(n \u2212          3)\nWe have that                                                        A1 = \u2211      i,j,a,b   WijWab1{i \u0338= a = j \u0338= b} .\n  \u2211 b   Wab1{i \u0338= a}1{a = j}1{j \u0338= b} = 1{i \u0338= a}1{a = j}                                                             \u2211b    Wab1{j \u0338= b} = \u22121{i \u0338= a}1{a = j}Waj .\nHence\n                               A1 = \u2212            \u2211     WijWaj1{i \u0338= a}1{a = j} = \u2212                                     \u2211     WijWjj1{i \u0338= j} = \u2211                          W2   jj .\n                                                 i,j,a                                                                  i,j                                            j\nDue to symmetry, A2 = \u2211i,j,a,b WijWab1{j \u0338= b = i \u0338= a} = \u2211j W2A3 = \u2211       i,j,a,b  WijWab1{i \u0338= a \u0338= j \u0338= b \u0338= i} .                   jj. It remains to argue about\nWe have that\n                       \u2211 b   Wab1{i \u0338= a}1{a \u0338= j}1{j \u0338= b}1{b \u0338= i} = \u22121{i \u0338= a}1{a \u0338= j}(Waj + Wai) .\n                                                                                                       39", "md": "Crucially, the allowed sequences are a proper subset of all possible permutations. For instance, the permutations with fixed points or small cycles are not allowed. In particular, the solution space of TSP corresponds to the set of cyclic permutations with no trivial cycles, i.e., containing an n-cycle. Clearly, the number of n-cycles is $$(n - 1)!$$. The goal is to find a tour of minimum cost. Our first goal is to write the cost objective as a linear function of the weight matrix W and the feasible solutions, which correspond to cyclic permutations. To this end, we can think of each cyclic permutation \u03c0 as a cyclic permutation matrix $$\\Pi \\in \\{0, 1\\}^{n \\times n}$$. Then, the desired linearization is given by $$L(\\Pi) = W \\cdot \\Pi$$ (for a fixed graph instance).\n\nOur next task is to provide a Gibbs measure that generates random cyclic permutations. Let $$C_n$$ be the space of $$n \\times n$$ cyclic permutation matrices. Then we have that the tour $$\\Pi$$ is drawn from\n\n$$\np_W(\\Pi) = \\exp(W \\cdot \\Pi)1\\{\\Pi \\in \\sum_{\\Pi' \\in C_n} \\exp(W \\cdot \\Pi') C_n\\},\n$$\n\nwhere W is the weight matrix. The following key lemma provides guarantees for the performance of our approach to TSP. This lemma allows us to show that the number of optimization steps is poly(n, 1/\u03f5).\n\nLemma 12 (Variance Lower Bound). Let U(C_n) be the uniform distribution over $$n \\times n$$ cyclic permutation matrices. For any matrix $$W \\in \\mathbb{R}^{n \\times n}$$, with $$\\sum_i W_{ij} = 0$$ and $$\\sum_j W_{ij} = 0$$ we have\n\n$$\n\\text{Var} \\left[ \\Pi \\sim U(C_n)[W \\cdot \\Pi] \\right] \\geq \\frac{\\|W\\|_F^2}{(n - 1)(n - 2)}.\n$$\n\nProof. The first step is to compute some standard statistics about cyclic permutations (see Lemma 13).\n\nLemma 13 and the analysis of Lemma 10 gives us that $$\\text{Var}[\\Pi \\sim U(C_n)[W \\cdot \\Pi]]$$ is equal to\n\n$$\n\\|W\\|_F^2 \\sum_{i,j,a,b} W_{ij}W_{ab} \\left(1_{i \\neq a = j \\neq b} + 1_{j \\neq b = i \\neq a} + 1_{i \\neq a \\neq j \\neq b \\neq i}\\right).\n$$\n\nLet us set\n\n$$\nn - 1 + \\sum_{i,j,a,b} (n - 1)(n - 2) (n - 1)(n - 2) (n - 1)(n - 3)\n$$\n\nWe have that\n\n$$\nA_1 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{i \\neq a = j \\neq b}.\n$$\n\n$$\\sum_b W_{ab}1_{i \\neq a}1_{a = j}1_{j \\neq b} = 1_{i \\neq a}1_{a = j}$$\n\n$$\\sum_b W_{ab}1_{j \\neq b} = -1_{i \\neq a}1_{a = j}W_{aj}.$$\n\nHence\n\n$$\nA_1 = - \\sum_i W_{ij}W_{aj}1_{i \\neq a}1_{a = j} = - \\sum_i W_{ij}W_{jj}1_{i \\neq j} = \\sum_j W^2_{jj}.\n$$\n\nDue to symmetry, $$A_2 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{j \\neq b = i \\neq a} = \\sum_j W^2$$ $$A_3 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{i \\neq a \\neq j \\neq b \\neq i}$$. It remains to argue about\n\n$$\n\\sum_b W_{ab}1_{i \\neq a}1_{a \\neq j}1_{j \\neq b}1_{b \\neq i} = -1_{i \\neq a}1_{a \\neq j}(W_{aj} + W_{ai}).\n$$", "images": [], "items": [{"type": "text", "value": "Crucially, the allowed sequences are a proper subset of all possible permutations. For instance, the permutations with fixed points or small cycles are not allowed. In particular, the solution space of TSP corresponds to the set of cyclic permutations with no trivial cycles, i.e., containing an n-cycle. Clearly, the number of n-cycles is $$(n - 1)!$$. The goal is to find a tour of minimum cost. Our first goal is to write the cost objective as a linear function of the weight matrix W and the feasible solutions, which correspond to cyclic permutations. To this end, we can think of each cyclic permutation \u03c0 as a cyclic permutation matrix $$\\Pi \\in \\{0, 1\\}^{n \\times n}$$. Then, the desired linearization is given by $$L(\\Pi) = W \\cdot \\Pi$$ (for a fixed graph instance).\n\nOur next task is to provide a Gibbs measure that generates random cyclic permutations. Let $$C_n$$ be the space of $$n \\times n$$ cyclic permutation matrices. Then we have that the tour $$\\Pi$$ is drawn from\n\n$$\np_W(\\Pi) = \\exp(W \\cdot \\Pi)1\\{\\Pi \\in \\sum_{\\Pi' \\in C_n} \\exp(W \\cdot \\Pi') C_n\\},\n$$\n\nwhere W is the weight matrix. The following key lemma provides guarantees for the performance of our approach to TSP. This lemma allows us to show that the number of optimization steps is poly(n, 1/\u03f5).\n\nLemma 12 (Variance Lower Bound). Let U(C_n) be the uniform distribution over $$n \\times n$$ cyclic permutation matrices. For any matrix $$W \\in \\mathbb{R}^{n \\times n}$$, with $$\\sum_i W_{ij} = 0$$ and $$\\sum_j W_{ij} = 0$$ we have\n\n$$\n\\text{Var} \\left[ \\Pi \\sim U(C_n)[W \\cdot \\Pi] \\right] \\geq \\frac{\\|W\\|_F^2}{(n - 1)(n - 2)}.\n$$\n\nProof. The first step is to compute some standard statistics about cyclic permutations (see Lemma 13).\n\nLemma 13 and the analysis of Lemma 10 gives us that $$\\text{Var}[\\Pi \\sim U(C_n)[W \\cdot \\Pi]]$$ is equal to\n\n$$\n\\|W\\|_F^2 \\sum_{i,j,a,b} W_{ij}W_{ab} \\left(1_{i \\neq a = j \\neq b} + 1_{j \\neq b = i \\neq a} + 1_{i \\neq a \\neq j \\neq b \\neq i}\\right).\n$$\n\nLet us set\n\n$$\nn - 1 + \\sum_{i,j,a,b} (n - 1)(n - 2) (n - 1)(n - 2) (n - 1)(n - 3)\n$$\n\nWe have that\n\n$$\nA_1 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{i \\neq a = j \\neq b}.\n$$\n\n$$\\sum_b W_{ab}1_{i \\neq a}1_{a = j}1_{j \\neq b} = 1_{i \\neq a}1_{a = j}$$\n\n$$\\sum_b W_{ab}1_{j \\neq b} = -1_{i \\neq a}1_{a = j}W_{aj}.$$\n\nHence\n\n$$\nA_1 = - \\sum_i W_{ij}W_{aj}1_{i \\neq a}1_{a = j} = - \\sum_i W_{ij}W_{jj}1_{i \\neq j} = \\sum_j W^2_{jj}.\n$$\n\nDue to symmetry, $$A_2 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{j \\neq b = i \\neq a} = \\sum_j W^2$$ $$A_3 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{i \\neq a \\neq j \\neq b \\neq i}$$. It remains to argue about\n\n$$\n\\sum_b W_{ab}1_{i \\neq a}1_{a \\neq j}1_{j \\neq b}1_{b \\neq i} = -1_{i \\neq a}1_{a \\neq j}(W_{aj} + W_{ai}).\n$$", "md": "Crucially, the allowed sequences are a proper subset of all possible permutations. For instance, the permutations with fixed points or small cycles are not allowed. In particular, the solution space of TSP corresponds to the set of cyclic permutations with no trivial cycles, i.e., containing an n-cycle. Clearly, the number of n-cycles is $$(n - 1)!$$. The goal is to find a tour of minimum cost. Our first goal is to write the cost objective as a linear function of the weight matrix W and the feasible solutions, which correspond to cyclic permutations. To this end, we can think of each cyclic permutation \u03c0 as a cyclic permutation matrix $$\\Pi \\in \\{0, 1\\}^{n \\times n}$$. Then, the desired linearization is given by $$L(\\Pi) = W \\cdot \\Pi$$ (for a fixed graph instance).\n\nOur next task is to provide a Gibbs measure that generates random cyclic permutations. Let $$C_n$$ be the space of $$n \\times n$$ cyclic permutation matrices. Then we have that the tour $$\\Pi$$ is drawn from\n\n$$\np_W(\\Pi) = \\exp(W \\cdot \\Pi)1\\{\\Pi \\in \\sum_{\\Pi' \\in C_n} \\exp(W \\cdot \\Pi') C_n\\},\n$$\n\nwhere W is the weight matrix. The following key lemma provides guarantees for the performance of our approach to TSP. This lemma allows us to show that the number of optimization steps is poly(n, 1/\u03f5).\n\nLemma 12 (Variance Lower Bound). Let U(C_n) be the uniform distribution over $$n \\times n$$ cyclic permutation matrices. For any matrix $$W \\in \\mathbb{R}^{n \\times n}$$, with $$\\sum_i W_{ij} = 0$$ and $$\\sum_j W_{ij} = 0$$ we have\n\n$$\n\\text{Var} \\left[ \\Pi \\sim U(C_n)[W \\cdot \\Pi] \\right] \\geq \\frac{\\|W\\|_F^2}{(n - 1)(n - 2)}.\n$$\n\nProof. The first step is to compute some standard statistics about cyclic permutations (see Lemma 13).\n\nLemma 13 and the analysis of Lemma 10 gives us that $$\\text{Var}[\\Pi \\sim U(C_n)[W \\cdot \\Pi]]$$ is equal to\n\n$$\n\\|W\\|_F^2 \\sum_{i,j,a,b} W_{ij}W_{ab} \\left(1_{i \\neq a = j \\neq b} + 1_{j \\neq b = i \\neq a} + 1_{i \\neq a \\neq j \\neq b \\neq i}\\right).\n$$\n\nLet us set\n\n$$\nn - 1 + \\sum_{i,j,a,b} (n - 1)(n - 2) (n - 1)(n - 2) (n - 1)(n - 3)\n$$\n\nWe have that\n\n$$\nA_1 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{i \\neq a = j \\neq b}.\n$$\n\n$$\\sum_b W_{ab}1_{i \\neq a}1_{a = j}1_{j \\neq b} = 1_{i \\neq a}1_{a = j}$$\n\n$$\\sum_b W_{ab}1_{j \\neq b} = -1_{i \\neq a}1_{a = j}W_{aj}.$$\n\nHence\n\n$$\nA_1 = - \\sum_i W_{ij}W_{aj}1_{i \\neq a}1_{a = j} = - \\sum_i W_{ij}W_{jj}1_{i \\neq j} = \\sum_j W^2_{jj}.\n$$\n\nDue to symmetry, $$A_2 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{j \\neq b = i \\neq a} = \\sum_j W^2$$ $$A_3 = \\sum_{i,j,a,b} W_{ij}W_{ab}1_{i \\neq a \\neq j \\neq b \\neq i}$$. It remains to argue about\n\n$$\n\\sum_b W_{ab}1_{i \\neq a}1_{a \\neq j}1_{j \\neq b}1_{b \\neq i} = -1_{i \\neq a}1_{a \\neq j}(W_{aj} + W_{ai}).\n$$"}]}, {"page": 40, "text": "This gives that\n                                                          A3 = \u2212           \u2211      Wij(Waj + Wai)1{i \u0338= a}1{a \u0338= j} .\n                                                                           i,j,a\nNote that\n                 \u2211     WijWaj1{i \u0338= a}1{a \u0338= j} = \u2211                                     Waj1{a \u0338= j}                \u2211     Wij1{i \u0338= a} = \u2212                     \u2211      W2  aj1{a \u0338= j} .\n                 i,j,a                                                             j,a                                i                                         j,a\nThis implies that\nIn total, this gives that                                            A3 = \u2211      j\u0338=a   W2  aj + \u2211    i\u0338=a  W2   ai = 2 \u2211     j\u0338=a   W2  aj .\n                                                                F                2 \u2211j W2       jj                    2 \u2211i\u0338=j W2        ij                        F                  \u2225W\u22252      F\n      Var     \u03a0\u223cU(Cn)[W \u00b7 \u03a0] = \u2225W\u22252                    n \u2212     1 +        (n \u2212       1)(n \u2212         2) +        (n \u2212       1)(n \u2212         3) \u2265         \u2225W\u22252\n                                                                                                                                                        n \u2212     1 +        (n \u2212      1)(n \u2212      2) .\nRemark 10. We note that in TSP the conditions \u2211i Wij = 0 and \u2211j Wij = 0 are without loss of generality.\n        The next lemma is a generic lemma that states some properties of random cyclic permutation\nmatrices.\nLemma 13. Consider a uniformly random cyclic permutation matrix \u03a0. Let us fix i \u0338= j and a \u0338= b. Then\n       \u2022 E[\u03a0ij] =               n\u221211    .\n       \u2022 E[\u03a0ij\u03a0ab] = 1{i\u0338=a=j\u0338=b}        (n\u22121)(n\u22122) + 1{j\u0338=b=i\u0338=a}  (n\u22121)(n\u22122) + 1{i\u0338=a\u0338=j\u0338=b\u0338=i} (n\u22121)(n\u22123)              + 1{i=a,j=b}n\u22121          .\nProof. First, note that any matrix that corresponds to a cyclic permutation does not contain fixed\npoints and so the diagonal elements are 0 deterministically. For the first item, the number of\ncyclic permutations such that i \u2192                                               j (i.e., \u03a0ij = 1) is (n \u2212                             2)!. This implies that the desired\nexpectation is (n \u2212                       2)!/|Cn| = 1/(n \u2212                         1). For the second item, if i = a, j = b, we recover the first\nitem. Otherwise if i = a or j = b, then the expectation vanishes since we deal with permutation\nmatrices. Finally, let us consider the case where i \u0338= a and j \u0338= b. Our goal is to count the number\nof cyclic permutations with i \u2192                                        j and a \u2192              b.\n       \u2022 If i \u0338= a \u0338= j \u0338= b \u0338= i, then there are n choices to place i and n \u2212                                                                            2 choices to place a. Then\n            there are (n \u2212                  4)! possible orderings for the remaining elements. This gives an expectation\n            equal to 1/((n \u2212                      1)(n \u2212         3)).\n       \u2022 If i \u0338= a = j \u0338= b or j \u0338= b = i \u0338= a, then there are n choices for i and (n \u2212                                                                                       3)! orderings for\n            the remaining elements. Hence, the expectation is 1/((n \u2212                                                                         1)(n \u2212         2)).\n        We note that sampling from our solution generators is the reason that we cannot find an\noptimal TSP solution efficiently. In general, an algorithm that has converged to an almost optimal\nparameter W\u22c6                     has to generate samples from the Gibbs measure that is concentrated on cycles\nwith minimum weight. In this low-temperature regime, sampling is NP-hard. We are now ready\nto state our result.\n                                                                                                       40", "md": "This gives that\n$$\nA3 = - \\sum_{i,j,a} W_{ij}(W_{aj} + W_{ai})1\\{i \\neq a\\}1\\{a \\neq j} .\n$$\n\nNote that\n$$\n\\sum_{i,j,a} W_{ij}W_{aj}1\\{i \\neq a\\}1\\{a \\neq j\\} = \\sum_{j,a} W_{aj}1\\{a \\neq j\\} \\sum_{i} W_{ij}1\\{i \\neq a\\} = - \\sum_{j,a} W_{aj}^2 1\\{a \\neq j} .\n$$\n\nThis implies that\nIn total, this gives that\n$$\nA3 = \\sum_{j \\neq a} W_{aj}^2 + \\sum_{i \\neq a} W_{ai}^2 = 2 \\sum_{j \\neq a} W_{aj}^2 .\n$$\n\n$$\n\\text{Var} \\ \\Pi \\sim U(Cn)[W \\cdot \\Pi] = \\|W\\|_F^2 \\geq n - 1 + (n - 1)(n - 2) + (n - 1)(n - 3) \\geq \\|W\\|_F^2 n - 1 + (n - 1)(n - 2) .\n$$\n\nRemark 10. We note that in TSP the conditions $\\sum_i W_{ij} = 0$ and $\\sum_j W_{ij} = 0$ are without loss of generality.\nThe next lemma is a generic lemma that states some properties of random cyclic permutation matrices.\n\nLemma 13. Consider a uniformly random cyclic permutation matrix $\\Pi$. Let us fix $i \\neq j$ and $a \\neq b$. Then\n- $E[\\Pi_{ij}] = \\frac{1}{n-1}$.\n- $E[\\Pi_{ij}\\Pi_{ab}] = 1\\{i \\neq a = j \\neq b\\}(n-1)(n-2) + 1\\{j \\neq b = i \\neq a\\}(n-1)(n-2) + 1\\{i \\neq a \\neq j \\neq b \\}(n-1)(n-3) + 1\\{i = a, j = b\\}(n-1)$.\n\nProof. First, note that any matrix that corresponds to a cyclic permutation does not contain fixed points and so the diagonal elements are 0 deterministically. For the first item, the number of cyclic permutations such that $i \\rightarrow j$ (i.e., $\\Pi_{ij} = 1$) is $(n-2)!$. This implies that the desired expectation is $(n-2)!/|Cn| = 1/(n-1)$. For the second item, if $i = a, j = b$, we recover the first item. Otherwise if $i = a$ or $j = b$, then the expectation vanishes since we deal with permutation matrices. Finally, let us consider the case where $i \\neq a$ and $j \\neq b$. Our goal is to count the number of cyclic permutations with $i \\rightarrow j$ and $a \\rightarrow b$.\n- If $i \\neq a \\neq j \\neq b \\neq i$, then there are $n$ choices to place $i$ and $n-2$ choices to place $a$. Then there are $(n-4)!$ possible orderings for the remaining elements. This gives an expectation equal to $1/((n-1)(n-3))$.\n- If $i \\neq a = j \\neq b$ or $j \\neq b = i \\neq a$, then there are $n$ choices for $i$ and $(n-3)!$ orderings for the remaining elements. Hence, the expectation is $1/((n-1)(n-2))$.\n\nWe note that sampling from our solution generators is the reason that we cannot find an optimal TSP solution efficiently. In general, an algorithm that has converged to an almost optimal parameter $W^*$ has to generate samples from the Gibbs measure that is concentrated on cycles with minimum weight. In this low-temperature regime, sampling is NP-hard. We are now ready to state our result.", "images": [], "items": [{"type": "text", "value": "This gives that\n$$\nA3 = - \\sum_{i,j,a} W_{ij}(W_{aj} + W_{ai})1\\{i \\neq a\\}1\\{a \\neq j} .\n$$\n\nNote that\n$$\n\\sum_{i,j,a} W_{ij}W_{aj}1\\{i \\neq a\\}1\\{a \\neq j\\} = \\sum_{j,a} W_{aj}1\\{a \\neq j\\} \\sum_{i} W_{ij}1\\{i \\neq a\\} = - \\sum_{j,a} W_{aj}^2 1\\{a \\neq j} .\n$$\n\nThis implies that\nIn total, this gives that\n$$\nA3 = \\sum_{j \\neq a} W_{aj}^2 + \\sum_{i \\neq a} W_{ai}^2 = 2 \\sum_{j \\neq a} W_{aj}^2 .\n$$\n\n$$\n\\text{Var} \\ \\Pi \\sim U(Cn)[W \\cdot \\Pi] = \\|W\\|_F^2 \\geq n - 1 + (n - 1)(n - 2) + (n - 1)(n - 3) \\geq \\|W\\|_F^2 n - 1 + (n - 1)(n - 2) .\n$$\n\nRemark 10. We note that in TSP the conditions $\\sum_i W_{ij} = 0$ and $\\sum_j W_{ij} = 0$ are without loss of generality.\nThe next lemma is a generic lemma that states some properties of random cyclic permutation matrices.\n\nLemma 13. Consider a uniformly random cyclic permutation matrix $\\Pi$. Let us fix $i \\neq j$ and $a \\neq b$. Then\n- $E[\\Pi_{ij}] = \\frac{1}{n-1}$.\n- $E[\\Pi_{ij}\\Pi_{ab}] = 1\\{i \\neq a = j \\neq b\\}(n-1)(n-2) + 1\\{j \\neq b = i \\neq a\\}(n-1)(n-2) + 1\\{i \\neq a \\neq j \\neq b \\}(n-1)(n-3) + 1\\{i = a, j = b\\}(n-1)$.\n\nProof. First, note that any matrix that corresponds to a cyclic permutation does not contain fixed points and so the diagonal elements are 0 deterministically. For the first item, the number of cyclic permutations such that $i \\rightarrow j$ (i.e., $\\Pi_{ij} = 1$) is $(n-2)!$. This implies that the desired expectation is $(n-2)!/|Cn| = 1/(n-1)$. For the second item, if $i = a, j = b$, we recover the first item. Otherwise if $i = a$ or $j = b$, then the expectation vanishes since we deal with permutation matrices. Finally, let us consider the case where $i \\neq a$ and $j \\neq b$. Our goal is to count the number of cyclic permutations with $i \\rightarrow j$ and $a \\rightarrow b$.\n- If $i \\neq a \\neq j \\neq b \\neq i$, then there are $n$ choices to place $i$ and $n-2$ choices to place $a$. Then there are $(n-4)!$ possible orderings for the remaining elements. This gives an expectation equal to $1/((n-1)(n-3))$.\n- If $i \\neq a = j \\neq b$ or $j \\neq b = i \\neq a$, then there are $n$ choices for $i$ and $(n-3)!$ orderings for the remaining elements. Hence, the expectation is $1/((n-1)(n-2))$.\n\nWe note that sampling from our solution generators is the reason that we cannot find an optimal TSP solution efficiently. In general, an algorithm that has converged to an almost optimal parameter $W^*$ has to generate samples from the Gibbs measure that is concentrated on cycles with minimum weight. In this low-temperature regime, sampling is NP-hard. We are now ready to state our result.", "md": "This gives that\n$$\nA3 = - \\sum_{i,j,a} W_{ij}(W_{aj} + W_{ai})1\\{i \\neq a\\}1\\{a \\neq j} .\n$$\n\nNote that\n$$\n\\sum_{i,j,a} W_{ij}W_{aj}1\\{i \\neq a\\}1\\{a \\neq j\\} = \\sum_{j,a} W_{aj}1\\{a \\neq j\\} \\sum_{i} W_{ij}1\\{i \\neq a\\} = - \\sum_{j,a} W_{aj}^2 1\\{a \\neq j} .\n$$\n\nThis implies that\nIn total, this gives that\n$$\nA3 = \\sum_{j \\neq a} W_{aj}^2 + \\sum_{i \\neq a} W_{ai}^2 = 2 \\sum_{j \\neq a} W_{aj}^2 .\n$$\n\n$$\n\\text{Var} \\ \\Pi \\sim U(Cn)[W \\cdot \\Pi] = \\|W\\|_F^2 \\geq n - 1 + (n - 1)(n - 2) + (n - 1)(n - 3) \\geq \\|W\\|_F^2 n - 1 + (n - 1)(n - 2) .\n$$\n\nRemark 10. We note that in TSP the conditions $\\sum_i W_{ij} = 0$ and $\\sum_j W_{ij} = 0$ are without loss of generality.\nThe next lemma is a generic lemma that states some properties of random cyclic permutation matrices.\n\nLemma 13. Consider a uniformly random cyclic permutation matrix $\\Pi$. Let us fix $i \\neq j$ and $a \\neq b$. Then\n- $E[\\Pi_{ij}] = \\frac{1}{n-1}$.\n- $E[\\Pi_{ij}\\Pi_{ab}] = 1\\{i \\neq a = j \\neq b\\}(n-1)(n-2) + 1\\{j \\neq b = i \\neq a\\}(n-1)(n-2) + 1\\{i \\neq a \\neq j \\neq b \\}(n-1)(n-3) + 1\\{i = a, j = b\\}(n-1)$.\n\nProof. First, note that any matrix that corresponds to a cyclic permutation does not contain fixed points and so the diagonal elements are 0 deterministically. For the first item, the number of cyclic permutations such that $i \\rightarrow j$ (i.e., $\\Pi_{ij} = 1$) is $(n-2)!$. This implies that the desired expectation is $(n-2)!/|Cn| = 1/(n-1)$. For the second item, if $i = a, j = b$, we recover the first item. Otherwise if $i = a$ or $j = b$, then the expectation vanishes since we deal with permutation matrices. Finally, let us consider the case where $i \\neq a$ and $j \\neq b$. Our goal is to count the number of cyclic permutations with $i \\rightarrow j$ and $a \\rightarrow b$.\n- If $i \\neq a \\neq j \\neq b \\neq i$, then there are $n$ choices to place $i$ and $n-2$ choices to place $a$. Then there are $(n-4)!$ possible orderings for the remaining elements. This gives an expectation equal to $1/((n-1)(n-3))$.\n- If $i \\neq a = j \\neq b$ or $j \\neq b = i \\neq a$, then there are $n$ choices for $i$ and $(n-3)!$ orderings for the remaining elements. Hence, the expectation is $1/((n-1)(n-2))$.\n\nWe note that sampling from our solution generators is the reason that we cannot find an optimal TSP solution efficiently. In general, an algorithm that has converged to an almost optimal parameter $W^*$ has to generate samples from the Gibbs measure that is concentrated on cycles with minimum weight. In this low-temperature regime, sampling is NP-hard. We are now ready to state our result."}]}, {"page": 41, "text": "Theorem 6 (TSP has a Compressed, Efficiently Optimizable Solution Generator). Consider a prior\nover TSP instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208              W}\nsuch that P is complete, compressed with description poly(n)polylog(1/\u03f5) and L + \u03bbR : W  \u2192                     R is\nefficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0.\nProof. Consider an input graph G with n nodes and weighted adjacency matrix E. The feature\nvector is again a permutation matrix \u03a0 with the additional constraint that \u03a0 has to represent a\nsingle cycle (a tour over all cities). Then\n                                                TSP = min     E \u00b7 \u03a0 .\n                                                        \u03a0\u2208Cn\nThe cost function for TSP is L(\u03a0; E) = \u2211ij EijMij\u03a0ij. We refer to Theorem 5 for the details about\nthe feature mappings. We can finally use Lemma 12 to obtain a variance lower bound (in the\nsubspace induced by the parameters satisfying this lemma, see Remark 9) under the uniform\nmeasure over the space of cyclic permutation matrices Cn.\nH      Sampling and Counting\nIn this section, we give a quick overview of the connections between approximate sampling and\ncounting. For a formal treatment, we refer to [Sin12].\n     In what follows, \u03c3 may be thought of as an encoding of an instance of some combinatorial\nproblem, and the \u03c9 of interest are encodings of the structures we wish to generate. Consider a\nweight function W and assume that W(\u03c3, \u03c9) is computable in time polynomial in |\u03c3|.\nDefinition 4 (Approximate Sampling). A fully polynomial approximate sampler for (\u2126\u03c3, \u03c0\u03c3) is a\nProbabilistic Turing Machine which, on inputs \u03c3 and \u03f5 \u2208         Q+ (0 < \u03f5 \u2264     1), outputs \u03c9 \u2208   \u03a3\u22c6, according to\na measure \u00b5\u03c3 satisfying TV(\u03c0\u03c3, \u00b5\u03c3) \u2264        \u03f5, in time bounded by a bivariate polynomial in |\u03c3| and log(1/\u03f5).\n     One of the main applications of sampling is to approximate integration. In our setting this\nmeans estimating Z(\u03c3) to some specified relative error.\nDefinition 5 (Approximate Integration). A fully polynomial randomized approximation scheme for\n Z(\u03c3) is a Probabilistic Turing Machine which on input \u03c3, \u03f5, outputs an estimate          Z so that\n                                    Pr[Z/(1 + \u03f5) \u2264     Z \u2264   (1 + \u03f5)Z] \u2265   3/4 ,\nand which runs in time polynomial in |\u03c3| and 1/\u03f5.\nDefinition 6 (Self-Reducible Problems). An NP search problem is self-reducible if the set of solutions\ncan be partitioned into polynomially many sets each of which is in a one-to-one correspondence with the\nset of solutions of a smaller instance of the problem, and the polynomial size set of smaller instances are\nefficiently computable.\n     For instance, consider the relation MATCH which associates with an undirected graph G all\nmatchings (independent sets of edges) of G. Then MATCH is self-reducible since, for any edge\n e = (u, v) \u2208  E(G), we have that\n                          MATCH(G) = MATCH(G1) \u222a        {M \u222a   {e} : M \u2208    MATCH(G2)} ,\nwhere G1 is the graph obtained by deleting e and G2 is the graph obtained be deleting both u\nand v together with all their incident edges.\n                                                         41", "md": "# Math Equations and Text\n\n## Theorem 6 (TSP has a Compressed, Efficiently Optimizable Solution Generator)\n\nConsider a prior over TSP instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0.\n\n### Proof:\n\nConsider an input graph G with n nodes and weighted adjacency matrix E. The feature vector is again a permutation matrix \u03a0 with the additional constraint that \u03a0 has to represent a single cycle (a tour over all cities). Then\n\n$$\nTSP = \\min_{\\Pi \\in Cn} E \\cdot \\Pi .\n$$\nThe cost function for TSP is L(\u03a0; E) = \u2211ij EijMij\u03a0ij. We refer to Theorem 5 for the details about the feature mappings. We can finally use Lemma 12 to obtain a variance lower bound (in the subspace induced by the parameters satisfying this lemma, see Remark 9) under the uniform measure over the space of cyclic permutation matrices Cn.\n\n### Sampling and Counting\n\nIn this section, we give a quick overview of the connections between approximate sampling and counting. For a formal treatment, we refer to [Sin12].\n\nIn what follows, \u03c3 may be thought of as an encoding of an instance of some combinatorial problem, and the \u03c9 of interest are encodings of the structures we wish to generate. Consider a weight function W and assume that W(\u03c3, \u03c9) is computable in time polynomial in |\u03c3|.\n\n#### Definition 4 (Approximate Sampling)\n\nA fully polynomial approximate sampler for (\u2126\u03c3, \u03c0\u03c3) is a Probabilistic Turing Machine which, on inputs \u03c3 and \u03f5 \u2208 Q+ (0 < \u03f5 \u2264 1), outputs \u03c9 \u2208 \u03a3\u22c6, according to a measure \u00b5\u03c3 satisfying TV(\u03c0\u03c3, \u00b5\u03c3) \u2264 \u03f5, in time bounded by a bivariate polynomial in |\u03c3| and log(1/\u03f5).\n\nOne of the main applications of sampling is to approximate integration. In our setting this means estimating Z(\u03c3) to some specified relative error.\n\n#### Definition 5 (Approximate Integration)\n\nA fully polynomial randomized approximation scheme for Z(\u03c3) is a Probabilistic Turing Machine which on input \u03c3, \u03f5, outputs an estimate Z so that Pr[Z/(1 + \u03f5) \u2264 Z \u2264 (1 + \u03f5)Z] \u2265 3/4, and which runs in time polynomial in |\u03c3| and 1/\u03f5.\n\n#### Definition 6 (Self-Reducible Problems)\n\nAn NP search problem is self-reducible if the set of solutions can be partitioned into polynomially many sets each of which is in a one-to-one correspondence with the set of solutions of a smaller instance of the problem, and the polynomial size set of smaller instances are efficiently computable.\n\nFor instance, consider the relation MATCH which associates with an undirected graph G all matchings (independent sets of edges) of G. Then MATCH is self-reducible since, for any edge e = (u, v) \u2208 E(G), we have that\n\n$$\nMATCH(G) = MATCH(G1) \\cup \\{M \\cup \\{e\\} : M \\in MATCH(G2)\\} ,\n$$\nwhere G1 is the graph obtained by deleting e and G2 is the graph obtained by deleting both u and v together with all their incident edges.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Theorem 6 (TSP has a Compressed, Efficiently Optimizable Solution Generator)", "md": "## Theorem 6 (TSP has a Compressed, Efficiently Optimizable Solution Generator)"}, {"type": "text", "value": "Consider a prior over TSP instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0.", "md": "Consider a prior over TSP instances with n nodes. For any \u03f5 > 0, there exists a solution generator P = {p(w) : w \u2208 W} such that P is complete, compressed with description poly(n)polylog(1/\u03f5) and L + \u03bbR : W \u2192 R is efficiently optimizable via projected stochastic gradient descent in poly(n, 1/\u03f5) steps for some \u03bb > 0."}, {"type": "heading", "lvl": 3, "value": "Proof:", "md": "### Proof:"}, {"type": "text", "value": "Consider an input graph G with n nodes and weighted adjacency matrix E. The feature vector is again a permutation matrix \u03a0 with the additional constraint that \u03a0 has to represent a single cycle (a tour over all cities). Then\n\n$$\nTSP = \\min_{\\Pi \\in Cn} E \\cdot \\Pi .\n$$\nThe cost function for TSP is L(\u03a0; E) = \u2211ij EijMij\u03a0ij. We refer to Theorem 5 for the details about the feature mappings. We can finally use Lemma 12 to obtain a variance lower bound (in the subspace induced by the parameters satisfying this lemma, see Remark 9) under the uniform measure over the space of cyclic permutation matrices Cn.", "md": "Consider an input graph G with n nodes and weighted adjacency matrix E. The feature vector is again a permutation matrix \u03a0 with the additional constraint that \u03a0 has to represent a single cycle (a tour over all cities). Then\n\n$$\nTSP = \\min_{\\Pi \\in Cn} E \\cdot \\Pi .\n$$\nThe cost function for TSP is L(\u03a0; E) = \u2211ij EijMij\u03a0ij. We refer to Theorem 5 for the details about the feature mappings. We can finally use Lemma 12 to obtain a variance lower bound (in the subspace induced by the parameters satisfying this lemma, see Remark 9) under the uniform measure over the space of cyclic permutation matrices Cn."}, {"type": "heading", "lvl": 3, "value": "Sampling and Counting", "md": "### Sampling and Counting"}, {"type": "text", "value": "In this section, we give a quick overview of the connections between approximate sampling and counting. For a formal treatment, we refer to [Sin12].\n\nIn what follows, \u03c3 may be thought of as an encoding of an instance of some combinatorial problem, and the \u03c9 of interest are encodings of the structures we wish to generate. Consider a weight function W and assume that W(\u03c3, \u03c9) is computable in time polynomial in |\u03c3|.", "md": "In this section, we give a quick overview of the connections between approximate sampling and counting. For a formal treatment, we refer to [Sin12].\n\nIn what follows, \u03c3 may be thought of as an encoding of an instance of some combinatorial problem, and the \u03c9 of interest are encodings of the structures we wish to generate. Consider a weight function W and assume that W(\u03c3, \u03c9) is computable in time polynomial in |\u03c3|."}, {"type": "heading", "lvl": 4, "value": "Definition 4 (Approximate Sampling)", "md": "#### Definition 4 (Approximate Sampling)"}, {"type": "text", "value": "A fully polynomial approximate sampler for (\u2126\u03c3, \u03c0\u03c3) is a Probabilistic Turing Machine which, on inputs \u03c3 and \u03f5 \u2208 Q+ (0 < \u03f5 \u2264 1), outputs \u03c9 \u2208 \u03a3\u22c6, according to a measure \u00b5\u03c3 satisfying TV(\u03c0\u03c3, \u00b5\u03c3) \u2264 \u03f5, in time bounded by a bivariate polynomial in |\u03c3| and log(1/\u03f5).\n\nOne of the main applications of sampling is to approximate integration. In our setting this means estimating Z(\u03c3) to some specified relative error.", "md": "A fully polynomial approximate sampler for (\u2126\u03c3, \u03c0\u03c3) is a Probabilistic Turing Machine which, on inputs \u03c3 and \u03f5 \u2208 Q+ (0 < \u03f5 \u2264 1), outputs \u03c9 \u2208 \u03a3\u22c6, according to a measure \u00b5\u03c3 satisfying TV(\u03c0\u03c3, \u00b5\u03c3) \u2264 \u03f5, in time bounded by a bivariate polynomial in |\u03c3| and log(1/\u03f5).\n\nOne of the main applications of sampling is to approximate integration. In our setting this means estimating Z(\u03c3) to some specified relative error."}, {"type": "heading", "lvl": 4, "value": "Definition 5 (Approximate Integration)", "md": "#### Definition 5 (Approximate Integration)"}, {"type": "text", "value": "A fully polynomial randomized approximation scheme for Z(\u03c3) is a Probabilistic Turing Machine which on input \u03c3, \u03f5, outputs an estimate Z so that Pr[Z/(1 + \u03f5) \u2264 Z \u2264 (1 + \u03f5)Z] \u2265 3/4, and which runs in time polynomial in |\u03c3| and 1/\u03f5.", "md": "A fully polynomial randomized approximation scheme for Z(\u03c3) is a Probabilistic Turing Machine which on input \u03c3, \u03f5, outputs an estimate Z so that Pr[Z/(1 + \u03f5) \u2264 Z \u2264 (1 + \u03f5)Z] \u2265 3/4, and which runs in time polynomial in |\u03c3| and 1/\u03f5."}, {"type": "heading", "lvl": 4, "value": "Definition 6 (Self-Reducible Problems)", "md": "#### Definition 6 (Self-Reducible Problems)"}, {"type": "text", "value": "An NP search problem is self-reducible if the set of solutions can be partitioned into polynomially many sets each of which is in a one-to-one correspondence with the set of solutions of a smaller instance of the problem, and the polynomial size set of smaller instances are efficiently computable.\n\nFor instance, consider the relation MATCH which associates with an undirected graph G all matchings (independent sets of edges) of G. Then MATCH is self-reducible since, for any edge e = (u, v) \u2208 E(G), we have that\n\n$$\nMATCH(G) = MATCH(G1) \\cup \\{M \\cup \\{e\\} : M \\in MATCH(G2)\\} ,\n$$\nwhere G1 is the graph obtained by deleting e and G2 is the graph obtained by deleting both u and v together with all their incident edges.", "md": "An NP search problem is self-reducible if the set of solutions can be partitioned into polynomially many sets each of which is in a one-to-one correspondence with the set of solutions of a smaller instance of the problem, and the polynomial size set of smaller instances are efficiently computable.\n\nFor instance, consider the relation MATCH which associates with an undirected graph G all matchings (independent sets of edges) of G. Then MATCH is self-reducible since, for any edge e = (u, v) \u2208 E(G), we have that\n\n$$\nMATCH(G) = MATCH(G1) \\cup \\{M \\cup \\{e\\} : M \\in MATCH(G2)\\} ,\n$$\nwhere G1 is the graph obtained by deleting e and G2 is the graph obtained by deleting both u and v together with all their incident edges."}]}, {"page": 42, "text": "Theorem 7 (See Corollary 3.16 in [Sin12]). For self-reducible problems, approximate integration and\ngood sampling are equivalent.\n     We remark that the above result holds for the more general class of self-partitionable prob-\nlems.\nI    Details of the Experimental Evaluation\nWe investigate the effect of the entropy regularizer (see Equation (2)) in a very simple setting:\nwe try to find the Max-Cut of a fixed graph G, i.e., the support of the prior R is a single graph.\nWe show that while the unregularized objective is often \u201cstuck\u201d at sub-optimal solutions \u2013 and\nthis happens even for very small instances (15 nodes) \u2013 of the Max-Cut problem, the regularized\nversion (with the fast/slow mixture scheme) is able to find the optimal solutions. We consider an\ninstance randomly generated by the Erd\u02dd         os\u2013R\u00e9nyi model G(n, p) and then optimize the \u201cvanilla\u201d\nloss L and the regularized loss L\u03bb defined in Equation (3). The solutions are vectors s \u2208                  {\u00b11}n.\nWe first use the feature mapping \u03c8S(s) = (ss\u22a4)\u266d               described in Section 1.1 and an exponential\nfamily solution generator that samples a solution s with probability \u221d                 exp(w \u00b7 \u03c8S(s)) for some\nweight vector w \u2208      Rn2. We also consider optimizing a simple 3-layer ReLU network as solution\ngenerator with input s \u2208       {\u00b11}n on the same random graphs. We generate 100 random G(n, p)\ngraphs with n = 15 nodes and p = 0.5 and train solution generators using both the \u201dvanilla\u201d\nand the entropy-regularized loss functions. We perform 600 iterations and, for the entropy reg-\nularization, we progressively decrease the regularization weight, starting from 10, and dividing\nit by 2 every 60 iterations. We used a fast/slow mixing with mixture probability 0.2 and inverse\ntemperature rho=0.03 (see Figure 3). For convenience, we present a PyTorch implementation of\nour simple 3-layer ReLU network here3.\n     Out of the 100 trials we found that our proposed objective was always able to find the optimal\ncut while the model trained with the vanilla loss was able to find it for approximately 65% of\nthe graphs (for 65 out of 100 using the linear network and for 66 using the ReLU network). In\nFigure 2 we show two instances where the model trained with the \u201dvanilla\u201d loss gets stuck on a\nsub-optimal solution while the entropy-regularized one succeeds in finding the optimal solution.\nOur experiments show that the regularization term and the fast/slow mixture scheme that we\nintroduced to achieve our main theoretical convergence result, see Section 3 and Proposition 4,\nare potentially useful for training more realistic models for bigger instances and we leave more\nextensive experimental evaluation as an interesting direction for future work.\n     We note that, similarly to our theoretical results, our sampler in this experimental section\nis of the form escore(s;w), where s \u2208       {\u22121, 1}n (here n is the number of nodes in the graph) is a\ncandidate solution of the Max-Cut problem. The function used is a 3-layer MLP (see Figure 3).\nSince the instances that we consider here are small (n = 15) we can explicitly compute the density\n(score) of every solution and use that to compute the expected gradient. The main message of\nthe current experimental section is that even for very small instances of Max-Cut (i.e., with 15\nnodes), optimizing the vanilla objective is not sufficient and the iteration gets trapped in local\noptima. In contrast, our entropy regularized always manages to find the optimal cut.\n   3For more details we refer to our full code, that is available in the Supplementary Material of the NeurIPS 2023\nProceedings and the GitHub repository AlkisK/Solution-Samplers.\n                                                         42", "md": "# Math Equations and Text\n\n## Theorem 7\n\n(See Corollary 3.16 in [Sin12]). For self-reducible problems, approximate integration and good sampling are equivalent.\n\nWe remark that the above result holds for the more general class of self-partitionable problems.\n\n### Details of the Experimental Evaluation\n\nWe investigate the effect of the entropy regularizer (see Equation (2)) in a very simple setting: we try to find the Max-Cut of a fixed graph G, i.e., the support of the prior R is a single graph. We show that while the unregularized objective is often \u201cstuck\u201d at sub-optimal solutions \u2013 and this happens even for very small instances (15 nodes) \u2013 of the Max-Cut problem, the regularized version (with the fast/slow mixture scheme) is able to find the optimal solutions. We consider an instance randomly generated by the Erd\u0151s\u2013R\u00e9nyi model G(n, p) and then optimize the \u201cvanilla\u201d loss L and the regularized loss L\u03bb defined in Equation (3). The solutions are vectors s \u2208 {\u00b11}n.\n\nWe first use the feature mapping $$\\psi_S(s) = (ss^\u22a4)^\u266d$$ described in Section 1.1 and an exponential family solution generator that samples a solution s with probability proportional to $$\\exp(w \\cdot \\psi_S(s))$$ for some weight vector $$w \\in \\mathbb{R}^{n^2}$$. We also consider optimizing a simple 3-layer ReLU network as a solution generator with input $$s \\in {\u00b11}^n$$ on the same random graphs. We generate 100 random G(n, p) graphs with n = 15 nodes and p = 0.5 and train solution generators using both the \"vanilla\" and the entropy-regularized loss functions. We perform 600 iterations and, for the entropy regularization, we progressively decrease the regularization weight, starting from 10, and dividing it by 2 every 60 iterations. We used a fast/slow mixing with a mixture probability of 0.2 and inverse temperature rho=0.03 (see Figure 3). For convenience, we present a PyTorch implementation of our simple 3-layer ReLU network here.\n\nOut of the 100 trials, we found that our proposed objective was always able to find the optimal cut while the model trained with the vanilla loss was able to find it for approximately 65% of the graphs (65 out of 100 using the linear network and 66 using the ReLU network). In Figure 2, we show two instances where the model trained with the \"vanilla\" loss gets stuck on a sub-optimal solution while the entropy-regularized one succeeds in finding the optimal solution.\n\nOur experiments show that the regularization term and the fast/slow mixture scheme that we introduced to achieve our main theoretical convergence result are potentially useful for training more realistic models for bigger instances, and we leave more extensive experimental evaluation as an interesting direction for future work.\n\nWe note that, similarly to our theoretical results, our sampler in this experimental section is of the form $$escore(s;w)$$, where $$s \\in {\u22121, 1}^n$$ (here n is the number of nodes in the graph) is a candidate solution of the Max-Cut problem. The function used is a 3-layer MLP. Since the instances that we consider here are small (n = 15), we can explicitly compute the density (score) of every solution and use that to compute the expected gradient. The main message of the current experimental section is that even for very small instances of Max-Cut (i.e., with 15 nodes), optimizing the vanilla objective is not sufficient and the iteration gets trapped in local optima. In contrast, our entropy regularized always manages to find the optimal cut.\n\nFor more details, we refer to our full code, which is available in the Supplementary Material of the NeurIPS 2023 Proceedings and the GitHub repository AlkisK/Solution-Samplers.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Theorem 7", "md": "## Theorem 7"}, {"type": "text", "value": "(See Corollary 3.16 in [Sin12]). For self-reducible problems, approximate integration and good sampling are equivalent.\n\nWe remark that the above result holds for the more general class of self-partitionable problems.", "md": "(See Corollary 3.16 in [Sin12]). For self-reducible problems, approximate integration and good sampling are equivalent.\n\nWe remark that the above result holds for the more general class of self-partitionable problems."}, {"type": "heading", "lvl": 3, "value": "Details of the Experimental Evaluation", "md": "### Details of the Experimental Evaluation"}, {"type": "text", "value": "We investigate the effect of the entropy regularizer (see Equation (2)) in a very simple setting: we try to find the Max-Cut of a fixed graph G, i.e., the support of the prior R is a single graph. We show that while the unregularized objective is often \u201cstuck\u201d at sub-optimal solutions \u2013 and this happens even for very small instances (15 nodes) \u2013 of the Max-Cut problem, the regularized version (with the fast/slow mixture scheme) is able to find the optimal solutions. We consider an instance randomly generated by the Erd\u0151s\u2013R\u00e9nyi model G(n, p) and then optimize the \u201cvanilla\u201d loss L and the regularized loss L\u03bb defined in Equation (3). The solutions are vectors s \u2208 {\u00b11}n.\n\nWe first use the feature mapping $$\\psi_S(s) = (ss^\u22a4)^\u266d$$ described in Section 1.1 and an exponential family solution generator that samples a solution s with probability proportional to $$\\exp(w \\cdot \\psi_S(s))$$ for some weight vector $$w \\in \\mathbb{R}^{n^2}$$. We also consider optimizing a simple 3-layer ReLU network as a solution generator with input $$s \\in {\u00b11}^n$$ on the same random graphs. We generate 100 random G(n, p) graphs with n = 15 nodes and p = 0.5 and train solution generators using both the \"vanilla\" and the entropy-regularized loss functions. We perform 600 iterations and, for the entropy regularization, we progressively decrease the regularization weight, starting from 10, and dividing it by 2 every 60 iterations. We used a fast/slow mixing with a mixture probability of 0.2 and inverse temperature rho=0.03 (see Figure 3). For convenience, we present a PyTorch implementation of our simple 3-layer ReLU network here.\n\nOut of the 100 trials, we found that our proposed objective was always able to find the optimal cut while the model trained with the vanilla loss was able to find it for approximately 65% of the graphs (65 out of 100 using the linear network and 66 using the ReLU network). In Figure 2, we show two instances where the model trained with the \"vanilla\" loss gets stuck on a sub-optimal solution while the entropy-regularized one succeeds in finding the optimal solution.\n\nOur experiments show that the regularization term and the fast/slow mixture scheme that we introduced to achieve our main theoretical convergence result are potentially useful for training more realistic models for bigger instances, and we leave more extensive experimental evaluation as an interesting direction for future work.\n\nWe note that, similarly to our theoretical results, our sampler in this experimental section is of the form $$escore(s;w)$$, where $$s \\in {\u22121, 1}^n$$ (here n is the number of nodes in the graph) is a candidate solution of the Max-Cut problem. The function used is a 3-layer MLP. Since the instances that we consider here are small (n = 15), we can explicitly compute the density (score) of every solution and use that to compute the expected gradient. The main message of the current experimental section is that even for very small instances of Max-Cut (i.e., with 15 nodes), optimizing the vanilla objective is not sufficient and the iteration gets trapped in local optima. In contrast, our entropy regularized always manages to find the optimal cut.\n\nFor more details, we refer to our full code, which is available in the Supplementary Material of the NeurIPS 2023 Proceedings and the GitHub repository AlkisK/Solution-Samplers.", "md": "We investigate the effect of the entropy regularizer (see Equation (2)) in a very simple setting: we try to find the Max-Cut of a fixed graph G, i.e., the support of the prior R is a single graph. We show that while the unregularized objective is often \u201cstuck\u201d at sub-optimal solutions \u2013 and this happens even for very small instances (15 nodes) \u2013 of the Max-Cut problem, the regularized version (with the fast/slow mixture scheme) is able to find the optimal solutions. We consider an instance randomly generated by the Erd\u0151s\u2013R\u00e9nyi model G(n, p) and then optimize the \u201cvanilla\u201d loss L and the regularized loss L\u03bb defined in Equation (3). The solutions are vectors s \u2208 {\u00b11}n.\n\nWe first use the feature mapping $$\\psi_S(s) = (ss^\u22a4)^\u266d$$ described in Section 1.1 and an exponential family solution generator that samples a solution s with probability proportional to $$\\exp(w \\cdot \\psi_S(s))$$ for some weight vector $$w \\in \\mathbb{R}^{n^2}$$. We also consider optimizing a simple 3-layer ReLU network as a solution generator with input $$s \\in {\u00b11}^n$$ on the same random graphs. We generate 100 random G(n, p) graphs with n = 15 nodes and p = 0.5 and train solution generators using both the \"vanilla\" and the entropy-regularized loss functions. We perform 600 iterations and, for the entropy regularization, we progressively decrease the regularization weight, starting from 10, and dividing it by 2 every 60 iterations. We used a fast/slow mixing with a mixture probability of 0.2 and inverse temperature rho=0.03 (see Figure 3). For convenience, we present a PyTorch implementation of our simple 3-layer ReLU network here.\n\nOut of the 100 trials, we found that our proposed objective was always able to find the optimal cut while the model trained with the vanilla loss was able to find it for approximately 65% of the graphs (65 out of 100 using the linear network and 66 using the ReLU network). In Figure 2, we show two instances where the model trained with the \"vanilla\" loss gets stuck on a sub-optimal solution while the entropy-regularized one succeeds in finding the optimal solution.\n\nOur experiments show that the regularization term and the fast/slow mixture scheme that we introduced to achieve our main theoretical convergence result are potentially useful for training more realistic models for bigger instances, and we leave more extensive experimental evaluation as an interesting direction for future work.\n\nWe note that, similarly to our theoretical results, our sampler in this experimental section is of the form $$escore(s;w)$$, where $$s \\in {\u22121, 1}^n$$ (here n is the number of nodes in the graph) is a candidate solution of the Max-Cut problem. The function used is a 3-layer MLP. Since the instances that we consider here are small (n = 15), we can explicitly compute the density (score) of every solution and use that to compute the expected gradient. The main message of the current experimental section is that even for very small instances of Max-Cut (i.e., with 15 nodes), optimizing the vanilla objective is not sufficient and the iteration gets trapped in local optima. In contrast, our entropy regularized always manages to find the optimal cut.\n\nFor more details, we refer to our full code, which is available in the Supplementary Material of the NeurIPS 2023 Proceedings and the GitHub repository AlkisK/Solution-Samplers."}]}, {"page": 43, "text": "class FastSlowMixture(torch.nn.Module):\ndef __init__(self, dimension, rho):\n      \"\"\"\n      The Model parameters.\n      \"\"\"\n      super().__init__()\n      self.l1 = torch.nn.Parameter(torch.empty(30, dimension))\n      torch.nn.init.kaiming_uniform_(self.l1, a=5**0.5)\n      self.l2 = torch.nn.Parameter(torch.empty(10, 30))\n      torch.nn.init.kaiming_uniform_(self.l2, a=5**0.5)\n      self.l3 = torch.nn.Parameter(torch.empty(1, 10))\n      torch.nn.init.kaiming_uniform_(self.l3, a=5**0.5)\n      self.a2 = torch.nn.ReLU()\n      self.a1 = torch.nn.ReLU()\n      self.rho = rho\ndef forward(self, x, is_cold=True):\n      temp = self.rho * (1. - is_cold) + is_cold\n      out = x\n      out = torch.nn.functional.linear(out, temp * self.l1)\n      out = self.a1(out)\n      out = torch.nn.functional.linear(out, temp * self.l2)\n      out = self.a2(out)\n      out = torch.nn.functional.linear(out, temp * self.l3)\n      return out\n Figure 3: Our implementation of the fast/slow network. The output of the network is the log-\n density (score) of a solution s \u2208 {\u00b11}dimension. If evaluated with the is-cold set to False, the\n parameters of every linear layer are re-scaled by the inverse temperature rho.\n                                               43", "md": "\\[\\text{class FastSlowMixture(torch.nn.Module):}\n\n\\[\\text{def __init__(self, dimension, rho):}\n\n\\[\\text{      \"\"\"}\n\n\\[\\text{      The Model parameters.}\n\n\\[\\text{      \"\"\"}\n\n\\[\\text{      super().__init__()}\n\n\\[\\text{      self.l1 = torch.nn.Parameter(torch.empty(30, dimension))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l1, a=5**0.5)}\n\n\\[\\text{      self.l2 = torch.nn.Parameter(torch.empty(10, 30))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l2, a=5**0.5)}\n\n\\[\\text{      self.l3 = torch.nn.Parameter(torch.empty(1, 10))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l3, a=5**0.5)}\n\n\\[\\text{      self.a2 = torch.nn.ReLU()}\n\n\\[\\text{      self.a1 = torch.nn.ReLU()}\n\n\\[\\text{      self.rho = rho}\n\n\\[\\text{def forward(self, x, is_cold=True):}\n\n\\[\\text{      temp = self.rho * (1. - is_cold) + is_cold}\n\n\\[\\text{      out = x}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l1)}\n\n\\[\\text{      out = self.a1(out)}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l2)}\n\n\\[\\text{      out = self.a2(out)}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l3)}\n\n\\[\\text{      return out}\n\nFigure 3: Our implementation of the fast/slow network. The output of the network is the log-\n\ndensity (score) of a solution \\(s \\in \\{\u00b11\\}dimension\\). If evaluated with the is-cold set to False, the\n\nparameters of every linear layer are re-scaled by the inverse temperature rho.\n\n43", "images": [], "items": [{"type": "text", "value": "\\[\\text{class FastSlowMixture(torch.nn.Module):}\n\n\\[\\text{def __init__(self, dimension, rho):}\n\n\\[\\text{      \"\"\"}\n\n\\[\\text{      The Model parameters.}\n\n\\[\\text{      \"\"\"}\n\n\\[\\text{      super().__init__()}\n\n\\[\\text{      self.l1 = torch.nn.Parameter(torch.empty(30, dimension))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l1, a=5**0.5)}\n\n\\[\\text{      self.l2 = torch.nn.Parameter(torch.empty(10, 30))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l2, a=5**0.5)}\n\n\\[\\text{      self.l3 = torch.nn.Parameter(torch.empty(1, 10))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l3, a=5**0.5)}\n\n\\[\\text{      self.a2 = torch.nn.ReLU()}\n\n\\[\\text{      self.a1 = torch.nn.ReLU()}\n\n\\[\\text{      self.rho = rho}\n\n\\[\\text{def forward(self, x, is_cold=True):}\n\n\\[\\text{      temp = self.rho * (1. - is_cold) + is_cold}\n\n\\[\\text{      out = x}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l1)}\n\n\\[\\text{      out = self.a1(out)}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l2)}\n\n\\[\\text{      out = self.a2(out)}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l3)}\n\n\\[\\text{      return out}\n\nFigure 3: Our implementation of the fast/slow network. The output of the network is the log-\n\ndensity (score) of a solution \\(s \\in \\{\u00b11\\}dimension\\). If evaluated with the is-cold set to False, the\n\nparameters of every linear layer are re-scaled by the inverse temperature rho.\n\n43", "md": "\\[\\text{class FastSlowMixture(torch.nn.Module):}\n\n\\[\\text{def __init__(self, dimension, rho):}\n\n\\[\\text{      \"\"\"}\n\n\\[\\text{      The Model parameters.}\n\n\\[\\text{      \"\"\"}\n\n\\[\\text{      super().__init__()}\n\n\\[\\text{      self.l1 = torch.nn.Parameter(torch.empty(30, dimension))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l1, a=5**0.5)}\n\n\\[\\text{      self.l2 = torch.nn.Parameter(torch.empty(10, 30))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l2, a=5**0.5)}\n\n\\[\\text{      self.l3 = torch.nn.Parameter(torch.empty(1, 10))}\n\n\\[\\text{      torch.nn.init.kaiming_uniform_(self.l3, a=5**0.5)}\n\n\\[\\text{      self.a2 = torch.nn.ReLU()}\n\n\\[\\text{      self.a1 = torch.nn.ReLU()}\n\n\\[\\text{      self.rho = rho}\n\n\\[\\text{def forward(self, x, is_cold=True):}\n\n\\[\\text{      temp = self.rho * (1. - is_cold) + is_cold}\n\n\\[\\text{      out = x}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l1)}\n\n\\[\\text{      out = self.a1(out)}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l2)}\n\n\\[\\text{      out = self.a2(out)}\n\n\\[\\text{      out = torch.nn.functional.linear(out, temp * self.l3)}\n\n\\[\\text{      return out}\n\nFigure 3: Our implementation of the fast/slow network. The output of the network is the log-\n\ndensity (score) of a solution \\(s \\in \\{\u00b11\\}dimension\\). If evaluated with the is-cold set to False, the\n\nparameters of every linear layer are re-scaled by the inverse temperature rho.\n\n43"}]}], "job_id": "a89a9bcf-0b6c-42c2-a377-a6d2ca8e893b", "file_path": "./corpus/2310.05309.pdf"}