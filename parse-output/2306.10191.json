{"pages": [{"page": 1, "text": "                          Neural Priming for Sample-Efficient Adaptation\n                                                Matthew Wallingford\u2217\u2020         Vivek Ramanujan\u2217\u2020\n                            Alex Fang\u2020      Aditya Kusupati\u2020        Roozbeh Mottaghi\u2020         Aniruddha Kembhavi\u22c4\n                                                      Ludwig Schmidt\u2020\u22c4\u2021        Ali Farhadi\u2020\n                                   \u2020University of Washington       \u22c4PRIOR, Allen Institute for AI       \u2021LAION\narXiv:2306.10191v3  [cs.LG]  5 Dec 2023            {mcw244,ramanv}@cs.washington.edu\n                                                                    Abstract\n                              We propose Neural Priming, a technique for adapting large pretrained models\n                              to distribution shifts and downstream tasks given few or no labeled examples.\n                              Presented with class names or unlabeled test samples, Neural Priming enables\n                              the model to recall and conditions its parameters on relevant data seen through-\n                              out pretraining, thereby priming it for the test distribution. Neural Priming can\n                              be performed at inference, even for pretraining datasets as large as LAION-2B.\n                              Performing lightweight updates on the recalled data significantly improves ac-\n                              curacy across a variety of distribution shift and transfer learning benchmarks.\n                              Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on\n                              ImageNet and 3.81% accuracy improvement on average across standard transfer\n                              learning benchmarks. Further, using Neural Priming at inference to adapt to dis-\n                              tribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These\n                              results demonstrate the effectiveness of Neural Priming in addressing the chal-\n                              lenge of limited labeled data and changing distributions. Code is available at\n                              https://github.com/RAIVNLab/neural-priming.\n                     1   Introduction\n                     Humans have a vast store of prior experience which we draw on to flexibly perform a diverse range\n                     of tasks [20, 5, 4, 12]. While engaging in an activity, we naturally retrieve relevant information\n                     or schema in a cognitive phenomena known as Priming [34]. This process ensures that necessary\n                     knowledge is readily accessible in memory, leading to enhanced performance for the task at hand\n                     [43]. Pre-trained, general-purpose models such as CLIP [40] and ALIGN [23] have extensive prior\n                     knowledge learned from large-scale, diverse datasets. These datasets seek to capture all natural\n                     variation in real data within their distribution. Can these models also benefit from something like\n                     priming? We observe that models trained even on the largest of such datasets often substantially\n                     improve in performance when fine-tuned on task-specific data. This begs the question of what the\n                     model learns from fine-tuning on the target dataset, if it already trained on many similar examples\n                     during pre-training.\n                    We speculate that the effect of fine-tuning a pre-trained model on task-specific data is similar to that of\n                     priming. Given the sheer size and diversity of the pre-training dataset it becomes challenging for the\n                     model to find a consistent solution that is optimal for all subsets of the data. This becomes particularly\n                     evident for open-vocabulary models such as CLIP, where multiple natural language descriptions can\n                     correspond to a single image, highlighting the challenge of accommodating diverse interpretations.\n                    We hypothesize that training on the downstream dataset re-aligns the model to the specific objective.\n                    With this in consideration, we propose Neural Priming. Specifically, Neural Priming recalls a\n                     subset of the pre-training data similar to the target distribution, re-aligns the natural language\n                        \u2217Equal contribution\n                     37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Neural Priming for Sample-Efficient Adaptation\n\n# Neural Priming for Sample-Efficient Adaptation\n\nMatthew Wallingford\u2217\u2020 Vivek Ramanujan\u2217\u2020 Alex Fang\u2020 Aditya Kusupati\u2020 Roozbeh Mottaghi\u2020 Aniruddha Kembhavi\u22c4 Ludwig Schmidt\u2020\u22c4\u2021 Ali Farhadi\u2020\n\n\u2020University of Washington \u22c4PRIOR, Allen Institute for AI \u2021LAION\n\narXiv:2306.10191v3 [cs.LG] 5 Dec 2023 {mcw244,ramanv}@cs.washington.edu\n\n## Abstract\n\nWe propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at inference, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addressing the challenge of limited labeled data and changing distributions. Code is available at https://github.com/RAIVNLab/neural-priming.\n\n## Introduction\n\nHumans have a vast store of prior experience which we draw on to flexibly perform a diverse range of tasks [20, 5, 4, 12]. While engaging in an activity, we naturally retrieve relevant information or schema in a cognitive phenomena known as Priming [34]. This process ensures that necessary knowledge is readily accessible in memory, leading to enhanced performance for the task at hand [43]. Pre-trained, general-purpose models such as CLIP [40] and ALIGN [23] have extensive prior knowledge learned from large-scale, diverse datasets. These datasets seek to capture all natural variation in real data within their distribution. Can these models also benefit from something like priming? We observe that models trained even on the largest of such datasets often substantially improve in performance when fine-tuned on task-specific data. This begs the question of what the model learns from fine-tuning on the target dataset, if it already trained on many similar examples during pre-training.\n\nWe speculate that the effect of fine-tuning a pre-trained model on task-specific data is similar to that of priming. Given the sheer size and diversity of the pre-training dataset it becomes challenging for the model to find a consistent solution that is optimal for all subsets of the data. This becomes particularly evident for open-vocabulary models such as CLIP, where multiple natural language descriptions can correspond to a single image, highlighting the challenge of accommodating diverse interpretations.\n\nWe hypothesize that training on the downstream dataset re-aligns the model to the specific objective. With this in consideration, we propose Neural Priming. Specifically, Neural Priming recalls a subset of the pre-training data similar to the target distribution, re-aligns the natural language\n\n\u2217Equal contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Neural Priming for Sample-Efficient Adaptation", "md": "# Neural Priming for Sample-Efficient Adaptation"}, {"type": "heading", "lvl": 1, "value": "Neural Priming for Sample-Efficient Adaptation", "md": "# Neural Priming for Sample-Efficient Adaptation"}, {"type": "text", "value": "Matthew Wallingford\u2217\u2020 Vivek Ramanujan\u2217\u2020 Alex Fang\u2020 Aditya Kusupati\u2020 Roozbeh Mottaghi\u2020 Aniruddha Kembhavi\u22c4 Ludwig Schmidt\u2020\u22c4\u2021 Ali Farhadi\u2020\n\n\u2020University of Washington \u22c4PRIOR, Allen Institute for AI \u2021LAION\n\narXiv:2306.10191v3 [cs.LG] 5 Dec 2023 {mcw244,ramanv}@cs.washington.edu", "md": "Matthew Wallingford\u2217\u2020 Vivek Ramanujan\u2217\u2020 Alex Fang\u2020 Aditya Kusupati\u2020 Roozbeh Mottaghi\u2020 Aniruddha Kembhavi\u22c4 Ludwig Schmidt\u2020\u22c4\u2021 Ali Farhadi\u2020\n\n\u2020University of Washington \u22c4PRIOR, Allen Institute for AI \u2021LAION\n\narXiv:2306.10191v3 [cs.LG] 5 Dec 2023 {mcw244,ramanv}@cs.washington.edu"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "We propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at inference, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addressing the challenge of limited labeled data and changing distributions. Code is available at https://github.com/RAIVNLab/neural-priming.", "md": "We propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at inference, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addressing the challenge of limited labeled data and changing distributions. Code is available at https://github.com/RAIVNLab/neural-priming."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Humans have a vast store of prior experience which we draw on to flexibly perform a diverse range of tasks [20, 5, 4, 12]. While engaging in an activity, we naturally retrieve relevant information or schema in a cognitive phenomena known as Priming [34]. This process ensures that necessary knowledge is readily accessible in memory, leading to enhanced performance for the task at hand [43]. Pre-trained, general-purpose models such as CLIP [40] and ALIGN [23] have extensive prior knowledge learned from large-scale, diverse datasets. These datasets seek to capture all natural variation in real data within their distribution. Can these models also benefit from something like priming? We observe that models trained even on the largest of such datasets often substantially improve in performance when fine-tuned on task-specific data. This begs the question of what the model learns from fine-tuning on the target dataset, if it already trained on many similar examples during pre-training.\n\nWe speculate that the effect of fine-tuning a pre-trained model on task-specific data is similar to that of priming. Given the sheer size and diversity of the pre-training dataset it becomes challenging for the model to find a consistent solution that is optimal for all subsets of the data. This becomes particularly evident for open-vocabulary models such as CLIP, where multiple natural language descriptions can correspond to a single image, highlighting the challenge of accommodating diverse interpretations.\n\nWe hypothesize that training on the downstream dataset re-aligns the model to the specific objective. With this in consideration, we propose Neural Priming. Specifically, Neural Priming recalls a subset of the pre-training data similar to the target distribution, re-aligns the natural language\n\n\u2217Equal contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Humans have a vast store of prior experience which we draw on to flexibly perform a diverse range of tasks [20, 5, 4, 12]. While engaging in an activity, we naturally retrieve relevant information or schema in a cognitive phenomena known as Priming [34]. This process ensures that necessary knowledge is readily accessible in memory, leading to enhanced performance for the task at hand [43]. Pre-trained, general-purpose models such as CLIP [40] and ALIGN [23] have extensive prior knowledge learned from large-scale, diverse datasets. These datasets seek to capture all natural variation in real data within their distribution. Can these models also benefit from something like priming? We observe that models trained even on the largest of such datasets often substantially improve in performance when fine-tuned on task-specific data. This begs the question of what the model learns from fine-tuning on the target dataset, if it already trained on many similar examples during pre-training.\n\nWe speculate that the effect of fine-tuning a pre-trained model on task-specific data is similar to that of priming. Given the sheer size and diversity of the pre-training dataset it becomes challenging for the model to find a consistent solution that is optimal for all subsets of the data. This becomes particularly evident for open-vocabulary models such as CLIP, where multiple natural language descriptions can correspond to a single image, highlighting the challenge of accommodating diverse interpretations.\n\nWe hypothesize that training on the downstream dataset re-aligns the model to the specific objective. With this in consideration, we propose Neural Priming. Specifically, Neural Priming recalls a subset of the pre-training data similar to the target distribution, re-aligns the natural language\n\n\u2217Equal contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "                                                                 Task Description\n          Pre-training Data                                                                                        CLIP Model\n                                                 Stimulus        Maltese     German\n                                                                  Poodle     Shepherd\n                                                                                          Priming Set\n       p\n                                                       Recall                                                    Attunement\nFigure 1: A diagram of Neural Priming, our proposed method. Neural Priming is a framework\nfor leveraging an open-vocabulary model\u2019s own pre-training data to improve performance on down-\nstream tasks. Neural Priming encompasses two processes: 1. Collecting a priming pool of relevant\nexamples from the pre-training set to prime with and 2. using these examples to attune our model\nto a given task. We show performance improvements across a wide range of transfer learning and\nrobustness benchmarks.\ndescriptions to the downstream task, and quickly adapts the model to the subset. We perform\nextensive experiments on 7 transfer learning and 4 distribution shift datasets to validate our method.\nWe use the OpenCLIP [40, 52] ViT [8] set of models pre-trained on the LAION-2B and 400M [44].\nWe find Neural Priming leads to significant accuracy improvements, particularly when labeled data\nis scarce and in specialized domains. Concretely, Neural Priming improves accuracy by 2.45%\non ImageNet and 4.25% on average across the other 6 datasets over the base CLIP model. In the\nfew-shot setting, Neural Priming improves accuracy by 3.81% on average over recent methods on\nstandard transfer learning benchmarks. We show Neural Priming is effi                            cient and can be performed\non-the-fly. For datasets containing more than 2 billion images, we can prime our model to ImageNet\nin less than 2 minutes with a single commercial GPU.\nNeural Priming is flexible and can be used with variable degrees of information about the down-\nstream distribution. When the model has language-only task descriptions, our approach can efficiently\nretrieve a priming pool of relevant examples from the pre-training set and attune the model to this data.\nAt inference time, given a set of test images to classify, Neural Priming is able to use test examples\nto adapt the priming pool to distribution shifts. When we have access to training examples in the\nfew-shot setting, Neural Priming can filter the priming pool to align with the training distribution.\nWe make the following contributions:\n\u2022 We introduce Neural Priming, a novel method that leverages retrieval from the pre-training dataset\n   for efficient and accurate adaptation of large, pre-trained models to downstream tasks.\n\u2022 Neural Priming achieves state-of-the-art zero-shot and few-shot accuracy on the standard transfer\n   learning datasets \u2013 up to 4.25% and 3.81% improvements respectively over baselines (Section 4.2).\n\u2022 Neural Priming also enables transductive learning and improves performance on standard distri-\n   bution shift datasets by 2.51% on average, all without using any additional data (Section 4.3).\n\u2022 Our approach generalizes to various architectures and pre-training datasets while being comple-\n   mentary to techniques [33, 39] that improve zero-shot performance of open-vocabulary models.\n2     Related Work\n2.1    Open-Vocabulary Models and Zero-shot Inference\nOpen-vocabulary models have proven to be an effective approach for transfer learning. Such models\nenable training on vast amounts of web-scale images without the need for labor-intensive human\n                                                                  2", "md": "# Task Description\n\n## Task Description\n\nPre-training Data\n\n|Stimulus|Maltese|German|\n|---|---|---|\n| | |Poodle|Shepherd|\n\nPriming Set\n\nFigure 1: A diagram of Neural Priming, our proposed method. Neural Priming is a framework for leveraging an open-vocabulary model\u2019s own pre-training data to improve performance on downstream tasks. Neural Priming encompasses two processes: 1. Collecting a priming pool of relevant examples from the pre-training set to prime with and 2. using these examples to attune our model to a given task. We show performance improvements across a wide range of transfer learning and robustness benchmarks.\n\nWe introduce Neural Priming, a novel method that leverages retrieval from the pre-training dataset for efficient and accurate adaptation of large, pre-trained models to downstream tasks.\n\nNeural Priming achieves state-of-the-art zero-shot and few-shot accuracy on the standard transfer learning datasets \u2013 up to 4.25% and 3.81% improvements respectively over baselines (Section 4.2).\n\nNeural Priming also enables transductive learning and improves performance on standard distribution shift datasets by 2.51% on average, all without using any additional data (Section 4.3).\n\nOur approach generalizes to various architectures and pre-training datasets while being complementary to techniques that improve zero-shot performance of open-vocabulary models.\n\n### Related Work\n\n#### Open-Vocabulary Models and Zero-shot Inference\n\nOpen-vocabulary models have proven to be an effective approach for transfer learning. Such models enable training on vast amounts of web-scale images without the need for labor-intensive human.", "images": [{"name": "page-2-0.jpg", "height": 43, "width": 145, "x": 113, "y": 73}, {"name": "page-2-1.jpg", "height": 64, "width": 80, "x": 431, "y": 69}], "items": [{"type": "heading", "lvl": 1, "value": "Task Description", "md": "# Task Description"}, {"type": "heading", "lvl": 2, "value": "Task Description", "md": "## Task Description"}, {"type": "text", "value": "Pre-training Data", "md": "Pre-training Data"}, {"type": "table", "rows": [["Stimulus", "Maltese", "German"], ["", "", "Poodle", "Shepherd"]], "md": "|Stimulus|Maltese|German|\n|---|---|---|\n| | |Poodle|Shepherd|", "isPerfectTable": false, "csv": "\"Stimulus\",\"Maltese\",\"German\"\n\"\",\"\",\"Poodle\",\"Shepherd\""}, {"type": "text", "value": "Priming Set\n\nFigure 1: A diagram of Neural Priming, our proposed method. Neural Priming is a framework for leveraging an open-vocabulary model\u2019s own pre-training data to improve performance on downstream tasks. Neural Priming encompasses two processes: 1. Collecting a priming pool of relevant examples from the pre-training set to prime with and 2. using these examples to attune our model to a given task. We show performance improvements across a wide range of transfer learning and robustness benchmarks.\n\nWe introduce Neural Priming, a novel method that leverages retrieval from the pre-training dataset for efficient and accurate adaptation of large, pre-trained models to downstream tasks.\n\nNeural Priming achieves state-of-the-art zero-shot and few-shot accuracy on the standard transfer learning datasets \u2013 up to 4.25% and 3.81% improvements respectively over baselines (Section 4.2).\n\nNeural Priming also enables transductive learning and improves performance on standard distribution shift datasets by 2.51% on average, all without using any additional data (Section 4.3).\n\nOur approach generalizes to various architectures and pre-training datasets while being complementary to techniques that improve zero-shot performance of open-vocabulary models.", "md": "Priming Set\n\nFigure 1: A diagram of Neural Priming, our proposed method. Neural Priming is a framework for leveraging an open-vocabulary model\u2019s own pre-training data to improve performance on downstream tasks. Neural Priming encompasses two processes: 1. Collecting a priming pool of relevant examples from the pre-training set to prime with and 2. using these examples to attune our model to a given task. We show performance improvements across a wide range of transfer learning and robustness benchmarks.\n\nWe introduce Neural Priming, a novel method that leverages retrieval from the pre-training dataset for efficient and accurate adaptation of large, pre-trained models to downstream tasks.\n\nNeural Priming achieves state-of-the-art zero-shot and few-shot accuracy on the standard transfer learning datasets \u2013 up to 4.25% and 3.81% improvements respectively over baselines (Section 4.2).\n\nNeural Priming also enables transductive learning and improves performance on standard distribution shift datasets by 2.51% on average, all without using any additional data (Section 4.3).\n\nOur approach generalizes to various architectures and pre-training datasets while being complementary to techniques that improve zero-shot performance of open-vocabulary models."}, {"type": "heading", "lvl": 3, "value": "Related Work", "md": "### Related Work"}, {"type": "heading", "lvl": 4, "value": "Open-Vocabulary Models and Zero-shot Inference", "md": "#### Open-Vocabulary Models and Zero-shot Inference"}, {"type": "text", "value": "Open-vocabulary models have proven to be an effective approach for transfer learning. Such models enable training on vast amounts of web-scale images without the need for labor-intensive human.", "md": "Open-vocabulary models have proven to be an effective approach for transfer learning. Such models enable training on vast amounts of web-scale images without the need for labor-intensive human."}]}, {"page": 3, "text": "labeling by leveraging pre-existing natural language descriptions [40]. Open-vocabulary models have\nset state-of-the-art on ImageNet [7] as well other transfer learning benchmarks [53, 1, 41].\nOpen-vocabulary models offer additional capabilities beyond standard pre-trained models. They can\nperform zero-shot inference, where predictions are made without training on target data. Additionally,\nthey are robust to distribution shifts [40], enable prompt-tuning methods [39, 33], and can be used for\ntext-based retrieval [10]. Zero-shot can have different meanings in the literature. In the context of\nthis paper, we consider zero-shot as the experimental setting in which the model receives no training\nexamples drawn from the training distribution.\nPrompt-tuning has emerged as a popular research direction in the domains of large language and\nopen-vocabulary models. In the context of open-vocabulary models, prompt-tuning can involve\nmodifying the textual prompts or queries used during the training or inference of the model to\nimprove its understanding of visual content or achieve specific goals. In the original CLIP paper,\nRadford et al. [40] design hand-crafted prompt templates for ImageNet and other transfer learning\ndatasets and show that this leads to substantial accuracy improvements. More recently, other work\n[33, 55] has used machine learning approaches to learn the prompts rather than hand-crafting them.\n2.2   Distribution Shifts\nRobustness to distribution shift is a key property of good machine learning models as it represents a\nnotion of reliability. In particular, studies on natural distribution shifts, including ImageNet-V2 [41],\nImageNet-Sketch [51], ImageNet-R [18], and ImageNet-A [19], find that models have a consistent\nperformance drop when exposed to a distribution at inference time not seen during train time [49]. In\norder to focus on robustness and eliminate the confounder of better models being generally better, this\nperformance gap is measured through effective robustness, which is the robustness improvement over\nImageNet trained models. Prior work has shown that the performance of models on in distribution\nand out of distribution is highly correlated across many algorithmic training interventions, except for\ncases where training on larger and more diverse datasets increases robustness [35].\nThe most significant recent improvement in effective robustness [41] is the introduction of open-\nvocabulary models. At its time of release, CLIP [40] achieved unprecedented effective robustness\non a variety of distribution shifts. Studies have suggested that these models achieve high effective\nrobustness through their data distribution [9], a result of training on large amounts of web-scraped data.\nHowever, these models are still worse at downstream tasks than models fine-tuned on in-distribution\ndata. Moreover, fine-tuning on downstream data causes robustness on other data distributions to\ndeteriorate [40, 52]. Many mitigation methods have been proposed to such as Wise-FT, FLYP, LP-FT,\nand model surgery [52, 15, 28, 29]. Our paper differs from these methods in goal: whereas they seek\nto keep model robustness while gaining the benefits of fine-tuning on task-specific data, we seek\nthe benefits of fine-tuning while not collecting any in-distribution data. Hence these methods are\ncomplementary to Neural Priming, and we employ Wise-FT in our model attunement procedure.\n2.3   Transductive Learning\nTransductive learning [13, 6] focuses on leveraging unlabeled data during inference. It differs\nfrom traditional supervised learning, which solely relies on labeled data at train time. Related to\ntransductive learning is test-time training [48, 14, 45]. Test-time training involves adapting and\nrefining the model\u2019s predictions based on the specific testing examples encountered. Transductive\nlearning differs from test-time training in that test-time training only considers one test sample at a\ntime, whereas transductive aims to learn from the entire test set.\n2.4   Few-Shot Learning\nFew-shot learning research aims to addresses the challenge of learning from a limited number of\nlabeled examples. In many real-world scenarios, acquiring large labeled datasets is impractical or\ncostly. Older lines of work have focused on meta-training small models [47, 11, 37, 22] on small-\nscale datasets. More recently, the approach for few-shot learning has shifted towards training large,\ngeneral-purpose models such as CLIP [40] and ALIGN [23] on web-scale datasets.\n                                                    3", "md": "# Document\n\nLabeling by leveraging pre-existing natural language descriptions [40]. Open-vocabulary models have set state-of-the-art on ImageNet [7] as well other transfer learning benchmarks [53, 1, 41]. Open-vocabulary models offer additional capabilities beyond standard pre-trained models. They can perform zero-shot inference, where predictions are made without training on target data. Additionally, they are robust to distribution shifts [40], enable prompt-tuning methods [39, 33], and can be used for text-based retrieval [10]. Zero-shot can have different meanings in the literature. In the context of this paper, we consider zero-shot as the experimental setting in which the model receives no training examples drawn from the training distribution.\n\nPrompt-tuning has emerged as a popular research direction in the domains of large language and open-vocabulary models. In the context of open-vocabulary models, prompt-tuning can involve modifying the textual prompts or queries used during the training or inference of the model to improve its understanding of visual content or achieve specific goals. In the original CLIP paper, Radford et al. [40] design hand-crafted prompt templates for ImageNet and other transfer learning datasets and show that this leads to substantial accuracy improvements. More recently, other work [33, 55] has used machine learning approaches to learn the prompts rather than hand-crafting them.\n\n## Distribution Shifts\n\nRobustness to distribution shift is a key property of good machine learning models as it represents a notion of reliability. In particular, studies on natural distribution shifts, including ImageNet-V2 [41], ImageNet-Sketch [51], ImageNet-R [18], and ImageNet-A [19], find that models have a consistent performance drop when exposed to a distribution at inference time not seen during train time [49]. In order to focus on robustness and eliminate the confounder of better models being generally better, this performance gap is measured through effective robustness, which is the robustness improvement over ImageNet trained models. Prior work has shown that the performance of models on in distribution and out of distribution is highly correlated across many algorithmic training interventions, except for cases where training on larger and more diverse datasets increases robustness [35].\n\nThe most significant recent improvement in effective robustness [41] is the introduction of open-vocabulary models. At its time of release, CLIP [40] achieved unprecedented effective robustness on a variety of distribution shifts. Studies have suggested that these models achieve high effective robustness through their data distribution [9], a result of training on large amounts of web-scraped data. However, these models are still worse at downstream tasks than models fine-tuned on in-distribution data. Moreover, fine-tuning on downstream data causes robustness on other data distributions to deteriorate [40, 52]. Many mitigation methods have been proposed to such as Wise-FT, FLYP, LP-FT, and model surgery [52, 15, 28, 29]. Our paper differs from these methods in goal: whereas they seek to keep model robustness while gaining the benefits of fine-tuning on task-specific data, we seek the benefits of fine-tuning while not collecting any in-distribution data. Hence these methods are complementary to Neural Priming, and we employ Wise-FT in our model attunement procedure.\n\n## Transductive Learning\n\nTransductive learning [13, 6] focuses on leveraging unlabeled data during inference. It differs from traditional supervised learning, which solely relies on labeled data at train time. Related to transductive learning is test-time training [48, 14, 45]. Test-time training involves adapting and refining the model\u2019s predictions based on the specific testing examples encountered. Transductive learning differs from test-time training in that test-time training only considers one test sample at a time, whereas transductive aims to learn from the entire test set.\n\n## Few-Shot Learning\n\nFew-shot learning research aims to addresses the challenge of learning from a limited number of labeled examples. In many real-world scenarios, acquiring large labeled datasets is impractical or costly. Older lines of work have focused on meta-training small models [47, 11, 37, 22] on small-scale datasets. More recently, the approach for few-shot learning has shifted towards training large, general-purpose models such as CLIP [40] and ALIGN [23] on web-scale datasets.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Labeling by leveraging pre-existing natural language descriptions [40]. Open-vocabulary models have set state-of-the-art on ImageNet [7] as well other transfer learning benchmarks [53, 1, 41]. Open-vocabulary models offer additional capabilities beyond standard pre-trained models. They can perform zero-shot inference, where predictions are made without training on target data. Additionally, they are robust to distribution shifts [40], enable prompt-tuning methods [39, 33], and can be used for text-based retrieval [10]. Zero-shot can have different meanings in the literature. In the context of this paper, we consider zero-shot as the experimental setting in which the model receives no training examples drawn from the training distribution.\n\nPrompt-tuning has emerged as a popular research direction in the domains of large language and open-vocabulary models. In the context of open-vocabulary models, prompt-tuning can involve modifying the textual prompts or queries used during the training or inference of the model to improve its understanding of visual content or achieve specific goals. In the original CLIP paper, Radford et al. [40] design hand-crafted prompt templates for ImageNet and other transfer learning datasets and show that this leads to substantial accuracy improvements. More recently, other work [33, 55] has used machine learning approaches to learn the prompts rather than hand-crafting them.", "md": "Labeling by leveraging pre-existing natural language descriptions [40]. Open-vocabulary models have set state-of-the-art on ImageNet [7] as well other transfer learning benchmarks [53, 1, 41]. Open-vocabulary models offer additional capabilities beyond standard pre-trained models. They can perform zero-shot inference, where predictions are made without training on target data. Additionally, they are robust to distribution shifts [40], enable prompt-tuning methods [39, 33], and can be used for text-based retrieval [10]. Zero-shot can have different meanings in the literature. In the context of this paper, we consider zero-shot as the experimental setting in which the model receives no training examples drawn from the training distribution.\n\nPrompt-tuning has emerged as a popular research direction in the domains of large language and open-vocabulary models. In the context of open-vocabulary models, prompt-tuning can involve modifying the textual prompts or queries used during the training or inference of the model to improve its understanding of visual content or achieve specific goals. In the original CLIP paper, Radford et al. [40] design hand-crafted prompt templates for ImageNet and other transfer learning datasets and show that this leads to substantial accuracy improvements. More recently, other work [33, 55] has used machine learning approaches to learn the prompts rather than hand-crafting them."}, {"type": "heading", "lvl": 2, "value": "Distribution Shifts", "md": "## Distribution Shifts"}, {"type": "text", "value": "Robustness to distribution shift is a key property of good machine learning models as it represents a notion of reliability. In particular, studies on natural distribution shifts, including ImageNet-V2 [41], ImageNet-Sketch [51], ImageNet-R [18], and ImageNet-A [19], find that models have a consistent performance drop when exposed to a distribution at inference time not seen during train time [49]. In order to focus on robustness and eliminate the confounder of better models being generally better, this performance gap is measured through effective robustness, which is the robustness improvement over ImageNet trained models. Prior work has shown that the performance of models on in distribution and out of distribution is highly correlated across many algorithmic training interventions, except for cases where training on larger and more diverse datasets increases robustness [35].\n\nThe most significant recent improvement in effective robustness [41] is the introduction of open-vocabulary models. At its time of release, CLIP [40] achieved unprecedented effective robustness on a variety of distribution shifts. Studies have suggested that these models achieve high effective robustness through their data distribution [9], a result of training on large amounts of web-scraped data. However, these models are still worse at downstream tasks than models fine-tuned on in-distribution data. Moreover, fine-tuning on downstream data causes robustness on other data distributions to deteriorate [40, 52]. Many mitigation methods have been proposed to such as Wise-FT, FLYP, LP-FT, and model surgery [52, 15, 28, 29]. Our paper differs from these methods in goal: whereas they seek to keep model robustness while gaining the benefits of fine-tuning on task-specific data, we seek the benefits of fine-tuning while not collecting any in-distribution data. Hence these methods are complementary to Neural Priming, and we employ Wise-FT in our model attunement procedure.", "md": "Robustness to distribution shift is a key property of good machine learning models as it represents a notion of reliability. In particular, studies on natural distribution shifts, including ImageNet-V2 [41], ImageNet-Sketch [51], ImageNet-R [18], and ImageNet-A [19], find that models have a consistent performance drop when exposed to a distribution at inference time not seen during train time [49]. In order to focus on robustness and eliminate the confounder of better models being generally better, this performance gap is measured through effective robustness, which is the robustness improvement over ImageNet trained models. Prior work has shown that the performance of models on in distribution and out of distribution is highly correlated across many algorithmic training interventions, except for cases where training on larger and more diverse datasets increases robustness [35].\n\nThe most significant recent improvement in effective robustness [41] is the introduction of open-vocabulary models. At its time of release, CLIP [40] achieved unprecedented effective robustness on a variety of distribution shifts. Studies have suggested that these models achieve high effective robustness through their data distribution [9], a result of training on large amounts of web-scraped data. However, these models are still worse at downstream tasks than models fine-tuned on in-distribution data. Moreover, fine-tuning on downstream data causes robustness on other data distributions to deteriorate [40, 52]. Many mitigation methods have been proposed to such as Wise-FT, FLYP, LP-FT, and model surgery [52, 15, 28, 29]. Our paper differs from these methods in goal: whereas they seek to keep model robustness while gaining the benefits of fine-tuning on task-specific data, we seek the benefits of fine-tuning while not collecting any in-distribution data. Hence these methods are complementary to Neural Priming, and we employ Wise-FT in our model attunement procedure."}, {"type": "heading", "lvl": 2, "value": "Transductive Learning", "md": "## Transductive Learning"}, {"type": "text", "value": "Transductive learning [13, 6] focuses on leveraging unlabeled data during inference. It differs from traditional supervised learning, which solely relies on labeled data at train time. Related to transductive learning is test-time training [48, 14, 45]. Test-time training involves adapting and refining the model\u2019s predictions based on the specific testing examples encountered. Transductive learning differs from test-time training in that test-time training only considers one test sample at a time, whereas transductive aims to learn from the entire test set.", "md": "Transductive learning [13, 6] focuses on leveraging unlabeled data during inference. It differs from traditional supervised learning, which solely relies on labeled data at train time. Related to transductive learning is test-time training [48, 14, 45]. Test-time training involves adapting and refining the model\u2019s predictions based on the specific testing examples encountered. Transductive learning differs from test-time training in that test-time training only considers one test sample at a time, whereas transductive aims to learn from the entire test set."}, {"type": "heading", "lvl": 2, "value": "Few-Shot Learning", "md": "## Few-Shot Learning"}, {"type": "text", "value": "Few-shot learning research aims to addresses the challenge of learning from a limited number of labeled examples. In many real-world scenarios, acquiring large labeled datasets is impractical or costly. Older lines of work have focused on meta-training small models [47, 11, 37, 22] on small-scale datasets. More recently, the approach for few-shot learning has shifted towards training large, general-purpose models such as CLIP [40] and ALIGN [23] on web-scale datasets.", "md": "Few-shot learning research aims to addresses the challenge of learning from a limited number of labeled examples. In many real-world scenarios, acquiring large labeled datasets is impractical or costly. Older lines of work have focused on meta-training small models [47, 11, 37, 22] on small-scale datasets. More recently, the approach for few-shot learning has shifted towards training large, general-purpose models such as CLIP [40] and ALIGN [23] on web-scale datasets."}]}, {"page": 4, "text": "2.5   Retrieval-Augmented Models\nIn language, works have demonstrated the effectiveness of retrieval from text corpora or structured\ndata for tasks such as question answering [3, 16, 25]. In general, these methods seek to recover facts\neither from a large corpus or knowledge graph, then use those to complete tasks. This differs from our\nscenario, where exact examples at inference time do not necessarily exist in the pre-training corpus.\nREACT [30] and SuS-X [50] are retrieval-augmented methods for open-vocabulary models which\nuse search to fine-tune with relevant examples [30]. We differ from Liu et al. [30] in that they add a\nsubstantial number of new parameters whereas we do not. Additionally, our approach is significantly\nmore efficient, both computationally and in terms of number of samples, enabling use at inference\nfor additional improvement (Section 3.1.2). We differ from [50] in that their work uses semantic\nretrieval whereas Neural Priming leverages language for fast initial filtering and image search for\naccurate retrieval. Further, Neural Priming shows that models can improve by revisiting examples\nseen throughout pretraining whereas other works retrieve new examples from external datasets.\n3   Method\nNeural Priming is the process of retrieving relevant information from the pre-training dataset and\nleveraging it for a specific task. We study it in the context of vision-language contrastive pre-training,\nso the form our task description takes is a set of class names, C, already in natural language. A CLIP\nmodel [40] consists of a vision embedding model, V , and a language embedding model, L, each\nproducing a vector representation in Rd. The pre-training dataset, D, consists of a large number of\nimage-text pairs collected from the web. The text component can be noisy, potentially containing\nirrelevant or inaccurate information about the image content.\nWe break our method down into two main steps: 1. Collecting the priming pool, where we gather\ndata from our pre-training dataset relevant to a particular task and 2. model attunement, where we\nleverage this data to improve our model.\n3.1   Collecting the Priming Pool\n3.1.1   Leveraging Natural Language Task Information\nThe goal of this step is to collect an initial pool of images relevant to the task at hand given the\npreviously defined natural language description C. For example, if our task is a set of dog breeds,\nideally we would collect sets of images belonging to those breeds and label them accordingly.\nA simple way to prime is by using retrieval to gather relevant data points from our pre-training\ndataset. An existing method for language-based retrieval involves using the CLIP text embedding of\na class description c \u2208 C for retrieval using semantic similarity scores on the pre-training set [2, 42].\nHowever, with neural priming, prioritizing precision over recall is crucial, considering the size,\ndiversity, and noise of the pre-training dataset. This form of semantic retrieval has a major downside:\nit is not clear where to threshold similarity scores to retrieve the most relevant images. Threshold too\nlate and we allow unrelated images to be included in our pool. Further, this threshold is often specific\nto a category, making it infeasible to search at scale.\nOur approach to language-based priming is to search for the existence of the class name, c \u2208             C,\nin the captions of our pre-training dataset to retrieve images relevant to a particular category. We\norganize these image-text pairs into separate categorical clusters {Bc} according to the class name c\nmentioned in their captions. This approach has a few advantages over semantic retrieval: 1. After\nsetting up an inverted index search structure for text retrieval [26, 46], exact string matching is far\nfaster than semantic retrieval, even when approximate nearest neighbor strategies are employed [24],\n2. with exact string search, the category boundary is clear and therefore does not require per category\ntuning, 3. the retrieval results are overall qualitatively more relevant. Finally, to leverage the semantic\nunderstanding of our CLIP model, we filter the priming pool using CLIP similarity score. We do\nthis by constructing a \u201czero-shot\u201d CLIP classifier, as defined by Radford et al. [40], and removing\nexamples from categorical clusters that do not align with their label according to the CLIP model.\n                                                    4", "md": "# Retrieval-Augmented Models\n\n## 2.5 Retrieval-Augmented Models\n\nIn language, works have demonstrated the effectiveness of retrieval from text corpora or structured\ndata for tasks such as question answering [3, 16, 25]. In general, these methods seek to recover facts\neither from a large corpus or knowledge graph, then use those to complete tasks. This differs from our\nscenario, where exact examples at inference time do not necessarily exist in the pre-training corpus.\nREACT [30] and SuS-X [50] are retrieval-augmented methods for open-vocabulary models which\nuse search to fine-tune with relevant examples [30]. We differ from Liu et al. [30] in that they add a\nsubstantial number of new parameters whereas we do not. Additionally, our approach is significantly\nmore efficient, both computationally and in terms of number of samples, enabling use at inference\nfor additional improvement (Section 3.1.2). We differ from [50] in that their work uses semantic\nretrieval whereas Neural Priming leverages language for fast initial filtering and image search for\naccurate retrieval. Further, Neural Priming shows that models can improve by revisiting examples\nseen throughout pretraining whereas other works retrieve new examples from external datasets.\n\n### 3 Method\n\nNeural Priming is the process of retrieving relevant information from the pre-training dataset and\nleveraging it for a specific task. We study it in the context of vision-language contrastive pre-training,\nso the form our task description takes is a set of class names, C, already in natural language. A CLIP\nmodel [40] consists of a vision embedding model, V , and a language embedding model, L, each\nproducing a vector representation in Rd. The pre-training dataset, D, consists of a large number of\nimage-text pairs collected from the web. The text component can be noisy, potentially containing\nirrelevant or inaccurate information about the image content.\n\nWe break our method down into two main steps: 1. Collecting the priming pool, where we gather\ndata from our pre-training dataset relevant to a particular task and 2. model attunement, where we\nleverage this data to improve our model.\n\n#### 3.1 Collecting the Priming Pool\n\n##### 3.1.1 Leveraging Natural Language Task Information\n\nThe goal of this step is to collect an initial pool of images relevant to the task at hand given the\npreviously defined natural language description C. For example, if our task is a set of dog breeds,\nideally we would collect sets of images belonging to those breeds and label them accordingly.\n\nA simple way to prime is by using retrieval to gather relevant data points from our pre-training\ndataset. An existing method for language-based retrieval involves using the CLIP text embedding of\na class description $$c \\in C$$ for retrieval using semantic similarity scores on the pre-training set [2, 42].\nHowever, with neural priming, prioritizing precision over recall is crucial, considering the size,\ndiversity, and noise of the pre-training dataset. This form of semantic retrieval has a major downside:\nit is not clear where to threshold similarity scores to retrieve the most relevant images. Threshold too\nlate and we allow unrelated images to be included in our pool. Further, this threshold is often specific\nto a category, making it infeasible to search at scale.\n\nOur approach to language-based priming is to search for the existence of the class name, $$c \\in C$$,\nin the captions of our pre-training dataset to retrieve images relevant to a particular category. We\norganize these image-text pairs into separate categorical clusters {Bc} according to the class name c\nmentioned in their captions. This approach has a few advantages over semantic retrieval: 1. After\nsetting up an inverted index search structure for text retrieval [26, 46], exact string matching is far\nfaster than semantic retrieval, even when approximate nearest neighbor strategies are employed [24],\n2. with exact string search, the category boundary is clear and therefore does not require per category\ntuning, 3. the retrieval results are overall qualitatively more relevant. Finally, to leverage the semantic\nunderstanding of our CLIP model, we filter the priming pool using CLIP similarity score. We do\nthis by constructing a \u201czero-shot\u201d CLIP classifier, as defined by Radford et al. [40], and removing\nexamples from categorical clusters that do not align with their label according to the CLIP model.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Retrieval-Augmented Models", "md": "# Retrieval-Augmented Models"}, {"type": "heading", "lvl": 2, "value": "2.5 Retrieval-Augmented Models", "md": "## 2.5 Retrieval-Augmented Models"}, {"type": "text", "value": "In language, works have demonstrated the effectiveness of retrieval from text corpora or structured\ndata for tasks such as question answering [3, 16, 25]. In general, these methods seek to recover facts\neither from a large corpus or knowledge graph, then use those to complete tasks. This differs from our\nscenario, where exact examples at inference time do not necessarily exist in the pre-training corpus.\nREACT [30] and SuS-X [50] are retrieval-augmented methods for open-vocabulary models which\nuse search to fine-tune with relevant examples [30]. We differ from Liu et al. [30] in that they add a\nsubstantial number of new parameters whereas we do not. Additionally, our approach is significantly\nmore efficient, both computationally and in terms of number of samples, enabling use at inference\nfor additional improvement (Section 3.1.2). We differ from [50] in that their work uses semantic\nretrieval whereas Neural Priming leverages language for fast initial filtering and image search for\naccurate retrieval. Further, Neural Priming shows that models can improve by revisiting examples\nseen throughout pretraining whereas other works retrieve new examples from external datasets.", "md": "In language, works have demonstrated the effectiveness of retrieval from text corpora or structured\ndata for tasks such as question answering [3, 16, 25]. In general, these methods seek to recover facts\neither from a large corpus or knowledge graph, then use those to complete tasks. This differs from our\nscenario, where exact examples at inference time do not necessarily exist in the pre-training corpus.\nREACT [30] and SuS-X [50] are retrieval-augmented methods for open-vocabulary models which\nuse search to fine-tune with relevant examples [30]. We differ from Liu et al. [30] in that they add a\nsubstantial number of new parameters whereas we do not. Additionally, our approach is significantly\nmore efficient, both computationally and in terms of number of samples, enabling use at inference\nfor additional improvement (Section 3.1.2). We differ from [50] in that their work uses semantic\nretrieval whereas Neural Priming leverages language for fast initial filtering and image search for\naccurate retrieval. Further, Neural Priming shows that models can improve by revisiting examples\nseen throughout pretraining whereas other works retrieve new examples from external datasets."}, {"type": "heading", "lvl": 3, "value": "3 Method", "md": "### 3 Method"}, {"type": "text", "value": "Neural Priming is the process of retrieving relevant information from the pre-training dataset and\nleveraging it for a specific task. We study it in the context of vision-language contrastive pre-training,\nso the form our task description takes is a set of class names, C, already in natural language. A CLIP\nmodel [40] consists of a vision embedding model, V , and a language embedding model, L, each\nproducing a vector representation in Rd. The pre-training dataset, D, consists of a large number of\nimage-text pairs collected from the web. The text component can be noisy, potentially containing\nirrelevant or inaccurate information about the image content.\n\nWe break our method down into two main steps: 1. Collecting the priming pool, where we gather\ndata from our pre-training dataset relevant to a particular task and 2. model attunement, where we\nleverage this data to improve our model.", "md": "Neural Priming is the process of retrieving relevant information from the pre-training dataset and\nleveraging it for a specific task. We study it in the context of vision-language contrastive pre-training,\nso the form our task description takes is a set of class names, C, already in natural language. A CLIP\nmodel [40] consists of a vision embedding model, V , and a language embedding model, L, each\nproducing a vector representation in Rd. The pre-training dataset, D, consists of a large number of\nimage-text pairs collected from the web. The text component can be noisy, potentially containing\nirrelevant or inaccurate information about the image content.\n\nWe break our method down into two main steps: 1. Collecting the priming pool, where we gather\ndata from our pre-training dataset relevant to a particular task and 2. model attunement, where we\nleverage this data to improve our model."}, {"type": "heading", "lvl": 4, "value": "3.1 Collecting the Priming Pool", "md": "#### 3.1 Collecting the Priming Pool"}, {"type": "heading", "lvl": 5, "value": "3.1.1 Leveraging Natural Language Task Information", "md": "##### 3.1.1 Leveraging Natural Language Task Information"}, {"type": "text", "value": "The goal of this step is to collect an initial pool of images relevant to the task at hand given the\npreviously defined natural language description C. For example, if our task is a set of dog breeds,\nideally we would collect sets of images belonging to those breeds and label them accordingly.\n\nA simple way to prime is by using retrieval to gather relevant data points from our pre-training\ndataset. An existing method for language-based retrieval involves using the CLIP text embedding of\na class description $$c \\in C$$ for retrieval using semantic similarity scores on the pre-training set [2, 42].\nHowever, with neural priming, prioritizing precision over recall is crucial, considering the size,\ndiversity, and noise of the pre-training dataset. This form of semantic retrieval has a major downside:\nit is not clear where to threshold similarity scores to retrieve the most relevant images. Threshold too\nlate and we allow unrelated images to be included in our pool. Further, this threshold is often specific\nto a category, making it infeasible to search at scale.\n\nOur approach to language-based priming is to search for the existence of the class name, $$c \\in C$$,\nin the captions of our pre-training dataset to retrieve images relevant to a particular category. We\norganize these image-text pairs into separate categorical clusters {Bc} according to the class name c\nmentioned in their captions. This approach has a few advantages over semantic retrieval: 1. After\nsetting up an inverted index search structure for text retrieval [26, 46], exact string matching is far\nfaster than semantic retrieval, even when approximate nearest neighbor strategies are employed [24],\n2. with exact string search, the category boundary is clear and therefore does not require per category\ntuning, 3. the retrieval results are overall qualitatively more relevant. Finally, to leverage the semantic\nunderstanding of our CLIP model, we filter the priming pool using CLIP similarity score. We do\nthis by constructing a \u201czero-shot\u201d CLIP classifier, as defined by Radford et al. [40], and removing\nexamples from categorical clusters that do not align with their label according to the CLIP model.", "md": "The goal of this step is to collect an initial pool of images relevant to the task at hand given the\npreviously defined natural language description C. For example, if our task is a set of dog breeds,\nideally we would collect sets of images belonging to those breeds and label them accordingly.\n\nA simple way to prime is by using retrieval to gather relevant data points from our pre-training\ndataset. An existing method for language-based retrieval involves using the CLIP text embedding of\na class description $$c \\in C$$ for retrieval using semantic similarity scores on the pre-training set [2, 42].\nHowever, with neural priming, prioritizing precision over recall is crucial, considering the size,\ndiversity, and noise of the pre-training dataset. This form of semantic retrieval has a major downside:\nit is not clear where to threshold similarity scores to retrieve the most relevant images. Threshold too\nlate and we allow unrelated images to be included in our pool. Further, this threshold is often specific\nto a category, making it infeasible to search at scale.\n\nOur approach to language-based priming is to search for the existence of the class name, $$c \\in C$$,\nin the captions of our pre-training dataset to retrieve images relevant to a particular category. We\norganize these image-text pairs into separate categorical clusters {Bc} according to the class name c\nmentioned in their captions. This approach has a few advantages over semantic retrieval: 1. After\nsetting up an inverted index search structure for text retrieval [26, 46], exact string matching is far\nfaster than semantic retrieval, even when approximate nearest neighbor strategies are employed [24],\n2. with exact string search, the category boundary is clear and therefore does not require per category\ntuning, 3. the retrieval results are overall qualitatively more relevant. Finally, to leverage the semantic\nunderstanding of our CLIP model, we filter the priming pool using CLIP similarity score. We do\nthis by constructing a \u201czero-shot\u201d CLIP classifier, as defined by Radford et al. [40], and removing\nexamples from categorical clusters that do not align with their label according to the CLIP model."}]}, {"page": 5, "text": "                                                           Transductive Image Filter\n        Initial Diverse Priming Pool                            (ImageNet-V2)                                  Filtered Priming Pool\n             (Great Grey Owl)\nFigure 2: A qualitative example of our approach for transductive image filtering. Given an\ninitial priming pool, acquired through natural-language text search on the captions of our pre-training\ndataset (Section 3.1), we filter out irrelevant examples using images from our test set. (left) we show\nexamples from the great owl categorical cluster of our priming pool before filtering, (center) we\nshow an example image from the same category of ImageNet-V2, (right) example retrievals using\nimage embedding similarity from the entire priming pool. The visual similarity of the retrievals are\napparent, and they are generally from the appropriate categorical cluster. Doing this filtering results\nin a significantly more relevant priming pool.\n3.1.2      Leveraging Image Information at Test Time\nAt inference time, the model can narrow the relevant priming pool even further by utilizing information\nabout the test distribution. To do this, given an image x in our test set, we compute the cosine similarity\nusing our CLIP image encoder V , cos(V (x), V (y)) for every y \u2208                                P  , our priming pool. We retrieve\nexamples with the top-k cosine similarity scores (k = 10 in most of our experiments). We do this\ncollectively for every image in the test set and collect the retrievals to form a fi                            ltered priming pool. If\nan example is retrieved twice, we de-duplicate them in the final priming pool. Since we do this for all\nimages in the test set, we consider this the transductive setting to align with prior work [13, 6].\n3.2     Attuning CLIP to the Priming Pool\nThe goal of this step is to modify our CLIP model to take advantage of the data in the priming pool\nP  . We fi  rst construct the task-specifi           c zero-shot linear head Wz \u2208               Rd\u00d7n using the text encoder and\nthe natural language names of each class, where d is the feature dimension and n is the number\nof classes. To get logits for a particular example, x, we compute Wz \u00b7 L(x), so our prediction is\narg max     c Wz \u00b7 L(x).\nTo attune our CLIP model to the priming pool, we perform nearest-class mean (NCM) [32] on all\nretrieved examples to obtain a classification head from the priming pool. Namely for a given class c,\nwe compute a centroid \u02dc           yc =      1        x\u2208Bc L(x) and then normalize this centroid to produce a class\n                                           |Bc|\nembedding yc = \u02dc         yc/\u2225yc\u2225. We define the collection of centroids as matrix Wft = [yc]c \u2208                                   Rd\u00d7n. To\nexpand this to few-shot scenarios, we mix the labeled data into the corresponding categorical clusters\nbefore performing NCM. Finally, we ensemble Wz and Wft using a mixing coefficient \u03b1 \u2208                                                [0, 1] as\nW\u03b1 = (1 \u2212         \u03b1) \u00b7 Wft + \u03b1 \u00b7 Wz, which is our final classification head. We choose alpha according\nto a heuristic \u03b1 = e\u2212|P |/\u03c32 which can be derived from a Bayesian prior over the text features. We\nexperiment with varying values of \u03c3 in Table 10. We also find that \u03b1 can be effectively chosen\nthrough cross-validation on a held-out portion of the retrieval set. Intuitively, if we do not have much\ndata in our priming pool, we want it to influence our model less. We use NCM as the classifier as it\nhas shown to be sample-efficient [47]. For comparison to a WISE-FT classifier [52] see Table 3.\n4     Experiments\nOur key results include: 1. Priming improves performance over baselines in the few-shot setting by\n3.81% on average across all datasets and 2.4% on ImageNet in the zero-shot setting 2. Priming in the\n                                                                       5", "md": "# Transductive Image Filter\n\n## Initial Diverse Priming Pool (Great Grey Owl) (ImageNet-V2) Filtered Priming Pool\n\nFigure 2: A qualitative example of our approach for transductive image filtering. Given an initial priming pool, acquired through natural-language text search on the captions of our pre-training dataset (Section 3.1), we filter out irrelevant examples using images from our test set. (left) we show examples from the great owl categorical cluster of our priming pool before filtering, (center) we show an example image from the same category of ImageNet-V2, (right) example retrievals using image embedding similarity from the entire priming pool. The visual similarity of the retrievals are apparent, and they are generally from the appropriate categorical cluster. Doing this filtering results in a significantly more relevant priming pool.\n\n### 3.1.2 Leveraging Image Information at Test Time\n\nAt inference time, the model can narrow the relevant priming pool even further by utilizing information about the test distribution. To do this, given an image x in our test set, we compute the cosine similarity using our CLIP image encoder V, $$\\cos(V(x), V(y))$$ for every y in P, our priming pool. We retrieve examples with the top-k cosine similarity scores (k = 10 in most of our experiments). We do this collectively for every image in the test set and collect the retrievals to form a filtered priming pool. If an example is retrieved twice, we de-duplicate them in the final priming pool. Since we do this for all images in the test set, we consider this the transductive setting to align with prior work [13, 6].\n\n### 3.2 Attuning CLIP to the Priming Pool\n\nThe goal of this step is to modify our CLIP model to take advantage of the data in the priming pool P. We first construct the task-specific zero-shot linear head $$W_z \\in \\mathbb{R}^{d \\times n}$$ using the text encoder and the natural language names of each class, where d is the feature dimension and n is the number of classes. To get logits for a particular example, x, we compute $$W_z \\cdot L(x)$$, so our prediction is $$\\arg \\max_c W_z \\cdot L(x)$$. To attune our CLIP model to the priming pool, we perform nearest-class mean (NCM) [32] on all retrieved examples to obtain a classification head from the priming pool. Namely for a given class c, we compute a centroid $$\\tilde{y}_c = \\frac{1}{|B_c|} \\sum_{x \\in B_c} L(x)$$ and then normalize this centroid to produce a class embedding $$y_c = \\frac{\\tilde{y}_c}{\\| \\tilde{y}_c \\|}$$. We define the collection of centroids as matrix $$W_{ft} = [y_c]_c \\in \\mathbb{R}^{d \\times n}$$. To expand this to few-shot scenarios, we mix the labeled data into the corresponding categorical clusters before performing NCM. Finally, we ensemble $$W_z$$ and $$W_{ft}$$ using a mixing coefficient $$\\alpha \\in [0, 1]$$ as $$W_{\\alpha} = (1 - \\alpha) \\cdot W_{ft} + \\alpha \\cdot W_z$$, which is our final classification head. We choose alpha according to a heuristic $$\\alpha = e^{-|P|/\\sigma^2}$$ which can be derived from a Bayesian prior over the text features. We experiment with varying values of $$\\sigma$$ in Table 10. We also find that $$\\alpha$$ can be effectively chosen through cross-validation on a held-out portion of the retrieval set. Intuitively, if we do not have much data in our priming pool, we want it to influence our model less. We use NCM as the classifier as it has shown to be sample-efficient [47]. For comparison to a WISE-FT classifier [52] see Table 3.\n\n## 4 Experiments\n\nOur key results include: 1. Priming improves performance over baselines in the few-shot setting by 3.81% on average across all datasets and 2.4% on ImageNet in the zero-shot setting 2. Priming in the", "images": [{"name": "page-5-1.jpg", "height": 121, "width": 121, "x": 383, "y": 72}, {"name": "page-5-0.jpg", "height": 121, "width": 121, "x": 108, "y": 72}, {"name": "page-5-2.jpg", "height": 96, "width": 96, "x": 258, "y": 84}], "items": [{"type": "heading", "lvl": 1, "value": "Transductive Image Filter", "md": "# Transductive Image Filter"}, {"type": "heading", "lvl": 2, "value": "Initial Diverse Priming Pool (Great Grey Owl) (ImageNet-V2) Filtered Priming Pool", "md": "## Initial Diverse Priming Pool (Great Grey Owl) (ImageNet-V2) Filtered Priming Pool"}, {"type": "text", "value": "Figure 2: A qualitative example of our approach for transductive image filtering. Given an initial priming pool, acquired through natural-language text search on the captions of our pre-training dataset (Section 3.1), we filter out irrelevant examples using images from our test set. (left) we show examples from the great owl categorical cluster of our priming pool before filtering, (center) we show an example image from the same category of ImageNet-V2, (right) example retrievals using image embedding similarity from the entire priming pool. The visual similarity of the retrievals are apparent, and they are generally from the appropriate categorical cluster. Doing this filtering results in a significantly more relevant priming pool.", "md": "Figure 2: A qualitative example of our approach for transductive image filtering. Given an initial priming pool, acquired through natural-language text search on the captions of our pre-training dataset (Section 3.1), we filter out irrelevant examples using images from our test set. (left) we show examples from the great owl categorical cluster of our priming pool before filtering, (center) we show an example image from the same category of ImageNet-V2, (right) example retrievals using image embedding similarity from the entire priming pool. The visual similarity of the retrievals are apparent, and they are generally from the appropriate categorical cluster. Doing this filtering results in a significantly more relevant priming pool."}, {"type": "heading", "lvl": 3, "value": "3.1.2 Leveraging Image Information at Test Time", "md": "### 3.1.2 Leveraging Image Information at Test Time"}, {"type": "text", "value": "At inference time, the model can narrow the relevant priming pool even further by utilizing information about the test distribution. To do this, given an image x in our test set, we compute the cosine similarity using our CLIP image encoder V, $$\\cos(V(x), V(y))$$ for every y in P, our priming pool. We retrieve examples with the top-k cosine similarity scores (k = 10 in most of our experiments). We do this collectively for every image in the test set and collect the retrievals to form a filtered priming pool. If an example is retrieved twice, we de-duplicate them in the final priming pool. Since we do this for all images in the test set, we consider this the transductive setting to align with prior work [13, 6].", "md": "At inference time, the model can narrow the relevant priming pool even further by utilizing information about the test distribution. To do this, given an image x in our test set, we compute the cosine similarity using our CLIP image encoder V, $$\\cos(V(x), V(y))$$ for every y in P, our priming pool. We retrieve examples with the top-k cosine similarity scores (k = 10 in most of our experiments). We do this collectively for every image in the test set and collect the retrievals to form a filtered priming pool. If an example is retrieved twice, we de-duplicate them in the final priming pool. Since we do this for all images in the test set, we consider this the transductive setting to align with prior work [13, 6]."}, {"type": "heading", "lvl": 3, "value": "3.2 Attuning CLIP to the Priming Pool", "md": "### 3.2 Attuning CLIP to the Priming Pool"}, {"type": "text", "value": "The goal of this step is to modify our CLIP model to take advantage of the data in the priming pool P. We first construct the task-specific zero-shot linear head $$W_z \\in \\mathbb{R}^{d \\times n}$$ using the text encoder and the natural language names of each class, where d is the feature dimension and n is the number of classes. To get logits for a particular example, x, we compute $$W_z \\cdot L(x)$$, so our prediction is $$\\arg \\max_c W_z \\cdot L(x)$$. To attune our CLIP model to the priming pool, we perform nearest-class mean (NCM) [32] on all retrieved examples to obtain a classification head from the priming pool. Namely for a given class c, we compute a centroid $$\\tilde{y}_c = \\frac{1}{|B_c|} \\sum_{x \\in B_c} L(x)$$ and then normalize this centroid to produce a class embedding $$y_c = \\frac{\\tilde{y}_c}{\\| \\tilde{y}_c \\|}$$. We define the collection of centroids as matrix $$W_{ft} = [y_c]_c \\in \\mathbb{R}^{d \\times n}$$. To expand this to few-shot scenarios, we mix the labeled data into the corresponding categorical clusters before performing NCM. Finally, we ensemble $$W_z$$ and $$W_{ft}$$ using a mixing coefficient $$\\alpha \\in [0, 1]$$ as $$W_{\\alpha} = (1 - \\alpha) \\cdot W_{ft} + \\alpha \\cdot W_z$$, which is our final classification head. We choose alpha according to a heuristic $$\\alpha = e^{-|P|/\\sigma^2}$$ which can be derived from a Bayesian prior over the text features. We experiment with varying values of $$\\sigma$$ in Table 10. We also find that $$\\alpha$$ can be effectively chosen through cross-validation on a held-out portion of the retrieval set. Intuitively, if we do not have much data in our priming pool, we want it to influence our model less. We use NCM as the classifier as it has shown to be sample-efficient [47]. For comparison to a WISE-FT classifier [52] see Table 3.", "md": "The goal of this step is to modify our CLIP model to take advantage of the data in the priming pool P. We first construct the task-specific zero-shot linear head $$W_z \\in \\mathbb{R}^{d \\times n}$$ using the text encoder and the natural language names of each class, where d is the feature dimension and n is the number of classes. To get logits for a particular example, x, we compute $$W_z \\cdot L(x)$$, so our prediction is $$\\arg \\max_c W_z \\cdot L(x)$$. To attune our CLIP model to the priming pool, we perform nearest-class mean (NCM) [32] on all retrieved examples to obtain a classification head from the priming pool. Namely for a given class c, we compute a centroid $$\\tilde{y}_c = \\frac{1}{|B_c|} \\sum_{x \\in B_c} L(x)$$ and then normalize this centroid to produce a class embedding $$y_c = \\frac{\\tilde{y}_c}{\\| \\tilde{y}_c \\|}$$. We define the collection of centroids as matrix $$W_{ft} = [y_c]_c \\in \\mathbb{R}^{d \\times n}$$. To expand this to few-shot scenarios, we mix the labeled data into the corresponding categorical clusters before performing NCM. Finally, we ensemble $$W_z$$ and $$W_{ft}$$ using a mixing coefficient $$\\alpha \\in [0, 1]$$ as $$W_{\\alpha} = (1 - \\alpha) \\cdot W_{ft} + \\alpha \\cdot W_z$$, which is our final classification head. We choose alpha according to a heuristic $$\\alpha = e^{-|P|/\\sigma^2}$$ which can be derived from a Bayesian prior over the text features. We experiment with varying values of $$\\sigma$$ in Table 10. We also find that $$\\alpha$$ can be effectively chosen through cross-validation on a held-out portion of the retrieval set. Intuitively, if we do not have much data in our priming pool, we want it to influence our model less. We use NCM as the classifier as it has shown to be sample-efficient [47]. For comparison to a WISE-FT classifier [52] see Table 3."}, {"type": "heading", "lvl": 2, "value": "4 Experiments", "md": "## 4 Experiments"}, {"type": "text", "value": "Our key results include: 1. Priming improves performance over baselines in the few-shot setting by 3.81% on average across all datasets and 2.4% on ImageNet in the zero-shot setting 2. Priming in the", "md": "Our key results include: 1. Priming improves performance over baselines in the few-shot setting by 3.81% on average across all datasets and 2.4% on ImageNet in the zero-shot setting 2. Priming in the"}]}, {"page": 6, "text": "Table 1: Performance of Neural Priming and comparable methods in the zero-shot setting.\nPriming consistently improves top-1 accuracy across standard transfer learning data sets. Performance\nreported for the OpenCLIP ViT-B-16 model pretrained on LAION-2B.\n                             ImageNet     Stanford     FGVC      Flowers102     Food101     Oxford    SUN397\n                                            Cars      Aircraft                                Pets\n       CLIP [40, 21]            68.30       87.40      25.86        71.65         86.58      90.21      67.35\n   Retrieval + Finetuning       70.28       87.95      26.22        72.15         86.63      90.35      68.01\n         VLM [33]               69.35       87.88      28.54        72.11         86.31      90.24      67.73\n         CuPL [39]              70.25       88.63      29.64        72.32         86.20      91.16      70.80\n       Priming (Ours)           70.75       89.30      33.03        79.81         86.66      91.87      71.21\n  Priming + CuPL (Ours)         71.38       90.23      36.00        80.04         86.86      91.85      72.35\ntransductive, or on-the-fly, setting further improves performance over baselines by 2.51% accuracy\nand 1.09% over standard Neural Priming 3. Priming is complementary to existing prompt-tuning\nmethods. Our finding indicates that images in the priming set impart distinct information to the model\ncompared to textual class descriptions. We include full details of hyperparameter choices and error\nbars included in the appendix.\n4.1    Datasets and Architectures\nWe evaluate on standard transfer learning and distribution shift benchmarks. ImageNet [7] is a large-\nscale, general classification dataset that has been well-studied in both transfer learning and distribution\nshift. ImageNetV2 [41] is one of its natural distribution shift test sets, made by reproducing the\noriginal data collection procedure of ImageNet, but even modern large-scale pre-trained models\nhave performance drops on it. ImageNet Sketch [51] and ImageNet-R [18] are natural distribution\nshifts created by assembling sketches and various renditions of the ImageNet classes. ImageNet-A\n[19] is a natural adversarial distribution shift of ImageNet, created by collecting images that are\nmisclassified by ResNets. StanfordCars [27], FGVCAircraft [31], Flowers102 [36], and OxfordPets\n[38] are fine-grained classification datasets which require understanding subtle visual differences\nbetween classes and are commonly used for transfer learning benchmarks [17, 21, 40, 55]. SUN397\n[53] is a large-scale scene recognition dataset with 397 scene categories.\nWe perform our experiments with OpenCLIP models [52] trained on LAION-2B and 400M [44]. We\nchoose OpenCLIP because their pretrain datasets are publicly available, therefore we can control what\ndata is introduced to the model. The model architecture reported in the main paper is the B-16 variant\ntrained on LAION-2B unless otherwise stated and we report L-14 and B-32 in the Appendix C.\n4.2    Zero-shot Results\nIn this setting, our model only has access to data it has seen during pre-training, in this case LAION-\n2B. Neural Priming improves top-1 accuracy by 2.45% on ImageNet and 4.25% on average across 6\nother diverse downstream datasets compared to the CLIP baseline. In the zero-shot setting, Neural\nPriming outperforms the 3-shot CLIP model on StanfordCars and FGVCAircraft. This result is\nparticularly noteworthy since traditionally training on in-distribution data generally outperforms\nzero-shot techniques [54]. Note that we do not present error-bars for the zero-shot experiments as the\nprocess is deterministic.\nWe also compare with VLM [33] and CuPL [39], two zero-shot prompt-tuning methods which\nobtain natural language descriptions of each class using language models, and a retrieval with fine-\ntuning baseline. For implementation details on how the retrieval and fine-tuning are performed see\nAppendix H. Interestingly, we find that Neural Priming is complementary to existing prompt-tuning\nmethods. The accuracy improvements from CuPL and VLM are additive with Neural Priming. For\nexample, CuPL and Neural Priming each improve performance by 3.78% and 7.17% respectively on\nFGVCAircraft. Ensembling the methods results in 10.74% improvement over the baseline (Table 2).\nThis surprising result suggests that the textual class descriptions in CuPL and VLM provide unique\ninformation to the model that differ from the information obtained from the images in the priming set.\n                                                        6", "md": "# Neural Priming Performance\n\n## Table 1: Performance of Neural Priming and comparable methods in the zero-shot setting\n\nPriming consistently improves top-1 accuracy across standard transfer learning data sets. Performance reported for the OpenCLIP ViT-B-16 model pretrained on LAION-2B.\n\n| |ImageNet|Stanford Cars|FGVC Aircraft|Flowers102|Food101|Oxford Pets|SUN397|\n|---|---|---|---|---|---|---|---|\n|CLIP [40, 21]|68.30|87.40|25.86|71.65|86.58|90.21|67.35|\n|Retrieval + Finetuning|70.28|87.95|26.22|72.15|86.63|90.35|68.01|\n|VLM [33]|69.35|87.88|28.54|72.11|86.31|90.24|67.73|\n|CuPL [39]|70.25|88.63|29.64|72.32|86.20|91.16|70.80|\n|Priming (Ours)|70.75|89.30|33.03|79.81|86.66|91.87|71.21|\n|Priming + CuPL (Ours)|71.38|90.23|36.00|80.04|86.86|91.85|72.35|\n\nTransductive, or on-the-fly, setting further improves performance over baselines by 2.51% accuracy and 1.09% over standard Neural Priming. Priming is complementary to existing prompt-tuning methods. Our finding indicates that images in the priming set impart distinct information to the model compared to textual class descriptions. We include full details of hyperparameter choices and error bars included in the appendix.\n\n### 4.1 Datasets and Architectures\n\nWe evaluate on standard transfer learning and distribution shift benchmarks. ImageNet is a large-scale, general classification dataset that has been well-studied in both transfer learning and distribution shift. ImageNetV2 is one of its natural distribution shift test sets, made by reproducing the original data collection procedure of ImageNet, but even modern large-scale pre-trained models have performance drops on it. ImageNet Sketch and ImageNet-R are natural distribution shifts created by assembling sketches and various renditions of the ImageNet classes. ImageNet-A is a natural adversarial distribution shift of ImageNet, created by collecting images that are misclassified by ResNets. StanfordCars, FGVCAircraft, Flowers102, and OxfordPets are fine-grained classification datasets which require understanding subtle visual differences between classes and are commonly used for transfer learning benchmarks. SUN397 is a large-scale scene recognition dataset with 397 scene categories.\n\nWe perform our experiments with OpenCLIP models trained on LAION-2B and 400M. We choose OpenCLIP because their pretrain datasets are publicly available, therefore we can control what data is introduced to the model. The model architecture reported in the main paper is the B-16 variant trained on LAION-2B unless otherwise stated and we report L-14 and B-32 in the Appendix C.\n\n### 4.2 Zero-shot Results\n\nIn this setting, our model only has access to data it has seen during pre-training, in this case LAION-2B. Neural Priming improves top-1 accuracy by 2.45% on ImageNet and 4.25% on average across 6 other diverse downstream datasets compared to the CLIP baseline. In the zero-shot setting, Neural Priming outperforms the 3-shot CLIP model on StanfordCars and FGVCAircraft. This result is particularly noteworthy since traditionally training on in-distribution data generally outperforms zero-shot techniques. Note that we do not present error-bars for the zero-shot experiments as the process is deterministic.\n\nWe also compare with VLM and CuPL, two zero-shot prompt-tuning methods which obtain natural language descriptions of each class using language models, and a retrieval with fine-tuning baseline. For implementation details on how the retrieval and fine-tuning are performed see Appendix H. Interestingly, we find that Neural Priming is complementary to existing prompt-tuning methods. The accuracy improvements from CuPL and VLM are additive with Neural Priming. Ensembling the methods results in improvement over the baseline (Table 2). This surprising result suggests that the textual class descriptions in CuPL and VLM provide unique information to the model that differ from the information obtained from the images in the priming set.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Neural Priming Performance", "md": "# Neural Priming Performance"}, {"type": "heading", "lvl": 2, "value": "Table 1: Performance of Neural Priming and comparable methods in the zero-shot setting", "md": "## Table 1: Performance of Neural Priming and comparable methods in the zero-shot setting"}, {"type": "text", "value": "Priming consistently improves top-1 accuracy across standard transfer learning data sets. Performance reported for the OpenCLIP ViT-B-16 model pretrained on LAION-2B.", "md": "Priming consistently improves top-1 accuracy across standard transfer learning data sets. Performance reported for the OpenCLIP ViT-B-16 model pretrained on LAION-2B."}, {"type": "table", "rows": [["", "ImageNet", "Stanford Cars", "FGVC Aircraft", "Flowers102", "Food101", "Oxford Pets", "SUN397"], ["CLIP [40, 21]", "68.30", "87.40", "25.86", "71.65", "86.58", "90.21", "67.35"], ["Retrieval + Finetuning", "70.28", "87.95", "26.22", "72.15", "86.63", "90.35", "68.01"], ["VLM [33]", "69.35", "87.88", "28.54", "72.11", "86.31", "90.24", "67.73"], ["CuPL [39]", "70.25", "88.63", "29.64", "72.32", "86.20", "91.16", "70.80"], ["Priming (Ours)", "70.75", "89.30", "33.03", "79.81", "86.66", "91.87", "71.21"], ["Priming + CuPL (Ours)", "71.38", "90.23", "36.00", "80.04", "86.86", "91.85", "72.35"]], "md": "| |ImageNet|Stanford Cars|FGVC Aircraft|Flowers102|Food101|Oxford Pets|SUN397|\n|---|---|---|---|---|---|---|---|\n|CLIP [40, 21]|68.30|87.40|25.86|71.65|86.58|90.21|67.35|\n|Retrieval + Finetuning|70.28|87.95|26.22|72.15|86.63|90.35|68.01|\n|VLM [33]|69.35|87.88|28.54|72.11|86.31|90.24|67.73|\n|CuPL [39]|70.25|88.63|29.64|72.32|86.20|91.16|70.80|\n|Priming (Ours)|70.75|89.30|33.03|79.81|86.66|91.87|71.21|\n|Priming + CuPL (Ours)|71.38|90.23|36.00|80.04|86.86|91.85|72.35|", "isPerfectTable": true, "csv": "\"\",\"ImageNet\",\"Stanford Cars\",\"FGVC Aircraft\",\"Flowers102\",\"Food101\",\"Oxford Pets\",\"SUN397\"\n\"CLIP [40, 21]\",\"68.30\",\"87.40\",\"25.86\",\"71.65\",\"86.58\",\"90.21\",\"67.35\"\n\"Retrieval + Finetuning\",\"70.28\",\"87.95\",\"26.22\",\"72.15\",\"86.63\",\"90.35\",\"68.01\"\n\"VLM [33]\",\"69.35\",\"87.88\",\"28.54\",\"72.11\",\"86.31\",\"90.24\",\"67.73\"\n\"CuPL [39]\",\"70.25\",\"88.63\",\"29.64\",\"72.32\",\"86.20\",\"91.16\",\"70.80\"\n\"Priming (Ours)\",\"70.75\",\"89.30\",\"33.03\",\"79.81\",\"86.66\",\"91.87\",\"71.21\"\n\"Priming + CuPL (Ours)\",\"71.38\",\"90.23\",\"36.00\",\"80.04\",\"86.86\",\"91.85\",\"72.35\""}, {"type": "text", "value": "Transductive, or on-the-fly, setting further improves performance over baselines by 2.51% accuracy and 1.09% over standard Neural Priming. Priming is complementary to existing prompt-tuning methods. Our finding indicates that images in the priming set impart distinct information to the model compared to textual class descriptions. We include full details of hyperparameter choices and error bars included in the appendix.", "md": "Transductive, or on-the-fly, setting further improves performance over baselines by 2.51% accuracy and 1.09% over standard Neural Priming. Priming is complementary to existing prompt-tuning methods. Our finding indicates that images in the priming set impart distinct information to the model compared to textual class descriptions. We include full details of hyperparameter choices and error bars included in the appendix."}, {"type": "heading", "lvl": 3, "value": "4.1 Datasets and Architectures", "md": "### 4.1 Datasets and Architectures"}, {"type": "text", "value": "We evaluate on standard transfer learning and distribution shift benchmarks. ImageNet is a large-scale, general classification dataset that has been well-studied in both transfer learning and distribution shift. ImageNetV2 is one of its natural distribution shift test sets, made by reproducing the original data collection procedure of ImageNet, but even modern large-scale pre-trained models have performance drops on it. ImageNet Sketch and ImageNet-R are natural distribution shifts created by assembling sketches and various renditions of the ImageNet classes. ImageNet-A is a natural adversarial distribution shift of ImageNet, created by collecting images that are misclassified by ResNets. StanfordCars, FGVCAircraft, Flowers102, and OxfordPets are fine-grained classification datasets which require understanding subtle visual differences between classes and are commonly used for transfer learning benchmarks. SUN397 is a large-scale scene recognition dataset with 397 scene categories.\n\nWe perform our experiments with OpenCLIP models trained on LAION-2B and 400M. We choose OpenCLIP because their pretrain datasets are publicly available, therefore we can control what data is introduced to the model. The model architecture reported in the main paper is the B-16 variant trained on LAION-2B unless otherwise stated and we report L-14 and B-32 in the Appendix C.", "md": "We evaluate on standard transfer learning and distribution shift benchmarks. ImageNet is a large-scale, general classification dataset that has been well-studied in both transfer learning and distribution shift. ImageNetV2 is one of its natural distribution shift test sets, made by reproducing the original data collection procedure of ImageNet, but even modern large-scale pre-trained models have performance drops on it. ImageNet Sketch and ImageNet-R are natural distribution shifts created by assembling sketches and various renditions of the ImageNet classes. ImageNet-A is a natural adversarial distribution shift of ImageNet, created by collecting images that are misclassified by ResNets. StanfordCars, FGVCAircraft, Flowers102, and OxfordPets are fine-grained classification datasets which require understanding subtle visual differences between classes and are commonly used for transfer learning benchmarks. SUN397 is a large-scale scene recognition dataset with 397 scene categories.\n\nWe perform our experiments with OpenCLIP models trained on LAION-2B and 400M. We choose OpenCLIP because their pretrain datasets are publicly available, therefore we can control what data is introduced to the model. The model architecture reported in the main paper is the B-16 variant trained on LAION-2B unless otherwise stated and we report L-14 and B-32 in the Appendix C."}, {"type": "heading", "lvl": 3, "value": "4.2 Zero-shot Results", "md": "### 4.2 Zero-shot Results"}, {"type": "text", "value": "In this setting, our model only has access to data it has seen during pre-training, in this case LAION-2B. Neural Priming improves top-1 accuracy by 2.45% on ImageNet and 4.25% on average across 6 other diverse downstream datasets compared to the CLIP baseline. In the zero-shot setting, Neural Priming outperforms the 3-shot CLIP model on StanfordCars and FGVCAircraft. This result is particularly noteworthy since traditionally training on in-distribution data generally outperforms zero-shot techniques. Note that we do not present error-bars for the zero-shot experiments as the process is deterministic.\n\nWe also compare with VLM and CuPL, two zero-shot prompt-tuning methods which obtain natural language descriptions of each class using language models, and a retrieval with fine-tuning baseline. For implementation details on how the retrieval and fine-tuning are performed see Appendix H. Interestingly, we find that Neural Priming is complementary to existing prompt-tuning methods. The accuracy improvements from CuPL and VLM are additive with Neural Priming. Ensembling the methods results in improvement over the baseline (Table 2). This surprising result suggests that the textual class descriptions in CuPL and VLM provide unique information to the model that differ from the information obtained from the images in the priming set.", "md": "In this setting, our model only has access to data it has seen during pre-training, in this case LAION-2B. Neural Priming improves top-1 accuracy by 2.45% on ImageNet and 4.25% on average across 6 other diverse downstream datasets compared to the CLIP baseline. In the zero-shot setting, Neural Priming outperforms the 3-shot CLIP model on StanfordCars and FGVCAircraft. This result is particularly noteworthy since traditionally training on in-distribution data generally outperforms zero-shot techniques. Note that we do not present error-bars for the zero-shot experiments as the process is deterministic.\n\nWe also compare with VLM and CuPL, two zero-shot prompt-tuning methods which obtain natural language descriptions of each class using language models, and a retrieval with fine-tuning baseline. For implementation details on how the retrieval and fine-tuning are performed see Appendix H. Interestingly, we find that Neural Priming is complementary to existing prompt-tuning methods. The accuracy improvements from CuPL and VLM are additive with Neural Priming. Ensembling the methods results in improvement over the baseline (Table 2). This surprising result suggests that the textual class descriptions in CuPL and VLM provide unique information to the model that differ from the information obtained from the images in the priming set."}]}, {"page": 7, "text": "                              ImageNet                                                     Stanford Cars\n          72                                                             91\n          71\n         Accuracy (%)                                                   Accuracy (%)\n          70                                                             90\n          69\n          68                       Neural Priming                        89                       Neural Priming\n                                   CoOp                                                           CoOp\n          67                       NCM (Baseline)                        88                       NCM (Baseline)\n          66                       Zero-shot OpenCLIP                                             Zero-shot OpenCLIP\n               0   1   2          5                  10                       0   1   2          5                  10\n                   # Training Samples per Class                                    # Training Samples per Class\n                             FGCV-Aircraft                               95                 Flowers102\n          42.5\n          40.0                                                           90\n         Accuracy (%)                                                   Accuracy (%)\n          37.5                                                           85\n          35.0\n          32.5                      Neural Priming                       80                       Neural Priming\n          30.0                      CoOp                                                          CoOp\n          27.5                      NCM (Baseline)                       75                       NCM (Baseline)\n          25.0                      Zero-shot OpenCLIP                                            Zero-shot OpenCLIP\n                0   1   2          5                 10                       0   1   2          5                  10\n                     # Training Samples per Class                                  # Training Samples per Class\nFigure 3: Performance of Neural Priming and comparable methods in the few-shot setting.\nWe find consistent improvement across shot numbers and datasets. In particular, Neural Priming\nespecially excels for fine-grained datasets such as FGVCAircraft and Flowers102. We hypothesize\nthat such fine-grained captioned images are not well represented in LAION-2B, therefore revisting\nthis subset of data improves the model more.\nAnother observation is that Neural Priming is especially effective for specialized domains such as\nStanfordCars, Flowers102, and FGVCAircraft. We speculate this is due to the fact that the label\nspace and image content differs from the majority of the pre-training set. For example, although\nairplanes occur frequently in LAION-2B, they are rarely described according to their specific model\nsuch as Boeing 737-200. Therefore, recalling and priming the model on pre-train images with such\nfine-grained classes significantly improves the model. For analysis of LAION-2B with regards to\nlabel statistics see Appendix B.\nIn contrast, for datasets which are more aligned with LAION-2B and the distribution of internet\nimages, such as ImageNet and SUN397, the accuracy gain provided from Neural Priming is smaller\nin comparison, albeit still significant. In the limit of this trend, Food101 sees almost no improvement\nacross all methods, and even training on in-distribution data for the few-shot case barely improves\nthe accuracy. We speculate that this is because images similar to those in Food101 are already well-\nrepresented in LAION-2B, rendering additional food images of marginal informational value. We\nprovide analysis of how well the attributes of each dataset are captured by LAION-2B in Appendix B.\nTo be precise, when we refer to term \u201cshot number\u201d throughout the experiments section, we mean the\nnumber of labeled examples from the target training set. We do not consider images retrieved from\nLAION-2B as shots in this setting because they are obtained from the pre-training set.\n4.3    Few-Shot Results\nNeural Priming improves performance for all datasets and shots in the few-shot setting. We compare\nwith CoOp, a recent method for few-shot prompt-tuning, and a Nearest-class-Mean (NCM) baseline.\nOn average across all shots and datasets Neural Priming improves by 3.81% in accuracy over the\nclosest baseline. Results can be found in Figure 3 and Table 9 of the Appendix.\nNotably, we find that Neural Priming can match the accuracy of models trained with a substantial\nnumber of training examples without using any of the labeled training data for all of the evaluated\n                                                              7", "md": "# Neural Priming Performance\n\n## Neural Priming Performance\n\n| |ImageNet|Stanford Cars|\n|---|---|---|\n|72| |91|\n|71| | |\n|Accuracy (%)|Accuracy (%)| |\n|70| |90|\n|69| | |\n|68|Neural Priming CoOp|89 Neural Priming CoOp|\n|67|NCM (Baseline)|88 NCM (Baseline)|\n|66|Zero-shot OpenCLIP|Zero-shot OpenCLIP|\n\n| |# Training Samples per Class| |# Training Samples per Class|\n|---|---|---|---|\n| |FGCV-Aircraft|95|Flowers102|\n|42.5| |90| |\n|40.0|Accuracy (%)|Accuracy (%)| |\n|37.5| |85| |\n|35.0| | | |\n|32.5|Neural Priming CoOp|80 Neural Priming CoOp| |\n|30.0|NCM (Baseline)|75 NCM (Baseline)| |\n|27.5|Zero-shot OpenCLIP|Zero-shot OpenCLIP| |\n\nFigure 3: Performance of Neural Priming and comparable methods in the few-shot setting.\n\nWe find consistent improvement across shot numbers and datasets. In particular, Neural Priming especially excels for fine-grained datasets such as FGVCAircraft and Flowers102. We hypothesize that such fine-grained captioned images are not well represented in LAION-2B, therefore revisiting this subset of data improves the model more.\n\nAnother observation is that Neural Priming is especially effective for specialized domains such as StanfordCars, Flowers102, and FGVCAircraft. We speculate this is due to the fact that the label space and image content differs from the majority of the pre-training set. For example, although airplanes occur frequently in LAION-2B, they are rarely described according to their specific model such as Boeing 737-200. Therefore, recalling and priming the model on pre-train images with such fine-grained classes significantly improves the model. For analysis of LAION-2B with regards to label statistics see Appendix B.\n\nIn contrast, for datasets which are more aligned with LAION-2B and the distribution of internet images, such as ImageNet and SUN397, the accuracy gain provided from Neural Priming is smaller in comparison, albeit still significant. In the limit of this trend, Food101 sees almost no improvement across all methods, and even training on in-distribution data for the few-shot case barely improves the accuracy. We speculate that this is because images similar to those in Food101 are already well-represented in LAION-2B, rendering additional food images of marginal informational value. We provide analysis of how well the attributes of each dataset are captured by LAION-2B in Appendix B.\n\nTo be precise, when we refer to term \u201cshot number\u201d throughout the experiments section, we mean the number of labeled examples from the target training set. We do not consider images retrieved from LAION-2B as shots in this setting because they are obtained from the pre-training set.\n\n### Few-Shot Results\n\nNeural Priming improves performance for all datasets and shots in the few-shot setting. We compare with CoOp, a recent method for few-shot prompt-tuning, and a Nearest-class-Mean (NCM) baseline. On average across all shots and datasets Neural Priming improves by 3.81% in accuracy over the closest baseline. Results can be found in Figure 3 and Table 9 of the Appendix.\n\nNotably, we find that Neural Priming can match the accuracy of models trained with a substantial number of training examples without using any of the labeled training data for all of the evaluated.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Neural Priming Performance", "md": "# Neural Priming Performance"}, {"type": "heading", "lvl": 2, "value": "Neural Priming Performance", "md": "## Neural Priming Performance"}, {"type": "table", "rows": [["", "ImageNet", "Stanford Cars"], ["72", "", "91"], ["71", "", ""], ["Accuracy (%)", "Accuracy (%)", ""], ["70", "", "90"], ["69", "", ""], ["68", "Neural Priming CoOp", "89 Neural Priming CoOp"], ["67", "NCM (Baseline)", "88 NCM (Baseline)"], ["66", "Zero-shot OpenCLIP", "Zero-shot OpenCLIP"]], "md": "| |ImageNet|Stanford Cars|\n|---|---|---|\n|72| |91|\n|71| | |\n|Accuracy (%)|Accuracy (%)| |\n|70| |90|\n|69| | |\n|68|Neural Priming CoOp|89 Neural Priming CoOp|\n|67|NCM (Baseline)|88 NCM (Baseline)|\n|66|Zero-shot OpenCLIP|Zero-shot OpenCLIP|", "isPerfectTable": true, "csv": "\"\",\"ImageNet\",\"Stanford Cars\"\n\"72\",\"\",\"91\"\n\"71\",\"\",\"\"\n\"Accuracy (%)\",\"Accuracy (%)\",\"\"\n\"70\",\"\",\"90\"\n\"69\",\"\",\"\"\n\"68\",\"Neural Priming CoOp\",\"89 Neural Priming CoOp\"\n\"67\",\"NCM (Baseline)\",\"88 NCM (Baseline)\"\n\"66\",\"Zero-shot OpenCLIP\",\"Zero-shot OpenCLIP\""}, {"type": "table", "rows": [["", "# Training Samples per Class", "", "# Training Samples per Class"], ["", "FGCV-Aircraft", "95", "Flowers102"], ["42.5", "", "90", ""], ["40.0", "Accuracy (%)", "Accuracy (%)", ""], ["37.5", "", "85", ""], ["35.0", "", "", ""], ["32.5", "Neural Priming CoOp", "80 Neural Priming CoOp", ""], ["30.0", "NCM (Baseline)", "75 NCM (Baseline)", ""], ["27.5", "Zero-shot OpenCLIP", "Zero-shot OpenCLIP", ""]], "md": "| |# Training Samples per Class| |# Training Samples per Class|\n|---|---|---|---|\n| |FGCV-Aircraft|95|Flowers102|\n|42.5| |90| |\n|40.0|Accuracy (%)|Accuracy (%)| |\n|37.5| |85| |\n|35.0| | | |\n|32.5|Neural Priming CoOp|80 Neural Priming CoOp| |\n|30.0|NCM (Baseline)|75 NCM (Baseline)| |\n|27.5|Zero-shot OpenCLIP|Zero-shot OpenCLIP| |", "isPerfectTable": true, "csv": "\"\",\"# Training Samples per Class\",\"\",\"# Training Samples per Class\"\n\"\",\"FGCV-Aircraft\",\"95\",\"Flowers102\"\n\"42.5\",\"\",\"90\",\"\"\n\"40.0\",\"Accuracy (%)\",\"Accuracy (%)\",\"\"\n\"37.5\",\"\",\"85\",\"\"\n\"35.0\",\"\",\"\",\"\"\n\"32.5\",\"Neural Priming CoOp\",\"80 Neural Priming CoOp\",\"\"\n\"30.0\",\"NCM (Baseline)\",\"75 NCM (Baseline)\",\"\"\n\"27.5\",\"Zero-shot OpenCLIP\",\"Zero-shot OpenCLIP\",\"\""}, {"type": "text", "value": "Figure 3: Performance of Neural Priming and comparable methods in the few-shot setting.\n\nWe find consistent improvement across shot numbers and datasets. In particular, Neural Priming especially excels for fine-grained datasets such as FGVCAircraft and Flowers102. We hypothesize that such fine-grained captioned images are not well represented in LAION-2B, therefore revisiting this subset of data improves the model more.\n\nAnother observation is that Neural Priming is especially effective for specialized domains such as StanfordCars, Flowers102, and FGVCAircraft. We speculate this is due to the fact that the label space and image content differs from the majority of the pre-training set. For example, although airplanes occur frequently in LAION-2B, they are rarely described according to their specific model such as Boeing 737-200. Therefore, recalling and priming the model on pre-train images with such fine-grained classes significantly improves the model. For analysis of LAION-2B with regards to label statistics see Appendix B.\n\nIn contrast, for datasets which are more aligned with LAION-2B and the distribution of internet images, such as ImageNet and SUN397, the accuracy gain provided from Neural Priming is smaller in comparison, albeit still significant. In the limit of this trend, Food101 sees almost no improvement across all methods, and even training on in-distribution data for the few-shot case barely improves the accuracy. We speculate that this is because images similar to those in Food101 are already well-represented in LAION-2B, rendering additional food images of marginal informational value. We provide analysis of how well the attributes of each dataset are captured by LAION-2B in Appendix B.\n\nTo be precise, when we refer to term \u201cshot number\u201d throughout the experiments section, we mean the number of labeled examples from the target training set. We do not consider images retrieved from LAION-2B as shots in this setting because they are obtained from the pre-training set.", "md": "Figure 3: Performance of Neural Priming and comparable methods in the few-shot setting.\n\nWe find consistent improvement across shot numbers and datasets. In particular, Neural Priming especially excels for fine-grained datasets such as FGVCAircraft and Flowers102. We hypothesize that such fine-grained captioned images are not well represented in LAION-2B, therefore revisiting this subset of data improves the model more.\n\nAnother observation is that Neural Priming is especially effective for specialized domains such as StanfordCars, Flowers102, and FGVCAircraft. We speculate this is due to the fact that the label space and image content differs from the majority of the pre-training set. For example, although airplanes occur frequently in LAION-2B, they are rarely described according to their specific model such as Boeing 737-200. Therefore, recalling and priming the model on pre-train images with such fine-grained classes significantly improves the model. For analysis of LAION-2B with regards to label statistics see Appendix B.\n\nIn contrast, for datasets which are more aligned with LAION-2B and the distribution of internet images, such as ImageNet and SUN397, the accuracy gain provided from Neural Priming is smaller in comparison, albeit still significant. In the limit of this trend, Food101 sees almost no improvement across all methods, and even training on in-distribution data for the few-shot case barely improves the accuracy. We speculate that this is because images similar to those in Food101 are already well-represented in LAION-2B, rendering additional food images of marginal informational value. We provide analysis of how well the attributes of each dataset are captured by LAION-2B in Appendix B.\n\nTo be precise, when we refer to term \u201cshot number\u201d throughout the experiments section, we mean the number of labeled examples from the target training set. We do not consider images retrieved from LAION-2B as shots in this setting because they are obtained from the pre-training set."}, {"type": "heading", "lvl": 3, "value": "Few-Shot Results", "md": "### Few-Shot Results"}, {"type": "text", "value": "Neural Priming improves performance for all datasets and shots in the few-shot setting. We compare with CoOp, a recent method for few-shot prompt-tuning, and a Nearest-class-Mean (NCM) baseline. On average across all shots and datasets Neural Priming improves by 3.81% in accuracy over the closest baseline. Results can be found in Figure 3 and Table 9 of the Appendix.\n\nNotably, we find that Neural Priming can match the accuracy of models trained with a substantial number of training examples without using any of the labeled training data for all of the evaluated.", "md": "Neural Priming improves performance for all datasets and shots in the few-shot setting. We compare with CoOp, a recent method for few-shot prompt-tuning, and a Nearest-class-Mean (NCM) baseline. On average across all shots and datasets Neural Priming improves by 3.81% in accuracy over the closest baseline. Results can be found in Figure 3 and Table 9 of the Appendix.\n\nNotably, we find that Neural Priming can match the accuracy of models trained with a substantial number of training examples without using any of the labeled training data for all of the evaluated."}]}, {"page": 8, "text": "Table 2: Performance of Neural Priming and relevant methods for the transductive setting.\nNeural Priming finds examples similar to the test image at inference to optimize the model. Models\nare evaluated zero-shot on 4 distribution shift datasets. Neural Priming excels on distribution shifts\nwhich differ significantly from the natural language description of the class names. Performance\nreported for the OpenCLIP ViT-B-16 model pretrained on LAION-2B.\n                                                            ImageNet-V2                  ImageNet-R                ImageNet             ImageNet-A\n                                                                                                                      Sketch\n                     CLIP [21, 40]                                 59.35                       64.57                   57.05                  35.95\n                         TPT [45]                                  59.84                       78.74                   52.75                  36.92\n                   Priming (Ours)                                  60.12                       77.98                   58.29                  37.56\n          Transduct. Priming (Ours)                                60.76                       79.37                   59.97                  38.20\ndatasets (Figure 3). Similar to the zero-shot setting, we observe that Neural Priming is complementary\nwith prompt-tuning methods (Appendix F). Additionally, we observe that as the shot number increases,\nimprovement over the baseline decreases. At 1-shot the improvement in accuracy over the baselines\nis 5.63% on average, while at 10-shot the improvement is 2.04%. Intuitively, as the model receives\nmore target training data, obtaining additional examples from the pretrain set becomes less necessary.\n4.4      Transductive Results\nWe compare Neural Priming in the transductive setting on 4 standard distribution shift datasets,\nImageNet-V2, ImageNet Sketch, ImageNet-R and ImageNet-A. Distribution shift datasets are a\nnatural application of adaptation at test-time. Often real-world datasets differ from the training data,\ntherefore models should be able to adapt on-the-fly. In this setting, the model can learn from the test\nimages without labels before making predictions. We compare with Test-Time Prompt-Tuning (TPT),\na state-of-the-art method which uses a self-supervised objective to learn from test data.\nWe find that Neural Priming with images in the test set improves performance over standard Neural\nPriming by 1.09% as well as 2.51% over TPT across the 4 distribution shifts (Table 2). Looking\nat Figure 2, we qualitatively see that the priming pool more closely matches the test images after\nfiltering for the closest images in the initial priming pool. Though the distribution shift can often be\nimperceptible such as between ImageNet and ImageNetv2, quantitatively we see that the transductive\nfiltering step fi        nds images in the pretraining close to the test distribution.\nThe transductive retrieval for 50,000 images in the test set on average takes 96 seconds for a priming\npool of 1 million images, while retraining the classifier takes on average 11.5 seconds for a priming\npool of size 10,000 on standard hardware. We provide further analysis of run-time efficiency of the\non-the-fly variant of Neural Priming in Appendix G.\n   80           Flowers-102                   90           Stanford Cars                 36           FGVC Aircraft                              Oxford Pets\n  op-1 Accuracy                              op-1 Accuracy                              op-1 Accuracy                          op-1 Accuracy\n                                                                                         34                                     91.75\n   78                                         89                                         32                                     91.50\n   76                                                                                    30                                     91.25\n  T                                          T88                                        T28                                    T91.00\n           20    40    60     80    100               20    40    60     80    100               20    40     60    80    100              20    40     60    80  100\n             Samples per class                          Samples per class                          Samples per class                          Samples per class\nFigure 4: Ablation over the number of samples per class in the priming pool. We observe a\nconsistent zero-shot accuracy improvement as the number of samples drawn from our pool increases.\n4.5      Ablations\nWe investigate the impact of the priming pool size on the zero-shot accuracy of downstream tasks\n(Figure 4). Our analysis reveals that as the size of the priming pool increases, there is a general\nimprovement in accuracy. However, there are certain limitations associated with enlarging the\npool. The majority of classes in the downstream task have a limited number of available images.\n                                                                                    8", "md": "Table 2: Performance of Neural Priming and relevant mepods for pe transductive setting\nNeural Priming finds examples similar to pe test image at inference to optimize pe model. Models are evaluated zero-shot on 4 distribution shift datasets. Neural Priming excels on distribution shifts which differ significantly from pe natural language description of pe class names. Performance reported for pe OpenCLIP ViT-B-16 model pretrained on LAION-2B.\n\n| Model               | ImageNet-V2 | ImageNet-R | ImageNet | ImageNet-A Sketch |\n|---------------------|-------------|------------|----------|-------------------|\n| CLIP [21, 40]       | 59.35       | 64.57      | 57.05    | 35.95             |\n| TPT [45]            | 59.84       | 78.74      | 52.75    | 36.92             |\n| Priming (Ours)      | 60.12       | 77.98      | 58.29    | 37.56             |\n| Transduct. Priming (Ours) | 60.76  | 79.37      | 59.97    | 38.20             |\n\nWe compare Neural Priming in the transductive setting on 4 standard distribution shift datasets, ImageNet-V2, ImageNet Sketch, ImageNet-R and ImageNet-A. Distribution shift datasets are a natural application of adaptation at test-time. Often real-world datasets differ from the training data, therefore models should be able to adapt on-the-fly. In this setting, the model can learn from the test images without labels before making predictions. We compare with Test-Time Prompt-Tuning (TPT), a state-of-the-art method which uses a self-supervised objective to learn from test data.\n\nWe find that Neural Priming with images in the test set improves performance over standard Neural Priming by 1.09% as well as 2.51% over TPT across the 4 distribution shifts (Table 2). Looking at Figure 2, we qualitatively see that the priming pool more closely matches the test images after filtering for the closest images in the initial priming pool. Though the distribution shift can often be imperceptible such as between ImageNet and ImageNetv2, quantitatively we see that the transductive filtering step finds images in the pretraining close to the test distribution.\n\nThe transductive retrieval for 50,000 images in the test set on average takes 96 seconds for a priming pool of 1 million images, while retraining the classifier takes on average 11.5 seconds for a priming pool of size 10,000 on standard hardware. We provide further analysis of run-time efficiency of the on-the-fly variant of Neural Priming in Appendix G.\n\n| |Flowers-102|Stanford Cars|FGVC Aircraft|Oxford Pets|\n|---|---|---|---|---|\n|Top-1 Accuracy|78|89|32|91.75|\n| |76| |30|91.50|\n|T|88|T|28|T|\n\nFigure 4: Ablation over the number of samples per class in the priming pool. We observe a consistent zero-shot accuracy improvement as the number of samples drawn from our pool increases.\n\nWe investigate the impact of the priming pool size on the zero-shot accuracy of downstream tasks (Figure 4). Our analysis reveals that as the size of the priming pool increases, there is a general improvement in accuracy. However, there are certain limitations associated with enlarging the pool. The majority of classes in the downstream task have a limited number of available images.", "images": [], "items": [{"type": "text", "value": "Table 2: Performance of Neural Priming and relevant mepods for pe transductive setting\nNeural Priming finds examples similar to pe test image at inference to optimize pe model. Models are evaluated zero-shot on 4 distribution shift datasets. Neural Priming excels on distribution shifts which differ significantly from pe natural language description of pe class names. Performance reported for pe OpenCLIP ViT-B-16 model pretrained on LAION-2B.", "md": "Table 2: Performance of Neural Priming and relevant mepods for pe transductive setting\nNeural Priming finds examples similar to pe test image at inference to optimize pe model. Models are evaluated zero-shot on 4 distribution shift datasets. Neural Priming excels on distribution shifts which differ significantly from pe natural language description of pe class names. Performance reported for pe OpenCLIP ViT-B-16 model pretrained on LAION-2B."}, {"type": "table", "rows": [["Model", "ImageNet-V2", "ImageNet-R", "ImageNet", "ImageNet-A Sketch"], ["CLIP [21, 40]", "59.35", "64.57", "57.05", "35.95"], ["TPT [45]", "59.84", "78.74", "52.75", "36.92"], ["Priming (Ours)", "60.12", "77.98", "58.29", "37.56"], ["Transduct. Priming (Ours)", "60.76", "79.37", "59.97", "38.20"]], "md": "| Model               | ImageNet-V2 | ImageNet-R | ImageNet | ImageNet-A Sketch |\n|---------------------|-------------|------------|----------|-------------------|\n| CLIP [21, 40]       | 59.35       | 64.57      | 57.05    | 35.95             |\n| TPT [45]            | 59.84       | 78.74      | 52.75    | 36.92             |\n| Priming (Ours)      | 60.12       | 77.98      | 58.29    | 37.56             |\n| Transduct. Priming (Ours) | 60.76  | 79.37      | 59.97    | 38.20             |", "isPerfectTable": true, "csv": "\"Model\",\"ImageNet-V2\",\"ImageNet-R\",\"ImageNet\",\"ImageNet-A Sketch\"\n\"CLIP [21, 40]\",\"59.35\",\"64.57\",\"57.05\",\"35.95\"\n\"TPT [45]\",\"59.84\",\"78.74\",\"52.75\",\"36.92\"\n\"Priming (Ours)\",\"60.12\",\"77.98\",\"58.29\",\"37.56\"\n\"Transduct. Priming (Ours)\",\"60.76\",\"79.37\",\"59.97\",\"38.20\""}, {"type": "text", "value": "We compare Neural Priming in the transductive setting on 4 standard distribution shift datasets, ImageNet-V2, ImageNet Sketch, ImageNet-R and ImageNet-A. Distribution shift datasets are a natural application of adaptation at test-time. Often real-world datasets differ from the training data, therefore models should be able to adapt on-the-fly. In this setting, the model can learn from the test images without labels before making predictions. We compare with Test-Time Prompt-Tuning (TPT), a state-of-the-art method which uses a self-supervised objective to learn from test data.\n\nWe find that Neural Priming with images in the test set improves performance over standard Neural Priming by 1.09% as well as 2.51% over TPT across the 4 distribution shifts (Table 2). Looking at Figure 2, we qualitatively see that the priming pool more closely matches the test images after filtering for the closest images in the initial priming pool. Though the distribution shift can often be imperceptible such as between ImageNet and ImageNetv2, quantitatively we see that the transductive filtering step finds images in the pretraining close to the test distribution.\n\nThe transductive retrieval for 50,000 images in the test set on average takes 96 seconds for a priming pool of 1 million images, while retraining the classifier takes on average 11.5 seconds for a priming pool of size 10,000 on standard hardware. We provide further analysis of run-time efficiency of the on-the-fly variant of Neural Priming in Appendix G.", "md": "We compare Neural Priming in the transductive setting on 4 standard distribution shift datasets, ImageNet-V2, ImageNet Sketch, ImageNet-R and ImageNet-A. Distribution shift datasets are a natural application of adaptation at test-time. Often real-world datasets differ from the training data, therefore models should be able to adapt on-the-fly. In this setting, the model can learn from the test images without labels before making predictions. We compare with Test-Time Prompt-Tuning (TPT), a state-of-the-art method which uses a self-supervised objective to learn from test data.\n\nWe find that Neural Priming with images in the test set improves performance over standard Neural Priming by 1.09% as well as 2.51% over TPT across the 4 distribution shifts (Table 2). Looking at Figure 2, we qualitatively see that the priming pool more closely matches the test images after filtering for the closest images in the initial priming pool. Though the distribution shift can often be imperceptible such as between ImageNet and ImageNetv2, quantitatively we see that the transductive filtering step finds images in the pretraining close to the test distribution.\n\nThe transductive retrieval for 50,000 images in the test set on average takes 96 seconds for a priming pool of 1 million images, while retraining the classifier takes on average 11.5 seconds for a priming pool of size 10,000 on standard hardware. We provide further analysis of run-time efficiency of the on-the-fly variant of Neural Priming in Appendix G."}, {"type": "table", "rows": [["", "Flowers-102", "Stanford Cars", "FGVC Aircraft", "Oxford Pets"], ["Top-1 Accuracy", "78", "89", "32", "91.75"], ["", "76", "", "30", "91.50"], ["T", "88", "T", "28", "T"]], "md": "| |Flowers-102|Stanford Cars|FGVC Aircraft|Oxford Pets|\n|---|---|---|---|---|\n|Top-1 Accuracy|78|89|32|91.75|\n| |76| |30|91.50|\n|T|88|T|28|T|", "isPerfectTable": true, "csv": "\"\",\"Flowers-102\",\"Stanford Cars\",\"FGVC Aircraft\",\"Oxford Pets\"\n\"Top-1 Accuracy\",\"78\",\"89\",\"32\",\"91.75\"\n\"\",\"76\",\"\",\"30\",\"91.50\"\n\"T\",\"88\",\"T\",\"28\",\"T\""}, {"type": "text", "value": "Figure 4: Ablation over the number of samples per class in the priming pool. We observe a consistent zero-shot accuracy improvement as the number of samples drawn from our pool increases.\n\nWe investigate the impact of the priming pool size on the zero-shot accuracy of downstream tasks (Figure 4). Our analysis reveals that as the size of the priming pool increases, there is a general improvement in accuracy. However, there are certain limitations associated with enlarging the pool. The majority of classes in the downstream task have a limited number of available images.", "md": "Figure 4: Ablation over the number of samples per class in the priming pool. We observe a consistent zero-shot accuracy improvement as the number of samples drawn from our pool increases.\n\nWe investigate the impact of the priming pool size on the zero-shot accuracy of downstream tasks (Figure 4). Our analysis reveals that as the size of the priming pool increases, there is a general improvement in accuracy. However, there are certain limitations associated with enlarging the pool. The majority of classes in the downstream task have a limited number of available images."}]}, {"page": 9, "text": "Consequently, when we retrieve a larger number of images for the priming pool, they tend to contain\nmore noise and mislabeled samples. Furthermore, for rare classes, the number of images obtained\nthrough exact string search is often less than 100. To address this, a potential extension could involve\nutilizing a language model to generate alias names for classes, which could then be used to perform\nadditional string searches, thereby expanding the initial priming pool size.\n                   ImageNet                                Stanford Cars                85            Flowers-102                             FGVC Aircraft\n               Neural Priming                            Neural Priming                            Neural Priming                           Neural Priming\n   op-1 Accuracy                             op-1 Accuracy                             op-1 Accuracy                            op-1 Accuracy\n               CLIP Zero-shot                 92         CLIP Zero-shot                 80         CLIP Zero-shot                40         CLIP Zero-shot\n    75                                        90                                                                                 35\n                                              88                                        75\n   T70                                       T86                                       T70                                      T30\n                                              84                                                                                 25\n       B/32           B/16          L/14         B/32           B/16           L/14        B/32           B/16           L/14       B/32           B/16       L/14\n                  Model (ViT)                               Model (ViT)                               Model (ViT)                              Model (ViT)\nFigure 5: Analyzing the effect of model capacity on Neural Priming. We find the relative error\nreduction stays consistent even as the scale of the model increases.\nWe also analyze the impact of the architecture on the accuracy improvement achieved by\nNeural Priming in the zero-shot setting (Figure 5). To examine this, we conduct experiments\nusing models of varying capacities, namely ViT B-32, B-16, and L-14. We observe that the gains\nremains consistent across the models. This finding suggests that even as we scale the architecture\u2019s\ncapacity, our method will continue to yield significant and consistent relative error reduction.\n5       Limitations\nNeural Priming has a few potential limitations. Firstly, it requires that the pre-train dataset contains\nimages similar to those in the downstream task. Though all of the datasets we benchmark have\nabundant relevant data, it is possible for more out-of-distribution datasets that LAION-2B simply does\nnot contain related or queryable images. Secondly, accurate class names are required for retrieval.\nMeaningful class names for some datasets can be difficult to obtain. For example, in the Flowers102\ndataset, some flower species are given by their latin names, which leads to poor retrieval. This\nissue generally affects open-vocabulary models which require accurate class names to initialize the\nzero-shot classifier. This limitation may be resolved by using language models to replace class\nnames with their more commonly known synonyms. Lastly, Neural Priming requires access to the\npre-training data set which is not always possible such as in the case of OpenAI variant of CLIP. In\nthis case a surrogate dataset would likely suffice, such as using LAION-2B.\n6       Discussion & Conclusion\nWe present Neural Priming, a method to improve the performance of open-vocabulary models by\nleveraging their own large-scale, diverse pre-training data with no additional data required. With\nNeural Priming, we demonstrate how to construct a high quality priming pool of examples from the\npre-training dataset relevant to a particular task and how to utilize this pool to improve our model.\nWe further show that our method is effective across a variety of downstream tasks and settings. In\nparticular, our method can be used in situations where only natural language descriptions of relevant\nclasses are given, when we have the ability to adapt at inference time, and when we are provided\nwith few labeled in-distribution examples. In all settings, our framework demonstrates a substantial\nimprovement in performance over existing interventions, and is in fact complementary with current\nprompt-tuning and robustness methods. Our method is also computationally cheap, not requiring any\nmodification of model backbone weights and only a fast text search on the pre-training corpus.\nThe effi     cacy of Neural Priming leads to some interesting questions for future work. For example, if\nthe model has seen this data before, why does it help to recall them? We hypothesize that this is due\nto the fact that the diversity of these datasets introduces competing objectives, which are diffi                                                            cult\nfor the model to optimize directly. For example, the same kind of image could appear with multiple\ncaptions and vice-versa, making it diffi                         cult to prompt a CLIP model trained on such data at inference\ntime for a particular task. A systematic study of this could elucidate important limitations of current\nlarge-scale training paradigms.\n                                                                                   9", "md": "Consequently, when we retrieve a larger number of images for the priming pool, they tend to contain more noise and mislabeled samples. Furthermore, for rare classes, the number of images obtained through exact string search is often less than 100. To address this, a potential extension could involve utilizing a language model to generate alias names for classes, which could then be used to perform additional string searches, thereby expanding the initial priming pool size.\n\n|ImageNet|Stanford Cars|Flowers-102|FGVC Aircraft|\n|---|---|---|---|\n|Neural Priming|Neural Priming|Neural Priming|Neural Priming|\n|op-1 Accuracy|op-1 Accuracy|op-1 Accuracy|op-1 Accuracy|\n|CLIP Zero-shot 75|CLIP Zero-shot 92|CLIP Zero-shot 80|CLIP Zero-shot 40|\n|T70|T86|T70|T30|\n|B/32 Model (ViT)|B/16 Model (ViT)|L/14 Model (ViT)|B/32 Model (ViT)|\n\nFigure 5: Analyzing the effect of model capacity on Neural Priming. We find the relative error reduction stays consistent even as the scale of the model increases.\n\nWe also analyze the impact of the architecture on the accuracy improvement achieved by Neural Priming in the zero-shot setting (Figure 5). To examine this, we conduct experiments using models of varying capacities, namely ViT B-32, B-16, and L-14. We observe that the gains remain consistent across the models. This finding suggests that even as we scale the architecture\u2019s capacity, our method will continue to yield significant and consistent relative error reduction.\n\n5 Limitations\n\nNeural Priming has a few potential limitations. Firstly, it requires that the pre-train dataset contains images similar to those in the downstream task. Though all of the datasets we benchmark have abundant relevant data, it is possible for more out-of-distribution datasets that LAION-2B simply does not contain related or queryable images. Secondly, accurate class names are required for retrieval. Meaningful class names for some datasets can be difficult to obtain. For example, in the Flowers102 dataset, some flower species are given by their latin names, which leads to poor retrieval. This issue generally affects open-vocabulary models which require accurate class names to initialize the zero-shot classifier. This limitation may be resolved by using language models to replace class names with their more commonly known synonyms. Lastly, Neural Priming requires access to the pre-training dataset which is not always possible such as in the case of OpenAI variant of CLIP. In this case a surrogate dataset would likely suffice, such as using LAION-2B.\n\n6 Discussion & Conclusion\n\nWe present Neural Priming, a method to improve the performance of open-vocabulary models by leveraging their own large-scale, diverse pre-training data with no additional data required. With Neural Priming, we demonstrate how to construct a high quality priming pool of examples from the pre-training dataset relevant to a particular task and how to utilize this pool to improve our model. We further show that our method is effective across a variety of downstream tasks and settings. In particular, our method can be used in situations where only natural language descriptions of relevant classes are given, when we have the ability to adapt at inference time, and when we are provided with few labeled in-distribution examples. In all settings, our framework demonstrates a substantial improvement in performance over existing interventions, and is in fact complementary with current prompt-tuning and robustness methods. Our method is also computationally cheap, not requiring any modification of model backbone weights and only a fast text search on the pre-training corpus.\n\nThe efficacy of Neural Priming leads to some interesting questions for future work. For example, if the model has seen this data before, why does it help to recall them? We hypothesize that this is due to the fact that the diversity of these datasets introduces competing objectives, which are difficult for the model to optimize directly. For example, the same kind of image could appear with multiple captions and vice-versa, making it difficult to prompt a CLIP model trained on such data at inference time for a particular task. A systematic study of this could elucidate important limitations of current large-scale training paradigms.", "images": [], "items": [{"type": "text", "value": "Consequently, when we retrieve a larger number of images for the priming pool, they tend to contain more noise and mislabeled samples. Furthermore, for rare classes, the number of images obtained through exact string search is often less than 100. To address this, a potential extension could involve utilizing a language model to generate alias names for classes, which could then be used to perform additional string searches, thereby expanding the initial priming pool size.", "md": "Consequently, when we retrieve a larger number of images for the priming pool, they tend to contain more noise and mislabeled samples. Furthermore, for rare classes, the number of images obtained through exact string search is often less than 100. To address this, a potential extension could involve utilizing a language model to generate alias names for classes, which could then be used to perform additional string searches, thereby expanding the initial priming pool size."}, {"type": "table", "rows": [["ImageNet", "Stanford Cars", "Flowers-102", "FGVC Aircraft"], ["Neural Priming", "Neural Priming", "Neural Priming", "Neural Priming"], ["op-1 Accuracy", "op-1 Accuracy", "op-1 Accuracy", "op-1 Accuracy"], ["CLIP Zero-shot 75", "CLIP Zero-shot 92", "CLIP Zero-shot 80", "CLIP Zero-shot 40"], ["T70", "T86", "T70", "T30"], ["B/32 Model (ViT)", "B/16 Model (ViT)", "L/14 Model (ViT)", "B/32 Model (ViT)"]], "md": "|ImageNet|Stanford Cars|Flowers-102|FGVC Aircraft|\n|---|---|---|---|\n|Neural Priming|Neural Priming|Neural Priming|Neural Priming|\n|op-1 Accuracy|op-1 Accuracy|op-1 Accuracy|op-1 Accuracy|\n|CLIP Zero-shot 75|CLIP Zero-shot 92|CLIP Zero-shot 80|CLIP Zero-shot 40|\n|T70|T86|T70|T30|\n|B/32 Model (ViT)|B/16 Model (ViT)|L/14 Model (ViT)|B/32 Model (ViT)|", "isPerfectTable": true, "csv": "\"ImageNet\",\"Stanford Cars\",\"Flowers-102\",\"FGVC Aircraft\"\n\"Neural Priming\",\"Neural Priming\",\"Neural Priming\",\"Neural Priming\"\n\"op-1 Accuracy\",\"op-1 Accuracy\",\"op-1 Accuracy\",\"op-1 Accuracy\"\n\"CLIP Zero-shot 75\",\"CLIP Zero-shot 92\",\"CLIP Zero-shot 80\",\"CLIP Zero-shot 40\"\n\"T70\",\"T86\",\"T70\",\"T30\"\n\"B/32 Model (ViT)\",\"B/16 Model (ViT)\",\"L/14 Model (ViT)\",\"B/32 Model (ViT)\""}, {"type": "text", "value": "Figure 5: Analyzing the effect of model capacity on Neural Priming. We find the relative error reduction stays consistent even as the scale of the model increases.\n\nWe also analyze the impact of the architecture on the accuracy improvement achieved by Neural Priming in the zero-shot setting (Figure 5). To examine this, we conduct experiments using models of varying capacities, namely ViT B-32, B-16, and L-14. We observe that the gains remain consistent across the models. This finding suggests that even as we scale the architecture\u2019s capacity, our method will continue to yield significant and consistent relative error reduction.\n\n5 Limitations\n\nNeural Priming has a few potential limitations. Firstly, it requires that the pre-train dataset contains images similar to those in the downstream task. Though all of the datasets we benchmark have abundant relevant data, it is possible for more out-of-distribution datasets that LAION-2B simply does not contain related or queryable images. Secondly, accurate class names are required for retrieval. Meaningful class names for some datasets can be difficult to obtain. For example, in the Flowers102 dataset, some flower species are given by their latin names, which leads to poor retrieval. This issue generally affects open-vocabulary models which require accurate class names to initialize the zero-shot classifier. This limitation may be resolved by using language models to replace class names with their more commonly known synonyms. Lastly, Neural Priming requires access to the pre-training dataset which is not always possible such as in the case of OpenAI variant of CLIP. In this case a surrogate dataset would likely suffice, such as using LAION-2B.\n\n6 Discussion & Conclusion\n\nWe present Neural Priming, a method to improve the performance of open-vocabulary models by leveraging their own large-scale, diverse pre-training data with no additional data required. With Neural Priming, we demonstrate how to construct a high quality priming pool of examples from the pre-training dataset relevant to a particular task and how to utilize this pool to improve our model. We further show that our method is effective across a variety of downstream tasks and settings. In particular, our method can be used in situations where only natural language descriptions of relevant classes are given, when we have the ability to adapt at inference time, and when we are provided with few labeled in-distribution examples. In all settings, our framework demonstrates a substantial improvement in performance over existing interventions, and is in fact complementary with current prompt-tuning and robustness methods. Our method is also computationally cheap, not requiring any modification of model backbone weights and only a fast text search on the pre-training corpus.\n\nThe efficacy of Neural Priming leads to some interesting questions for future work. For example, if the model has seen this data before, why does it help to recall them? We hypothesize that this is due to the fact that the diversity of these datasets introduces competing objectives, which are difficult for the model to optimize directly. For example, the same kind of image could appear with multiple captions and vice-versa, making it difficult to prompt a CLIP model trained on such data at inference time for a particular task. A systematic study of this could elucidate important limitations of current large-scale training paradigms.", "md": "Figure 5: Analyzing the effect of model capacity on Neural Priming. We find the relative error reduction stays consistent even as the scale of the model increases.\n\nWe also analyze the impact of the architecture on the accuracy improvement achieved by Neural Priming in the zero-shot setting (Figure 5). To examine this, we conduct experiments using models of varying capacities, namely ViT B-32, B-16, and L-14. We observe that the gains remain consistent across the models. This finding suggests that even as we scale the architecture\u2019s capacity, our method will continue to yield significant and consistent relative error reduction.\n\n5 Limitations\n\nNeural Priming has a few potential limitations. Firstly, it requires that the pre-train dataset contains images similar to those in the downstream task. Though all of the datasets we benchmark have abundant relevant data, it is possible for more out-of-distribution datasets that LAION-2B simply does not contain related or queryable images. Secondly, accurate class names are required for retrieval. Meaningful class names for some datasets can be difficult to obtain. For example, in the Flowers102 dataset, some flower species are given by their latin names, which leads to poor retrieval. This issue generally affects open-vocabulary models which require accurate class names to initialize the zero-shot classifier. This limitation may be resolved by using language models to replace class names with their more commonly known synonyms. Lastly, Neural Priming requires access to the pre-training dataset which is not always possible such as in the case of OpenAI variant of CLIP. In this case a surrogate dataset would likely suffice, such as using LAION-2B.\n\n6 Discussion & Conclusion\n\nWe present Neural Priming, a method to improve the performance of open-vocabulary models by leveraging their own large-scale, diverse pre-training data with no additional data required. With Neural Priming, we demonstrate how to construct a high quality priming pool of examples from the pre-training dataset relevant to a particular task and how to utilize this pool to improve our model. We further show that our method is effective across a variety of downstream tasks and settings. In particular, our method can be used in situations where only natural language descriptions of relevant classes are given, when we have the ability to adapt at inference time, and when we are provided with few labeled in-distribution examples. In all settings, our framework demonstrates a substantial improvement in performance over existing interventions, and is in fact complementary with current prompt-tuning and robustness methods. Our method is also computationally cheap, not requiring any modification of model backbone weights and only a fast text search on the pre-training corpus.\n\nThe efficacy of Neural Priming leads to some interesting questions for future work. For example, if the model has seen this data before, why does it help to recall them? We hypothesize that this is due to the fact that the diversity of these datasets introduces competing objectives, which are difficult for the model to optimize directly. For example, the same kind of image could appear with multiple captions and vice-versa, making it difficult to prompt a CLIP model trained on such data at inference time for a particular task. A systematic study of this could elucidate important limitations of current large-scale training paradigms."}]}, {"page": 10, "text": "Acknowledgments\nWe are grateful to Sarah Pratt, Mitchell Wortsman, and Romain Beaumont for helpful discussions\nand feedback. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166,\nDARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial\nIntelligence, Google, and Apple. Ludwig Schmidt and Alex Fang are in part supported by the NSF AI\nInstitute for Foundations of Machine Learning (IFML, CCF-2019844), Open Philanthropy, Google,\nand the Allen Institute for AI.\nReferences\n [1] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\n     Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\n     models. Advances in neural information processing systems, 32, 2019.\n [2] R.   Beaumont.         Clip  Retrieval,   2021.       URL     https://github.com/rom1504/\n     clip-retrieval.\n [3] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driess-\n     che, J.-B. Lespiau, B. Damoc, A. Clark, et al. Improving language models by retrieving from\n     trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR,\n     2022.\n [4] G. Brod, M. Werkle-Bergner, and Y. L. Shing. The influence of prior knowledge on memory: a\n     developmental cognitive neuroscience perspective. Frontiers in behavioral neuroscience, 7:139,\n     2013.\n [5] A. L. Brown and M. J. Kane. Preschool children can learn to transfer: Learning to learn and\n     learning from example. Cognitive psychology, 20(4):493\u2013523, 1988.\n [6] O. Chapelle, B. Sch\u00f6lkopf, and A. Zien. A discussion of semi-supervised learning and transduc-\n     tion. In Semi-supervised learning, pages 473\u2013478. MIT Press, 2006.\n [7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\n     image database. In 2009 IEEE conference on computer vision and pattern recognition, pages\n     248\u2013255. Ieee, 2009.\n [8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\n     M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\n     image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n [9] A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data\n     determines distributional robustness in contrastive language image pre-training (CLIP). In\n     K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv\u00e1ri, G. Niu, and S. Sabato, editors, International\n     Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\n     volume 162 of Proceedings of Machine Learning Research, pages 6216\u20136234. PMLR, 2022.\n     URL https://proceedings.mlr.press/v162/fang22a.html.\n[10] H. Fang, P. Xiong, L. Xu, and Y. Chen. Clip2video: Mastering video-text retrieval via image\n     clip. arXiv preprint arXiv:2106.11097, 2021.\n[11] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep\n     networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.\n[12] N. T. Franklin and M. J. Frank. Generalizing to generalize: Humans flexibly switch between\n     compositional and conjunctive structures during reinforcement learning. PLoS computational\n     biology, 16(4):e1007720, 2020.\n[13] A. Gammerman, V. Vovk, and V. Vapnik.              Learning by transduction.       arXiv preprint\n     arXiv:1301.7375, 2013.\n[14] Y. Gandelsman, Y. Sun, X. Chen, and A. Efros. Test-time training with masked autoencoders.\n     Advances in Neural Information Processing Systems, 35:29374\u201329385, 2022.\n                                                  10", "md": "# Acknowledgments and References\n\n## Acknowledgments\n\nWe are grateful to Sarah Pratt, Mitchell Wortsman, and Romain Beaumont for helpful discussions and feedback. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence, Google, and Apple. Ludwig Schmidt and Alex Fang are in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML, CCF-2019844), Open Philanthropy, Google, and the Allen Institute for AI.\n\n## References\n\n1. A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\nObjectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models.\nAdvances in neural information processing systems, 32, 2019.\n2. R. Beaumont. Clip Retrieval, 2021. URL https://github.com/rom1504/clip-retrieval.\n3. S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR, 2022.\n4. G. Brod, M. Werkle-Bergner, and Y. L. Shing. The influence of prior knowledge on memory: a developmental cognitive neuroscience perspective.\nFrontiers in behavioral neuroscience, 7:139, 2013.\n5. A. L. Brown and M. J. Kane. Preschool children can learn to transfer: Learning to learn and learning from example.\nCognitive psychology, 20(4):493\u2013523, 1988.\n6. O. Chapelle, B. Sch\u00f6lkopf, and A. Zien. A discussion of semi-supervised learning and transduction.\nIn Semi-supervised learning, pages 473\u2013478. MIT Press, 2006.\n7. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\n8. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n9. A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (CLIP).\nIn K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv\u00e1ri, G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 6216\u20136234. PMLR, 2022.\nURL https://proceedings.mlr.press/v162/fang22a.html.\n10. H. Fang, P. Xiong, L. Xu, and Y. Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.\n11. C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.\nIn International conference on machine learning, pages 1126\u20131135. PMLR, 2017.\n12. N. T. Franklin and M. J. Frank. Generalizing to generalize: Humans flexibly switch between compositional and conjunctive structures during reinforcement learning.\nPLoS computational biology, 16(4):e1007720, 2020.\n13. A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. arXiv preprint arXiv:1301.7375, 2013.\n14. Y. Gandelsman, Y. Sun, X. Chen, and A. Efros. Test-time training with masked autoencoders.\nAdvances in Neural Information Processing Systems, 35:29374\u201329385, 2022.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgments and References", "md": "# Acknowledgments and References"}, {"type": "heading", "lvl": 2, "value": "Acknowledgments", "md": "## Acknowledgments"}, {"type": "text", "value": "We are grateful to Sarah Pratt, Mitchell Wortsman, and Romain Beaumont for helpful discussions and feedback. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence, Google, and Apple. Ludwig Schmidt and Alex Fang are in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML, CCF-2019844), Open Philanthropy, Google, and the Allen Institute for AI.", "md": "We are grateful to Sarah Pratt, Mitchell Wortsman, and Romain Beaumont for helpful discussions and feedback. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence, Google, and Apple. Ludwig Schmidt and Alex Fang are in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML, CCF-2019844), Open Philanthropy, Google, and the Allen Institute for AI."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\nObjectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models.\nAdvances in neural information processing systems, 32, 2019.\n2. R. Beaumont. Clip Retrieval, 2021. URL https://github.com/rom1504/clip-retrieval.\n3. S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR, 2022.\n4. G. Brod, M. Werkle-Bergner, and Y. L. Shing. The influence of prior knowledge on memory: a developmental cognitive neuroscience perspective.\nFrontiers in behavioral neuroscience, 7:139, 2013.\n5. A. L. Brown and M. J. Kane. Preschool children can learn to transfer: Learning to learn and learning from example.\nCognitive psychology, 20(4):493\u2013523, 1988.\n6. O. Chapelle, B. Sch\u00f6lkopf, and A. Zien. A discussion of semi-supervised learning and transduction.\nIn Semi-supervised learning, pages 473\u2013478. MIT Press, 2006.\n7. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\n8. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n9. A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (CLIP).\nIn K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv\u00e1ri, G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 6216\u20136234. PMLR, 2022.\nURL https://proceedings.mlr.press/v162/fang22a.html.\n10. H. Fang, P. Xiong, L. Xu, and Y. Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.\n11. C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.\nIn International conference on machine learning, pages 1126\u20131135. PMLR, 2017.\n12. N. T. Franklin and M. J. Frank. Generalizing to generalize: Humans flexibly switch between compositional and conjunctive structures during reinforcement learning.\nPLoS computational biology, 16(4):e1007720, 2020.\n13. A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. arXiv preprint arXiv:1301.7375, 2013.\n14. Y. Gandelsman, Y. Sun, X. Chen, and A. Efros. Test-time training with masked autoencoders.\nAdvances in Neural Information Processing Systems, 35:29374\u201329385, 2022.", "md": "1. A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\nObjectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models.\nAdvances in neural information processing systems, 32, 2019.\n2. R. Beaumont. Clip Retrieval, 2021. URL https://github.com/rom1504/clip-retrieval.\n3. S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR, 2022.\n4. G. Brod, M. Werkle-Bergner, and Y. L. Shing. The influence of prior knowledge on memory: a developmental cognitive neuroscience perspective.\nFrontiers in behavioral neuroscience, 7:139, 2013.\n5. A. L. Brown and M. J. Kane. Preschool children can learn to transfer: Learning to learn and learning from example.\nCognitive psychology, 20(4):493\u2013523, 1988.\n6. O. Chapelle, B. Sch\u00f6lkopf, and A. Zien. A discussion of semi-supervised learning and transduction.\nIn Semi-supervised learning, pages 473\u2013478. MIT Press, 2006.\n7. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\n8. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n9. A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (CLIP).\nIn K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv\u00e1ri, G. Niu, and S. Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 6216\u20136234. PMLR, 2022.\nURL https://proceedings.mlr.press/v162/fang22a.html.\n10. H. Fang, P. Xiong, L. Xu, and Y. Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.\n11. C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.\nIn International conference on machine learning, pages 1126\u20131135. PMLR, 2017.\n12. N. T. Franklin and M. J. Frank. Generalizing to generalize: Humans flexibly switch between compositional and conjunctive structures during reinforcement learning.\nPLoS computational biology, 16(4):e1007720, 2020.\n13. A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. arXiv preprint arXiv:1301.7375, 2013.\n14. Y. Gandelsman, Y. Sun, X. Chen, and A. Efros. Test-time training with masked autoencoders.\nAdvances in Neural Information Processing Systems, 35:29374\u201329385, 2022."}]}, {"page": 11, "text": "[15] S. Goyal, A. Kumar, S. Garg, Z. Kolter, and A. Raghunathan. Finetune like you pretrain:\n      Improved finetuning of zero-shot vision models. arXiv preprint arXiv:2212.00638, 2022.\n[16] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model\n      pre-training. In International conference on machine learning, pages 3929\u20133938. PMLR, 2020.\n[17] K. He, R. Girshick, and P. Doll\u00e1r. Rethinking imagenet pre-training. In Proceedings of the\n      IEEE/CVF International Conference on Computer Vision, pages 4918\u20134927, 2019.\n[18] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli,\n      M. Guo, D. Song, J. Steinhardt, and J. Gilmer. The many faces of robustness: A critical analysis\n      of out-of-distribution generalization. In ICCV, 2021.\n[19] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In\n      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n      15262\u201315271, 2021.\n[20] L. Hermer-Vazquez, A. Moffet, and P. Munkholm. Language, space, and the development\n      of cognitive flexibility in humans: The case of two spatial memory tasks. Cognition, 79(3):\n      263\u2013299, 2001.\n[21] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar,\n      H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL\n      https://doi.org/10.5281/zenodo.5143773.\n[22] M. A. Jamal and G.-J. Qi. Task agnostic meta-learning for few-shot learning. In Proceedings of\n      the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11719\u201311727,\n      2019.\n[23] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig.\n      Scaling up visual and vision-language representation learning with noisy text supervision. arXiv\n      preprint arXiv:2102.05918, 2021.\n[24] J. Johnson, M. Douze, and H. J\u00e9gou.        Billion-scale similarity search with GPUs.      IEEE\n      Transactions on Big Data, 7(3):535\u2013547, 2019.\n[25] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through\n      memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.\n[26] D. E. Knuth. Retrieval on secondary keys. The art of computer programming: Sorting and\n      Searching, 3:550\u2013567, 1997.\n[27] J. Krause, M. Stark, J. Deng, and L. Fei-Fei.       3d object representations for fine-grained\n      categorization.  In Proceedings of the IEEE international conference on computer vision\n      workshops, pages 554\u2013561, 2013.\n[28] A. Kumar, A. Raghunathan, R. M. Jones, T. Ma, and P. Liang. Fine-tuning can distort pretrained\n      features and underperform out-of-distribution. In The Tenth International Conference on\n      Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\n      URL https://openreview.net/forum?id=UYneFzXSJWh.\n[29] Y. Lee, A. S. Chen, F. Tajwar, A. Kumar, H. Yao, P. Liang, and C. Finn. Surgical fine-tuning\n      improves adaptation to distribution shifts. CoRR, abs/2210.11466, 2022. doi: 10.48550/arXiv.\n      2210.11466. URL https://doi.org/10.48550/arXiv.2210.11466.\n[30] H. Liu, K. Son, J. Yang, C. Liu, J. Gao, Y. J. Lee, and C. Li. Learning customized visual models\n      with retrieval-augmented knowledge. arXiv preprint arXiv:2301.07094, 2023.\n[31] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification\n      of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n[32] C. Manning, P. Raghavan, and H. Sch\u00fctze. Vector space classifi   cation. Introduction to Informa-\n      tion Retrieval, pages 289\u2013317, 2008.\n                                                  11", "md": "1. S. Goyal, A. Kumar, S. Garg, Z. Kolter, and A. Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. arXiv preprint arXiv:2212.00638, 2022.\n2. K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929\u20133938. PMLR, 2020.\n3. K. He, R. Girshick, and P. Doll\u00e1r. Rethinking imagenet pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4918\u20134927, 2019.\n4. D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, D. Song, J. Steinhardt, and J. Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021.\n5. D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271, 2021.\n6. L. Hermer-Vazquez, A. Moffet, and P. Munkholm. Language, space, and the development of cognitive flexibility in humans: The case of two spatial memory tasks. Cognition, 79(3): 263\u2013299, 2001.\n7. G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773.\n8. M. A. Jamal and G.-J. Qi. Task agnostic meta-learning for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11719\u201311727, 2019.\n9. C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021.\n10. J. Johnson, M. Douze, and H. J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3): 535\u2013547, 2019.\n11. U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.\n12. D. E. Knuth. Retrieval on secondary keys. The art of computer programming: Sorting and Searching, 3: 550\u2013567, 1997.\n13. J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.\n14. A. Kumar, A. Raghunathan, R. M. Jones, T. Ma, and P. Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=UYneFzXSJWh.\n15. Y. Lee, A. S. Chen, F. Tajwar, A. Kumar, H. Yao, P. Liang, and C. Finn. Surgical fine-tuning improves adaptation to distribution shifts. CoRR, abs/2210.11466, 2022. doi: 10.48550/arXiv.2210.11466. URL https://doi.org/10.48550/arXiv.2210.11466.\n16. H. Liu, K. Son, J. Yang, C. Liu, J. Gao, Y. J. Lee, and C. Li. Learning customized visual models with retrieval-augmented knowledge. arXiv preprint arXiv:2301.07094, 2023.\n17. S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n18. C. Manning, P. Raghavan, and H. Sch\u00fctze. Vector space classification. Introduction to Information Retrieval, pages 289\u2013317, 2008.", "images": [], "items": [{"type": "text", "value": "1. S. Goyal, A. Kumar, S. Garg, Z. Kolter, and A. Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. arXiv preprint arXiv:2212.00638, 2022.\n2. K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929\u20133938. PMLR, 2020.\n3. K. He, R. Girshick, and P. Doll\u00e1r. Rethinking imagenet pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4918\u20134927, 2019.\n4. D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, D. Song, J. Steinhardt, and J. Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021.\n5. D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271, 2021.\n6. L. Hermer-Vazquez, A. Moffet, and P. Munkholm. Language, space, and the development of cognitive flexibility in humans: The case of two spatial memory tasks. Cognition, 79(3): 263\u2013299, 2001.\n7. G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773.\n8. M. A. Jamal and G.-J. Qi. Task agnostic meta-learning for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11719\u201311727, 2019.\n9. C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021.\n10. J. Johnson, M. Douze, and H. J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3): 535\u2013547, 2019.\n11. U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.\n12. D. E. Knuth. Retrieval on secondary keys. The art of computer programming: Sorting and Searching, 3: 550\u2013567, 1997.\n13. J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.\n14. A. Kumar, A. Raghunathan, R. M. Jones, T. Ma, and P. Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=UYneFzXSJWh.\n15. Y. Lee, A. S. Chen, F. Tajwar, A. Kumar, H. Yao, P. Liang, and C. Finn. Surgical fine-tuning improves adaptation to distribution shifts. CoRR, abs/2210.11466, 2022. doi: 10.48550/arXiv.2210.11466. URL https://doi.org/10.48550/arXiv.2210.11466.\n16. H. Liu, K. Son, J. Yang, C. Liu, J. Gao, Y. J. Lee, and C. Li. Learning customized visual models with retrieval-augmented knowledge. arXiv preprint arXiv:2301.07094, 2023.\n17. S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n18. C. Manning, P. Raghavan, and H. Sch\u00fctze. Vector space classification. Introduction to Information Retrieval, pages 289\u2013317, 2008.", "md": "1. S. Goyal, A. Kumar, S. Garg, Z. Kolter, and A. Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. arXiv preprint arXiv:2212.00638, 2022.\n2. K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929\u20133938. PMLR, 2020.\n3. K. He, R. Girshick, and P. Doll\u00e1r. Rethinking imagenet pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4918\u20134927, 2019.\n4. D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, D. Song, J. Steinhardt, and J. Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021.\n5. D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271, 2021.\n6. L. Hermer-Vazquez, A. Moffet, and P. Munkholm. Language, space, and the development of cognitive flexibility in humans: The case of two spatial memory tasks. Cognition, 79(3): 263\u2013299, 2001.\n7. G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773.\n8. M. A. Jamal and G.-J. Qi. Task agnostic meta-learning for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11719\u201311727, 2019.\n9. C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021.\n10. J. Johnson, M. Douze, and H. J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3): 535\u2013547, 2019.\n11. U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.\n12. D. E. Knuth. Retrieval on secondary keys. The art of computer programming: Sorting and Searching, 3: 550\u2013567, 1997.\n13. J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.\n14. A. Kumar, A. Raghunathan, R. M. Jones, T. Ma, and P. Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=UYneFzXSJWh.\n15. Y. Lee, A. S. Chen, F. Tajwar, A. Kumar, H. Yao, P. Liang, and C. Finn. Surgical fine-tuning improves adaptation to distribution shifts. CoRR, abs/2210.11466, 2022. doi: 10.48550/arXiv.2210.11466. URL https://doi.org/10.48550/arXiv.2210.11466.\n16. H. Liu, K. Son, J. Yang, C. Liu, J. Gao, Y. J. Lee, and C. Li. Learning customized visual models with retrieval-augmented knowledge. arXiv preprint arXiv:2301.07094, 2023.\n17. S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n18. C. Manning, P. Raghavan, and H. Sch\u00fctze. Vector space classification. Introduction to Information Retrieval, pages 289\u2013317, 2008."}]}, {"page": 12, "text": "[33] S. Menon and C. Vondrick. Visual classification via description from large language models.\n      arXiv preprint arXiv:2210.07183, 2022.\n[34] D. E. Meyer and R. W. Schvaneveldt. Facilitation in recognizing pairs of words: evidence of a\n      dependence between retrieval operations. Journal of experimental psychology, 90(2):227, 1971.\n[35] J. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon,\n      and L. Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution\n      and in-distribution generalization. In M. Meila and T. Zhang, editors, Proceedings of the 38th\n      International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,\n     volume 139 of Proceedings of Machine Learning Research, pages 7721\u20137735. PMLR, 2021.\n      URL http://proceedings.mlr.press/v139/miller21b.html.\n[36] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of\n      classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,\n      pages 722\u2013729. IEEE, 2008.\n[37] B. Oreshkin, P. Rodr\u00edguez L\u00f3pez, and A. Lacoste. Tadam: Task dependent adaptive metric for\n      improved few-shot learning. Advances in neural information processing systems, 31, 2018.\n[38] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In 2012 IEEE\n      conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.\n[39] S. Pratt, R. Liu, and A. Farhadi. What does a platypus look like? generating customized prompts\n      for zero-shot image classification. arXiv preprint arXiv:2209.03320, 2022.\n[40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n      P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\n      In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n[41] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\n      imagenet? In International conference on machine learning, pages 5389\u20135400. PMLR, 2019.\n[42] A. Rege, A. Kusupati, A. Fan, Q. Cao, S. Kakade, P. Jain, A. Farhadi, et al. Adanns: A\n      framework for adaptive semantic search. arXiv preprint arXiv:2305.19435, 2023.\n[43] H. L. Roediger and K. B. McDermott. Creating false memories: Remembering words not\n      presented in lists. Journal of experimental psychology: Learning, Memory, and Cognition, 21\n     (4):803, 1995.\n[44] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\n     A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next\n      generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n[45] M. Shu, W. Nie, D.-A. Huang, Z. Yu, T. Goldstein, A. Anandkumar, and C. Xiao. Test-\n      time prompt tuning for zero-shot generalization in vision-language models. arXiv preprint\n      arXiv:2209.07511, 2022.\n[46] J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos.\n      In Computer Vision, IEEE International Conference on, volume 3, pages 1470\u20131470. IEEE\n      Computer Society, 2003.\n[47] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. Advances in\n      neural information processing systems, 30, 2017.\n[48] Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt. Test-time training with self-\n      supervision for generalization under distribution shifts. In International conference on machine\n      learning, pages 9229\u20139248. PMLR, 2020.\n[49] R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to\n      natural distribution shifts in image classification. In H. Larochelle, M. Ranzato, R. Hadsell,\n      M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual\n     Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n     12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n      d8330f857a17c53d217014ee776bfd50-Abstract.html.\n                                                   12", "md": "1. S. Menon and C. Vondrick. Visual classification via description from large language models.\n*arXiv preprint arXiv:2210.07183*, 2022.\n2. D. E. Meyer and R. W. Schvaneveldt. Facilitation in recognizing pairs of words: evidence of a\ndependence between retrieval operations. *Journal of experimental psychology*, 90(2):227, 1971.\n3. J. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon,\nand L. Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution\nand in-distribution generalization. In M. Meila and T. Zhang, editors, Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,\nvolume 139 of *Proceedings of Machine Learning Research*, pages 7721\u20137735. PMLR, 2021.\nURL http://proceedings.mlr.press/v139/miller21b.html.\n4. M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of\nclasses. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,\npages 722\u2013729. IEEE, 2008.\n5. B. Oreshkin, P. Rodr\u00edguez L\u00f3pez, and A. Lacoste. Tadam: Task dependent adaptive metric for\nimproved few-shot learning. *Advances in neural information processing systems*, 31, 2018.\n6. O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In 2012 IEEE\nconference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.\n7. S. Pratt, R. Liu, and A. Farhadi. What does a platypus look like? generating customized prompts\nfor zero-shot image classification. *arXiv preprint arXiv:2209.03320*, 2022.\n8. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\nIn *International conference on machine learning*, pages 8748\u20138763. PMLR, 2021.\n9. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\nimagenet? In *International conference on machine learning*, pages 5389\u20135400. PMLR, 2019.\n10. A. Rege, A. Kusupati, A. Fan, Q. Cao, S. Kakade, P. Jain, A. Farhadi, et al. Adanns: A\nframework for adaptive semantic search. *arXiv preprint arXiv:2305.19435*, 2023.\n11. H. L. Roediger and K. B. McDermott. Creating false memories: Remembering words not\npresented in lists. *Journal of experimental psychology: Learning, Memory, and Cognition*, 21\n(4):803, 1995.\n12. C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\nA. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next\ngeneration image-text models. *arXiv preprint arXiv:2210.08402*, 2022.\n13. M. Shu, W. Nie, D.-A. Huang, Z. Yu, T. Goldstein, A. Anandkumar, and C. Xiao. Test-\ntime prompt tuning for zero-shot generalization in vision-language models. arXiv preprint\narXiv:2209.07511, 2022.\n14. J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos.\nIn *Computer Vision, IEEE International Conference on*, volume 3, pages 1470\u20131470. IEEE\nComputer Society, 2003.\n15. J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. Advances in\nneural information processing systems, 30, 2017.\n16. Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt. Test-time training with self-\nsupervision for generalization under distribution shifts. In International conference on machine\nlearning, pages 9229\u20139248. PMLR, 2020.\n17. R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to\nnatural distribution shifts in image classification. In H. Larochelle, M. Ranzato, R. Hadsell,\nM. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nd8330f857a17c53d217014ee776bfd50-Abstract.html.", "images": [], "items": [{"type": "text", "value": "1. S. Menon and C. Vondrick. Visual classification via description from large language models.\n*arXiv preprint arXiv:2210.07183*, 2022.\n2. D. E. Meyer and R. W. Schvaneveldt. Facilitation in recognizing pairs of words: evidence of a\ndependence between retrieval operations. *Journal of experimental psychology*, 90(2):227, 1971.\n3. J. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon,\nand L. Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution\nand in-distribution generalization. In M. Meila and T. Zhang, editors, Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,\nvolume 139 of *Proceedings of Machine Learning Research*, pages 7721\u20137735. PMLR, 2021.\nURL http://proceedings.mlr.press/v139/miller21b.html.\n4. M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of\nclasses. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,\npages 722\u2013729. IEEE, 2008.\n5. B. Oreshkin, P. Rodr\u00edguez L\u00f3pez, and A. Lacoste. Tadam: Task dependent adaptive metric for\nimproved few-shot learning. *Advances in neural information processing systems*, 31, 2018.\n6. O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In 2012 IEEE\nconference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.\n7. S. Pratt, R. Liu, and A. Farhadi. What does a platypus look like? generating customized prompts\nfor zero-shot image classification. *arXiv preprint arXiv:2209.03320*, 2022.\n8. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\nIn *International conference on machine learning*, pages 8748\u20138763. PMLR, 2021.\n9. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\nimagenet? In *International conference on machine learning*, pages 5389\u20135400. PMLR, 2019.\n10. A. Rege, A. Kusupati, A. Fan, Q. Cao, S. Kakade, P. Jain, A. Farhadi, et al. Adanns: A\nframework for adaptive semantic search. *arXiv preprint arXiv:2305.19435*, 2023.\n11. H. L. Roediger and K. B. McDermott. Creating false memories: Remembering words not\npresented in lists. *Journal of experimental psychology: Learning, Memory, and Cognition*, 21\n(4):803, 1995.\n12. C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\nA. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next\ngeneration image-text models. *arXiv preprint arXiv:2210.08402*, 2022.\n13. M. Shu, W. Nie, D.-A. Huang, Z. Yu, T. Goldstein, A. Anandkumar, and C. Xiao. Test-\ntime prompt tuning for zero-shot generalization in vision-language models. arXiv preprint\narXiv:2209.07511, 2022.\n14. J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos.\nIn *Computer Vision, IEEE International Conference on*, volume 3, pages 1470\u20131470. IEEE\nComputer Society, 2003.\n15. J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. Advances in\nneural information processing systems, 30, 2017.\n16. Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt. Test-time training with self-\nsupervision for generalization under distribution shifts. In International conference on machine\nlearning, pages 9229\u20139248. PMLR, 2020.\n17. R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to\nnatural distribution shifts in image classification. In H. Larochelle, M. Ranzato, R. Hadsell,\nM. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nd8330f857a17c53d217014ee776bfd50-Abstract.html.", "md": "1. S. Menon and C. Vondrick. Visual classification via description from large language models.\n*arXiv preprint arXiv:2210.07183*, 2022.\n2. D. E. Meyer and R. W. Schvaneveldt. Facilitation in recognizing pairs of words: evidence of a\ndependence between retrieval operations. *Journal of experimental psychology*, 90(2):227, 1971.\n3. J. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon,\nand L. Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution\nand in-distribution generalization. In M. Meila and T. Zhang, editors, Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,\nvolume 139 of *Proceedings of Machine Learning Research*, pages 7721\u20137735. PMLR, 2021.\nURL http://proceedings.mlr.press/v139/miller21b.html.\n4. M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of\nclasses. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,\npages 722\u2013729. IEEE, 2008.\n5. B. Oreshkin, P. Rodr\u00edguez L\u00f3pez, and A. Lacoste. Tadam: Task dependent adaptive metric for\nimproved few-shot learning. *Advances in neural information processing systems*, 31, 2018.\n6. O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In 2012 IEEE\nconference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.\n7. S. Pratt, R. Liu, and A. Farhadi. What does a platypus look like? generating customized prompts\nfor zero-shot image classification. *arXiv preprint arXiv:2209.03320*, 2022.\n8. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\nIn *International conference on machine learning*, pages 8748\u20138763. PMLR, 2021.\n9. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\nimagenet? In *International conference on machine learning*, pages 5389\u20135400. PMLR, 2019.\n10. A. Rege, A. Kusupati, A. Fan, Q. Cao, S. Kakade, P. Jain, A. Farhadi, et al. Adanns: A\nframework for adaptive semantic search. *arXiv preprint arXiv:2305.19435*, 2023.\n11. H. L. Roediger and K. B. McDermott. Creating false memories: Remembering words not\npresented in lists. *Journal of experimental psychology: Learning, Memory, and Cognition*, 21\n(4):803, 1995.\n12. C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\nA. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next\ngeneration image-text models. *arXiv preprint arXiv:2210.08402*, 2022.\n13. M. Shu, W. Nie, D.-A. Huang, Z. Yu, T. Goldstein, A. Anandkumar, and C. Xiao. Test-\ntime prompt tuning for zero-shot generalization in vision-language models. arXiv preprint\narXiv:2209.07511, 2022.\n14. J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos.\nIn *Computer Vision, IEEE International Conference on*, volume 3, pages 1470\u20131470. IEEE\nComputer Society, 2003.\n15. J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. Advances in\nneural information processing systems, 30, 2017.\n16. Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt. Test-time training with self-\nsupervision for generalization under distribution shifts. In International conference on machine\nlearning, pages 9229\u20139248. PMLR, 2020.\n17. R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to\nnatural distribution shifts in image classification. In H. Larochelle, M. Ranzato, R. Hadsell,\nM. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nd8330f857a17c53d217014ee776bfd50-Abstract.html."}]}, {"page": 13, "text": "[50] V. Udandarao, A. Gupta, and S. Albanie. Sus-x: Training-free name-only transfer of vision-\n     language models. arXiv preprint arXiv:2211.16198, 2022.\n[51] H. Wang, S. Ge, Z. C. Lipton, and E. P. Xing.          Learning robust global representations\n     by penalizing local predictive power.       In H. M. Wallach, H. Larochelle, A. Beygelz-\n     imer, F. d\u2019Alch\u00e9-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Infor-\n     mation Processing Systems 32:       Annual Conference on Neural Information Processing\n     Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages\n     10506\u201310518, 2019.         URL https://proceedings.neurips.cc/paper/2019/hash/\n     3eefceb8087e964f89c2d59e8a249915-Abstract.html.\n[52] M. Wortsman, G. Ilharco, J. W. Kim, M. Li, S. Kornblith, R. Roelofs, R. G. Lopes, H. Hajishirzi,\n     A. Farhadi, H. Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the\n     IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959\u20137971, 2022.\n[53] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene\n     recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision\n     and pattern recognition, pages 3485\u20133492. IEEE, 2010.\n[54] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li. Tip-adapter: Training-\n     free adaption of clip for few-shot classification. In Computer Vision\u2013ECCV 2022: 17th European\n     Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXV, pages 493\u2013510.\n     Springer, 2022.\n[55] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models.\n     International Journal of Computer Vision, 130(9):2337\u20132348, 2022.\n                                                 13", "md": "[50] V. Udandarao, A. Gupta, and S. Albanie. Sus-x: Training-free name-only transfer of vision-language models. arXiv preprint arXiv:2211.16198, 2022.\n\n[51] H. Wang, S. Ge, Z. C. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 10506\u201310518, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html.\n\n[52] M. Wortsman, G. Ilharco, J. W. Kim, M. Li, S. Kornblith, R. Roelofs, R. G. Lopes, H. Hajishirzi, A. Farhadi, H. Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959\u20137971, 2022.\n\n[53] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.\n\n[54] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXV, pages 493\u2013510. Springer, 2022.\n\n[55] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.\n\n13", "images": [], "items": [{"type": "text", "value": "[50] V. Udandarao, A. Gupta, and S. Albanie. Sus-x: Training-free name-only transfer of vision-language models. arXiv preprint arXiv:2211.16198, 2022.\n\n[51] H. Wang, S. Ge, Z. C. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 10506\u201310518, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html.\n\n[52] M. Wortsman, G. Ilharco, J. W. Kim, M. Li, S. Kornblith, R. Roelofs, R. G. Lopes, H. Hajishirzi, A. Farhadi, H. Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959\u20137971, 2022.\n\n[53] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.\n\n[54] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXV, pages 493\u2013510. Springer, 2022.\n\n[55] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.\n\n13", "md": "[50] V. Udandarao, A. Gupta, and S. Albanie. Sus-x: Training-free name-only transfer of vision-language models. arXiv preprint arXiv:2211.16198, 2022.\n\n[51] H. Wang, S. Ge, Z. C. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 10506\u201310518, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html.\n\n[52] M. Wortsman, G. Ilharco, J. W. Kim, M. Li, S. Kornblith, R. Roelofs, R. G. Lopes, H. Hajishirzi, A. Farhadi, H. Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959\u20137971, 2022.\n\n[53] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.\n\n[54] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXV, pages 493\u2013510. Springer, 2022.\n\n[55] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.\n\n13"}]}, {"page": 14, "text": "A     Implementation Details\nWe perform last layer retraining for Neural Priming. We experimented with fine-tuning all layers, but\nfound minimal performance benefits (Table 4). It has been noted in few-shot literature [47, 37] that\nretraining only the last layer is sufficient when few training examples are present and we confirm\nthis empirically. When retraining the last layer, we ensemble the text-classifier. We use a mixing\nco-efficient to ensemble the text-classifier with the image centroids. We found that as the number of\nsamples in the image centroid increases, the ensemble should more heavily favor the image centroid.\nFormally we use \u03b1 = e\u2212|P |2/\u03c3 as the mixing-coefficient, where |P| is the number of image examples\nwith \u03c3 equal to 100.\nThe OpenCLIP models we use can be found at https://github.com/mlfoundations/open_clip.\nWe use the following specific models:            (ViT-B-32, laion2b_s34b_b79k), (ViT-B-16,\nlaion2b_s34b_b88k), (ViT-L-14, laion2b_s32b_b82k).\nFor the zero-shot CLIP baseline presented in Table 2, we use the baseline prompts from [33]. When\nensembling with the image centroids from NCM, we use the text-classifier initialized with these\nprompts unless otherwise stated.\nTo perform fast substring search, we set up a Full Text Search (FTS) database over the metadata\nshards of LAION-2B using SQLite. Each shard is a SQLite database composed of 18.1 million\nrows consisting of information about the URL of the image location, constructed from the original\ndistributed shards from LAION-5B [44]. The nature of FTS and storage reading attributes means that\nit is much faster to search for rare substrings. More details on this are provided in Appendix G.\nTable 3: Comparing Neural Priming to other classification approaches. NCM uses only the image\nfeatures. WISE-FT fine-tunes a linear classifier and ensembles it with the text classifier.\n                             Shots    Neural Priming     NCM       Wise-FT\n                               0           71.38          68.3      67.75\n                               1           71.56         68.89      68.14\n                               2           71.81         68.85      68.25\n                               5           72.03         69.12      68.78\n                              10           72.36         70.12      69.02\nB    LAION-2B Analysis\nWe analyze the prevalence of each domain in the LAION pretrain set (Figure 6). We find that\nfor datasets where the domain images and captions occur infrequently such as Flowers102 and\nStanfordCars, the performance gain from Neural Priming is significantly larger. We measure domain\nprevalence in LAION-2B by substring searching for captions which contain the class names and\ncounting the total number of images. Image content is another factor for measuring how close\nthe pretraining set is to the downstream dataset in feature space, however this is computationally\nexpensive for a dataset of 2 billion. Also this would depend on the model used to extract features. We\nchoose to look at the caption statistics as a proxy because it is computationally feasible with a dataset\nat the size of LAION-2B and model agnostic.\nC     Other Architectures and Datasets\nWe present the full results for ViT B/16, B/32 and ViT L/14 on LAION-2B for ImageNet, Flowers102,\nStanfordCars, FGVC-Aircraft, and OxfordPets. We find similar results across architectures. Results\ncan be found in Table 6. We also reproduce experiments for the LAION-400m dataset with the B/16\narchitecture.\nD     Dataset Statistics\nFor ImageNet we performed URL deduplication using the LAION-2B URL list provided on Hug-\ngingFace. We found no ImageNet images in the pretrain data. We also randomly sampled images\n                                                   14", "md": "# Document\n\n## Implementation Details\n\nWe perform last layer retraining for Neural Priming. We experimented with fine-tuning all layers, but found minimal performance benefits (Table 4). It has been noted in few-shot literature [47, 37] that retraining only the last layer is sufficient when few training examples are present and we confirm this empirically. When retraining the last layer, we ensemble the text-classifier. We use a mixing co-efficient to ensemble the text-classifier with the image centroids. We found that as the number of samples in the image centroid increases, the ensemble should more heavily favor the image centroid. Formally we use $$\\alpha = e^{-|P|^2/\\sigma}$$ as the mixing-coefficient, where $|P|$ is the number of image examples with $\\sigma$ equal to 100.\n\nThe OpenCLIP models we use can be found at https://github.com/mlfoundations/open_clip.\n\nWe use the following specific models: (ViT-B-32, laion2b_s34b_b79k), (ViT-B-16, laion2b_s34b_b88k), (ViT-L-14, laion2b_s32b_b82k).\n\nFor the zero-shot CLIP baseline presented in Table 2, we use the baseline prompts from [33]. When ensembling with the image centroids from NCM, we use the text-classifier initialized with these prompts unless otherwise stated.\n\nTo perform fast substring search, we set up a Full Text Search (FTS) database over the metadata shards of LAION-2B using SQLite. Each shard is a SQLite database composed of 18.1 million rows consisting of information about the URL of the image location, constructed from the original distributed shards from LAION-5B [44]. The nature of FTS and storage reading attributes means that it is much faster to search for rare substrings. More details on this are provided in Appendix G.\n\n**Table 3: Comparing Neural Priming to other classification approaches.**\n|Shots|Neural Priming|NCM|Wise-FT|\n|---|---|---|---|\n|0|71.38|68.3|67.75|\n|1|71.56|68.89|68.14|\n|2|71.81|68.85|68.25|\n|5|72.03|69.12|68.78|\n|10|72.36|70.12|69.02|\n\n## LAION-2B Analysis\n\nWe analyze the prevalence of each domain in the LAION pretrain set (Figure 6). We find that for datasets where the domain images and captions occur infrequently such as Flowers102 and StanfordCars, the performance gain from Neural Priming is significantly larger. We measure domain prevalence in LAION-2B by substring searching for captions which contain the class names and counting the total number of images. Image content is another factor for measuring how close the pretraining set is to the downstream dataset in feature space, however this is computationally expensive for a dataset of 2 billion. Also this would depend on the model used to extract features. We choose to look at the caption statistics as a proxy because it is computationally feasible with a dataset at the size of LAION-2B and model agnostic.\n\n## Other Architectures and Datasets\n\nWe present the full results for ViT B/16, B/32 and ViT L/14 on LAION-2B for ImageNet, Flowers102, StanfordCars, FGVC-Aircraft, and OxfordPets. We find similar results across architectures. Results can be found in Table 6. We also reproduce experiments for the LAION-400m dataset with the B/16 architecture.\n\n## Dataset Statistics\n\nFor ImageNet we performed URL deduplication using the LAION-2B URL list provided on HuggingFace. We found no ImageNet images in the pretrain data. We also randomly sampled images.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Implementation Details", "md": "## Implementation Details"}, {"type": "text", "value": "We perform last layer retraining for Neural Priming. We experimented with fine-tuning all layers, but found minimal performance benefits (Table 4). It has been noted in few-shot literature [47, 37] that retraining only the last layer is sufficient when few training examples are present and we confirm this empirically. When retraining the last layer, we ensemble the text-classifier. We use a mixing co-efficient to ensemble the text-classifier with the image centroids. We found that as the number of samples in the image centroid increases, the ensemble should more heavily favor the image centroid. Formally we use $$\\alpha = e^{-|P|^2/\\sigma}$$ as the mixing-coefficient, where $|P|$ is the number of image examples with $\\sigma$ equal to 100.\n\nThe OpenCLIP models we use can be found at https://github.com/mlfoundations/open_clip.\n\nWe use the following specific models: (ViT-B-32, laion2b_s34b_b79k), (ViT-B-16, laion2b_s34b_b88k), (ViT-L-14, laion2b_s32b_b82k).\n\nFor the zero-shot CLIP baseline presented in Table 2, we use the baseline prompts from [33]. When ensembling with the image centroids from NCM, we use the text-classifier initialized with these prompts unless otherwise stated.\n\nTo perform fast substring search, we set up a Full Text Search (FTS) database over the metadata shards of LAION-2B using SQLite. Each shard is a SQLite database composed of 18.1 million rows consisting of information about the URL of the image location, constructed from the original distributed shards from LAION-5B [44]. The nature of FTS and storage reading attributes means that it is much faster to search for rare substrings. More details on this are provided in Appendix G.\n\n**Table 3: Comparing Neural Priming to other classification approaches.**", "md": "We perform last layer retraining for Neural Priming. We experimented with fine-tuning all layers, but found minimal performance benefits (Table 4). It has been noted in few-shot literature [47, 37] that retraining only the last layer is sufficient when few training examples are present and we confirm this empirically. When retraining the last layer, we ensemble the text-classifier. We use a mixing co-efficient to ensemble the text-classifier with the image centroids. We found that as the number of samples in the image centroid increases, the ensemble should more heavily favor the image centroid. Formally we use $$\\alpha = e^{-|P|^2/\\sigma}$$ as the mixing-coefficient, where $|P|$ is the number of image examples with $\\sigma$ equal to 100.\n\nThe OpenCLIP models we use can be found at https://github.com/mlfoundations/open_clip.\n\nWe use the following specific models: (ViT-B-32, laion2b_s34b_b79k), (ViT-B-16, laion2b_s34b_b88k), (ViT-L-14, laion2b_s32b_b82k).\n\nFor the zero-shot CLIP baseline presented in Table 2, we use the baseline prompts from [33]. When ensembling with the image centroids from NCM, we use the text-classifier initialized with these prompts unless otherwise stated.\n\nTo perform fast substring search, we set up a Full Text Search (FTS) database over the metadata shards of LAION-2B using SQLite. Each shard is a SQLite database composed of 18.1 million rows consisting of information about the URL of the image location, constructed from the original distributed shards from LAION-5B [44]. The nature of FTS and storage reading attributes means that it is much faster to search for rare substrings. More details on this are provided in Appendix G.\n\n**Table 3: Comparing Neural Priming to other classification approaches.**"}, {"type": "table", "rows": [["Shots", "Neural Priming", "NCM", "Wise-FT"], ["0", "71.38", "68.3", "67.75"], ["1", "71.56", "68.89", "68.14"], ["2", "71.81", "68.85", "68.25"], ["5", "72.03", "69.12", "68.78"], ["10", "72.36", "70.12", "69.02"]], "md": "|Shots|Neural Priming|NCM|Wise-FT|\n|---|---|---|---|\n|0|71.38|68.3|67.75|\n|1|71.56|68.89|68.14|\n|2|71.81|68.85|68.25|\n|5|72.03|69.12|68.78|\n|10|72.36|70.12|69.02|", "isPerfectTable": true, "csv": "\"Shots\",\"Neural Priming\",\"NCM\",\"Wise-FT\"\n\"0\",\"71.38\",\"68.3\",\"67.75\"\n\"1\",\"71.56\",\"68.89\",\"68.14\"\n\"2\",\"71.81\",\"68.85\",\"68.25\"\n\"5\",\"72.03\",\"69.12\",\"68.78\"\n\"10\",\"72.36\",\"70.12\",\"69.02\""}, {"type": "heading", "lvl": 2, "value": "LAION-2B Analysis", "md": "## LAION-2B Analysis"}, {"type": "text", "value": "We analyze the prevalence of each domain in the LAION pretrain set (Figure 6). We find that for datasets where the domain images and captions occur infrequently such as Flowers102 and StanfordCars, the performance gain from Neural Priming is significantly larger. We measure domain prevalence in LAION-2B by substring searching for captions which contain the class names and counting the total number of images. Image content is another factor for measuring how close the pretraining set is to the downstream dataset in feature space, however this is computationally expensive for a dataset of 2 billion. Also this would depend on the model used to extract features. We choose to look at the caption statistics as a proxy because it is computationally feasible with a dataset at the size of LAION-2B and model agnostic.", "md": "We analyze the prevalence of each domain in the LAION pretrain set (Figure 6). We find that for datasets where the domain images and captions occur infrequently such as Flowers102 and StanfordCars, the performance gain from Neural Priming is significantly larger. We measure domain prevalence in LAION-2B by substring searching for captions which contain the class names and counting the total number of images. Image content is another factor for measuring how close the pretraining set is to the downstream dataset in feature space, however this is computationally expensive for a dataset of 2 billion. Also this would depend on the model used to extract features. We choose to look at the caption statistics as a proxy because it is computationally feasible with a dataset at the size of LAION-2B and model agnostic."}, {"type": "heading", "lvl": 2, "value": "Other Architectures and Datasets", "md": "## Other Architectures and Datasets"}, {"type": "text", "value": "We present the full results for ViT B/16, B/32 and ViT L/14 on LAION-2B for ImageNet, Flowers102, StanfordCars, FGVC-Aircraft, and OxfordPets. We find similar results across architectures. Results can be found in Table 6. We also reproduce experiments for the LAION-400m dataset with the B/16 architecture.", "md": "We present the full results for ViT B/16, B/32 and ViT L/14 on LAION-2B for ImageNet, Flowers102, StanfordCars, FGVC-Aircraft, and OxfordPets. We find similar results across architectures. Results can be found in Table 6. We also reproduce experiments for the LAION-400m dataset with the B/16 architecture."}, {"type": "heading", "lvl": 2, "value": "Dataset Statistics", "md": "## Dataset Statistics"}, {"type": "text", "value": "For ImageNet we performed URL deduplication using the LAION-2B URL list provided on HuggingFace. We found no ImageNet images in the pretrain data. We also randomly sampled images.", "md": "For ImageNet we performed URL deduplication using the LAION-2B URL list provided on HuggingFace. We found no ImageNet images in the pretrain data. We also randomly sampled images."}]}, {"page": 15, "text": "Table 4: Comparison of full fine-tuning and NCM with Neural Priming. We find NCM performs\nsimilarly in the low-sample regime.\n                             ImageNet     Stanford     FGVC      Flowers102     Food101     Oxford    SUN397\n                                            Cars      Aircraft                                Pets\n       CLIP [40, 21]            68.30       87.40      25.86        71.65         86.58      90.21      67.35\n       Retrieval + FT           70.28       87.95      26.22        72.15         86.63      90.35      68.01\n         VLM [33]               69.35       87.88      28.54        72.11         86.31      90.24      67.73\n         CuPL [39]              70.25       88.63      29.64        72.32         86.20      91.16      70.80\n       Priming (Ours)           70.75       89.30      33.03        79.81         86.66      91.87      71.21\n  Priming + CuPL (Ours)         71.38       90.23      36.00        80.04         86.86      91.85      72.35\n                                      Prevalence of Datasets In LAION-2B\n                 1.0\n                Relative Frequency\n                 0.8\n                 0.6\n                 0.4\n                 0.2\n                 0.0                    FGVC-AirCraft                             ImageNet\n                              Stanford Cars        Flowers102Food101    OxfordPets           SUN397\nFigure 6: Frequency of dataset class names in the LAION-2B dataset normalized to 1. We fi                      nd\nNeural Priming improves accuracy more for datasets that are rare.\nfrom the other datasets (Flowers102, FGVCAirCraft, etc.) and looked for duplicates, though we\nfound no exact matches. The exact dataset statistics are reported for context in Table 7.\nE     Comparison to REACT\nWe do not compare directly with REACT [30] in the main paper for two reasons. The first is that\nREACT adds a signifi     cant number of parameters (50 % for the B/16 model) to the network, making\nan unequal comparison from a parameter and FLOPS perspective. Second, REACT uses 6-10 million\nimages as the support set for each dataset which makes it computationally infeasible to rerun ourselves\nfor all of the data sets we benchmark on and without the parameter adding version. We compare on\nImageNet using the numbers reported in their work for comparable models trained on LAION-2B\n(Table 8).\nWe differ from REACT in a few key aspects. First, we realign the images labels to the downstream\ntask. For example, for the OxfordPets data set, given an image which has the caption \"maltese\non a leash walking through the parking with its owner\" we map the label to \"maltese\". REACT\ntrains contrastively using the original captions. We find this step significantly affects accuracy and\nsample-efficiency. For comparison, REACT retrieves 6-10 million images from LAION whereas we\nuse 10,000-100,000. Second, our method is significantly more efficient in the retrieval and fine-tuning\nstep so can be performed at test-time. This is due to our multi-stage fi         ltering with the fi rst stage of\ncaption filtering being extremely fast. Third, REACT adds 50% additional parameters to the ViT\nB/16 model.\n                                                       15", "md": "| |ImageNet|Stanford Cars|FGVC Aircraft|Flowers102|Food101|Oxford Pets|SUN397|\n|---|---|---|---|---|---|---|---|\n|CLIP [40, 21]|68.30|87.40|25.86|71.65|86.58|90.21|67.35|\n|Retrieval + FT|70.28|87.95|26.22|72.15|86.63|90.35|68.01|\n|VLM [33]|69.35|87.88|28.54|72.11|86.31|90.24|67.73|\n|CuPL [39]|70.25|88.63|29.64|72.32|86.20|91.16|70.80|\n|Priming (Ours)|70.75|89.30|33.03|79.81|86.66|91.87|71.21|\n|Priming + CuPL (Ours)|71.38|90.23|36.00|80.04|86.86|91.85|72.35|\n\nPrevalence of Datasets In LAION-2B\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{FGVC-Aircraft} & 1.0 \\\\\n\\text{ImageNet} & 0.0 \\\\\n\\text{Stanford Cars} & 0.8 \\\\\n\\text{Flowers102} & 0.6 \\\\\n\\text{Food101} & 0.4 \\\\\n\\text{Oxford Pets} & 0.2 \\\\\n\\text{SUN397} & 0.0 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 6: Frequency of dataset class names in the LAION-2B dataset normalized to 1. We find Neural Priming improves accuracy more for datasets that are rare.\n\nComparison to REACT\n\nWe do not compare directly with REACT [30] in the main paper for two reasons. The first is that REACT adds a significant number of parameters (50% for the B/16 model) to the network, making an unequal comparison from a parameter and FLOPS perspective. Second, REACT uses 6-10 million images as the support set for each dataset which makes it computationally infeasible to rerun ourselves for all of the data sets we benchmark on and without the parameter adding version. We compare on ImageNet using the numbers reported in their work for comparable models trained on LAION-2B (Table 8).\n\nWe differ from REACT in a few key aspects. First, we realign the images labels to the downstream task. For example, for the OxfordPets data set, given an image which has the caption \"maltese on a leash walking through the parking with its owner\" we map the label to \"maltese\". REACT trains contrastively using the original captions. We find this step significantly affects accuracy and sample-efficiency. For comparison, REACT retrieves 6-10 million images from LAION whereas we use 10,000-100,000. Second, our method is significantly more efficient in the retrieval and fine-tuning step so can be performed at test-time. This is due to our multi-stage filtering with the first stage of caption filtering being extremely fast. Third, REACT adds 50% additional parameters to the ViT B/16 model.", "images": [], "items": [{"type": "table", "rows": [["", "ImageNet", "Stanford Cars", "FGVC Aircraft", "Flowers102", "Food101", "Oxford Pets", "SUN397"], ["CLIP [40, 21]", "68.30", "87.40", "25.86", "71.65", "86.58", "90.21", "67.35"], ["Retrieval + FT", "70.28", "87.95", "26.22", "72.15", "86.63", "90.35", "68.01"], ["VLM [33]", "69.35", "87.88", "28.54", "72.11", "86.31", "90.24", "67.73"], ["CuPL [39]", "70.25", "88.63", "29.64", "72.32", "86.20", "91.16", "70.80"], ["Priming (Ours)", "70.75", "89.30", "33.03", "79.81", "86.66", "91.87", "71.21"], ["Priming + CuPL (Ours)", "71.38", "90.23", "36.00", "80.04", "86.86", "91.85", "72.35"]], "md": "| |ImageNet|Stanford Cars|FGVC Aircraft|Flowers102|Food101|Oxford Pets|SUN397|\n|---|---|---|---|---|---|---|---|\n|CLIP [40, 21]|68.30|87.40|25.86|71.65|86.58|90.21|67.35|\n|Retrieval + FT|70.28|87.95|26.22|72.15|86.63|90.35|68.01|\n|VLM [33]|69.35|87.88|28.54|72.11|86.31|90.24|67.73|\n|CuPL [39]|70.25|88.63|29.64|72.32|86.20|91.16|70.80|\n|Priming (Ours)|70.75|89.30|33.03|79.81|86.66|91.87|71.21|\n|Priming + CuPL (Ours)|71.38|90.23|36.00|80.04|86.86|91.85|72.35|", "isPerfectTable": true, "csv": "\"\",\"ImageNet\",\"Stanford Cars\",\"FGVC Aircraft\",\"Flowers102\",\"Food101\",\"Oxford Pets\",\"SUN397\"\n\"CLIP [40, 21]\",\"68.30\",\"87.40\",\"25.86\",\"71.65\",\"86.58\",\"90.21\",\"67.35\"\n\"Retrieval + FT\",\"70.28\",\"87.95\",\"26.22\",\"72.15\",\"86.63\",\"90.35\",\"68.01\"\n\"VLM [33]\",\"69.35\",\"87.88\",\"28.54\",\"72.11\",\"86.31\",\"90.24\",\"67.73\"\n\"CuPL [39]\",\"70.25\",\"88.63\",\"29.64\",\"72.32\",\"86.20\",\"91.16\",\"70.80\"\n\"Priming (Ours)\",\"70.75\",\"89.30\",\"33.03\",\"79.81\",\"86.66\",\"91.87\",\"71.21\"\n\"Priming + CuPL (Ours)\",\"71.38\",\"90.23\",\"36.00\",\"80.04\",\"86.86\",\"91.85\",\"72.35\""}, {"type": "text", "value": "Prevalence of Datasets In LAION-2B\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{FGVC-Aircraft} & 1.0 \\\\\n\\text{ImageNet} & 0.0 \\\\\n\\text{Stanford Cars} & 0.8 \\\\\n\\text{Flowers102} & 0.6 \\\\\n\\text{Food101} & 0.4 \\\\\n\\text{Oxford Pets} & 0.2 \\\\\n\\text{SUN397} & 0.0 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 6: Frequency of dataset class names in the LAION-2B dataset normalized to 1. We find Neural Priming improves accuracy more for datasets that are rare.\n\nComparison to REACT\n\nWe do not compare directly with REACT [30] in the main paper for two reasons. The first is that REACT adds a significant number of parameters (50% for the B/16 model) to the network, making an unequal comparison from a parameter and FLOPS perspective. Second, REACT uses 6-10 million images as the support set for each dataset which makes it computationally infeasible to rerun ourselves for all of the data sets we benchmark on and without the parameter adding version. We compare on ImageNet using the numbers reported in their work for comparable models trained on LAION-2B (Table 8).\n\nWe differ from REACT in a few key aspects. First, we realign the images labels to the downstream task. For example, for the OxfordPets data set, given an image which has the caption \"maltese on a leash walking through the parking with its owner\" we map the label to \"maltese\". REACT trains contrastively using the original captions. We find this step significantly affects accuracy and sample-efficiency. For comparison, REACT retrieves 6-10 million images from LAION whereas we use 10,000-100,000. Second, our method is significantly more efficient in the retrieval and fine-tuning step so can be performed at test-time. This is due to our multi-stage filtering with the first stage of caption filtering being extremely fast. Third, REACT adds 50% additional parameters to the ViT B/16 model.", "md": "Prevalence of Datasets In LAION-2B\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{FGVC-Aircraft} & 1.0 \\\\\n\\text{ImageNet} & 0.0 \\\\\n\\text{Stanford Cars} & 0.8 \\\\\n\\text{Flowers102} & 0.6 \\\\\n\\text{Food101} & 0.4 \\\\\n\\text{Oxford Pets} & 0.2 \\\\\n\\text{SUN397} & 0.0 \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 6: Frequency of dataset class names in the LAION-2B dataset normalized to 1. We find Neural Priming improves accuracy more for datasets that are rare.\n\nComparison to REACT\n\nWe do not compare directly with REACT [30] in the main paper for two reasons. The first is that REACT adds a significant number of parameters (50% for the B/16 model) to the network, making an unequal comparison from a parameter and FLOPS perspective. Second, REACT uses 6-10 million images as the support set for each dataset which makes it computationally infeasible to rerun ourselves for all of the data sets we benchmark on and without the parameter adding version. We compare on ImageNet using the numbers reported in their work for comparable models trained on LAION-2B (Table 8).\n\nWe differ from REACT in a few key aspects. First, we realign the images labels to the downstream task. For example, for the OxfordPets data set, given an image which has the caption \"maltese on a leash walking through the parking with its owner\" we map the label to \"maltese\". REACT trains contrastively using the original captions. We find this step significantly affects accuracy and sample-efficiency. For comparison, REACT retrieves 6-10 million images from LAION whereas we use 10,000-100,000. Second, our method is significantly more efficient in the retrieval and fine-tuning step so can be performed at test-time. This is due to our multi-stage filtering with the first stage of caption filtering being extremely fast. Third, REACT adds 50% additional parameters to the ViT B/16 model."}]}, {"page": 16, "text": "Table 5: Performance of Neural Priming and the CLIP baseline with the ViT B/16 architecture.\nWe observe similar trends as for the LAION-2B dataset. Neural Priming improves over the zero-shot\n performance with similar relative error reduction.\n                                ImageNet      Stanford     FGVC       Flowers102      Food101      Oxford   SUN397\n                                                Cars      Aircraft                                   Pets\n        CLIP [40, 21]             66.72         82.47       16.05         71.47         86.33       88.68     61.13\n   Neural Priming (Ours)          69.21         86.83       26.73         82.17         86.62       90.08     62.27\nTable 6: Zero-shot results for ViT B/32 and L/14 architectures. We find similar results across\n architectures. Surprisingly, the relative error often decreases more with architecture scale.\n Capacity      ImageNet      Stanford Cars     Flowers102      FGVCAircraft       Food101      Oxford Pets    SUN397\n                                                    Baseline (NCM)\n ViT-B/32        67.12           84.21            67.40             23.79           82.53          90.62       61.97\n ViT-B/16        68.30           87.40            71.65             25.86           86.58          90.21       67.35\n ViT-L/14        76.62           91.82            75.14             33.36           90.96          93.40       71.29\n                                                     Neural Priming\n ViT-B/32        68.38           87.45            72.26             31.68           82.91          91.25       63.84\n ViT-B/16        70.75           90.23            79.81             33.03           86.66          91.87       71.21\n ViT-L/14        78.72           93.12            84.48             43.62           91.06          94.05       72.49\n F    Few-Shot\nWe report the few-shot performance of a model pretrained on LAION-2B with CuPL prompts (Table\n 9). We find that better prompts synergize with Neural Priming similar to the zero-shot case. The\n mixing coeffi   cients used for these experiments are set by the schedule given in section A.\n G     Transductive Run-Time Analysis\nTransductive run-time can be broken up into two components: 1. Time to get the initial priming pool\n and 2. Time to perform exact retrieval on image features of the priming pool.\n G.1    Initial Priming Pool construction timings\n Here we study the wall-clock time for construction of the initial priming pool on consumer hardware\n(Intel 10900k CPU and Samsung 980 Pro NVME drive). Wall-clock times for priming pool construc-\n tion vary greatly depending on a variety of factors. The two main factors affecting timings are: 1.\n speed of the storage on which the fast text search (FTS) databases are stored (see Appendix A) and\n 2. whether or not LAION-2b is stored locally (if it is not stored locally, then you need to access the\n                        102\n                      Seconds to search\n                        101\n                        100\n                       10  1\n                                0           200           400          600          800          1000\n                                                       Sorted query index\n Figure 7: Wall-clock times by query for construction of the initial priming pool. The queries used\n here are the ones used for the construction of the ImageNet pool. Due to the nature of full text search,\n rare queries are very fast, with most classes taking less than 1 second to search all of LAION-2b.\n                                                            16", "md": "**Table 5: Performance of Neural Priming and the CLIP baseline with the ViT B/16 architecture.**\n| |ImageNet|Stanford Cars|FGVC Aircraft|Flowers102|Food101|Oxford Pets|SUN397|\n|---|---|---|---|---|---|---|---|\n|CLIP [40, 21]|66.72|82.47|16.05|71.47|86.33|88.68|61.13|\n|Neural Priming (Ours)|69.21|86.83|26.73|82.17|86.62|90.08|62.27|\n\n**Table 6: Zero-shot results for ViT B/32 and L/14 architectures.**\n|Capacity|ImageNet|Stanford Cars|Flowers102|FGVC Aircraft|Food101|Oxford Pets|SUN397|\n|---|---|---|---|---|---|---|---|\n|ViT-B/32|67.12|84.21|67.40|23.79|82.53|90.62|61.97|\n|ViT-B/16|68.30|87.40|71.65|25.86|86.58|90.21|67.35|\n|ViT-L/14|76.62|91.82|75.14|33.36|90.96|93.40|71.29|\n|ViT-B/32 Neural Priming|68.38|87.45|72.26|31.68|82.91|91.25|63.84|\n|ViT-B/16 Neural Priming|70.75|90.23|79.81|33.03|86.66|91.87|71.21|\n|ViT-L/14 Neural Priming|78.72|93.12|84.48|43.62|91.06|94.05|72.49|\n\nFigure 7: Wall-clock times by query for construction of the initial priming pool. The queries used here are the ones used for the construction of the ImageNet pool. Due to the nature of full text search, rare queries are very fast, with most classes taking less than 1 second to search all of LAION-2b.", "images": [], "items": [{"type": "text", "value": "**Table 5: Performance of Neural Priming and the CLIP baseline with the ViT B/16 architecture.**", "md": "**Table 5: Performance of Neural Priming and the CLIP baseline with the ViT B/16 architecture.**"}, {"type": "table", "rows": [["", "ImageNet", "Stanford Cars", "FGVC Aircraft", "Flowers102", "Food101", "Oxford Pets", "SUN397"], ["CLIP [40, 21]", "66.72", "82.47", "16.05", "71.47", "86.33", "88.68", "61.13"], ["Neural Priming (Ours)", "69.21", "86.83", "26.73", "82.17", "86.62", "90.08", "62.27"]], "md": "| |ImageNet|Stanford Cars|FGVC Aircraft|Flowers102|Food101|Oxford Pets|SUN397|\n|---|---|---|---|---|---|---|---|\n|CLIP [40, 21]|66.72|82.47|16.05|71.47|86.33|88.68|61.13|\n|Neural Priming (Ours)|69.21|86.83|26.73|82.17|86.62|90.08|62.27|", "isPerfectTable": true, "csv": "\"\",\"ImageNet\",\"Stanford Cars\",\"FGVC Aircraft\",\"Flowers102\",\"Food101\",\"Oxford Pets\",\"SUN397\"\n\"CLIP [40, 21]\",\"66.72\",\"82.47\",\"16.05\",\"71.47\",\"86.33\",\"88.68\",\"61.13\"\n\"Neural Priming (Ours)\",\"69.21\",\"86.83\",\"26.73\",\"82.17\",\"86.62\",\"90.08\",\"62.27\""}, {"type": "text", "value": "**Table 6: Zero-shot results for ViT B/32 and L/14 architectures.**", "md": "**Table 6: Zero-shot results for ViT B/32 and L/14 architectures.**"}, {"type": "table", "rows": [["Capacity", "ImageNet", "Stanford Cars", "Flowers102", "FGVC Aircraft", "Food101", "Oxford Pets", "SUN397"], ["ViT-B/32", "67.12", "84.21", "67.40", "23.79", "82.53", "90.62", "61.97"], ["ViT-B/16", "68.30", "87.40", "71.65", "25.86", "86.58", "90.21", "67.35"], ["ViT-L/14", "76.62", "91.82", "75.14", "33.36", "90.96", "93.40", "71.29"], ["ViT-B/32 Neural Priming", "68.38", "87.45", "72.26", "31.68", "82.91", "91.25", "63.84"], ["ViT-B/16 Neural Priming", "70.75", "90.23", "79.81", "33.03", "86.66", "91.87", "71.21"], ["ViT-L/14 Neural Priming", "78.72", "93.12", "84.48", "43.62", "91.06", "94.05", "72.49"]], "md": "|Capacity|ImageNet|Stanford Cars|Flowers102|FGVC Aircraft|Food101|Oxford Pets|SUN397|\n|---|---|---|---|---|---|---|---|\n|ViT-B/32|67.12|84.21|67.40|23.79|82.53|90.62|61.97|\n|ViT-B/16|68.30|87.40|71.65|25.86|86.58|90.21|67.35|\n|ViT-L/14|76.62|91.82|75.14|33.36|90.96|93.40|71.29|\n|ViT-B/32 Neural Priming|68.38|87.45|72.26|31.68|82.91|91.25|63.84|\n|ViT-B/16 Neural Priming|70.75|90.23|79.81|33.03|86.66|91.87|71.21|\n|ViT-L/14 Neural Priming|78.72|93.12|84.48|43.62|91.06|94.05|72.49|", "isPerfectTable": true, "csv": "\"Capacity\",\"ImageNet\",\"Stanford Cars\",\"Flowers102\",\"FGVC Aircraft\",\"Food101\",\"Oxford Pets\",\"SUN397\"\n\"ViT-B/32\",\"67.12\",\"84.21\",\"67.40\",\"23.79\",\"82.53\",\"90.62\",\"61.97\"\n\"ViT-B/16\",\"68.30\",\"87.40\",\"71.65\",\"25.86\",\"86.58\",\"90.21\",\"67.35\"\n\"ViT-L/14\",\"76.62\",\"91.82\",\"75.14\",\"33.36\",\"90.96\",\"93.40\",\"71.29\"\n\"ViT-B/32 Neural Priming\",\"68.38\",\"87.45\",\"72.26\",\"31.68\",\"82.91\",\"91.25\",\"63.84\"\n\"ViT-B/16 Neural Priming\",\"70.75\",\"90.23\",\"79.81\",\"33.03\",\"86.66\",\"91.87\",\"71.21\"\n\"ViT-L/14 Neural Priming\",\"78.72\",\"93.12\",\"84.48\",\"43.62\",\"91.06\",\"94.05\",\"72.49\""}, {"type": "text", "value": "Figure 7: Wall-clock times by query for construction of the initial priming pool. The queries used here are the ones used for the construction of the ImageNet pool. Due to the nature of full text search, rare queries are very fast, with most classes taking less than 1 second to search all of LAION-2b.", "md": "Figure 7: Wall-clock times by query for construction of the initial priming pool. The queries used here are the ones used for the construction of the ImageNet pool. Due to the nature of full text search, rare queries are very fast, with most classes taking less than 1 second to search all of LAION-2b."}]}, {"page": 17, "text": "                       Dataset        Number of Classes      Train Size    Test Size\n                      ImageNet               1000             1281167       50000\n                    Stanford Cars             196               8144         8041\n                   FGVC Aircraft              100               6667         3333\n                     Flowers102               102               2040         6149\n                      Food101                 102              75750        25250\n                     Oxford Pets               37               3680         3669\n                      SUN397                  397              19850        19850\nTable 7: Dataset statistics for every dataset evaluated on in Section 4. We evaluated using the accuracy\nmetric for every dataset.\nTable 8: Comparison with REACT on ImageNet for varying capacities. Neural Priming outper-\nforms REACT on ImageNet without adding parameters while retraining on 1% of the data REACT\ndoes (10 million vs 100,000.)\n                                                      B/16     L/14\n                                   REACT              67.5     76.4\n                                   Neural Priming     68.38    78.72\noriginal URL to get the image, which adds another factor of variation). Given the massive changes in\nperformance between the two options for (2), we only give wall-clock timing estimates for (1).\nFTS databases in SQLite use an inverted index over the tokens of their entries. In this case, the\ntokenization over full words and some more common suffix strings. Given this tokenization, it is\neasier to search for less common phrases and words. In Figure 7, we analyze this effect with respect\nto ImageNet priming pool construction. We see that most queries take less than 1 second to search\nall of LAION-2b. A few very common queries take much longer. For example, the most common\nquery, T-shirt, takes >100s to search. The overall time to complete these searches for ImageNet was\n21 minutes. For smaller datasets with less common class names, this process is significantly shorter.\nThis is the majority of the runtime of our overall method.\nG.2   Exact Retrieval Time\nThe next part of our process was performing exact retrieval on the priming pool at test time. The\nmethod for this is described in Section 3.1.2 This is generally effi   cient, as the priming pool is a\nsignificantly filtered version of the pre-training dataset. To fi\n                                                                lter a priming pool of 1.1m images to\n51k using the test set of ImageNetV2 took 3.2 minutes on consumer hardware (two 3090 GPUs), not\nincluding the initial feature extraction time since these are easily pre-computed.\nH    Other Implementation Details\nRetrieval + Finetuning We retrieve 100 images per class using HNSW, an approximate nearest\nneighbors method, with the CLIP text-embedding from the class name. We use contrastive fine-tuning\nwith the CLIP objective as the pretrain data only has natural language descriptions as labels. We\nfine-tune for with a learning rate of 1e-6 and batch size 1024.\nCoOp [55] CoOp tries to learn the embeddings for the prompts using a few examples for each of the\nclasses. We use the official implementation2 provided by the authors with default hyperparameters.\nFor ImageNet experiments, we the models for 50 epochs regardless of the shots. For the smaller\ndatasets like Stanford Cars, Flower102 and FGCV-Aircraft, we train the 1-shot models for 50 epochs,\n2-shot and 5-shot models for 100 epochs, and 10-shot models for 200 epochs. We use the standard\ntrain-test split for all the datasets to ensure consistency across all the methods. Lastly, instead of\nthe CLIP models provided by OpenAI, we use OpenCLIP [21] models for fair comparison against\nNeural Priming.\n   2https://github.com/KaiyangZhou/CoOp\n                                                  17", "md": "# Dataset Statistics\n\n## Dataset Statistics\n\n|Dataset|Number of Classes|Train Size|Test Size|\n|---|---|---|---|\n|ImageNet|1000|1281167|50000|\n|Stanford Cars|196|8144|8041|\n|FGVC Aircraft|100|6667|3333|\n|Flowers102|102|2040|6149|\n|Food101|102|75750|25250|\n|Oxford Pets|37|3680|3669|\n|SUN397|397|19850|19850|\n\n## Comparison with REACT on ImageNet\n\n| |B/16|L/14|\n|---|---|---|\n|REACT|67.5|76.4|\n|Neural Priming|68.38|78.72|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Dataset Statistics", "md": "# Dataset Statistics"}, {"type": "heading", "lvl": 2, "value": "Dataset Statistics", "md": "## Dataset Statistics"}, {"type": "table", "rows": [["Dataset", "Number of Classes", "Train Size", "Test Size"], ["ImageNet", "1000", "1281167", "50000"], ["Stanford Cars", "196", "8144", "8041"], ["FGVC Aircraft", "100", "6667", "3333"], ["Flowers102", "102", "2040", "6149"], ["Food101", "102", "75750", "25250"], ["Oxford Pets", "37", "3680", "3669"], ["SUN397", "397", "19850", "19850"]], "md": "|Dataset|Number of Classes|Train Size|Test Size|\n|---|---|---|---|\n|ImageNet|1000|1281167|50000|\n|Stanford Cars|196|8144|8041|\n|FGVC Aircraft|100|6667|3333|\n|Flowers102|102|2040|6149|\n|Food101|102|75750|25250|\n|Oxford Pets|37|3680|3669|\n|SUN397|397|19850|19850|", "isPerfectTable": true, "csv": "\"Dataset\",\"Number of Classes\",\"Train Size\",\"Test Size\"\n\"ImageNet\",\"1000\",\"1281167\",\"50000\"\n\"Stanford Cars\",\"196\",\"8144\",\"8041\"\n\"FGVC Aircraft\",\"100\",\"6667\",\"3333\"\n\"Flowers102\",\"102\",\"2040\",\"6149\"\n\"Food101\",\"102\",\"75750\",\"25250\"\n\"Oxford Pets\",\"37\",\"3680\",\"3669\"\n\"SUN397\",\"397\",\"19850\",\"19850\""}, {"type": "heading", "lvl": 2, "value": "Comparison with REACT on ImageNet", "md": "## Comparison with REACT on ImageNet"}, {"type": "table", "rows": [["", "B/16", "L/14"], ["REACT", "67.5", "76.4"], ["Neural Priming", "68.38", "78.72"]], "md": "| |B/16|L/14|\n|---|---|---|\n|REACT|67.5|76.4|\n|Neural Priming|68.38|78.72|", "isPerfectTable": true, "csv": "\"\",\"B/16\",\"L/14\"\n\"REACT\",\"67.5\",\"76.4\"\n\"Neural Priming\",\"68.38\",\"78.72\""}]}, {"page": 18, "text": " Table 9: Performance on Neural Priming (NP) with prompt-tuning method in the few-shot\n setting. We find that prompt-tuning methods are compatible with Neural Priming even when training\n examples are available.\n               Shots        ImageNet            Stanford Cars          Flowers102           FGVCAircraft\n                       NP + CuPL      NP     NP + CuPL      NP     NP + CuPL      NP     NP + CuPL      NP\n                 0        71.38      71.15      90.23      90.11      80.04      79.78      35.70      35.38\n                 1        71.56      71.32      90.31      90.17      80.77      80.72      36.39      36.55\n                 2        71.81      71.50      90.55      90.21      88.90      88.60      38.16      37.94\n                 5        72.03      71.92      91.03      90.93      94.66      93.43      40.01      39.84\n                10        72.36      72.27      91.54      91.49      94.83      94.75      43.83      43.67\n TPT [45] Test-time Prompt Tuning (TPT) combines the ideas of test-time training [48] and prompt\n learning [55]. For a given test image, TPT tries to learn a soft prompt that maximizes agreement\n between multiple augmented views. We use the official implementation3 for all the experiments and\n train TPT with 64 augmented views to ensure the best performance on all the datasets.\n                      \u03c3      ImageNet        Stanford Cars        FGVC-Aircraft         Flowers102\n                      1          68.9             87.54                 25.9                71.94\n                     10          69.1             88.98                 29.75               74.97\n                    100         70.75              89.3                 33.03               79.81\n                    1000        70.64             89.11                 31.32               78.81\n Table 10: An ablation over the sigma value used to calculate the mixing coeffi                     cient between the\n image and text features. A larger sigma value is associated with a stronger prior for the text features.\n I    Confidence Intervals\nWe report the confidence intervals and average accuracy for the few-shot experiments (Table 11). For\n the zero-shot experiments, performance does not vary. For the few-shot experiments variation derives\n from sampling the training examples. We average accuracy across 10 runs and find the variance to be\n minimal (less than 1% for most settings).\n J    Broader Impact\n One potential risk of Neural Priming is amplifying bias in the dataset. By retraining on a subset of the\n data, it is possible to condition the model in a way that is biased given a biased prompt or captions.\n For downstream tasks which are sensitive to bias, the risk could be mitigated by fi               ltering the captions\n for inappropriate text or using an auxiliary model to detect inappropriate images. This approach of\n filtering the priming dataset is a potential direction for reducing bias in a model on the fly. Another\n potential drawback of Neural Priming is that it increases the carbon footprint of the trained model by\n retraining it on more data. However, the size of our priming set is small relative to LAION-2B (less\n than 1%) and we only retrain on the last layer which minimizes compute cost.\n     3https://github.com/azshue/TPT\n                                                            18", "md": "**Table 9: Performance on Neural Priming (NP) with prompt-tuning method in the few-shot setting.**\n|Shots|ImageNet|Stanford Cars|Flowers102|FGVCAircraft|\n|---|---|---|---|---|\n| |NP + CuPL|NP|NP + CuPL|NP|\n|0|71.38|71.15|90.23|90.11|\n|1|71.56|71.32|90.31|90.17|\n|2|71.81|71.50|90.55|90.21|\n|5|72.03|71.92|91.03|90.93|\n|10|72.36|72.27|91.54|91.49|\n\nTPT [45] Test-time Prompt Tuning (TPT) combines the ideas of test-time training [48] and prompt learning [55]. For a given test image, TPT tries to learn a soft prompt that maximizes agreement between multiple augmented views. We use the official implementation for all the experiments and train TPT with 64 augmented views to ensure the best performance on all the datasets.\n\n**\u03c3 values for different datasets**\n|\u03c3|ImageNet|Stanford Cars|FGVC-Aircraft|Flowers102|\n|---|---|---|---|---|\n|1|68.9|87.54|25.9|71.94|\n|10|69.1|88.98|29.75|74.97|\n|100|70.75|89.3|33.03|79.81|\n|1000|70.64|89.11|31.32|78.81|\n\nTable 10: An ablation over the sigma value used to calculate the mixing coefficient between the image and text features. A larger sigma value is associated with a stronger prior for the text features.\n\n## Confidence Intervals\n\nWe report the confidence intervals and average accuracy for the few-shot experiments (Table 11). For the zero-shot experiments, performance does not vary. For the few-shot experiments variation derives from sampling the training examples. We average accuracy across 10 runs and find the variance to be minimal (less than 1% for most settings).\n\n## Broader Impact\n\nOne potential risk of Neural Priming is amplifying bias in the dataset. By retraining on a subset of the data, it is possible to condition the model in a way that is biased given a biased prompt or captions. For downstream tasks which are sensitive to bias, the risk could be mitigated by filtering the captions for inappropriate text or using an auxiliary model to detect inappropriate images. This approach of filtering the priming dataset is a potential direction for reducing bias in a model on the fly. Another potential drawback of Neural Priming is that it increases the carbon footprint of the trained model by retraining it on more data. However, the size of our priming set is small relative to LAION-2B (less than 1%) and we only retrain on the last layer which minimizes compute cost.\n\nOfficial implementation for TPT can be found here.", "images": [], "items": [{"type": "text", "value": "**Table 9: Performance on Neural Priming (NP) with prompt-tuning method in the few-shot setting.**", "md": "**Table 9: Performance on Neural Priming (NP) with prompt-tuning method in the few-shot setting.**"}, {"type": "table", "rows": [["Shots", "ImageNet", "Stanford Cars", "Flowers102", "FGVCAircraft"], ["", "NP + CuPL", "NP", "NP + CuPL", "NP"], ["0", "71.38", "71.15", "90.23", "90.11"], ["1", "71.56", "71.32", "90.31", "90.17"], ["2", "71.81", "71.50", "90.55", "90.21"], ["5", "72.03", "71.92", "91.03", "90.93"], ["10", "72.36", "72.27", "91.54", "91.49"]], "md": "|Shots|ImageNet|Stanford Cars|Flowers102|FGVCAircraft|\n|---|---|---|---|---|\n| |NP + CuPL|NP|NP + CuPL|NP|\n|0|71.38|71.15|90.23|90.11|\n|1|71.56|71.32|90.31|90.17|\n|2|71.81|71.50|90.55|90.21|\n|5|72.03|71.92|91.03|90.93|\n|10|72.36|72.27|91.54|91.49|", "isPerfectTable": true, "csv": "\"Shots\",\"ImageNet\",\"Stanford Cars\",\"Flowers102\",\"FGVCAircraft\"\n\"\",\"NP + CuPL\",\"NP\",\"NP + CuPL\",\"NP\"\n\"0\",\"71.38\",\"71.15\",\"90.23\",\"90.11\"\n\"1\",\"71.56\",\"71.32\",\"90.31\",\"90.17\"\n\"2\",\"71.81\",\"71.50\",\"90.55\",\"90.21\"\n\"5\",\"72.03\",\"71.92\",\"91.03\",\"90.93\"\n\"10\",\"72.36\",\"72.27\",\"91.54\",\"91.49\""}, {"type": "text", "value": "TPT [45] Test-time Prompt Tuning (TPT) combines the ideas of test-time training [48] and prompt learning [55]. For a given test image, TPT tries to learn a soft prompt that maximizes agreement between multiple augmented views. We use the official implementation for all the experiments and train TPT with 64 augmented views to ensure the best performance on all the datasets.\n\n**\u03c3 values for different datasets**", "md": "TPT [45] Test-time Prompt Tuning (TPT) combines the ideas of test-time training [48] and prompt learning [55]. For a given test image, TPT tries to learn a soft prompt that maximizes agreement between multiple augmented views. We use the official implementation for all the experiments and train TPT with 64 augmented views to ensure the best performance on all the datasets.\n\n**\u03c3 values for different datasets**"}, {"type": "table", "rows": [["\u03c3", "ImageNet", "Stanford Cars", "FGVC-Aircraft", "Flowers102"], ["1", "68.9", "87.54", "25.9", "71.94"], ["10", "69.1", "88.98", "29.75", "74.97"], ["100", "70.75", "89.3", "33.03", "79.81"], ["1000", "70.64", "89.11", "31.32", "78.81"]], "md": "|\u03c3|ImageNet|Stanford Cars|FGVC-Aircraft|Flowers102|\n|---|---|---|---|---|\n|1|68.9|87.54|25.9|71.94|\n|10|69.1|88.98|29.75|74.97|\n|100|70.75|89.3|33.03|79.81|\n|1000|70.64|89.11|31.32|78.81|", "isPerfectTable": true, "csv": "\"\u03c3\",\"ImageNet\",\"Stanford Cars\",\"FGVC-Aircraft\",\"Flowers102\"\n\"1\",\"68.9\",\"87.54\",\"25.9\",\"71.94\"\n\"10\",\"69.1\",\"88.98\",\"29.75\",\"74.97\"\n\"100\",\"70.75\",\"89.3\",\"33.03\",\"79.81\"\n\"1000\",\"70.64\",\"89.11\",\"31.32\",\"78.81\""}, {"type": "text", "value": "Table 10: An ablation over the sigma value used to calculate the mixing coefficient between the image and text features. A larger sigma value is associated with a stronger prior for the text features.", "md": "Table 10: An ablation over the sigma value used to calculate the mixing coefficient between the image and text features. A larger sigma value is associated with a stronger prior for the text features."}, {"type": "heading", "lvl": 2, "value": "Confidence Intervals", "md": "## Confidence Intervals"}, {"type": "text", "value": "We report the confidence intervals and average accuracy for the few-shot experiments (Table 11). For the zero-shot experiments, performance does not vary. For the few-shot experiments variation derives from sampling the training examples. We average accuracy across 10 runs and find the variance to be minimal (less than 1% for most settings).", "md": "We report the confidence intervals and average accuracy for the few-shot experiments (Table 11). For the zero-shot experiments, performance does not vary. For the few-shot experiments variation derives from sampling the training examples. We average accuracy across 10 runs and find the variance to be minimal (less than 1% for most settings)."}, {"type": "heading", "lvl": 2, "value": "Broader Impact", "md": "## Broader Impact"}, {"type": "text", "value": "One potential risk of Neural Priming is amplifying bias in the dataset. By retraining on a subset of the data, it is possible to condition the model in a way that is biased given a biased prompt or captions. For downstream tasks which are sensitive to bias, the risk could be mitigated by filtering the captions for inappropriate text or using an auxiliary model to detect inappropriate images. This approach of filtering the priming dataset is a potential direction for reducing bias in a model on the fly. Another potential drawback of Neural Priming is that it increases the carbon footprint of the trained model by retraining it on more data. However, the size of our priming set is small relative to LAION-2B (less than 1%) and we only retrain on the last layer which minimizes compute cost.\n\nOfficial implementation for TPT can be found here.", "md": "One potential risk of Neural Priming is amplifying bias in the dataset. By retraining on a subset of the data, it is possible to condition the model in a way that is biased given a biased prompt or captions. For downstream tasks which are sensitive to bias, the risk could be mitigated by filtering the captions for inappropriate text or using an auxiliary model to detect inappropriate images. This approach of filtering the priming dataset is a potential direction for reducing bias in a model on the fly. Another potential drawback of Neural Priming is that it increases the carbon footprint of the trained model by retraining it on more data. However, the size of our priming set is small relative to LAION-2B (less than 1%) and we only retrain on the last layer which minimizes compute cost.\n\nOfficial implementation for TPT can be found here."}]}, {"page": 19, "text": "Table 11: Few-shot results with confidence intervals. We report the average performance of Neural\nPriming across 10 runs with the standard deviation. We find that Neural Priming has lower variance\nthan the baseline as the constant priming pool leads to less variation in the centroid of images.\n  Shots               ImageNet                          Stanford Cars                        Flowers102                     FGVCAircraft\n          Neural Priming   Baseline (NCM)     Neural Priming   Baseline (NCM)     Neural Priming   Baseline (NCM)  Neural Priming   Baseline (NCM)\n   1       71.53 \u00b1 0.04      69.81 \u00b1 0.05      90.31 \u00b1 0.16      88.57 \u00b1 0.48      80.77 \u00b1 0.14      76.09 \u00b1 1.14   36.39 \u00b1 0.22      27.56 \u00b1 1.12\n   2       71.83 \u00b1 0.05      70.05 \u00b1 0.05      90.55 \u00b1 0.15      88.73 \u00b1 0.43      88.90 \u00b1 0.13      82.50 \u00b1 0.74   38.16 \u00b1 0.17      30.01 \u00b1 0.67\n   5       72.02 \u00b1 0.04      70.43 \u00b1 0.02      91.03 \u00b1 0.12      89.19 \u00b1 0.18      94.66 \u00b1 0.10      89.46 \u00b1 0.29   40.01 \u00b1 0.11      35.50 \u00b1 0.77\n  10       72.34 \u00b1 0.03      70.90 \u00b1 0.04      91.54 \u00b1 0.10      90.86 \u00b1 0.16      94.83 \u00b1 0.01      92.74 \u00b1 0.02   43.83 \u00b1 0.09      40.84 \u00b1 0.42\n                                                                          19", "md": "|Shots|ImageNet|ImageNet|Stanford Cars|Stanford Cars|Flowers102|Flowers102|FGVCAircraft|FGVCAircraft|\n|---|---|---|---|---|\n| |Neural Priming|Baseline (NCM)|Neural Priming|Baseline (NCM)|Neural Priming|Baseline (NCM)|Neural Priming|Baseline (NCM)|\n|1|$71.53 \\pm 0.04$|$69.81 \\pm 0.05$|$90.31 \\pm 0.16$|$88.57 \\pm 0.48$|$80.77 \\pm 0.14$|$76.09 \\pm 1.14$|$36.39 \\pm 0.22$|$27.56 \\pm 1.12$|\n|2|$71.83 \\pm 0.05$|$70.05 \\pm 0.05$|$90.55 \\pm 0.15$|$88.73 \\pm 0.43$|$88.90 \\pm 0.13$|$82.50 \\pm 0.74$|$38.16 \\pm 0.17$|$30.01 \\pm 0.67$|\n|5|$72.02 \\pm 0.04$|$70.43 \\pm 0.02$|$91.03 \\pm 0.12$|$89.19 \\pm 0.18$|$94.66 \\pm 0.10$|$89.46 \\pm 0.29$|$40.01 \\pm 0.11$|$35.50 \\pm 0.77$|\n|10|$72.34 \\pm 0.03$|$70.90 \\pm 0.04$|$91.54 \\pm 0.10$|$90.86 \\pm 0.16$|$94.83 \\pm 0.01$|$92.74 \\pm 0.02$|$43.83 \\pm 0.09$|$40.84 \\pm 0.42$|", "images": [], "items": [{"type": "table", "rows": [["Shots", "ImageNet", "ImageNet", "Stanford Cars", "Stanford Cars", "Flowers102", "Flowers102", "FGVCAircraft", "FGVCAircraft"], ["", "Neural Priming", "Baseline (NCM)", "Neural Priming", "Baseline (NCM)", "Neural Priming", "Baseline (NCM)", "Neural Priming", "Baseline (NCM)"], ["1", "$71.53 \\pm 0.04$", "$69.81 \\pm 0.05$", "$90.31 \\pm 0.16$", "$88.57 \\pm 0.48$", "$80.77 \\pm 0.14$", "$76.09 \\pm 1.14$", "$36.39 \\pm 0.22$", "$27.56 \\pm 1.12$"], ["2", "$71.83 \\pm 0.05$", "$70.05 \\pm 0.05$", "$90.55 \\pm 0.15$", "$88.73 \\pm 0.43$", "$88.90 \\pm 0.13$", "$82.50 \\pm 0.74$", "$38.16 \\pm 0.17$", "$30.01 \\pm 0.67$"], ["5", "$72.02 \\pm 0.04$", "$70.43 \\pm 0.02$", "$91.03 \\pm 0.12$", "$89.19 \\pm 0.18$", "$94.66 \\pm 0.10$", "$89.46 \\pm 0.29$", "$40.01 \\pm 0.11$", "$35.50 \\pm 0.77$"], ["10", "$72.34 \\pm 0.03$", "$70.90 \\pm 0.04$", "$91.54 \\pm 0.10$", "$90.86 \\pm 0.16$", "$94.83 \\pm 0.01$", "$92.74 \\pm 0.02$", "$43.83 \\pm 0.09$", "$40.84 \\pm 0.42$"]], "md": "|Shots|ImageNet|ImageNet|Stanford Cars|Stanford Cars|Flowers102|Flowers102|FGVCAircraft|FGVCAircraft|\n|---|---|---|---|---|\n| |Neural Priming|Baseline (NCM)|Neural Priming|Baseline (NCM)|Neural Priming|Baseline (NCM)|Neural Priming|Baseline (NCM)|\n|1|$71.53 \\pm 0.04$|$69.81 \\pm 0.05$|$90.31 \\pm 0.16$|$88.57 \\pm 0.48$|$80.77 \\pm 0.14$|$76.09 \\pm 1.14$|$36.39 \\pm 0.22$|$27.56 \\pm 1.12$|\n|2|$71.83 \\pm 0.05$|$70.05 \\pm 0.05$|$90.55 \\pm 0.15$|$88.73 \\pm 0.43$|$88.90 \\pm 0.13$|$82.50 \\pm 0.74$|$38.16 \\pm 0.17$|$30.01 \\pm 0.67$|\n|5|$72.02 \\pm 0.04$|$70.43 \\pm 0.02$|$91.03 \\pm 0.12$|$89.19 \\pm 0.18$|$94.66 \\pm 0.10$|$89.46 \\pm 0.29$|$40.01 \\pm 0.11$|$35.50 \\pm 0.77$|\n|10|$72.34 \\pm 0.03$|$70.90 \\pm 0.04$|$91.54 \\pm 0.10$|$90.86 \\pm 0.16$|$94.83 \\pm 0.01$|$92.74 \\pm 0.02$|$43.83 \\pm 0.09$|$40.84 \\pm 0.42$|", "isPerfectTable": true, "csv": "\"Shots\",\"ImageNet\",\"ImageNet\",\"Stanford Cars\",\"Stanford Cars\",\"Flowers102\",\"Flowers102\",\"FGVCAircraft\",\"FGVCAircraft\"\n\"\",\"Neural Priming\",\"Baseline (NCM)\",\"Neural Priming\",\"Baseline (NCM)\",\"Neural Priming\",\"Baseline (NCM)\",\"Neural Priming\",\"Baseline (NCM)\"\n\"1\",\"$71.53 \\pm 0.04$\",\"$69.81 \\pm 0.05$\",\"$90.31 \\pm 0.16$\",\"$88.57 \\pm 0.48$\",\"$80.77 \\pm 0.14$\",\"$76.09 \\pm 1.14$\",\"$36.39 \\pm 0.22$\",\"$27.56 \\pm 1.12$\"\n\"2\",\"$71.83 \\pm 0.05$\",\"$70.05 \\pm 0.05$\",\"$90.55 \\pm 0.15$\",\"$88.73 \\pm 0.43$\",\"$88.90 \\pm 0.13$\",\"$82.50 \\pm 0.74$\",\"$38.16 \\pm 0.17$\",\"$30.01 \\pm 0.67$\"\n\"5\",\"$72.02 \\pm 0.04$\",\"$70.43 \\pm 0.02$\",\"$91.03 \\pm 0.12$\",\"$89.19 \\pm 0.18$\",\"$94.66 \\pm 0.10$\",\"$89.46 \\pm 0.29$\",\"$40.01 \\pm 0.11$\",\"$35.50 \\pm 0.77$\"\n\"10\",\"$72.34 \\pm 0.03$\",\"$70.90 \\pm 0.04$\",\"$91.54 \\pm 0.10$\",\"$90.86 \\pm 0.16$\",\"$94.83 \\pm 0.01$\",\"$92.74 \\pm 0.02$\",\"$43.83 \\pm 0.09$\",\"$40.84 \\pm 0.42$\""}]}, {"page": 20, "text": "arXiv:2306.10191v3  [cs.LG]  5 Dec 2023\n          Figure 1: (left) Examples from the \u201cpig\u201d priming pool used for our ImageNet experiments after text retrieval\n          and CLIP similarity filtering (process described in Section 3.1.1). (right) Examples of \u2018guinea pigs\u201d which\n          were filtered from the pig pool using CLIP semantic embeddings. We see that it is crucial to use these\n          embeddings to prevent label confusion (e.g.         the \u201cpig\u201d class will pick up \u201cguinea pig\u201d examples without\n          semantic filtering).\n                                 102\n                               Seconds to search\n                                 101\n                                 100\n                                10  1\n                                         0           200           400          600          800          1000\n                                                                Sorted query index\n          Figure 2: (reproduced from appendix) Wall-clock times by query for construction of the initial\n          priming pool. The queries used here are the ones used for the construction of the ImageNet pool. Due to\n          the nature of full text search, rare queries are very fast, with most classes taking less than 1 second to search\n          all of LAION-2b.\n                                                                      1", "md": "arXiv:2306.10191v3 [cs.LG] 5 Dec 2023\n\nFigure 1: (left) Examples from the \u201cpig\u201d priming pool used for our ImageNet experiments after text retrieval and CLIP similarity filtering (process described in Section 3.1.1). (right) Examples of \u2018guinea pigs\u201d which were filtered from the pig pool using CLIP semantic embeddings. We see that it is crucial to use these embeddings to prevent label confusion (e.g. the \u201cpig\u201d class will pick up \u201cguinea pig\u201d examples without semantic filtering).\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n102 & \\text{Seconds to search} & & & & & \\\\\n\\hline\n101 & & & & & & \\\\\n\\hline\n100 & & & & & & \\\\\n\\hline\n10 & 1 & & & & & \\\\\n\\hline\n0 & 200 & 400 & 600 & 800 & 1000 & \\\\\n\\hline\n\\end{array}\n$$\nFigure 2: (reproduced from appendix) Wall-clock times by query for construction of the initial priming pool. The queries used here are the ones used for the construction of the ImageNet pool. Due to the nature of full text search, rare queries are very fast, with most classes taking less than 1 second to search all of LAION-2b.", "images": [{"name": "page-20-1.jpg", "height": 189, "width": 189, "x": 323, "y": 117}, {"name": "page-20-0.jpg", "height": 189, "width": 189, "x": 100, "y": 117}], "items": [{"type": "text", "value": "arXiv:2306.10191v3 [cs.LG] 5 Dec 2023\n\nFigure 1: (left) Examples from the \u201cpig\u201d priming pool used for our ImageNet experiments after text retrieval and CLIP similarity filtering (process described in Section 3.1.1). (right) Examples of \u2018guinea pigs\u201d which were filtered from the pig pool using CLIP semantic embeddings. We see that it is crucial to use these embeddings to prevent label confusion (e.g. the \u201cpig\u201d class will pick up \u201cguinea pig\u201d examples without semantic filtering).\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n102 & \\text{Seconds to search} & & & & & \\\\\n\\hline\n101 & & & & & & \\\\\n\\hline\n100 & & & & & & \\\\\n\\hline\n10 & 1 & & & & & \\\\\n\\hline\n0 & 200 & 400 & 600 & 800 & 1000 & \\\\\n\\hline\n\\end{array}\n$$\nFigure 2: (reproduced from appendix) Wall-clock times by query for construction of the initial priming pool. The queries used here are the ones used for the construction of the ImageNet pool. Due to the nature of full text search, rare queries are very fast, with most classes taking less than 1 second to search all of LAION-2b.", "md": "arXiv:2306.10191v3 [cs.LG] 5 Dec 2023\n\nFigure 1: (left) Examples from the \u201cpig\u201d priming pool used for our ImageNet experiments after text retrieval and CLIP similarity filtering (process described in Section 3.1.1). (right) Examples of \u2018guinea pigs\u201d which were filtered from the pig pool using CLIP semantic embeddings. We see that it is crucial to use these embeddings to prevent label confusion (e.g. the \u201cpig\u201d class will pick up \u201cguinea pig\u201d examples without semantic filtering).\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n102 & \\text{Seconds to search} & & & & & \\\\\n\\hline\n101 & & & & & & \\\\\n\\hline\n100 & & & & & & \\\\\n\\hline\n10 & 1 & & & & & \\\\\n\\hline\n0 & 200 & 400 & 600 & 800 & 1000 & \\\\\n\\hline\n\\end{array}\n$$\nFigure 2: (reproduced from appendix) Wall-clock times by query for construction of the initial priming pool. The queries used here are the ones used for the construction of the ImageNet pool. Due to the nature of full text search, rare queries are very fast, with most classes taking less than 1 second to search all of LAION-2b."}]}], "job_id": "9de5979e-e672-48a8-80c7-8e7d9555f476", "file_path": "./corpus/2306.10191.pdf"}