{"pages": [{"page": 1, "text": "                                                              Multimodal C4:\n                                   An Open, Billion-scale Corpus of Images\n                                                        Interleaved with Text\n                                                           Wanrong Zhu\u2663\u02da Jack Hessel\u2661\u02da\n                                 Anas Awadalla\u2660         Samir Yitzhak Gadre\u2662            Jesse Dodge\u2661        Alex Fang\u2660\narXiv:2304.06939v3  [cs.CV]  28 Oct 2023\n                              Youngjae Yu:         Ludwig Schmidt\u2660\u2661;           William Yang Wang\u2663            Yejin Choi\u2660\u2661\n                              \u2663University of California, Santa Barbara         \u2661Allen Institute for Artificial Intelligence\n                                     \u2660Paul G. Allen School of Computer Science, University of Washington\n                                              \u2662Columbia University         :Yonsei University       ;LAION\n                                                       https://github.com/allenai/mmc4\n                                                                        Abstract\n                                In-context vision and language models like Flamingo [2] support arbitrarily in-\n                                terleaved sequences of images and text as input. This format not only enables\n                                few-shot learning via interleaving independent supervised (image, text) examples,\n                                but also, more complex prompts involving interaction between images, e.g., \u201cWhat\n                                do image A and image B have in common?\u201d To support this interface, pretraining\n                                occurs over web corpora that similarly contain interleaved images+text. To date,\n                                however, large-scale data of this form have not been publicly available.\n                               We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4\n                                corpus2 with images interleaved. We use a linear assignment algorithm to place\n                                images into longer bodies of text using CLIP features [24], a process that we\n                                show outperforms alternatives. mmc4 spans everyday topics like cooking, travel,\n                                technology, etc. A manual inspection of a random sample of documents shows that\n                                a vast majority (88%) of images are topically relevant, and that linear assignment\n                                frequently selects individual sentences specifically well-aligned with each image\n                                (80%). After filtering NSFW images, ads, etc., the resulting mmc4 corpus consists\n                                of 101.2M documents with 571M images interleaved in 43B English tokens.\n                     1     Introduction\n                     In-context learning [7] enables sequence models to adapt to new tasks without any parameter updates.\n                     By interleaving a few supervised examples in a prompt, few-shot learning can be formatted as a\n                     next-token prediction task, i.e., x1, y1, x2, y2, . . . , xn is input to predict \u02c6\n                                                                                                      yn. Some image+text models\n                     also support in-context learning via interleaving of images/text jointly. Prior experiments [2] suggest\n                     that performant multimodal in-context learning is dependent upon pretraining on similarly interleaved\n                     sequences of images and text (rather than single image/caption pairs). However, such a large-scale\n                     corpus has not been made publicly available.\n                     To address this, we introduce Multimodal C4 (mmc4), a public, billion-scale image-text dataset\n                     consisting of interleaved image/text sequences.3 mmc4 is constructed from public webpages contained\n                     in the cleaned English c4 corpus. In addition to standard preprocessing steps like deduplication,\n                        \u02daequal contribution; work partly conducted while Wanrong Zhu was an intern at AI2.\n                         2https://www.tensorflow.org/datasets/catalog/c4\n                         3mmc4\u2019s datasheet [15] is available here.\n                     37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "md": "# Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text\n\n# Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text\n\nWanrong Zhu\u2663\u02da Jack Hessel\u2661\u02da Anas Awadalla\u2660 Samir Yitzhak Gadre\u2662 Jesse Dodge\u2661 Alex Fang\u2660\n\narXiv:2304.06939v3 [cs.CV] 28 Oct 2023\n\nYoungjae Yu: Ludwig Schmidt\u2660\u2661; William Yang Wang\u2663 Yejin Choi\u2660\u2661\n\n\u2663University of California, Santa Barbara \u2661Allen Institute for Artificial Intelligence\n\n\u2660Paul G. Allen School of Computer Science, University of Washington \u2662Columbia University :Yonsei University ;LAION\n\nhttps://github.com/allenai/mmc4\n\n## Abstract\n\nIn-context vision and language models like Flamingo [2] support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., \"What do image A and image B have in common?\" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.\n\nWe release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus2 with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features [24], a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80%). After filtering NSFW images, ads, etc., the resulting mmc4 corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens.\n\n## Introduction\n\nIn-context learning [7] enables sequence models to adapt to new tasks without any parameter updates. By interleaving a few supervised examples in a prompt, few-shot learning can be formatted as a next-token prediction task, i.e., $$x_1, y_1, x_2, y_2, . . . , x_n$$ is input to predict $$\\hat{y}_n$$. Some image+text models also support in-context learning via interleaving of images/text jointly. Prior experiments [2] suggest that performant multimodal in-context learning is dependent upon pretraining on similarly interleaved sequences of images and text (rather than single image/caption pairs). However, such a large-scale corpus has not been made publicly available.\n\nTo address this, we introduce Multimodal C4 (mmc4), a public, billion-scale image-text dataset consisting of interleaved image/text sequences. mmc4 is constructed from public webpages contained in the cleaned English c4 corpus. In addition to standard preprocessing steps like deduplication,\n\n\u02daequal contribution; work partly conducted while Wanrong Zhu was an intern at AI2.\n\n2https://www.tensorflow.org/datasets/catalog/c4\n\n3mmc4\u2019s datasheet [15] is available here.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text", "md": "# Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text"}, {"type": "heading", "lvl": 1, "value": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text", "md": "# Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text"}, {"type": "text", "value": "Wanrong Zhu\u2663\u02da Jack Hessel\u2661\u02da Anas Awadalla\u2660 Samir Yitzhak Gadre\u2662 Jesse Dodge\u2661 Alex Fang\u2660\n\narXiv:2304.06939v3 [cs.CV] 28 Oct 2023\n\nYoungjae Yu: Ludwig Schmidt\u2660\u2661; William Yang Wang\u2663 Yejin Choi\u2660\u2661\n\n\u2663University of California, Santa Barbara \u2661Allen Institute for Artificial Intelligence\n\n\u2660Paul G. Allen School of Computer Science, University of Washington \u2662Columbia University :Yonsei University ;LAION\n\nhttps://github.com/allenai/mmc4", "md": "Wanrong Zhu\u2663\u02da Jack Hessel\u2661\u02da Anas Awadalla\u2660 Samir Yitzhak Gadre\u2662 Jesse Dodge\u2661 Alex Fang\u2660\n\narXiv:2304.06939v3 [cs.CV] 28 Oct 2023\n\nYoungjae Yu: Ludwig Schmidt\u2660\u2661; William Yang Wang\u2663 Yejin Choi\u2660\u2661\n\n\u2663University of California, Santa Barbara \u2661Allen Institute for Artificial Intelligence\n\n\u2660Paul G. Allen School of Computer Science, University of Washington \u2662Columbia University :Yonsei University ;LAION\n\nhttps://github.com/allenai/mmc4"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "In-context vision and language models like Flamingo [2] support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., \"What do image A and image B have in common?\" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.\n\nWe release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus2 with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features [24], a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80%). After filtering NSFW images, ads, etc., the resulting mmc4 corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens.", "md": "In-context vision and language models like Flamingo [2] support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., \"What do image A and image B have in common?\" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.\n\nWe release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus2 with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features [24], a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80%). After filtering NSFW images, ads, etc., the resulting mmc4 corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "In-context learning [7] enables sequence models to adapt to new tasks without any parameter updates. By interleaving a few supervised examples in a prompt, few-shot learning can be formatted as a next-token prediction task, i.e., $$x_1, y_1, x_2, y_2, . . . , x_n$$ is input to predict $$\\hat{y}_n$$. Some image+text models also support in-context learning via interleaving of images/text jointly. Prior experiments [2] suggest that performant multimodal in-context learning is dependent upon pretraining on similarly interleaved sequences of images and text (rather than single image/caption pairs). However, such a large-scale corpus has not been made publicly available.\n\nTo address this, we introduce Multimodal C4 (mmc4), a public, billion-scale image-text dataset consisting of interleaved image/text sequences. mmc4 is constructed from public webpages contained in the cleaned English c4 corpus. In addition to standard preprocessing steps like deduplication,\n\n\u02daequal contribution; work partly conducted while Wanrong Zhu was an intern at AI2.\n\n2https://www.tensorflow.org/datasets/catalog/c4\n\n3mmc4\u2019s datasheet [15] is available here.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "md": "In-context learning [7] enables sequence models to adapt to new tasks without any parameter updates. By interleaving a few supervised examples in a prompt, few-shot learning can be formatted as a next-token prediction task, i.e., $$x_1, y_1, x_2, y_2, . . . , x_n$$ is input to predict $$\\hat{y}_n$$. Some image+text models also support in-context learning via interleaving of images/text jointly. Prior experiments [2] suggest that performant multimodal in-context learning is dependent upon pretraining on similarly interleaved sequences of images and text (rather than single image/caption pairs). However, such a large-scale corpus has not been made publicly available.\n\nTo address this, we introduce Multimodal C4 (mmc4), a public, billion-scale image-text dataset consisting of interleaved image/text sequences. mmc4 is constructed from public webpages contained in the cleaned English c4 corpus. In addition to standard preprocessing steps like deduplication,\n\n\u02daequal contribution; work partly conducted while Wanrong Zhu was an intern at AI2.\n\n2https://www.tensorflow.org/datasets/catalog/c4\n\n3mmc4\u2019s datasheet [15] is available here.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks."}]}, {"page": 2, "text": "Table 1: Comparison of mmc4 with other interleaved image/text pretraining corpora. In addition to the\nfull version of the dataset, we also release: 1) fewer-faces subsets, which aim to remove all depicted\nhuman faces; and 2) \u201ccore\u201d subsets, result from more stringent filtering.\n                                                                # images       # docs      # tokens      Public?\n                    M3W (Flamingo) [2]                            185M          43M             -           \u02c6\n           Interleaved training data for CM3 [1]                   25M          61M          223B           \u02c6\n     Interleaved training data for KOSMOS-1 [17]                \u010f 355M          71M             -           \u02c6\n                   Multimodal C4 (mmc4)                           571M        101.2M          43B           \u2713\n         Multimodal C4 fewer-faces (mmc4-ff)                      375M         77.7M          33B           \u2713\n                  mmc4 core (mmc4-core)                          29.9M          7.3M          2.4B          \u2713\n         mmc4 core fewer-faces (mmc4-core-ff)                    22.4M          5.5M          1.8B          \u2713\nNSFW removal, etc., we place images into sequences of sentences by treating each document as an\ninstance of a bipartite linear assignment problem, with images being assigned to sentences (under the\nconstraint that each sentence is assigned at most one image). We show that applying CLIP ViT-L/14\n[24] to estimate bipartite weights in a zero-shot fashion results in state-of-the-art performance on\nintra-document alignment benchmarks, and then apply this process to 100M+ documents to construct\nmmc4. Apart from the full corpus, we have created two additional subsets: mmc4-ff, which removes\nimages with detected faces, and mmc4-core, a more strictly filtered and downsized version of the\ncorpus, serving as an initial corpus for developers.\nWe explore mmc4, showing that: 1) the text and images in the corpus span expected everyday topics\nlike cooking and travel; 2) filters like NSFW/ad removal work with high accuracy; and 3) the resulting\nimages are relevant to the associated documents, and often, appropriately aligned to the most-relevant\nindividual sentence. We conclude by discussing initial use-cases of mmc4, including OpenFlamingo\n[3],4 an open source version of Flamingo [2]. Initial ablations show that training on the sequences of\nmmc4 enables few-shot, in-context adaptation to image captioning datasets.\n2    Related Dataset Work\nMost million/billion-scale, public multimodal pretraining datasets consist of images paired with\ntheir literal descriptions, e.g., LAION-2B [26], CC-12M [8], YFCC100M [32]. However, literal\ndescription is only one of many ways images can relate to text on the web [21]. mmc4 aims to capture\na broader range of these relationship types. Some web datasets collect multiple images for one text\nsnippet (e.g., the Google Local Restaurant Reviews Dataset [36] with 4.4M images), or situate images\nin longer bodies of text (e.g., the Wikipedia-based Image Text Dataset [30] with 11.5M images), but\ndo not directly cover multi-image/multi-sentence interleaving. Table 1 provides summary statistics of\nother large-scale interleaved pretraining datasets. mmc4 contains more images than prior non-public\ndatasets. [5] highlight risks associated with web-scale multimodal data.\nIn addition to the detailed curation steps described in \u00a7 3 and the considerations for data release\noutlined in \u00a7 3.1, we are hopeful that the availability of mmc4 can facilitate a more transparent and\ncritical examination of interleaved corpora compared to previous privately held training sets. Models\ntrained on mmc4 inherit its risks; we selected the widely-adopted c4 corpus as a starting point in\npart because there are existing auditing efforts on the text-only corpus, see \u00a7 3 and [23] for more\ndiscussion of transparency.\n3    Data Curation Process\nInitial data collection.       Multimodal C4 is an expansion of the text-only c4 dataset [25], which was\ncreated by taking the April 2019 snapshot from Common Crawl5 and applying several filters with\nthe intention of retaining high-quality, natural English text. Each document in c4 consists of the text\n    4https://github.com/mlfoundations/open_flamingo\n    5https://commoncrawl.org/\n                                                           2", "md": "# Comparison of mmc4 with other interleaved image/text pretraining corpora\n\n## Table 1: Comparison of mmc4 with other interleaved image/text pretraining corpora\n\n| |# images|# docs|# tokens|Public?|\n|---|---|---|---|---|\n|M3W (Flamingo) [2]|185M|43M|-|\u02c6|\n|Interleaved training data for CM3 [1]|25M|61M|223B|\u02c6|\n|Interleaved training data for KOSMOS-1 [17]|\u010f 355M|71M|-|\u02c6|\n|Multimodal C4 (mmc4)|571M|101.2M|43B|\u2713|\n|Multimodal C4 fewer-faces (mmc4-ff)|375M|77.7M|33B|\u2713|\n|mmc4 core (mmc4-core)|29.9M|7.3M|2.4B|\u2713|\n|mmc4 core fewer-faces (mmc4-core-ff)|22.4M|5.5M|1.8B|\u2713|\n\nNSFW removal, etc., we place images into sequences of sentences by treating each document as an instance of a bipartite linear assignment problem, with images being assigned to sentences (under the constraint that each sentence is assigned at most one image). We show that applying CLIP ViT-L/14 [24] to estimate bipartite weights in a zero-shot fashion results in state-of-the-art performance on intra-document alignment benchmarks, and then apply this process to 100M+ documents to construct mmc4. Apart from the full corpus, we have created two additional subsets: mmc4-ff, which removes images with detected faces, and mmc4-core, a more strictly filtered and downsized version of the corpus, serving as an initial corpus for developers.\n\nWe explore mmc4, showing that: 1) the text and images in the corpus span expected everyday topics like cooking and travel; 2) filters like NSFW/ad removal work with high accuracy; and 3) the resulting images are relevant to the associated documents, and often, appropriately aligned to the most-relevant individual sentence. We conclude by discussing initial use-cases of mmc4, including OpenFlamingo [3],4 an open source version of Flamingo [2]. Initial ablations show that training on the sequences of mmc4 enables few-shot, in-context adaptation to image captioning datasets.\n\n## Related Dataset Work\n\nMost million/billion-scale, public multimodal pretraining datasets consist of images paired with their literal descriptions, e.g., LAION-2B [26], CC-12M [8], YFCC100M [32]. However, literal description is only one of many ways images can relate to text on the web [21]. mmc4 aims to capture a broader range of these relationship types. Some web datasets collect multiple images for one text snippet (e.g., the Google Local Restaurant Reviews Dataset [36] with 4.4M images), or situate images in longer bodies of text (e.g., the Wikipedia-based Image Text Dataset [30] with 11.5M images), but do not directly cover multi-image/multi-sentence interleaving. Table 1 provides summary statistics of other large-scale interleaved pretraining datasets. mmc4 contains more images than prior non-public datasets. [5] highlight risks associated with web-scale multimodal data.\n\nIn addition to the detailed curation steps described in \u00a7 3 and the considerations for data release outlined in \u00a7 3.1, we are hopeful that the availability of mmc4 can facilitate a more transparent and critical examination of interleaved corpora compared to previous privately held training sets. Models trained on mmc4 inherit its risks; we selected the widely-adopted c4 corpus as a starting point in part because there are existing auditing efforts on the text-only corpus, see \u00a7 3 and [23] for more discussion of transparency.\n\n## Data Curation Process\n\nInitial data collection. Multimodal C4 is an expansion of the text-only c4 dataset [25], which was created by taking the April 2019 snapshot from Common Crawl5 and applying several filters with the intention of retaining high-quality, natural English text. Each document in c4 consists of the text\n\nOpenFlamingo\n\nCommon Crawl", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Comparison of mmc4 with other interleaved image/text pretraining corpora", "md": "# Comparison of mmc4 with other interleaved image/text pretraining corpora"}, {"type": "heading", "lvl": 2, "value": "Table 1: Comparison of mmc4 with other interleaved image/text pretraining corpora", "md": "## Table 1: Comparison of mmc4 with other interleaved image/text pretraining corpora"}, {"type": "table", "rows": [["", "# images", "# docs", "# tokens", "Public?"], ["M3W (Flamingo) [2]", "185M", "43M", "-", "\u02c6"], ["Interleaved training data for CM3 [1]", "25M", "61M", "223B", "\u02c6"], ["Interleaved training data for KOSMOS-1 [17]", "\u010f 355M", "71M", "-", "\u02c6"], ["Multimodal C4 (mmc4)", "571M", "101.2M", "43B", "\u2713"], ["Multimodal C4 fewer-faces (mmc4-ff)", "375M", "77.7M", "33B", "\u2713"], ["mmc4 core (mmc4-core)", "29.9M", "7.3M", "2.4B", "\u2713"], ["mmc4 core fewer-faces (mmc4-core-ff)", "22.4M", "5.5M", "1.8B", "\u2713"]], "md": "| |# images|# docs|# tokens|Public?|\n|---|---|---|---|---|\n|M3W (Flamingo) [2]|185M|43M|-|\u02c6|\n|Interleaved training data for CM3 [1]|25M|61M|223B|\u02c6|\n|Interleaved training data for KOSMOS-1 [17]|\u010f 355M|71M|-|\u02c6|\n|Multimodal C4 (mmc4)|571M|101.2M|43B|\u2713|\n|Multimodal C4 fewer-faces (mmc4-ff)|375M|77.7M|33B|\u2713|\n|mmc4 core (mmc4-core)|29.9M|7.3M|2.4B|\u2713|\n|mmc4 core fewer-faces (mmc4-core-ff)|22.4M|5.5M|1.8B|\u2713|", "isPerfectTable": true, "csv": "\"\",\"# images\",\"# docs\",\"# tokens\",\"Public?\"\n\"M3W (Flamingo) [2]\",\"185M\",\"43M\",\"-\",\"\u02c6\"\n\"Interleaved training data for CM3 [1]\",\"25M\",\"61M\",\"223B\",\"\u02c6\"\n\"Interleaved training data for KOSMOS-1 [17]\",\"\u010f 355M\",\"71M\",\"-\",\"\u02c6\"\n\"Multimodal C4 (mmc4)\",\"571M\",\"101.2M\",\"43B\",\"\u2713\"\n\"Multimodal C4 fewer-faces (mmc4-ff)\",\"375M\",\"77.7M\",\"33B\",\"\u2713\"\n\"mmc4 core (mmc4-core)\",\"29.9M\",\"7.3M\",\"2.4B\",\"\u2713\"\n\"mmc4 core fewer-faces (mmc4-core-ff)\",\"22.4M\",\"5.5M\",\"1.8B\",\"\u2713\""}, {"type": "text", "value": "NSFW removal, etc., we place images into sequences of sentences by treating each document as an instance of a bipartite linear assignment problem, with images being assigned to sentences (under the constraint that each sentence is assigned at most one image). We show that applying CLIP ViT-L/14 [24] to estimate bipartite weights in a zero-shot fashion results in state-of-the-art performance on intra-document alignment benchmarks, and then apply this process to 100M+ documents to construct mmc4. Apart from the full corpus, we have created two additional subsets: mmc4-ff, which removes images with detected faces, and mmc4-core, a more strictly filtered and downsized version of the corpus, serving as an initial corpus for developers.\n\nWe explore mmc4, showing that: 1) the text and images in the corpus span expected everyday topics like cooking and travel; 2) filters like NSFW/ad removal work with high accuracy; and 3) the resulting images are relevant to the associated documents, and often, appropriately aligned to the most-relevant individual sentence. We conclude by discussing initial use-cases of mmc4, including OpenFlamingo [3],4 an open source version of Flamingo [2]. Initial ablations show that training on the sequences of mmc4 enables few-shot, in-context adaptation to image captioning datasets.", "md": "NSFW removal, etc., we place images into sequences of sentences by treating each document as an instance of a bipartite linear assignment problem, with images being assigned to sentences (under the constraint that each sentence is assigned at most one image). We show that applying CLIP ViT-L/14 [24] to estimate bipartite weights in a zero-shot fashion results in state-of-the-art performance on intra-document alignment benchmarks, and then apply this process to 100M+ documents to construct mmc4. Apart from the full corpus, we have created two additional subsets: mmc4-ff, which removes images with detected faces, and mmc4-core, a more strictly filtered and downsized version of the corpus, serving as an initial corpus for developers.\n\nWe explore mmc4, showing that: 1) the text and images in the corpus span expected everyday topics like cooking and travel; 2) filters like NSFW/ad removal work with high accuracy; and 3) the resulting images are relevant to the associated documents, and often, appropriately aligned to the most-relevant individual sentence. We conclude by discussing initial use-cases of mmc4, including OpenFlamingo [3],4 an open source version of Flamingo [2]. Initial ablations show that training on the sequences of mmc4 enables few-shot, in-context adaptation to image captioning datasets."}, {"type": "heading", "lvl": 2, "value": "Related Dataset Work", "md": "## Related Dataset Work"}, {"type": "text", "value": "Most million/billion-scale, public multimodal pretraining datasets consist of images paired with their literal descriptions, e.g., LAION-2B [26], CC-12M [8], YFCC100M [32]. However, literal description is only one of many ways images can relate to text on the web [21]. mmc4 aims to capture a broader range of these relationship types. Some web datasets collect multiple images for one text snippet (e.g., the Google Local Restaurant Reviews Dataset [36] with 4.4M images), or situate images in longer bodies of text (e.g., the Wikipedia-based Image Text Dataset [30] with 11.5M images), but do not directly cover multi-image/multi-sentence interleaving. Table 1 provides summary statistics of other large-scale interleaved pretraining datasets. mmc4 contains more images than prior non-public datasets. [5] highlight risks associated with web-scale multimodal data.\n\nIn addition to the detailed curation steps described in \u00a7 3 and the considerations for data release outlined in \u00a7 3.1, we are hopeful that the availability of mmc4 can facilitate a more transparent and critical examination of interleaved corpora compared to previous privately held training sets. Models trained on mmc4 inherit its risks; we selected the widely-adopted c4 corpus as a starting point in part because there are existing auditing efforts on the text-only corpus, see \u00a7 3 and [23] for more discussion of transparency.", "md": "Most million/billion-scale, public multimodal pretraining datasets consist of images paired with their literal descriptions, e.g., LAION-2B [26], CC-12M [8], YFCC100M [32]. However, literal description is only one of many ways images can relate to text on the web [21]. mmc4 aims to capture a broader range of these relationship types. Some web datasets collect multiple images for one text snippet (e.g., the Google Local Restaurant Reviews Dataset [36] with 4.4M images), or situate images in longer bodies of text (e.g., the Wikipedia-based Image Text Dataset [30] with 11.5M images), but do not directly cover multi-image/multi-sentence interleaving. Table 1 provides summary statistics of other large-scale interleaved pretraining datasets. mmc4 contains more images than prior non-public datasets. [5] highlight risks associated with web-scale multimodal data.\n\nIn addition to the detailed curation steps described in \u00a7 3 and the considerations for data release outlined in \u00a7 3.1, we are hopeful that the availability of mmc4 can facilitate a more transparent and critical examination of interleaved corpora compared to previous privately held training sets. Models trained on mmc4 inherit its risks; we selected the widely-adopted c4 corpus as a starting point in part because there are existing auditing efforts on the text-only corpus, see \u00a7 3 and [23] for more discussion of transparency."}, {"type": "heading", "lvl": 2, "value": "Data Curation Process", "md": "## Data Curation Process"}, {"type": "text", "value": "Initial data collection. Multimodal C4 is an expansion of the text-only c4 dataset [25], which was created by taking the April 2019 snapshot from Common Crawl5 and applying several filters with the intention of retaining high-quality, natural English text. Each document in c4 consists of the text\n\nOpenFlamingo\n\nCommon Crawl", "md": "Initial data collection. Multimodal C4 is an expansion of the text-only c4 dataset [25], which was created by taking the April 2019 snapshot from Common Crawl5 and applying several filters with the intention of retaining high-quality, natural English text. Each document in c4 consists of the text\n\nOpenFlamingo\n\nCommon Crawl"}]}, {"page": 3, "text": "               Interiors (3.1%):                                                                                                                                                                   Technology (2.8%):\n               room space house kitchen floor living pool building large                                                                                                                           water energy system power air temperature heat systems gas\n               Self Care (1.9%):                                                                                                                                                                   Travel (4.0%):\n               skin hair oil natural organic wine plant products plants water                                                                                                                      city hotel park visit travel trip tour enjoy beach town stay local\n               Cooking (3.3%):                                                                                                                                                                     Music (2.5%):\n               food add recipe minutes chocolate cream delicious chicken                                                                                                                           music band album song sound songs dance show live musical\nFigure 1: A T-SNE [34] projection of LDA [6] topic clusters from a random sample of 22K documents\nfrom mmc4; mmc4 spans a variety of everyday topics, e.g., cooking, technology travel, etc. For 6\nselected topics, we also show a sample of most-central images to the topic according to CLIP ViT-\nL/14 [24].\nscraped from one URL. The full c4 dataset has 365M documents and 156B tokens, covering many\ndomains [12]; it was first used to train T5 [25]. We built the mmc4 dataset on top of c4 because: 1) c4\nis a web-scale dataset widely adopted as a pre-training corpus [25, 29, 9, 33, 31]; 2) c4 is constructed\nfrom web pages, which frequently contain multimedia content like images, which makes it a suitable\nbasis for extending to a multimodal sequence version; and 3) c4-en,6 the specific underlying subset\nfrom which we construct mmc4 has already been processed with several data-cleaning steps (including\nEnglish-language identification by langdetect7 with at least 0.99 confidence; text deduplication\nremoving duplicate three-sentence spans + placeholder text like \u201clorem ipsum\u201d; and removal of any\ndocument containing any word on the \u201cList of Dirty, Naughty, Obscene or Otherwise Bad Words\u201d).8\nSee [25] for more information about the text-only c4. Importantly, by building on the popular\ntext-only c4, prior text-only documentation efforts [12] can provide insight about potential biases\nand risks that could arise when training on our multimodal extension. We use the NLTK [4] sentence\ntokenizer to chunk each c4 document into a list of sentences.\nGathering images.                                                              We first retrieve the original webpages for each document in the c4-en dataset\nfrom the Common Crawl version 2019-18, which is the default version for c4. Next, we extract\nthe URLs for downloadable images from the raw WAT files. We restrict the image extension to\neither png/jpeg/jpg, and exclude image URLs that contain the following tokens: tlogo, button,\nicon, plugin, widgetu. We attempt to download from these URLs, and resize images to a\nmaximum dimension of 800px. We eliminate any c4 documents that do not contain valid, download-\nable images at the time of collection (mid-to-late 2022). The starting point after this step is 115M\ndocuments and 1.37B images.\nDe-duplication+small resolution.                                                                                                           We next run duplicate image detection using opennota\u2019s\nfindimagedupes9 which uses phash10 to identify visually similar images.11 We keep only one copy\nof an image if multiple versions are detected within the same document. We also remove images\nwith more than 10 duplicates in a sample of 60K images. We discard images with a width or height\nsmaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images\nwith an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. In\n            6  https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config\n            7  https://pypi.org/project/langdetect/\n            8  https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n         10 9  https://gitlab.com/opennota/findimagedupes\n         11    http://www.phash.org/\n              We use a more aggressive de-duplication threshold of 5 compared to the default library setting of 0; this\nremoves roughly 10M additional images. While some duplicates survive this process, we qualitatively found a\nthreshold of 5 to be an appropriate balance of false positives/negatives.\n                                                                                                                                                                                 3", "md": "Interiors (3.1%):\n\nroom space house kitchen floor living pool building large\n\nSelf Care (1.9%):\n\nskin hair oil natural organic wine plant products plants water\n\nCooking (3.3%):\n\nfood add recipe minutes chocolate cream delicious chicken\n\nTechnology (2.8%):\n\nwater energy system power air temperature heat systems gas\n\nTravel (4.0%):\n\ncity hotel park visit travel trip tour enjoy beach town stay local\n\nMusic (2.5%):\n\nmusic band album song sound songs dance show live musical\n\nFigure 1: A T-SNE $$[34]$$ projection of LDA $$[6]$$ topic clusters from a random sample of 22K documents from mmc4; mmc4 spans a variety of everyday topics, e.g., cooking, technology travel, etc. For 6 selected topics, we also show a sample of most-central images to the topic according to CLIP ViT-L/14 $$[24]$$.\n\nscraped from one URL. The full c4 dataset has 365M documents and 156B tokens, covering many domains $$[12]$$; it was first used to train T5 $$[25]$$. We built the mmc4 dataset on top of c4 because: 1) c4 is a web-scale dataset widely adopted as a pre-training corpus $$[25, 29, 9, 33, 31]$$; 2) c4 is constructed from web pages, which frequently contain multimedia content like images, which makes it a suitable basis for extending to a multimodal sequence version; and 3) c4-en,6 the specific underlying subset from which we construct mmc4 has already been processed with several data-cleaning steps (including English-language identification by langdetect7 with at least 0.99 confidence; text deduplication removing duplicate three-sentence spans + placeholder text like \u201clorem ipsum\u201d; and removal of any document containing any word on the \u201cList of Dirty, Naughty, Obscene or Otherwise Bad Words\u201d) $$[8]$$. See $$[25]$$ for more information about the text-only c4. Importantly, by building on the popular text-only c4, prior text-only documentation efforts $$[12]$$ can provide insight about potential biases and risks that could arise when training on our multimodal extension. We use the NLTK $$[4]$$ sentence tokenizer to chunk each c4 document into a list of sentences.\n\nGathering images. We first retrieve the original webpages for each document in the c4-en dataset from the Common Crawl version 2019-18, which is the default version for c4. Next, we extract the URLs for downloadable images from the raw WAT files. We restrict the image extension to either png/jpeg/jpg, and exclude image URLs that contain the following tokens: tlogo, button, icon, plugin, widgetu. We attempt to download from these URLs, and resize images to a maximum dimension of 800px. We eliminate any c4 documents that do not contain valid, downloadable images at the time of collection (mid-to-late 2022). The starting point after this step is 115M documents and 1.37B images.\n\nDe-duplication+small resolution. We next run duplicate image detection using opennota\u2019s findimagedupes $$[9]$$ which uses phash $$[10]$$ to identify visually similar images $$[11]$$. We keep only one copy of an image if multiple versions are detected within the same document. We also remove images with more than 10 duplicates in a sample of 60K images. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. In\n\n6 https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config\n\n7 https://pypi.org/project/langdetect/\n\n8 https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n\n10 9 https://gitlab.com/opennota/findimagedupes\n\n11 http://www.phash.org/\n\nWe use a more aggressive de-duplication threshold of 5 compared to the default library setting of 0; this removes roughly 10M additional images. While some duplicates survive this process, we qualitatively found a threshold of 5 to be an appropriate balance of false positives/negatives.", "images": [{"name": "page-3-0.jpg", "height": 142, "width": 377, "x": 117, "y": 72}], "items": [{"type": "text", "value": "Interiors (3.1%):\n\nroom space house kitchen floor living pool building large\n\nSelf Care (1.9%):\n\nskin hair oil natural organic wine plant products plants water\n\nCooking (3.3%):\n\nfood add recipe minutes chocolate cream delicious chicken\n\nTechnology (2.8%):\n\nwater energy system power air temperature heat systems gas\n\nTravel (4.0%):\n\ncity hotel park visit travel trip tour enjoy beach town stay local\n\nMusic (2.5%):\n\nmusic band album song sound songs dance show live musical\n\nFigure 1: A T-SNE $$[34]$$ projection of LDA $$[6]$$ topic clusters from a random sample of 22K documents from mmc4; mmc4 spans a variety of everyday topics, e.g., cooking, technology travel, etc. For 6 selected topics, we also show a sample of most-central images to the topic according to CLIP ViT-L/14 $$[24]$$.\n\nscraped from one URL. The full c4 dataset has 365M documents and 156B tokens, covering many domains $$[12]$$; it was first used to train T5 $$[25]$$. We built the mmc4 dataset on top of c4 because: 1) c4 is a web-scale dataset widely adopted as a pre-training corpus $$[25, 29, 9, 33, 31]$$; 2) c4 is constructed from web pages, which frequently contain multimedia content like images, which makes it a suitable basis for extending to a multimodal sequence version; and 3) c4-en,6 the specific underlying subset from which we construct mmc4 has already been processed with several data-cleaning steps (including English-language identification by langdetect7 with at least 0.99 confidence; text deduplication removing duplicate three-sentence spans + placeholder text like \u201clorem ipsum\u201d; and removal of any document containing any word on the \u201cList of Dirty, Naughty, Obscene or Otherwise Bad Words\u201d) $$[8]$$. See $$[25]$$ for more information about the text-only c4. Importantly, by building on the popular text-only c4, prior text-only documentation efforts $$[12]$$ can provide insight about potential biases and risks that could arise when training on our multimodal extension. We use the NLTK $$[4]$$ sentence tokenizer to chunk each c4 document into a list of sentences.\n\nGathering images. We first retrieve the original webpages for each document in the c4-en dataset from the Common Crawl version 2019-18, which is the default version for c4. Next, we extract the URLs for downloadable images from the raw WAT files. We restrict the image extension to either png/jpeg/jpg, and exclude image URLs that contain the following tokens: tlogo, button, icon, plugin, widgetu. We attempt to download from these URLs, and resize images to a maximum dimension of 800px. We eliminate any c4 documents that do not contain valid, downloadable images at the time of collection (mid-to-late 2022). The starting point after this step is 115M documents and 1.37B images.\n\nDe-duplication+small resolution. We next run duplicate image detection using opennota\u2019s findimagedupes $$[9]$$ which uses phash $$[10]$$ to identify visually similar images $$[11]$$. We keep only one copy of an image if multiple versions are detected within the same document. We also remove images with more than 10 duplicates in a sample of 60K images. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. In\n\n6 https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config\n\n7 https://pypi.org/project/langdetect/\n\n8 https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n\n10 9 https://gitlab.com/opennota/findimagedupes\n\n11 http://www.phash.org/\n\nWe use a more aggressive de-duplication threshold of 5 compared to the default library setting of 0; this removes roughly 10M additional images. While some duplicates survive this process, we qualitatively found a threshold of 5 to be an appropriate balance of false positives/negatives.", "md": "Interiors (3.1%):\n\nroom space house kitchen floor living pool building large\n\nSelf Care (1.9%):\n\nskin hair oil natural organic wine plant products plants water\n\nCooking (3.3%):\n\nfood add recipe minutes chocolate cream delicious chicken\n\nTechnology (2.8%):\n\nwater energy system power air temperature heat systems gas\n\nTravel (4.0%):\n\ncity hotel park visit travel trip tour enjoy beach town stay local\n\nMusic (2.5%):\n\nmusic band album song sound songs dance show live musical\n\nFigure 1: A T-SNE $$[34]$$ projection of LDA $$[6]$$ topic clusters from a random sample of 22K documents from mmc4; mmc4 spans a variety of everyday topics, e.g., cooking, technology travel, etc. For 6 selected topics, we also show a sample of most-central images to the topic according to CLIP ViT-L/14 $$[24]$$.\n\nscraped from one URL. The full c4 dataset has 365M documents and 156B tokens, covering many domains $$[12]$$; it was first used to train T5 $$[25]$$. We built the mmc4 dataset on top of c4 because: 1) c4 is a web-scale dataset widely adopted as a pre-training corpus $$[25, 29, 9, 33, 31]$$; 2) c4 is constructed from web pages, which frequently contain multimedia content like images, which makes it a suitable basis for extending to a multimodal sequence version; and 3) c4-en,6 the specific underlying subset from which we construct mmc4 has already been processed with several data-cleaning steps (including English-language identification by langdetect7 with at least 0.99 confidence; text deduplication removing duplicate three-sentence spans + placeholder text like \u201clorem ipsum\u201d; and removal of any document containing any word on the \u201cList of Dirty, Naughty, Obscene or Otherwise Bad Words\u201d) $$[8]$$. See $$[25]$$ for more information about the text-only c4. Importantly, by building on the popular text-only c4, prior text-only documentation efforts $$[12]$$ can provide insight about potential biases and risks that could arise when training on our multimodal extension. We use the NLTK $$[4]$$ sentence tokenizer to chunk each c4 document into a list of sentences.\n\nGathering images. We first retrieve the original webpages for each document in the c4-en dataset from the Common Crawl version 2019-18, which is the default version for c4. Next, we extract the URLs for downloadable images from the raw WAT files. We restrict the image extension to either png/jpeg/jpg, and exclude image URLs that contain the following tokens: tlogo, button, icon, plugin, widgetu. We attempt to download from these URLs, and resize images to a maximum dimension of 800px. We eliminate any c4 documents that do not contain valid, downloadable images at the time of collection (mid-to-late 2022). The starting point after this step is 115M documents and 1.37B images.\n\nDe-duplication+small resolution. We next run duplicate image detection using opennota\u2019s findimagedupes $$[9]$$ which uses phash $$[10]$$ to identify visually similar images $$[11]$$. We keep only one copy of an image if multiple versions are detected within the same document. We also remove images with more than 10 duplicates in a sample of 60K images. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. In\n\n6 https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config\n\n7 https://pypi.org/project/langdetect/\n\n8 https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n\n10 9 https://gitlab.com/opennota/findimagedupes\n\n11 http://www.phash.org/\n\nWe use a more aggressive de-duplication threshold of 5 compared to the default library setting of 0; this removes roughly 10M additional images. While some duplicates survive this process, we qualitatively found a threshold of 5 to be an appropriate balance of false positives/negatives."}]}, {"page": 4, "text": "Table 2: Performance on single document image-text benchmarks from [16] (higher=better in all\ncases). Applying CLIP ViT-L/14 in a zero-shot fashion [24] produces better within-document\nalignments compared to prior methods which rely on fine-tuning.\n                                  MSCOCO         Story-DII    Story-SIS     DII-Stress      RQA           DIY\n                                   AUC p@1       AUC p@1       AUC p@1      AUC p@1       AUC p@1       AUC p@1\n  Random                          49.7 5.0       49.4 19.5    50.0 19.4     50.0 2.0      49.4 17.8     49.8 6.3\n  Hessel et al. (2019) [16]       98.7 91.0      82.6 70.5    68.5 50.5     95.3 65.5     69.3 47.3     61.8 22.5\n  Li et al. (2021) [20]           99.3 97.6      85.5 77.2    70.2 53.1       \u2013    \u2013        \u2013    \u2013       \u2013     \u2013\n  CLIP ViT-L/14 (Zero Shot)       99.4 95.7      92.8 93.9    79.1 73.3     98.7 93.0     80.7 70.7     74.0 57.6\na manual sample of 3.7K images that survive this (and the NSFW) filter, 91 images (2.5%) were\nidentified as ads potentially unrelated to document contents.12\nDiscarding NSFW images.              We employ strict NSFW image filtering, using DataComp\u2019s [14]\ndataset2metadata13 NSFW binary image classifier. The model is a 4-layer MLP, trained on the\nNSFW dataset introduced in LAION-2B [26]. This MLP takes as input image features extracted\nfrom OpenAI\u2019s CLIP ViT-L/14 [24] and achieves 97.4% accuracy on the NSFW test set. We run this\nclassifier on each image and discard cases with a model-predicted NSFW probability over 0.1, which\nremoves approximately 10% of remaining images. Because the data distribution of the classifier and\nmmc4 may be slightly different, we also conduct a spot check on images that are marked safe for\nwork. In a manual sample of 3.7K images, we discovered zero NSFW images.\nAligning images and sentences.             After collecting a set of images for each document, we now\ndescribe our intra-document alignment process to interleave the collected images with the sentences.\nGiven that the scope of the images and sentences may be different \u2013 the image set is collected from\nthe whole webpage, while the sentence list is subject to preprocessing within the c4 dataset and\nthus may not represent the complete content of the webpage \u2013 we did not rely on Document Object\nModel placements in the raw HTML to establish the alignment between images and sentences in\neach document. Instead, to associate each image with a sentence, we consider each document as\nan instance of a bipartite assignment problem [19, 16], and use CLIP ViT-L/14 compute pairwise\nsimilarities between all sentences/images on a single page. Then, we discard images without at\nleast a 0.15 CLIP cosine similarity to at least one sentence in the document. Finally, we use [18] to\ncompute a bipartite assignment of images to sentences, under the constraint that each sentence can\nonly be assigned a single image.14 Table 2 shows that this zero-shot application of CLIP ViT-L/14 for\nwithin-document matching surpasses prior competitive, fine-tuned methods on image-text alignment\nbenchmarks from [16] (we also distribute the raw intra-document similarity matrices with mmc4 so\nalternate assignment methods can be explored). Figure 2 illustrates two example documents with the\nimages interleaved before or after the assigned sentences.\n3.1   Considerations for data release\nmmc4 contains all images that survive the previously described filters. In addition to the full version\nof the corpus, we construct two additional types of subsets.\n3.1.1    Fewer Faces (mmc4-ff)\nLike the text-only version of c4, mmc4 may contain webpages with personal information that individ-\nuals had not explicitly intended to make available for model training. For an initial public release,\nwe make a version of mmc4 available, mmc4-ff (ff stands for \u201cfewer faces\u201d); similar to some prior\nimage dataset curation efforts [13, 11], mmc4-ff aims to remove images containing detected faces.\n   12\n    The delineation between an \u201cirrelevant advertisement\u201d and a \u201crelevant image\u201d is inexact: for example, we\ndiscovered images advertising specific, small events, e.g., ones hosted by a fishing club within a city (this type of\nimage was not included in this count). We later assess advertisement-ess in the context of the text of documents,\nrather than assessing based on the image alone.\n   13\n    https://github.com/mlfoundations/dataset2metadata\n   14\n    For documents with more images than sentences, after assigning an image to each sentence, we assign\naccording to max similarity.\n                                                          4", "md": "```markdown\nTable 2: Performance on single document image-text benchmarks from [16] (higher=better in all cases). Applying CLIP ViT-L/14 in a zero-shot fashion [24] produces better within-document alignments compared to prior methods which rely on fine-tuning.\n\n| Benchmark                     | MSCOCO AUC p@1 | Story-DII AUC p@1 | Story-SIS AUC p@1 | DII-Stress AUC p@1 | RQA AUC p@1 | DIY AUC p@1 |\n|-------------------------------|----------------|-------------------|-------------------|--------------------|-------------|-------------|\n| Random                        | 49.7 5.0       | 49.4 19.5         | 50.0 19.4         | 50.0 2.0          | 49.4 17.8   | 49.8 6.3   |\n| Hessel et al. (2019) [16]     | 98.7 91.0      | 82.6 70.5         | 68.5 50.5         | 95.3 65.5         | 69.3 47.3   | 61.8 22.5  |\n| Li et al. (2021) [20]         | 99.3 97.6      | 85.5 77.2         | 70.2 53.1         | \u2013       \u2013         | \u2013     \u2013     | \u2013     \u2013    |\n| CLIP ViT-L/14 (Zero Shot)     | 99.4 95.7      | 92.8 93.9         | 79.1 73.3         | 98.7 93.0         | 80.7 70.7   | 74.0 57.6  |\n\na manual sample of 3.7K images that survive this (and the NSFW) filter, 91 images (2.5%) were identified as ads potentially unrelated to document contents.12\n\nDiscarding NSFW images. We employ strict NSFW image filtering, using DataComp\u2019s [14] dataset2metadata13 NSFW binary image classifier. The model is a 4-layer MLP, trained on the NSFW dataset introduced in LAION-2B [26]. This MLP takes as input image features extracted from OpenAI\u2019s CLIP ViT-L/14 [24] and achieves 97.4% accuracy on the NSFW test set. We run this classifier on each image and discard cases with a model-predicted NSFW probability over 0.1, which removes approximately 10% of remaining images. Because the data distribution of the classifier and mmc4 may be slightly different, we also conduct a spot check on images that are marked safe for work. In a manual sample of 3.7K images, we discovered zero NSFW images.\n\nAligning images and sentences. After collecting a set of images for each document, we now describe our intra-document alignment process to interleave the collected images with the sentences. Given that the scope of the images and sentences may be different \u2013 the image set is collected from the whole webpage, while the sentence list is subject to preprocessing within the c4 dataset and thus may not represent the complete content of the webpage \u2013 we did not rely on Document Object Model placements in the raw HTML to establish the alignment between images and sentences in each document. Instead, to associate each image with a sentence, we consider each document as an instance of a bipartite assignment problem [19, 16], and use CLIP ViT-L/14 compute pairwise similarities between all sentences/images on a single page. Then, we discard images without at least a 0.15 CLIP cosine similarity to at least one sentence in the document. Finally, we use [18] to compute a bipartite assignment of images to sentences, under the constraint that each sentence can only be assigned a single image.14 Table 2 shows that this zero-shot application of CLIP ViT-L/14 for within-document matching surpasses prior competitive, fine-tuned methods on image-text alignment benchmarks from [16] (we also distribute the raw intra-document similarity matrices with mmc4 so alternate assignment methods can be explored). Figure 2 illustrates two example documents with the images interleaved before or after the assigned sentences.\n\n3.1 Considerations for data release\nmmc4 contains all images that survive the previously described filters. In addition to the full version of the corpus, we construct two additional types of subsets.\n\n3.1.1 Fewer Faces (mmc4-ff)\nLike the text-only version of c4, mmc4 may contain webpages with personal information that individuals had not explicitly intended to make available for model training. For an initial public release, we make a version of mmc4 available, mmc4-ff (ff stands for \u201cfewer faces\u201d); similar to some prior image dataset curation efforts [13, 11], mmc4-ff aims to remove images containing detected faces.\n\n12 The delineation between an \u201cirrelevant advertisement\u201d and a \u201crelevant image\u201d is inexact: for example, we discovered images advertising specific, small events, e.g., ones hosted by a fishing club within a city (this type of image was not included in this count). We later assess advertisement-ess in the context of the text of documents, rather than assessing based on the image alone.\n13 [DataComp's dataset2metadata](https://github.com/mlfoundations/dataset2metadata)\n14 For documents with more images than sentences, after assigning an image to each sentence, we assign according to max similarity.\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nTable 2: Performance on single document image-text benchmarks from [16] (higher=better in all cases). Applying CLIP ViT-L/14 in a zero-shot fashion [24] produces better within-document alignments compared to prior methods which rely on fine-tuning.", "md": "```markdown\nTable 2: Performance on single document image-text benchmarks from [16] (higher=better in all cases). Applying CLIP ViT-L/14 in a zero-shot fashion [24] produces better within-document alignments compared to prior methods which rely on fine-tuning."}, {"type": "table", "rows": [["Benchmark", "MSCOCO AUC p@1", "Story-DII AUC p@1", "Story-SIS AUC p@1", "DII-Stress AUC p@1", "RQA AUC p@1", "DIY AUC p@1"], ["Random", "49.7 5.0", "49.4 19.5", "50.0 19.4", "50.0 2.0", "49.4 17.8", "49.8 6.3"], ["Hessel et al. (2019) [16]", "98.7 91.0", "82.6 70.5", "68.5 50.5", "95.3 65.5", "69.3 47.3", "61.8 22.5"], ["Li et al. (2021) [20]", "99.3 97.6", "85.5 77.2", "70.2 53.1", "\u2013       \u2013", "\u2013     \u2013", "\u2013     \u2013"], ["CLIP ViT-L/14 (Zero Shot)", "99.4 95.7", "92.8 93.9", "79.1 73.3", "98.7 93.0", "80.7 70.7", "74.0 57.6"]], "md": "| Benchmark                     | MSCOCO AUC p@1 | Story-DII AUC p@1 | Story-SIS AUC p@1 | DII-Stress AUC p@1 | RQA AUC p@1 | DIY AUC p@1 |\n|-------------------------------|----------------|-------------------|-------------------|--------------------|-------------|-------------|\n| Random                        | 49.7 5.0       | 49.4 19.5         | 50.0 19.4         | 50.0 2.0          | 49.4 17.8   | 49.8 6.3   |\n| Hessel et al. (2019) [16]     | 98.7 91.0      | 82.6 70.5         | 68.5 50.5         | 95.3 65.5         | 69.3 47.3   | 61.8 22.5  |\n| Li et al. (2021) [20]         | 99.3 97.6      | 85.5 77.2         | 70.2 53.1         | \u2013       \u2013         | \u2013     \u2013     | \u2013     \u2013    |\n| CLIP ViT-L/14 (Zero Shot)     | 99.4 95.7      | 92.8 93.9         | 79.1 73.3         | 98.7 93.0         | 80.7 70.7   | 74.0 57.6  |", "isPerfectTable": true, "csv": "\"Benchmark\",\"MSCOCO AUC p@1\",\"Story-DII AUC p@1\",\"Story-SIS AUC p@1\",\"DII-Stress AUC p@1\",\"RQA AUC p@1\",\"DIY AUC p@1\"\n\"Random\",\"49.7 5.0\",\"49.4 19.5\",\"50.0 19.4\",\"50.0 2.0\",\"49.4 17.8\",\"49.8 6.3\"\n\"Hessel et al. (2019) [16]\",\"98.7 91.0\",\"82.6 70.5\",\"68.5 50.5\",\"95.3 65.5\",\"69.3 47.3\",\"61.8 22.5\"\n\"Li et al. (2021) [20]\",\"99.3 97.6\",\"85.5 77.2\",\"70.2 53.1\",\"\u2013       \u2013\",\"\u2013     \u2013\",\"\u2013     \u2013\"\n\"CLIP ViT-L/14 (Zero Shot)\",\"99.4 95.7\",\"92.8 93.9\",\"79.1 73.3\",\"98.7 93.0\",\"80.7 70.7\",\"74.0 57.6\""}, {"type": "text", "value": "a manual sample of 3.7K images that survive this (and the NSFW) filter, 91 images (2.5%) were identified as ads potentially unrelated to document contents.12\n\nDiscarding NSFW images. We employ strict NSFW image filtering, using DataComp\u2019s [14] dataset2metadata13 NSFW binary image classifier. The model is a 4-layer MLP, trained on the NSFW dataset introduced in LAION-2B [26]. This MLP takes as input image features extracted from OpenAI\u2019s CLIP ViT-L/14 [24] and achieves 97.4% accuracy on the NSFW test set. We run this classifier on each image and discard cases with a model-predicted NSFW probability over 0.1, which removes approximately 10% of remaining images. Because the data distribution of the classifier and mmc4 may be slightly different, we also conduct a spot check on images that are marked safe for work. In a manual sample of 3.7K images, we discovered zero NSFW images.\n\nAligning images and sentences. After collecting a set of images for each document, we now describe our intra-document alignment process to interleave the collected images with the sentences. Given that the scope of the images and sentences may be different \u2013 the image set is collected from the whole webpage, while the sentence list is subject to preprocessing within the c4 dataset and thus may not represent the complete content of the webpage \u2013 we did not rely on Document Object Model placements in the raw HTML to establish the alignment between images and sentences in each document. Instead, to associate each image with a sentence, we consider each document as an instance of a bipartite assignment problem [19, 16], and use CLIP ViT-L/14 compute pairwise similarities between all sentences/images on a single page. Then, we discard images without at least a 0.15 CLIP cosine similarity to at least one sentence in the document. Finally, we use [18] to compute a bipartite assignment of images to sentences, under the constraint that each sentence can only be assigned a single image.14 Table 2 shows that this zero-shot application of CLIP ViT-L/14 for within-document matching surpasses prior competitive, fine-tuned methods on image-text alignment benchmarks from [16] (we also distribute the raw intra-document similarity matrices with mmc4 so alternate assignment methods can be explored). Figure 2 illustrates two example documents with the images interleaved before or after the assigned sentences.\n\n3.1 Considerations for data release\nmmc4 contains all images that survive the previously described filters. In addition to the full version of the corpus, we construct two additional types of subsets.\n\n3.1.1 Fewer Faces (mmc4-ff)\nLike the text-only version of c4, mmc4 may contain webpages with personal information that individuals had not explicitly intended to make available for model training. For an initial public release, we make a version of mmc4 available, mmc4-ff (ff stands for \u201cfewer faces\u201d); similar to some prior image dataset curation efforts [13, 11], mmc4-ff aims to remove images containing detected faces.\n\n12 The delineation between an \u201cirrelevant advertisement\u201d and a \u201crelevant image\u201d is inexact: for example, we discovered images advertising specific, small events, e.g., ones hosted by a fishing club within a city (this type of image was not included in this count). We later assess advertisement-ess in the context of the text of documents, rather than assessing based on the image alone.\n13 [DataComp's dataset2metadata](https://github.com/mlfoundations/dataset2metadata)\n14 For documents with more images than sentences, after assigning an image to each sentence, we assign according to max similarity.\n```", "md": "a manual sample of 3.7K images that survive this (and the NSFW) filter, 91 images (2.5%) were identified as ads potentially unrelated to document contents.12\n\nDiscarding NSFW images. We employ strict NSFW image filtering, using DataComp\u2019s [14] dataset2metadata13 NSFW binary image classifier. The model is a 4-layer MLP, trained on the NSFW dataset introduced in LAION-2B [26]. This MLP takes as input image features extracted from OpenAI\u2019s CLIP ViT-L/14 [24] and achieves 97.4% accuracy on the NSFW test set. We run this classifier on each image and discard cases with a model-predicted NSFW probability over 0.1, which removes approximately 10% of remaining images. Because the data distribution of the classifier and mmc4 may be slightly different, we also conduct a spot check on images that are marked safe for work. In a manual sample of 3.7K images, we discovered zero NSFW images.\n\nAligning images and sentences. After collecting a set of images for each document, we now describe our intra-document alignment process to interleave the collected images with the sentences. Given that the scope of the images and sentences may be different \u2013 the image set is collected from the whole webpage, while the sentence list is subject to preprocessing within the c4 dataset and thus may not represent the complete content of the webpage \u2013 we did not rely on Document Object Model placements in the raw HTML to establish the alignment between images and sentences in each document. Instead, to associate each image with a sentence, we consider each document as an instance of a bipartite assignment problem [19, 16], and use CLIP ViT-L/14 compute pairwise similarities between all sentences/images on a single page. Then, we discard images without at least a 0.15 CLIP cosine similarity to at least one sentence in the document. Finally, we use [18] to compute a bipartite assignment of images to sentences, under the constraint that each sentence can only be assigned a single image.14 Table 2 shows that this zero-shot application of CLIP ViT-L/14 for within-document matching surpasses prior competitive, fine-tuned methods on image-text alignment benchmarks from [16] (we also distribute the raw intra-document similarity matrices with mmc4 so alternate assignment methods can be explored). Figure 2 illustrates two example documents with the images interleaved before or after the assigned sentences.\n\n3.1 Considerations for data release\nmmc4 contains all images that survive the previously described filters. In addition to the full version of the corpus, we construct two additional types of subsets.\n\n3.1.1 Fewer Faces (mmc4-ff)\nLike the text-only version of c4, mmc4 may contain webpages with personal information that individuals had not explicitly intended to make available for model training. For an initial public release, we make a version of mmc4 available, mmc4-ff (ff stands for \u201cfewer faces\u201d); similar to some prior image dataset curation efforts [13, 11], mmc4-ff aims to remove images containing detected faces.\n\n12 The delineation between an \u201cirrelevant advertisement\u201d and a \u201crelevant image\u201d is inexact: for example, we discovered images advertising specific, small events, e.g., ones hosted by a fishing club within a city (this type of image was not included in this count). We later assess advertisement-ess in the context of the text of documents, rather than assessing based on the image alone.\n13 [DataComp's dataset2metadata](https://github.com/mlfoundations/dataset2metadata)\n14 For documents with more images than sentences, after assigning an image to each sentence, we assign according to max similarity.\n```"}]}, {"page": 5, "text": " Example#1: Interleaving the image before each corresponding text\n  [..., \"Check out Shane Driscoll\u2019s take on sustainable communities and how his photograph fits this year\u2019s Green Cities theme.\", ...,        ,\"Man-\n  made platforms like the one pictured here allow these fish-eating birds of prey to thrive in developed coastal areas.\", \"A city surrounded by\n  mountains.\", \"I took this photo in October on a hike in New Hampshire.\",              , \"It is looking at Mt. Chicora from the middle sister\n  mountain.\", \"Getting people out into beautiful places like this is becoming more and more popular, and each time we bring a little piece of nature\n  back with us that inspires us to make our cities better.\", ...]\n Example#2: Interleaving the image after each corresponding text\n  [\"This Walnut and Blue Cheese Stuffed Mushrooms recipe is sponsored by Fisher Nuts.\",             , \"Stuffed mushrooms are an appetizer that always\n  grabs my attention at a party.\",            , \"If you are a mushroom lover, like me, you probably feel the same.\", \"The ideas for stuffing mushrooms\n  are endless, so many combinations to play with, a couple of my personal favorites are these Mediterranean Stuffed Mushrooms and these Spinach and\n  Toasted Pine Nut Stuffed Mushrooms.\",        , \"Well, you can officially add these Walnut and Blue Cheese Stuffed Mushrooms to my favorites list.\",\n  \"The ingredients for the stuffing are simple, which is always best.\", ... ]\nFigure 2: Two example image+text documents from mmc4. Following Flamingo [2], during training,\nimages can be interleaved before or after their assigned sentences. More example documents are\ngiven in Appendix D.2.\nRemoving images with detected faces.                             To detect faces at billion-scale with the intent of removing\nthem from the dataset, we first run RetinaFace[10]15 over a sample of 60K images with the default\nsettings. This detector runs at a high resolution and would be computationally prohibitive to run in full\nprecision for the whole corpus; it produces detailed localization information about the coordinates of\neach face in each image (which we discard). Using an 80/20 train/test split, we train a cross-validated\nlogistic regression over CLIP ViT-L/14 features to predict whether or not RetinaFace detects a face:\nthis classifier is several orders of magnitude faster compared to RetinaFace. This approximation\nperforms well: we choose a confidence cutoff that achieves 95% recall16 for the label \u201cRetinaFace\ndetected any face\u201d over the test set while preserving 65% of the original images.\nManual sample-based face image risk assessment.                                       We performed a manual verification of face\nremoval. In a random sample of 912 images that pass all filters including the \u201cno faces\u201d filter, 23\n(2.5%) images arguably contain a mostly-un-obscured human face. In most cases (12/23), faces\nare very low resolution, e.g., a 150x150px image of a crowd of people from a distance, where each\nface accounts for 3x4 pixels, or are motion shots where the face is blurred. In one case, the face\nis Marilyn Monroe\u2019s as depicted in art on a wall. In 6 cases, there is a plausibly identifiable face\ndepicted: in 2 cases, these are models posing in ads; in 1 case, there is a low resolution image of\npoliticians giving a speech; in 2 cases, the faces are obscured; in 1 case, a passerby was caught in the\nbackground of a city photograph and could feasibly be individually identified. Overall: the rate of\nunobscured, high-resolution, identifiable faces in mmc4-ff is low.\n3.1.2       Core (mmc4-core)\nEarly conversations with some model developers revealed a desire to work with a smaller subset of the\ncorpus as an initial step. We thus additionally release core versions of mmc4 (and mmc4-ff), which\napply even more stringent filtration criteria. The aim of core is to identify a \u201chigher-precision\u201d subset\nof documents that: 1) have a minimum/maximum number of sentences/images per document; 2) pass\nan even stricter deduplication step; and 3) have a higher image-text similarity. Hyperparameters17 are\nselected heuristically and are balanced to downsize the original corpus by an order of magnitude.\n    15As implemented by [27, 28] available from https://github.com/serengil/retinaface.\n    16RetinaFace is not perfectly accurate, so selecting a more aggressive threshold (e.g., 99.99%) would not\nnecessarily result in significantly fewer face-containing images removed.\n    17Min/max number of sentences: 4/40; min/max number of images 2/15; findimagedupes applied with\na threshold of 10; documents are required to have at least 75% of image assignments have CLIP ViT-L/14\nsimilarity of greater than 25.\n                                                                            5", "md": "# Document\n\n## Example#1: Interleaving the image before each corresponding text\n\n[..., \"Check out Shane Driscoll\u2019s take on sustainable communities and how his photograph fits this year\u2019s Green Cities theme.\", ..., , \"Man-made platforms like the one pictured here allow these fish-eating birds of prey to thrive in developed coastal areas.\", \"A city surrounded by mountains.\", \"I took this photo in October on a hike in New Hampshire.\", , \"It is looking at Mt. Chicora from the middle sister mountain.\", \"Getting people out into beautiful places like this is becoming more and more popular, and each time we bring a little piece of nature back with us that inspires us to make our cities better.\", ...]\n\n## Example#2: Interleaving the image after each corresponding text\n\n[\"This Walnut and Blue Cheese Stuffed Mushrooms recipe is sponsored by Fisher Nuts.\", , \"Stuffed mushrooms are an appetizer that always grabs my attention at a party.\", , \"If you are a mushroom lover, like me, you probably feel the same.\", \"The ideas for stuffing mushrooms are endless, so many combinations to play with, a couple of my personal favorites are these Mediterranean Stuffed Mushrooms and these Spinach and Toasted Pine Nut Stuffed Mushrooms.\", , \"Well, you can officially add these Walnut and Blue Cheese Stuffed Mushrooms to my favorites list.\", \"The ingredients for the stuffing are simple, which is always best.\", ... ]\n\n### Figure 2: Two example image+text documents from mmc4. Following Flamingo [2], during training, images can be interleaved before or after their assigned sentences. More example documents are given in Appendix D.2.\n\nRemoving images with detected faces. To detect faces at billion-scale with the intent of removing them from the dataset, we first run RetinaFace[10] over a sample of 60K images with the default settings. This detector runs at a high resolution and would be computationally prohibitive to run in full precision for the whole corpus; it produces detailed localization information about the coordinates of each face in each image (which we discard). Using an 80/20 train/test split, we train a cross-validated logistic regression over CLIP ViT-L/14 features to predict whether or not RetinaFace detects a face: this classifier is several orders of magnitude faster compared to RetinaFace. This approximation performs well: we choose a confidence cutoff that achieves 95% recall for the label \u201cRetinaFace detected any face\u201d over the test set while preserving 65% of the original images.\n\nManual sample-based face image risk assessment. We performed a manual verification of face removal. In a random sample of 912 images that pass all filters including the \u201cno faces\u201d filter, 23 (2.5%) images arguably contain a mostly-un-obscured human face. In most cases (12/23), faces are very low resolution, e.g., a 150x150px image of a crowd of people from a distance, where each face accounts for 3x4 pixels, or are motion shots where the face is blurred. In one case, the face is Marilyn Monroe\u2019s as depicted in art on a wall. In 6 cases, there is a plausibly identifiable face depicted: in 2 cases, these are models posing in ads; in 1 case, there is a low resolution image of politicians giving a speech; in 2 cases, the faces are obscured; in 1 case, a passerby was caught in the background of a city photograph and could feasibly be individually identified. Overall: the rate of unobscured, high-resolution, identifiable faces in mmc4-ff is low.\n\n#### Core (mmc4-core)\n\nEarly conversations with some model developers revealed a desire to work with a smaller subset of the corpus as an initial step. We thus additionally release core versions of mmc4 (and mmc4-ff), which apply even more stringent filtration criteria. The aim of core is to identify a \u201chigher-precision\u201d subset of documents that: 1) have a minimum/maximum number of sentences/images per document; 2) pass an even stricter deduplication step; and 3) have a higher image-text similarity. Hyperparameters are selected heuristically and are balanced to downsize the original corpus by an order of magnitude.\n\n15As implemented by [27, 28] available from https://github.com/serengil/retinaface.\n\n16RetinaFace is not perfectly accurate, so selecting a more aggressive threshold (e.g., 99.99%) would not necessarily result in significantly fewer face-containing images removed.\n\n17Min/max number of sentences: 4/40; min/max number of images 2/15; findimagedupes applied with a threshold of 10; documents are required to have at least 75% of image assignments have CLIP ViT-L/14 similarity of greater than 25.", "images": [{"name": "page-5-1.jpg", "height": 77, "width": 396, "x": 108, "y": 80}, {"name": "page-5-0.jpg", "height": 74, "width": 396, "x": 108, "y": 167}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Example1: Interleaving the image before each corresponding text", "md": "## Example#1: Interleaving the image before each corresponding text"}, {"type": "text", "value": "[..., \"Check out Shane Driscoll\u2019s take on sustainable communities and how his photograph fits this year\u2019s Green Cities theme.\", ..., , \"Man-made platforms like the one pictured here allow these fish-eating birds of prey to thrive in developed coastal areas.\", \"A city surrounded by mountains.\", \"I took this photo in October on a hike in New Hampshire.\", , \"It is looking at Mt. Chicora from the middle sister mountain.\", \"Getting people out into beautiful places like this is becoming more and more popular, and each time we bring a little piece of nature back with us that inspires us to make our cities better.\", ...]", "md": "[..., \"Check out Shane Driscoll\u2019s take on sustainable communities and how his photograph fits this year\u2019s Green Cities theme.\", ..., , \"Man-made platforms like the one pictured here allow these fish-eating birds of prey to thrive in developed coastal areas.\", \"A city surrounded by mountains.\", \"I took this photo in October on a hike in New Hampshire.\", , \"It is looking at Mt. Chicora from the middle sister mountain.\", \"Getting people out into beautiful places like this is becoming more and more popular, and each time we bring a little piece of nature back with us that inspires us to make our cities better.\", ...]"}, {"type": "heading", "lvl": 2, "value": "Example2: Interleaving the image after each corresponding text", "md": "## Example#2: Interleaving the image after each corresponding text"}, {"type": "text", "value": "[\"This Walnut and Blue Cheese Stuffed Mushrooms recipe is sponsored by Fisher Nuts.\", , \"Stuffed mushrooms are an appetizer that always grabs my attention at a party.\", , \"If you are a mushroom lover, like me, you probably feel the same.\", \"The ideas for stuffing mushrooms are endless, so many combinations to play with, a couple of my personal favorites are these Mediterranean Stuffed Mushrooms and these Spinach and Toasted Pine Nut Stuffed Mushrooms.\", , \"Well, you can officially add these Walnut and Blue Cheese Stuffed Mushrooms to my favorites list.\", \"The ingredients for the stuffing are simple, which is always best.\", ... ]", "md": "[\"This Walnut and Blue Cheese Stuffed Mushrooms recipe is sponsored by Fisher Nuts.\", , \"Stuffed mushrooms are an appetizer that always grabs my attention at a party.\", , \"If you are a mushroom lover, like me, you probably feel the same.\", \"The ideas for stuffing mushrooms are endless, so many combinations to play with, a couple of my personal favorites are these Mediterranean Stuffed Mushrooms and these Spinach and Toasted Pine Nut Stuffed Mushrooms.\", , \"Well, you can officially add these Walnut and Blue Cheese Stuffed Mushrooms to my favorites list.\", \"The ingredients for the stuffing are simple, which is always best.\", ... ]"}, {"type": "heading", "lvl": 3, "value": "Figure 2: Two example image+text documents from mmc4. Following Flamingo [2], during training, images can be interleaved before or after their assigned sentences. More example documents are given in Appendix D.2.", "md": "### Figure 2: Two example image+text documents from mmc4. Following Flamingo [2], during training, images can be interleaved before or after their assigned sentences. More example documents are given in Appendix D.2."}, {"type": "text", "value": "Removing images with detected faces. To detect faces at billion-scale with the intent of removing them from the dataset, we first run RetinaFace[10] over a sample of 60K images with the default settings. This detector runs at a high resolution and would be computationally prohibitive to run in full precision for the whole corpus; it produces detailed localization information about the coordinates of each face in each image (which we discard). Using an 80/20 train/test split, we train a cross-validated logistic regression over CLIP ViT-L/14 features to predict whether or not RetinaFace detects a face: this classifier is several orders of magnitude faster compared to RetinaFace. This approximation performs well: we choose a confidence cutoff that achieves 95% recall for the label \u201cRetinaFace detected any face\u201d over the test set while preserving 65% of the original images.\n\nManual sample-based face image risk assessment. We performed a manual verification of face removal. In a random sample of 912 images that pass all filters including the \u201cno faces\u201d filter, 23 (2.5%) images arguably contain a mostly-un-obscured human face. In most cases (12/23), faces are very low resolution, e.g., a 150x150px image of a crowd of people from a distance, where each face accounts for 3x4 pixels, or are motion shots where the face is blurred. In one case, the face is Marilyn Monroe\u2019s as depicted in art on a wall. In 6 cases, there is a plausibly identifiable face depicted: in 2 cases, these are models posing in ads; in 1 case, there is a low resolution image of politicians giving a speech; in 2 cases, the faces are obscured; in 1 case, a passerby was caught in the background of a city photograph and could feasibly be individually identified. Overall: the rate of unobscured, high-resolution, identifiable faces in mmc4-ff is low.", "md": "Removing images with detected faces. To detect faces at billion-scale with the intent of removing them from the dataset, we first run RetinaFace[10] over a sample of 60K images with the default settings. This detector runs at a high resolution and would be computationally prohibitive to run in full precision for the whole corpus; it produces detailed localization information about the coordinates of each face in each image (which we discard). Using an 80/20 train/test split, we train a cross-validated logistic regression over CLIP ViT-L/14 features to predict whether or not RetinaFace detects a face: this classifier is several orders of magnitude faster compared to RetinaFace. This approximation performs well: we choose a confidence cutoff that achieves 95% recall for the label \u201cRetinaFace detected any face\u201d over the test set while preserving 65% of the original images.\n\nManual sample-based face image risk assessment. We performed a manual verification of face removal. In a random sample of 912 images that pass all filters including the \u201cno faces\u201d filter, 23 (2.5%) images arguably contain a mostly-un-obscured human face. In most cases (12/23), faces are very low resolution, e.g., a 150x150px image of a crowd of people from a distance, where each face accounts for 3x4 pixels, or are motion shots where the face is blurred. In one case, the face is Marilyn Monroe\u2019s as depicted in art on a wall. In 6 cases, there is a plausibly identifiable face depicted: in 2 cases, these are models posing in ads; in 1 case, there is a low resolution image of politicians giving a speech; in 2 cases, the faces are obscured; in 1 case, a passerby was caught in the background of a city photograph and could feasibly be individually identified. Overall: the rate of unobscured, high-resolution, identifiable faces in mmc4-ff is low."}, {"type": "heading", "lvl": 4, "value": "Core (mmc4-core)", "md": "#### Core (mmc4-core)"}, {"type": "text", "value": "Early conversations with some model developers revealed a desire to work with a smaller subset of the corpus as an initial step. We thus additionally release core versions of mmc4 (and mmc4-ff), which apply even more stringent filtration criteria. The aim of core is to identify a \u201chigher-precision\u201d subset of documents that: 1) have a minimum/maximum number of sentences/images per document; 2) pass an even stricter deduplication step; and 3) have a higher image-text similarity. Hyperparameters are selected heuristically and are balanced to downsize the original corpus by an order of magnitude.\n\n15As implemented by [27, 28] available from https://github.com/serengil/retinaface.\n\n16RetinaFace is not perfectly accurate, so selecting a more aggressive threshold (e.g., 99.99%) would not necessarily result in significantly fewer face-containing images removed.\n\n17Min/max number of sentences: 4/40; min/max number of images 2/15; findimagedupes applied with a threshold of 10; documents are required to have at least 75% of image assignments have CLIP ViT-L/14 similarity of greater than 25.", "md": "Early conversations with some model developers revealed a desire to work with a smaller subset of the corpus as an initial step. We thus additionally release core versions of mmc4 (and mmc4-ff), which apply even more stringent filtration criteria. The aim of core is to identify a \u201chigher-precision\u201d subset of documents that: 1) have a minimum/maximum number of sentences/images per document; 2) pass an even stricter deduplication step; and 3) have a higher image-text similarity. Hyperparameters are selected heuristically and are balanced to downsize the original corpus by an order of magnitude.\n\n15As implemented by [27, 28] available from https://github.com/serengil/retinaface.\n\n16RetinaFace is not perfectly accurate, so selecting a more aggressive threshold (e.g., 99.99%) would not necessarily result in significantly fewer face-containing images removed.\n\n17Min/max number of sentences: 4/40; min/max number of images 2/15; findimagedupes applied with a threshold of 10; documents are required to have at least 75% of image assignments have CLIP ViT-L/14 similarity of greater than 25."}]}, {"page": 6, "text": "                                                                                                                                                                                 120\n                                                      Lin. Assignment                                                                       Lin. Assignment                    Sentences per doc\n                                                      (  = 24.0)                                                                            (  = 34%)                            100           M = 2.0  = 5.7                    = 10.3\n                                                      Maximum                                                                               Maximum\n                                                      (  = 24.5)                                                                            (  = 22%)\n     99.95%    98.01%     72.53%    37.28%     9.27%                                                                                                                               80\n                                                                                                                                                                                   60\n   10        15         20        25         30        35         40        45           0%           20%           40%           60%           80%          100%                  40\n                     CLIP ViT-L/14 Similarity                                                   % of Sentences w/ Image (per doc)                                                                                                      = 24.3\n (a) CLIP sim is similar between                                                     (b) Lin. assignment results in a                                                              200                                              M = 13.0\n lin. assignment + max. In red:                                                      higher percentage of sentences                                                                      0              10             20    30          40    50\n percent of images remaining at                                                      being associated with an image.                                                                                         Images per doc\n various CLIP thresholds.                                                                                                                                             Figure 5: Distribution of images and\n Figure 4: Using linear assignment results in comparable                                                                                                              sentences per document; the median\n image-text similarities to max assignment, but the former                                                                                                            document has 2 images/13 sentences.\n spreads images much more evenly, e.g., the per-document                                                                                                              Documents with more sentences tend\n mean percent of sentences with an associated image in-                                                                                                               to have more images, but the correla-\n creases from 22% to 34%.                                                                                                                                             tion is weak (Spearman \u03c1 \u201c 10.3).\n 4            Exploring mmc4\n Statistics.                        Table 1 gives basic summary statistics of mmc4 (and fewer-faces/core subsets) compared\n to some other interleaved image/text corpora. Overall, the full version of mmc4 is larger than prior\n non-public datasets across axes like number of images/number of documents. In addition, the various\n subsets of the corpus offer trade-offs between privacy, image/text similarity thresholds, etc. Figure 5\n gives details about the mean/median number of images/sentences in each document (mean/median #\n sent.=2.0/5.7; # im = 13.0/24.3) based on a random sample of 22K documents.\n Sources of documents & images.                                                                          We trace back the top-level domains of documents (webpages)\n and images to better understand the origins of contents in mmc4. Figure 6 presents the top-20 top-level\n domains that host the highest number of documents and images in mmc4. The distribution of document\n sources in mmc4 reveals a relatively uniform pattern, with 101.2M documents distributed across 6.0M\n unique domains. On average, each domain contains approximately 16.9 documents, with a median\n value of 2.0. The top 10% most frequently appeared domains account for 77% of all documents in\n mmc4. The documents are most commonly hosted on news media outlets (e.g., BBC, NY Times,\n Daily Express, Daily Mail), academic publication sites (e.g., Springer), online encyclopedias (e.g.,\n Wikipedia), and e-commerce sites (e.g., iTunes, Etsy). Conversely, the sources of images in mmc4\n exhibit a higher level of clustering. The 571.4M images are hosted on 4.9M domains, with each\n domain having an average of 116.0 images and a median value of 7.0 images. The top 10% most\n frequent domains are responsible for hosting 89% of all images. Images are most commonly hosted\n on blogs (e.g., Blogspot, WordPress), shopping sites (e.g., Amazon), cloud storage sites (e.g., AWS\n S3, Google storage), or general image hosting sites (e.g., Flickr, Imgur). More detailed lists of top\n document/image domains in mmc4 and mmc4-core can be found in Appendix C.\n Image-text similarity.                                                 Figure 4 provides detail about the linear assignment process compared to a\n\u201cmax\u201d assignment alternative, where each image is simply assigned to its maximally CLIP-similar\n sentence. The linear assignment process slightly decreases the average CLIP similarity between\n images/sentences (from 24.5 \u00d1 24.0), but significantly more evenly \u201cspreads\u201d images throughout the\n documents: per-document, the mean percentage of sentences with an associated image rises from\n 22% \u00d1 34%.\n Topic-based assessment.                                                        We ran LDA [6] as implemented by Mallet [22] on a random sample of\n 22K documents from mmc4 with k \u201c 30 topics. The resulting clusters span a broad set of topics like\n cooking, communities, travel, music, art, etc. Figure 1 shows some example LDA topic clusters.19 In\n        18   Multiple sub-sites may exist within a given domain (e.g., https://i0.wp.com and https://i1.wp.com).\n We replace specific patterns such as digits or location acronyms with \u201c*\u201d to cluster these sub-sites together.\n        19  A full list of topics and their frequencies according to the model is in Appendix B.\n                                                                                                                                           6", "md": "## Lin. Assignment\n\n(  = 24.0)\n\nMaximum (  = 24.5)\n\n99.95%\n98.01%\n72.53%\n37.28%\n9.27%\n\nCLIP ViT-L/14 Similarity\n\n(a) CLIP sim is similar between lin. assignment + max. In red: percent of images remaining at various CLIP thresholds.\n\nFigure 4: Using linear assignment results in comparable image-text similarities to max assignment, but the former spreads images much more evenly, e.g., the per-document mean percent of sentences with an associated image increases from 22% to 34%.\n\n## Lin. Assignment\n\n(  = 34%)\n\n100\nM = 2.0\n= 5.7\n= 10.3\n\n(b) Lin. assignment results in a higher percentage of sentences being associated with an image.\n\n% of Sentences w/ Image (per doc) = 24.3\n\nFigure 5: Distribution of images and sentences per document; the median document has 2 images/13 sentences.\n\nDocuments with more sentences tend to have more images, but the correlation is weak (Spearman \u03c1 = 10.3).\n\n## Exploring mmc4 Statistics\n\nTable 1 gives basic summary statistics of mmc4 (and fewer-faces/core subsets) compared to some other interleaved image/text corpora. Overall, the full version of mmc4 is larger than prior non-public datasets across axes like number of images/number of documents. In addition, the various subsets of the corpus offer trade-offs between privacy, image/text similarity thresholds, etc. Figure 5 gives details about the mean/median number of images/sentences in each document (mean/median # sent.=2.0/5.7; # im = 13.0/24.3) based on a random sample of 22K documents.\n\nSources of documents & images.\n\nWe trace back the top-level domains of documents (webpages) and images to better understand the origins of contents in mmc4. Figure 6 presents the top-20 top-level domains that host the highest number of documents and images in mmc4. The distribution of document sources in mmc4 reveals a relatively uniform pattern, with 101.2M documents distributed across 6.0M unique domains. On average, each domain contains approximately 16.9 documents, with a median value of 2.0. The top 10% most frequently appeared domains account for 77% of all documents in mmc4. The documents are most commonly hosted on news media outlets (e.g., BBC, NY Times, Daily Express, Daily Mail), academic publication sites (e.g., Springer), online encyclopedias (e.g., Wikipedia), and e-commerce sites (e.g., iTunes, Etsy). Conversely, the sources of images in mmc4 exhibit a higher level of clustering. The 571.4M images are hosted on 4.9M domains, with each domain having an average of 116.0 images and a median value of 7.0 images. The top 10% most frequent domains are responsible for hosting 89% of all images. Images are most commonly hosted on blogs (e.g., Blogspot, WordPress), shopping sites (e.g., Amazon), cloud storage sites (e.g., AWS S3, Google storage), or general image hosting sites (e.g., Flickr, Imgur). More detailed lists of top document/image domains in mmc4 and mmc4-core can be found in Appendix C.\n\nImage-text similarity.\n\nFigure 4 provides detail about the linear assignment process compared to a \"max\" assignment alternative, where each image is simply assigned to its maximally CLIP-similar sentence. The linear assignment process slightly decreases the average CLIP similarity between images/sentences (from 24.5 to 24.0), but significantly more evenly \"spreads\" images throughout the documents: per-document, the mean percentage of sentences with an associated image rises from 22% to 34%.\n\nTopic-based assessment.\n\nWe ran LDA [6] as implemented by Mallet [22] on a random sample of 22K documents from mmc4 with k = 30 topics. The resulting clusters span a broad set of topics like cooking, communities, travel, music, art, etc. Figure 1 shows some example LDA topic clusters.18 In\n\n18 Multiple sub-sites may exist within a given domain (e.g., https://i0.wp.com and https://i1.wp.com). We replace specific patterns such as digits or location acronyms with \"*\" to cluster these sub-sites together.\n\nA full list of topics and their frequencies according to the model is in Appendix B.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Lin. Assignment", "md": "## Lin. Assignment"}, {"type": "text", "value": "(  = 24.0)\n\nMaximum (  = 24.5)\n\n99.95%\n98.01%\n72.53%\n37.28%\n9.27%\n\nCLIP ViT-L/14 Similarity\n\n(a) CLIP sim is similar between lin. assignment + max. In red: percent of images remaining at various CLIP thresholds.\n\nFigure 4: Using linear assignment results in comparable image-text similarities to max assignment, but the former spreads images much more evenly, e.g., the per-document mean percent of sentences with an associated image increases from 22% to 34%.", "md": "(  = 24.0)\n\nMaximum (  = 24.5)\n\n99.95%\n98.01%\n72.53%\n37.28%\n9.27%\n\nCLIP ViT-L/14 Similarity\n\n(a) CLIP sim is similar between lin. assignment + max. In red: percent of images remaining at various CLIP thresholds.\n\nFigure 4: Using linear assignment results in comparable image-text similarities to max assignment, but the former spreads images much more evenly, e.g., the per-document mean percent of sentences with an associated image increases from 22% to 34%."}, {"type": "heading", "lvl": 2, "value": "Lin. Assignment", "md": "## Lin. Assignment"}, {"type": "text", "value": "(  = 34%)\n\n100\nM = 2.0\n= 5.7\n= 10.3\n\n(b) Lin. assignment results in a higher percentage of sentences being associated with an image.\n\n% of Sentences w/ Image (per doc) = 24.3\n\nFigure 5: Distribution of images and sentences per document; the median document has 2 images/13 sentences.\n\nDocuments with more sentences tend to have more images, but the correlation is weak (Spearman \u03c1 = 10.3).", "md": "(  = 34%)\n\n100\nM = 2.0\n= 5.7\n= 10.3\n\n(b) Lin. assignment results in a higher percentage of sentences being associated with an image.\n\n% of Sentences w/ Image (per doc) = 24.3\n\nFigure 5: Distribution of images and sentences per document; the median document has 2 images/13 sentences.\n\nDocuments with more sentences tend to have more images, but the correlation is weak (Spearman \u03c1 = 10.3)."}, {"type": "heading", "lvl": 2, "value": "Exploring mmc4 Statistics", "md": "## Exploring mmc4 Statistics"}, {"type": "text", "value": "Table 1 gives basic summary statistics of mmc4 (and fewer-faces/core subsets) compared to some other interleaved image/text corpora. Overall, the full version of mmc4 is larger than prior non-public datasets across axes like number of images/number of documents. In addition, the various subsets of the corpus offer trade-offs between privacy, image/text similarity thresholds, etc. Figure 5 gives details about the mean/median number of images/sentences in each document (mean/median # sent.=2.0/5.7; # im = 13.0/24.3) based on a random sample of 22K documents.\n\nSources of documents & images.\n\nWe trace back the top-level domains of documents (webpages) and images to better understand the origins of contents in mmc4. Figure 6 presents the top-20 top-level domains that host the highest number of documents and images in mmc4. The distribution of document sources in mmc4 reveals a relatively uniform pattern, with 101.2M documents distributed across 6.0M unique domains. On average, each domain contains approximately 16.9 documents, with a median value of 2.0. The top 10% most frequently appeared domains account for 77% of all documents in mmc4. The documents are most commonly hosted on news media outlets (e.g., BBC, NY Times, Daily Express, Daily Mail), academic publication sites (e.g., Springer), online encyclopedias (e.g., Wikipedia), and e-commerce sites (e.g., iTunes, Etsy). Conversely, the sources of images in mmc4 exhibit a higher level of clustering. The 571.4M images are hosted on 4.9M domains, with each domain having an average of 116.0 images and a median value of 7.0 images. The top 10% most frequent domains are responsible for hosting 89% of all images. Images are most commonly hosted on blogs (e.g., Blogspot, WordPress), shopping sites (e.g., Amazon), cloud storage sites (e.g., AWS S3, Google storage), or general image hosting sites (e.g., Flickr, Imgur). More detailed lists of top document/image domains in mmc4 and mmc4-core can be found in Appendix C.\n\nImage-text similarity.\n\nFigure 4 provides detail about the linear assignment process compared to a \"max\" assignment alternative, where each image is simply assigned to its maximally CLIP-similar sentence. The linear assignment process slightly decreases the average CLIP similarity between images/sentences (from 24.5 to 24.0), but significantly more evenly \"spreads\" images throughout the documents: per-document, the mean percentage of sentences with an associated image rises from 22% to 34%.\n\nTopic-based assessment.\n\nWe ran LDA [6] as implemented by Mallet [22] on a random sample of 22K documents from mmc4 with k = 30 topics. The resulting clusters span a broad set of topics like cooking, communities, travel, music, art, etc. Figure 1 shows some example LDA topic clusters.18 In\n\n18 Multiple sub-sites may exist within a given domain (e.g., https://i0.wp.com and https://i1.wp.com). We replace specific patterns such as digits or location acronyms with \"*\" to cluster these sub-sites together.\n\nA full list of topics and their frequencies according to the model is in Appendix B.", "md": "Table 1 gives basic summary statistics of mmc4 (and fewer-faces/core subsets) compared to some other interleaved image/text corpora. Overall, the full version of mmc4 is larger than prior non-public datasets across axes like number of images/number of documents. In addition, the various subsets of the corpus offer trade-offs between privacy, image/text similarity thresholds, etc. Figure 5 gives details about the mean/median number of images/sentences in each document (mean/median # sent.=2.0/5.7; # im = 13.0/24.3) based on a random sample of 22K documents.\n\nSources of documents & images.\n\nWe trace back the top-level domains of documents (webpages) and images to better understand the origins of contents in mmc4. Figure 6 presents the top-20 top-level domains that host the highest number of documents and images in mmc4. The distribution of document sources in mmc4 reveals a relatively uniform pattern, with 101.2M documents distributed across 6.0M unique domains. On average, each domain contains approximately 16.9 documents, with a median value of 2.0. The top 10% most frequently appeared domains account for 77% of all documents in mmc4. The documents are most commonly hosted on news media outlets (e.g., BBC, NY Times, Daily Express, Daily Mail), academic publication sites (e.g., Springer), online encyclopedias (e.g., Wikipedia), and e-commerce sites (e.g., iTunes, Etsy). Conversely, the sources of images in mmc4 exhibit a higher level of clustering. The 571.4M images are hosted on 4.9M domains, with each domain having an average of 116.0 images and a median value of 7.0 images. The top 10% most frequent domains are responsible for hosting 89% of all images. Images are most commonly hosted on blogs (e.g., Blogspot, WordPress), shopping sites (e.g., Amazon), cloud storage sites (e.g., AWS S3, Google storage), or general image hosting sites (e.g., Flickr, Imgur). More detailed lists of top document/image domains in mmc4 and mmc4-core can be found in Appendix C.\n\nImage-text similarity.\n\nFigure 4 provides detail about the linear assignment process compared to a \"max\" assignment alternative, where each image is simply assigned to its maximally CLIP-similar sentence. The linear assignment process slightly decreases the average CLIP similarity between images/sentences (from 24.5 to 24.0), but significantly more evenly \"spreads\" images throughout the documents: per-document, the mean percentage of sentences with an associated image rises from 22% to 34%.\n\nTopic-based assessment.\n\nWe ran LDA [6] as implemented by Mallet [22] on a random sample of 22K documents from mmc4 with k = 30 topics. The resulting clusters span a broad set of topics like cooking, communities, travel, music, art, etc. Figure 1 shows some example LDA topic clusters.18 In\n\n18 Multiple sub-sites may exist within a given domain (e.g., https://i0.wp.com and https://i1.wp.com). We replace specific patterns such as digits or location acronyms with \"*\" to cluster these sub-sites together.\n\nA full list of topics and their frequencies according to the model is in Appendix B."}]}, {"page": 7, "text": "                                                                                                                           8.745\n                                                                                                                    8\n                                                                                                                   Percentage (%)\n     0.10       0.0990.099                                                                                          6\n    Percentage (%)       0.075\n                             0.069                                                                                  4\n                                 0.057\n                                     0.0530.052\n                                              0.051\n                                                  0.048\n                                                      0.047                                                         2\n     0.05                                                 0.0430.043                                                           1.318\n                                                                                                                                   1.253\n                                                                   0.041                                                               1.243\n                                                                       0.041                                                              1.136\n                                                                           0.039\n                                                                               0.038                                                          0.884\n                                                                                   0.0370.037\n                                                                                            0.037\n                                                                                                0.036                                             0.699\n                                                                                                                                                      0.664\n                                                                                                                                                          0.507\n                                                                                                                                                              0.368\n                                                                                                                                                                  0.347\n                                                                                                                                                                      0.340\n                                                                                                                                                                          0.286\n                                                                                                                                                                              0.276\n                                                                                                                                                                                  0.244\n                                                                                                                                                                                      0.227\n                                                                                                                                                                                          0.195\n                                                                                                                                                                                              0.193\n                                                                                                                                                                                                  0.192\n                                                                                                                    0                                                                                 0.182\n     0.00                                                                                                           i*.wp.com             i.ytimg.com         i.imgur.com\n                                                                                                                  *.staticflickr.com i.pinimg.com                   *.bstatic.com\n                                                                                                         *.bp.blogspot.com                  res.cloudinary.com           img.youtube.com\n www.bbc.com                www.rt.com              www.cnn.com       www.npr.org                                      s3.amazonaws.comi*.photobucket.com                         is*-ssl.mzstatic.com\n                                   www.etsy.com                        www.wired.com                                                       storage.googleapis.com       photos.smugmug.com\n          www.nytimes.com  itunes.apple.com         www.booking.com                                                    static*.squarespace.com\n www.springer.com                                           www.firstpost.com                                                                       lh*.googleusercontent.com\n             www.express.co.uk            fineartamerica.com            www.breitbart.com\n    www.wikipedia.com                                                      www.indiamart.com\n                www.dailymail.co.uk                  www.tripadvisor.com                                                                                             cdn.photos.sparkplatform.com\n                              www.agreatertown.com                                                     images-*.ssl-images-amazon.com                   s-media-cache-ak*.pinimg.com\n                       app-wiringdiagram.herokuapp.com                                                                       primarysite-prod-sorted.s3.amazonaws.com\n(a) Top-20 most frequent domains for mmc4 documents.(b) Top-20 most frequent domains for images in mmc4.18\n          Figure 6: The top-20 most frequent top-level domains for documents and images in mmc4.\naddition, we explore a sample of the images most associated with the corresponding topic,20 finding\nthat, in general, image topic clusters align with qualitative expectations.\nManual verification of image relevance+properties.                                                                  We randomly sample 200 documents from\nmmc4 with the goal of assessing how relevant the images contained in the document are to the assigned\nsentences and to the document as a whole. Table 3 shows the results on the 836 images contained\nin the 200 documents. 87.7% of all examined images are topically related to the corresponding\ndocument, and 80.4% images are well-aligned to the assigned sentences within each document.21\nWe also assessed several other factors, finding that: 1) 28.3% contain recognizable human faces;\n2) 1.6% contain recognizable watermarks; 3) 3.9% are related to logos;22 4) 3.2% are related to\nadvertisements; and 5) 0.7% are duplicated with other images in the same document. Appendix D.1\nshows more discussion of images with watermarks, ads/logos, etc.\n5        OpenFlamingo: An Early Application of mmc4\nThe first publicly available model to be trained on mmc4 is OpenFlamingo [3]. We run ablations on a\nsmall version of OpenFlamingo (3B: backbone = OPT-1.3B [37] language model and CLIP ViT-L/14\n[24] vision model) to compare direct training on image captions (LAION-2B [26]) to the interleaved\nsequences of mmc4-core.23 To flatten mmc4 documents to training sequences,24 we: 1) sample a 256\ntoken sub-sequence from each training document; 2) discard images with CLIP image-text similarity\nless than 20; 3) discard sequences that contain no images after filtering; 4) discard images if there are\nmore than 5 in the resulting sequence.25 As in [17] we randomly drop sequences with a single image\nto increase multi-image sequences in the sample.\n     20 We compute the mean CLIP ViT-L/14 image vector for each topic by associating each image in a document\nthe document\u2019s most common topic; then, we compute the mean image vector per topic. Finally, cosine similarity\nto this mean vector is used to identify the \u201cmost topically central\u201d images per-topic.\n     21 The alignment between an image and its assigned sentence is a qualitative criterion. We consider an\nimage-sentence pair to be \u201cwell-aligned\u201d when the visual elements of the image have a direct and relevant\nrelationship with the text. This can include instances where the image depicts the context or content of the\nsentence, or where there is a plausible literal overlap between the text and the image, etc.\n     22 The logos can be website logos, commercial logos used by businesses or companies to represent their brand\nor product, or logos for organizations or events. In all cases, the label is assigned if the logo is the primary focus\nof the image.\n     23 These experiments were conducted using a preliminary v1 of the mmc4-core corpus, see this pull request\nfor discussion of small bugfixes in the current v1.1 version.\n     24 Future work would be well-suited to investigate the impact of various flattening schemes on downstream\nperformance; the method described here is just one possible method.\n     25 Similar to [2], we find that training on a maximum of fi                                                  ve image sequences can be sufficient for Open-\nFlamingo models to generalize to 32 shots during inference.\n                                                                                                    7", "md": "## 8.745\n\n## 8\n\n## Percentage (%)\n\n|0.10|0.099|0.099|\n|---|---|---|\n|Percentage (%)|0.075| |\n|0.069| | |\n|0.057|0.053|0.052|\n|0.051|0.048|0.047|\n|0.05|0.043|0.043|\n| |0.041|1.243|\n| |0.041|1.136|\n| |0.039| |\n| |0.038|0.884|\n|0.037|0.037|0.037|\n| |0.036|0.699|\n| |0.664| |\n| |0.507| |\n| |0.368| |\n| |0.347| |\n| |0.340| |\n| |0.286| |\n| |0.276| |\n| |0.244| |\n| |0.227| |\n| |0.195| |\n| |0.193| |\n| |0.192| |\n| |0.182| |\n|0.00| | |\n\ni*.wp.com, i.ytimg.com, i.imgur.com, *.staticflickr.com, i.pinimg.com, *.bstatic.com, *.bp.blogspot.com, res.cloudinary.com, img.youtube.com, www.bbc.com, www.rt.com, www.cnn.com, www.npr.org, s3.amazonaws.com, i*.photobucket.com, is*-ssl.mzstatic.com, www.etsy.com, www.wired.com, storage.googleapis.com, photos.smugmug.com, www.nytimes.com, itunes.apple.com, www.booking.com, static*.squarespace.com, www.springer.com, www.firstpost.com, lh*.googleusercontent.com, www.express.co.uk, fineartamerica.com, www.breitbart.com, www.wikipedia.com, www.indiamart.com, www.dailymail.co.uk, www.tripadvisor.com, cdn.photos.sparkplatform.com, www.agreatertown.com, images-*.ssl-images-amazon.com, s-media-cache-ak*.pinimg.com, app-wiringdiagram.herokuapp.com, primarysite-prod-sorted.s3.amazonaws.com\n\n(a) Top-20 most frequent domains for mmc4 documents.\n(b) Top-20 most frequent domains for images in mmc4.18\n\nFigure 6: The top-20 most frequent top-level domains for documents and images in mmc4.\n\nIn addition, we explore a sample of the images most associated with the corresponding topic,20 finding that, in general, image topic clusters align with qualitative expectations.\n\nManual verification of image relevance+properties. We randomly sample 200 documents from mmc4 with the goal of assessing how relevant the images contained in the document are to the assigned sentences and to the document as a whole. Table 3 shows the results on the 836 images contained in the 200 documents. 87.7% of all examined images are topically related to the corresponding document, and 80.4% images are well-aligned to the assigned sentences within each document.21 We also assessed several other factors, finding that: 1) 28.3% contain recognizable human faces; 2) 1.6% contain recognizable watermarks; 3) 3.9% are related to logos;22 4) 3.2% are related to advertisements; and 5) 0.7% are duplicated with other images in the same document. Appendix D.1 shows more discussion of images with watermarks, ads/logos, etc.\n\n### 5 OpenFlamingo: An Early Application of mmc4\n\nThe first publicly available model to be trained on mmc4 is OpenFlamingo [3]. We run ablations on a small version of OpenFlamingo (3B: backbone = OPT-1.3B [37] language model and CLIP ViT-L/14 [24] vision model) to compare direct training on image captions (LAION-2B [26]) to the interleaved sequences of mmc4-core.23 To flatten mmc4 documents to training sequences,24 we: 1) sample a 256 token sub-sequence from each training document; 2) discard images with CLIP image-text similarity less than 20; 3) discard sequences that contain no images after filtering; 4) discard images if there are more than 5 in the resulting sequence.25 As in [17] we randomly drop sequences with a single image to increase multi-image sequences in the sample.\n\n20 We compute the mean CLIP ViT-L/14 image vector for each topic by associating each image in a document the document\u2019s most common topic; then, we compute the mean image vector per topic. Finally, cosine similarity to this mean vector is used to identify the \u201cmost topically central\u201d images per-topic.\n\n21 The alignment between an image and its assigned sentence is a qualitative criterion. We consider an image-sentence pair to be \u201cwell-aligned\u201d when the visual elements of the image have a direct and relevant relationship with the text. This can include instances where the image depicts the context or content of the sentence, or where there is a plausible literal overlap between the text and the image, etc.\n\n22 The logos can be website logos, commercial logos used by businesses or companies to represent their brand or product, or logos for organizations or events. In all cases, the label is assigned if the logo is the primary focus of the image.\n\n23 These experiments were conducted using a preliminary v1 of the mmc4-core corpus, see this pull request for discussion of small bugfixes in the current v1.1 version.\n\n24 Future work would be well-suited to investigate the impact of various flattening schemes on downstream performance; the method described here is just one possible method.\n\n25 Similar to [2], we find that training on a maximum of five image sequences can be sufficient for Open- Flamingo models to generalize to 32 shots during inference.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "8.745", "md": "## 8.745"}, {"type": "heading", "lvl": 2, "value": "8", "md": "## 8"}, {"type": "heading", "lvl": 2, "value": "Percentage (%)", "md": "## Percentage (%)"}, {"type": "table", "rows": [["0.10", "0.099", "0.099"], ["Percentage (%)", "0.075", ""], ["0.069", "", ""], ["0.057", "0.053", "0.052"], ["0.051", "0.048", "0.047"], ["0.05", "0.043", "0.043"], ["", "0.041", "1.243"], ["", "0.041", "1.136"], ["", "0.039", ""], ["", "0.038", "0.884"], ["0.037", "0.037", "0.037"], ["", "0.036", "0.699"], ["", "0.664", ""], ["", "0.507", ""], ["", "0.368", ""], ["", "0.347", ""], ["", "0.340", ""], ["", "0.286", ""], ["", "0.276", ""], ["", "0.244", ""], ["", "0.227", ""], ["", "0.195", ""], ["", "0.193", ""], ["", "0.192", ""], ["", "0.182", ""], ["0.00", "", ""]], "md": "|0.10|0.099|0.099|\n|---|---|---|\n|Percentage (%)|0.075| |\n|0.069| | |\n|0.057|0.053|0.052|\n|0.051|0.048|0.047|\n|0.05|0.043|0.043|\n| |0.041|1.243|\n| |0.041|1.136|\n| |0.039| |\n| |0.038|0.884|\n|0.037|0.037|0.037|\n| |0.036|0.699|\n| |0.664| |\n| |0.507| |\n| |0.368| |\n| |0.347| |\n| |0.340| |\n| |0.286| |\n| |0.276| |\n| |0.244| |\n| |0.227| |\n| |0.195| |\n| |0.193| |\n| |0.192| |\n| |0.182| |\n|0.00| | |", "isPerfectTable": true, "csv": "\"0.10\",\"0.099\",\"0.099\"\n\"Percentage (%)\",\"0.075\",\"\"\n\"0.069\",\"\",\"\"\n\"0.057\",\"0.053\",\"0.052\"\n\"0.051\",\"0.048\",\"0.047\"\n\"0.05\",\"0.043\",\"0.043\"\n\"\",\"0.041\",\"1.243\"\n\"\",\"0.041\",\"1.136\"\n\"\",\"0.039\",\"\"\n\"\",\"0.038\",\"0.884\"\n\"0.037\",\"0.037\",\"0.037\"\n\"\",\"0.036\",\"0.699\"\n\"\",\"0.664\",\"\"\n\"\",\"0.507\",\"\"\n\"\",\"0.368\",\"\"\n\"\",\"0.347\",\"\"\n\"\",\"0.340\",\"\"\n\"\",\"0.286\",\"\"\n\"\",\"0.276\",\"\"\n\"\",\"0.244\",\"\"\n\"\",\"0.227\",\"\"\n\"\",\"0.195\",\"\"\n\"\",\"0.193\",\"\"\n\"\",\"0.192\",\"\"\n\"\",\"0.182\",\"\"\n\"0.00\",\"\",\"\""}, {"type": "text", "value": "i*.wp.com, i.ytimg.com, i.imgur.com, *.staticflickr.com, i.pinimg.com, *.bstatic.com, *.bp.blogspot.com, res.cloudinary.com, img.youtube.com, www.bbc.com, www.rt.com, www.cnn.com, www.npr.org, s3.amazonaws.com, i*.photobucket.com, is*-ssl.mzstatic.com, www.etsy.com, www.wired.com, storage.googleapis.com, photos.smugmug.com, www.nytimes.com, itunes.apple.com, www.booking.com, static*.squarespace.com, www.springer.com, www.firstpost.com, lh*.googleusercontent.com, www.express.co.uk, fineartamerica.com, www.breitbart.com, www.wikipedia.com, www.indiamart.com, www.dailymail.co.uk, www.tripadvisor.com, cdn.photos.sparkplatform.com, www.agreatertown.com, images-*.ssl-images-amazon.com, s-media-cache-ak*.pinimg.com, app-wiringdiagram.herokuapp.com, primarysite-prod-sorted.s3.amazonaws.com\n\n(a) Top-20 most frequent domains for mmc4 documents.\n(b) Top-20 most frequent domains for images in mmc4.18\n\nFigure 6: The top-20 most frequent top-level domains for documents and images in mmc4.\n\nIn addition, we explore a sample of the images most associated with the corresponding topic,20 finding that, in general, image topic clusters align with qualitative expectations.\n\nManual verification of image relevance+properties. We randomly sample 200 documents from mmc4 with the goal of assessing how relevant the images contained in the document are to the assigned sentences and to the document as a whole. Table 3 shows the results on the 836 images contained in the 200 documents. 87.7% of all examined images are topically related to the corresponding document, and 80.4% images are well-aligned to the assigned sentences within each document.21 We also assessed several other factors, finding that: 1) 28.3% contain recognizable human faces; 2) 1.6% contain recognizable watermarks; 3) 3.9% are related to logos;22 4) 3.2% are related to advertisements; and 5) 0.7% are duplicated with other images in the same document. Appendix D.1 shows more discussion of images with watermarks, ads/logos, etc.", "md": "i*.wp.com, i.ytimg.com, i.imgur.com, *.staticflickr.com, i.pinimg.com, *.bstatic.com, *.bp.blogspot.com, res.cloudinary.com, img.youtube.com, www.bbc.com, www.rt.com, www.cnn.com, www.npr.org, s3.amazonaws.com, i*.photobucket.com, is*-ssl.mzstatic.com, www.etsy.com, www.wired.com, storage.googleapis.com, photos.smugmug.com, www.nytimes.com, itunes.apple.com, www.booking.com, static*.squarespace.com, www.springer.com, www.firstpost.com, lh*.googleusercontent.com, www.express.co.uk, fineartamerica.com, www.breitbart.com, www.wikipedia.com, www.indiamart.com, www.dailymail.co.uk, www.tripadvisor.com, cdn.photos.sparkplatform.com, www.agreatertown.com, images-*.ssl-images-amazon.com, s-media-cache-ak*.pinimg.com, app-wiringdiagram.herokuapp.com, primarysite-prod-sorted.s3.amazonaws.com\n\n(a) Top-20 most frequent domains for mmc4 documents.\n(b) Top-20 most frequent domains for images in mmc4.18\n\nFigure 6: The top-20 most frequent top-level domains for documents and images in mmc4.\n\nIn addition, we explore a sample of the images most associated with the corresponding topic,20 finding that, in general, image topic clusters align with qualitative expectations.\n\nManual verification of image relevance+properties. We randomly sample 200 documents from mmc4 with the goal of assessing how relevant the images contained in the document are to the assigned sentences and to the document as a whole. Table 3 shows the results on the 836 images contained in the 200 documents. 87.7% of all examined images are topically related to the corresponding document, and 80.4% images are well-aligned to the assigned sentences within each document.21 We also assessed several other factors, finding that: 1) 28.3% contain recognizable human faces; 2) 1.6% contain recognizable watermarks; 3) 3.9% are related to logos;22 4) 3.2% are related to advertisements; and 5) 0.7% are duplicated with other images in the same document. Appendix D.1 shows more discussion of images with watermarks, ads/logos, etc."}, {"type": "heading", "lvl": 3, "value": "5 OpenFlamingo: An Early Application of mmc4", "md": "### 5 OpenFlamingo: An Early Application of mmc4"}, {"type": "text", "value": "The first publicly available model to be trained on mmc4 is OpenFlamingo [3]. We run ablations on a small version of OpenFlamingo (3B: backbone = OPT-1.3B [37] language model and CLIP ViT-L/14 [24] vision model) to compare direct training on image captions (LAION-2B [26]) to the interleaved sequences of mmc4-core.23 To flatten mmc4 documents to training sequences,24 we: 1) sample a 256 token sub-sequence from each training document; 2) discard images with CLIP image-text similarity less than 20; 3) discard sequences that contain no images after filtering; 4) discard images if there are more than 5 in the resulting sequence.25 As in [17] we randomly drop sequences with a single image to increase multi-image sequences in the sample.\n\n20 We compute the mean CLIP ViT-L/14 image vector for each topic by associating each image in a document the document\u2019s most common topic; then, we compute the mean image vector per topic. Finally, cosine similarity to this mean vector is used to identify the \u201cmost topically central\u201d images per-topic.\n\n21 The alignment between an image and its assigned sentence is a qualitative criterion. We consider an image-sentence pair to be \u201cwell-aligned\u201d when the visual elements of the image have a direct and relevant relationship with the text. This can include instances where the image depicts the context or content of the sentence, or where there is a plausible literal overlap between the text and the image, etc.\n\n22 The logos can be website logos, commercial logos used by businesses or companies to represent their brand or product, or logos for organizations or events. In all cases, the label is assigned if the logo is the primary focus of the image.\n\n23 These experiments were conducted using a preliminary v1 of the mmc4-core corpus, see this pull request for discussion of small bugfixes in the current v1.1 version.\n\n24 Future work would be well-suited to investigate the impact of various flattening schemes on downstream performance; the method described here is just one possible method.\n\n25 Similar to [2], we find that training on a maximum of five image sequences can be sufficient for Open- Flamingo models to generalize to 32 shots during inference.", "md": "The first publicly available model to be trained on mmc4 is OpenFlamingo [3]. We run ablations on a small version of OpenFlamingo (3B: backbone = OPT-1.3B [37] language model and CLIP ViT-L/14 [24] vision model) to compare direct training on image captions (LAION-2B [26]) to the interleaved sequences of mmc4-core.23 To flatten mmc4 documents to training sequences,24 we: 1) sample a 256 token sub-sequence from each training document; 2) discard images with CLIP image-text similarity less than 20; 3) discard sequences that contain no images after filtering; 4) discard images if there are more than 5 in the resulting sequence.25 As in [17] we randomly drop sequences with a single image to increase multi-image sequences in the sample.\n\n20 We compute the mean CLIP ViT-L/14 image vector for each topic by associating each image in a document the document\u2019s most common topic; then, we compute the mean image vector per topic. Finally, cosine similarity to this mean vector is used to identify the \u201cmost topically central\u201d images per-topic.\n\n21 The alignment between an image and its assigned sentence is a qualitative criterion. We consider an image-sentence pair to be \u201cwell-aligned\u201d when the visual elements of the image have a direct and relevant relationship with the text. This can include instances where the image depicts the context or content of the sentence, or where there is a plausible literal overlap between the text and the image, etc.\n\n22 The logos can be website logos, commercial logos used by businesses or companies to represent their brand or product, or logos for organizations or events. In all cases, the label is assigned if the logo is the primary focus of the image.\n\n23 These experiments were conducted using a preliminary v1 of the mmc4-core corpus, see this pull request for discussion of small bugfixes in the current v1.1 version.\n\n24 Future work would be well-suited to investigate the impact of various flattening schemes on downstream performance; the method described here is just one possible method.\n\n25 Similar to [2], we find that training on a maximum of five image sequences can be sufficient for Open- Flamingo models to generalize to 32 shots during inference."}]}, {"page": 8, "text": "     60            Captions-only                     60\n                   + mmc4\n     50                                              50\n   Val CIDEr                                       Val CIDEr                                  Table 3: Results of manual verification\n     40                                              40                                       of 200 randomly sampled documents\n                                                                                              containing 836 images. A majority of\n     30   Zero Shot Caption-only                     30   Zero Shot Caption-only              images are topically relevant and well\n                                                                                              sentence-aligned.               The rate of water-\n     20                                              20                                       marks, ads, duplicates, etc. is low.\n     10                                              10                                                                 % of 836 images\n      0     5M Caps   10M Caps   15M Caps             0     5M Caps   10M Caps   15M Caps               Topically-related               87.7%\n          +2.5M mmc4  +5M mmc4  +7.5M mmc4                +2.5M mmc4  +5M mmc4  +7.5M mmc4              Sentence-aligned                80.4%\n               (a) 4-shot                                      (b) 8-shot                               Has face?                       28.3%\nFigure 7: Few shot, in-context MSCOCO captioning per-                                                   Has watermark?                    1.6%\nformance of OpenFlamingo-3B when training on just                                                       Logo-related                      3.9%\ncaptions from LAION-2B vs. mixing in mmc4-core se-                                                      Ads-related                       3.2%\nquences. The model trained on mmc4 sequences is able to                                                 Duplicated                        0.7%\ngeneralize to MSCOCO-style captions more effectively vs.\nthe model trained just on LAION-2B image/caption pairs.\n(Zero shot caption-only=15M caption LAION-2B model)\nValidation CIDEr [35] results for COCO image captioning are in Figure 7. For 4/8-shot in-context\nlearning settings, the model trained on mmc4-core shows 20-30 CIDEr point improvements. The\nperformance of OpenFlamingo-3B trained on just 5M captions/2.5M mmc4 sequences also exceeds\na zero-shot application of OpenFlamingo-3B trained on much more data (15M LAION-2B cap-\ntions); this provides additional evidence that the interleaving in-context setup enables adaptation to\nMSCOCO-style captions. The performance of the captions-only OpenFlamingo-3B model degrades\nfrom 4-shot to 8-shot learning presumably because these longer sequences are significantly different\nfrom the single image/captions it\u2019s seen at training time.\n6      Conclusion\nWe introduce mmc4, a corpus of 100M+ documents with 571M images interleaved in 43B English\ntokens from the popular c4 dataset. Initial experimental results show that models trained on image/text\nsequences from mmc4 can more effectively perform multimodal in-context learning compared to\nmodels trained on single image/captions. We expect interleaving will be important not only for\nfew-shot learning, but also for more diverse multimodal language technologies wherein users may\nseek to converse with agents with and about visual content in new ways. Future work includes:\n1. More precise empirical evaluation of in-context abilities: can models really reason across im-\n     ages/texts in a prompt in flexible ways, or are they limited to interleaved and independent super-\n     vised examples?\n2. Data scaling: is the performance of in-context vision+language learning bottlenecked by the\n     availability of large-scale interleaved corpora? Or is improved single-modal pretraining sufficient\n     to un-bottleneck multimodal models?\n3. Instruction tuning: while interleaving of independent supervised image+text examples enables\n     in-context learning, training an instruction-following multimodal model directly for this case is a\n     promising complementary direction.\nAcknowledgements\nWe thank the OpenFlamingo team, Sangho Lee, and Jiasen Lu for the helpful discussions, and for\nbeing early adopters of mmc4. In addition, we thank Jingkang Yang for helpful discussions inspiring\nmmc4-core. We thank Stability AI for the compute for the OpenFlamingo experiments. This work\nwas supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), the\n                                                                             8", "md": "# Document\n\n## Table 3: Results of manual verification of 200 randomly sampled documents containing 836 images\n\n| |Topically-related|Sentence-aligned|Has face?|Has watermark?|Logo-related|Ads-related|Duplicated|\n|---|---|---|---|---|---|---|---|\n|% of 836 images|87.7%|80.4%|28.3%|1.6%|3.9%|3.2%|0.7%|\n\n## Figure 7: Few shot, in-context MSCOCO captioning performance of OpenFlamingo-3B\n\n(a) 4-shot\n\n(b) 8-shot\n\nFigure 7: Few shot, in-context MSCOCO captioning performance of OpenFlamingo-3B when training on just captions from LAION-2B vs. mixing in mmc4-core sequences. The model trained on mmc4 sequences is able to generalize to MSCOCO-style captions more effectively vs. the model trained just on LAION-2B image/caption pairs. (Zero shot caption-only=15M caption LAION-2B model)\n\nValidation CIDEr [35] results for COCO image captioning are in Figure 7. For 4/8-shot in-context learning settings, the model trained on mmc4-core shows 20-30 CIDEr point improvements. The performance of OpenFlamingo-3B trained on just 5M captions/2.5M mmc4 sequences also exceeds a zero-shot application of OpenFlamingo-3B trained on much more data (15M LAION-2B captions); this provides additional evidence that the interleaving in-context setup enables adaptation to MSCOCO-style captions. The performance of the captions-only OpenFlamingo-3B model degrades from 4-shot to 8-shot learning presumably because these longer sequences are significantly different from the single image/captions it\u2019s seen at training time.\n\n## Conclusion\n\nWe introduce mmc4, a corpus of 100M+ documents with 571M images interleaved in 43B English tokens from the popular c4 dataset. Initial experimental results show that models trained on image/text sequences from mmc4 can more effectively perform multimodal in-context learning compared to models trained on single image/captions. We expect interleaving will be important not only for few-shot learning, but also for more diverse multimodal language technologies wherein users may seek to converse with agents with and about visual content in new ways.\n\nFuture work includes:\n\n1. More precise empirical evaluation of in-context abilities: can models really reason across images/texts in a prompt in flexible ways, or are they limited to interleaved and independent supervised examples?\n2. Data scaling: is the performance of in-context vision+language learning bottlenecked by the availability of large-scale interleaved corpora? Or is improved single-modal pretraining sufficient to un-bottleneck multimodal models?\n3. Instruction tuning: while interleaving of independent supervised image+text examples enables in-context learning, training an instruction-following multimodal model directly for this case is a promising complementary direction.\n\n## Acknowledgements\n\nWe thank the OpenFlamingo team, Sangho Lee, and Jiasen Lu for the helpful discussions, and for being early adopters of mmc4. In addition, we thank Jingkang Yang for helpful discussions inspiring mmc4-core. We thank Stability AI for the compute for the OpenFlamingo experiments. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), the", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Table 3: Results of manual verification of 200 randomly sampled documents containing 836 images", "md": "## Table 3: Results of manual verification of 200 randomly sampled documents containing 836 images"}, {"type": "table", "rows": [["", "Topically-related", "Sentence-aligned", "Has face?", "Has watermark?", "Logo-related", "Ads-related", "Duplicated"], ["% of 836 images", "87.7%", "80.4%", "28.3%", "1.6%", "3.9%", "3.2%", "0.7%"]], "md": "| |Topically-related|Sentence-aligned|Has face?|Has watermark?|Logo-related|Ads-related|Duplicated|\n|---|---|---|---|---|---|---|---|\n|% of 836 images|87.7%|80.4%|28.3%|1.6%|3.9%|3.2%|0.7%|", "isPerfectTable": true, "csv": "\"\",\"Topically-related\",\"Sentence-aligned\",\"Has face?\",\"Has watermark?\",\"Logo-related\",\"Ads-related\",\"Duplicated\"\n\"% of 836 images\",\"87.7%\",\"80.4%\",\"28.3%\",\"1.6%\",\"3.9%\",\"3.2%\",\"0.7%\""}, {"type": "heading", "lvl": 2, "value": "Figure 7: Few shot, in-context MSCOCO captioning performance of OpenFlamingo-3B", "md": "## Figure 7: Few shot, in-context MSCOCO captioning performance of OpenFlamingo-3B"}, {"type": "text", "value": "(a) 4-shot\n\n(b) 8-shot\n\nFigure 7: Few shot, in-context MSCOCO captioning performance of OpenFlamingo-3B when training on just captions from LAION-2B vs. mixing in mmc4-core sequences. The model trained on mmc4 sequences is able to generalize to MSCOCO-style captions more effectively vs. the model trained just on LAION-2B image/caption pairs. (Zero shot caption-only=15M caption LAION-2B model)\n\nValidation CIDEr [35] results for COCO image captioning are in Figure 7. For 4/8-shot in-context learning settings, the model trained on mmc4-core shows 20-30 CIDEr point improvements. The performance of OpenFlamingo-3B trained on just 5M captions/2.5M mmc4 sequences also exceeds a zero-shot application of OpenFlamingo-3B trained on much more data (15M LAION-2B captions); this provides additional evidence that the interleaving in-context setup enables adaptation to MSCOCO-style captions. The performance of the captions-only OpenFlamingo-3B model degrades from 4-shot to 8-shot learning presumably because these longer sequences are significantly different from the single image/captions it\u2019s seen at training time.", "md": "(a) 4-shot\n\n(b) 8-shot\n\nFigure 7: Few shot, in-context MSCOCO captioning performance of OpenFlamingo-3B when training on just captions from LAION-2B vs. mixing in mmc4-core sequences. The model trained on mmc4 sequences is able to generalize to MSCOCO-style captions more effectively vs. the model trained just on LAION-2B image/caption pairs. (Zero shot caption-only=15M caption LAION-2B model)\n\nValidation CIDEr [35] results for COCO image captioning are in Figure 7. For 4/8-shot in-context learning settings, the model trained on mmc4-core shows 20-30 CIDEr point improvements. The performance of OpenFlamingo-3B trained on just 5M captions/2.5M mmc4 sequences also exceeds a zero-shot application of OpenFlamingo-3B trained on much more data (15M LAION-2B captions); this provides additional evidence that the interleaving in-context setup enables adaptation to MSCOCO-style captions. The performance of the captions-only OpenFlamingo-3B model degrades from 4-shot to 8-shot learning presumably because these longer sequences are significantly different from the single image/captions it\u2019s seen at training time."}, {"type": "heading", "lvl": 2, "value": "Conclusion", "md": "## Conclusion"}, {"type": "text", "value": "We introduce mmc4, a corpus of 100M+ documents with 571M images interleaved in 43B English tokens from the popular c4 dataset. Initial experimental results show that models trained on image/text sequences from mmc4 can more effectively perform multimodal in-context learning compared to models trained on single image/captions. We expect interleaving will be important not only for few-shot learning, but also for more diverse multimodal language technologies wherein users may seek to converse with agents with and about visual content in new ways.\n\nFuture work includes:\n\n1. More precise empirical evaluation of in-context abilities: can models really reason across images/texts in a prompt in flexible ways, or are they limited to interleaved and independent supervised examples?\n2. Data scaling: is the performance of in-context vision+language learning bottlenecked by the availability of large-scale interleaved corpora? Or is improved single-modal pretraining sufficient to un-bottleneck multimodal models?\n3. Instruction tuning: while interleaving of independent supervised image+text examples enables in-context learning, training an instruction-following multimodal model directly for this case is a promising complementary direction.", "md": "We introduce mmc4, a corpus of 100M+ documents with 571M images interleaved in 43B English tokens from the popular c4 dataset. Initial experimental results show that models trained on image/text sequences from mmc4 can more effectively perform multimodal in-context learning compared to models trained on single image/captions. We expect interleaving will be important not only for few-shot learning, but also for more diverse multimodal language technologies wherein users may seek to converse with agents with and about visual content in new ways.\n\nFuture work includes:\n\n1. More precise empirical evaluation of in-context abilities: can models really reason across images/texts in a prompt in flexible ways, or are they limited to interleaved and independent supervised examples?\n2. Data scaling: is the performance of in-context vision+language learning bottlenecked by the availability of large-scale interleaved corpora? Or is improved single-modal pretraining sufficient to un-bottleneck multimodal models?\n3. Instruction tuning: while interleaving of independent supervised image+text examples enables in-context learning, training an instruction-following multimodal model directly for this case is a promising complementary direction."}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "We thank the OpenFlamingo team, Sangho Lee, and Jiasen Lu for the helpful discussions, and for being early adopters of mmc4. In addition, we thank Jingkang Yang for helpful discussions inspiring mmc4-core. We thank Stability AI for the compute for the OpenFlamingo experiments. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), the", "md": "We thank the OpenFlamingo team, Sangho Lee, and Jiasen Lu for the helpful discussions, and for being early adopters of mmc4. In addition, we thank Jingkang Yang for helpful discussions inspiring mmc4-core. We thank Stability AI for the compute for the OpenFlamingo experiments. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), the"}]}, {"page": 9, "text": "NSF AI Institute for Foundations of Machine Learning (IFML, CCF-2019844), Open Philanthropy,\nGoogle, and the Allen Institute for AI.\nReferences\n [1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\n      Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. Cm3: A\n      causal masked multimodal model of the internet. ArXiv, abs/2201.07520, 2022.\n [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\n      Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\n      language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\n [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\n      Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei\n      Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-\n      source framework for training large autoregressive vision-language models. arXiv preprint\n      arXiv:2308.01390, 2023.\n [4] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python:\n      analyzing text with the natural language toolkit. \" O\u2019Reilly Media, Inc.\", 2009.\n [5] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misog-\n      yny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.\n [6] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of\n      machine Learning research, 3(Jan):993\u20131022, 2003.\n [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n      Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n      few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n [8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing\n      web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.\n [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\n      Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\n      Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\n      Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson,\n      Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\n      Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\n      Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\n      Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\n      Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\n      Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\n      Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei,\n      Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm:\n      Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\n[10] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface:\n      Single-shot multi-level face localisation in the wild. In Proceedings of the IEEE/CVF conference\n      on computer vision and pattern recognition, 2020.\n[11] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. RedCaps: Web-curated image-\n      text data created by the people, for the people. In NeurIPS Datasets and Benchmarks, 2021.\n[12] Jesse Dodge, Maarten Sap, Ana Marasovi\u00b4    c, William Agnew, Gabriel Ilharco, Dirk Groeneveld,\n      Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the\n      colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in\n      Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic,\n      November 2021. Association for Computational Linguistics.\n                                                  9", "md": "# References\n\n## References\n\n|[1]|Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. *Cm3: A causal masked multimodal model of the internet.* ArXiv, abs/2201.07520, 2022.|\n|---|---|\n|[2]|Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. *Flamingo: a visual language model for few-shot learning.* arXiv preprint arXiv:2204.14198, 2022.|\n|[3]|Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. *Openflamingo: An open-source framework for training large autoregressive vision-language models.* arXiv preprint arXiv:2308.01390, 2023.|\n|[4]|Steven Bird, Ewan Klein, and Edward Loper. *Natural language processing with Python: analyzing text with the natural language toolkit.* O\u2019Reilly Media, Inc., 2009.|\n|[5]|Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. *Multimodal datasets: misogyny, pornography, and malignant stereotypes.* arXiv preprint arXiv:2110.01963, 2021.|\n|[6]|David M Blei, Andrew Y Ng, and Michael I Jordan. *Latent dirichlet allocation.* Journal of Machine Learning Research, 3(Jan):993\u20131022, 2003.|\n|[7]|Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. *Language models are few-shot learners.* Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.|\n|[8]|Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. *Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.* In CVPR, 2021.|\n|[9]|Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. *Palm: Scaling language modeling with pathways.* ArXiv, abs/2204.02311, 2022.|\n|[10]|Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. *Retinaface: Single-shot multi-level face localisation in the wild.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020.|\n|[11]|Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. *RedCaps: Web-curated image-text data created by the people, for the people.* In NeurIPS Datasets and Benchmarks, 2021.|\n|[12]|Jesse Dodge, Maarten Sap, Ana Marasovi\u00b4c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. *Documenting large webtext corpora: A case study on the colossal clean crawled corpus.* In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "table", "rows": [["[1]", "Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. *Cm3: A causal masked multimodal model of the internet.* ArXiv, abs/2201.07520, 2022."], ["[2]", "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. *Flamingo: a visual language model for few-shot learning.* arXiv preprint arXiv:2204.14198, 2022."], ["[3]", "Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. *Openflamingo: An open-source framework for training large autoregressive vision-language models.* arXiv preprint arXiv:2308.01390, 2023."], ["[4]", "Steven Bird, Ewan Klein, and Edward Loper. *Natural language processing with Python: analyzing text with the natural language toolkit.* O\u2019Reilly Media, Inc., 2009."], ["[5]", "Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. *Multimodal datasets: misogyny, pornography, and malignant stereotypes.* arXiv preprint arXiv:2110.01963, 2021."], ["[6]", "David M Blei, Andrew Y Ng, and Michael I Jordan. *Latent dirichlet allocation.* Journal of Machine Learning Research, 3(Jan):993\u20131022, 2003."], ["[7]", "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. *Language models are few-shot learners.* Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020."], ["[8]", "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. *Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.* In CVPR, 2021."], ["[9]", "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. *Palm: Scaling language modeling with pathways.* ArXiv, abs/2204.02311, 2022."], ["[10]", "Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. *Retinaface: Single-shot multi-level face localisation in the wild.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020."], ["[11]", "Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. *RedCaps: Web-curated image-text data created by the people, for the people.* In NeurIPS Datasets and Benchmarks, 2021."], ["[12]", "Jesse Dodge, Maarten Sap, Ana Marasovi\u00b4c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. *Documenting large webtext corpora: A case study on the colossal clean crawled corpus.* In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics."]], "md": "|[1]|Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. *Cm3: A causal masked multimodal model of the internet.* ArXiv, abs/2201.07520, 2022.|\n|---|---|\n|[2]|Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. *Flamingo: a visual language model for few-shot learning.* arXiv preprint arXiv:2204.14198, 2022.|\n|[3]|Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. *Openflamingo: An open-source framework for training large autoregressive vision-language models.* arXiv preprint arXiv:2308.01390, 2023.|\n|[4]|Steven Bird, Ewan Klein, and Edward Loper. *Natural language processing with Python: analyzing text with the natural language toolkit.* O\u2019Reilly Media, Inc., 2009.|\n|[5]|Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. *Multimodal datasets: misogyny, pornography, and malignant stereotypes.* arXiv preprint arXiv:2110.01963, 2021.|\n|[6]|David M Blei, Andrew Y Ng, and Michael I Jordan. *Latent dirichlet allocation.* Journal of Machine Learning Research, 3(Jan):993\u20131022, 2003.|\n|[7]|Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. *Language models are few-shot learners.* Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.|\n|[8]|Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. *Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.* In CVPR, 2021.|\n|[9]|Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. *Palm: Scaling language modeling with pathways.* ArXiv, abs/2204.02311, 2022.|\n|[10]|Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. *Retinaface: Single-shot multi-level face localisation in the wild.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020.|\n|[11]|Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. *RedCaps: Web-curated image-text data created by the people, for the people.* In NeurIPS Datasets and Benchmarks, 2021.|\n|[12]|Jesse Dodge, Maarten Sap, Ana Marasovi\u00b4c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. *Documenting large webtext corpora: A case study on the colossal clean crawled corpus.* In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.|", "isPerfectTable": true, "csv": "\"[1]\",\"Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. *Cm3: A causal masked multimodal model of the internet.* ArXiv, abs/2201.07520, 2022.\"\n\"[2]\",\"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. *Flamingo: a visual language model for few-shot learning.* arXiv preprint arXiv:2204.14198, 2022.\"\n\"[3]\",\"Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. *Openflamingo: An open-source framework for training large autoregressive vision-language models.* arXiv preprint arXiv:2308.01390, 2023.\"\n\"[4]\",\"Steven Bird, Ewan Klein, and Edward Loper. *Natural language processing with Python: analyzing text with the natural language toolkit.* O\u2019Reilly Media, Inc., 2009.\"\n\"[5]\",\"Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. *Multimodal datasets: misogyny, pornography, and malignant stereotypes.* arXiv preprint arXiv:2110.01963, 2021.\"\n\"[6]\",\"David M Blei, Andrew Y Ng, and Michael I Jordan. *Latent dirichlet allocation.* Journal of Machine Learning Research, 3(Jan):993\u20131022, 2003.\"\n\"[7]\",\"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. *Language models are few-shot learners.* Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.\"\n\"[8]\",\"Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. *Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.* In CVPR, 2021.\"\n\"[9]\",\"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. *Palm: Scaling language modeling with pathways.* ArXiv, abs/2204.02311, 2022.\"\n\"[10]\",\"Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. *Retinaface: Single-shot multi-level face localisation in the wild.* In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020.\"\n\"[11]\",\"Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. *RedCaps: Web-curated image-text data created by the people, for the people.* In NeurIPS Datasets and Benchmarks, 2021.\"\n\"[12]\",\"Jesse Dodge, Maarten Sap, Ana Marasovi\u00b4c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. *Documenting large webtext corpora: A case study on the colossal clean crawled corpus.* In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\""}]}, {"page": 10, "text": "[13] Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro\n     Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection\n     in google street view. In 2009 IEEE 12th international conference on computer vision, pages\n     2373\u20132380. IEEE, 2009.\n[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\n     Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim\n     Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe,\n     Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga\n     Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\n     Sewoong Oh, Alexandros G. Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig\n     Schmidt. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint\n     arXiv:2304.14108, 2023.\n[15] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna\n     Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the\n     ACM, 64(12):86\u201392, 2021.\n[16] Jack Hessel, Lillian Lee, and David Mimno. Unsupervised discovery of multimodal links in\n     multi-image, multi-sentence documents. In EMNLP, 2019.\n[17] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\n     Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,\n     Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need:\n     Aligning perception with language models. ArXiv, abs/2302.14045, 2023.\n[18] Roy Jonker and Ton Volgenant. A shortest augmenting path algorithm for dense and sparse\n     linear assignment problems. In DGOR/NSOR: Papers of the 16th Annual Meeting of DGOR in\n     Cooperation with NSOR/Vortr\u00e4ge der 16. Jahrestagung der DGOR zusammen mit der NSOR,\n     pages 622\u2013622. Springer, 1988.\n[19] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\n     quarterly, 2(1-2):83\u201397, 1955.\n[20] Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, and Xuanjing Huang. An unsupervised\n     sampling approach for image-sentence matching using document-level structural information.\n     In AAAI, 2021.\n[21] Emily E Marsh and Marilyn Domas White. A taxonomy of relationships between images and\n     text. Journal of documentation, 59(6):647\u2013672, 2003.\n[22] Andrew Kachites McCallum. Mallet: A machine learning for language toolkit, 2002.\n[23] Alex Mei, Michael Saxon, Shiyu Chang, Zachary C Lipton, and William Yang Wang. Users are\n     the north star for ai transparency. arXiv preprint arXiv:2303.05500, 2023.\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n     Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n     models from natural language supervision. In International conference on machine learning,\n     pages 8748\u20138763. PMLR, 2021.\n[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\n     Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\n     text-to-text transformer. JMLR, 2020.\n[26] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\n     Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-\n     5B: An open large-scale dataset for training next generation image-text models. arXiv preprint\n     arXiv:2210.08402, 2022.\n[27] Sefik Ilkin Serengil and Alper Ozpinar. Lightface: A hybrid deep face recognition framework.\n     In 2020 Innovations in Intelligent Systems and Applications Conference (ASYU), pages 23\u201327.\n     IEEE, 2020.\n                                                10", "md": "# References\n\n# List of References\n\n1. Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection in google street view. In 2009 IEEE 12th international conference on computer vision, pages 2373\u20132380. IEEE, 2009.\n2. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alexandros G. Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\n3. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\n4. Jack Hessel, Lillian Lee, and David Mimno. Unsupervised discovery of multimodal links in multi-image, multi-sentence documents. In EMNLP, 2019.\n5. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023.\n6. Roy Jonker and Ton Volgenant. A shortest augmenting path algorithm for dense and sparse linear assignment problems. In DGOR/NSOR: Papers of the 16th Annual Meeting of DGOR in Cooperation with NSOR/Vortr\u00e4ge der 16. Jahrestagung der DGOR zusammen mit der NSOR, pages 622\u2013622. Springer, 1988.\n7. Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.\n8. Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, and Xuanjing Huang. An unsupervised sampling approach for image-sentence matching using document-level structural information. In AAAI, 2021.\n9. Emily E Marsh and Marilyn Domas White. A taxonomy of relationships between images and text. Journal of documentation, 59(6):647\u2013672, 2003.\n10. Andrew Kachites McCallum. Mallet: A machine learning for language toolkit, 2002.\n11. Alex Mei, Michael Saxon, Shiyu Chang, Zachary C Lipton, and William Yang Wang. Users are the north star for ai transparency. arXiv preprint arXiv:2303.05500, 2023.\n12. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n13. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\n14. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION- 5B: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n15. Sefik Ilkin Serengil and Alper Ozpinar. Lightface: A hybrid deep face recognition framework. In 2020 Innovations in Intelligent Systems and Applications Conference (ASYU), pages 23\u201327. IEEE, 2020.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "1. Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection in google street view. In 2009 IEEE 12th international conference on computer vision, pages 2373\u20132380. IEEE, 2009.\n2. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alexandros G. Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\n3. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\n4. Jack Hessel, Lillian Lee, and David Mimno. Unsupervised discovery of multimodal links in multi-image, multi-sentence documents. In EMNLP, 2019.\n5. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023.\n6. Roy Jonker and Ton Volgenant. A shortest augmenting path algorithm for dense and sparse linear assignment problems. In DGOR/NSOR: Papers of the 16th Annual Meeting of DGOR in Cooperation with NSOR/Vortr\u00e4ge der 16. Jahrestagung der DGOR zusammen mit der NSOR, pages 622\u2013622. Springer, 1988.\n7. Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.\n8. Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, and Xuanjing Huang. An unsupervised sampling approach for image-sentence matching using document-level structural information. In AAAI, 2021.\n9. Emily E Marsh and Marilyn Domas White. A taxonomy of relationships between images and text. Journal of documentation, 59(6):647\u2013672, 2003.\n10. Andrew Kachites McCallum. Mallet: A machine learning for language toolkit, 2002.\n11. Alex Mei, Michael Saxon, Shiyu Chang, Zachary C Lipton, and William Yang Wang. Users are the north star for ai transparency. arXiv preprint arXiv:2303.05500, 2023.\n12. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n13. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\n14. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION- 5B: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n15. Sefik Ilkin Serengil and Alper Ozpinar. Lightface: A hybrid deep face recognition framework. In 2020 Innovations in Intelligent Systems and Applications Conference (ASYU), pages 23\u201327. IEEE, 2020.", "md": "1. Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection in google street view. In 2009 IEEE 12th international conference on computer vision, pages 2373\u20132380. IEEE, 2009.\n2. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alexandros G. Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\n3. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\n4. Jack Hessel, Lillian Lee, and David Mimno. Unsupervised discovery of multimodal links in multi-image, multi-sentence documents. In EMNLP, 2019.\n5. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023.\n6. Roy Jonker and Ton Volgenant. A shortest augmenting path algorithm for dense and sparse linear assignment problems. In DGOR/NSOR: Papers of the 16th Annual Meeting of DGOR in Cooperation with NSOR/Vortr\u00e4ge der 16. Jahrestagung der DGOR zusammen mit der NSOR, pages 622\u2013622. Springer, 1988.\n7. Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.\n8. Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, and Xuanjing Huang. An unsupervised sampling approach for image-sentence matching using document-level structural information. In AAAI, 2021.\n9. Emily E Marsh and Marilyn Domas White. A taxonomy of relationships between images and text. Journal of documentation, 59(6):647\u2013672, 2003.\n10. Andrew Kachites McCallum. Mallet: A machine learning for language toolkit, 2002.\n11. Alex Mei, Michael Saxon, Shiyu Chang, Zachary C Lipton, and William Yang Wang. Users are the north star for ai transparency. arXiv preprint arXiv:2303.05500, 2023.\n12. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n13. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\n14. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION- 5B: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n15. Sefik Ilkin Serengil and Alper Ozpinar. Lightface: A hybrid deep face recognition framework. In 2020 Innovations in Intelligent Systems and Applications Conference (ASYU), pages 23\u201327. IEEE, 2020."}]}, {"page": 11, "text": "[28] Sefik Ilkin Serengil and Alper Ozpinar. Hyperextended lightface: A facial attribute analysis\n     framework. In 2021 International Conference on Engineering and Emerging Technologies\n     (ICEET), pages 1\u20134. IEEE, 2021.\n[29] David So, Wojciech Ma\u00b4 nke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Searching\n     for efficient transformers for language modeling. In A. Beygelzimer, Y. Dauphin, P. Liang, and\n     J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n[30] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT:\n     Wikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR,\n     2021.\n[31] Jun Suzuki, Heiga Zen, and Hideto Kazawa. Extracting representative subset from extensive\n     text data for training pre-trained language models. Information Processing & Management,\n     60(3):103249, 2023.\n[32] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas\n     Poland, Damian Borth, and Li-Jia Li. YFCC100M: the new data in multimedia research.\n     Communications of the ACM, 59(2):64\u201373, 2016.\n[33] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha,\n     Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee,\n     Huaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry\n     Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten\n     Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett,\n     Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos,\n     Toju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz,\n     Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo,\n     Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron\n     Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak,\n     Ed Huai hsin Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv,\n     abs/2201.08239, 2022.\n[34] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\n     learning research, 9(11), 2008.\n[35] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image\n     description evaluation. In Proceedings of the IEEE conference on computer vision and pattern\n     recognition, pages 4566\u20134575, 2015.\n[36] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. Personalized show-\n     cases: Generating multi-modal explanations for recommendations. In Proceedings of the 46th\n     International ACM SIGIR Conference on Research and Development in Information Retrieval,\n     pages 2251\u20132255, 2023.\n[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\n     Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\n     Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\n     Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068,\n     2022.\n                                                11", "md": "[28] Sefik Ilkin Serengil and Alper Ozpinar. Hyperextended lightface: A facial attribute analysis\nframework. In 2021 International Conference on Engineering and Emerging Technologies\n(ICEET), pages 1\u20134. IEEE, 2021.\n\n[29] David So, Wojciech Ma\u00b4 nke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Searching\nfor efficient transformers for language modeling. In A. Beygelzimer, Y. Dauphin, P. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n\n[30] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT:\nWikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR,\n2021.\n\n[31] Jun Suzuki, Heiga Zen, and Hideto Kazawa. Extracting representative subset from extensive\ntext data for training pre-trained language models. Information Processing & Management,\n60(3):103249, 2023.\n\n[32] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas\nPoland, Damian Borth, and Li-Jia Li. YFCC100M: the new data in multimedia research.\nCommunications of the ACM, 59(2):64\u201373, 2016.\n\n[33] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha,\nHeng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee,\nHuaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry\nLepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten\nBosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett,\nKathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos,\nToju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz,\nBen Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo,\nRavindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron\nCohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd Huai hsin Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv,\nabs/2201.08239, 2022.\n\n[34] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\n\n[35] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4566\u20134575, 2015.\n\n[36] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. Personalized show-\ncases: Generating multi-modal explanations for recommendations. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval,\npages 2251\u20132255, 2023.\n\n[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068,\n2022.", "images": [], "items": [{"type": "text", "value": "[28] Sefik Ilkin Serengil and Alper Ozpinar. Hyperextended lightface: A facial attribute analysis\nframework. In 2021 International Conference on Engineering and Emerging Technologies\n(ICEET), pages 1\u20134. IEEE, 2021.\n\n[29] David So, Wojciech Ma\u00b4 nke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Searching\nfor efficient transformers for language modeling. In A. Beygelzimer, Y. Dauphin, P. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n\n[30] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT:\nWikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR,\n2021.\n\n[31] Jun Suzuki, Heiga Zen, and Hideto Kazawa. Extracting representative subset from extensive\ntext data for training pre-trained language models. Information Processing & Management,\n60(3):103249, 2023.\n\n[32] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas\nPoland, Damian Borth, and Li-Jia Li. YFCC100M: the new data in multimedia research.\nCommunications of the ACM, 59(2):64\u201373, 2016.\n\n[33] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha,\nHeng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee,\nHuaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry\nLepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten\nBosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett,\nKathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos,\nToju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz,\nBen Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo,\nRavindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron\nCohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd Huai hsin Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv,\nabs/2201.08239, 2022.\n\n[34] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\n\n[35] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4566\u20134575, 2015.\n\n[36] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. Personalized show-\ncases: Generating multi-modal explanations for recommendations. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval,\npages 2251\u20132255, 2023.\n\n[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068,\n2022.", "md": "[28] Sefik Ilkin Serengil and Alper Ozpinar. Hyperextended lightface: A facial attribute analysis\nframework. In 2021 International Conference on Engineering and Emerging Technologies\n(ICEET), pages 1\u20134. IEEE, 2021.\n\n[29] David So, Wojciech Ma\u00b4 nke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le. Searching\nfor efficient transformers for language modeling. In A. Beygelzimer, Y. Dauphin, P. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n\n[30] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT:\nWikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR,\n2021.\n\n[31] Jun Suzuki, Heiga Zen, and Hideto Kazawa. Extracting representative subset from extensive\ntext data for training pre-trained language models. Information Processing & Management,\n60(3):103249, 2023.\n\n[32] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas\nPoland, Damian Borth, and Li-Jia Li. YFCC100M: the new data in multimedia research.\nCommunications of the ACM, 59(2):64\u201373, 2016.\n\n[33] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha,\nHeng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee,\nHuaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry\nLepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten\nBosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett,\nKathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos,\nToju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz,\nBen Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo,\nRavindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron\nCohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd Huai hsin Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv,\nabs/2201.08239, 2022.\n\n[34] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\n\n[35] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image\ndescription evaluation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4566\u20134575, 2015.\n\n[36] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. Personalized show-\ncases: Generating multi-modal explanations for recommendations. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval,\npages 2251\u20132255, 2023.\n\n[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068,\n2022."}]}, {"page": 12, "text": "A     Dataset Card\nOur dataset card is available at https://github.com/allenai/mmc4/blob/main/DATASET_\nCARD.md\nB     Full Set of LDA Topics\nTable 4 contains the full set of topics for the k \u201c 30 LDA model introduced in \u00a7 4.\nTable 4: LDA[6] topic modeling outputs (k=30 topics) when trained on a random sample of documents\nfrom mmc4. Topic frequencies are determined by taking the mean distribution over documents in\nthe corpus. Topic names are generated by GPT-4 conditioned on the top 20 words for each topic,\nprompted by a request for a short 1-2 word summary.\nTopic name              Rate      Top Words\nE-commerce              4.61%     products, quality, price, product, online, offer, buy, customers, services,\n                                  order\nHealthcare              2.55%     health, care, body, patients, treatment, medical, pain, cancer, blood,\n                                  mental\nTravel                  3.98%     city, hotel, park, visit, travel, trip, tour, enjoy, beach, town\nCelebrations            3.94%     fun, wedding, beautiful, christmas, happy, card, birthday, gift, blog,\n                                  perfect\nMusic                   2.50%     music, band, album, song, sound, songs, dance, show, live, musical\nReligion                2.05%     god, church, jesus, lord, faith, man, father, heart, christ, gods\nFashion                 4.86%     black, white, size, color, design, wear, style, fabric, cut, fit\nNature                  3.05%     water, dog, river, fish, dogs, species, animals, fishing, sea, weather\nGeography               3.56%     city, county, state, york, san, north, west, st, john, south\nBusiness                4.15%     management, company, marketing, technology, data, services, team,\n                                  industry, project, clients\nTechnology              4.89%     page, app, site, download, website, data, click, google, web, email\nEducation               2.39%     students, school, learning, skills, children, education, learn, student,\n                                  training, class\nResearch                1.43%     data, download, research, analysis, study, al, cells, memory, studies,\n                                  results\nFood                    3.31%     food, add, recipe, minutes, chocolate, cream, delicious, chicken, sugar,\n                                  cheese\nLaw                     2.14%     law, insurance, court, legal, case, state, letter, act, cover, policy\nWellness                1.92%     skin, hair, oil, natural, organic, wine, plant, products, plants, water\nSelf-improvement        5.27%     change, youre, mind, point, means, fact, thing, ways, question, process\nPolitics                2.73%     government, president, police, political, war, trump, military, state, party,\n                                  security\nEngineering             2.81%     water, energy, system, power, air, temperature, heat, systems, gas, solar\nSports                  3.01%     game, games, team, play, season, players, win, league, player, football\nEconomy                 2.29%     percent, market, million, \u2014, trade, billion, growth, price, company,\n                                  report\nArchitecture            3.08%     room, space, house, kitchen, floor, living, pool, building, large, bedroom\nAutomotive              3.20%     car, vehicle, camera, engine, power, system, model, control, speed, phone\nCommunity               3.91%     community, university, program, research, members, support, develop-\n                                  ment, public, national, group\nFinance                 1.72%     money, credit, card, real, property, estate, loan, pay, financial, tax\nInternational           2.31%     international, india, countries, china, south, history, united, country,\n                                  europe, indian\nEvents                  3.93%     2018, event, pm, 2019, 2017, april, 2016, posted, friday, june\nLiterature              3.73%     book, story, books, film, series, movie, read, characters, stories, reading\nPersonal                7.96%     ive, didnt, thing, bit, thought, week, wanted, started, pretty, id\nArt                     2.70%     art, design, de, images, ikea, image, painting, collection, piano, photo\n                                                       12", "md": "# Dataset Card\n\n## A Dataset Card\n\nOur dataset card is available at https://github.com/allenai/mmc4/blob/main/DATASET_CARD.md\n\n## Full Set of LDA Topics\n\nTable 4 contains the full set of topics for the k = 30 LDA model introduced in \u00a7 4.\n\n**Table 4: LDA[6] topic modeling outputs (k=30 topics) when trained on a random sample of documents from mmc4.**\n|Topic name|Rate|Top Words|\n|---|---|---|\n|E-commerce|4.61%|products, quality, price, product, online, offer, buy, customers, services, order|\n|Healthcare|2.55%|health, care, body, patients, treatment, medical, pain, cancer, blood, mental|\n|Travel|3.98%|city, hotel, park, visit, travel, trip, tour, enjoy, beach, town|\n|\n|Art|2.70%|art, design, de, images, ikea, image, painting, collection, piano, photo|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Dataset Card", "md": "# Dataset Card"}, {"type": "heading", "lvl": 2, "value": "A Dataset Card", "md": "## A Dataset Card"}, {"type": "text", "value": "Our dataset card is available at https://github.com/allenai/mmc4/blob/main/DATASET_CARD.md", "md": "Our dataset card is available at https://github.com/allenai/mmc4/blob/main/DATASET_CARD.md"}, {"type": "heading", "lvl": 2, "value": "Full Set of LDA Topics", "md": "## Full Set of LDA Topics"}, {"type": "text", "value": "Table 4 contains the full set of topics for the k = 30 LDA model introduced in \u00a7 4.\n\n**Table 4: LDA[6] topic modeling outputs (k=30 topics) when trained on a random sample of documents from mmc4.**", "md": "Table 4 contains the full set of topics for the k = 30 LDA model introduced in \u00a7 4.\n\n**Table 4: LDA[6] topic modeling outputs (k=30 topics) when trained on a random sample of documents from mmc4.**"}, {"type": "table", "rows": [["Topic name", "Rate", "Top Words"], ["E-commerce", "4.61%", "products, quality, price, product, online, offer, buy, customers, services, order"], ["Healthcare", "2.55%", "health, care, body, patients, treatment, medical, pain, cancer, blood, mental"], ["Travel", "3.98%", "city, hotel, park, visit, travel, trip, tour, enjoy, beach, town"], [], ["Art", "2.70%", "art, design, de, images, ikea, image, painting, collection, piano, photo"]], "md": "|Topic name|Rate|Top Words|\n|---|---|---|\n|E-commerce|4.61%|products, quality, price, product, online, offer, buy, customers, services, order|\n|Healthcare|2.55%|health, care, body, patients, treatment, medical, pain, cancer, blood, mental|\n|Travel|3.98%|city, hotel, park, visit, travel, trip, tour, enjoy, beach, town|\n|\n|Art|2.70%|art, design, de, images, ikea, image, painting, collection, piano, photo|", "isPerfectTable": false, "csv": "\"Topic name\",\"Rate\",\"Top Words\"\n\"E-commerce\",\"4.61%\",\"products, quality, price, product, online, offer, buy, customers, services, order\"\n\"Healthcare\",\"2.55%\",\"health, care, body, patients, treatment, medical, pain, cancer, blood, mental\"\n\"Travel\",\"3.98%\",\"city, hotel, park, visit, travel, trip, tour, enjoy, beach, town\"\n\n\"Art\",\"2.70%\",\"art, design, de, images, ikea, image, painting, collection, piano, photo\""}]}, {"page": 13, "text": "C        Most Frequent Top-Level Domains\nTable 5 and Table 6 list the top-50 most frequent top-level domains for documents and images as\ndiscussed in \u00a7 4. We show domain statistics in both mmc4 and mmc4-core.\nFigure 8 shows the top-50 top-level domains of documents in c4-en for reference purposes. The\ndomains are sorted by the frequency of occurrence, as the same with results presented in Figure 6.\nPrevious work also discussed the most represented websites in c4-en ranked by the total number of\ntokens [12].\n           0.10     0.1014\n          Percentage (%)\n           0.08\n           0.06        0.0418\n                          0.0397\n                             0.0370\n                                0.0354\n           0.04                    0.0354\n                                       0.0288\n                                          0.0286\n                                             0.0250\n                                                0.0246\n                                                   0.0241\n                                                      0.0241\n                                                         0.0217\n                                                            0.0210\n                                                               0.0208\n                                                                  0.0205\n                                                                     0.0202\n                                                                        0.0199\n                                                                           0.0194\n                                                                              0.0187\n                                                                                  0.0186\n                                                                                     0.0185\n                                                                                        0.0184\n                                                                                           0.0182\n                                                                                              0.0177\n                                                                                                 0.0177\n                                                                                                    0.0171\n                                                                                                       0.0168\n                                                                                                          0.0167\n                                                                                                             0.0166\n                                                                                                                0.0161\n                                                                                                                   0.0161\n                                                                                                                      0.0161\n                                                                                                                          0.0161\n                                                                                                                             0.0161\n                                                                                                                                0.0160\n                                                                                                                                   0.0159\n                                                                                                                                      0.0159\n                                                                                                                                         0.0158\n                                                                                                                                            0.0155\n                                                                                                                                               0.0154\n                                                                                                                                                  0.0154\n           0.02                                                                                                                                      0.0152\n                                                                                                                                                        0.0150\n                                                                                                                                                           0.0150\n                                                                                                                                                              0.0148\n                                                                                                                                                                 0.0147\n                                                                                                                                                                     0.0146\n                                                                                                                                                                        0.0146\n                                                                                                                                                                           0.0145\n           0.00                                                                                                                      github.com     lists.w3.org\n                                                                                                  www.rt.com         www.si.com                              nypost.com\n              www.bbc.com                                www.cnn.com                                            www.fool.comwww.etsy.com                 www.npr.org\n                                                                 www.cnet.com             mashable.com\n                                                                                www.zdnet.com\n                              www.forbes.com                          www.oreilly.com            itunes.apple.com   www.prweb.com  answers.sap.com\n                 www.latimes.com                                                                          www.reuters.com\n       www.nytimes.com                                                                                www.booking.com\n                   do5.b00kmedia.ru\n             www.springer.com                  www.foxnews.com                                                                       www.youtube.com\n                         www.huffpost.com                                       www.ibtimes.co.uk\n                                                                 www.express.co.uk                                                     www.usatoday.com\n   www.wikipedia.com          patents.google.com       www.aljazeera.com                                                                  www.reference.com\n                                       www.dailymail.co.uk                              www.deviantart.com\n                                 www.eventbrite.com                                                                                                     www.tripadvisor.com\n                                                www.telegraph.co.uk     www.theatlantic.com\n                                                        www.ncbi.nlm.nih.gov\n                   www.theguardian.com                               www.foxbusiness.com                          www.instructables.com\n                                      www.chicagotribune.com                                   forums.macrumors.com    www.agreatertown.com\n                                www.businessinsider.com                       www.washingtonpost.com                                       www.hindustantimes.com\n                  Figure 8: The top-50 most frequent top-level domains for documents in c4-en.\nD        Demonstrative Examples\nD.1        Images w/ Watermarks/Ads/Logos\nFigure 9a depicts a few sample images containing watermarks in various forms, Figure 9b shows\nimages that are associated with logos, and Figure 9c lists a few sample images related to advertise-\nments. Notice that the dissimilarity between images associated with logos and those pertaining to\nadvertisements is relatively modest. Although images connected to advertisements may occasionally\nencompass promotional language or persuasive expressions, they may also solely feature logos.\nNotably, the principal criterion for determining whether an image is ad-related is contingent upon\nassessing its relevance to the document. If the image is less related to the document, it is more aptly\ncategorized as ad-related. For instance, the interleaved document presented in Table 7 contains two\nimages associated with logos that are intricately linked to the commercial brand being presented\nwithin the document. Consequently, these two images are not classified as advertisements.\nD.2        Interleaved Document\nTable 7 and Table 8 show two interleaved docs from mmc4, displaying the list of sentences and the\ncorresponding assigned images, alongside the CLIP ViT/L-14 image-text similarity score.\n                                                                                       13", "md": "# Document\n\n## Most Frequent Top-Level Domains\n\nTable 5 and Table 6 list the top-50 most frequent top-level domains for documents and images as discussed in \u00a7 4. We show domain statistics in both mmc4 and mmc4-core.\n\nFigure 8 shows the top-50 top-level domains of documents in c4-en for reference purposes. The domains are sorted by the frequency of occurrence, as the same with results presented in Figure 6. Previous work also discussed the most represented websites in c4-en ranked by the total number of tokens [12].\n\nPercentage (%)\n0.10\n0.08\n0.06\n0.04\n0.02\n0.00\n\ngithub.com, lists.w3.org, www.rt.com, www.si.com, nypost.com, www.bbc.com, www.cnn.com, www.fool.com, www.etsy.com, www.npr.org, www.cnet.com, mashable.com, www.zdnet.com, www.forbes.com, www.oreilly.com, itunes.apple.com, www.prweb.com, answers.sap.com, www.latimes.com, www.nytimes.com, do5.b00kmedia.ru, www.springer.com, www.foxnews.com, www.youtube.com, www.huffpost.com, www.ibtimes.co.uk, www.express.co.uk, www.usatoday.com, www.wikipedia.com, patents.google.com, www.aljazeera.com, www.reference.com, www.reuters.com, www.booking.com, www.deviantart.com, www.eventbrite.com, www.tripadvisor.com, www.telegraph.co.uk, www.theatlantic.com, www.ncbi.nlm.nih.gov, www.theguardian.com, www.foxbusiness.com, www.instructables.com, www.chicagotribune.com, forums.macrumors.com, www.agreatertown.com, www.businessinsider.com, www.washingtonpost.com, www.hindustantimes.com\n\nFigure 8: The top-50 most frequent top-level domains for documents in c4-en.\n\n## Demonstrative Examples\n\n### Images w/ Watermarks/Ads/Logos\n\nFigure 9a depicts a few sample images containing watermarks in various forms, Figure 9b shows images that are associated with logos, and Figure 9c lists a few sample images related to advertisements. Notice that the dissimilarity between images associated with logos and those pertaining to advertisements is relatively modest. Although images connected to advertisements may occasionally encompass promotional language or persuasive expressions, they may also solely feature logos. Notably, the principal criterion for determining whether an image is ad-related is contingent upon assessing its relevance to the document. If the image is less related to the document, it is more aptly categorized as ad-related. For instance, the interleaved document presented in Table 7 contains two images associated with logos that are intricately linked to the commercial brand being presented within the document. Consequently, these two images are not classified as advertisements.\n\n### Interleaved Document\n\nTable 7 and Table 8 show two interleaved docs from mmc4, displaying the list of sentences and the corresponding assigned images, alongside the CLIP ViT/L-14 image-text similarity score.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Most Frequent Top-Level Domains", "md": "## Most Frequent Top-Level Domains"}, {"type": "text", "value": "Table 5 and Table 6 list the top-50 most frequent top-level domains for documents and images as discussed in \u00a7 4. We show domain statistics in both mmc4 and mmc4-core.\n\nFigure 8 shows the top-50 top-level domains of documents in c4-en for reference purposes. The domains are sorted by the frequency of occurrence, as the same with results presented in Figure 6. Previous work also discussed the most represented websites in c4-en ranked by the total number of tokens [12].\n\nPercentage (%)\n0.10\n0.08\n0.06\n0.04\n0.02\n0.00\n\ngithub.com, lists.w3.org, www.rt.com, www.si.com, nypost.com, www.bbc.com, www.cnn.com, www.fool.com, www.etsy.com, www.npr.org, www.cnet.com, mashable.com, www.zdnet.com, www.forbes.com, www.oreilly.com, itunes.apple.com, www.prweb.com, answers.sap.com, www.latimes.com, www.nytimes.com, do5.b00kmedia.ru, www.springer.com, www.foxnews.com, www.youtube.com, www.huffpost.com, www.ibtimes.co.uk, www.express.co.uk, www.usatoday.com, www.wikipedia.com, patents.google.com, www.aljazeera.com, www.reference.com, www.reuters.com, www.booking.com, www.deviantart.com, www.eventbrite.com, www.tripadvisor.com, www.telegraph.co.uk, www.theatlantic.com, www.ncbi.nlm.nih.gov, www.theguardian.com, www.foxbusiness.com, www.instructables.com, www.chicagotribune.com, forums.macrumors.com, www.agreatertown.com, www.businessinsider.com, www.washingtonpost.com, www.hindustantimes.com\n\nFigure 8: The top-50 most frequent top-level domains for documents in c4-en.", "md": "Table 5 and Table 6 list the top-50 most frequent top-level domains for documents and images as discussed in \u00a7 4. We show domain statistics in both mmc4 and mmc4-core.\n\nFigure 8 shows the top-50 top-level domains of documents in c4-en for reference purposes. The domains are sorted by the frequency of occurrence, as the same with results presented in Figure 6. Previous work also discussed the most represented websites in c4-en ranked by the total number of tokens [12].\n\nPercentage (%)\n0.10\n0.08\n0.06\n0.04\n0.02\n0.00\n\ngithub.com, lists.w3.org, www.rt.com, www.si.com, nypost.com, www.bbc.com, www.cnn.com, www.fool.com, www.etsy.com, www.npr.org, www.cnet.com, mashable.com, www.zdnet.com, www.forbes.com, www.oreilly.com, itunes.apple.com, www.prweb.com, answers.sap.com, www.latimes.com, www.nytimes.com, do5.b00kmedia.ru, www.springer.com, www.foxnews.com, www.youtube.com, www.huffpost.com, www.ibtimes.co.uk, www.express.co.uk, www.usatoday.com, www.wikipedia.com, patents.google.com, www.aljazeera.com, www.reference.com, www.reuters.com, www.booking.com, www.deviantart.com, www.eventbrite.com, www.tripadvisor.com, www.telegraph.co.uk, www.theatlantic.com, www.ncbi.nlm.nih.gov, www.theguardian.com, www.foxbusiness.com, www.instructables.com, www.chicagotribune.com, forums.macrumors.com, www.agreatertown.com, www.businessinsider.com, www.washingtonpost.com, www.hindustantimes.com\n\nFigure 8: The top-50 most frequent top-level domains for documents in c4-en."}, {"type": "heading", "lvl": 2, "value": "Demonstrative Examples", "md": "## Demonstrative Examples"}, {"type": "heading", "lvl": 3, "value": "Images w/ Watermarks/Ads/Logos", "md": "### Images w/ Watermarks/Ads/Logos"}, {"type": "text", "value": "Figure 9a depicts a few sample images containing watermarks in various forms, Figure 9b shows images that are associated with logos, and Figure 9c lists a few sample images related to advertisements. Notice that the dissimilarity between images associated with logos and those pertaining to advertisements is relatively modest. Although images connected to advertisements may occasionally encompass promotional language or persuasive expressions, they may also solely feature logos. Notably, the principal criterion for determining whether an image is ad-related is contingent upon assessing its relevance to the document. If the image is less related to the document, it is more aptly categorized as ad-related. For instance, the interleaved document presented in Table 7 contains two images associated with logos that are intricately linked to the commercial brand being presented within the document. Consequently, these two images are not classified as advertisements.", "md": "Figure 9a depicts a few sample images containing watermarks in various forms, Figure 9b shows images that are associated with logos, and Figure 9c lists a few sample images related to advertisements. Notice that the dissimilarity between images associated with logos and those pertaining to advertisements is relatively modest. Although images connected to advertisements may occasionally encompass promotional language or persuasive expressions, they may also solely feature logos. Notably, the principal criterion for determining whether an image is ad-related is contingent upon assessing its relevance to the document. If the image is less related to the document, it is more aptly categorized as ad-related. For instance, the interleaved document presented in Table 7 contains two images associated with logos that are intricately linked to the commercial brand being presented within the document. Consequently, these two images are not classified as advertisements."}, {"type": "heading", "lvl": 3, "value": "Interleaved Document", "md": "### Interleaved Document"}, {"type": "text", "value": "Table 7 and Table 8 show two interleaved docs from mmc4, displaying the list of sentences and the corresponding assigned images, alongside the CLIP ViT/L-14 image-text similarity score.", "md": "Table 7 and Table 8 show two interleaved docs from mmc4, displaying the list of sentences and the corresponding assigned images, alongside the CLIP ViT/L-14 image-text similarity score."}]}, {"page": 14, "text": "           Table 5: Top-50 top-level domains for documents in mmc4 and mmc4-core.\n                mmc4 documents                                  mmc4-core documents\nDomain Name                          Percentage    Domain Name                          Percentage\n www.bbc.com                           0.0994%     www.dailymail.co.uk                   0.2352%\n www.springer.com                      0.0993%     www.alibaba.com                       0.1601%\n www.wikipedia.com                     0.0750%     www.indiamart.com                     0.1287%\n www.nytimes.com                       0.0690%     www.teacherspayteachers.com           0.1116%\n www.express.co.uk                     0.0573%     www.rt.com                            0.0858%\n www.dailymail.co.uk                   0.0530%     www.bbc.com                           0.0730%\n www.rt.com                            0.0519%     www.digit-life.com                    0.0728%\n itunes.apple.com                      0.0508%     www.cbc.ca                            0.0673%\n www.etsy.com                          0.0475%     www.stitcher.com                      0.0665%\n www.agreatertown.com                  0.0468%     local.firestonecompleteautocare.com   0.0636%\n app-wiringdiagram.herokuapp.com       0.0429%     www.monfrague.online                  0.0629%\n fineartamerica.com                    0.0425%     www.firstpost.com                     0.0555%\n www.cnn.com                           0.0407%     www.express.co.uk                     0.0552%\n www.booking.com                       0.0406%     www.androidpolice.com                 0.0535%\n www.tripadvisor.com                   0.0393%     www.usatoday.com                      0.0528%\n www.firstpost.com                     0.0377%     www.audible.com                       0.0481%\n www.npr.org                           0.0368%     itunes.apple.com                      0.0479%\n www.wired.com                         0.0367%     inhabitat.com                         0.0455%\n www.breitbart.com                     0.0367%     www.cnn.com                           0.0435%\n www.indiamart.com                     0.0364%     www.giftacrossindia.com               0.0433%\n www.audible.com                       0.0346%     www.houzz.com                         0.0428%\n medium.com                            0.0342%     appadvice.com                         0.0421%\n www.dailystar.co.uk                   0.0338%     www.prweb.com                         0.0419%\n www.weddingwire.com                   0.0336%     www.timeout.com                       0.0414%\n appadvice.com                         0.0333%     wccftech.com                          0.0412%\n www.businessinsider.com               0.0310%     www.ifompt.com                        0.0403%\n hubpages.com                          0.0303%     phys.org                              0.0383%\n www.shutterstock.com                  0.0285%     www.abc.net.au                        0.0381%\n www.alibaba.com                       0.0282%     www.acahome.org                       0.0371%\n www.techradar.com                     0.0276%     www.npr.org                           0.0368%\n www.timeout.com                       0.0265%     www.redmondpie.com                    0.0368%\n economictimes.indiatimes.com          0.0259%     babyology.com.au                      0.0367%\n www.prweb.com                         0.0256%     www.etsy.com                          0.0367%\n www.cbc.ca                            0.0246%     fgontheweb.com                        0.0365%\n www.houzz.com                         0.0244%     www.pcworld.com                       0.0359%\n www.ndtv.com                          0.0243%     www.dailystar.co.uk                   0.0350%\n www.gsmarena.com                      0.0243%     www.realtor.com                       0.0348%\n gizmodo.com                           0.0243%     www.wikipedia.com                     0.0342%\n wn.com                                0.0242%     www.advanceduninstaller.com           0.0342%\n www.thestar.com                       0.0240%     shopwizion.com                        0.0337%\n www.deviantart.com                    0.0240%     www.drivermax.com                     0.0337%\n www.indiebound.org                    0.0238%     www.template.net                      0.0334%\n www.telegraph.co.uk                   0.0238%     clemsontigers.com                     0.0330%\n www.teacherspayteachers.com           0.0236%     www.comparometer.in                   0.0329%\n www.imdb.com                          0.0234%     maybeloan.com                         0.0320%\n sg.carousell.com                      0.0233%     medium.com                            0.0320%\n pixels.com                            0.0228%     shoplionly.com                        0.0320%\n timesofi\n        ndia.indiatimes.com            0.0227%     www.replacement-laptop-battery.com    0.0314%\n www.blogtalkradio.com                 0.0227%     www.businessinsider.com.au            0.0312%\n www.glamour.com                       0.0223%     www.dummies.com                       0.0312%\n                                                 14", "md": "|Domain Name|Percentage|Domain Name|Percentage|\n|---|---|---|---|\n|www.bbc.com|0.0994%|www.dailymail.co.uk|0.2352%|\n|www.springer.com|0.0993%|www.alibaba.com|0.1601%|\n|www.wikipedia.com|0.0750%|www.indiamart.com|0.1287%|\n|www.nytimes.com|0.0690%|www.teacherspayteachers.com|0.1116%|\n|www.express.co.uk|0.0573%|www.rt.com|0.0858%|\n|www.dailymail.co.uk|0.0530%|www.bbc.com|0.0730%|\n|www.rt.com|0.0519%|www.digit-life.com|0.0728%|\n|itunes.apple.com|0.0508%|www.cbc.ca|0.0673%|\n|www.etsy.com|0.0475%|www.stitcher.com|0.0665%|\n|www.agreatertown.com|0.0468%|local.firestonecompleteautocare.com|0.0636%|\n|app-wiringdiagram.herokuapp.com|0.0429%|www.monfrague.online|0.0629%|\n|fineartamerica.com|0.0425%|www.firstpost.com|0.0555%|\n|www.cnn.com|0.0407%|www.express.co.uk|0.0552%|\n|www.booking.com|0.0406%|www.androidpolice.com|0.0535%|\n|www.tripadvisor.com|0.0393%|www.usatoday.com|0.0528%|\n|www.firstpost.com|0.0377%|www.audible.com|0.0481%|\n|www.npr.org|0.0368%|itunes.apple.com|0.0479%|\n|www.wired.com|0.0367%|inhabitat.com|0.0455%|\n|www.breitbart.com|0.0367%|www.cnn.com|0.0435%|\n|www.indiamart.com|0.0364%|www.giftacrossindia.com|0.0433%|\n|www.audible.com|0.0346%|www.houzz.com|0.0428%|\n|medium.com|0.0342%|appadvice.com|0.0421%|\n|www.dailystar.co.uk|0.0338%|www.prweb.com|0.0419%|\n|www.weddingwire.com|0.0336%|www.timeout.com|0.0414%|\n|appadvice.com|0.0333%|wccftech.com|0.0412%|\n|www.businessinsider.com|0.0310%|www.ifompt.com|0.0403%|\n|hubpages.com|0.0303%|phys.org|0.0383%|\n|www.shutterstock.com|0.0285%|www.abc.net.au|0.0381%|\n|www.alibaba.com|0.0282%|www.acahome.org|0.0371%|\n|www.techradar.com|0.0276%|www.npr.org|0.0368%|\n|www.timeout.com|0.0265%|www.redmondpie.com|0.0368%|\n|economictimes.indiatimes.com|0.0259%|babyology.com.au|0.0367%|\n|www.prweb.com|0.0256%|www.etsy.com|0.0367%|\n|www.cbc.ca|0.0246%|fgontheweb.com|0.0365%|\n|www.houzz.com|0.0244%|www.pcworld.com|0.0359%|\n|www.ndtv.com|0.0243%|www.dailystar.co.uk|0.0350%|\n|www.gsmarena.com|0.0243%|www.realtor.com|0.0348%|\n|gizmodo.com|0.0243%|www.wikipedia.com|0.0342%|\n|wn.com|0.0242%|www.advanceduninstaller.com|0.0342%|\n|www.thestar.com|0.0240%|shopwizion.com|0.0337%|\n|www.deviantart.com|0.0240%|www.drivermax.com|0.0337%|\n|www.indiebound.org|0.0238%|www.template.net|0.0334%|\n|www.telegraph.co.uk|0.0238%|clemsontigers.com|0.0330%|\n|www.teacherspayteachers.com|0.0236%|www.comparometer.in|0.0329%|\n|www.imdb.com|0.0234%|maybeloan.com|0.0320%|\n|sg.carousell.com|0.0233%|medium.com|0.0320%|\n|pixels.com|0.0228%|shoplionly.com|0.0320%|\n|timesofindia.indiatimes.com|0.0227%|www.replacement-laptop-battery.com|0.0314%|\n|www.blogtalkradio.com|0.0227%|www.businessinsider.com.au|0.0312%|\n|www.glamour.com|0.0223%|www.dummies.com|0.0312%|", "images": [], "items": [{"type": "table", "rows": [["Domain Name", "Percentage", "Domain Name", "Percentage"], ["www.bbc.com", "0.0994%", "www.dailymail.co.uk", "0.2352%"], ["www.springer.com", "0.0993%", "www.alibaba.com", "0.1601%"], ["www.wikipedia.com", "0.0750%", "www.indiamart.com", "0.1287%"], ["www.nytimes.com", "0.0690%", "www.teacherspayteachers.com", "0.1116%"], ["www.express.co.uk", "0.0573%", "www.rt.com", "0.0858%"], ["www.dailymail.co.uk", "0.0530%", "www.bbc.com", "0.0730%"], ["www.rt.com", "0.0519%", "www.digit-life.com", "0.0728%"], ["itunes.apple.com", "0.0508%", "www.cbc.ca", "0.0673%"], ["www.etsy.com", "0.0475%", "www.stitcher.com", "0.0665%"], ["www.agreatertown.com", "0.0468%", "local.firestonecompleteautocare.com", "0.0636%"], ["app-wiringdiagram.herokuapp.com", "0.0429%", "www.monfrague.online", "0.0629%"], ["fineartamerica.com", "0.0425%", "www.firstpost.com", "0.0555%"], ["www.cnn.com", "0.0407%", "www.express.co.uk", "0.0552%"], ["www.booking.com", "0.0406%", "www.androidpolice.com", "0.0535%"], ["www.tripadvisor.com", "0.0393%", "www.usatoday.com", "0.0528%"], ["www.firstpost.com", "0.0377%", "www.audible.com", "0.0481%"], ["www.npr.org", "0.0368%", "itunes.apple.com", "0.0479%"], ["www.wired.com", "0.0367%", "inhabitat.com", "0.0455%"], ["www.breitbart.com", "0.0367%", "www.cnn.com", "0.0435%"], ["www.indiamart.com", "0.0364%", "www.giftacrossindia.com", "0.0433%"], ["www.audible.com", "0.0346%", "www.houzz.com", "0.0428%"], ["medium.com", "0.0342%", "appadvice.com", "0.0421%"], ["www.dailystar.co.uk", "0.0338%", "www.prweb.com", "0.0419%"], ["www.weddingwire.com", "0.0336%", "www.timeout.com", "0.0414%"], ["appadvice.com", "0.0333%", "wccftech.com", "0.0412%"], ["www.businessinsider.com", "0.0310%", "www.ifompt.com", "0.0403%"], ["hubpages.com", "0.0303%", "phys.org", "0.0383%"], ["www.shutterstock.com", "0.0285%", "www.abc.net.au", "0.0381%"], ["www.alibaba.com", "0.0282%", "www.acahome.org", "0.0371%"], ["www.techradar.com", "0.0276%", "www.npr.org", "0.0368%"], ["www.timeout.com", "0.0265%", "www.redmondpie.com", "0.0368%"], ["economictimes.indiatimes.com", "0.0259%", "babyology.com.au", "0.0367%"], ["www.prweb.com", "0.0256%", "www.etsy.com", "0.0367%"], ["www.cbc.ca", "0.0246%", "fgontheweb.com", "0.0365%"], ["www.houzz.com", "0.0244%", "www.pcworld.com", "0.0359%"], ["www.ndtv.com", "0.0243%", "www.dailystar.co.uk", "0.0350%"], ["www.gsmarena.com", "0.0243%", "www.realtor.com", "0.0348%"], ["gizmodo.com", "0.0243%", "www.wikipedia.com", "0.0342%"], ["wn.com", "0.0242%", "www.advanceduninstaller.com", "0.0342%"], ["www.thestar.com", "0.0240%", "shopwizion.com", "0.0337%"], ["www.deviantart.com", "0.0240%", "www.drivermax.com", "0.0337%"], ["www.indiebound.org", "0.0238%", "www.template.net", "0.0334%"], ["www.telegraph.co.uk", "0.0238%", "clemsontigers.com", "0.0330%"], ["www.teacherspayteachers.com", "0.0236%", "www.comparometer.in", "0.0329%"], ["www.imdb.com", "0.0234%", "maybeloan.com", "0.0320%"], ["sg.carousell.com", "0.0233%", "medium.com", "0.0320%"], ["pixels.com", "0.0228%", "shoplionly.com", "0.0320%"], ["timesofindia.indiatimes.com", "0.0227%", "www.replacement-laptop-battery.com", "0.0314%"], ["www.blogtalkradio.com", "0.0227%", "www.businessinsider.com.au", "0.0312%"], ["www.glamour.com", "0.0223%", "www.dummies.com", "0.0312%"]], "md": "|Domain Name|Percentage|Domain Name|Percentage|\n|---|---|---|---|\n|www.bbc.com|0.0994%|www.dailymail.co.uk|0.2352%|\n|www.springer.com|0.0993%|www.alibaba.com|0.1601%|\n|www.wikipedia.com|0.0750%|www.indiamart.com|0.1287%|\n|www.nytimes.com|0.0690%|www.teacherspayteachers.com|0.1116%|\n|www.express.co.uk|0.0573%|www.rt.com|0.0858%|\n|www.dailymail.co.uk|0.0530%|www.bbc.com|0.0730%|\n|www.rt.com|0.0519%|www.digit-life.com|0.0728%|\n|itunes.apple.com|0.0508%|www.cbc.ca|0.0673%|\n|www.etsy.com|0.0475%|www.stitcher.com|0.0665%|\n|www.agreatertown.com|0.0468%|local.firestonecompleteautocare.com|0.0636%|\n|app-wiringdiagram.herokuapp.com|0.0429%|www.monfrague.online|0.0629%|\n|fineartamerica.com|0.0425%|www.firstpost.com|0.0555%|\n|www.cnn.com|0.0407%|www.express.co.uk|0.0552%|\n|www.booking.com|0.0406%|www.androidpolice.com|0.0535%|\n|www.tripadvisor.com|0.0393%|www.usatoday.com|0.0528%|\n|www.firstpost.com|0.0377%|www.audible.com|0.0481%|\n|www.npr.org|0.0368%|itunes.apple.com|0.0479%|\n|www.wired.com|0.0367%|inhabitat.com|0.0455%|\n|www.breitbart.com|0.0367%|www.cnn.com|0.0435%|\n|www.indiamart.com|0.0364%|www.giftacrossindia.com|0.0433%|\n|www.audible.com|0.0346%|www.houzz.com|0.0428%|\n|medium.com|0.0342%|appadvice.com|0.0421%|\n|www.dailystar.co.uk|0.0338%|www.prweb.com|0.0419%|\n|www.weddingwire.com|0.0336%|www.timeout.com|0.0414%|\n|appadvice.com|0.0333%|wccftech.com|0.0412%|\n|www.businessinsider.com|0.0310%|www.ifompt.com|0.0403%|\n|hubpages.com|0.0303%|phys.org|0.0383%|\n|www.shutterstock.com|0.0285%|www.abc.net.au|0.0381%|\n|www.alibaba.com|0.0282%|www.acahome.org|0.0371%|\n|www.techradar.com|0.0276%|www.npr.org|0.0368%|\n|www.timeout.com|0.0265%|www.redmondpie.com|0.0368%|\n|economictimes.indiatimes.com|0.0259%|babyology.com.au|0.0367%|\n|www.prweb.com|0.0256%|www.etsy.com|0.0367%|\n|www.cbc.ca|0.0246%|fgontheweb.com|0.0365%|\n|www.houzz.com|0.0244%|www.pcworld.com|0.0359%|\n|www.ndtv.com|0.0243%|www.dailystar.co.uk|0.0350%|\n|www.gsmarena.com|0.0243%|www.realtor.com|0.0348%|\n|gizmodo.com|0.0243%|www.wikipedia.com|0.0342%|\n|wn.com|0.0242%|www.advanceduninstaller.com|0.0342%|\n|www.thestar.com|0.0240%|shopwizion.com|0.0337%|\n|www.deviantart.com|0.0240%|www.drivermax.com|0.0337%|\n|www.indiebound.org|0.0238%|www.template.net|0.0334%|\n|www.telegraph.co.uk|0.0238%|clemsontigers.com|0.0330%|\n|www.teacherspayteachers.com|0.0236%|www.comparometer.in|0.0329%|\n|www.imdb.com|0.0234%|maybeloan.com|0.0320%|\n|sg.carousell.com|0.0233%|medium.com|0.0320%|\n|pixels.com|0.0228%|shoplionly.com|0.0320%|\n|timesofindia.indiatimes.com|0.0227%|www.replacement-laptop-battery.com|0.0314%|\n|www.blogtalkradio.com|0.0227%|www.businessinsider.com.au|0.0312%|\n|www.glamour.com|0.0223%|www.dummies.com|0.0312%|", "isPerfectTable": true, "csv": "\"Domain Name\",\"Percentage\",\"Domain Name\",\"Percentage\"\n\"www.bbc.com\",\"0.0994%\",\"www.dailymail.co.uk\",\"0.2352%\"\n\"www.springer.com\",\"0.0993%\",\"www.alibaba.com\",\"0.1601%\"\n\"www.wikipedia.com\",\"0.0750%\",\"www.indiamart.com\",\"0.1287%\"\n\"www.nytimes.com\",\"0.0690%\",\"www.teacherspayteachers.com\",\"0.1116%\"\n\"www.express.co.uk\",\"0.0573%\",\"www.rt.com\",\"0.0858%\"\n\"www.dailymail.co.uk\",\"0.0530%\",\"www.bbc.com\",\"0.0730%\"\n\"www.rt.com\",\"0.0519%\",\"www.digit-life.com\",\"0.0728%\"\n\"itunes.apple.com\",\"0.0508%\",\"www.cbc.ca\",\"0.0673%\"\n\"www.etsy.com\",\"0.0475%\",\"www.stitcher.com\",\"0.0665%\"\n\"www.agreatertown.com\",\"0.0468%\",\"local.firestonecompleteautocare.com\",\"0.0636%\"\n\"app-wiringdiagram.herokuapp.com\",\"0.0429%\",\"www.monfrague.online\",\"0.0629%\"\n\"fineartamerica.com\",\"0.0425%\",\"www.firstpost.com\",\"0.0555%\"\n\"www.cnn.com\",\"0.0407%\",\"www.express.co.uk\",\"0.0552%\"\n\"www.booking.com\",\"0.0406%\",\"www.androidpolice.com\",\"0.0535%\"\n\"www.tripadvisor.com\",\"0.0393%\",\"www.usatoday.com\",\"0.0528%\"\n\"www.firstpost.com\",\"0.0377%\",\"www.audible.com\",\"0.0481%\"\n\"www.npr.org\",\"0.0368%\",\"itunes.apple.com\",\"0.0479%\"\n\"www.wired.com\",\"0.0367%\",\"inhabitat.com\",\"0.0455%\"\n\"www.breitbart.com\",\"0.0367%\",\"www.cnn.com\",\"0.0435%\"\n\"www.indiamart.com\",\"0.0364%\",\"www.giftacrossindia.com\",\"0.0433%\"\n\"www.audible.com\",\"0.0346%\",\"www.houzz.com\",\"0.0428%\"\n\"medium.com\",\"0.0342%\",\"appadvice.com\",\"0.0421%\"\n\"www.dailystar.co.uk\",\"0.0338%\",\"www.prweb.com\",\"0.0419%\"\n\"www.weddingwire.com\",\"0.0336%\",\"www.timeout.com\",\"0.0414%\"\n\"appadvice.com\",\"0.0333%\",\"wccftech.com\",\"0.0412%\"\n\"www.businessinsider.com\",\"0.0310%\",\"www.ifompt.com\",\"0.0403%\"\n\"hubpages.com\",\"0.0303%\",\"phys.org\",\"0.0383%\"\n\"www.shutterstock.com\",\"0.0285%\",\"www.abc.net.au\",\"0.0381%\"\n\"www.alibaba.com\",\"0.0282%\",\"www.acahome.org\",\"0.0371%\"\n\"www.techradar.com\",\"0.0276%\",\"www.npr.org\",\"0.0368%\"\n\"www.timeout.com\",\"0.0265%\",\"www.redmondpie.com\",\"0.0368%\"\n\"economictimes.indiatimes.com\",\"0.0259%\",\"babyology.com.au\",\"0.0367%\"\n\"www.prweb.com\",\"0.0256%\",\"www.etsy.com\",\"0.0367%\"\n\"www.cbc.ca\",\"0.0246%\",\"fgontheweb.com\",\"0.0365%\"\n\"www.houzz.com\",\"0.0244%\",\"www.pcworld.com\",\"0.0359%\"\n\"www.ndtv.com\",\"0.0243%\",\"www.dailystar.co.uk\",\"0.0350%\"\n\"www.gsmarena.com\",\"0.0243%\",\"www.realtor.com\",\"0.0348%\"\n\"gizmodo.com\",\"0.0243%\",\"www.wikipedia.com\",\"0.0342%\"\n\"wn.com\",\"0.0242%\",\"www.advanceduninstaller.com\",\"0.0342%\"\n\"www.thestar.com\",\"0.0240%\",\"shopwizion.com\",\"0.0337%\"\n\"www.deviantart.com\",\"0.0240%\",\"www.drivermax.com\",\"0.0337%\"\n\"www.indiebound.org\",\"0.0238%\",\"www.template.net\",\"0.0334%\"\n\"www.telegraph.co.uk\",\"0.0238%\",\"clemsontigers.com\",\"0.0330%\"\n\"www.teacherspayteachers.com\",\"0.0236%\",\"www.comparometer.in\",\"0.0329%\"\n\"www.imdb.com\",\"0.0234%\",\"maybeloan.com\",\"0.0320%\"\n\"sg.carousell.com\",\"0.0233%\",\"medium.com\",\"0.0320%\"\n\"pixels.com\",\"0.0228%\",\"shoplionly.com\",\"0.0320%\"\n\"timesofindia.indiatimes.com\",\"0.0227%\",\"www.replacement-laptop-battery.com\",\"0.0314%\"\n\"www.blogtalkradio.com\",\"0.0227%\",\"www.businessinsider.com.au\",\"0.0312%\"\n\"www.glamour.com\",\"0.0223%\",\"www.dummies.com\",\"0.0312%\""}]}, {"page": 15, "text": "Table 6: Top-50 top-level domains for images in mmc4 and mmc4-core. The symbol \u201c*\u201d is employed\nto denote specific patterns, such as digits or location acronyms, commonly utilized to differentiate\nsub-sites within the same domain.\n                     mmc4 images                                        mmc4-core images\n  Domain Name                             Percentage     Domain Name                            Percentage\n   *.bp.blogspot.com                        8.7454%      *.bp.blogspot.com                       6.8934%\n   s3.amazonaws.com                         2.1629%      s3.amazonaws.com                        1.8782%\n   i*.wp.com                                1.3176%      images-*.ssl-images-amazon.com          1.7976%\n   *.staticfl\n            ickr.com                        1.2530%      i*.wp.com                               1.4590%\n   images-*.ssl-images-amazon.com           1.2430%      static*.squarespace.com                 0.9530%\n   static*.squarespace.com                  0.8838%      cdn.atwilltech.com                      0.9009%\n   i.pinimg.com                             0.6992%      *.staticflickr.com                      0.7968%\n   i.ytimg.com                              0.6644%      i.ytimg.com                             0.4446%\n   i*.photobucket.com                       0.5075%      *.imimg.com                             0.4308%\n   res.cloudinary.com                       0.3683%      bt-photos.global.ssl.fastly.net         0.3827%\n   storage.googleapis.com                   0.3466%      sc*.alicdn.com                          0.3700%\n   i.imgur.com                              0.2858%      i.etsystatic.com                        0.3494%\n   lh*.googleusercontent.com                0.2762%      i.pinimg.com                            0.3356%\n   *.bstatic.com                            0.2436%      i.dailymail.co.uk                       0.2896%\n   s-media-cache-ak*.pinimg.com             0.2270%      s-media-cache-ak*.pinimg.com            0.2705%\n   img.youtube.com                          0.1954%      i.imgur.com                             0.2638%\n   photos.smugmug.com                       0.1934%      i*.photobucket.com                      0.2603%\n   cdn.photos.sparkplatform.com             0.1915%      lh*.googleusercontent.com               0.2435%\n   is*-ssl.mzstatic.com                     0.1821%      res.cloudinary.com                      0.2349%\n   i.etsystatic.com                         0.1727%      is*-ssl.mzstatic.com                    0.2142%\n   odis.homeaway.com                        0.1657%      i.bosscdn.com                           0.1989%\n   media-cdn.tripadvisor.com                0.1605%      assets.eflorist.com                     0.1927%\n   media.karousell.com                      0.1584%      *.yimg.com                              0.1828%\n   www.picclickimg.com                      0.1550%      ecx.images-amazon.com                   0.1356%\n   ae*.alicdn.com                           0.1547%      storage.googleapis.com                  0.1329%\n   m.media-amazon.com                       0.1418%      img.youtube.com                         0.1192%\n   ecx.images-amazon.com                    0.1385%      cdn.shoplightspeed.com                  0.1186%\n   images.furnituredealer.net               0.1382%      img-aws.ehowcdn.com                     0.1163%\n   image.jimcdn.com                         0.1362%      photos.smugmug.com                      0.1137%\n   bt-photos.global.ssl.fastly.net          0.1254%      ecdn.teacherspayteachers.com            0.1047%\n   t.realgeeks.media                        0.1234%      image.jimcdn.com                        0.1035%\n   pbs.twimg.com                            0.1194%      m.media-amazon.com                      0.1006%\n   content.cdntwrk.com                      0.1126%      cdn.webshopapp.com                      0.1000%\n   www.wikihow.com                          0.1106%      i.ebayimg.com                           0.0986%\n   cdn.atwilltech.com                       0.1092%      mediad.publicbroadcasting.net           0.0915%\n   *.yimg.com                               0.1065%      images.template.net                     0.0906%\n   upload.wikimedia.org                     0.0960%      ae*.alicdn.com                          0.0871%\n   *.media.tumblr.com                       0.0942%      secure.img*-fg.wfcdn.com                0.0861%\n   f*.bcbits.com                            0.0886%      s*.pcdn.co                              0.0848%\n   f.dvipcdn.com                            0.0848%      st.hzcdn.com                            0.0838%\n   photos*.blogger.com                      0.0833%      assets.simpleviewinc.com                0.0813%\n   cdn*.weddingwire.com                     0.0822%      fgontheweb.com                          0.0793%\n   static.shareasale.com                    0.0815%      images.navidirect.org                   0.0790%\n   secure.img*-fg.wfcdn.com                 0.0812%      cdni.rt.com                             0.0786%\n   c*.alamy.com                             0.0812%      downloads.intercomcdn.com               0.0777%\n   usercontent*.hubstatic.com               0.0810%      gallery.mailchimp.com                   0.0750%\n   sc*.alicdn.com                           0.0803%      slideplayer.com                         0.0690%\n   static.showit.co                         0.0783%      cdn.displays*go.com                     0.0677%\n   i.bosscdn.com                            0.0764%      dta*yqvfnusiq.cloudfront.net            0.0660%\n   *.imimg.com                              0.0742%      images.clickdealer.co.uk                0.0644%\n                                                      15", "md": "markdown\n|Domain Name|Percentage|Domain Name|Percentage|\n|---|---|---|---|\n|*.bp.blogspot.com|8.7454%|*.bp.blogspot.com|6.8934%|\n|s3.amazonaws.com|2.1629%|s3.amazonaws.com|1.8782%|\n|i*.wp.com|1.3176%|images-*.ssl-images-amazon.com|1.7976%|\n|*.staticflickr.com|1.2530%|i*.wp.com|1.4590%|\n|images-*.ssl-images-amazon.com|1.2430%|static*.squarespace.com|0.9530%|\n|markdown<br/>static*.squarespace.com|0.8838%|cdn.atwilltech.com|0.9009%|\n|i.pinimg.com|0.6992%|*.staticflickr.com|0.7968%|\n|i.ytimg.com|0.6644%|i.ytimg.com|0.4446%|\n|i*.photobucket.com|0.5075%|*.imimg.com|0.4308%|\n|res.cloudinary.com|0.3683%|bt-photos.global.ssl.fastly.net|0.3827%|\n|storage.googleapis.com|0.3466%|sc*.alicdn.com|0.3700%|\n|i.imgur.com|0.2858%|i.etsystatic.com|0.3494%|\n|lh*.googleusercontent.com|0.2762%|i.pinimg.com|0.3356%|\n|*.bstatic.com|0.2436%|i.dailymail.co.uk|0.2896%|\n|s-media-cache-ak*.pinimg.com|0.2270%|s-media-cache-ak*.pinimg.com|0.2705%|\n|img.youtube.com|0.1954%|i.imgur.com|0.2638%|\n|photos.smugmug.com|0.1934%|i*.photobucket.com|0.2603%|\n|cdn.photos.sparkplatform.com|0.1915%|lh*.googleusercontent.com|0.2435%|\n|is*-ssl.mzstatic.com|0.1821%|res.cloudinary.com|0.2349%|\n|i.etsystatic.com|0.1727%|is*-ssl.mzstatic.com|0.2142%|\n|odis.homeaway.com|0.1657%|i.bosscdn.com|0.1989%|\n|media-cdn.tripadvisor.com|0.1605%|assets.eflorist.com|0.1927%|\n|media.karousell.com|0.1584%|*.yimg.com|0.1828%|\n|www.picclickimg.com|0.1550%|ecx.images-amazon.com|0.1356%|\n|ae*.alicdn.com|0.1547%|storage.googleapis.com|0.1329%|\n|m.media-amazon.com|0.1418%|img.youtube.com|0.1192%|\n|ecx.images-amazon.com|0.1385%|cdn.shoplightspeed.com|0.1186%|\n|images.furnituredealer.net|0.1382%|img-aws.ehowcdn.com|0.1163%|\n|image.jimcdn.com|0.1362%|photos.smugmug.com|0.1137%|\n|bt-photos.global.ssl.fastly.net|0.1254%|ecdn.teacherspayteachers.com|0.1047%|\n|t.realgeeks.media|0.1234%|image.jimcdn.com|0.1035%|\n|pbs.twimg.com|0.1194%|m.media-amazon.com|0.1006%|\n|content.cdntwrk.com|0.1126%|cdn.webshopapp.com|0.1000%|\n|www.wikihow.com|0.1106%|i.ebayimg.com|0.0986%|\n|cdn.atwilltech.com|0.1092%|mediad.publicbroadcasting.net|0.0915%|\n|*.yimg.com|0.1065%|images.template.net|0.0906%|\n|upload.wikimedia.org|0.0960%|ae*.alicdn.com|0.0871%|\n|*.media.tumblr.com|0.0942%|secure.img*-fg.wfcdn.com|0.0861%|\n|f*.bcbits.com|0.0886%|s*.pcdn.co|0.0848%|\n|f.dvipcdn.com|0.0848%|st.hzcdn.com|0.0838%|\n|photos*.blogger.com|0.0833%|assets.simpleviewinc.com|0.0813%|\n|cdn*.weddingwire.com|0.0822%|fgontheweb.com|0.0793%|\n|static.shareasale.com|0.0815%|images.navidirect.org|0.0790%|\n|secure.img*-fg.wfcdn.com|0.0812%|cdni.rt.com|0.0786%|\n|c*.alamy.com|0.0812%|downloads.intercomcdn.com|0.0777%|\n|usercontent*.hubstatic.com|0.0810%|gallery.mailchimp.com|0.0750%|\n|sc*.alicdn.com|0.0803%|slideplayer.com|0.0690%|\n|static.showit.co|0.0783%|cdn.displays*go.com|0.0677%|\n|i.bosscdn.com|0.0764%|dta*yqvfnusiq.cloudfront.net|0.0660%|\n|*.imimg.com|0.0742%| | |", "images": [], "items": [{"type": "text", "value": "markdown", "md": "markdown"}, {"type": "table", "rows": [["Domain Name", "Percentage", "Domain Name", "Percentage"], ["*.bp.blogspot.com", "8.7454%", "*.bp.blogspot.com", "6.8934%"], ["s3.amazonaws.com", "2.1629%", "s3.amazonaws.com", "1.8782%"], ["i*.wp.com", "1.3176%", "images-*.ssl-images-amazon.com", "1.7976%"], ["*.staticflickr.com", "1.2530%", "i*.wp.com", "1.4590%"], ["images-*.ssl-images-amazon.com", "1.2430%", "static*.squarespace.com", "0.9530%"], ["markdown<br/>static*.squarespace.com", "0.8838%", "cdn.atwilltech.com", "0.9009%"], ["i.pinimg.com", "0.6992%", "*.staticflickr.com", "0.7968%"], ["i.ytimg.com", "0.6644%", "i.ytimg.com", "0.4446%"], ["i*.photobucket.com", "0.5075%", "*.imimg.com", "0.4308%"], ["res.cloudinary.com", "0.3683%", "bt-photos.global.ssl.fastly.net", "0.3827%"], ["storage.googleapis.com", "0.3466%", "sc*.alicdn.com", "0.3700%"], ["i.imgur.com", "0.2858%", "i.etsystatic.com", "0.3494%"], ["lh*.googleusercontent.com", "0.2762%", "i.pinimg.com", "0.3356%"], ["*.bstatic.com", "0.2436%", "i.dailymail.co.uk", "0.2896%"], ["s-media-cache-ak*.pinimg.com", "0.2270%", "s-media-cache-ak*.pinimg.com", "0.2705%"], ["img.youtube.com", "0.1954%", "i.imgur.com", "0.2638%"], ["photos.smugmug.com", "0.1934%", "i*.photobucket.com", "0.2603%"], ["cdn.photos.sparkplatform.com", "0.1915%", "lh*.googleusercontent.com", "0.2435%"], ["is*-ssl.mzstatic.com", "0.1821%", "res.cloudinary.com", "0.2349%"], ["i.etsystatic.com", "0.1727%", "is*-ssl.mzstatic.com", "0.2142%"], ["odis.homeaway.com", "0.1657%", "i.bosscdn.com", "0.1989%"], ["media-cdn.tripadvisor.com", "0.1605%", "assets.eflorist.com", "0.1927%"], ["media.karousell.com", "0.1584%", "*.yimg.com", "0.1828%"], ["www.picclickimg.com", "0.1550%", "ecx.images-amazon.com", "0.1356%"], ["ae*.alicdn.com", "0.1547%", "storage.googleapis.com", "0.1329%"], ["m.media-amazon.com", "0.1418%", "img.youtube.com", "0.1192%"], ["ecx.images-amazon.com", "0.1385%", "cdn.shoplightspeed.com", "0.1186%"], ["images.furnituredealer.net", "0.1382%", "img-aws.ehowcdn.com", "0.1163%"], ["image.jimcdn.com", "0.1362%", "photos.smugmug.com", "0.1137%"], ["bt-photos.global.ssl.fastly.net", "0.1254%", "ecdn.teacherspayteachers.com", "0.1047%"], ["t.realgeeks.media", "0.1234%", "image.jimcdn.com", "0.1035%"], ["pbs.twimg.com", "0.1194%", "m.media-amazon.com", "0.1006%"], ["content.cdntwrk.com", "0.1126%", "cdn.webshopapp.com", "0.1000%"], ["www.wikihow.com", "0.1106%", "i.ebayimg.com", "0.0986%"], ["cdn.atwilltech.com", "0.1092%", "mediad.publicbroadcasting.net", "0.0915%"], ["*.yimg.com", "0.1065%", "images.template.net", "0.0906%"], ["upload.wikimedia.org", "0.0960%", "ae*.alicdn.com", "0.0871%"], ["*.media.tumblr.com", "0.0942%", "secure.img*-fg.wfcdn.com", "0.0861%"], ["f*.bcbits.com", "0.0886%", "s*.pcdn.co", "0.0848%"], ["f.dvipcdn.com", "0.0848%", "st.hzcdn.com", "0.0838%"], ["photos*.blogger.com", "0.0833%", "assets.simpleviewinc.com", "0.0813%"], ["cdn*.weddingwire.com", "0.0822%", "fgontheweb.com", "0.0793%"], ["static.shareasale.com", "0.0815%", "images.navidirect.org", "0.0790%"], ["secure.img*-fg.wfcdn.com", "0.0812%", "cdni.rt.com", "0.0786%"], ["c*.alamy.com", "0.0812%", "downloads.intercomcdn.com", "0.0777%"], ["usercontent*.hubstatic.com", "0.0810%", "gallery.mailchimp.com", "0.0750%"], ["sc*.alicdn.com", "0.0803%", "slideplayer.com", "0.0690%"], ["static.showit.co", "0.0783%", "cdn.displays*go.com", "0.0677%"], ["i.bosscdn.com", "0.0764%", "dta*yqvfnusiq.cloudfront.net", "0.0660%"], ["*.imimg.com", "0.0742%", "", ""]], "md": "|Domain Name|Percentage|Domain Name|Percentage|\n|---|---|---|---|\n|*.bp.blogspot.com|8.7454%|*.bp.blogspot.com|6.8934%|\n|s3.amazonaws.com|2.1629%|s3.amazonaws.com|1.8782%|\n|i*.wp.com|1.3176%|images-*.ssl-images-amazon.com|1.7976%|\n|*.staticflickr.com|1.2530%|i*.wp.com|1.4590%|\n|images-*.ssl-images-amazon.com|1.2430%|static*.squarespace.com|0.9530%|\n|markdown<br/>static*.squarespace.com|0.8838%|cdn.atwilltech.com|0.9009%|\n|i.pinimg.com|0.6992%|*.staticflickr.com|0.7968%|\n|i.ytimg.com|0.6644%|i.ytimg.com|0.4446%|\n|i*.photobucket.com|0.5075%|*.imimg.com|0.4308%|\n|res.cloudinary.com|0.3683%|bt-photos.global.ssl.fastly.net|0.3827%|\n|storage.googleapis.com|0.3466%|sc*.alicdn.com|0.3700%|\n|i.imgur.com|0.2858%|i.etsystatic.com|0.3494%|\n|lh*.googleusercontent.com|0.2762%|i.pinimg.com|0.3356%|\n|*.bstatic.com|0.2436%|i.dailymail.co.uk|0.2896%|\n|s-media-cache-ak*.pinimg.com|0.2270%|s-media-cache-ak*.pinimg.com|0.2705%|\n|img.youtube.com|0.1954%|i.imgur.com|0.2638%|\n|photos.smugmug.com|0.1934%|i*.photobucket.com|0.2603%|\n|cdn.photos.sparkplatform.com|0.1915%|lh*.googleusercontent.com|0.2435%|\n|is*-ssl.mzstatic.com|0.1821%|res.cloudinary.com|0.2349%|\n|i.etsystatic.com|0.1727%|is*-ssl.mzstatic.com|0.2142%|\n|odis.homeaway.com|0.1657%|i.bosscdn.com|0.1989%|\n|media-cdn.tripadvisor.com|0.1605%|assets.eflorist.com|0.1927%|\n|media.karousell.com|0.1584%|*.yimg.com|0.1828%|\n|www.picclickimg.com|0.1550%|ecx.images-amazon.com|0.1356%|\n|ae*.alicdn.com|0.1547%|storage.googleapis.com|0.1329%|\n|m.media-amazon.com|0.1418%|img.youtube.com|0.1192%|\n|ecx.images-amazon.com|0.1385%|cdn.shoplightspeed.com|0.1186%|\n|images.furnituredealer.net|0.1382%|img-aws.ehowcdn.com|0.1163%|\n|image.jimcdn.com|0.1362%|photos.smugmug.com|0.1137%|\n|bt-photos.global.ssl.fastly.net|0.1254%|ecdn.teacherspayteachers.com|0.1047%|\n|t.realgeeks.media|0.1234%|image.jimcdn.com|0.1035%|\n|pbs.twimg.com|0.1194%|m.media-amazon.com|0.1006%|\n|content.cdntwrk.com|0.1126%|cdn.webshopapp.com|0.1000%|\n|www.wikihow.com|0.1106%|i.ebayimg.com|0.0986%|\n|cdn.atwilltech.com|0.1092%|mediad.publicbroadcasting.net|0.0915%|\n|*.yimg.com|0.1065%|images.template.net|0.0906%|\n|upload.wikimedia.org|0.0960%|ae*.alicdn.com|0.0871%|\n|*.media.tumblr.com|0.0942%|secure.img*-fg.wfcdn.com|0.0861%|\n|f*.bcbits.com|0.0886%|s*.pcdn.co|0.0848%|\n|f.dvipcdn.com|0.0848%|st.hzcdn.com|0.0838%|\n|photos*.blogger.com|0.0833%|assets.simpleviewinc.com|0.0813%|\n|cdn*.weddingwire.com|0.0822%|fgontheweb.com|0.0793%|\n|static.shareasale.com|0.0815%|images.navidirect.org|0.0790%|\n|secure.img*-fg.wfcdn.com|0.0812%|cdni.rt.com|0.0786%|\n|c*.alamy.com|0.0812%|downloads.intercomcdn.com|0.0777%|\n|usercontent*.hubstatic.com|0.0810%|gallery.mailchimp.com|0.0750%|\n|sc*.alicdn.com|0.0803%|slideplayer.com|0.0690%|\n|static.showit.co|0.0783%|cdn.displays*go.com|0.0677%|\n|i.bosscdn.com|0.0764%|dta*yqvfnusiq.cloudfront.net|0.0660%|\n|*.imimg.com|0.0742%| | |", "isPerfectTable": true, "csv": "\"Domain Name\",\"Percentage\",\"Domain Name\",\"Percentage\"\n\"*.bp.blogspot.com\",\"8.7454%\",\"*.bp.blogspot.com\",\"6.8934%\"\n\"s3.amazonaws.com\",\"2.1629%\",\"s3.amazonaws.com\",\"1.8782%\"\n\"i*.wp.com\",\"1.3176%\",\"images-*.ssl-images-amazon.com\",\"1.7976%\"\n\"*.staticflickr.com\",\"1.2530%\",\"i*.wp.com\",\"1.4590%\"\n\"images-*.ssl-images-amazon.com\",\"1.2430%\",\"static*.squarespace.com\",\"0.9530%\"\n\"markdown<br/>static*.squarespace.com\",\"0.8838%\",\"cdn.atwilltech.com\",\"0.9009%\"\n\"i.pinimg.com\",\"0.6992%\",\"*.staticflickr.com\",\"0.7968%\"\n\"i.ytimg.com\",\"0.6644%\",\"i.ytimg.com\",\"0.4446%\"\n\"i*.photobucket.com\",\"0.5075%\",\"*.imimg.com\",\"0.4308%\"\n\"res.cloudinary.com\",\"0.3683%\",\"bt-photos.global.ssl.fastly.net\",\"0.3827%\"\n\"storage.googleapis.com\",\"0.3466%\",\"sc*.alicdn.com\",\"0.3700%\"\n\"i.imgur.com\",\"0.2858%\",\"i.etsystatic.com\",\"0.3494%\"\n\"lh*.googleusercontent.com\",\"0.2762%\",\"i.pinimg.com\",\"0.3356%\"\n\"*.bstatic.com\",\"0.2436%\",\"i.dailymail.co.uk\",\"0.2896%\"\n\"s-media-cache-ak*.pinimg.com\",\"0.2270%\",\"s-media-cache-ak*.pinimg.com\",\"0.2705%\"\n\"img.youtube.com\",\"0.1954%\",\"i.imgur.com\",\"0.2638%\"\n\"photos.smugmug.com\",\"0.1934%\",\"i*.photobucket.com\",\"0.2603%\"\n\"cdn.photos.sparkplatform.com\",\"0.1915%\",\"lh*.googleusercontent.com\",\"0.2435%\"\n\"is*-ssl.mzstatic.com\",\"0.1821%\",\"res.cloudinary.com\",\"0.2349%\"\n\"i.etsystatic.com\",\"0.1727%\",\"is*-ssl.mzstatic.com\",\"0.2142%\"\n\"odis.homeaway.com\",\"0.1657%\",\"i.bosscdn.com\",\"0.1989%\"\n\"media-cdn.tripadvisor.com\",\"0.1605%\",\"assets.eflorist.com\",\"0.1927%\"\n\"media.karousell.com\",\"0.1584%\",\"*.yimg.com\",\"0.1828%\"\n\"www.picclickimg.com\",\"0.1550%\",\"ecx.images-amazon.com\",\"0.1356%\"\n\"ae*.alicdn.com\",\"0.1547%\",\"storage.googleapis.com\",\"0.1329%\"\n\"m.media-amazon.com\",\"0.1418%\",\"img.youtube.com\",\"0.1192%\"\n\"ecx.images-amazon.com\",\"0.1385%\",\"cdn.shoplightspeed.com\",\"0.1186%\"\n\"images.furnituredealer.net\",\"0.1382%\",\"img-aws.ehowcdn.com\",\"0.1163%\"\n\"image.jimcdn.com\",\"0.1362%\",\"photos.smugmug.com\",\"0.1137%\"\n\"bt-photos.global.ssl.fastly.net\",\"0.1254%\",\"ecdn.teacherspayteachers.com\",\"0.1047%\"\n\"t.realgeeks.media\",\"0.1234%\",\"image.jimcdn.com\",\"0.1035%\"\n\"pbs.twimg.com\",\"0.1194%\",\"m.media-amazon.com\",\"0.1006%\"\n\"content.cdntwrk.com\",\"0.1126%\",\"cdn.webshopapp.com\",\"0.1000%\"\n\"www.wikihow.com\",\"0.1106%\",\"i.ebayimg.com\",\"0.0986%\"\n\"cdn.atwilltech.com\",\"0.1092%\",\"mediad.publicbroadcasting.net\",\"0.0915%\"\n\"*.yimg.com\",\"0.1065%\",\"images.template.net\",\"0.0906%\"\n\"upload.wikimedia.org\",\"0.0960%\",\"ae*.alicdn.com\",\"0.0871%\"\n\"*.media.tumblr.com\",\"0.0942%\",\"secure.img*-fg.wfcdn.com\",\"0.0861%\"\n\"f*.bcbits.com\",\"0.0886%\",\"s*.pcdn.co\",\"0.0848%\"\n\"f.dvipcdn.com\",\"0.0848%\",\"st.hzcdn.com\",\"0.0838%\"\n\"photos*.blogger.com\",\"0.0833%\",\"assets.simpleviewinc.com\",\"0.0813%\"\n\"cdn*.weddingwire.com\",\"0.0822%\",\"fgontheweb.com\",\"0.0793%\"\n\"static.shareasale.com\",\"0.0815%\",\"images.navidirect.org\",\"0.0790%\"\n\"secure.img*-fg.wfcdn.com\",\"0.0812%\",\"cdni.rt.com\",\"0.0786%\"\n\"c*.alamy.com\",\"0.0812%\",\"downloads.intercomcdn.com\",\"0.0777%\"\n\"usercontent*.hubstatic.com\",\"0.0810%\",\"gallery.mailchimp.com\",\"0.0750%\"\n\"sc*.alicdn.com\",\"0.0803%\",\"slideplayer.com\",\"0.0690%\"\n\"static.showit.co\",\"0.0783%\",\"cdn.displays*go.com\",\"0.0677%\"\n\"i.bosscdn.com\",\"0.0764%\",\"dta*yqvfnusiq.cloudfront.net\",\"0.0660%\"\n\"*.imimg.com\",\"0.0742%\",\"\",\"\""}]}, {"page": 16, "text": "                                          (a) Images with watermarks.\n     Et                                  KG               twitchy                      E              Kidrey\n              BICOL STAIIDARDSTAHDARD                    twitch\n                                           (b) Images related to logos.\n                     ADVERTISE\n                    WITH US            Awards           6cet                   Peledijul\n                    PPLATINUM )           2016             Tuzo                                  sasOl\n                                            (c) Images related to ads.\n       Figure 9: Manually labeled images with watermarks and images related to logos or ads.\nTable 7: An example document from mmc4 with interleaved sentences and images, together with\n the CLIP ViT/-14 image-text similarities. This document contains two logo-related images (the 2nd\n& 3rd images with \u201cNELO\u201d) that are relevant to the content of this document, and are therefore\n excluded from the category of advertisement.\n Sentence                                                                       Image          CLIP Similarity\n Our new service for teams to manage their fleets\n for racing.\n Getting boats has never been this easy.\n Get a step ahead with the planning for your team                                                    23.51\n and get all the boats you need for next season\n races.\n Our new service for teams to manage their fleets                                                    22.40\n for racing.\n As easy as adding boats to a list, this service\n aims to be the simplest way to rent boats, no\n extra knowledge needed and with full support from\n our staff.\n Get all the features of a Nelo boat, from                                      OELD                 28.76\n having great equipment to our service team for\n a fraction of the price of a new boat.\n All our rental boats for racing are carefully\n maintained and revised between each race so each\n boat is as good as new.\n                                                        16", "md": "# Document\n\n(a) Images with watermarks.\n\n$$Et \\quad KG \\quad twitchy \\quad E \\quad Kidrey$$\n\nBICOL STAIIDARDSTAHDARD twitch\n\n(b) Images related to logos.\n\nADVERTISE WITH US Awards 6cet Peledijul PPLATINUM ) 2016 Tuzo sasOl\n\n(c) Images related to ads.\n\nFigure 9: Manually labeled images with watermarks and images related to logos or ads.\n\n**Table 7: An example document from mmc4 with interleaved sentences and images, together with the CLIP ViT/-14 image-text similarities.**\n|Sentence|Image|CLIP Similarity|\n|---|---|---|\n|Our new service for teams to manage their fleets for racing.| |23.51|\n|Getting boats has never been this easy.| | |\n|Get a step ahead with the planning for your team and get all the boats you need for next season races.| | |\n|Our new service for teams to manage their fleets for racing.| |22.40|\n|As easy as adding boats to a list, this service aims to be the simplest way to rent boats, no extra knowledge needed and with full support from our staff.| | |\n|Get all the features of a Nelo boat, from having great equipment to our service team for a fraction of the price of a new boat.|OELD|28.76|\n|All our rental boats for racing are carefully maintained and revised between each race so each boat is as good as new.| | |", "images": [{"name": "page-16-2.jpg", "height": 53, "width": 60, "x": 282, "y": 72}, {"name": "page-16-1.jpg", "height": 54, "width": 80, "x": 296, "y": 63}, {"name": "page-16-5.jpg", "height": 56, "width": 167, "x": 112, "y": 132}, {"name": "page-16-6.jpg", "height": 53, "width": 73, "x": 173, "y": 141}, {"name": "page-16-3.jpg", "height": 54, "width": 114, "x": 377, "y": 63}, {"name": "page-16-0.jpg", "height": 54, "width": 114, "x": 173, "y": 63}, {"name": "page-16-4.jpg", "height": 57, "width": 80, "x": 111, "y": 60}, {"name": "page-16-8.jpg", "height": 53, "width": 137, "x": 289, "y": 132}, {"name": "page-16-9.jpg", "height": 53, "width": 123, "x": 376, "y": 132}, {"name": "page-16-7.jpg", "height": 53, "width": 110, "x": 233, "y": 132}, {"name": "page-16-11.jpg", "height": 56, "width": 50, "x": 115, "y": 197}, {"name": "page-16-10.jpg", "height": 53, "width": 53, "x": 436, "y": 141}, {"name": "page-16-12.jpg", "height": 53, "width": 71, "x": 156, "y": 200}, {"name": "page-16-13.jpg", "height": 53, "width": 53, "x": 437, "y": 200}, {"name": "page-16-19.jpg", "height": 54, "width": 86, "x": 193, "y": -5}, {"name": "page-16-15.jpg", "height": 53, "width": 80, "x": 288, "y": 200}, {"name": "page-16-20.jpg", "height": 54, "width": 80, "x": 345, "y": -5}, {"name": "page-16-16.jpg", "height": 35, "width": 35, "x": 211, "y": 219}, {"name": "page-16-23.jpg", "height": 54, "width": 80, "x": 110, "y": -5}, {"name": "page-16-17.jpg", "height": 53, "width": 73, "x": 357, "y": 200}, {"name": "page-16-26.jpg", "height": 38, "width": 46, "x": 396, "y": 279}, {"name": "page-16-14.jpg", "height": 53, "width": 53, "x": 235, "y": 200}, {"name": "page-16-27.jpg", "height": 35, "width": 37, "x": 250, "y": 282}, {"name": "page-16-18.jpg", "height": 35, "width": 46, "x": 394, "y": 222}, {"name": "page-16-29.jpg", "height": 35, "width": 35, "x": 213, "y": 279}, {"name": "page-16-21.jpg", "height": 54, "width": 60, "x": 283, "y": -5}, {"name": "page-16-30.jpg", "height": 35, "width": 35, "x": 358, "y": 282}, {"name": "page-16-22.jpg", "height": 54, "width": 71, "x": 428, "y": -5}, {"name": "page-16-28.jpg", "height": 35, "width": 66, "x": 290, "y": 282}, {"name": "page-16-24.jpg", "height": 35, "width": 35, "x": 120, "y": 282}, {"name": "page-16-25.jpg", "height": 35, "width": 53, "x": 157, "y": 282}, {"name": "page-16-32.jpg", "height": 26, "width": 43, "x": 383, "y": 466}, {"name": "page-16-31.jpg", "height": 26, "width": 43, "x": 383, "y": 423}, {"name": "page-16-33.jpg", "height": 20, "width": 43, "x": 383, "y": 547}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "(a) Images with watermarks.\n\n$$Et \\quad KG \\quad twitchy \\quad E \\quad Kidrey$$\n\nBICOL STAIIDARDSTAHDARD twitch\n\n(b) Images related to logos.\n\nADVERTISE WITH US Awards 6cet Peledijul PPLATINUM ) 2016 Tuzo sasOl\n\n(c) Images related to ads.\n\nFigure 9: Manually labeled images with watermarks and images related to logos or ads.\n\n**Table 7: An example document from mmc4 with interleaved sentences and images, together with the CLIP ViT/-14 image-text similarities.**", "md": "(a) Images with watermarks.\n\n$$Et \\quad KG \\quad twitchy \\quad E \\quad Kidrey$$\n\nBICOL STAIIDARDSTAHDARD twitch\n\n(b) Images related to logos.\n\nADVERTISE WITH US Awards 6cet Peledijul PPLATINUM ) 2016 Tuzo sasOl\n\n(c) Images related to ads.\n\nFigure 9: Manually labeled images with watermarks and images related to logos or ads.\n\n**Table 7: An example document from mmc4 with interleaved sentences and images, together with the CLIP ViT/-14 image-text similarities.**"}, {"type": "table", "rows": [["Sentence", "Image", "CLIP Similarity"], ["Our new service for teams to manage their fleets for racing.", "", "23.51"], ["Getting boats has never been this easy.", "", ""], ["Get a step ahead with the planning for your team and get all the boats you need for next season races.", "", ""], ["Our new service for teams to manage their fleets for racing.", "", "22.40"], ["As easy as adding boats to a list, this service aims to be the simplest way to rent boats, no extra knowledge needed and with full support from our staff.", "", ""], ["Get all the features of a Nelo boat, from having great equipment to our service team for a fraction of the price of a new boat.", "OELD", "28.76"], ["All our rental boats for racing are carefully maintained and revised between each race so each boat is as good as new.", "", ""]], "md": "|Sentence|Image|CLIP Similarity|\n|---|---|---|\n|Our new service for teams to manage their fleets for racing.| |23.51|\n|Getting boats has never been this easy.| | |\n|Get a step ahead with the planning for your team and get all the boats you need for next season races.| | |\n|Our new service for teams to manage their fleets for racing.| |22.40|\n|As easy as adding boats to a list, this service aims to be the simplest way to rent boats, no extra knowledge needed and with full support from our staff.| | |\n|Get all the features of a Nelo boat, from having great equipment to our service team for a fraction of the price of a new boat.|OELD|28.76|\n|All our rental boats for racing are carefully maintained and revised between each race so each boat is as good as new.| | |", "isPerfectTable": true, "csv": "\"Sentence\",\"Image\",\"CLIP Similarity\"\n\"Our new service for teams to manage their fleets for racing.\",\"\",\"23.51\"\n\"Getting boats has never been this easy.\",\"\",\"\"\n\"Get a step ahead with the planning for your team and get all the boats you need for next season races.\",\"\",\"\"\n\"Our new service for teams to manage their fleets for racing.\",\"\",\"22.40\"\n\"As easy as adding boats to a list, this service aims to be the simplest way to rent boats, no extra knowledge needed and with full support from our staff.\",\"\",\"\"\n\"Get all the features of a Nelo boat, from having great equipment to our service team for a fraction of the price of a new boat.\",\"OELD\",\"28.76\"\n\"All our rental boats for racing are carefully maintained and revised between each race so each boat is as good as new.\",\"\",\"\""}]}, {"page": 17, "text": "Table 8: A document instance retrieved from the mmc4 dataset is presented, consisting of interleaved\ntextual sentences and accompanying images, along with the CLIP ViT/-14 image-text similarity\nscores.\nSentence                                                                Image         CLIP Similarity\nAre you thinking about running a retreat for your                                          25.93\nown group of people?\nWe are happy to help you hosting and organizing                                            19.71\nyour own retreat.\nWe work with your interest in mind in designing                                            21.29\nyour retreat, and we facilitate the logistics,\nsupporting you all the way for a great experience.\nNestled within powerful and deeply inspiring                                               22.35\nnature, in the heart of Tuscany, Italy, Podere\nDi Maggio is a place born of dreams.\nThe dream to be close to and learn from nature.                                            19.37\nThe dream to create and share beauty.                                                      19.16\nThe dream to discover and develop the poetry of                                            18.21\nbeing and doing.\nWe offer an invitation to explore a wide range of                                          22.69\nlife arts:      poetry, dance, music, yoga, meditation,\nritual, ceramics, painting, singing, photography,\nseeing, hearing, touching, feeling, cooking,\ncommunicating and collaborating; sharing and daring\nto discover and unfold yourself.\n                                                 17", "md": "|Sentence|Image|CLIP Similarity|\n|---|---|---|\n|Are you thinking about running a retreat for your own group of people?| |25.93|\n|We are happy to help you hosting and organizing your own retreat.| |19.71|\n|We work with your interest in mind in designing your retreat, and we facilitate the logistics, supporting you all the way for a great experience.| |21.29|\n|Nestled within powerful and deeply inspiring nature, in the heart of Tuscany, Italy, Podere Di Maggio is a place born of dreams.| |22.35|\n|The dream to be close to and learn from nature.| |19.37|\n|The dream to create and share beauty.| |19.16|\n|The dream to discover and develop the poetry of being and doing.| |18.21|\n|We offer an invitation to explore a wide range of life arts: poetry, dance, music, yoga, meditation, ritual, ceramics, painting, singing, photography, seeing, hearing, touching, feeling, cooking, communicating and collaborating; sharing and daring to discover and unfold yourself.| |22.69|", "images": [{"name": "page-17-0.jpg", "height": 43, "width": 43, "x": 389, "y": 228}, {"name": "page-17-1.jpg", "height": 32, "width": 43, "x": 389, "y": 276}, {"name": "page-17-4.jpg", "height": 64, "width": 43, "x": 389, "y": 409}, {"name": "page-17-2.jpg", "height": 30, "width": 43, "x": 389, "y": 320}, {"name": "page-17-3.jpg", "height": 43, "width": 43, "x": 389, "y": 361}, {"name": "page-17-6.jpg", "height": 43, "width": 43, "x": 389, "y": 510}, {"name": "page-17-7.jpg", "height": 43, "width": 43, "x": 389, "y": 564}, {"name": "page-17-5.jpg", "height": 27, "width": 43, "x": 389, "y": 478}], "items": [{"type": "table", "rows": [["Sentence", "Image", "CLIP Similarity"], ["Are you thinking about running a retreat for your own group of people?", "", "25.93"], ["We are happy to help you hosting and organizing your own retreat.", "", "19.71"], ["We work with your interest in mind in designing your retreat, and we facilitate the logistics, supporting you all the way for a great experience.", "", "21.29"], ["Nestled within powerful and deeply inspiring nature, in the heart of Tuscany, Italy, Podere Di Maggio is a place born of dreams.", "", "22.35"], ["The dream to be close to and learn from nature.", "", "19.37"], ["The dream to create and share beauty.", "", "19.16"], ["The dream to discover and develop the poetry of being and doing.", "", "18.21"], ["We offer an invitation to explore a wide range of life arts: poetry, dance, music, yoga, meditation, ritual, ceramics, painting, singing, photography, seeing, hearing, touching, feeling, cooking, communicating and collaborating; sharing and daring to discover and unfold yourself.", "", "22.69"]], "md": "|Sentence|Image|CLIP Similarity|\n|---|---|---|\n|Are you thinking about running a retreat for your own group of people?| |25.93|\n|We are happy to help you hosting and organizing your own retreat.| |19.71|\n|We work with your interest in mind in designing your retreat, and we facilitate the logistics, supporting you all the way for a great experience.| |21.29|\n|Nestled within powerful and deeply inspiring nature, in the heart of Tuscany, Italy, Podere Di Maggio is a place born of dreams.| |22.35|\n|The dream to be close to and learn from nature.| |19.37|\n|The dream to create and share beauty.| |19.16|\n|The dream to discover and develop the poetry of being and doing.| |18.21|\n|We offer an invitation to explore a wide range of life arts: poetry, dance, music, yoga, meditation, ritual, ceramics, painting, singing, photography, seeing, hearing, touching, feeling, cooking, communicating and collaborating; sharing and daring to discover and unfold yourself.| |22.69|", "isPerfectTable": true, "csv": "\"Sentence\",\"Image\",\"CLIP Similarity\"\n\"Are you thinking about running a retreat for your own group of people?\",\"\",\"25.93\"\n\"We are happy to help you hosting and organizing your own retreat.\",\"\",\"19.71\"\n\"We work with your interest in mind in designing your retreat, and we facilitate the logistics, supporting you all the way for a great experience.\",\"\",\"21.29\"\n\"Nestled within powerful and deeply inspiring nature, in the heart of Tuscany, Italy, Podere Di Maggio is a place born of dreams.\",\"\",\"22.35\"\n\"The dream to be close to and learn from nature.\",\"\",\"19.37\"\n\"The dream to create and share beauty.\",\"\",\"19.16\"\n\"The dream to discover and develop the poetry of being and doing.\",\"\",\"18.21\"\n\"We offer an invitation to explore a wide range of life arts: poetry, dance, music, yoga, meditation, ritual, ceramics, painting, singing, photography, seeing, hearing, touching, feeling, cooking, communicating and collaborating; sharing and daring to discover and unfold yourself.\",\"\",\"22.69\""}]}], "job_id": "51b4b228-16a9-4b97-996c-48821a1a16c4", "file_path": "./corpus/2304.06939.pdf"}