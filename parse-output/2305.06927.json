{"pages": [{"page": 1, "text": "                          Convergence of Alternating Gradient Descent for\n                                                      Matrix Factorization\n                                           Rachel Ward                                  Tamara G. Kolda\n                                        University of Texas                                  MathSci.ai\n                                             Austin, TX                                     Dublin, CA\narXiv:2305.06927v1  [cs.LG]  11 May 2023                                           tammy.kolda@mathsci.ai\n                                    rward@math.utexas.edu\n                                                                     Abstract\n                               We consider alternating gradient descent (AGD) with fixed step size \u03b7 > 0, applied\n                               to the asymmetric matrix factorization objective. We show that, for a rank-r matrix\n                               A \u2208  Rm\u00d7n, T =         \u03c31(A)   2 log(1/\u03f5)    iterations of alternating gradient descent\n                                                      \u03c3r(A)\n                               suffice to reach an \u03f5-optimal factorization \u2225A \u2212      XT Y\u22ba     F \u2264  \u03f5\u2225A\u22252 F with high\n                                                                                            T \u22252\n                               probability starting from an atypical random initialization. The factors have rank\n                               d > r so that XT \u2208       Rm\u00d7d and YT \u2208         Rn\u00d7d. Experiments suggest that our\n                               proposed initialization is not merely of theoretical benefit, but rather significantly\n                               improves convergence of gradient descent in practice. Our proof is conceptually\n                               simple: a uniform PL-inequality and uniform Lipschitz smoothness constant are\n                               guaranteed for a sufficient number of iterations, starting from our random initializa-\n                               tion. Our proof method should be useful for extending and simplifying convergence\n                               analyses for a broader class of nonconvex low-rank factorization problems.\n                     1    Introduction\n                     This paper focuses on the convergence behavior of alternating gradient descent (AGD) on the low-rank\n                     matrix factorization objective\n                                    min f(X, Y) \u2261     1                F   subject to   X \u2208   Rm\u00d7d, Y \u2208     Rn\u00d7d.             (1)\n                                                      2\u2225XY\u22ba     \u2212  A\u22252\n                     Here, we assume m, n \u226b         d > r = rank(A). While there are a multitude of more efficient\n                     algorithms for low-rank matrix approximation, this serves as a simple prototype and special case of\n                     more complicated nonlinear optimization problems where gradient descent (or stochastic gradient\n                     descent) is the method of choice but not well-understood theoretically. Such problems include\n                     low-rank tensor factorization using the GCP algorithm descent [HKD20], a stochastic gradient variant\n                     of the GCP algorithm [KH20], as well as deep learning optimization.\n                     Surprisingly, the convergence behavior of gradient descent for low-rank matrix factorization is still\n                     not optimally understood, in the sense that there is a large gap between theoretical guarantees and\n                     empirical performance. We take a step in closing this gap, providing a sharp linear convergence\n                     rate from a simple asymmetric random initialization. Precisely, we show that if A is rank-r, then a\n                     number of iterations T = C         d  \u03c31(A) 2 log(1/\u03f5)      suffices to obtain an \u03f5-optimal factorization\n                                                      d\u2212r   \u03c3r(A)\n                     with high probability. Here \u03c3k(A) denotes the kth singular value of A. To the authors\u2019 knowledge,\n                     this improves on the state-of-art convergence result in the literature [JCD22], which provides an\n                     Preprint. Under review.", "md": "# Convergence of Alternating Gradient Descent for Matrix Factorization\n\n# Convergence of Alternating Gradient Descent for Matrix Factorization\n\nRachel Ward - University of Texas, Austin, TX\n\nTamara G. Kolda - MathSci.ai, Dublin, CA\n\nEmail: tammy.kolda@mathsci.ai, rward@math.utexas.edu\n\n## Abstract\n\nWe consider alternating gradient descent (AGD) with fixed step size $$\\eta > 0$$, applied to the asymmetric matrix factorization objective. We show that, for a rank-r matrix $$A \\in \\mathbb{R}^{m \\times n}$$, $$T = \\frac{\\sigma_1(A)^2 \\log(1/\\epsilon)}{\\sigma_r(A)}$$ iterations of alternating gradient descent suffice to reach an $$\\epsilon$$-optimal factorization $$\\|A - X^TY^\\top\\|_F \\leq \\epsilon\\|A\\|_F$$ with high probability starting from an atypical random initialization. The factors have rank $$d > r$$ so that $$X^T \\in \\mathbb{R}^{m \\times d}$$ and $$Y^T \\in \\mathbb{R}^{n \\times d}$$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization. Our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems.\n\n## Introduction\n\nThis paper focuses on the convergence behavior of alternating gradient descent (AGD) on the low-rank matrix factorization objective\n\n$$\\min f(X, Y) \\equiv \\frac{1}{2}\\|XY^\\top - A\\|_F \\quad \\text{subject to} \\quad X \\in \\mathbb{R}^{m \\times d}, Y \\in \\mathbb{R}^{n \\times d} \\quad (1)$$\nHere, we assume $$m, n \\gg d > r = \\text{rank}(A)$$. While there are a multitude of more efficient algorithms for low-rank matrix approximation, this serves as a simple prototype and special case of more complicated nonlinear optimization problems where gradient descent (or stochastic gradient descent) is the method of choice but not well-understood theoretically. Such problems include low-rank tensor factorization using the GCP algorithm descent [HKD20], a stochastic gradient variant of the GCP algorithm [KH20], as well as deep learning optimization.\n\nSurprisingly, the convergence behavior of gradient descent for low-rank matrix factorization is still not optimally understood, in the sense that there is a large gap between theoretical guarantees and empirical performance. We take a step in closing this gap, providing a sharp linear convergence rate from a simple asymmetric random initialization. Precisely, we show that if $$A$$ is rank-r, then a number of iterations $$T = C \\frac{d \\sigma_1(A)^2 \\log(1/\\epsilon)}{d-r \\sigma_r(A)}$$ suffices to obtain an $$\\epsilon$$-optimal factorization with high probability. Here $$\\sigma_k(A)$$ denotes the $$k$$th singular value of $$A$$. To the authors\u2019 knowledge, this improves on the state-of-art convergence result in the literature [JCD22], which provides an Preprint. Under review.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Convergence of Alternating Gradient Descent for Matrix Factorization", "md": "# Convergence of Alternating Gradient Descent for Matrix Factorization"}, {"type": "heading", "lvl": 1, "value": "Convergence of Alternating Gradient Descent for Matrix Factorization", "md": "# Convergence of Alternating Gradient Descent for Matrix Factorization"}, {"type": "text", "value": "Rachel Ward - University of Texas, Austin, TX\n\nTamara G. Kolda - MathSci.ai, Dublin, CA\n\nEmail: tammy.kolda@mathsci.ai, rward@math.utexas.edu", "md": "Rachel Ward - University of Texas, Austin, TX\n\nTamara G. Kolda - MathSci.ai, Dublin, CA\n\nEmail: tammy.kolda@mathsci.ai, rward@math.utexas.edu"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "We consider alternating gradient descent (AGD) with fixed step size $$\\eta > 0$$, applied to the asymmetric matrix factorization objective. We show that, for a rank-r matrix $$A \\in \\mathbb{R}^{m \\times n}$$, $$T = \\frac{\\sigma_1(A)^2 \\log(1/\\epsilon)}{\\sigma_r(A)}$$ iterations of alternating gradient descent suffice to reach an $$\\epsilon$$-optimal factorization $$\\|A - X^TY^\\top\\|_F \\leq \\epsilon\\|A\\|_F$$ with high probability starting from an atypical random initialization. The factors have rank $$d > r$$ so that $$X^T \\in \\mathbb{R}^{m \\times d}$$ and $$Y^T \\in \\mathbb{R}^{n \\times d}$$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization. Our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems.", "md": "We consider alternating gradient descent (AGD) with fixed step size $$\\eta > 0$$, applied to the asymmetric matrix factorization objective. We show that, for a rank-r matrix $$A \\in \\mathbb{R}^{m \\times n}$$, $$T = \\frac{\\sigma_1(A)^2 \\log(1/\\epsilon)}{\\sigma_r(A)}$$ iterations of alternating gradient descent suffice to reach an $$\\epsilon$$-optimal factorization $$\\|A - X^TY^\\top\\|_F \\leq \\epsilon\\|A\\|_F$$ with high probability starting from an atypical random initialization. The factors have rank $$d > r$$ so that $$X^T \\in \\mathbb{R}^{m \\times d}$$ and $$Y^T \\in \\mathbb{R}^{n \\times d}$$. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves convergence of gradient descent in practice. Our proof is conceptually simple: a uniform PL-inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization. Our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "This paper focuses on the convergence behavior of alternating gradient descent (AGD) on the low-rank matrix factorization objective\n\n$$\\min f(X, Y) \\equiv \\frac{1}{2}\\|XY^\\top - A\\|_F \\quad \\text{subject to} \\quad X \\in \\mathbb{R}^{m \\times d}, Y \\in \\mathbb{R}^{n \\times d} \\quad (1)$$\nHere, we assume $$m, n \\gg d > r = \\text{rank}(A)$$. While there are a multitude of more efficient algorithms for low-rank matrix approximation, this serves as a simple prototype and special case of more complicated nonlinear optimization problems where gradient descent (or stochastic gradient descent) is the method of choice but not well-understood theoretically. Such problems include low-rank tensor factorization using the GCP algorithm descent [HKD20], a stochastic gradient variant of the GCP algorithm [KH20], as well as deep learning optimization.\n\nSurprisingly, the convergence behavior of gradient descent for low-rank matrix factorization is still not optimally understood, in the sense that there is a large gap between theoretical guarantees and empirical performance. We take a step in closing this gap, providing a sharp linear convergence rate from a simple asymmetric random initialization. Precisely, we show that if $$A$$ is rank-r, then a number of iterations $$T = C \\frac{d \\sigma_1(A)^2 \\log(1/\\epsilon)}{d-r \\sigma_r(A)}$$ suffices to obtain an $$\\epsilon$$-optimal factorization with high probability. Here $$\\sigma_k(A)$$ denotes the $$k$$th singular value of $$A$$. To the authors\u2019 knowledge, this improves on the state-of-art convergence result in the literature [JCD22], which provides an Preprint. Under review.", "md": "This paper focuses on the convergence behavior of alternating gradient descent (AGD) on the low-rank matrix factorization objective\n\n$$\\min f(X, Y) \\equiv \\frac{1}{2}\\|XY^\\top - A\\|_F \\quad \\text{subject to} \\quad X \\in \\mathbb{R}^{m \\times d}, Y \\in \\mathbb{R}^{n \\times d} \\quad (1)$$\nHere, we assume $$m, n \\gg d > r = \\text{rank}(A)$$. While there are a multitude of more efficient algorithms for low-rank matrix approximation, this serves as a simple prototype and special case of more complicated nonlinear optimization problems where gradient descent (or stochastic gradient descent) is the method of choice but not well-understood theoretically. Such problems include low-rank tensor factorization using the GCP algorithm descent [HKD20], a stochastic gradient variant of the GCP algorithm [KH20], as well as deep learning optimization.\n\nSurprisingly, the convergence behavior of gradient descent for low-rank matrix factorization is still not optimally understood, in the sense that there is a large gap between theoretical guarantees and empirical performance. We take a step in closing this gap, providing a sharp linear convergence rate from a simple asymmetric random initialization. Precisely, we show that if $$A$$ is rank-r, then a number of iterations $$T = C \\frac{d \\sigma_1(A)^2 \\log(1/\\epsilon)}{d-r \\sigma_r(A)}$$ suffices to obtain an $$\\epsilon$$-optimal factorization with high probability. Here $$\\sigma_k(A)$$ denotes the $$k$$th singular value of $$A$$. To the authors\u2019 knowledge, this improves on the state-of-art convergence result in the literature [JCD22], which provides an Preprint. Under review."}]}, {"page": 2, "text": "iteration complexity T = C         \u03c31(A)    3 log(1/\u03f5)     for gradient descent to reach an \u03f5-approximate\n                                     \u03c3r(A)\nrank-r approximation1.\nOur improved convergence analysis is likely due to our choice of initialization of X0, Y0, which\nappears to be new in the literature and is distinct from the standard Gaussian initialization. Specifically,\nfor \u03a61 and \u03a62 independent Gaussian matrices, we consider an \u201cunbalanced\u201d random initialization of\n                    1\nthe form X0 \u223c      \u221a\u03b7A\u03a61 and Y0 \u223c        \u221a\u03b7\u03a62. A crucial feature of this initialization is that the columns\nof X0 are in the column span of A, and thus by invariance of the alternating gradient update steps,\nthe columns of Xt remain in the column span of A throughout the optimization. Because of this, a\npositive rth singular value of Xt serves as a PL-inequality for the optimization process.\nThe second crucial feature of this initialization is that by Gaussian concentration, the pseudo-condition\nnumbers \u03c31(X0)\n           \u03c3r(X0) \u223c    \u03c31(A)\nthat \u03c31(Xt)            \u03c3r(A) are comparable with high probability2; for a range of step-size \u03b7, we show\n      \u03c3r(Xt) is guaranteed to remain comparable to \u03c31(A) \u03c3r(A) for a sufficiently large number of iterations t\nthat we are guaranteed a linear rate of convergence with rate        \u03c3r(X0) 2. We expect that this approach\n                                                                      \u03c31(X0)\nshould be of use in the analysis of a broader class of nonconvex optimization problems.\nOur analysis crucially depends on an unbalanced initialization of X0 and Y0, which is different\nfrom balanced random initializations standard in neural network training, which is the standard\nmotivation for studying matrix factorization with gradient descent. The unbalanced initialization is\nnot in contradiction to the known implicit bias of gradient descent towards balanced factorizations\n[CKL+21, WCZT21, ABC+22, CB22], which is of interest due to the potential link between balanced\nfactorizations and better generalization. Indeed, our experiments strongly suggest that as gradient\ndescent progresses, the unbalanced initial factor matrices become more balanced in that the factors\nincreasingly share the condition number of A between them, experiments indicate the rate of linear\nconvergence improves, potentially to proportional to ( \u03c3r(X0) \u03c31(X0)) rather than ( \u03c3r(X0)\n                                                                                      \u03c31(X0))2; see Fig. 2.\n2    Preliminaries\nConsider the square loss applied to the matrix factorization problem (1). The gradients are\n                                       \u2207 Xf(X, Y) = (XY\u22ba        \u2212  A)Y,                                      (2a)\nThe details are in Appendix B.         \u2207Yf(X, Y) = (XY\u22ba         \u2212  A)\u22baX.                                     (2b)\nWe will analyze alternating gradient descent, defined as follows.\nAssumption 1 (Alternating Gradient Descent). For fixed stepsize \u03b7 > 0 and initial condition (X0, Y0),\nthe update is\n                                      Xt+1 = Xt \u2212      \u03b7\u2207  Xf(Xt, Yt),                                     (A1a)\n                                      Yt+1 = Yt \u2212      \u03b7\u2207  Yf(Xt+1, Yt).                                   (A1b)\nWe assume that the iterations are initialized in an asymmetric way, which depends on the step size \u03b7\nand assumes a known upper bound on the spectral norm of A. The matrix factorization is of rank\nd > r, and we also make assumptions about the relationship of d, r, and quantities s, \u03b2, and \u03b4 that\nwill impact the bounds on the probability of finding and \u03f5-optimal factorization.\n    1We note that our results are not precisely directly comparable as our analysis is for alternating gradient\ndescent whereas existing results hold for gradient descent. However, empirically, alternating and non-alternating\ngradient descent exhibit similar behavior across many experiments\n    2The pseudo-condition number, \u03c31(A)\n                                     \u03c3r(A), is equivalent to and sometimes discussed as the product of the product\nof the spectral norms of the matrix and its pseudoinverse, i.e., \u2225A\u2225\u2225A  \u2020\u2225.\n                                                        2", "md": "# Math Equations and Text\n\nIteration complexity \\(T = C \\sigma_1(A) 3 \\log(1/\\epsilon)\\) for gradient descent to reach an \\(\\epsilon\\)-approximate rank-r approximation.\n\nOur improved convergence analysis is likely due to our choice of initialization of \\(X_0\\), \\(Y_0\\), which appears to be new in the literature and is distinct from the standard Gaussian initialization. Specifically, for \\(\\Phi_1\\) and \\(\\Phi_2\\) independent Gaussian matrices, we consider an \"unbalanced\" random initialization of the form \\(X_0 \\sim \\sqrt{\\eta}A\\Phi_1\\) and \\(Y_0 \\sim \\sqrt{\\eta}\\Phi_2\\). A crucial feature of this initialization is that the columns of \\(X_0\\) are in the column span of \\(A\\), and thus by invariance of the alternating gradient update steps, the columns of \\(X_t\\) remain in the column span of \\(A\\) throughout the optimization. Because of this, a positive \\(r\\)th singular value of \\(X_t\\) serves as a PL-inequality for the optimization process.\n\nThe second crucial feature of this initialization is that by Gaussian concentration, the pseudo-condition numbers \\(\\frac{\\sigma_1(X_0)}{\\sigma_r(X_0)} \\approx \\frac{\\sigma_1(A)}{\\sigma_r(A)}\\) are comparable with high probability; for a range of step-size \\(\\eta\\), we show that \\(\\sigma_r(X_t)\\) is guaranteed to remain comparable to \\(\\sigma_1(A) \\sigma_r(A)\\) for a sufficiently large number of iterations \\(t\\) that we are guaranteed a linear rate of convergence with rate \\(\\frac{\\sigma_r(X_0)}{\\sigma_1(X_0)}^2\\). We expect that this approach should be of use in the analysis of a broader class of nonconvex optimization problems.\n\nOur analysis crucially depends on an unbalanced initialization of \\(X_0\\) and \\(Y_0\\), which is different from balanced random initializations standard in neural network training, which is the standard motivation for studying matrix factorization with gradient descent. The unbalanced initialization is not in contradiction to the known implicit bias of gradient descent towards balanced factorizations, which is of interest due to the potential link between balanced factorizations and better generalization. Indeed, our experiments strongly suggest that as gradient descent progresses, the unbalanced initial factor matrices become more balanced in that the factors increasingly share the condition number of \\(A\\) between them, experiments indicate the rate of linear convergence improves, potentially to be proportional to \\(\\frac{\\sigma_r(X_0) \\sigma_1(X_0)}{\\sigma_r(X_0) \\sigma_1(X_0)^2}\\); see Fig. 2.\n\n## Preliminaries\n\nConsider the square loss applied to the matrix factorization problem (1). The gradients are\n\n\\(\\nabla_X f(X, Y) = (XY^\\top - A)Y\\)\n\n\\(\\nabla_Y f(X, Y) = (XY^\\top - A)^\\top X\\)\n\nWe will analyze alternating gradient descent, defined as follows.\n\nAssumption 1 (Alternating Gradient Descent). For fixed stepsize \\(\\eta > 0\\) and initial condition \\((X_0, Y_0)\\), the update is\n\n\\(X_{t+1} = X_t - \\eta \\nabla_X f(X_t, Y_t)\\)\n\n\\(Y_{t+1} = Y_t - \\eta \\nabla_Y f(X_{t+1}, Y_t)\\)\n\nWe assume that the iterations are initialized in an asymmetric way, which depends on the step size \\(\\eta\\) and assumes a known upper bound on the spectral norm of \\(A\\). The matrix factorization is of rank \\(d > r\\), and we also make assumptions about the relationship of \\(d\\), \\(r\\), and quantities \\(s\\), \\(\\beta\\), and \\(\\delta\\) that will impact the bounds on the probability of finding an \\(\\epsilon\\)-optimal factorization.\n\n1We note that our results are not precisely directly comparable as our analysis is for alternating gradient descent whereas existing results hold for gradient descent. However, empirically, alternating and non-alternating gradient descent exhibit similar behavior across many experiments.\n\n2The pseudo-condition number, \\(\\frac{\\sigma_1(A)}{\\sigma_r(A)}\\), is equivalent to and sometimes discussed as the product of the spectral norms of the matrix and its pseudoinverse, i.e., \\(\\|A\\| \\|A^\\dagger\\|\\).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Iteration complexity \\(T = C \\sigma_1(A) 3 \\log(1/\\epsilon)\\) for gradient descent to reach an \\(\\epsilon\\)-approximate rank-r approximation.\n\nOur improved convergence analysis is likely due to our choice of initialization of \\(X_0\\), \\(Y_0\\), which appears to be new in the literature and is distinct from the standard Gaussian initialization. Specifically, for \\(\\Phi_1\\) and \\(\\Phi_2\\) independent Gaussian matrices, we consider an \"unbalanced\" random initialization of the form \\(X_0 \\sim \\sqrt{\\eta}A\\Phi_1\\) and \\(Y_0 \\sim \\sqrt{\\eta}\\Phi_2\\). A crucial feature of this initialization is that the columns of \\(X_0\\) are in the column span of \\(A\\), and thus by invariance of the alternating gradient update steps, the columns of \\(X_t\\) remain in the column span of \\(A\\) throughout the optimization. Because of this, a positive \\(r\\)th singular value of \\(X_t\\) serves as a PL-inequality for the optimization process.\n\nThe second crucial feature of this initialization is that by Gaussian concentration, the pseudo-condition numbers \\(\\frac{\\sigma_1(X_0)}{\\sigma_r(X_0)} \\approx \\frac{\\sigma_1(A)}{\\sigma_r(A)}\\) are comparable with high probability; for a range of step-size \\(\\eta\\), we show that \\(\\sigma_r(X_t)\\) is guaranteed to remain comparable to \\(\\sigma_1(A) \\sigma_r(A)\\) for a sufficiently large number of iterations \\(t\\) that we are guaranteed a linear rate of convergence with rate \\(\\frac{\\sigma_r(X_0)}{\\sigma_1(X_0)}^2\\). We expect that this approach should be of use in the analysis of a broader class of nonconvex optimization problems.\n\nOur analysis crucially depends on an unbalanced initialization of \\(X_0\\) and \\(Y_0\\), which is different from balanced random initializations standard in neural network training, which is the standard motivation for studying matrix factorization with gradient descent. The unbalanced initialization is not in contradiction to the known implicit bias of gradient descent towards balanced factorizations, which is of interest due to the potential link between balanced factorizations and better generalization. Indeed, our experiments strongly suggest that as gradient descent progresses, the unbalanced initial factor matrices become more balanced in that the factors increasingly share the condition number of \\(A\\) between them, experiments indicate the rate of linear convergence improves, potentially to be proportional to \\(\\frac{\\sigma_r(X_0) \\sigma_1(X_0)}{\\sigma_r(X_0) \\sigma_1(X_0)^2}\\); see Fig. 2.", "md": "Iteration complexity \\(T = C \\sigma_1(A) 3 \\log(1/\\epsilon)\\) for gradient descent to reach an \\(\\epsilon\\)-approximate rank-r approximation.\n\nOur improved convergence analysis is likely due to our choice of initialization of \\(X_0\\), \\(Y_0\\), which appears to be new in the literature and is distinct from the standard Gaussian initialization. Specifically, for \\(\\Phi_1\\) and \\(\\Phi_2\\) independent Gaussian matrices, we consider an \"unbalanced\" random initialization of the form \\(X_0 \\sim \\sqrt{\\eta}A\\Phi_1\\) and \\(Y_0 \\sim \\sqrt{\\eta}\\Phi_2\\). A crucial feature of this initialization is that the columns of \\(X_0\\) are in the column span of \\(A\\), and thus by invariance of the alternating gradient update steps, the columns of \\(X_t\\) remain in the column span of \\(A\\) throughout the optimization. Because of this, a positive \\(r\\)th singular value of \\(X_t\\) serves as a PL-inequality for the optimization process.\n\nThe second crucial feature of this initialization is that by Gaussian concentration, the pseudo-condition numbers \\(\\frac{\\sigma_1(X_0)}{\\sigma_r(X_0)} \\approx \\frac{\\sigma_1(A)}{\\sigma_r(A)}\\) are comparable with high probability; for a range of step-size \\(\\eta\\), we show that \\(\\sigma_r(X_t)\\) is guaranteed to remain comparable to \\(\\sigma_1(A) \\sigma_r(A)\\) for a sufficiently large number of iterations \\(t\\) that we are guaranteed a linear rate of convergence with rate \\(\\frac{\\sigma_r(X_0)}{\\sigma_1(X_0)}^2\\). We expect that this approach should be of use in the analysis of a broader class of nonconvex optimization problems.\n\nOur analysis crucially depends on an unbalanced initialization of \\(X_0\\) and \\(Y_0\\), which is different from balanced random initializations standard in neural network training, which is the standard motivation for studying matrix factorization with gradient descent. The unbalanced initialization is not in contradiction to the known implicit bias of gradient descent towards balanced factorizations, which is of interest due to the potential link between balanced factorizations and better generalization. Indeed, our experiments strongly suggest that as gradient descent progresses, the unbalanced initial factor matrices become more balanced in that the factors increasingly share the condition number of \\(A\\) between them, experiments indicate the rate of linear convergence improves, potentially to be proportional to \\(\\frac{\\sigma_r(X_0) \\sigma_1(X_0)}{\\sigma_r(X_0) \\sigma_1(X_0)^2}\\); see Fig. 2."}, {"type": "heading", "lvl": 2, "value": "Preliminaries", "md": "## Preliminaries"}, {"type": "text", "value": "Consider the square loss applied to the matrix factorization problem (1). The gradients are\n\n\\(\\nabla_X f(X, Y) = (XY^\\top - A)Y\\)\n\n\\(\\nabla_Y f(X, Y) = (XY^\\top - A)^\\top X\\)\n\nWe will analyze alternating gradient descent, defined as follows.\n\nAssumption 1 (Alternating Gradient Descent). For fixed stepsize \\(\\eta > 0\\) and initial condition \\((X_0, Y_0)\\), the update is\n\n\\(X_{t+1} = X_t - \\eta \\nabla_X f(X_t, Y_t)\\)\n\n\\(Y_{t+1} = Y_t - \\eta \\nabla_Y f(X_{t+1}, Y_t)\\)\n\nWe assume that the iterations are initialized in an asymmetric way, which depends on the step size \\(\\eta\\) and assumes a known upper bound on the spectral norm of \\(A\\). The matrix factorization is of rank \\(d > r\\), and we also make assumptions about the relationship of \\(d\\), \\(r\\), and quantities \\(s\\), \\(\\beta\\), and \\(\\delta\\) that will impact the bounds on the probability of finding an \\(\\epsilon\\)-optimal factorization.\n\n1We note that our results are not precisely directly comparable as our analysis is for alternating gradient descent whereas existing results hold for gradient descent. However, empirically, alternating and non-alternating gradient descent exhibit similar behavior across many experiments.\n\n2The pseudo-condition number, \\(\\frac{\\sigma_1(A)}{\\sigma_r(A)}\\), is equivalent to and sometimes discussed as the product of the spectral norms of the matrix and its pseudoinverse, i.e., \\(\\|A\\| \\|A^\\dagger\\|\\).", "md": "Consider the square loss applied to the matrix factorization problem (1). The gradients are\n\n\\(\\nabla_X f(X, Y) = (XY^\\top - A)Y\\)\n\n\\(\\nabla_Y f(X, Y) = (XY^\\top - A)^\\top X\\)\n\nWe will analyze alternating gradient descent, defined as follows.\n\nAssumption 1 (Alternating Gradient Descent). For fixed stepsize \\(\\eta > 0\\) and initial condition \\((X_0, Y_0)\\), the update is\n\n\\(X_{t+1} = X_t - \\eta \\nabla_X f(X_t, Y_t)\\)\n\n\\(Y_{t+1} = Y_t - \\eta \\nabla_Y f(X_{t+1}, Y_t)\\)\n\nWe assume that the iterations are initialized in an asymmetric way, which depends on the step size \\(\\eta\\) and assumes a known upper bound on the spectral norm of \\(A\\). The matrix factorization is of rank \\(d > r\\), and we also make assumptions about the relationship of \\(d\\), \\(r\\), and quantities \\(s\\), \\(\\beta\\), and \\(\\delta\\) that will impact the bounds on the probability of finding an \\(\\epsilon\\)-optimal factorization.\n\n1We note that our results are not precisely directly comparable as our analysis is for alternating gradient descent whereas existing results hold for gradient descent. However, empirically, alternating and non-alternating gradient descent exhibit similar behavior across many experiments.\n\n2The pseudo-condition number, \\(\\frac{\\sigma_1(A)}{\\sigma_r(A)}\\), is equivalent to and sometimes discussed as the product of the spectral norms of the matrix and its pseudoinverse, i.e., \\(\\|A\\| \\|A^\\dagger\\|\\)."}]}, {"page": 3, "text": "Assumption 2 (Initialization and key quantities). Draw random matrices \u03a61, \u03a62 \u2208                                           Rn\u00d7d with i.i.d.\n N  (0, 1/d) and N        (0, 1/n) entries, respectively. Fix C \u2265                  1, \u03bd < 1, and D \u2264            C9 \u03bd, and let\n                                               1\n                            X0 =      \u03b71/2 C \u03c31(A) A\u03a61,                 and      Y0 = \u03b71/2 D \u03c31(A) \u03a62.                                  (A2a)\n The factor matrices each have d columns where d > r satisfies the following conditions. There exists\n s > 0 such that                                            \u221a  r + \u221as <        \u221a  d.                                                    (A2b)\n Define the quantity                                             \u221a  r + \u221as\n                                                    \u03c1 = 1 \u2212           \u221a  d      \u2208   (0, 1).                                             (A2c)\n The number of iterations for convergence to \u03f5-optimal factorization will ultimately be shown to\n depend on                                                   \u03b2 = \u03c12\u03c32     r(A)                                                          (A2d)\n                                                                    C2\u03c32   1(A).\n The quantity s will impact the probability of finding this \u03f5-optimal factorization. For this purpose, we\n define                                            \u03b4 = e\u2212s/2 + e\u2212r/2 + e\u2212d/2,                                                           (A2e)\n to be employed later.\n Observe that the initialization of X0 ensures its columns are in the column span of A.\n It is useful to define residuals which yield the following relations:\n          Rt \u2261     XtY\u22ba   t \u2212    A,                  f(Xt, Yt) = \u2225Rt\u22252            F,             \u2207  Xf(Xt, Yt) = RtYt,\n          \u00af                                       f(Xt+1, Yt) = \u2225           \u00af                 \u2207\n          Rt = YtX\u22ba       t+1 \u2212     A\u22ba                                     Rt\u22252   F,            Yf(Xt+1, Yt) = \u00af          RtXt+1.\nWe have conveniently transposed the second residual.\n Remark 2.1. Observe that R0 and hence f(X0, Y0) does not depend on the step size \u03b7 of \u03c31(A) in\n Assumption 2 since                       R0 = X0Y\u22ba        0 \u2212   A = A        D C \u03a61\u03a6\u22ba     2 \u2212   I    .\n Remark 2.2. In Assumption 2, we may assume that s \u226a                               1, so for all intents and purposes\n                                                           \u221a  r              1           \u221a  d\n                                             \u03c1 = 1 \u2212      \u221a  d     and       \u03c1 =    \u221a  d \u2212   \u221ar    .\n                                           1                                                  1         \u221a \u221a 1+\u03b1      2\n Further, if d = r + 1, then              \u03c12 \u2248     4r2. If d = (1 + \u03b1)r, then                \u03c12 \u2248          1+\u03b1\u22121        , and there is no\n dependency on r.\n 3     Main results\n Our first main result gives a sharp guarantee on the number of iterations necessary for alternating\n gradient descent to be guaranteed to product an \u03f5-optimal factorization.\n                                                                       3", "md": "# Math Equations and Text\n\n## Assumption 2 (Initialization and key quantities)\n\nDraw random matrices $$\\Phi_1, \\Phi_2 \\in \\mathbb{R}^{n \\times d}$$ with i.i.d. $$\\mathcal{N}(0, 1/d)$$ and $$\\mathcal{N}(0, 1/n)$$ entries, respectively. Fix $$C \\geq 1$$, $$\\nu < 1$$, and $$D \\leq C9\\nu$$, and let\n\n$$X_0 = \\eta^{1/2}C\\sigma_1(A)A\\Phi_1$$, and $$Y_0 = \\eta^{1/2}D\\sigma_1(A)\\Phi_2$$.\n\nThe factor matrices each have $$d$$ columns where $$d > r$$ satisfies the following conditions. There exists $$s > 0$$ such that\n\n$$\\sqrt{r} + \\sqrt{s} < \\sqrt{d}$$.\n\nDefine the quantity\n\n$$\\rho = 1 - \\frac{\\sqrt{r} + \\sqrt{s}}{\\sqrt{d}} \\in (0, 1)$$.\n\nThe number of iterations for convergence to $$\\epsilon$$-optimal factorization will ultimately be shown to depend on\n\n$$\\beta = \\rho^2\\sigma_2r(A) / (C^2\\sigma_1^2(A))$$.\n\nThe quantity $$s$$ will impact the probability of finding this $$\\epsilon$$-optimal factorization. For this purpose, we define\n\n$$\\delta = e^{-s/2} + e^{-r/2} + e^{-d/2}$$, to be employed later.\n\nObserve that the initialization of $$X_0$$ ensures its columns are in the column span of $$A$$.\n\nIt is useful to define residuals which yield the following relations:\n\n$$R_t = X_tY_t^T - A$$, $$f(X_t, Y_t) = ||R_t||_F$$, $$\\nabla_X f(X_t, Y_t) = R_tY_t$$,\n\n$$R_t^T = Y_tX_t^T - A^T$$, $$||R_t||_F$$, $$Yf(X_{t+1}, Y_t) = R_tX_{t+1}$$.\n\n### Remark 2.1\n\nObserve that $$R_0$$ and hence $$f(X_0, Y_0)$$ does not depend on the step size $$\\eta$$ of $$\\sigma_1(A)$$ in Assumption 2 since\n\n$$R_0 = X_0Y_0^T - A = A^TDC\\Phi_1\\Phi_2^T - I$$.\n\n### Remark 2.2\n\nIn Assumption 2, we may assume that $$s \\ll 1$$, so for all intents and purposes\n\n$$\\rho = 1 - \\frac{\\sqrt{r}}{\\sqrt{d}}$$ and $$\\rho = \\sqrt{d} - \\sqrt{r}$$.\n\nIf $$d = r + 1$$, then $$\\rho^2 \\approx 4r^2$$. If $$d = (1 + \\alpha)r$$, then $$\\rho^2 \\approx \\frac{1+\\alpha-1}{1+\\sqrt{1+\\alpha}}$$, and there is no dependency on $$r$$.\n\n## Main results\n\nOur first main result gives a sharp guarantee on the number of iterations necessary for alternating gradient descent to be guaranteed to produce an $$\\epsilon$$-optimal factorization.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Assumption 2 (Initialization and key quantities)", "md": "## Assumption 2 (Initialization and key quantities)"}, {"type": "text", "value": "Draw random matrices $$\\Phi_1, \\Phi_2 \\in \\mathbb{R}^{n \\times d}$$ with i.i.d. $$\\mathcal{N}(0, 1/d)$$ and $$\\mathcal{N}(0, 1/n)$$ entries, respectively. Fix $$C \\geq 1$$, $$\\nu < 1$$, and $$D \\leq C9\\nu$$, and let\n\n$$X_0 = \\eta^{1/2}C\\sigma_1(A)A\\Phi_1$$, and $$Y_0 = \\eta^{1/2}D\\sigma_1(A)\\Phi_2$$.\n\nThe factor matrices each have $$d$$ columns where $$d > r$$ satisfies the following conditions. There exists $$s > 0$$ such that\n\n$$\\sqrt{r} + \\sqrt{s} < \\sqrt{d}$$.\n\nDefine the quantity\n\n$$\\rho = 1 - \\frac{\\sqrt{r} + \\sqrt{s}}{\\sqrt{d}} \\in (0, 1)$$.\n\nThe number of iterations for convergence to $$\\epsilon$$-optimal factorization will ultimately be shown to depend on\n\n$$\\beta = \\rho^2\\sigma_2r(A) / (C^2\\sigma_1^2(A))$$.\n\nThe quantity $$s$$ will impact the probability of finding this $$\\epsilon$$-optimal factorization. For this purpose, we define\n\n$$\\delta = e^{-s/2} + e^{-r/2} + e^{-d/2}$$, to be employed later.\n\nObserve that the initialization of $$X_0$$ ensures its columns are in the column span of $$A$$.\n\nIt is useful to define residuals which yield the following relations:\n\n$$R_t = X_tY_t^T - A$$, $$f(X_t, Y_t) = ||R_t||_F$$, $$\\nabla_X f(X_t, Y_t) = R_tY_t$$,\n\n$$R_t^T = Y_tX_t^T - A^T$$, $$||R_t||_F$$, $$Yf(X_{t+1}, Y_t) = R_tX_{t+1}$$.", "md": "Draw random matrices $$\\Phi_1, \\Phi_2 \\in \\mathbb{R}^{n \\times d}$$ with i.i.d. $$\\mathcal{N}(0, 1/d)$$ and $$\\mathcal{N}(0, 1/n)$$ entries, respectively. Fix $$C \\geq 1$$, $$\\nu < 1$$, and $$D \\leq C9\\nu$$, and let\n\n$$X_0 = \\eta^{1/2}C\\sigma_1(A)A\\Phi_1$$, and $$Y_0 = \\eta^{1/2}D\\sigma_1(A)\\Phi_2$$.\n\nThe factor matrices each have $$d$$ columns where $$d > r$$ satisfies the following conditions. There exists $$s > 0$$ such that\n\n$$\\sqrt{r} + \\sqrt{s} < \\sqrt{d}$$.\n\nDefine the quantity\n\n$$\\rho = 1 - \\frac{\\sqrt{r} + \\sqrt{s}}{\\sqrt{d}} \\in (0, 1)$$.\n\nThe number of iterations for convergence to $$\\epsilon$$-optimal factorization will ultimately be shown to depend on\n\n$$\\beta = \\rho^2\\sigma_2r(A) / (C^2\\sigma_1^2(A))$$.\n\nThe quantity $$s$$ will impact the probability of finding this $$\\epsilon$$-optimal factorization. For this purpose, we define\n\n$$\\delta = e^{-s/2} + e^{-r/2} + e^{-d/2}$$, to be employed later.\n\nObserve that the initialization of $$X_0$$ ensures its columns are in the column span of $$A$$.\n\nIt is useful to define residuals which yield the following relations:\n\n$$R_t = X_tY_t^T - A$$, $$f(X_t, Y_t) = ||R_t||_F$$, $$\\nabla_X f(X_t, Y_t) = R_tY_t$$,\n\n$$R_t^T = Y_tX_t^T - A^T$$, $$||R_t||_F$$, $$Yf(X_{t+1}, Y_t) = R_tX_{t+1}$$."}, {"type": "heading", "lvl": 3, "value": "Remark 2.1", "md": "### Remark 2.1"}, {"type": "text", "value": "Observe that $$R_0$$ and hence $$f(X_0, Y_0)$$ does not depend on the step size $$\\eta$$ of $$\\sigma_1(A)$$ in Assumption 2 since\n\n$$R_0 = X_0Y_0^T - A = A^TDC\\Phi_1\\Phi_2^T - I$$.", "md": "Observe that $$R_0$$ and hence $$f(X_0, Y_0)$$ does not depend on the step size $$\\eta$$ of $$\\sigma_1(A)$$ in Assumption 2 since\n\n$$R_0 = X_0Y_0^T - A = A^TDC\\Phi_1\\Phi_2^T - I$$."}, {"type": "heading", "lvl": 3, "value": "Remark 2.2", "md": "### Remark 2.2"}, {"type": "text", "value": "In Assumption 2, we may assume that $$s \\ll 1$$, so for all intents and purposes\n\n$$\\rho = 1 - \\frac{\\sqrt{r}}{\\sqrt{d}}$$ and $$\\rho = \\sqrt{d} - \\sqrt{r}$$.\n\nIf $$d = r + 1$$, then $$\\rho^2 \\approx 4r^2$$. If $$d = (1 + \\alpha)r$$, then $$\\rho^2 \\approx \\frac{1+\\alpha-1}{1+\\sqrt{1+\\alpha}}$$, and there is no dependency on $$r$$.", "md": "In Assumption 2, we may assume that $$s \\ll 1$$, so for all intents and purposes\n\n$$\\rho = 1 - \\frac{\\sqrt{r}}{\\sqrt{d}}$$ and $$\\rho = \\sqrt{d} - \\sqrt{r}$$.\n\nIf $$d = r + 1$$, then $$\\rho^2 \\approx 4r^2$$. If $$d = (1 + \\alpha)r$$, then $$\\rho^2 \\approx \\frac{1+\\alpha-1}{1+\\sqrt{1+\\alpha}}$$, and there is no dependency on $$r$$."}, {"type": "heading", "lvl": 2, "value": "Main results", "md": "## Main results"}, {"type": "text", "value": "Our first main result gives a sharp guarantee on the number of iterations necessary for alternating gradient descent to be guaranteed to produce an $$\\epsilon$$-optimal factorization.", "md": "Our first main result gives a sharp guarantee on the number of iterations necessary for alternating gradient descent to be guaranteed to produce an $$\\epsilon$$-optimal factorization."}]}, {"page": 4, "text": "Theorem 3.1 (Main result, informal). For a rank-r matrix A \u2208                Rm\u00d7n, set d > r and consider\nX0, Y0 randomly initialized as in Assumption 2. For any \u03f5 > 0, there is a step-size \u03b7 = \u03b7(\u03f5) > 0 for\nalternating gradient descent as in Assumption 1 such that\n                    \u2225A \u2212   XT Y\u22ba     F \u2264   \u03f5   for all  T \u2265   C   \u03c32 1(A)   1            F\n                                  T \u22252                             \u03c32r(A)   \u03c12 log \u2225A\u22252\u03f5\nwith probability 1 \u2212     \u03b4 with respect to the draw of X0 and Y0 where \u03b4 is defined in (A2e). Here,\nC > 0 is an explicit numerical constant.\nFor more complete theorem statements, see Corollary 5.2 and Corollary 5.3 below.\nWe highlight a few points below.\n       1. The iteration complexity in Theorem 3.1 is independent of the ambient dimensions n, m. In\n                                                       1\n          the edge case d = r + 1, it holds that       \u03c12 \u2264  4r2, and so the iteration complexity depends\n          linearly on r (see Remark 2.2). With mild multiplicative overparameterization d = (1+\u03b1)r,\n                                              1        (1+\u03b1)\n          the dependence on r is erased:     \u03c12 =   (\u221a1+\u03b1\u22121)2 Here, the improved convergence guarantee\n          from mild overparameterization is directly traced to the improvement in the condition\n          number of a d \u00d7 r Gaussian random matrix as d grows from r to (1 + \u03b1)r. We note that by\n          appealing to high-probability bounds on the smallest and largest singular values of a square\n          Gaussian matrix d = r, we could also derive a result for the case d = r; see e.g. [DS01,\n          Theorem II.13] and [AZL17, Lemma i.A.3].\n       2. Experiments illustrate that initializing X0 in the column space of A, as well as rescaling\n          X0 and Y0 asymmetrically, lead to significant practical convergence improvements, and are\n          not just an artifact of our analysis; see Fig. 1.\n       3. The iteration complexity in Theorem 3.1 is conservative. In experiments, the convergence\n          tends to switch from a linear rate proportional to \u03c32  \u03c32r(A)\n           \u03c3r(A)                                                   1(A) to a faster linear rate proportional to\n           \u03c31(A).\n3.1    Our contribution and prior work\nThe seminal work of Burer and Monteiro [BM03, BM05] advocated for the general approach of using\nsimple algorithms such as gradient descent directly applied to low-rank factor matrices for solving\nnon-convex optimization problems with low-rank matrix solutions. Initial theoretical work on gradi-\nent descent for low-rank factorization problems such as [ZWL15], [TBS+16], [ZL16], [SWW17],\n[BKS16] did not prove global convergence of gradient descent, but rather local convergence of\ngradient descent starting from a spectral initialization (that is, an initialization involving SVD compu-\ntations). In almost all cases, the spectral initialization is the dominant computation, and thus a more\nglobal convergence analysis for gradient descent is desirable.\nGlobal convergence for gradient descent for matrix factorization problems without additional explicit\nregularization was first derived in the symmetric setting, where A \u2208          Rn\u00d7n is positive semi-definite,\nand f(X) = \u2225A \u2212       XX\u22ba\u22252   F, see for example [GHJY15, JJKN17, CCFM19].\nFor overparametrized symmetric matrix factorization, the convergence behavior and implicit bias\ntowards particular solutions for gradient descent with small step-size and from small initialization\nwas analyzed in the work [GWB+17, LMZ18, ACHL19, CGMR20].\nThe paper [YD21] initiated a study of gradient descent with fixed step-size in the more challenging\nsetting of asymmetric matrix factorization, where A \u2208           Rm\u00d7n is rank-r and the objective is \u2225A \u2212\nXY\u22ba\u22252   F. This work improved on previous work in the setting of gradient flow and gradient descent\nwith decreasing step-size [DHL18]. The paper [YD21] proved an iteration complexity of T =\nO   nd  \u03c31(A) 4 log(1/\u03f5)      for reaching an \u03f5-approximate matrix factorization, starting from small i.i.d.\n         \u03c31(A)\nGaussian initialization for the factors X0, Y0. More recently, [JCD22] studied gradient descent for\n                                                                                           \u03c31(A) 3 log(1/\u03f5)\nasymmetric matrix factorization, and proved an iteration complexity T = O              Cd   \u03c3r(A)\nto reach an \u03f5-optimal factorization, starting from small i.i.d. Gaussian initialization.\n                                                        4", "md": "# Math Equations and Theorems\n\n## Theorem 3.1 (Main result, informal)\n\nFor a rank-r matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), set \\(d > r\\) and consider \\(X_0, Y_0\\) randomly initialized as in Assumption 2. For any \\(\\epsilon > 0\\), there is a step-size \\(\\eta = \\eta(\\epsilon) > 0\\) for alternating gradient descent as in Assumption 1 such that\n\n$$\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon \\text{ for all } T \\geq C \\frac{\\sigma_1^2(A)}{\\sigma_r^2(A) \\rho^2 \\log \\|A\\|^2/\\epsilon}\n$$\nwith probability \\(1 - \\delta\\) with respect to the draw of \\(X_0\\) and \\(Y_0\\) where \\(\\delta\\) is defined in (A2e). Here, \\(C > 0\\) is an explicit numerical constant.\n\nFor more complete theorem statements, see Corollary 5.2 and Corollary 5.3 below.\n\n### We highlight a few points below:\n\n1. The iteration complexity in Theorem 3.1 is independent of the ambient dimensions \\(n, m\\). In the edge case \\(d = r + 1\\), it holds that \\(\\rho^2 \\leq 4r^2\\), and so the iteration complexity depends linearly on \\(r\\) (see Remark 2.2). With mild multiplicative overparameterization \\(d = (1+\\alpha)r\\), the dependence on \\(r\\) is erased: \\(\\rho^2 = (\\sqrt{1+\\alpha}-1)^2\\). Here, the improved convergence guarantee from mild overparameterization is directly traced to the improvement in the condition number of a \\(d \\times r\\) Gaussian random matrix as \\(d\\) grows from \\(r\\) to \\((1 + \\alpha)r\\). We note that by appealing to high-probability bounds on the smallest and largest singular values of a square Gaussian matrix \\(d = r\\), we could also derive a result for the case \\(d = r\\); see e.g. [DS01, Theorem II.13] and [AZL17, Lemma i.A.3].\n2. Experiments illustrate that initializing \\(X_0\\) in the column space of \\(A\\), as well as rescaling \\(X_0\\) and \\(Y_0\\) asymmetrically, lead to significant practical convergence improvements, and are not just an artifact of our analysis; see Fig. 1.\n3. The iteration complexity in Theorem 3.1 is conservative. In experiments, the convergence tends to switch from a linear rate proportional to \\(\\sigma_2 \\sigma_r(A) / \\sigma_r(A)\\) to a faster linear rate proportional to \\(\\sigma_1(A)\\).\n\n### 3.1 Our contribution and prior work\n\nThe seminal work of Burer and Monteiro [BM03, BM05] advocated for the general approach of using simple algorithms such as gradient descent directly applied to low-rank factor matrices for solving non-convex optimization problems with low-rank matrix solutions. Initial theoretical work on gradient descent for low-rank factorization problems such as [ZWL15], [TBS+16], [ZL16], [SWW17], [BKS16] did not prove global convergence of gradient descent, but rather local convergence of gradient descent starting from a spectral initialization (that is, an initialization involving SVD computations). In almost all cases, the spectral initialization is the dominant computation, and thus a more global convergence analysis for gradient descent is desirable.\n\nGlobal convergence for gradient descent for matrix factorization problems without additional explicit regularization was first derived in the symmetric setting, where \\(A \\in \\mathbb{R}^{n \\times n}\\) is positive semi-definite, and \\(f(X) = \\|A - XX^\\top\\|_F^2\\), see for example [GHJY15, JJKN17, CCFM19].\n\nFor overparametrized symmetric matrix factorization, the convergence behavior and implicit bias towards particular solutions for gradient descent with small step-size and from small initialization was analyzed in the work [GWB+17, LMZ18, ACHL19, CGMR20].\n\nThe paper [YD21] initiated a study of gradient descent with fixed step-size in the more challenging setting of asymmetric matrix factorization, where \\(A \\in \\mathbb{R}^{m \\times n}\\) is rank-r and the objective is \\(\\|A - XY^\\top\\|_F^2\\). This work improved on previous work in the setting of gradient flow and gradient descent with decreasing step-size [DHL18]. The paper [YD21] proved an iteration complexity of \\(T = O(nd \\sigma_1(A)^4 \\log(1/\\epsilon) / \\sigma_1(A))\\) for reaching an \\(\\epsilon\\)-approximate matrix factorization, starting from small i.i.d. Gaussian initialization for the factors \\(X_0, Y_0\\). More recently, [JCD22] studied gradient descent for asymmetric matrix factorization, and proved an iteration complexity \\(T = O(Cd \\sigma_r(A)^4 / \\sigma_1(A)^3 \\log(1/\\epsilon))\\) to reach an \\(\\epsilon\\)-optimal factorization, starting from small i.i.d. Gaussian initialization.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Theorems", "md": "# Math Equations and Theorems"}, {"type": "heading", "lvl": 2, "value": "Theorem 3.1 (Main result, informal)", "md": "## Theorem 3.1 (Main result, informal)"}, {"type": "text", "value": "For a rank-r matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), set \\(d > r\\) and consider \\(X_0, Y_0\\) randomly initialized as in Assumption 2. For any \\(\\epsilon > 0\\), there is a step-size \\(\\eta = \\eta(\\epsilon) > 0\\) for alternating gradient descent as in Assumption 1 such that\n\n$$\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon \\text{ for all } T \\geq C \\frac{\\sigma_1^2(A)}{\\sigma_r^2(A) \\rho^2 \\log \\|A\\|^2/\\epsilon}\n$$\nwith probability \\(1 - \\delta\\) with respect to the draw of \\(X_0\\) and \\(Y_0\\) where \\(\\delta\\) is defined in (A2e). Here, \\(C > 0\\) is an explicit numerical constant.\n\nFor more complete theorem statements, see Corollary 5.2 and Corollary 5.3 below.", "md": "For a rank-r matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), set \\(d > r\\) and consider \\(X_0, Y_0\\) randomly initialized as in Assumption 2. For any \\(\\epsilon > 0\\), there is a step-size \\(\\eta = \\eta(\\epsilon) > 0\\) for alternating gradient descent as in Assumption 1 such that\n\n$$\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon \\text{ for all } T \\geq C \\frac{\\sigma_1^2(A)}{\\sigma_r^2(A) \\rho^2 \\log \\|A\\|^2/\\epsilon}\n$$\nwith probability \\(1 - \\delta\\) with respect to the draw of \\(X_0\\) and \\(Y_0\\) where \\(\\delta\\) is defined in (A2e). Here, \\(C > 0\\) is an explicit numerical constant.\n\nFor more complete theorem statements, see Corollary 5.2 and Corollary 5.3 below."}, {"type": "heading", "lvl": 3, "value": "We highlight a few points below:", "md": "### We highlight a few points below:"}, {"type": "text", "value": "1. The iteration complexity in Theorem 3.1 is independent of the ambient dimensions \\(n, m\\). In the edge case \\(d = r + 1\\), it holds that \\(\\rho^2 \\leq 4r^2\\), and so the iteration complexity depends linearly on \\(r\\) (see Remark 2.2). With mild multiplicative overparameterization \\(d = (1+\\alpha)r\\), the dependence on \\(r\\) is erased: \\(\\rho^2 = (\\sqrt{1+\\alpha}-1)^2\\). Here, the improved convergence guarantee from mild overparameterization is directly traced to the improvement in the condition number of a \\(d \\times r\\) Gaussian random matrix as \\(d\\) grows from \\(r\\) to \\((1 + \\alpha)r\\). We note that by appealing to high-probability bounds on the smallest and largest singular values of a square Gaussian matrix \\(d = r\\), we could also derive a result for the case \\(d = r\\); see e.g. [DS01, Theorem II.13] and [AZL17, Lemma i.A.3].\n2. Experiments illustrate that initializing \\(X_0\\) in the column space of \\(A\\), as well as rescaling \\(X_0\\) and \\(Y_0\\) asymmetrically, lead to significant practical convergence improvements, and are not just an artifact of our analysis; see Fig. 1.\n3. The iteration complexity in Theorem 3.1 is conservative. In experiments, the convergence tends to switch from a linear rate proportional to \\(\\sigma_2 \\sigma_r(A) / \\sigma_r(A)\\) to a faster linear rate proportional to \\(\\sigma_1(A)\\).", "md": "1. The iteration complexity in Theorem 3.1 is independent of the ambient dimensions \\(n, m\\). In the edge case \\(d = r + 1\\), it holds that \\(\\rho^2 \\leq 4r^2\\), and so the iteration complexity depends linearly on \\(r\\) (see Remark 2.2). With mild multiplicative overparameterization \\(d = (1+\\alpha)r\\), the dependence on \\(r\\) is erased: \\(\\rho^2 = (\\sqrt{1+\\alpha}-1)^2\\). Here, the improved convergence guarantee from mild overparameterization is directly traced to the improvement in the condition number of a \\(d \\times r\\) Gaussian random matrix as \\(d\\) grows from \\(r\\) to \\((1 + \\alpha)r\\). We note that by appealing to high-probability bounds on the smallest and largest singular values of a square Gaussian matrix \\(d = r\\), we could also derive a result for the case \\(d = r\\); see e.g. [DS01, Theorem II.13] and [AZL17, Lemma i.A.3].\n2. Experiments illustrate that initializing \\(X_0\\) in the column space of \\(A\\), as well as rescaling \\(X_0\\) and \\(Y_0\\) asymmetrically, lead to significant practical convergence improvements, and are not just an artifact of our analysis; see Fig. 1.\n3. The iteration complexity in Theorem 3.1 is conservative. In experiments, the convergence tends to switch from a linear rate proportional to \\(\\sigma_2 \\sigma_r(A) / \\sigma_r(A)\\) to a faster linear rate proportional to \\(\\sigma_1(A)\\)."}, {"type": "heading", "lvl": 3, "value": "3.1 Our contribution and prior work", "md": "### 3.1 Our contribution and prior work"}, {"type": "text", "value": "The seminal work of Burer and Monteiro [BM03, BM05] advocated for the general approach of using simple algorithms such as gradient descent directly applied to low-rank factor matrices for solving non-convex optimization problems with low-rank matrix solutions. Initial theoretical work on gradient descent for low-rank factorization problems such as [ZWL15], [TBS+16], [ZL16], [SWW17], [BKS16] did not prove global convergence of gradient descent, but rather local convergence of gradient descent starting from a spectral initialization (that is, an initialization involving SVD computations). In almost all cases, the spectral initialization is the dominant computation, and thus a more global convergence analysis for gradient descent is desirable.\n\nGlobal convergence for gradient descent for matrix factorization problems without additional explicit regularization was first derived in the symmetric setting, where \\(A \\in \\mathbb{R}^{n \\times n}\\) is positive semi-definite, and \\(f(X) = \\|A - XX^\\top\\|_F^2\\), see for example [GHJY15, JJKN17, CCFM19].\n\nFor overparametrized symmetric matrix factorization, the convergence behavior and implicit bias towards particular solutions for gradient descent with small step-size and from small initialization was analyzed in the work [GWB+17, LMZ18, ACHL19, CGMR20].\n\nThe paper [YD21] initiated a study of gradient descent with fixed step-size in the more challenging setting of asymmetric matrix factorization, where \\(A \\in \\mathbb{R}^{m \\times n}\\) is rank-r and the objective is \\(\\|A - XY^\\top\\|_F^2\\). This work improved on previous work in the setting of gradient flow and gradient descent with decreasing step-size [DHL18]. The paper [YD21] proved an iteration complexity of \\(T = O(nd \\sigma_1(A)^4 \\log(1/\\epsilon) / \\sigma_1(A))\\) for reaching an \\(\\epsilon\\)-approximate matrix factorization, starting from small i.i.d. Gaussian initialization for the factors \\(X_0, Y_0\\). More recently, [JCD22] studied gradient descent for asymmetric matrix factorization, and proved an iteration complexity \\(T = O(Cd \\sigma_r(A)^4 / \\sigma_1(A)^3 \\log(1/\\epsilon))\\) to reach an \\(\\epsilon\\)-optimal factorization, starting from small i.i.d. Gaussian initialization.", "md": "The seminal work of Burer and Monteiro [BM03, BM05] advocated for the general approach of using simple algorithms such as gradient descent directly applied to low-rank factor matrices for solving non-convex optimization problems with low-rank matrix solutions. Initial theoretical work on gradient descent for low-rank factorization problems such as [ZWL15], [TBS+16], [ZL16], [SWW17], [BKS16] did not prove global convergence of gradient descent, but rather local convergence of gradient descent starting from a spectral initialization (that is, an initialization involving SVD computations). In almost all cases, the spectral initialization is the dominant computation, and thus a more global convergence analysis for gradient descent is desirable.\n\nGlobal convergence for gradient descent for matrix factorization problems without additional explicit regularization was first derived in the symmetric setting, where \\(A \\in \\mathbb{R}^{n \\times n}\\) is positive semi-definite, and \\(f(X) = \\|A - XX^\\top\\|_F^2\\), see for example [GHJY15, JJKN17, CCFM19].\n\nFor overparametrized symmetric matrix factorization, the convergence behavior and implicit bias towards particular solutions for gradient descent with small step-size and from small initialization was analyzed in the work [GWB+17, LMZ18, ACHL19, CGMR20].\n\nThe paper [YD21] initiated a study of gradient descent with fixed step-size in the more challenging setting of asymmetric matrix factorization, where \\(A \\in \\mathbb{R}^{m \\times n}\\) is rank-r and the objective is \\(\\|A - XY^\\top\\|_F^2\\). This work improved on previous work in the setting of gradient flow and gradient descent with decreasing step-size [DHL18]. The paper [YD21] proved an iteration complexity of \\(T = O(nd \\sigma_1(A)^4 \\log(1/\\epsilon) / \\sigma_1(A))\\) for reaching an \\(\\epsilon\\)-approximate matrix factorization, starting from small i.i.d. Gaussian initialization for the factors \\(X_0, Y_0\\). More recently, [JCD22] studied gradient descent for asymmetric matrix factorization, and proved an iteration complexity \\(T = O(Cd \\sigma_r(A)^4 / \\sigma_1(A)^3 \\log(1/\\epsilon))\\) to reach an \\(\\epsilon\\)-optimal factorization, starting from small i.i.d. Gaussian initialization."}]}, {"page": 5, "text": "We improve on previous analysis of gradient descent applied to objectives of the form (1), providing an\n improved iteration complexity T = O                      \u03c31(A) 2 log(1/\u03f5)             to reach an \u03f5-approximate factorization.\n                                                            \u03c3r(A)\n There is no dependence on the matrix dimensions in our bound, and the dependence on the rank\n r disappears if the optimization is mildly over-parameterized, i.e., d = (1 + \u03b1)r. We do note that\n our results are not directly comparable to previous work as we analyze alternating gradient descent\n rather than full gradient descent. Our method of proof is conceptually simpler than previous works;\n in particular, because our initialization X0 is in the column span of A, we do not require a two-stage\n analysis and instead can prove a fast linear convergence from the initial iteration.\n 4     Preliminary lemmas\n Lemma 4.1 (Bounding sum of norms of gradients). Consider alternating gradient decent as in\nAssumption 1. If \u2225Yt\u22252 \u2264                1\n                                        \u03b7 , then\n                                  \u2225\u2207   Xf(Xt, Yt)\u22252       F \u2264    2   f(Xt, Yt) \u2212         f(Xt+1, Yt)          .                             (3)\n                                                                 \u03b7\n                                   2                                                                                                1\n If moreover \u2225Xt\u22252 \u2264               \u03b7 , then f(Xt, Yt) \u2264             f(Xt, Yt\u22121). Consequently, if \u2225Yt\u22252 \u2264                           \u03b7 for all\n t = 0, . . . , T, and \u2225Xt\u22252 \u2264            2\n                                          \u03b7 for all t = 0, . . . , T, then\n                                              T    \u2225\u2207  Xf(Xt, Yt)\u22252       F \u2264    2                                                          (4)\n                                             t=0                                 \u03b7 f(X0, Y0)\n Likewise, if \u2225Xt+1\u22252 \u2264             1\n                                    \u03b7 , then\n                            \u2225\u2207Y f(Xt+1, Yt)\u22252            F \u2264    2                                                                           (5)\n                                    2                           \u03b7 (f(Xt+1, Yt) \u2212           f(Xt+1, Yt+1)).                   1\n If moreover \u2225Yt\u22252 \u2264               \u03b7 , then f(Xt+1, Yt) \u2264               f(Xt, Yt), and so if \u2225Xt+1\u22252 \u2264                       \u03b7 for all t =\n 0, . . . , T, and \u2225Yt\u22252 \u2264          2\n                                    \u03b7 for all t = 0, . . . , T, then\n                                             T   \u2225\u2207Yf(Xt+1, Yt)\u22252           F \u2264    2                                                        (6)\n                                           t=0                                     \u03b7 f(X0, Y0)\n Proof. Using alternating gradient descent per Assumption 1, we have\n                          \u00af\n                          Rt = Rt(I \u2212          \u03b7YtY\u22ba             and      Rt+1 = (I \u2212         \u03b7Xt+1X\u22ba            Rt.\n                                                         t ),                                              t+1)   \u00af\n If \u2225Yt\u2225     \u2264   \u221a\u03b71  , we have \u2225  \u00af\n                                  Rt\u22252  F = \u2225Rt\u22252      F \u2212    2\u03b7\u2225RtYt\u22252       F + \u03b72\u2225RtYtY\u22ba           t \u22252F\n                                            \u2264  \u2225Rt\u22252   F \u2212    2\u03b7\u2225RtYt\u22252       F + \u03b72\u2225Yt\u22252\u2225RtYt\u22252              F\n                                            \u2264  \u2225Rt\u22252   F \u2212    \u03b7\u2225RtYt\u22252      F.\n The first line uses the cyclic invariance of trace, i.e., trace(YtY\u22ba                        t R\u22ba t Rt) = trace(R\u22ba         t RtYtY\u22ba      t ) =\n trace(RtYtY\u22ba         t R\u22ba t ). The second lines uses the inequality \u2225AB\u2225F \u2264                         \u2225A\u2225\u2225B\u2225F (and also the cyclic\n invariance of trace). The last line uses the assumption on \u2225Yt\u2225. Thus, rearranging terms yields\n                                              \u2225RtYt\u22252                  \u2225Rt\u22252           Rt\u22252      .                                          (7)\n                                                           F \u2264    1            F \u2212   \u2225 \u00af     F\n                                                                  \u03b7\n Now, observe that if \u2225Xt\u22252 \u2264                 2\n                                              \u03b7 , then\n                                                          \u2225Rt\u22252           Rt\u22121\u22252                                                            (8)\n                                                                  F \u2264  5\u2225  \u00af        F.", "md": "We improve on previous analysis of gradient descent applied to objectives of the form (1), providing an improved iteration complexity \\(T = O\\frac{{\\sigma_1(A)^2 \\log(1/\\epsilon)}}{{\\sigma_r(A)}}\\) to reach an \\(\\epsilon\\)-approximate factorization.\n\nThere is no dependence on the matrix dimensions in our bound, and the dependence on the rank \\(r\\) disappears if the optimization is mildly over-parameterized, i.e., \\(d = (1 + \\alpha)r\\). We do note that our results are not directly comparable to previous work as we analyze alternating gradient descent rather than full gradient descent. Our method of proof is conceptually simpler than previous works; in particular, because our initialization \\(X_0\\) is in the column span of \\(A\\), we do not require a two-stage analysis and instead can prove a fast linear convergence from the initial iteration.\n\n### Preliminary lemmas\n\n**Lemma 4.1 (Bounding sum of norms of gradients):** Consider alternating gradient descent as in Assumption 1. If \\(\\|Y_t\\|_2 \\leq \\eta\\), then\n\n\\[\n\\|\\nabla_X f(X_t, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} [f(X_t, Y_t) - f(X_{t+1}, Y_t)] \\quad (3)\n\\]\n\nIf moreover \\(\\|X_t\\|_2 \\leq \\eta\\), then \\(f(X_t, Y_t) \\leq f(X_t, Y_{t-1})\\). Consequently, if \\(\\|Y_t\\|_2 \\leq \\eta\\) for all \\(t = 0, \\ldots, T\\), and \\(\\|X_t\\|_2 \\leq 2\\eta\\) for all \\(t = 0, \\ldots, T\\), then\n\n\\[\n\\sum_{t=0}^T \\|\\nabla_X f(X_t, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} f(X_0, Y_0) \\quad (4)\n\\]\n\nLikewise, if \\(\\|X_{t+1}\\|_2 \\leq \\frac{1}{\\eta}\\), then\n\n\\[\n\\|\\nabla_Y f(X_{t+1}, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} [f(X_{t+1}, Y_t) - f(X_{t+1}, Y_{t+1})] \\quad (5)\n\\]\n\nIf moreover \\(\\|Y_t\\|_2 \\leq \\eta\\), then \\(f(X_{t+1}, Y_t) \\leq f(X_t, Y_t)\\), and so if \\(\\|X_{t+1}\\|_2 \\leq \\eta\\) for all \\(t = 0, \\ldots, T\\), and \\(\\|Y_t\\|_2 \\leq 2\\eta\\) for all \\(t = 0, \\ldots, T\\), then\n\n\\[\n\\sum_{t=0}^T \\|\\nabla_Y f(X_{t+1}, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} f(X_0, Y_0) \\quad (6)\n\\]\n\n**Proof:** Using alternating gradient descent per Assumption 1, we have\n\n\\[\n\\bar{R}_t = R_t(I - \\eta Y_t Y_t^\\top) \\quad \\text{and} \\quad \\bar{R}_{t+1} = (I - \\eta X_{t+1} X_t^\\top) \\bar{R}_t.\n\\]\n\nIf \\(\\|Y_t\\| \\leq \\sqrt{\\eta}\\), we have\n\n\\[\n\\|\\bar{R}_t\\|_2^F = \\|R_t\\|_2^F - 2\\eta \\|R_t Y_t\\|_2^F + \\eta^2 \\|R_t Y_t Y_t^\\top\\|_2^F \\leq \\|R_t\\|_2^F - 2\\eta \\|R_t Y_t\\|_2^F + \\eta^2 \\|Y_t\\|_2 \\|R_t Y_t\\|_2^F \\leq \\|R_t\\|_2^F - \\eta \\|R_t Y_t\\|_2^F.\n\\]\n\nNow, observe that if \\(\\|X_t\\|_2 \\leq 2\\eta\\), then\n\n\\[\n\\|R_t\\|_2^F \\leq 5\\|\\bar{R}_t\\|_2^F.\n\\]", "images": [], "items": [{"type": "text", "value": "We improve on previous analysis of gradient descent applied to objectives of the form (1), providing an improved iteration complexity \\(T = O\\frac{{\\sigma_1(A)^2 \\log(1/\\epsilon)}}{{\\sigma_r(A)}}\\) to reach an \\(\\epsilon\\)-approximate factorization.\n\nThere is no dependence on the matrix dimensions in our bound, and the dependence on the rank \\(r\\) disappears if the optimization is mildly over-parameterized, i.e., \\(d = (1 + \\alpha)r\\). We do note that our results are not directly comparable to previous work as we analyze alternating gradient descent rather than full gradient descent. Our method of proof is conceptually simpler than previous works; in particular, because our initialization \\(X_0\\) is in the column span of \\(A\\), we do not require a two-stage analysis and instead can prove a fast linear convergence from the initial iteration.", "md": "We improve on previous analysis of gradient descent applied to objectives of the form (1), providing an improved iteration complexity \\(T = O\\frac{{\\sigma_1(A)^2 \\log(1/\\epsilon)}}{{\\sigma_r(A)}}\\) to reach an \\(\\epsilon\\)-approximate factorization.\n\nThere is no dependence on the matrix dimensions in our bound, and the dependence on the rank \\(r\\) disappears if the optimization is mildly over-parameterized, i.e., \\(d = (1 + \\alpha)r\\). We do note that our results are not directly comparable to previous work as we analyze alternating gradient descent rather than full gradient descent. Our method of proof is conceptually simpler than previous works; in particular, because our initialization \\(X_0\\) is in the column span of \\(A\\), we do not require a two-stage analysis and instead can prove a fast linear convergence from the initial iteration."}, {"type": "heading", "lvl": 3, "value": "Preliminary lemmas", "md": "### Preliminary lemmas"}, {"type": "text", "value": "**Lemma 4.1 (Bounding sum of norms of gradients):** Consider alternating gradient descent as in Assumption 1. If \\(\\|Y_t\\|_2 \\leq \\eta\\), then\n\n\\[\n\\|\\nabla_X f(X_t, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} [f(X_t, Y_t) - f(X_{t+1}, Y_t)] \\quad (3)\n\\]\n\nIf moreover \\(\\|X_t\\|_2 \\leq \\eta\\), then \\(f(X_t, Y_t) \\leq f(X_t, Y_{t-1})\\). Consequently, if \\(\\|Y_t\\|_2 \\leq \\eta\\) for all \\(t = 0, \\ldots, T\\), and \\(\\|X_t\\|_2 \\leq 2\\eta\\) for all \\(t = 0, \\ldots, T\\), then\n\n\\[\n\\sum_{t=0}^T \\|\\nabla_X f(X_t, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} f(X_0, Y_0) \\quad (4)\n\\]\n\nLikewise, if \\(\\|X_{t+1}\\|_2 \\leq \\frac{1}{\\eta}\\), then\n\n\\[\n\\|\\nabla_Y f(X_{t+1}, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} [f(X_{t+1}, Y_t) - f(X_{t+1}, Y_{t+1})] \\quad (5)\n\\]\n\nIf moreover \\(\\|Y_t\\|_2 \\leq \\eta\\), then \\(f(X_{t+1}, Y_t) \\leq f(X_t, Y_t)\\), and so if \\(\\|X_{t+1}\\|_2 \\leq \\eta\\) for all \\(t = 0, \\ldots, T\\), and \\(\\|Y_t\\|_2 \\leq 2\\eta\\) for all \\(t = 0, \\ldots, T\\), then\n\n\\[\n\\sum_{t=0}^T \\|\\nabla_Y f(X_{t+1}, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} f(X_0, Y_0) \\quad (6)\n\\]\n\n**Proof:** Using alternating gradient descent per Assumption 1, we have\n\n\\[\n\\bar{R}_t = R_t(I - \\eta Y_t Y_t^\\top) \\quad \\text{and} \\quad \\bar{R}_{t+1} = (I - \\eta X_{t+1} X_t^\\top) \\bar{R}_t.\n\\]\n\nIf \\(\\|Y_t\\| \\leq \\sqrt{\\eta}\\), we have\n\n\\[\n\\|\\bar{R}_t\\|_2^F = \\|R_t\\|_2^F - 2\\eta \\|R_t Y_t\\|_2^F + \\eta^2 \\|R_t Y_t Y_t^\\top\\|_2^F \\leq \\|R_t\\|_2^F - 2\\eta \\|R_t Y_t\\|_2^F + \\eta^2 \\|Y_t\\|_2 \\|R_t Y_t\\|_2^F \\leq \\|R_t\\|_2^F - \\eta \\|R_t Y_t\\|_2^F.\n\\]\n\nNow, observe that if \\(\\|X_t\\|_2 \\leq 2\\eta\\), then\n\n\\[\n\\|R_t\\|_2^F \\leq 5\\|\\bar{R}_t\\|_2^F.\n\\]", "md": "**Lemma 4.1 (Bounding sum of norms of gradients):** Consider alternating gradient descent as in Assumption 1. If \\(\\|Y_t\\|_2 \\leq \\eta\\), then\n\n\\[\n\\|\\nabla_X f(X_t, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} [f(X_t, Y_t) - f(X_{t+1}, Y_t)] \\quad (3)\n\\]\n\nIf moreover \\(\\|X_t\\|_2 \\leq \\eta\\), then \\(f(X_t, Y_t) \\leq f(X_t, Y_{t-1})\\). Consequently, if \\(\\|Y_t\\|_2 \\leq \\eta\\) for all \\(t = 0, \\ldots, T\\), and \\(\\|X_t\\|_2 \\leq 2\\eta\\) for all \\(t = 0, \\ldots, T\\), then\n\n\\[\n\\sum_{t=0}^T \\|\\nabla_X f(X_t, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} f(X_0, Y_0) \\quad (4)\n\\]\n\nLikewise, if \\(\\|X_{t+1}\\|_2 \\leq \\frac{1}{\\eta}\\), then\n\n\\[\n\\|\\nabla_Y f(X_{t+1}, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} [f(X_{t+1}, Y_t) - f(X_{t+1}, Y_{t+1})] \\quad (5)\n\\]\n\nIf moreover \\(\\|Y_t\\|_2 \\leq \\eta\\), then \\(f(X_{t+1}, Y_t) \\leq f(X_t, Y_t)\\), and so if \\(\\|X_{t+1}\\|_2 \\leq \\eta\\) for all \\(t = 0, \\ldots, T\\), and \\(\\|Y_t\\|_2 \\leq 2\\eta\\) for all \\(t = 0, \\ldots, T\\), then\n\n\\[\n\\sum_{t=0}^T \\|\\nabla_Y f(X_{t+1}, Y_t)\\|_2^F \\leq \\frac{2}{\\eta} f(X_0, Y_0) \\quad (6)\n\\]\n\n**Proof:** Using alternating gradient descent per Assumption 1, we have\n\n\\[\n\\bar{R}_t = R_t(I - \\eta Y_t Y_t^\\top) \\quad \\text{and} \\quad \\bar{R}_{t+1} = (I - \\eta X_{t+1} X_t^\\top) \\bar{R}_t.\n\\]\n\nIf \\(\\|Y_t\\| \\leq \\sqrt{\\eta}\\), we have\n\n\\[\n\\|\\bar{R}_t\\|_2^F = \\|R_t\\|_2^F - 2\\eta \\|R_t Y_t\\|_2^F + \\eta^2 \\|R_t Y_t Y_t^\\top\\|_2^F \\leq \\|R_t\\|_2^F - 2\\eta \\|R_t Y_t\\|_2^F + \\eta^2 \\|Y_t\\|_2 \\|R_t Y_t\\|_2^F \\leq \\|R_t\\|_2^F - \\eta \\|R_t Y_t\\|_2^F.\n\\]\n\nNow, observe that if \\(\\|X_t\\|_2 \\leq 2\\eta\\), then\n\n\\[\n\\|R_t\\|_2^F \\leq 5\\|\\bar{R}_t\\|_2^F.\n\\]"}]}, {"page": 6, "text": "Using the assumptions on the norms of the factors, we then have\n                   T   \u2225\u2207Xf(Xt, Yt)\u22252       F =    T   \u2225RtYt\u22252    F \u2264   1   T    \u2225\u00afRt\u22121\u22252  F \u2212   \u2225\u00afRt\u22252 F\n                  t=0                             t=0                   \u03b7  t=0\n                             \u2264   1  \u00afR0   2F.\n                                 \u03b7\nThe first inequality comes from (7). The last summation is a telescoping sum.\nThe second inequality is proven analogously.\nProposition 4.2 (Bounding singular values of iterates). Consider alternating gradient decent as in\nAssumption 1. Set f0 := f(X0, Y0). Set T\u2217                =       1      . Suppose \u03c32   1(X0) \u2264        9     1(Y0) \u2264       9\n                                                              32\u03b72f  0                              16\u03b7, \u03c32              16\u03b7.\nThen for all 0 \u2264     T \u2264 1  T\u2217,                     1\n       1. \u2225XT \u2225     \u2264   \u221a\u03b7     and     \u2225YT \u2225    \u2264  \u221a\u03b7,\n       2. \u03c3r(X0) \u2212       \u221a2T\u03b7f0 \u2264       \u03c3r(XT ) \u2264      \u03c31(XT ) \u2264      \u03c31(X0) + \u221a2T\u03b7f0,\n       3. \u03c3r(Y0) \u2212       \u221a2T\u03b7f0 \u2264        \u03c3r(YT ) \u2264     \u03c31(YT ) \u2264      \u03c31(Y0) + \u221a2T\u03b7f0.\n                                                                                  9        1\nProof. First, observe that by assumption, \u2225X0\u22252, \u2225Y0\u22252 \u2264                         16\u03b7 \u2264     \u03b7. Now, suppose that that\n                         9                                                                                     1\n\u2225X0\u22252, \u2225Y0\u22252 \u2264          16\u03b7 and \u2225Xt\u22252, \u2225Yt\u22252 \u2264            1\nby Lemma 4.1,                                             \u03b7 for t = 0, . . . T \u2212    1, and 1 \u2264     T \u2264    \u230a32\u03b72f0 \u230b. Then\n                                            T \u22121     Xf(Xt, Yt)       2 \u2264   2                                             (9)\nHence,                                      t=0   \u2207                   F     \u03b7 f0.\n                                   \u2225XT \u2212     X0\u2225    \u2264  \u03b7  T \u22121  \u2207 Xf(Xt, Yt)\n                                                           t=0\n                                                    \u2264  \u03b7  T \u22121  \u2207 Xf(Xt, Yt)      F\n                                                           t=0\n                                                    \u2264  \u03b7  T    T \u22121 \u2225\u2207Xf(Xt, Yt)\u22252       F\n                                                               t=0\nThen, for T \u2264      T\u2217,                              \u2264  \u03b7  2T \u03b7 f0.                                                       (10)\n                                           \u2225XT \u2225    \u2264  \u2225X0\u2225    + \u2225XT \u2212       X0\u2225\n                                                    \u2264     3          2T\u03b7f0\n                                                        4\u221a\u03b7 +\n                                                         1\n                                                    \u2264   \u221a\u03b7 .                                                             (11)\nand so it follows that \u2225Xt\u22252 \u2264          \u03b71for t = 0, . . . T. Using Lemma 4.1 again, and repeating the same\nargument,\n                                                         1\n                                            \u2225Yt\u2225    \u2264   \u221a\u03b7 ,     t = 0, . . . , T.\n                                                                          1                                            1\nThus, we can iterate the induction until T = T\u2217                  = \u230a32\u03b72f0 \u230b, to obtain \u2225Xt\u22252, \u2225Yt\u22252 \u2264                 \u03b7 for\nt = 1, . . . , T\u2217.\n                                                              6", "md": "# Math Equations and Text\n\nUsing the assumptions on the norms of the factors, we then have\n\n$$\n\\begin{align*}\nT \\sum_{t=0} \\|\\nabla Xf(X_t, Y_t)\\|_2^2 &\\leq F = \\sum_{t=0} \\|\\bar{R}_t Y_t\\|_2^2 \\leq \\frac{1}{\\eta} \\sum_{t=0} \\|\\bar{R}_{t-1}\\|_2^F - \\|\\bar{R}_t\\|_2^F \\\\\n&\\leq \\frac{1}{\\eta} \\|\\bar{R}_0\\|_2^F.\n\\end{align*}\n$$\nThe first inequality comes from (7). The last summation is a telescoping sum.\n\nThe second inequality is proven analogously.\n\nProposition 4.2 (Bounding singular values of iterates). Consider alternating gradient descent as in Assumption 1. Set $f_0 := f(X_0, Y_0)$. Set $T^* = \\frac{1}{32\\eta^2f_0}$. Suppose $\\sigma_2^1(X_0) \\leq \\frac{9}{16\\eta}$, $\\sigma_1^1(Y_0) \\leq \\frac{9}{16\\eta}$, $\\sigma_2 \\leq \\frac{16\\eta}{\\sigma_1}$, and $\\sigma_1 \\leq \\frac{16\\eta}{\\sigma_1}$. Then for all $0 \\leq T \\leq T^*$,\n\n1. $\\|X_T\\| \\leq \\sqrt{\\eta}$ and $\\|Y_T\\| \\leq \\sqrt{\\eta}$,\n2. $\\sigma_r(X_0) - \\sqrt{2T\\eta f_0} \\leq \\sigma_r(X_T) \\leq \\sigma_1(X_T) \\leq \\sigma_1(X_0) + \\sqrt{2T\\eta f_0}$,\n3. $\\sigma_r(Y_0) - \\sqrt{2T\\eta f_0} \\leq \\sigma_r(Y_T) \\leq \\sigma_1(Y_T) \\leq \\sigma_1(Y_0) + \\sqrt{2T\\eta f_0}$.\n\nProof. First, observe that by assumption, $\\|X_0\\|_2, \\|Y_0\\|_2 \\leq \\frac{16\\eta}{\\sigma_1} \\leq \\eta$. Now, suppose that $\\|X_0\\|_2, \\|Y_0\\|_2 \\leq \\frac{16\\eta}{\\sigma_1}$ and $\\|X_t\\|_2, \\|Y_t\\|_2 \\leq \\frac{1}{\\eta}$ for $t = 0, \\ldots, T - 1$, and $1 \\leq T \\leq \\left\\lfloor 32\\eta^2f_0 \\right\\rfloor$. Then\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\|Xf(X_t, Y_t)\\|_2^2 &\\leq 2\\eta f_0. \\tag{9} \\\\\n\\|X_T - X_0\\| &\\leq \\eta \\sum_{t=0}^{T-1} \\nabla Xf(X_t, Y_t) \\\\\n&\\leq \\eta \\sum_{t=0}^{T-1} \\nabla Xf(X_t, Y_t) F \\\\\n&\\leq \\eta T(T-1) \\|\\nabla Xf(X_t, Y_t)\\|_2^2 F \\\\\n&\\leq \\eta 2T\\eta f_0. \\tag{10} \\\\\n\\|X_T\\| &\\leq \\|X_0\\| + \\|X_T - X_0\\| \\\\\n&\\leq \\frac{3}{4}\\sqrt{\\eta f_0} + \\frac{1}{4}\\sqrt{\\eta} \\\\\n&\\leq \\sqrt{\\eta}. \\tag{11}\n\\end{align*}\n$$\nand so it follows that $\\|X_t\\|_2 \\leq \\eta$ for $t = 0, \\ldots, T$. Using Lemma 4.1 again, and repeating the same argument,\n\n$$\n\\|Y_t\\| \\leq \\sqrt{\\eta}, \\quad t = 0, \\ldots, T. \\tag{12}\n$$\nThus, we can iterate the induction until $T = T^* = \\left\\lfloor 32\\eta^2f_0 \\right\\rfloor$, to obtain $\\|X_t\\|_2, \\|Y_t\\|_2 \\leq \\eta$ for $t = 1, \\ldots, T^*$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Using the assumptions on the norms of the factors, we then have\n\n$$\n\\begin{align*}\nT \\sum_{t=0} \\|\\nabla Xf(X_t, Y_t)\\|_2^2 &\\leq F = \\sum_{t=0} \\|\\bar{R}_t Y_t\\|_2^2 \\leq \\frac{1}{\\eta} \\sum_{t=0} \\|\\bar{R}_{t-1}\\|_2^F - \\|\\bar{R}_t\\|_2^F \\\\\n&\\leq \\frac{1}{\\eta} \\|\\bar{R}_0\\|_2^F.\n\\end{align*}\n$$\nThe first inequality comes from (7). The last summation is a telescoping sum.\n\nThe second inequality is proven analogously.\n\nProposition 4.2 (Bounding singular values of iterates). Consider alternating gradient descent as in Assumption 1. Set $f_0 := f(X_0, Y_0)$. Set $T^* = \\frac{1}{32\\eta^2f_0}$. Suppose $\\sigma_2^1(X_0) \\leq \\frac{9}{16\\eta}$, $\\sigma_1^1(Y_0) \\leq \\frac{9}{16\\eta}$, $\\sigma_2 \\leq \\frac{16\\eta}{\\sigma_1}$, and $\\sigma_1 \\leq \\frac{16\\eta}{\\sigma_1}$. Then for all $0 \\leq T \\leq T^*$,\n\n1. $\\|X_T\\| \\leq \\sqrt{\\eta}$ and $\\|Y_T\\| \\leq \\sqrt{\\eta}$,\n2. $\\sigma_r(X_0) - \\sqrt{2T\\eta f_0} \\leq \\sigma_r(X_T) \\leq \\sigma_1(X_T) \\leq \\sigma_1(X_0) + \\sqrt{2T\\eta f_0}$,\n3. $\\sigma_r(Y_0) - \\sqrt{2T\\eta f_0} \\leq \\sigma_r(Y_T) \\leq \\sigma_1(Y_T) \\leq \\sigma_1(Y_0) + \\sqrt{2T\\eta f_0}$.\n\nProof. First, observe that by assumption, $\\|X_0\\|_2, \\|Y_0\\|_2 \\leq \\frac{16\\eta}{\\sigma_1} \\leq \\eta$. Now, suppose that $\\|X_0\\|_2, \\|Y_0\\|_2 \\leq \\frac{16\\eta}{\\sigma_1}$ and $\\|X_t\\|_2, \\|Y_t\\|_2 \\leq \\frac{1}{\\eta}$ for $t = 0, \\ldots, T - 1$, and $1 \\leq T \\leq \\left\\lfloor 32\\eta^2f_0 \\right\\rfloor$. Then\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\|Xf(X_t, Y_t)\\|_2^2 &\\leq 2\\eta f_0. \\tag{9} \\\\\n\\|X_T - X_0\\| &\\leq \\eta \\sum_{t=0}^{T-1} \\nabla Xf(X_t, Y_t) \\\\\n&\\leq \\eta \\sum_{t=0}^{T-1} \\nabla Xf(X_t, Y_t) F \\\\\n&\\leq \\eta T(T-1) \\|\\nabla Xf(X_t, Y_t)\\|_2^2 F \\\\\n&\\leq \\eta 2T\\eta f_0. \\tag{10} \\\\\n\\|X_T\\| &\\leq \\|X_0\\| + \\|X_T - X_0\\| \\\\\n&\\leq \\frac{3}{4}\\sqrt{\\eta f_0} + \\frac{1}{4}\\sqrt{\\eta} \\\\\n&\\leq \\sqrt{\\eta}. \\tag{11}\n\\end{align*}\n$$\nand so it follows that $\\|X_t\\|_2 \\leq \\eta$ for $t = 0, \\ldots, T$. Using Lemma 4.1 again, and repeating the same argument,\n\n$$\n\\|Y_t\\| \\leq \\sqrt{\\eta}, \\quad t = 0, \\ldots, T. \\tag{12}\n$$\nThus, we can iterate the induction until $T = T^* = \\left\\lfloor 32\\eta^2f_0 \\right\\rfloor$, to obtain $\\|X_t\\|_2, \\|Y_t\\|_2 \\leq \\eta$ for $t = 1, \\ldots, T^*$.", "md": "Using the assumptions on the norms of the factors, we then have\n\n$$\n\\begin{align*}\nT \\sum_{t=0} \\|\\nabla Xf(X_t, Y_t)\\|_2^2 &\\leq F = \\sum_{t=0} \\|\\bar{R}_t Y_t\\|_2^2 \\leq \\frac{1}{\\eta} \\sum_{t=0} \\|\\bar{R}_{t-1}\\|_2^F - \\|\\bar{R}_t\\|_2^F \\\\\n&\\leq \\frac{1}{\\eta} \\|\\bar{R}_0\\|_2^F.\n\\end{align*}\n$$\nThe first inequality comes from (7). The last summation is a telescoping sum.\n\nThe second inequality is proven analogously.\n\nProposition 4.2 (Bounding singular values of iterates). Consider alternating gradient descent as in Assumption 1. Set $f_0 := f(X_0, Y_0)$. Set $T^* = \\frac{1}{32\\eta^2f_0}$. Suppose $\\sigma_2^1(X_0) \\leq \\frac{9}{16\\eta}$, $\\sigma_1^1(Y_0) \\leq \\frac{9}{16\\eta}$, $\\sigma_2 \\leq \\frac{16\\eta}{\\sigma_1}$, and $\\sigma_1 \\leq \\frac{16\\eta}{\\sigma_1}$. Then for all $0 \\leq T \\leq T^*$,\n\n1. $\\|X_T\\| \\leq \\sqrt{\\eta}$ and $\\|Y_T\\| \\leq \\sqrt{\\eta}$,\n2. $\\sigma_r(X_0) - \\sqrt{2T\\eta f_0} \\leq \\sigma_r(X_T) \\leq \\sigma_1(X_T) \\leq \\sigma_1(X_0) + \\sqrt{2T\\eta f_0}$,\n3. $\\sigma_r(Y_0) - \\sqrt{2T\\eta f_0} \\leq \\sigma_r(Y_T) \\leq \\sigma_1(Y_T) \\leq \\sigma_1(Y_0) + \\sqrt{2T\\eta f_0}$.\n\nProof. First, observe that by assumption, $\\|X_0\\|_2, \\|Y_0\\|_2 \\leq \\frac{16\\eta}{\\sigma_1} \\leq \\eta$. Now, suppose that $\\|X_0\\|_2, \\|Y_0\\|_2 \\leq \\frac{16\\eta}{\\sigma_1}$ and $\\|X_t\\|_2, \\|Y_t\\|_2 \\leq \\frac{1}{\\eta}$ for $t = 0, \\ldots, T - 1$, and $1 \\leq T \\leq \\left\\lfloor 32\\eta^2f_0 \\right\\rfloor$. Then\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\|Xf(X_t, Y_t)\\|_2^2 &\\leq 2\\eta f_0. \\tag{9} \\\\\n\\|X_T - X_0\\| &\\leq \\eta \\sum_{t=0}^{T-1} \\nabla Xf(X_t, Y_t) \\\\\n&\\leq \\eta \\sum_{t=0}^{T-1} \\nabla Xf(X_t, Y_t) F \\\\\n&\\leq \\eta T(T-1) \\|\\nabla Xf(X_t, Y_t)\\|_2^2 F \\\\\n&\\leq \\eta 2T\\eta f_0. \\tag{10} \\\\\n\\|X_T\\| &\\leq \\|X_0\\| + \\|X_T - X_0\\| \\\\\n&\\leq \\frac{3}{4}\\sqrt{\\eta f_0} + \\frac{1}{4}\\sqrt{\\eta} \\\\\n&\\leq \\sqrt{\\eta}. \\tag{11}\n\\end{align*}\n$$\nand so it follows that $\\|X_t\\|_2 \\leq \\eta$ for $t = 0, \\ldots, T$. Using Lemma 4.1 again, and repeating the same argument,\n\n$$\n\\|Y_t\\| \\leq \\sqrt{\\eta}, \\quad t = 0, \\ldots, T. \\tag{12}\n$$\nThus, we can iterate the induction until $T = T^* = \\left\\lfloor 32\\eta^2f_0 \\right\\rfloor$, to obtain $\\|X_t\\|_2, \\|Y_t\\|_2 \\leq \\eta$ for $t = 1, \\ldots, T^*$."}]}, {"page": 7, "text": "                                                                                    1\nBecause \u2225XT \u2212              X0\u2225     \u2264    \u221a2\u03b7T      f0 for T \u2264         T\u2217   = \u230a32\u03b72f0 \u230b,\n                  \u03c3r(XT ) \u2265         \u03c3r(X0) \u2212          \u2225XT \u2212        X0\u2225;             \u03c31(XT ) \u2264         \u03c31(X0) + \u2225XT \u2212              X0\u2225.\nA similar argument applies to achieve the stated bounds for \u03c3r(YT ) and \u03c31(YT ).\nProposition 4.3 (Initialization). Assume X0 and Y0 are initialized as in Assumption 2, which fixes\nC \u2265      1, \u03bd < 1, and D \u2264                C9 \u03bd, and consider alternating gradient decent as in Assumption 1. Then\nwith probability at least 1 \u2212                 \u03b4, with respect to the random initialization and \u03b4 defined in (A2e), the\nfollowing hold:\n                1     \u03c1  \u03c3r(A)\n         1.    \u221a\u03b7    C   \u03c31(A) \u2264    3     \u03c3r(X0),\n         2. \u03c31(X0) \u2264             C\u221a\u03b7 ,\n                                 \u221a\u03b7 C \u03bd \u03c31(A)\n         3. \u03c31(Y0) \u2264                       3             ,\n         4. f(X0, Y0) \u2264               1                       F.\n                                      2(1 + \u03bd)2\u2225A\u22252\nProof. Write the SVD A = Um\u00d7r\u03a3r\u00d7rV\u22ba                                     r\u00d7n so that A\u03a61 = Um\u00d7r\u03a3r\u00d7r(V\u22ba\u03a61). Note that\nV\u22ba\u03a6      1 \u2208    Rr\u00d7d has i.i.d. Gaussian entries N                      (0, 1 d). By concentration of measure Proposition A.1,\nwith probability at least 1 \u2212                e\u2212s/2,            \u221a  r + \u221as\n                                                        1 \u2212         \u221a  d          \u2264    \u03c3r(V\u22ba\u03a61).\nApplying concentration of measure Proposition A.1 again, with probability at least 1\u2212e\u2212r/2 \u2212e\u2212d/2,\n                                                 \u03c31(\u03a61) \u2264            1 + 2\u221ar \u221a   d      \u2264   3,      and\nIf these events hold,                            \u03c31(\u03a62) \u2264            1 + 2   \u221a \u221am d     \u2264   3.\nand,                                                \u03c31(V\u22ba\u03a61) \u2264             \u03c31(V)\u03c31(\u03a61) \u2264                3,\n                           \u03c1                                                                  3\n                   \u221a\u03b7C\u03c31(A)\u03c3r(A) \u2264                    \u03c3r(X0) \u2264          \u03c31(X0) \u2264          \u221a\u03b7C ,\nwhere the last inequality uses D \u2264                       C\u03bd             \u03c31(Y0) \u2264          3\u221a\u03b7D\u03c31(A) \u2264                \u221a\u03b7C\u03bd\u03c31(A) 3         .\n                                                          9 . Consequently,\n          1 \u2212    \u03bd \u2264     1 \u2212    D                                  I \u2212     D            2    \u2264   1 + D\nHence,                          C \u03c31(\u03a61)\u03c31(\u03a62) \u2264                           C \u03a61\u03a6\u22ba                        C \u03c31(\u03a61)\u03c31(\u03a62) \u2264               1 + \u03bd.\n                                  2f(X0, Y0) =               A(I \u2212        D           2)   2                             F.\nCombining the previous two propositions gives the following corollary.    C \u03a61\u03a6\u22ba            F \u2264     (1 + \u03bd)2\u2225A\u22252\n                                                                              7", "md": "Because $$\\|X^T - X^0\\| \\leq \\sqrt{2}\\eta^T f^0$$ for $$T \\leq T^* = \\left\\lfloor 32\\eta^2f^0 \\right\\rfloor$$,\n\n$$\\sigma_r(X^T) \\geq \\sigma_r(X^0) - \\|X^T - X^0\\|; \\quad \\sigma_1(X^T) \\leq \\sigma_1(X^0) + \\|X^T - X^0\\|.$$\n\nA similar argument applies to achieve the stated bounds for $$\\sigma_r(Y^T)$$ and $$\\sigma_1(Y^T)$$.\n\nProposition 4.3 (Initialization). Assume $$X^0$$ and $$Y^0$$ are initialized as in Assumption 2, which fixes $$C \\geq 1, \\nu < 1$$, and $$D \\leq C9\\nu$$, and consider alternating gradient descent as in Assumption 1. Then with probability at least $$1 - \\delta$$, with respect to the random initialization and $$\\delta$$ defined in (A2e), the following hold:\n\n1. $\\frac{1}{\\sqrt{\\eta}}\\rho\\sigma_r(A) \\leq 3\\sigma_r(X^0),$\n2. $\\sigma_1(X^0) \\leq C\\sqrt{\\eta},$\n3. $\\sigma_1(Y^0) \\leq \\frac{3\\sqrt{\\eta}C\\nu\\sigma_1(A)}{3},$\n4. $f(X^0, Y^0) \\leq \\frac{1}{2(1 + \\nu)^2\\|A\\|^2}F.$\n\nProof. Write the SVD $$A = U_{m \\times r}\\Sigma_{r \\times r}V^{\\top}_{r \\times n}$$ so that $$A\\Phi_1 = U_{m \\times r}\\Sigma_{r \\times r}(V^{\\top}\\Phi_1)$$. Note that $$V^{\\top}\\Phi_1 \\in \\mathbb{R}^{r \\times d}$$ has i.i.d. Gaussian entries $$N(0, \\frac{1}{d})$$. By concentration of measure Proposition A.1, with probability at least $$1 - e^{-s/2}, 1 - \\frac{\\sqrt{r} + \\sqrt{s}}{\\sqrt{d}} \\leq \\sigma_r(V^{\\top}\\Phi_1)$$. Applying concentration of measure Proposition A.1 again, with probability at least $$1 - e^{-r/2} - e^{-d/2}$$,\n\n$$\\sigma_1(\\Phi_1) \\leq 1 + 2\\sqrt{r\\sqrt{d}} \\leq 3$$, and\n\nIf these events hold,\n\n$$\\sigma_1(\\Phi_2) \\leq 1 + 2\\sqrt{\\sqrt{m}d} \\leq 3$$.\n\nand,\n\n$$\\sigma_1(V^{\\top}\\Phi_1) \\leq \\sigma_1(V)\\sigma_1(\\Phi_1) \\leq 3$$,\n\n$$\\frac{\\rho}{\\sqrt{\\eta}C\\sigma_1(A)\\sigma_r(A)} \\leq \\sigma_r(X^0) \\leq \\sigma_1(X^0) \\leq \\sqrt{\\eta}C$$,\n\nwhere the last inequality uses $$D \\leq C\\nu$$, $$\\sigma_1(Y^0) \\leq 3\\sqrt{\\eta}D\\sigma_1(A) \\leq \\sqrt{\\eta}C\\nu\\sigma_1(A)3$$.\n\n$$1 - \\nu \\leq 1 - D, I - D2 \\leq 1 + D$$\n\nHence, $$C\\sigma_1(\\Phi_1)\\sigma_1(\\Phi_2) \\leq C\\Phi_1\\Phi^{\\top} \\leq C\\sigma_1(\\Phi_1)\\sigma_1(\\Phi_2) \\leq 1 + \\nu$$.\n\n$$\\frac{1}{2}f(X^0, Y^0) = A(I - D2)2F$$.\n\nCombining the previous two propositions gives the following corollary. $$C\\Phi_1\\Phi^{\\top}F \\leq (1 + \\nu)^2\\|A\\|^2$$.\n\n$$\\frac{7}{\\sqrt{}}$$", "images": [], "items": [{"type": "text", "value": "Because $$\\|X^T - X^0\\| \\leq \\sqrt{2}\\eta^T f^0$$ for $$T \\leq T^* = \\left\\lfloor 32\\eta^2f^0 \\right\\rfloor$$,\n\n$$\\sigma_r(X^T) \\geq \\sigma_r(X^0) - \\|X^T - X^0\\|; \\quad \\sigma_1(X^T) \\leq \\sigma_1(X^0) + \\|X^T - X^0\\|.$$\n\nA similar argument applies to achieve the stated bounds for $$\\sigma_r(Y^T)$$ and $$\\sigma_1(Y^T)$$.\n\nProposition 4.3 (Initialization). Assume $$X^0$$ and $$Y^0$$ are initialized as in Assumption 2, which fixes $$C \\geq 1, \\nu < 1$$, and $$D \\leq C9\\nu$$, and consider alternating gradient descent as in Assumption 1. Then with probability at least $$1 - \\delta$$, with respect to the random initialization and $$\\delta$$ defined in (A2e), the following hold:\n\n1. $\\frac{1}{\\sqrt{\\eta}}\\rho\\sigma_r(A) \\leq 3\\sigma_r(X^0),$\n2. $\\sigma_1(X^0) \\leq C\\sqrt{\\eta},$\n3. $\\sigma_1(Y^0) \\leq \\frac{3\\sqrt{\\eta}C\\nu\\sigma_1(A)}{3},$\n4. $f(X^0, Y^0) \\leq \\frac{1}{2(1 + \\nu)^2\\|A\\|^2}F.$\n\nProof. Write the SVD $$A = U_{m \\times r}\\Sigma_{r \\times r}V^{\\top}_{r \\times n}$$ so that $$A\\Phi_1 = U_{m \\times r}\\Sigma_{r \\times r}(V^{\\top}\\Phi_1)$$. Note that $$V^{\\top}\\Phi_1 \\in \\mathbb{R}^{r \\times d}$$ has i.i.d. Gaussian entries $$N(0, \\frac{1}{d})$$. By concentration of measure Proposition A.1, with probability at least $$1 - e^{-s/2}, 1 - \\frac{\\sqrt{r} + \\sqrt{s}}{\\sqrt{d}} \\leq \\sigma_r(V^{\\top}\\Phi_1)$$. Applying concentration of measure Proposition A.1 again, with probability at least $$1 - e^{-r/2} - e^{-d/2}$$,\n\n$$\\sigma_1(\\Phi_1) \\leq 1 + 2\\sqrt{r\\sqrt{d}} \\leq 3$$, and\n\nIf these events hold,\n\n$$\\sigma_1(\\Phi_2) \\leq 1 + 2\\sqrt{\\sqrt{m}d} \\leq 3$$.\n\nand,\n\n$$\\sigma_1(V^{\\top}\\Phi_1) \\leq \\sigma_1(V)\\sigma_1(\\Phi_1) \\leq 3$$,\n\n$$\\frac{\\rho}{\\sqrt{\\eta}C\\sigma_1(A)\\sigma_r(A)} \\leq \\sigma_r(X^0) \\leq \\sigma_1(X^0) \\leq \\sqrt{\\eta}C$$,\n\nwhere the last inequality uses $$D \\leq C\\nu$$, $$\\sigma_1(Y^0) \\leq 3\\sqrt{\\eta}D\\sigma_1(A) \\leq \\sqrt{\\eta}C\\nu\\sigma_1(A)3$$.\n\n$$1 - \\nu \\leq 1 - D, I - D2 \\leq 1 + D$$\n\nHence, $$C\\sigma_1(\\Phi_1)\\sigma_1(\\Phi_2) \\leq C\\Phi_1\\Phi^{\\top} \\leq C\\sigma_1(\\Phi_1)\\sigma_1(\\Phi_2) \\leq 1 + \\nu$$.\n\n$$\\frac{1}{2}f(X^0, Y^0) = A(I - D2)2F$$.\n\nCombining the previous two propositions gives the following corollary. $$C\\Phi_1\\Phi^{\\top}F \\leq (1 + \\nu)^2\\|A\\|^2$$.\n\n$$\\frac{7}{\\sqrt{}}$$", "md": "Because $$\\|X^T - X^0\\| \\leq \\sqrt{2}\\eta^T f^0$$ for $$T \\leq T^* = \\left\\lfloor 32\\eta^2f^0 \\right\\rfloor$$,\n\n$$\\sigma_r(X^T) \\geq \\sigma_r(X^0) - \\|X^T - X^0\\|; \\quad \\sigma_1(X^T) \\leq \\sigma_1(X^0) + \\|X^T - X^0\\|.$$\n\nA similar argument applies to achieve the stated bounds for $$\\sigma_r(Y^T)$$ and $$\\sigma_1(Y^T)$$.\n\nProposition 4.3 (Initialization). Assume $$X^0$$ and $$Y^0$$ are initialized as in Assumption 2, which fixes $$C \\geq 1, \\nu < 1$$, and $$D \\leq C9\\nu$$, and consider alternating gradient descent as in Assumption 1. Then with probability at least $$1 - \\delta$$, with respect to the random initialization and $$\\delta$$ defined in (A2e), the following hold:\n\n1. $\\frac{1}{\\sqrt{\\eta}}\\rho\\sigma_r(A) \\leq 3\\sigma_r(X^0),$\n2. $\\sigma_1(X^0) \\leq C\\sqrt{\\eta},$\n3. $\\sigma_1(Y^0) \\leq \\frac{3\\sqrt{\\eta}C\\nu\\sigma_1(A)}{3},$\n4. $f(X^0, Y^0) \\leq \\frac{1}{2(1 + \\nu)^2\\|A\\|^2}F.$\n\nProof. Write the SVD $$A = U_{m \\times r}\\Sigma_{r \\times r}V^{\\top}_{r \\times n}$$ so that $$A\\Phi_1 = U_{m \\times r}\\Sigma_{r \\times r}(V^{\\top}\\Phi_1)$$. Note that $$V^{\\top}\\Phi_1 \\in \\mathbb{R}^{r \\times d}$$ has i.i.d. Gaussian entries $$N(0, \\frac{1}{d})$$. By concentration of measure Proposition A.1, with probability at least $$1 - e^{-s/2}, 1 - \\frac{\\sqrt{r} + \\sqrt{s}}{\\sqrt{d}} \\leq \\sigma_r(V^{\\top}\\Phi_1)$$. Applying concentration of measure Proposition A.1 again, with probability at least $$1 - e^{-r/2} - e^{-d/2}$$,\n\n$$\\sigma_1(\\Phi_1) \\leq 1 + 2\\sqrt{r\\sqrt{d}} \\leq 3$$, and\n\nIf these events hold,\n\n$$\\sigma_1(\\Phi_2) \\leq 1 + 2\\sqrt{\\sqrt{m}d} \\leq 3$$.\n\nand,\n\n$$\\sigma_1(V^{\\top}\\Phi_1) \\leq \\sigma_1(V)\\sigma_1(\\Phi_1) \\leq 3$$,\n\n$$\\frac{\\rho}{\\sqrt{\\eta}C\\sigma_1(A)\\sigma_r(A)} \\leq \\sigma_r(X^0) \\leq \\sigma_1(X^0) \\leq \\sqrt{\\eta}C$$,\n\nwhere the last inequality uses $$D \\leq C\\nu$$, $$\\sigma_1(Y^0) \\leq 3\\sqrt{\\eta}D\\sigma_1(A) \\leq \\sqrt{\\eta}C\\nu\\sigma_1(A)3$$.\n\n$$1 - \\nu \\leq 1 - D, I - D2 \\leq 1 + D$$\n\nHence, $$C\\sigma_1(\\Phi_1)\\sigma_1(\\Phi_2) \\leq C\\Phi_1\\Phi^{\\top} \\leq C\\sigma_1(\\Phi_1)\\sigma_1(\\Phi_2) \\leq 1 + \\nu$$.\n\n$$\\frac{1}{2}f(X^0, Y^0) = A(I - D2)2F$$.\n\nCombining the previous two propositions gives the following corollary. $$C\\Phi_1\\Phi^{\\top}F \\leq (1 + \\nu)^2\\|A\\|^2$$.\n\n$$\\frac{7}{\\sqrt{}}$$"}]}, {"page": 8, "text": " Corollary 4.4. Assume X0 and Y0 are initialized as in Assumption 2, with the stronger assumption\n that C \u2265      4. Consider alternating gradient decent as in Assumption 1 with\n                                                                          9\nWith \u03b2 as in (A2d) and f0 = f(X0, Y0), set                  \u03b7 \u2264    4C\u03bd\u03c31(A).\n                                                             T =          \u03b2        .\n                                                                       8\u03b72f   0\nThen with probability at least 1 \u2212                \u03b4, with respect to the random initialization and \u03b4 defined in (A2e),\n the following hold for all t = 1, . . . , T            :\n         1. \u03c3r(Xt) \u2265          1  \u03b2\n                              2     \u03b7\n         2. \u03c31(Xt), \u03c31(Yt) \u2264              C\u221a\u03b73+ 1     2  \u03b2  \u03b7\n Proof. By Proposition 4.3, we have the following event occurring with the stated probability:\n                                            \u03c12 \u03c32 r(A)                                             9\n                                           C2\u03c32  1(A)\u03b7 \u2264        \u03c32r(X0) \u2264       \u03c321(X0) \u2264        16\u03b7   9\n where the upper bound uses that C \u2265                    4. Moreover, using that \u03b7 \u2264        9      4C\u03bd\u03c31(A),\n For \u03b2 as in (A2d), note that                   \u03c321(Y0) \u2264        9\u03b7C2\u03c31(A)2 \u2264            16\u03b7 .\n                                                   T =          \u03b2        \u2264          1        ,\n                                                             8\u03b72f   0           32\u03b72f0\n which means that we can apply Proposition 4.2 up to iteration T                             , resulting in the bound\n                                          \u03c3r(Xt) \u2265        \u03c3r(X0) \u2212           2T   \u03b7f0 \u2265     1      \u03b2\n                                                                                            2      \u03b7 ,\n                                                     3           \u03b2\n and, similarly, \u03c31(Xt), \u03c31(Yt) \u2264                  C\u221a\u03b7 + 1    2     \u03b7 .\n Finally, we use a couple crucial lemmas which apply to our initialization of X0 and Y0.\n Lemma 4.5. Consider alternating gradient decent as in Assumption 1.                                               If ColSpan(X0) \u2282\n ColSpan(A), then ColSpan(Xt) \u2282                     ColSpan(A) for all t.\n Proof. Suppose ColSpan(Xt)                      \u2282      ColSpan(A) and ColSpan(Yt)                         \u2282     RowSpan(A). Then\n ColSpan(XtY\u22ba         t Yt) \u2282      ColSpan(A) and by the update of Assumption 1,\n                  ColSpan(Xt+1) = ColSpan(Xt + \u03b7AYt \u2212                              \u03b7XtY\u22ba     t Yt)\n                                           \u2282   ColSpan(Xt) \u222a          ColSpan(AYt) \u222a             ColSpan(XtY\u22ba          t Yt)\n                                           \u2282   ColSpan(A).\n Lemma 4.6. If A is rank r, and if ColSpan(Xt) \u2282                            ColSpan(A) and \u03c3r(Xt) > 0 then\n                                       \u2225\u2207  Yf(Xt, Yt\u22121)\u22252         F \u2265    2\u03c32 r(Xt)f(Xt, Yt\u22121).                                       (12)\n                                                                       8", "md": "Corollary 4.4. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumption\nthat \\(C \\geq 4\\). Consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\beta \\text{ as in (A2d) and } f_0 = f(X_0, Y_0), \\text{ set } \\eta \\leq \\frac{4C\\nu\\sigma_1(A)}{8\\eta^2f_0}\n$$\n\nThen with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e),\nthe following hold for all \\(t = 1, ..., T\\):\n\n1. \\(\\sigma_r(X_t) \\geq \\frac{1}{2\\beta\\eta}\\)\n2. \\(\\sigma_1(X_t), \\sigma_1(Y_t) \\leq \\frac{C\\sqrt{\\eta^3 + 1}}{2\\beta\\eta}\\)\n\nProof. By Proposition 4.3, we have the following event occurring with the stated probability:\n\n$$\nC^2\\sigma_1^2(A)\\eta \\leq \\sigma_2r(X_0) \\leq \\sigma_1^2(X_0) \\leq 16\\eta\n$$\n\nwhere the upper bound uses that \\(C \\geq 4\\). Moreover, using that \\(\\eta \\leq \\frac{9}{4C\\nu\\sigma_1(A)}\\),\nFor \\(\\beta\\) as in (A2d), note that \\(\\sigma_1^2(Y_0) \\leq 9\\eta C^2\\sigma_1(A)^2 \\leq 16\\eta\\).\n\n$$\nT = \\frac{\\beta}{8\\eta^2f_0} \\leq 1\n$$\n\nwhich means that we can apply Proposition 4.2 up to iteration \\(T\\), resulting in the bound\n\n$$\n\\sigma_r(X_t) \\geq \\sigma_r(X_0) - \\frac{2T\\eta f_0}{2\\eta} \\geq \\frac{1}{\\beta\\eta}\n$$\n\nand, similarly, \\(\\sigma_1(X_t), \\sigma_1(Y_t) \\leq \\frac{C\\sqrt{\\eta} + 1}{2\\eta}\\).\n\nFinally, we use a couple crucial lemmas which apply to our initialization of \\(X_0\\) and \\(Y_0\\).\n\nLemma 4.5. Consider alternating gradient descent as in Assumption 1. If \\(\\text{ColSpan}(X_0) \\subset \\text{ColSpan}(A)\\),\nthen \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) for all \\(t\\).\n\nProof. Suppose \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) and \\(\\text{ColSpan}(Y_t) \\subset \\text{RowSpan}(A)\\). Then\n\n$$\n\\text{ColSpan}(X_tY_t^T Y_t) \\subset \\text{ColSpan}(A) \\text{ and by the update of Assumption 1,}\n$$\n\n$$\n\\text{ColSpan}(X_{t+1}) = \\text{ColSpan}(X_t + \\eta AY_t - \\eta X_tY_t^T Y_t) \\subset \\text{ColSpan}(X_t) \\cup \\text{ColSpan}(AY_t) \\cup \\text{ColSpan}(X_tY_t^T Y_t) \\subset \\text{ColSpan}(A).\n$$\n\nLemma 4.6. If \\(A\\) is rank \\(r\\), and if \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) and \\(\\sigma_r(X_t) > 0\\) then\n\n$$\n\\| \\nabla_Y f(X_t, Y_{t-1}) \\|_F \\geq \\frac{2\\sigma_r^2(X_t)f(X_t, Y_{t-1})}{8}\n$$", "images": [], "items": [{"type": "text", "value": "Corollary 4.4. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumption\nthat \\(C \\geq 4\\). Consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\beta \\text{ as in (A2d) and } f_0 = f(X_0, Y_0), \\text{ set } \\eta \\leq \\frac{4C\\nu\\sigma_1(A)}{8\\eta^2f_0}\n$$\n\nThen with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e),\nthe following hold for all \\(t = 1, ..., T\\):\n\n1. \\(\\sigma_r(X_t) \\geq \\frac{1}{2\\beta\\eta}\\)\n2. \\(\\sigma_1(X_t), \\sigma_1(Y_t) \\leq \\frac{C\\sqrt{\\eta^3 + 1}}{2\\beta\\eta}\\)\n\nProof. By Proposition 4.3, we have the following event occurring with the stated probability:\n\n$$\nC^2\\sigma_1^2(A)\\eta \\leq \\sigma_2r(X_0) \\leq \\sigma_1^2(X_0) \\leq 16\\eta\n$$\n\nwhere the upper bound uses that \\(C \\geq 4\\). Moreover, using that \\(\\eta \\leq \\frac{9}{4C\\nu\\sigma_1(A)}\\),\nFor \\(\\beta\\) as in (A2d), note that \\(\\sigma_1^2(Y_0) \\leq 9\\eta C^2\\sigma_1(A)^2 \\leq 16\\eta\\).\n\n$$\nT = \\frac{\\beta}{8\\eta^2f_0} \\leq 1\n$$\n\nwhich means that we can apply Proposition 4.2 up to iteration \\(T\\), resulting in the bound\n\n$$\n\\sigma_r(X_t) \\geq \\sigma_r(X_0) - \\frac{2T\\eta f_0}{2\\eta} \\geq \\frac{1}{\\beta\\eta}\n$$\n\nand, similarly, \\(\\sigma_1(X_t), \\sigma_1(Y_t) \\leq \\frac{C\\sqrt{\\eta} + 1}{2\\eta}\\).\n\nFinally, we use a couple crucial lemmas which apply to our initialization of \\(X_0\\) and \\(Y_0\\).\n\nLemma 4.5. Consider alternating gradient descent as in Assumption 1. If \\(\\text{ColSpan}(X_0) \\subset \\text{ColSpan}(A)\\),\nthen \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) for all \\(t\\).\n\nProof. Suppose \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) and \\(\\text{ColSpan}(Y_t) \\subset \\text{RowSpan}(A)\\). Then\n\n$$\n\\text{ColSpan}(X_tY_t^T Y_t) \\subset \\text{ColSpan}(A) \\text{ and by the update of Assumption 1,}\n$$\n\n$$\n\\text{ColSpan}(X_{t+1}) = \\text{ColSpan}(X_t + \\eta AY_t - \\eta X_tY_t^T Y_t) \\subset \\text{ColSpan}(X_t) \\cup \\text{ColSpan}(AY_t) \\cup \\text{ColSpan}(X_tY_t^T Y_t) \\subset \\text{ColSpan}(A).\n$$\n\nLemma 4.6. If \\(A\\) is rank \\(r\\), and if \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) and \\(\\sigma_r(X_t) > 0\\) then\n\n$$\n\\| \\nabla_Y f(X_t, Y_{t-1}) \\|_F \\geq \\frac{2\\sigma_r^2(X_t)f(X_t, Y_{t-1})}{8}\n$$", "md": "Corollary 4.4. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumption\nthat \\(C \\geq 4\\). Consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\beta \\text{ as in (A2d) and } f_0 = f(X_0, Y_0), \\text{ set } \\eta \\leq \\frac{4C\\nu\\sigma_1(A)}{8\\eta^2f_0}\n$$\n\nThen with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e),\nthe following hold for all \\(t = 1, ..., T\\):\n\n1. \\(\\sigma_r(X_t) \\geq \\frac{1}{2\\beta\\eta}\\)\n2. \\(\\sigma_1(X_t), \\sigma_1(Y_t) \\leq \\frac{C\\sqrt{\\eta^3 + 1}}{2\\beta\\eta}\\)\n\nProof. By Proposition 4.3, we have the following event occurring with the stated probability:\n\n$$\nC^2\\sigma_1^2(A)\\eta \\leq \\sigma_2r(X_0) \\leq \\sigma_1^2(X_0) \\leq 16\\eta\n$$\n\nwhere the upper bound uses that \\(C \\geq 4\\). Moreover, using that \\(\\eta \\leq \\frac{9}{4C\\nu\\sigma_1(A)}\\),\nFor \\(\\beta\\) as in (A2d), note that \\(\\sigma_1^2(Y_0) \\leq 9\\eta C^2\\sigma_1(A)^2 \\leq 16\\eta\\).\n\n$$\nT = \\frac{\\beta}{8\\eta^2f_0} \\leq 1\n$$\n\nwhich means that we can apply Proposition 4.2 up to iteration \\(T\\), resulting in the bound\n\n$$\n\\sigma_r(X_t) \\geq \\sigma_r(X_0) - \\frac{2T\\eta f_0}{2\\eta} \\geq \\frac{1}{\\beta\\eta}\n$$\n\nand, similarly, \\(\\sigma_1(X_t), \\sigma_1(Y_t) \\leq \\frac{C\\sqrt{\\eta} + 1}{2\\eta}\\).\n\nFinally, we use a couple crucial lemmas which apply to our initialization of \\(X_0\\) and \\(Y_0\\).\n\nLemma 4.5. Consider alternating gradient descent as in Assumption 1. If \\(\\text{ColSpan}(X_0) \\subset \\text{ColSpan}(A)\\),\nthen \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) for all \\(t\\).\n\nProof. Suppose \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) and \\(\\text{ColSpan}(Y_t) \\subset \\text{RowSpan}(A)\\). Then\n\n$$\n\\text{ColSpan}(X_tY_t^T Y_t) \\subset \\text{ColSpan}(A) \\text{ and by the update of Assumption 1,}\n$$\n\n$$\n\\text{ColSpan}(X_{t+1}) = \\text{ColSpan}(X_t + \\eta AY_t - \\eta X_tY_t^T Y_t) \\subset \\text{ColSpan}(X_t) \\cup \\text{ColSpan}(AY_t) \\cup \\text{ColSpan}(X_tY_t^T Y_t) \\subset \\text{ColSpan}(A).\n$$\n\nLemma 4.6. If \\(A\\) is rank \\(r\\), and if \\(\\text{ColSpan}(X_t) \\subset \\text{ColSpan}(A)\\) and \\(\\sigma_r(X_t) > 0\\) then\n\n$$\n\\| \\nabla_Y f(X_t, Y_{t-1}) \\|_F \\geq \\frac{2\\sigma_r^2(X_t)f(X_t, Y_{t-1})}{8}\n$$"}]}, {"page": 9, "text": "Proof. If A is rank r, if ColSpan(Xt) \u2282                             ColSpan(A), and if \u03c3r(Xt) > 0, then Xt is rank-r and\nColSpan(Xt) = ColSpan(A). In this case, each column of (XtY\u22ba                                              t \u2212   A) is in the row span of X\u22ba      t ,\nand so                                 \u2225\u2207Yf(Xt, Yt\u22121)\u22252              F = \u2225(XtY\u22ba         t\u22121 \u2212      A)\u22baXt\u22252      F\n                                                                        = \u2225X\u22ba     t (XtY\u22ba     t\u22121 \u2212     A)\u22252   F\n5       Main results                                                    \u2265   \u03c32 r(Xt) \u2225XtY\u22ba          t\u22121 \u2212     A\u22252   F.\nWe are now ready to prove the main results.\nTheorem 5.1. Assume X0 and Y0 are initialized as in Assumption 2, with the stronger assumption\nthat C \u2265        4. Consider alternating gradient decent as in Assumption 1 with  9\nWith \u03b2 as in (A2d) and f0 = f(X0, Y0), set                        \u03b7 \u2264    4C\u03bd\u03c31(A).\n                                                                   T =           \u03b2        .\n                                                                              8\u03b72f    0\nThen with probability at least 1 \u2212                    \u03b4, with respect to the random initialization and \u03b4 defined in (A2e),\nthe following hold for all t = 1, . . . , T                  :\n                                         \u2225A \u2212      XtY\u22ba    t \u22252F \u2264     2 exp (\u2212\u03b2t/4) f0\n                                                                   \u2264   exp\u03b2(\u2212\u03b2t/4) (1 + \u03bd)2 \u2225A\u22252                  F.                         (13)\nProof. Corollary 4.4 implies that \u03c3r(Xt)2 \u2265                              4\u03b7 for t = 1, . . . , T        . Lemmas 4.5 and 4.6 imply since\nX0 is initialized in the column space of A, Xt remains in the column space of A for all t, and\n                                   \u2225\u2207Yf(Xt+1, Yt)\u22252              F = \u2225(A\u22ba         \u2212   YtX\u22ba    t+1)Xt+1\u22252        F\n                                                                    \u2265   \u03c3r(Xt+1)2\u2225(A\u22ba               \u2212   YtX\u22ba    t+1)\u22252   F\n                                                                    \u2265    \u03b2                          t \u22252F                                    (14)\n                                                                         4\u03b7 \u2225A \u2212       Xt+1Y\u22ba\n                                                                    = \u03b2  2\u03b7 f(Xt+1, Yt).\nThat is, a lower bound on \u03c3r(Xt)2 implies that the gradient step with respect to Y satisfies the\nPolyak-Lojasiewicz (PL)-equality3.\nWe can combine this PL inequality with the Lipschitz bound from Lemma 4.1 to derive the linear\nconvergence rate. Indeed, by (3),\n                                f(Xt+1, Yt+1) \u2212               f(Xt+1, Yt) \u2264             \u2212\u03b7  2 \u2225\u2207   Yf(Xt+1, Yt)\u22252           F\nwhere the final inequality is (14). Consequently,                                   \u2264   \u2212\u03b2  4 f(Xt+1, Yt).\n                                              f(XT , YT ) \u2264            (1 \u2212    \u03b24 )f(XT \u22121, YT \u22121)\n                                                                  \u2264    (1 \u2212    \u03b2/4)T f(X0, Y0)\nThe final inequality uses Proposition 4.3.                        \u2264    exp (\u2212\u03b2T/4) f(X0, Y0).\n     3 A function f satisfies the PL-equality if for all x \u2208                    Rm, f(x) \u2212         f(x\u2217) \u2264         1\nminx f(x)                                                                                                         2m\u2225\u2207f(x)\u22252, where f(x\u2217) =\n                                                                              9", "md": "# Math Equations and Text\n\nProof. If A is rank r, if ColSpan(Xt) \u2282 ColSpan(A), and if \u03c3r(Xt) > 0, then Xt is rank-r and ColSpan(Xt) = ColSpan(A). In this case, each column of (XtYt - A) is in the row span of Xt, and so\n\n$$\n\\| \\nabla Yf(Xt, Yt-1) \\|_F = \\| (XtYt-1 - A)^T Xt \\|_F = \\| Xt (XtYt-1 - A) \\|_F \\geq \\sigma^2_r(Xt) \\| XtYt-1 - A \\|_F.\n$$\nWe are now ready to prove the main results.\n\nTheorem 5.1. Assume X0 and Y0 are initialized as in Assumption 2, with the stronger assumption that C \u2265 4. Consider alternating gradient descent as in Assumption 1 with \u03b2 as in (A2d) and f0 = f(X0, Y0), set \u03b7 \u2264 4C\u03bd\u03c31(A), T = \u03b2 / (8\u03b72f0). Then with probability at least 1 - \u03b4, with respect to the random initialization and \u03b4 defined in (A2e), the following hold for all t = 1, ..., T:\n\n$$\n\\| A - XtYt \\|_F^2 \\leq 2 \\exp(-\u03b2t/4) f0 \\leq \\exp(\u03b2(-\u03b2t/4) (1 + \u03bd)2 \\| A \\|_F^2. \\quad (13)\n$$\nProof. Corollary 4.4 implies that \u03c3r(Xt)2 \u2265 4\u03b7 for t = 1, ..., T. Lemmas 4.5 and 4.6 imply since X0 is initialized in the column space of A, Xt remains in the column space of A for all t, and\n\n$$\n\\| \\nabla Yf(Xt+1, Yt) \\|_F = \\| (A^T - YtXt+1)Xt+1 \\|_F \\geq \u03c3r(Xt+1)2 \\| (A^T - YtXt+1) \\|_F \\geq \\beta / (2\u03b7) f(Xt+1, Yt).\n$$\nThat is, a lower bound on \u03c3r(Xt)2 implies that the gradient step with respect to Y satisfies the Polyak-Lojasiewicz (PL)-equality. We can combine this PL inequality with the Lipschitz bound from Lemma 4.1 to derive the linear convergence rate. Indeed, by (3),\n\n$$\nf(Xt+1, Yt+1) - f(Xt+1, Yt) \\leq -\u03b7 / 2 \\| \\nabla Yf(Xt+1, Yt) \\|_F\n$$\nwhere the final inequality is (14). Consequently,\n\n$$\nf(XT, YT) \\leq (1 - \u03b2 / 4) f(XT-1, YT-1) \\leq (1 - \u03b2 / 4)T f(X0, Y0) \\leq \\exp(-\u03b2T/4) f(X0, Y0).\n$$\n3A function f satisfies the PL-equality if for all x \u2208 Rm, f(x) - f(x*) \u2264 1 / minx f(x) 2m \\| \\nabla f(x) \\|2, where f(x*) =", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Proof. If A is rank r, if ColSpan(Xt) \u2282 ColSpan(A), and if \u03c3r(Xt) > 0, then Xt is rank-r and ColSpan(Xt) = ColSpan(A). In this case, each column of (XtYt - A) is in the row span of Xt, and so\n\n$$\n\\| \\nabla Yf(Xt, Yt-1) \\|_F = \\| (XtYt-1 - A)^T Xt \\|_F = \\| Xt (XtYt-1 - A) \\|_F \\geq \\sigma^2_r(Xt) \\| XtYt-1 - A \\|_F.\n$$\nWe are now ready to prove the main results.\n\nTheorem 5.1. Assume X0 and Y0 are initialized as in Assumption 2, with the stronger assumption that C \u2265 4. Consider alternating gradient descent as in Assumption 1 with \u03b2 as in (A2d) and f0 = f(X0, Y0), set \u03b7 \u2264 4C\u03bd\u03c31(A), T = \u03b2 / (8\u03b72f0). Then with probability at least 1 - \u03b4, with respect to the random initialization and \u03b4 defined in (A2e), the following hold for all t = 1, ..., T:\n\n$$\n\\| A - XtYt \\|_F^2 \\leq 2 \\exp(-\u03b2t/4) f0 \\leq \\exp(\u03b2(-\u03b2t/4) (1 + \u03bd)2 \\| A \\|_F^2. \\quad (13)\n$$\nProof. Corollary 4.4 implies that \u03c3r(Xt)2 \u2265 4\u03b7 for t = 1, ..., T. Lemmas 4.5 and 4.6 imply since X0 is initialized in the column space of A, Xt remains in the column space of A for all t, and\n\n$$\n\\| \\nabla Yf(Xt+1, Yt) \\|_F = \\| (A^T - YtXt+1)Xt+1 \\|_F \\geq \u03c3r(Xt+1)2 \\| (A^T - YtXt+1) \\|_F \\geq \\beta / (2\u03b7) f(Xt+1, Yt).\n$$\nThat is, a lower bound on \u03c3r(Xt)2 implies that the gradient step with respect to Y satisfies the Polyak-Lojasiewicz (PL)-equality. We can combine this PL inequality with the Lipschitz bound from Lemma 4.1 to derive the linear convergence rate. Indeed, by (3),\n\n$$\nf(Xt+1, Yt+1) - f(Xt+1, Yt) \\leq -\u03b7 / 2 \\| \\nabla Yf(Xt+1, Yt) \\|_F\n$$\nwhere the final inequality is (14). Consequently,\n\n$$\nf(XT, YT) \\leq (1 - \u03b2 / 4) f(XT-1, YT-1) \\leq (1 - \u03b2 / 4)T f(X0, Y0) \\leq \\exp(-\u03b2T/4) f(X0, Y0).\n$$\n3A function f satisfies the PL-equality if for all x \u2208 Rm, f(x) - f(x*) \u2264 1 / minx f(x) 2m \\| \\nabla f(x) \\|2, where f(x*) =", "md": "Proof. If A is rank r, if ColSpan(Xt) \u2282 ColSpan(A), and if \u03c3r(Xt) > 0, then Xt is rank-r and ColSpan(Xt) = ColSpan(A). In this case, each column of (XtYt - A) is in the row span of Xt, and so\n\n$$\n\\| \\nabla Yf(Xt, Yt-1) \\|_F = \\| (XtYt-1 - A)^T Xt \\|_F = \\| Xt (XtYt-1 - A) \\|_F \\geq \\sigma^2_r(Xt) \\| XtYt-1 - A \\|_F.\n$$\nWe are now ready to prove the main results.\n\nTheorem 5.1. Assume X0 and Y0 are initialized as in Assumption 2, with the stronger assumption that C \u2265 4. Consider alternating gradient descent as in Assumption 1 with \u03b2 as in (A2d) and f0 = f(X0, Y0), set \u03b7 \u2264 4C\u03bd\u03c31(A), T = \u03b2 / (8\u03b72f0). Then with probability at least 1 - \u03b4, with respect to the random initialization and \u03b4 defined in (A2e), the following hold for all t = 1, ..., T:\n\n$$\n\\| A - XtYt \\|_F^2 \\leq 2 \\exp(-\u03b2t/4) f0 \\leq \\exp(\u03b2(-\u03b2t/4) (1 + \u03bd)2 \\| A \\|_F^2. \\quad (13)\n$$\nProof. Corollary 4.4 implies that \u03c3r(Xt)2 \u2265 4\u03b7 for t = 1, ..., T. Lemmas 4.5 and 4.6 imply since X0 is initialized in the column space of A, Xt remains in the column space of A for all t, and\n\n$$\n\\| \\nabla Yf(Xt+1, Yt) \\|_F = \\| (A^T - YtXt+1)Xt+1 \\|_F \\geq \u03c3r(Xt+1)2 \\| (A^T - YtXt+1) \\|_F \\geq \\beta / (2\u03b7) f(Xt+1, Yt).\n$$\nThat is, a lower bound on \u03c3r(Xt)2 implies that the gradient step with respect to Y satisfies the Polyak-Lojasiewicz (PL)-equality. We can combine this PL inequality with the Lipschitz bound from Lemma 4.1 to derive the linear convergence rate. Indeed, by (3),\n\n$$\nf(Xt+1, Yt+1) - f(Xt+1, Yt) \\leq -\u03b7 / 2 \\| \\nabla Yf(Xt+1, Yt) \\|_F\n$$\nwhere the final inequality is (14). Consequently,\n\n$$\nf(XT, YT) \\leq (1 - \u03b2 / 4) f(XT-1, YT-1) \\leq (1 - \u03b2 / 4)T f(X0, Y0) \\leq \\exp(-\u03b2T/4) f(X0, Y0).\n$$\n3A function f satisfies the PL-equality if for all x \u2208 Rm, f(x) - f(x*) \u2264 1 / minx f(x) 2m \\| \\nabla f(x) \\|2, where f(x*) ="}]}, {"page": 10, "text": "Corollary 5.2. Assume X0 and Y0 are initialized as in Assumption 2, with the stronger assumptions\nthat C \u2265        4 and \u03bd \u2264         1\n                                  2. Consider alternating gradient decent as in Assumption 1 with\n                                                                                 \u03b2\n                                                            \u03b7 \u2264        32f0 log(2f0/\u03f5)            ,                                                   (15)\nwhere \u03b2 is defined in (A2d) and f0 = f(X0, Y0). Then with probability at least 1 \u2212                                                    \u03b4, with respect\nto the random initialization and \u03b4 defined in (A2e), it holds\n                                    \u2225A \u2212       XT Y\u22ba         F \u2264    \u03f5     at interation           T =           \u03b2      .\n                                                        T \u22252                                                 8\u03b72f0\nHere \u03c1 is defined in (A2c). Using the upper bound for \u03b7 in (15), the iteration complexity to reach an\n\u03f5-optimal loss value is                      T = O         \u03c31(A) 2 1                         \u2225A\u22252      F        .\n                                                                \u03c3r(A)            \u03c12 log            \u03f5\nThis corollary follows from Theorem 5.1 by solving for \u03b7 so that the RHS of (13) is at most \u03f5, and\n                                             \u03b2                                               9\nthen noting that \u03b7 \u2264             \u221a                                                     4C\u03bd\u03c31(A) when \u03bd \u2264              1\non f0 from Proposition 4.3.          32f0 log(2f0/\u03f5) implies that \u03b7 \u2264                                                 2, using the lower bound\n                                                                                                                                                     \u03b2\nUsing this corollary recursively, we can prove that the loss value remains small for T \u2032 \u2265                                                      \u230a8\u03b72f0 \u230b,\nprovided we increase the lower bound on C by a factor of 2.\nCorollary 5.3. Assume X0 and Y0 are initialized as in Assumption 2, with the stronger assumptions\nthat C \u2265        8 and \u03bd \u2264         1\n                                  2. Let \u03f5 < 1/16, and consider alternating gradient decent as in Assumption 1\nwith                                                                             \u03b2\n                                                             \u03b7 \u2264         32f0 log(1/\u03f5)          ,                                                     (16)\nwhere \u03b2 is defined in (A2d) and f0 = f(X0, Y0). Then with probability at least 1 \u2212                                                    \u03b4, with respect\nto the random initialization and \u03b4 defined in (A2e), it holds that\n            \u2225A \u2212      XT Y\u22ba         F \u2264     \u03f5\u2225A \u2212       X0Y\u22ba     0\u22252 F                  for      T \u2265            \u03b2      ;\n                                T \u22252                                                                        8\u03b72f0\n            \u2225A \u2212      XT Y\u22ba         F \u2264     \u03f52\u2225A \u2212       X0Y\u2032     0\u22252 F                 for      T \u2265            \u03b2      +       1      \u03b2         ;\n                                T \u22252 .                                                                      8\u03b72f0             4\u03f5   8\u03b72f0\n                                     .\n                                     .\n            \u2225A \u2212      XT Y\u22ba         F \u2264     \u03f5k\u2225A \u2212        X0Y\u2032     0\u22252F                 for      T \u2265     k\u22121         1   \u2113      \u03b2         .\n                                T \u22252                                                                     \u2113=0        4\u03f5       8\u03b72f0\nProof. Set \u03b21 = \u03b2 as in (A2d). Set f0                        (1) = f0.\nBy Corollary 5.2, iterating (1) for T1 = \u230a8\u03b72f0(1) \u230b                  \u03b21         iterations with step-size\n                                                                                 \u03b21\nguarantees that                                  1          \u03b7 \u2264         32f0    (1) log(1/\u03f5)\n                                                 2\u2225A \u2212       XT1Y\u2032      T1\u22252  F \u2264    f0(2) := \u03f5f0(1);\n                                                            \u2225\u03c3r(XT1)\u22252 \u2265              1  \u03b21\n                                                                                      4   \u03b7 .\n                                                                             10", "md": "Corollary 5.2. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumptions that \\(C \\geq 4\\) and \\(\\nu \\leq \\frac{1}{2}\\). Consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\eta \\leq \\frac{32f_0 \\log(2f_0/\\epsilon)}{\\beta}, \\quad (15)\n$$\n\nwhere \\(\\beta\\) is defined in (A2d) and \\(f_0 = f(X_0, Y_0)\\). Then with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e), it holds\n\n$$\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon \\text{ at iteration } T = \\frac{\\beta}{8\\eta^2f_0}.\n$$\n\nHere \\(\\rho\\) is defined in (A2c). Using the upper bound for \\(\\eta\\) in (15), the iteration complexity to reach an \\(\\epsilon\\)-optimal loss value is\n\n$$\nT = O\\left(\\frac{\\sigma_1(A)^2}{\\sigma_r(A) \\rho^2 \\log(1/\\epsilon)}\\|A\\|_F\\right).\n$$\n\nThis corollary follows from Theorem 5.1 by solving for \\(\\eta\\) so that the RHS of (13) is at most \\(\\epsilon\\), and\n\n$$\n\\beta \\geq 9 \\quad \\text{then noting that} \\quad \\eta \\leq \\sqrt{\\frac{4C\\nu\\sigma_1(A)}{f_0}} \\quad \\text{when} \\quad \\nu \\leq \\frac{1}{2}\n$$\n\non \\(f_0\\) from Proposition 4.3. \\(32f_0 \\log(2f_0/\\epsilon)\\) implies that \\(\\eta \\leq \\frac{1}{2}\\), using the lower bound\n\n$$\n\\beta.\n$$\n\nUsing this corollary recursively, we can prove that the loss value remains small for \\(T' \\geq \\left\\lfloor 8\\eta^2f_0 \\right\\rfloor\\), provided we increase the lower bound on \\(C\\) by a factor of 2.\n\nCorollary 5.3. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumptions that \\(C \\geq 8\\) and \\(\\nu \\leq \\frac{1}{2}\\). Let \\(\\epsilon < 1/16\\), and consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\eta \\leq \\frac{32f_0 \\log(1/\\epsilon)}{\\beta}, \\quad (16)\n$$\n\nwhere \\(\\beta\\) is defined in (A2d) and \\(f_0 = f(X_0, Y_0)\\). Then with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e), it holds that\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon\\|A - X_0Y_0^\\top\\|_2 F \\text{ for } T \\geq \\frac{\\beta}{8\\eta^2f_0};\n\\]\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon^2\\|A - X_0Y'_0\\|_2 F \\text{ for } T \\geq \\frac{\\beta}{8\\eta^2f_0} + \\frac{1}{4\\epsilon 8\\eta^2f_0};\n\\]\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon^k\\|A - X_0Y'_0\\|_2F \\text{ for } T \\geq \\sum_{\\ell=0}^{k-1} \\frac{1}{4\\epsilon 8\\eta^2f_0}.\n\\]\n\n**Proof.** Set \\(\\beta_1 = \\beta\\) as in (A2d). Set \\(f_0^{(1)} = f_0\\).\n\nBy Corollary 5.2, iterating (1) for \\(T_1 = \\left\\lfloor 8\\eta^2f_0^{(1)} \\right\\rfloor \\beta_1\\) iterations with step-size\n\n$$\n\\eta \\leq \\frac{32f_0^{(1)} \\log(1/\\epsilon)}{2\\|A - X^{T_1}Y'_0\\|_2 F \\geq \\frac{1}{4\\beta_1\\eta}.\n$$", "images": [], "items": [{"type": "text", "value": "Corollary 5.2. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumptions that \\(C \\geq 4\\) and \\(\\nu \\leq \\frac{1}{2}\\). Consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\eta \\leq \\frac{32f_0 \\log(2f_0/\\epsilon)}{\\beta}, \\quad (15)\n$$\n\nwhere \\(\\beta\\) is defined in (A2d) and \\(f_0 = f(X_0, Y_0)\\). Then with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e), it holds\n\n$$\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon \\text{ at iteration } T = \\frac{\\beta}{8\\eta^2f_0}.\n$$\n\nHere \\(\\rho\\) is defined in (A2c). Using the upper bound for \\(\\eta\\) in (15), the iteration complexity to reach an \\(\\epsilon\\)-optimal loss value is\n\n$$\nT = O\\left(\\frac{\\sigma_1(A)^2}{\\sigma_r(A) \\rho^2 \\log(1/\\epsilon)}\\|A\\|_F\\right).\n$$\n\nThis corollary follows from Theorem 5.1 by solving for \\(\\eta\\) so that the RHS of (13) is at most \\(\\epsilon\\), and\n\n$$\n\\beta \\geq 9 \\quad \\text{then noting that} \\quad \\eta \\leq \\sqrt{\\frac{4C\\nu\\sigma_1(A)}{f_0}} \\quad \\text{when} \\quad \\nu \\leq \\frac{1}{2}\n$$\n\non \\(f_0\\) from Proposition 4.3. \\(32f_0 \\log(2f_0/\\epsilon)\\) implies that \\(\\eta \\leq \\frac{1}{2}\\), using the lower bound\n\n$$\n\\beta.\n$$\n\nUsing this corollary recursively, we can prove that the loss value remains small for \\(T' \\geq \\left\\lfloor 8\\eta^2f_0 \\right\\rfloor\\), provided we increase the lower bound on \\(C\\) by a factor of 2.\n\nCorollary 5.3. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumptions that \\(C \\geq 8\\) and \\(\\nu \\leq \\frac{1}{2}\\). Let \\(\\epsilon < 1/16\\), and consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\eta \\leq \\frac{32f_0 \\log(1/\\epsilon)}{\\beta}, \\quad (16)\n$$\n\nwhere \\(\\beta\\) is defined in (A2d) and \\(f_0 = f(X_0, Y_0)\\). Then with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e), it holds that\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon\\|A - X_0Y_0^\\top\\|_2 F \\text{ for } T \\geq \\frac{\\beta}{8\\eta^2f_0};\n\\]\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon^2\\|A - X_0Y'_0\\|_2 F \\text{ for } T \\geq \\frac{\\beta}{8\\eta^2f_0} + \\frac{1}{4\\epsilon 8\\eta^2f_0};\n\\]\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon^k\\|A - X_0Y'_0\\|_2F \\text{ for } T \\geq \\sum_{\\ell=0}^{k-1} \\frac{1}{4\\epsilon 8\\eta^2f_0}.\n\\]\n\n**Proof.** Set \\(\\beta_1 = \\beta\\) as in (A2d). Set \\(f_0^{(1)} = f_0\\).\n\nBy Corollary 5.2, iterating (1) for \\(T_1 = \\left\\lfloor 8\\eta^2f_0^{(1)} \\right\\rfloor \\beta_1\\) iterations with step-size\n\n$$\n\\eta \\leq \\frac{32f_0^{(1)} \\log(1/\\epsilon)}{2\\|A - X^{T_1}Y'_0\\|_2 F \\geq \\frac{1}{4\\beta_1\\eta}.\n$$", "md": "Corollary 5.2. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumptions that \\(C \\geq 4\\) and \\(\\nu \\leq \\frac{1}{2}\\). Consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\eta \\leq \\frac{32f_0 \\log(2f_0/\\epsilon)}{\\beta}, \\quad (15)\n$$\n\nwhere \\(\\beta\\) is defined in (A2d) and \\(f_0 = f(X_0, Y_0)\\). Then with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e), it holds\n\n$$\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon \\text{ at iteration } T = \\frac{\\beta}{8\\eta^2f_0}.\n$$\n\nHere \\(\\rho\\) is defined in (A2c). Using the upper bound for \\(\\eta\\) in (15), the iteration complexity to reach an \\(\\epsilon\\)-optimal loss value is\n\n$$\nT = O\\left(\\frac{\\sigma_1(A)^2}{\\sigma_r(A) \\rho^2 \\log(1/\\epsilon)}\\|A\\|_F\\right).\n$$\n\nThis corollary follows from Theorem 5.1 by solving for \\(\\eta\\) so that the RHS of (13) is at most \\(\\epsilon\\), and\n\n$$\n\\beta \\geq 9 \\quad \\text{then noting that} \\quad \\eta \\leq \\sqrt{\\frac{4C\\nu\\sigma_1(A)}{f_0}} \\quad \\text{when} \\quad \\nu \\leq \\frac{1}{2}\n$$\n\non \\(f_0\\) from Proposition 4.3. \\(32f_0 \\log(2f_0/\\epsilon)\\) implies that \\(\\eta \\leq \\frac{1}{2}\\), using the lower bound\n\n$$\n\\beta.\n$$\n\nUsing this corollary recursively, we can prove that the loss value remains small for \\(T' \\geq \\left\\lfloor 8\\eta^2f_0 \\right\\rfloor\\), provided we increase the lower bound on \\(C\\) by a factor of 2.\n\nCorollary 5.3. Assume \\(X_0\\) and \\(Y_0\\) are initialized as in Assumption 2, with the stronger assumptions that \\(C \\geq 8\\) and \\(\\nu \\leq \\frac{1}{2}\\). Let \\(\\epsilon < 1/16\\), and consider alternating gradient descent as in Assumption 1 with\n\n$$\n\\eta \\leq \\frac{32f_0 \\log(1/\\epsilon)}{\\beta}, \\quad (16)\n$$\n\nwhere \\(\\beta\\) is defined in (A2d) and \\(f_0 = f(X_0, Y_0)\\). Then with probability at least \\(1 - \\delta\\), with respect to the random initialization and \\(\\delta\\) defined in (A2e), it holds that\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon\\|A - X_0Y_0^\\top\\|_2 F \\text{ for } T \\geq \\frac{\\beta}{8\\eta^2f_0};\n\\]\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon^2\\|A - X_0Y'_0\\|_2 F \\text{ for } T \\geq \\frac{\\beta}{8\\eta^2f_0} + \\frac{1}{4\\epsilon 8\\eta^2f_0};\n\\]\n\n\\[\n\\|A - X^T Y^\\top\\|_F \\leq \\epsilon^k\\|A - X_0Y'_0\\|_2F \\text{ for } T \\geq \\sum_{\\ell=0}^{k-1} \\frac{1}{4\\epsilon 8\\eta^2f_0}.\n\\]\n\n**Proof.** Set \\(\\beta_1 = \\beta\\) as in (A2d). Set \\(f_0^{(1)} = f_0\\).\n\nBy Corollary 5.2, iterating (1) for \\(T_1 = \\left\\lfloor 8\\eta^2f_0^{(1)} \\right\\rfloor \\beta_1\\) iterations with step-size\n\n$$\n\\eta \\leq \\frac{32f_0^{(1)} \\log(1/\\epsilon)}{2\\|A - X^{T_1}Y'_0\\|_2 F \\geq \\frac{1}{4\\beta_1\\eta}.\n$$"}]}, {"page": 11, "text": "This means that at time T1, we can restart the analysis, and appeal again to Proposition 4.2 with\nmodified parameters\n         \u2022 f(XT   1, YT1) \u2264    f02 := \u03f5f01,\n         \u2022 \u03b22 := \u03b21 4 .\nCorollary 5.2 again guarantees that provided\n                                            \u03b22                 1             \u03b21\n                            \u03b7 \u2264      32f0  (2) log(1/\u03f5)   =  4\u221a\u03f5      32f0(1) log(1/\u03f5)                           (17)\nthen f(XT    1+T2, YT1+T2) \u2264       \u03f5f(XT1, YT1) \u2264       \u03f52f(X0, Y0) where\n                                                     T2 = T1 4\u03f5 .                                                (18)\n                                                                             1\nWe have that (17) is satisfied by assumption as we assume \u03f5 \u2264                16. Repeating this inductively, we\n                                                  k\u22121\nfind that after T = T1 + \u00b7 \u00b7 \u00b7 + Tk = T1            \u2113=0 ( 1\n                                                          4\u03f5)\u2113  \u2264  T1( 14\u03f5)k iterations, we are guaranteed that\nf(XT , YT ) \u2264     \u03f5kf(X0, Y0). This is valid for any k \u2208      N because we may always apply Proposition 4.2\nin light of summability and C \u2265         8: for any t,      k\n                           \u03c31(Xt) \u2264     \u03c31(X0) + \u221a\u03b7       j=1    2Tkf0(k)\n                                     \u2264     3      \u221a\u03b7     k     2(1/(4\u03f5))j \u03b21      \u03f5jf0(1)\n                                         8\u221a\u03b7 + 1       j=1                  8f01\n                                     \u2264     3       \u221a  \u03b2   k  (1/2)j\n                                         8\u221a\u03b7 +    2\u221a\u03b7    j=1\n                                     \u2264   3 + 4\u221a\u03b2     \u2264    1\n                                           8\u221a\u03b7          2\u221a\u03b7 .\n6     Numerical experiments\nWe perform an illustrative numerical experiment to demonstrate the effect of different choice\nof initialization.     In Fig. 1, we factorize a rank-5 matrix of size 100 \u00d7 100.                   The matrix is\nconstructed as A = U\u03a3V\u22ba              with U and V random 100 \u00d7 5 orthonormal matrices and \u03a3 =\ndiag(1.000, 0.875, 0.750, 0.625, 0.500). The same matrix is used for all experiments. We consider\nthree initializations:\n       1. Orange. The orange initialization is from Assumption 2. Here, X0 is in the column space\n           of A. Further, the scale is asymmetric, with Y0 multiplied by \u221a\u03b7\u03c31(A) and X0 by the\n           inverse. In this case, the constants are C = 4, \u03bd = 1e-9, and D = C\u03bd/9. This initialization\n           provides a significant \u201chead start\u201d in convergence, moving quickly to fast convergence.\n       2. Blue. The blue initialization keeps X0 in the column space of A but scales both matrices\n           similarly. Here we see that the multiplication by A is helpful but not the sole source of\n           improvement.\n       3. Green. The green initialization is the typical initialization: choose X0 and Y0 as suitably\n           normalized random Gaussians, similarly scaled. From this initialization, the alternating\n           gradient descent has a slow start.\nThe factorization uses rank d = 6 = r + 1. The step length for all runs is \u03b7 = 0.0683. The rate of\nconvergence is much improved for the initialization from Assumption 2. We explore this further in\nthe next experiment.\nThe rate of convergence is tied to the ratio of the largest and smallest singular values, which we\nillustrate in Figs. 2 and 3. In these experiments, we factorize three different rank-5 matrices of size\n                                                          11", "md": "This means that at time \\(T_1\\), we can restart the analysis, and appeal again to Proposition 4.2 with modified parameters\n\n$$\n\\begin{align*}\n&f(X_{T1}, Y_{T1}) \\leq f_{02} := \\epsilon f_{01}, \\\\\n&\\beta_2 := \\frac{\\beta_1}{4}.\n\\end{align*}\n$$\nCorollary 5.2 again guarantees that provided\n\n$$\n\\eta \\leq \\frac{32f_0(2) \\log(1/\\epsilon)}{4\\sqrt{\\epsilon} 32f_0(1) \\log(1/\\epsilon)} \\quad (17)\n$$\nthen \\(f(X_{T1+T2}, Y_{T1+T2}) \\leq \\epsilon f(X_{T1}, Y_{T1}) \\leq \\epsilon^2 f(X_0, Y_0)\\) where\n\n$$\nT2 = \\frac{T1}{4\\epsilon} \\quad (18)\n$$\nWe have that (17) is satisfied by assumption as we assume \\(\\epsilon \\leq \\frac{1}{16}\\). Repeating this inductively, we find that after \\(T = T1 + \\ldots + Tk = T1 \\sum_{\\ell=0}^{k-1} (1/4\\epsilon)^\\ell \\leq T1 (1/4\\epsilon)^k\\) iterations, we are guaranteed that \\(f(X_T, Y_T) \\leq \\epsilon^k f(X_0, Y_0)\\). This is valid for any \\(k \\in \\mathbb{N}\\) because we may always apply Proposition 4.2 in light of summability and \\(C \\geq 8\\): for any \\(t\\),\n\n$$\n\\begin{align*}\n\\sigma_1(X_t) &\\leq \\sigma_1(X_0) + \\sqrt{\\eta} \\sum_{j=1}^{k} 2Tkf_0(k) \\\\\n&\\leq 3\\sqrt{\\eta} k 2(1/(4\\epsilon))^j \\beta_1 \\epsilon^j f_0(1) \\\\\n&\\leq 8\\sqrt{\\eta} + 1 \\sum_{j=1}^{k} 8f_{01} \\\\\n&\\leq 3\\sqrt{\\beta} k (1/2)^j 8\\sqrt{\\eta} + 2\\sqrt{\\eta} \\sum_{j=1}^{k} \\\\\n&\\leq 3 + 4\\sqrt{\\beta} \\leq 1 8\\sqrt{\\eta} 2\\sqrt{\\eta}.\n\\end{align*}\n$$\nNumerical experiments\n\nWe perform an illustrative numerical experiment to demonstrate the effect of different choice of initialization. In Fig. 1, we factorize a rank-5 matrix of size 100 \u00d7 100. The matrix is constructed as \\(A = U\u03a3V^\u22ba\\) with \\(U\\) and \\(V\\) random 100 \u00d7 5 orthonormal matrices and \\(\u03a3 = \\text{diag}(1.000, 0.875, 0.750, 0.625, 0.500)\\). The same matrix is used for all experiments. We consider three initializations:\n\n1. Orange. The orange initialization is from Assumption 2. Here, \\(X_0\\) is in the column space of \\(A\\). Further, the scale is asymmetric, with \\(Y_0\\) multiplied by \\(\\sqrt{\\eta}\\sigma_1(A)\\) and \\(X_0\\) by the inverse. In this case, the constants are \\(C = 4\\), \\(\u03bd = 1e-9\\), and \\(D = C\u03bd/9\\). This initialization provides a significant \u201chead start\u201d in convergence, moving quickly to fast convergence.\n2. Blue. The blue initialization keeps \\(X_0\\) in the column space of \\(A\\) but scales both matrices similarly. Here we see that the multiplication by \\(A\\) is helpful but not the sole source of improvement.\n3. Green. The green initialization is the typical initialization: choose \\(X_0\\) and \\(Y_0\\) as suitably normalized random Gaussians, similarly scaled. From this initialization, the alternating gradient descent has a slow start.\n\nThe factorization uses rank \\(d = 6 = r + 1\\). The step length for all runs is \\(\\eta = 0.0683\\). The rate of convergence is much improved for the initialization from Assumption 2. We explore this further in the next experiment.\n\nThe rate of convergence is tied to the ratio of the largest and smallest singular values, which we illustrate in Figs. 2 and 3. In these experiments, we factorize three different rank-5 matrices of size 11", "images": [], "items": [{"type": "text", "value": "This means that at time \\(T_1\\), we can restart the analysis, and appeal again to Proposition 4.2 with modified parameters\n\n$$\n\\begin{align*}\n&f(X_{T1}, Y_{T1}) \\leq f_{02} := \\epsilon f_{01}, \\\\\n&\\beta_2 := \\frac{\\beta_1}{4}.\n\\end{align*}\n$$\nCorollary 5.2 again guarantees that provided\n\n$$\n\\eta \\leq \\frac{32f_0(2) \\log(1/\\epsilon)}{4\\sqrt{\\epsilon} 32f_0(1) \\log(1/\\epsilon)} \\quad (17)\n$$\nthen \\(f(X_{T1+T2}, Y_{T1+T2}) \\leq \\epsilon f(X_{T1}, Y_{T1}) \\leq \\epsilon^2 f(X_0, Y_0)\\) where\n\n$$\nT2 = \\frac{T1}{4\\epsilon} \\quad (18)\n$$\nWe have that (17) is satisfied by assumption as we assume \\(\\epsilon \\leq \\frac{1}{16}\\). Repeating this inductively, we find that after \\(T = T1 + \\ldots + Tk = T1 \\sum_{\\ell=0}^{k-1} (1/4\\epsilon)^\\ell \\leq T1 (1/4\\epsilon)^k\\) iterations, we are guaranteed that \\(f(X_T, Y_T) \\leq \\epsilon^k f(X_0, Y_0)\\). This is valid for any \\(k \\in \\mathbb{N}\\) because we may always apply Proposition 4.2 in light of summability and \\(C \\geq 8\\): for any \\(t\\),\n\n$$\n\\begin{align*}\n\\sigma_1(X_t) &\\leq \\sigma_1(X_0) + \\sqrt{\\eta} \\sum_{j=1}^{k} 2Tkf_0(k) \\\\\n&\\leq 3\\sqrt{\\eta} k 2(1/(4\\epsilon))^j \\beta_1 \\epsilon^j f_0(1) \\\\\n&\\leq 8\\sqrt{\\eta} + 1 \\sum_{j=1}^{k} 8f_{01} \\\\\n&\\leq 3\\sqrt{\\beta} k (1/2)^j 8\\sqrt{\\eta} + 2\\sqrt{\\eta} \\sum_{j=1}^{k} \\\\\n&\\leq 3 + 4\\sqrt{\\beta} \\leq 1 8\\sqrt{\\eta} 2\\sqrt{\\eta}.\n\\end{align*}\n$$\nNumerical experiments\n\nWe perform an illustrative numerical experiment to demonstrate the effect of different choice of initialization. In Fig. 1, we factorize a rank-5 matrix of size 100 \u00d7 100. The matrix is constructed as \\(A = U\u03a3V^\u22ba\\) with \\(U\\) and \\(V\\) random 100 \u00d7 5 orthonormal matrices and \\(\u03a3 = \\text{diag}(1.000, 0.875, 0.750, 0.625, 0.500)\\). The same matrix is used for all experiments. We consider three initializations:\n\n1. Orange. The orange initialization is from Assumption 2. Here, \\(X_0\\) is in the column space of \\(A\\). Further, the scale is asymmetric, with \\(Y_0\\) multiplied by \\(\\sqrt{\\eta}\\sigma_1(A)\\) and \\(X_0\\) by the inverse. In this case, the constants are \\(C = 4\\), \\(\u03bd = 1e-9\\), and \\(D = C\u03bd/9\\). This initialization provides a significant \u201chead start\u201d in convergence, moving quickly to fast convergence.\n2. Blue. The blue initialization keeps \\(X_0\\) in the column space of \\(A\\) but scales both matrices similarly. Here we see that the multiplication by \\(A\\) is helpful but not the sole source of improvement.\n3. Green. The green initialization is the typical initialization: choose \\(X_0\\) and \\(Y_0\\) as suitably normalized random Gaussians, similarly scaled. From this initialization, the alternating gradient descent has a slow start.\n\nThe factorization uses rank \\(d = 6 = r + 1\\). The step length for all runs is \\(\\eta = 0.0683\\). The rate of convergence is much improved for the initialization from Assumption 2. We explore this further in the next experiment.\n\nThe rate of convergence is tied to the ratio of the largest and smallest singular values, which we illustrate in Figs. 2 and 3. In these experiments, we factorize three different rank-5 matrices of size 11", "md": "This means that at time \\(T_1\\), we can restart the analysis, and appeal again to Proposition 4.2 with modified parameters\n\n$$\n\\begin{align*}\n&f(X_{T1}, Y_{T1}) \\leq f_{02} := \\epsilon f_{01}, \\\\\n&\\beta_2 := \\frac{\\beta_1}{4}.\n\\end{align*}\n$$\nCorollary 5.2 again guarantees that provided\n\n$$\n\\eta \\leq \\frac{32f_0(2) \\log(1/\\epsilon)}{4\\sqrt{\\epsilon} 32f_0(1) \\log(1/\\epsilon)} \\quad (17)\n$$\nthen \\(f(X_{T1+T2}, Y_{T1+T2}) \\leq \\epsilon f(X_{T1}, Y_{T1}) \\leq \\epsilon^2 f(X_0, Y_0)\\) where\n\n$$\nT2 = \\frac{T1}{4\\epsilon} \\quad (18)\n$$\nWe have that (17) is satisfied by assumption as we assume \\(\\epsilon \\leq \\frac{1}{16}\\). Repeating this inductively, we find that after \\(T = T1 + \\ldots + Tk = T1 \\sum_{\\ell=0}^{k-1} (1/4\\epsilon)^\\ell \\leq T1 (1/4\\epsilon)^k\\) iterations, we are guaranteed that \\(f(X_T, Y_T) \\leq \\epsilon^k f(X_0, Y_0)\\). This is valid for any \\(k \\in \\mathbb{N}\\) because we may always apply Proposition 4.2 in light of summability and \\(C \\geq 8\\): for any \\(t\\),\n\n$$\n\\begin{align*}\n\\sigma_1(X_t) &\\leq \\sigma_1(X_0) + \\sqrt{\\eta} \\sum_{j=1}^{k} 2Tkf_0(k) \\\\\n&\\leq 3\\sqrt{\\eta} k 2(1/(4\\epsilon))^j \\beta_1 \\epsilon^j f_0(1) \\\\\n&\\leq 8\\sqrt{\\eta} + 1 \\sum_{j=1}^{k} 8f_{01} \\\\\n&\\leq 3\\sqrt{\\beta} k (1/2)^j 8\\sqrt{\\eta} + 2\\sqrt{\\eta} \\sum_{j=1}^{k} \\\\\n&\\leq 3 + 4\\sqrt{\\beta} \\leq 1 8\\sqrt{\\eta} 2\\sqrt{\\eta}.\n\\end{align*}\n$$\nNumerical experiments\n\nWe perform an illustrative numerical experiment to demonstrate the effect of different choice of initialization. In Fig. 1, we factorize a rank-5 matrix of size 100 \u00d7 100. The matrix is constructed as \\(A = U\u03a3V^\u22ba\\) with \\(U\\) and \\(V\\) random 100 \u00d7 5 orthonormal matrices and \\(\u03a3 = \\text{diag}(1.000, 0.875, 0.750, 0.625, 0.500)\\). The same matrix is used for all experiments. We consider three initializations:\n\n1. Orange. The orange initialization is from Assumption 2. Here, \\(X_0\\) is in the column space of \\(A\\). Further, the scale is asymmetric, with \\(Y_0\\) multiplied by \\(\\sqrt{\\eta}\\sigma_1(A)\\) and \\(X_0\\) by the inverse. In this case, the constants are \\(C = 4\\), \\(\u03bd = 1e-9\\), and \\(D = C\u03bd/9\\). This initialization provides a significant \u201chead start\u201d in convergence, moving quickly to fast convergence.\n2. Blue. The blue initialization keeps \\(X_0\\) in the column space of \\(A\\) but scales both matrices similarly. Here we see that the multiplication by \\(A\\) is helpful but not the sole source of improvement.\n3. Green. The green initialization is the typical initialization: choose \\(X_0\\) and \\(Y_0\\) as suitably normalized random Gaussians, similarly scaled. From this initialization, the alternating gradient descent has a slow start.\n\nThe factorization uses rank \\(d = 6 = r + 1\\). The step length for all runs is \\(\\eta = 0.0683\\). The rate of convergence is much improved for the initialization from Assumption 2. We explore this further in the next experiment.\n\nThe rate of convergence is tied to the ratio of the largest and smallest singular values, which we illustrate in Figs. 2 and 3. In these experiments, we factorize three different rank-5 matrices of size 11"}]}, {"page": 12, "text": "       1                                                       1                        \u221a\u03b7D\u03c31\n                                                   X0 =   \u221a\u03b7 \u221a dC\u03c31 A\u03a6(n\u00d7d), Y0 =         \u221a n  \u03a6(n\u00d7d)\n                                                                1                       1\n     0.8                                              X0 =    10\u221a dA\u03a6(n\u00d7d), Y0 =      10\u221an\u03a6(n\u00d7d)\n                                                                 1                      1\n                                                       X0 =   10\u221am\u03a6(m\u00d7d), Y0 =        10\u221an\u03a6(n\u00d7d)\n   relative error\n     0.6\n     0.4\n     0.2\n       0\n          0      20     40      60     80     100     120    140     160    180     200    220     240\n                                                   iteration (t)\nFigure 1: Effect of initialization. Five runs each of alternating gradient descent for matrix factoriza-\ntion with matrix of size 100 \u00d7 100 (m = n = 100) and rank 5 (r = 5). All runs use the same matrix,\nwhich is constructed such that \u03c31 = 1, \u03c3r = 0.5, and the remaining singular values are evenly spaced\nbetween. For all runs, we have \u03b7 = 0.0683 (which is 1e6 times the theoretical value), d = 6, C = 4,\n\u03bd = 1e-10, D = C\u03bd/9, and each \u03a6 is an indepedent Gaussian random matrix.\n100 \u00d7 100 with different singular values ratios. The matrices are each constructed as A = U\u03a3V\u22ba\nwith U and V random 100 \u00d7 5 orthonormal matrices. The difference is that each has a different set\nof singular values. All three have \u03c31 = 1, and then we have \u03c3r = 0.1 in orange, \u03c3r = 0.5 in blue,\nand \u03c3r = 0.9 in green. The intermediate singular values are evenly spaced between the specified\nlargest and smallest singular values. The factors are rank d = r + 1 = 6. The step length varies as\nwe use 10000 times the bound (15) with C = 4. The initialization is proposed in Assumption 2 with\n\u03bd = 1e-10. We run each method fi      ve times. The number of iterations until convergence depends\non \u03c32r(A)/\u03c32 1(A), so we plot reference curves for comparison, as dashed lines. Smaller values of\n\u03c3r(A)/\u03c31(A) indicate worse conditioning of the problem and result in slower convergence. We\nzoom in on the first few iterations in Fig. 3 to show that this bound is fairly tight initially.\nAcknowledgments\nRW thanks Sebastien Bubeck, Sinho Chewi, and Nathan Srebro for discussions which motivated\nthis work. RW is supported in part by AFOSR MURI FA9550-19-1-0005, NSF DMS-1952735, NSF\nIFML grant 2019844, NSF DMS-N2109155, and NSF 2217033. TK thanks the Oden Institute for the\nJ. T. Oden Faculty Fellowship that facilitated this collaboration.\n                                                    12", "md": "# Math Equations and Table\n\n## Math Equations:\n\n$$X_0 = \\sqrt{\\eta} \\sqrt{dC\\sigma_1} A\\Phi(n \\times d), \\quad Y_0 = \\frac{\\sqrt{n}}{1} \\Phi(n \\times d)$$\n\n$$X_0 = 10\\sqrt{d}A\\Phi(n \\times d), \\quad Y_0 = 10\\sqrt{n}\\Phi(n \\times d)$$\n\n$$X_0 = 10\\sqrt{m}\\Phi(m \\times d), \\quad Y_0 = 10\\sqrt{n}\\Phi(n \\times d)$$\n\n## Table:\n\n|iteration (t)|\n|---|\n|0|20|40|60|80|100|120|140|160|180|200|220|240|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Table", "md": "# Math Equations and Table"}, {"type": "heading", "lvl": 2, "value": "Math Equations:", "md": "## Math Equations:"}, {"type": "text", "value": "$$X_0 = \\sqrt{\\eta} \\sqrt{dC\\sigma_1} A\\Phi(n \\times d), \\quad Y_0 = \\frac{\\sqrt{n}}{1} \\Phi(n \\times d)$$\n\n$$X_0 = 10\\sqrt{d}A\\Phi(n \\times d), \\quad Y_0 = 10\\sqrt{n}\\Phi(n \\times d)$$\n\n$$X_0 = 10\\sqrt{m}\\Phi(m \\times d), \\quad Y_0 = 10\\sqrt{n}\\Phi(n \\times d)$$", "md": "$$X_0 = \\sqrt{\\eta} \\sqrt{dC\\sigma_1} A\\Phi(n \\times d), \\quad Y_0 = \\frac{\\sqrt{n}}{1} \\Phi(n \\times d)$$\n\n$$X_0 = 10\\sqrt{d}A\\Phi(n \\times d), \\quad Y_0 = 10\\sqrt{n}\\Phi(n \\times d)$$\n\n$$X_0 = 10\\sqrt{m}\\Phi(m \\times d), \\quad Y_0 = 10\\sqrt{n}\\Phi(n \\times d)$$"}, {"type": "heading", "lvl": 2, "value": "Table:", "md": "## Table:"}, {"type": "table", "rows": [["iteration (t)"], ["0", "20", "40", "60", "80", "100", "120", "140", "160", "180", "200", "220", "240"]], "md": "|iteration (t)|\n|---|\n|0|20|40|60|80|100|120|140|160|180|200|220|240|", "isPerfectTable": false, "csv": "\"iteration (t)\"\n\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"140\",\"160\",\"180\",\"200\",\"220\",\"240\""}]}, {"page": 13, "text": "       1\n                                                                         \u03c3r/\u03c31 = 0.1, \u03b7 = 0.0033\n                                                                              exp[\u2212(0.1)2t/20]\n                                                                         \u03c3r/\u03c31 = 0.5, \u03b7 = 0.0682\n     0.8                                                                      exp[\u2212(0.5)2t/20]\n                                                                         \u03c3r/\u03c31 = 0.9, \u03b7 = 0.1792\n   relative error                                                             exp[\u2212(0.9)2t/20]\n     0.6\n     0.4\n     0.2\n       0\n          0      20     40     60      80    100     120    140    160     180    200    220     240\n                                                  iteration (t)\nFigure 2: Five runs each of alternating gradient descent for matrix factorization with matrix of size\n100 \u00d7 100 and rank 5. Each matrix is constructed such that \u03c31 = 1, \u03c3r \u2208         {0.1, 0.5, 0.9}, and the\nremaining singular values are evenly spaced between. The \u03b7 is 1e4 times the theoretical value using\nd = 6, C = 4, and \u03bd = 1e-10.\n      100\n   relative error\n                    \u03c3r/\u03c31 = 0.1, \u03b7 = 0.0033\n                        exp[\u2212(0.1)2t/20]\n                    \u03c3r/\u03c31 = 0.5, \u03b7 = 0.0682\n                        exp[\u2212(0.5)2t/20]\n                    \u03c3r/\u03c31 = 0.9, \u03b7 = 0.1792\n     10\u22121               exp[\u2212(0.9)2t/20]\n          0        5        10       15       20       25       30        35       40       45       50\n                                                    iteration (t)\n          Figure 3: Zoom into the first few iterations of Fig. 2, with the y-axis in log scale.\n                                                   13", "md": "# Math Equations\n\n$$\\sigma_r/\\sigma_1 = 0.1, \\eta = 0.0033$$\n\n$$\\exp\\left[-\\left(0.1\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.5, \\eta = 0.0682$$\n\n$$0.8 \\quad \\exp\\left[-\\left(0.5\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.9, \\eta = 0.1792$$\n\n$$\\text{relative error} \\quad \\exp\\left[-\\left(0.9\\right)^2t/20\\right]$$\n\n$$0.6$$\n\n$$0.4$$\n\n$$0.2$$\n\n$$0$$\n\n|0|20|40|60|80|100|120|140|160|180|200|220|240|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | | |iteration (t)| | | | | | |\n\nFigure 2: Five runs each of alternating gradient descent for matrix factorization with matrix of size 100 \u00d7 100 and rank 5. Each matrix is constructed such that $$\\sigma_1 = 1$$, $$\\sigma_r \\in \\{0.1, 0.5, 0.9\\}$$, and the remaining singular values are evenly spaced between. The $$\\eta$$ is $$1e4$$ times the theoretical value using $$d = 6$$, $$C = 4$$, and $$\\nu = 1e-10$$.\n\n$$\\text{relative error}$$\n\n$$\\sigma_r/\\sigma_1 = 0.1, \\eta = 0.0033$$\n\n$$\\exp\\left[-\\left(0.1\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.5, \\eta = 0.0682$$\n\n$$\\exp\\left[-\\left(0.5\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.9, \\eta = 0.1792$$\n\n$$10^{-1} \\quad \\exp\\left[-\\left(0.9\\right)^2t/20\\right]$$\n\n|0|5|10|15|20|25|30|35|40|45|50|\n|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | |iteration (t)| | | | | |\n\nFigure 3: Zoom into the first few iterations of Fig. 2, with the y-axis in log scale.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "$$\\sigma_r/\\sigma_1 = 0.1, \\eta = 0.0033$$\n\n$$\\exp\\left[-\\left(0.1\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.5, \\eta = 0.0682$$\n\n$$0.8 \\quad \\exp\\left[-\\left(0.5\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.9, \\eta = 0.1792$$\n\n$$\\text{relative error} \\quad \\exp\\left[-\\left(0.9\\right)^2t/20\\right]$$\n\n$$0.6$$\n\n$$0.4$$\n\n$$0.2$$\n\n$$0$$", "md": "$$\\sigma_r/\\sigma_1 = 0.1, \\eta = 0.0033$$\n\n$$\\exp\\left[-\\left(0.1\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.5, \\eta = 0.0682$$\n\n$$0.8 \\quad \\exp\\left[-\\left(0.5\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.9, \\eta = 0.1792$$\n\n$$\\text{relative error} \\quad \\exp\\left[-\\left(0.9\\right)^2t/20\\right]$$\n\n$$0.6$$\n\n$$0.4$$\n\n$$0.2$$\n\n$$0$$"}, {"type": "table", "rows": [["0", "20", "40", "60", "80", "100", "120", "140", "160", "180", "200", "220", "240"], ["", "", "", "", "", "", "iteration (t)", "", "", "", "", "", ""]], "md": "|0|20|40|60|80|100|120|140|160|180|200|220|240|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | | |iteration (t)| | | | | | |", "isPerfectTable": true, "csv": "\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"140\",\"160\",\"180\",\"200\",\"220\",\"240\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"iteration (t)\",\"\",\"\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 2: Five runs each of alternating gradient descent for matrix factorization with matrix of size 100 \u00d7 100 and rank 5. Each matrix is constructed such that $$\\sigma_1 = 1$$, $$\\sigma_r \\in \\{0.1, 0.5, 0.9\\}$$, and the remaining singular values are evenly spaced between. The $$\\eta$$ is $$1e4$$ times the theoretical value using $$d = 6$$, $$C = 4$$, and $$\\nu = 1e-10$$.\n\n$$\\text{relative error}$$\n\n$$\\sigma_r/\\sigma_1 = 0.1, \\eta = 0.0033$$\n\n$$\\exp\\left[-\\left(0.1\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.5, \\eta = 0.0682$$\n\n$$\\exp\\left[-\\left(0.5\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.9, \\eta = 0.1792$$\n\n$$10^{-1} \\quad \\exp\\left[-\\left(0.9\\right)^2t/20\\right]$$", "md": "Figure 2: Five runs each of alternating gradient descent for matrix factorization with matrix of size 100 \u00d7 100 and rank 5. Each matrix is constructed such that $$\\sigma_1 = 1$$, $$\\sigma_r \\in \\{0.1, 0.5, 0.9\\}$$, and the remaining singular values are evenly spaced between. The $$\\eta$$ is $$1e4$$ times the theoretical value using $$d = 6$$, $$C = 4$$, and $$\\nu = 1e-10$$.\n\n$$\\text{relative error}$$\n\n$$\\sigma_r/\\sigma_1 = 0.1, \\eta = 0.0033$$\n\n$$\\exp\\left[-\\left(0.1\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.5, \\eta = 0.0682$$\n\n$$\\exp\\left[-\\left(0.5\\right)^2t/20\\right]$$\n\n$$\\sigma_r/\\sigma_1 = 0.9, \\eta = 0.1792$$\n\n$$10^{-1} \\quad \\exp\\left[-\\left(0.9\\right)^2t/20\\right]$$"}, {"type": "table", "rows": [["0", "5", "10", "15", "20", "25", "30", "35", "40", "45", "50"], ["", "", "", "", "", "iteration (t)", "", "", "", "", ""]], "md": "|0|5|10|15|20|25|30|35|40|45|50|\n|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | |iteration (t)| | | | | |", "isPerfectTable": true, "csv": "\"0\",\"5\",\"10\",\"15\",\"20\",\"25\",\"30\",\"35\",\"40\",\"45\",\"50\"\n\"\",\"\",\"\",\"\",\"\",\"iteration (t)\",\"\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 3: Zoom into the first few iterations of Fig. 2, with the y-axis in log scale.", "md": "Figure 3: Zoom into the first few iterations of Fig. 2, with the y-axis in log scale."}]}, {"page": 14, "text": "A     Non-asympototic singular value bounds\nProposition A.1 ([Ver10]). Let A be an d \u00d7 r matrix whose entries are independently drawn from\nN(0, 1). Then for every t \u2265     0, with probability at least 1 \u2212   exp(\u2212t2/2), we have\n                                         E\u03c3r(A) \u2265     \u221a d \u2212  \u221ar \u2212   t\nand for every t \u2265   0, with probability at least 1 \u2212   exp(\u2212t2/2), we have\n                                         E\u03c31(A) \u2264     \u221a d + \u221ar + t\nIf the variance if v, then everything in the above gets divided by \u221av.\nB     Differentiation\nConsider the function                  f(X, Y) \u2261     1 XY\u22ba     \u2212 A 2 F.\n                                                     2\nwith X \u2208    Rm\u00d7d and Y \u2208       Rn\u00d7d. Using the Kronecker relation vec(ACB\u22ba) = (B \u2297               A) vec(C),\nwe can express f as a function of vec(X):\n                             f(X, Y) = 1   2\u2225(Y \u2297    Im) vec(X) \u2212     vec(A)\u22252 2.\nNow we can apply vector calculus to observe\n                  \u2207Xf(X, Y) = (Y \u2297        Im)\u22ba  ((Y \u2297   Im) vec(X) \u2212    vec(A))\n                                = (Y\u22baY \u2297      Im) vec(X) \u2212    (Y \u2297   Im)\u22ba  vec(A) \u2208    Rmd\n                                = X(Y\u22baY) \u2212       AY\n                                = (XY\u22ba     \u2212  A)Y \u2208    Rm\u00d7d.\nApplying the same logic to the equivalent f(X, Y) = 1        2 YX\u22ba     \u2212 A\u22ba   2F yields\n                                 \u2207Yf(X, Y) = (XY\u22ba        \u2212  A)\u22baX \u2208     Rn\u00d7d.\nReferences\n[ABC+22] Kwangjun Ahn, S\u00e9bastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and\n              Yi Zhang. Learning threshold neurons via the \"edge of stability\". arXiv preprint\n               arXiv:2212.07469, December 2022.\n[ACHL19]       Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep\n               matrix factorization. Advances in Neural Information Processing Systems, 32, 2019.\n[AZL17]        Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca:\n               a global, gap-free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on\n              Foundations of Computer Science (FOCS), pages 487\u2013492. IEEE, 2017.\n[BKS16]        Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity\n               for faster semi-definite optimization. In Conference on Learning Theory, pages 530\u2013582.\n               PMLR, 2016.\n[BM03]         Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for\n               solving semidefinite programs via low-rank factorization. Mathematical Programming,\n               95(2):329\u2013357, 2003.\n[BM05]         Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank\n               semidefinite programming. Mathematical programming, 103(3):427\u2013444, 2005.\n[CB22]         Lei Chen and Joan Bruna. On gradient descent convergence beyond the edge of stability.\n               arXiv preprint arXiv:2206.04172, June 2022.\n                                                      14", "md": "# Math Equations and References\n\n## Non-asymptotic singular value bounds\n\nProposition A.1 ([Ver10]). Let A be an \\(d \\times r\\) matrix whose entries are independently drawn from \\(N(0, 1)\\). Then for every \\(t \\geq 0\\), with probability at least \\(1 - \\exp(-t^2/2)\\), we have\n\n$$\nE\\sigma_r(A) \\geq \\sqrt{d} - \\sqrt{r} - t\n$$\nand for every \\(t \\geq 0\\), with probability at least \\(1 - \\exp(-t^2/2)\\), we have\n\n$$\nE\\sigma_1(A) \\leq \\sqrt{d} + \\sqrt{r} + t\n$$\nIf the variance is \\(v\\), then everything in the above gets divided by \\(\\sqrt{v}\\).\n\n## Differentiation\n\nConsider the function \\(f(X, Y) \\equiv \\frac{1}{2} XY^\\top - \\frac{1}{2} A\\).\n\nwith \\(X \\in \\mathbb{R}^{m \\times d}\\) and \\(Y \\in \\mathbb{R}^{n \\times d}\\). Using the Kronecker relation \\(\\text{vec}(ACB^\\top) = (B \\otimes A) \\text{vec}(C)\\), we can express \\(f\\) as a function of \\(\\text{vec}(X)\\):\n\n$$\nf(X, Y) = \\frac{1}{2} \\left\\| (Y \\otimes I_m) \\text{vec}(X) - \\text{vec}(A) \\right\\|_2^2.\n$$\nNow we can apply vector calculus to observe\n\n$$\n\\nabla_X f(X, Y) = (Y \\otimes I_m)^\\top \\left( (Y \\otimes I_m) \\text{vec}(X) - \\text{vec}(A) \\right) = X(Y^\\top Y) - AY = (XY^\\top - A)Y.\n$$\nApplying the same logic to the equivalent \\(f(X, Y) = \\frac{1}{2} YX^\\top - \\frac{1}{2} A^\\top\\) yields\n\n$$\n\\nabla_Y f(X, Y) = (XY^\\top - A)^\\top X.\n$$\n\n## References\n\n- [ABC+22] Kwangjun Ahn, S\u00e9bastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via the \"edge of stability\". arXiv preprint arXiv:2212.07469, December 2022.\n- [ACHL19] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019.\n- [AZL17] Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global, gap-free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 487\u2013492. IEEE, 2017.\n- [BKS16] Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster semi-definite optimization. In Conference on Learning Theory, pages 530\u2013582. PMLR, 2016.\n- [BM03] Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329\u2013357, 2003.\n- [BM05] Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank semidefinite programming. Mathematical programming, 103(3):427\u2013444, 2005.\n- [CB22] Lei Chen and Joan Bruna. On gradient descent convergence beyond the edge of stability. arXiv preprint arXiv:2206.04172, June 2022.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and References", "md": "# Math Equations and References"}, {"type": "heading", "lvl": 2, "value": "Non-asymptotic singular value bounds", "md": "## Non-asymptotic singular value bounds"}, {"type": "text", "value": "Proposition A.1 ([Ver10]). Let A be an \\(d \\times r\\) matrix whose entries are independently drawn from \\(N(0, 1)\\). Then for every \\(t \\geq 0\\), with probability at least \\(1 - \\exp(-t^2/2)\\), we have\n\n$$\nE\\sigma_r(A) \\geq \\sqrt{d} - \\sqrt{r} - t\n$$\nand for every \\(t \\geq 0\\), with probability at least \\(1 - \\exp(-t^2/2)\\), we have\n\n$$\nE\\sigma_1(A) \\leq \\sqrt{d} + \\sqrt{r} + t\n$$\nIf the variance is \\(v\\), then everything in the above gets divided by \\(\\sqrt{v}\\).", "md": "Proposition A.1 ([Ver10]). Let A be an \\(d \\times r\\) matrix whose entries are independently drawn from \\(N(0, 1)\\). Then for every \\(t \\geq 0\\), with probability at least \\(1 - \\exp(-t^2/2)\\), we have\n\n$$\nE\\sigma_r(A) \\geq \\sqrt{d} - \\sqrt{r} - t\n$$\nand for every \\(t \\geq 0\\), with probability at least \\(1 - \\exp(-t^2/2)\\), we have\n\n$$\nE\\sigma_1(A) \\leq \\sqrt{d} + \\sqrt{r} + t\n$$\nIf the variance is \\(v\\), then everything in the above gets divided by \\(\\sqrt{v}\\)."}, {"type": "heading", "lvl": 2, "value": "Differentiation", "md": "## Differentiation"}, {"type": "text", "value": "Consider the function \\(f(X, Y) \\equiv \\frac{1}{2} XY^\\top - \\frac{1}{2} A\\).\n\nwith \\(X \\in \\mathbb{R}^{m \\times d}\\) and \\(Y \\in \\mathbb{R}^{n \\times d}\\). Using the Kronecker relation \\(\\text{vec}(ACB^\\top) = (B \\otimes A) \\text{vec}(C)\\), we can express \\(f\\) as a function of \\(\\text{vec}(X)\\):\n\n$$\nf(X, Y) = \\frac{1}{2} \\left\\| (Y \\otimes I_m) \\text{vec}(X) - \\text{vec}(A) \\right\\|_2^2.\n$$\nNow we can apply vector calculus to observe\n\n$$\n\\nabla_X f(X, Y) = (Y \\otimes I_m)^\\top \\left( (Y \\otimes I_m) \\text{vec}(X) - \\text{vec}(A) \\right) = X(Y^\\top Y) - AY = (XY^\\top - A)Y.\n$$\nApplying the same logic to the equivalent \\(f(X, Y) = \\frac{1}{2} YX^\\top - \\frac{1}{2} A^\\top\\) yields\n\n$$\n\\nabla_Y f(X, Y) = (XY^\\top - A)^\\top X.\n$$", "md": "Consider the function \\(f(X, Y) \\equiv \\frac{1}{2} XY^\\top - \\frac{1}{2} A\\).\n\nwith \\(X \\in \\mathbb{R}^{m \\times d}\\) and \\(Y \\in \\mathbb{R}^{n \\times d}\\). Using the Kronecker relation \\(\\text{vec}(ACB^\\top) = (B \\otimes A) \\text{vec}(C)\\), we can express \\(f\\) as a function of \\(\\text{vec}(X)\\):\n\n$$\nf(X, Y) = \\frac{1}{2} \\left\\| (Y \\otimes I_m) \\text{vec}(X) - \\text{vec}(A) \\right\\|_2^2.\n$$\nNow we can apply vector calculus to observe\n\n$$\n\\nabla_X f(X, Y) = (Y \\otimes I_m)^\\top \\left( (Y \\otimes I_m) \\text{vec}(X) - \\text{vec}(A) \\right) = X(Y^\\top Y) - AY = (XY^\\top - A)Y.\n$$\nApplying the same logic to the equivalent \\(f(X, Y) = \\frac{1}{2} YX^\\top - \\frac{1}{2} A^\\top\\) yields\n\n$$\n\\nabla_Y f(X, Y) = (XY^\\top - A)^\\top X.\n$$"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- [ABC+22] Kwangjun Ahn, S\u00e9bastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via the \"edge of stability\". arXiv preprint arXiv:2212.07469, December 2022.\n- [ACHL19] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019.\n- [AZL17] Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global, gap-free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 487\u2013492. IEEE, 2017.\n- [BKS16] Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster semi-definite optimization. In Conference on Learning Theory, pages 530\u2013582. PMLR, 2016.\n- [BM03] Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329\u2013357, 2003.\n- [BM05] Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank semidefinite programming. Mathematical programming, 103(3):427\u2013444, 2005.\n- [CB22] Lei Chen and Joan Bruna. On gradient descent convergence beyond the edge of stability. arXiv preprint arXiv:2206.04172, June 2022.", "md": "- [ABC+22] Kwangjun Ahn, S\u00e9bastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via the \"edge of stability\". arXiv preprint arXiv:2212.07469, December 2022.\n- [ACHL19] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019.\n- [AZL17] Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global, gap-free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 487\u2013492. IEEE, 2017.\n- [BKS16] Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster semi-definite optimization. In Conference on Learning Theory, pages 530\u2013582. PMLR, 2016.\n- [BM03] Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329\u2013357, 2003.\n- [BM05] Samuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank semidefinite programming. Mathematical programming, 103(3):427\u2013444, 2005.\n- [CB22] Lei Chen and Joan Bruna. On gradient descent convergence beyond the edge of stability. arXiv preprint arXiv:2206.04172, June 2022."}]}, {"page": 15, "text": "[CCFM19] Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Gradient descent with random\n             initialization: Fast global convergence for nonconvex phase retrieval. Mathematical\n             Programming, 176:5\u201337, 2019.\n[CGMR20] Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut. Gradient\n             descent for deep matrix factorization: Dynamics and implicit bias towards low rank.\n             arXiv preprint arXiv:2011.13772, 2020.\n[CKL+21]     Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar.\n             Gradient descent on neural networks typically occurs at the edge of stability. arXiv\n             preprint arXiv:2103.00065, February 2021.\n[DHL18]      Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep ho-\n             mogeneous models: Layers are automatically balanced. Advances in neural information\n             processing systems, 31, 2018.\n[DS01]       Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices\n             and Banach spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.\n[GHJY15]     Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online\n             stochastic gradient for tensor decomposition. In Conference on learning theory, pages\n             797\u2013842. PMLR, 2015.\n[GWB+17] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,\n             and Nati Srebro. Implicit regularization in matrix factorization. Advances in Neural\n             Information Processing Systems, 30, 2017.\n[HKD20]      David Hong, Tamara G Kolda, and Jed A Duersch. Generalized canonical polyadic\n             tensor decomposition. SIAM Review, 62(1):133\u2013163, 2020.\n[JCD22]      Liwei Jiang, Yudong Chen, and Lijun Ding. Algorithmic regularization in model-free\n             overparametrized asymmetric matrix factorization. arXiv preprint arXiv:2203.02839,\n             March 2022.\n[JJKN17]     Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli. Global convergence of\n             non-convex gradient descent for computing matrix squareroot. In Artificial Intelligence\n             and Statistics, pages 479\u2013488. PMLR, 2017.\n[KH20]       Tamara G Kolda and David Hong. Stochastic gradients for large-scale tensor decompo-\n             sition. SIAM Journal on Mathematics of Data Science, 2(4):1066\u20131095, 2020.\n[LMZ18]      Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-\n             parameterized matrix sensing and neural networks with quadratic activations. In Confer-\n             ence On Learning Theory, pages 2\u201347. PMLR, 2018.\n[SWW17]      Sujay Sanghavi, Rachel Ward, and Chris D White. The local convexity of solving\n             systems of quadratic equations. Results in Mathematics, 71:569\u2013608, 2017.\n[TBS+16]     Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht.\n             Low-rank solutions of linear matrix equations via Procrustes flow. In International\n             Conference on Machine Learning, pages 964\u2013973. PMLR, 2016.\n[Ver10]      Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In\n             Compressed Sensing, pages 210\u2013268. Cambridge University Press, may 2010.\n[WCZT21] Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao. Large learning rate tames\n             homogeneity: Convergence and balancing effect. arXiv preprint arXiv:2110.03677,\n             2021.\n[YD21]       Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric\n             low-rank matrix factorization. Advances in Neural Information Processing Systems,\n             34:1429\u20131439, 2021.\n[ZL16]       Qinqing Zheng and John Lafferty.       Convergence analysis for rectangular matrix\n             completion using Burer-Monteiro factorization and gradient descent. arXiv preprint\n             arXiv:1605.07051, 2016.\n[ZWL15]      Tuo Zhao, Zhaoran Wang, and Han Liu. Nonconvex low rank matrix factorization\n             via inexact first order oracle. Advances in Neural Information Processing Systems,\n             458:461\u2013462, 2015.\n                                                 15", "md": "# References\n\n# List of References\n\n|Reference|Authors|Title|Journal/Conference|Year|\n|---|---|---|---|---|\n|[CCFM19]|Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma|Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval|Mathematical Programming|2019|\n|[CGMR20]|Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut|Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank|arXiv preprint arXiv:2011.13772|2020|\n|[CKL+21]|Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar|Gradient descent on neural networks typically occurs at the edge of stability|arXiv preprint arXiv:2103.00065|February 2021|\n|[DHL18]|Simon S Du, Wei Hu, and Jason D Lee|Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced|Advances in neural information processing systems|2018|\n|[DS01]|Kenneth R Davidson and Stanislaw J Szarek|Local operator theory, random matrices and Banach spaces|Handbook of the geometry of Banach spaces|2001|\n|[GHJY15]|Rong Ge, Furong Huang, Chi Jin, and Yang Yuan|Escaping from saddle points\u2014online stochastic gradient for tensor decomposition|Conference on learning theory|2015|\n|[GWB+17]|Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro|Implicit regularization in matrix factorization|Advances in Neural Information Processing Systems|2017|\n|[HKD20]|David Hong, Tamara G Kolda, and Jed A Duersch|Generalized canonical polyadic tensor decomposition|SIAM Review|2020|\n|[JCD22]|Liwei Jiang, Yudong Chen, and Lijun Ding|Algorithmic regularization in model-free overparametrized asymmetric matrix factorization|arXiv preprint arXiv:2203.02839|March 2022|\n|[JJKN17]|Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli|Global convergence of non-convex gradient descent for computing matrix squareroot|Artificial Intelligence and Statistics|2017|\n|[KH20]|Tamara G Kolda and David Hong|Stochastic gradients for large-scale tensor decomposition|SIAM Journal on Mathematics of Data Science|2020|\n|[LMZ18]|Yuanzhi Li, Tengyu Ma, and Hongyang Zhang|Algorithmic regularization in overparameterized matrix sensing and neural networks with quadratic activations|Conference On Learning Theory|2018|\n|[SWW17]|Sujay Sanghavi, Rachel Ward, and Chris D White|The local convexity of solving systems of quadratic equations|Results in Mathematics|2017|\n|[TBS+16]|Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht|Low-rank solutions of linear matrix equations via Procrustes flow|International Conference on Machine Learning|2016|\n|[Ver10]|Roman Vershynin|Introduction to the non-asymptotic analysis of random matrices|Compressed Sensing|May 2010|\n|[WCZT21]|Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao|Large learning rate tames homogeneity: Convergence and balancing effect|arXiv preprint arXiv:2110.03677|2021|\n|[YD21]|Tian Ye and Simon S Du|Global convergence of gradient descent for asymmetric low-rank matrix factorization|Advances in Neural Information Processing Systems|2021|\n|[ZL16]|Qinqing Zheng and John Lafferty|Convergence analysis for rectangular matrix completion using Burer-Monteiro factorization and gradient descent|arXiv preprint arXiv:1605.07051|2016|\n|[ZWL15]|Tuo Zhao, Zhaoran Wang, and Han Liu|Nonconvex low rank matrix factorization via inexact first order oracle|Advances in Neural Information Processing Systems|2015|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["Reference", "Authors", "Title", "Journal/Conference", "Year"], ["[CCFM19]", "Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma", "Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval", "Mathematical Programming", "2019"], ["[CGMR20]", "Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut", "Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank", "arXiv preprint arXiv:2011.13772", "2020"], ["[CKL+21]", "Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar", "Gradient descent on neural networks typically occurs at the edge of stability", "arXiv preprint arXiv:2103.00065", "February 2021"], ["[DHL18]", "Simon S Du, Wei Hu, and Jason D Lee", "Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced", "Advances in neural information processing systems", "2018"], ["[DS01]", "Kenneth R Davidson and Stanislaw J Szarek", "Local operator theory, random matrices and Banach spaces", "Handbook of the geometry of Banach spaces", "2001"], ["[GHJY15]", "Rong Ge, Furong Huang, Chi Jin, and Yang Yuan", "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "Conference on learning theory", "2015"], ["[GWB+17]", "Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro", "Implicit regularization in matrix factorization", "Advances in Neural Information Processing Systems", "2017"], ["[HKD20]", "David Hong, Tamara G Kolda, and Jed A Duersch", "Generalized canonical polyadic tensor decomposition", "SIAM Review", "2020"], ["[JCD22]", "Liwei Jiang, Yudong Chen, and Lijun Ding", "Algorithmic regularization in model-free overparametrized asymmetric matrix factorization", "arXiv preprint arXiv:2203.02839", "March 2022"], ["[JJKN17]", "Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli", "Global convergence of non-convex gradient descent for computing matrix squareroot", "Artificial Intelligence and Statistics", "2017"], ["[KH20]", "Tamara G Kolda and David Hong", "Stochastic gradients for large-scale tensor decomposition", "SIAM Journal on Mathematics of Data Science", "2020"], ["[LMZ18]", "Yuanzhi Li, Tengyu Ma, and Hongyang Zhang", "Algorithmic regularization in overparameterized matrix sensing and neural networks with quadratic activations", "Conference On Learning Theory", "2018"], ["[SWW17]", "Sujay Sanghavi, Rachel Ward, and Chris D White", "The local convexity of solving systems of quadratic equations", "Results in Mathematics", "2017"], ["[TBS+16]", "Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht", "Low-rank solutions of linear matrix equations via Procrustes flow", "International Conference on Machine Learning", "2016"], ["[Ver10]", "Roman Vershynin", "Introduction to the non-asymptotic analysis of random matrices", "Compressed Sensing", "May 2010"], ["[WCZT21]", "Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao", "Large learning rate tames homogeneity: Convergence and balancing effect", "arXiv preprint arXiv:2110.03677", "2021"], ["[YD21]", "Tian Ye and Simon S Du", "Global convergence of gradient descent for asymmetric low-rank matrix factorization", "Advances in Neural Information Processing Systems", "2021"], ["[ZL16]", "Qinqing Zheng and John Lafferty", "Convergence analysis for rectangular matrix completion using Burer-Monteiro factorization and gradient descent", "arXiv preprint arXiv:1605.07051", "2016"], ["[ZWL15]", "Tuo Zhao, Zhaoran Wang, and Han Liu", "Nonconvex low rank matrix factorization via inexact first order oracle", "Advances in Neural Information Processing Systems", "2015"]], "md": "|Reference|Authors|Title|Journal/Conference|Year|\n|---|---|---|---|---|\n|[CCFM19]|Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma|Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval|Mathematical Programming|2019|\n|[CGMR20]|Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut|Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank|arXiv preprint arXiv:2011.13772|2020|\n|[CKL+21]|Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar|Gradient descent on neural networks typically occurs at the edge of stability|arXiv preprint arXiv:2103.00065|February 2021|\n|[DHL18]|Simon S Du, Wei Hu, and Jason D Lee|Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced|Advances in neural information processing systems|2018|\n|[DS01]|Kenneth R Davidson and Stanislaw J Szarek|Local operator theory, random matrices and Banach spaces|Handbook of the geometry of Banach spaces|2001|\n|[GHJY15]|Rong Ge, Furong Huang, Chi Jin, and Yang Yuan|Escaping from saddle points\u2014online stochastic gradient for tensor decomposition|Conference on learning theory|2015|\n|[GWB+17]|Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro|Implicit regularization in matrix factorization|Advances in Neural Information Processing Systems|2017|\n|[HKD20]|David Hong, Tamara G Kolda, and Jed A Duersch|Generalized canonical polyadic tensor decomposition|SIAM Review|2020|\n|[JCD22]|Liwei Jiang, Yudong Chen, and Lijun Ding|Algorithmic regularization in model-free overparametrized asymmetric matrix factorization|arXiv preprint arXiv:2203.02839|March 2022|\n|[JJKN17]|Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli|Global convergence of non-convex gradient descent for computing matrix squareroot|Artificial Intelligence and Statistics|2017|\n|[KH20]|Tamara G Kolda and David Hong|Stochastic gradients for large-scale tensor decomposition|SIAM Journal on Mathematics of Data Science|2020|\n|[LMZ18]|Yuanzhi Li, Tengyu Ma, and Hongyang Zhang|Algorithmic regularization in overparameterized matrix sensing and neural networks with quadratic activations|Conference On Learning Theory|2018|\n|[SWW17]|Sujay Sanghavi, Rachel Ward, and Chris D White|The local convexity of solving systems of quadratic equations|Results in Mathematics|2017|\n|[TBS+16]|Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht|Low-rank solutions of linear matrix equations via Procrustes flow|International Conference on Machine Learning|2016|\n|[Ver10]|Roman Vershynin|Introduction to the non-asymptotic analysis of random matrices|Compressed Sensing|May 2010|\n|[WCZT21]|Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao|Large learning rate tames homogeneity: Convergence and balancing effect|arXiv preprint arXiv:2110.03677|2021|\n|[YD21]|Tian Ye and Simon S Du|Global convergence of gradient descent for asymmetric low-rank matrix factorization|Advances in Neural Information Processing Systems|2021|\n|[ZL16]|Qinqing Zheng and John Lafferty|Convergence analysis for rectangular matrix completion using Burer-Monteiro factorization and gradient descent|arXiv preprint arXiv:1605.07051|2016|\n|[ZWL15]|Tuo Zhao, Zhaoran Wang, and Han Liu|Nonconvex low rank matrix factorization via inexact first order oracle|Advances in Neural Information Processing Systems|2015|", "isPerfectTable": true, "csv": "\"Reference\",\"Authors\",\"Title\",\"Journal/Conference\",\"Year\"\n\"[CCFM19]\",\"Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma\",\"Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval\",\"Mathematical Programming\",\"2019\"\n\"[CGMR20]\",\"Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut\",\"Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank\",\"arXiv preprint arXiv:2011.13772\",\"2020\"\n\"[CKL+21]\",\"Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar\",\"Gradient descent on neural networks typically occurs at the edge of stability\",\"arXiv preprint arXiv:2103.00065\",\"February 2021\"\n\"[DHL18]\",\"Simon S Du, Wei Hu, and Jason D Lee\",\"Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced\",\"Advances in neural information processing systems\",\"2018\"\n\"[DS01]\",\"Kenneth R Davidson and Stanislaw J Szarek\",\"Local operator theory, random matrices and Banach spaces\",\"Handbook of the geometry of Banach spaces\",\"2001\"\n\"[GHJY15]\",\"Rong Ge, Furong Huang, Chi Jin, and Yang Yuan\",\"Escaping from saddle points\u2014online stochastic gradient for tensor decomposition\",\"Conference on learning theory\",\"2015\"\n\"[GWB+17]\",\"Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro\",\"Implicit regularization in matrix factorization\",\"Advances in Neural Information Processing Systems\",\"2017\"\n\"[HKD20]\",\"David Hong, Tamara G Kolda, and Jed A Duersch\",\"Generalized canonical polyadic tensor decomposition\",\"SIAM Review\",\"2020\"\n\"[JCD22]\",\"Liwei Jiang, Yudong Chen, and Lijun Ding\",\"Algorithmic regularization in model-free overparametrized asymmetric matrix factorization\",\"arXiv preprint arXiv:2203.02839\",\"March 2022\"\n\"[JJKN17]\",\"Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli\",\"Global convergence of non-convex gradient descent for computing matrix squareroot\",\"Artificial Intelligence and Statistics\",\"2017\"\n\"[KH20]\",\"Tamara G Kolda and David Hong\",\"Stochastic gradients for large-scale tensor decomposition\",\"SIAM Journal on Mathematics of Data Science\",\"2020\"\n\"[LMZ18]\",\"Yuanzhi Li, Tengyu Ma, and Hongyang Zhang\",\"Algorithmic regularization in overparameterized matrix sensing and neural networks with quadratic activations\",\"Conference On Learning Theory\",\"2018\"\n\"[SWW17]\",\"Sujay Sanghavi, Rachel Ward, and Chris D White\",\"The local convexity of solving systems of quadratic equations\",\"Results in Mathematics\",\"2017\"\n\"[TBS+16]\",\"Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht\",\"Low-rank solutions of linear matrix equations via Procrustes flow\",\"International Conference on Machine Learning\",\"2016\"\n\"[Ver10]\",\"Roman Vershynin\",\"Introduction to the non-asymptotic analysis of random matrices\",\"Compressed Sensing\",\"May 2010\"\n\"[WCZT21]\",\"Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao\",\"Large learning rate tames homogeneity: Convergence and balancing effect\",\"arXiv preprint arXiv:2110.03677\",\"2021\"\n\"[YD21]\",\"Tian Ye and Simon S Du\",\"Global convergence of gradient descent for asymmetric low-rank matrix factorization\",\"Advances in Neural Information Processing Systems\",\"2021\"\n\"[ZL16]\",\"Qinqing Zheng and John Lafferty\",\"Convergence analysis for rectangular matrix completion using Burer-Monteiro factorization and gradient descent\",\"arXiv preprint arXiv:1605.07051\",\"2016\"\n\"[ZWL15]\",\"Tuo Zhao, Zhaoran Wang, and Han Liu\",\"Nonconvex low rank matrix factorization via inexact first order oracle\",\"Advances in Neural Information Processing Systems\",\"2015\""}]}], "job_id": "1bd1b165-6f8a-4832-b3cf-969ffef50822", "file_path": "./corpus/2305.06927.pdf"}