{"pages": [{"page": 1, "text": "                      Tester-Learners for Halfspaces: Universal Algorithms\n                    Aravind Gollakota\u2217             Adam R. Klivans\u2020            Konstantinos Stavropoulos\u2021\n                         UT Austin                     UT Austin                         UT Austin\n                                                         Arsen Vasilyan\u00a7\n                                                                MIT\narXiv:2305.11765v1  [cs.LG]  19 May 2023                  May 19, 2023\n                                                             Abstract\n                    We give the first tester-learner for halfspaces that succeeds universally over a wide class\n                 of structured distributions. Our universal tester-learner runs in fully polynomial time and has\n                 the following guarantee: the learner achieves error O(opt) + \u03f5 on any labeled distribution that\n                 the tester accepts, and moreover, the tester accepts whenever the marginal is any distribution\n                 that satisfies a Poincar\u00b4e inequality. In contrast to prior work on testable learning, our tester\n                 is not tailored to any single target distribution but rather succeeds for an entire target class of\n                 distributions. The class of Poincar\u00b4e distributions includes all strongly log-concave distributions,\n                 and, assuming the Kannan\u2013L\u00b4    ovasz\u2013Simonovits (KLS) conjecture, includes all log-concave dis-\n                 tributions. In the special case where the label noise is known to be Massart, our tester-learner\n                 achieves error opt + \u03f5 while accepting all log-concave distributions unconditionally (without as-\n                 suming KLS). Our tests rely on checking hypercontractivity of the unknown distribution using\n                 a sum-of-squares (SOS) program, and crucially make use of the fact that Poincar\u00b4e distributions\n                 are certifiably hypercontractive in the SOS framework.\n             \u2217aravindg@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\n          Machine Learning (IFML).\n             \u2020klivans@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\n          Machine Learning (IFML).\n             \u2021kstavrop@cs.utexas.edu.     Supported by NSF award AF-1909204, the NSF AI Institute for Foundations of\n          Machine Learning (IFML), by scholarships from Bodossaki Foundation and from Leventis Foundation.\n             \u00a7vasilyan@mit.edu.    Supported in part by NSF awards CCF-2006664, DMS-2022448, CCF-1565235, CCF-\n          1955217, Big George Fellowship and Fintech@CSAIL. Work done in part while visiting UT Austin.\n                                                                  1", "md": "# Tester-Learners for Halfspaces: Universal Algorithms\n\n# Tester-Learners for Halfspaces: Universal Algorithms\n\nAravind Gollakota - UT Austin\n\nAdam R. Klivans - UT Austin\n\nKonstantinos Stavropoulos - UT Austin\n\nArsen Vasilyan - MIT\n\narXiv:2305.11765v1 [cs.LG] 19 May 2023\n\n## Abstract\n\nWe give the first tester-learner for halfspaces that succeeds universally over a wide class\nof structured distributions. Our universal tester-learner runs in fully polynomial time and has\nthe following guarantee: the learner achieves error $$O(opt) + \\epsilon$$ on any labeled distribution that\nthe tester accepts, and moreover, the tester accepts whenever the marginal is any distribution\nthat satisfies a Poincar\u00e9 inequality. In contrast to prior work on testable learning, our tester\nis not tailored to any single target distribution but rather succeeds for an entire target class of\ndistributions. The class of Poincar\u00e9 distributions includes all strongly log-concave distributions,\nand, assuming the Kannan\u2013Lov\u00e1sz\u2013Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner\nachieves error $$opt + \\epsilon$$ while accepting all log-concave distributions unconditionally (without assuming KLS). Our tests rely on checking hypercontractivity of the unknown distribution using\na sum-of-squares (SOS) program, and crucially make use of the fact that Poincar\u00e9 distributions\nare certifiably hypercontractive in the SOS framework.\n\n\u2217 aravindg@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2020 klivans@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2021 kstavrop@cs.utexas.edu. Supported by NSF award AF-1909204, the NSF AI Institute for Foundations of\nMachine Learning (IFML), by scholarships from Bodossaki Foundation and from Leventis Foundation.\n\n\u00a7 vasilyan@mit.edu. Supported in part by NSF awards CCF-2006664, DMS-2022448, CCF-1565235, CCF-1955217, Big George Fellowship and Fintech@CSAIL. Work done in part while visiting UT Austin.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Tester-Learners for Halfspaces: Universal Algorithms", "md": "# Tester-Learners for Halfspaces: Universal Algorithms"}, {"type": "heading", "lvl": 1, "value": "Tester-Learners for Halfspaces: Universal Algorithms", "md": "# Tester-Learners for Halfspaces: Universal Algorithms"}, {"type": "text", "value": "Aravind Gollakota - UT Austin\n\nAdam R. Klivans - UT Austin\n\nKonstantinos Stavropoulos - UT Austin\n\nArsen Vasilyan - MIT\n\narXiv:2305.11765v1 [cs.LG] 19 May 2023", "md": "Aravind Gollakota - UT Austin\n\nAdam R. Klivans - UT Austin\n\nKonstantinos Stavropoulos - UT Austin\n\nArsen Vasilyan - MIT\n\narXiv:2305.11765v1 [cs.LG] 19 May 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "We give the first tester-learner for halfspaces that succeeds universally over a wide class\nof structured distributions. Our universal tester-learner runs in fully polynomial time and has\nthe following guarantee: the learner achieves error $$O(opt) + \\epsilon$$ on any labeled distribution that\nthe tester accepts, and moreover, the tester accepts whenever the marginal is any distribution\nthat satisfies a Poincar\u00e9 inequality. In contrast to prior work on testable learning, our tester\nis not tailored to any single target distribution but rather succeeds for an entire target class of\ndistributions. The class of Poincar\u00e9 distributions includes all strongly log-concave distributions,\nand, assuming the Kannan\u2013Lov\u00e1sz\u2013Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner\nachieves error $$opt + \\epsilon$$ while accepting all log-concave distributions unconditionally (without assuming KLS). Our tests rely on checking hypercontractivity of the unknown distribution using\na sum-of-squares (SOS) program, and crucially make use of the fact that Poincar\u00e9 distributions\nare certifiably hypercontractive in the SOS framework.\n\n\u2217 aravindg@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2020 klivans@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2021 kstavrop@cs.utexas.edu. Supported by NSF award AF-1909204, the NSF AI Institute for Foundations of\nMachine Learning (IFML), by scholarships from Bodossaki Foundation and from Leventis Foundation.\n\n\u00a7 vasilyan@mit.edu. Supported in part by NSF awards CCF-2006664, DMS-2022448, CCF-1565235, CCF-1955217, Big George Fellowship and Fintech@CSAIL. Work done in part while visiting UT Austin.", "md": "We give the first tester-learner for halfspaces that succeeds universally over a wide class\nof structured distributions. Our universal tester-learner runs in fully polynomial time and has\nthe following guarantee: the learner achieves error $$O(opt) + \\epsilon$$ on any labeled distribution that\nthe tester accepts, and moreover, the tester accepts whenever the marginal is any distribution\nthat satisfies a Poincar\u00e9 inequality. In contrast to prior work on testable learning, our tester\nis not tailored to any single target distribution but rather succeeds for an entire target class of\ndistributions. The class of Poincar\u00e9 distributions includes all strongly log-concave distributions,\nand, assuming the Kannan\u2013Lov\u00e1sz\u2013Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner\nachieves error $$opt + \\epsilon$$ while accepting all log-concave distributions unconditionally (without assuming KLS). Our tests rely on checking hypercontractivity of the unknown distribution using\na sum-of-squares (SOS) program, and crucially make use of the fact that Poincar\u00e9 distributions\nare certifiably hypercontractive in the SOS framework.\n\n\u2217 aravindg@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2020 klivans@cs.utexas.edu. Supported by NSF award AF-1909204 and the NSF AI Institute for Foundations of\nMachine Learning (IFML).\n\n\u2021 kstavrop@cs.utexas.edu. Supported by NSF award AF-1909204, the NSF AI Institute for Foundations of\nMachine Learning (IFML), by scholarships from Bodossaki Foundation and from Leventis Foundation.\n\n\u00a7 vasilyan@mit.edu. Supported in part by NSF awards CCF-2006664, DMS-2022448, CCF-1565235, CCF-1955217, Big George Fellowship and Fintech@CSAIL. Work done in part while visiting UT Austin."}]}, {"page": 2, "text": "1    Introduction\nIn this paper we study the recent model of testable learning, due to Rubinfeld and Vasilyan [RV23].\nTestable learning addresses a key issue with essentially all known algorithms for the basic problem\nof agnostic learning, in which a learner is required to produce a hypothesis competitive with the\nbest-fitting hypothesis in a concept class C. The issue is that these algorithms make distributional\nassumptions (such as Gaussianity or log-concavity) that are in general hard to verify. This means\nthat in the absence of any prior information about the distribution or the optimal achievable error,\nit can be hard to check that the learner has even succeeded at meeting its guarantee.\n   In the testable learning model, the learning algorithm, or tester-learner, is given access to labeled\nexamples from an unknown distribution and may either reject or accept the unknown distribution.\nIf it accepts, it must successfully produce a near-optimal hypothesis. Moreover, it is also required\nto accept whenever the unknown distribution truly has a certain target marginal D\u2217. Work of\n[RV23, GKK23, GKSV23, DKK+23] provided tester-learners for a range of basic classes (including\nhalfspaces, intersections of halfspaces, and more) with respect to particular target marginals D\u2217\n(such as the standard Gaussian). All of these algorithms, however, have the shortcoming that they\nare closely tailored to the particular target marginal D\u2217    that is chosen. Indeed, their tests would\nreject many well-behaved distributions that are appreciably far from D\u2217. A highly natural question\nfrom both a theoretical and a practical perspective is: can we design tester-learners that accept a\nwide class of distributions simultaneously, without being tailored to any particular one?\n   In this work we answer this question in the affirmative by introducing and studying universally\ntestable learning. We formally define this model as follows.\nDefinition 1.1 (Universally Testable Learning). Let C be a concept class mapping Rd to {\u00b11}.\nLet D be a family of distributions over Rd. Let \u03f5, \u03b4 > 0 be parameters, and let \u03c8 : [0, 1] \u2192    [0, 1] be\nsome function. We say C can be universally testably learned w.r.t. D up to error \u03c8(opt) + \u03f5 with\nfailure probability \u03b4 if there exists a tester-learner A meeting the following specification. For any\ndistribution DXY on Rd \u00d7 {\u00b11}, A takes in a large sample S drawn from DXY, and either rejects\nS or accepts and produces a hypothesis h : Rd \u2192      {\u00b11} such that:\n (a) (Soundness.) With probability at least 1 \u2212     \u03b4 over the sample S the following is true:\n      If A accepts, then the output h satisfies P(x,y)\u223cDXY[h(x) \u0338= y] \u2264     \u03c8(opt(C, DXY)) + \u03f5, where\n      opt(C, DXY) = inff\u2208C P(x,y)\u223cDXY[h(x) \u0338= y].\n (b) (Completeness.) Whenever the marginal of DXY lies within D, A accepts with probability at\n      least 1 \u2212 \u03b4 over the sample S.\n   In this terminology, the original definition of testable learning reduces to the special case where\nD = {D\u2217}.     We stress that while the prior work of [GKK23] allowed D\u2217           to be, say, any fixed\nstrongly log-concave distribution, their tester-learners are still tailored to the particular D\u2217  that is\nselected. This is because their tests rely on checking that the unknown distribution closely matches\nmoments with D\u2217. By contrast, a universal tester-learner must accept all marginals in a family D.\n   Our main contribution in this paper is the first universal tester-learner for the class of half-\nspaces with respect to a broad family of structured continuous distributions. This family is the\nset of all distributions with bounded Poincar\u00b4e constant (see Definition 2.4) and some mild concen-\ntration and anti-concentration properties (see Definition 2.1). It captures all strongly log-concave\ndistributions, and in fact, under the well-known Kannan\u2013L\u00b4    ovasz\u2013Simonovits (KLS) conjecture (see\nConjecture 2.6), it captures all log-concave distributions as well. Our universal tester-learner sig-\nnificantly generalizes the main result of [GKSV23], who showed comparable guarantees only for the\ncase where the target marginal is the standard Gaussian.\n                                                   2", "md": "# Testable Learning\n\n# Introduction\n\nIn this paper we study the recent model of testable learning, due to Rubinfeld and Vasilyan [RV23]. Testable learning addresses a key issue with essentially all known algorithms for the basic problem of agnostic learning, in which a learner is required to produce a hypothesis competitive with the best-fitting hypothesis in a concept class C. The issue is that these algorithms make distributional assumptions (such as Gaussianity or log-concavity) that are in general hard to verify. This means that in the absence of any prior information about the distribution or the optimal achievable error, it can be hard to check that the learner has even succeeded at meeting its guarantee.\n\nIn the testable learning model, the learning algorithm, or tester-learner, is given access to labeled examples from an unknown distribution and may either reject or accept the unknown distribution. If it accepts, it must successfully produce a near-optimal hypothesis. Moreover, it is also required to accept whenever the unknown distribution truly has a certain target marginal D\u2217. Work of [RV23, GKK23, GKSV23, DKK+23] provided tester-learners for a range of basic classes (including halfspaces, intersections of halfspaces, and more) with respect to particular target marginals D\u2217 (such as the standard Gaussian). All of these algorithms, however, have the shortcoming that they are closely tailored to the particular target marginal D\u2217 that is chosen. Indeed, their tests would reject many well-behaved distributions that are appreciably far from D\u2217. A highly natural question from both a theoretical and a practical perspective is: can we design tester-learners that accept a wide class of distributions simultaneously, without being tailored to any particular one?\n\nIn this work we answer this question in the affirmative by introducing and studying universally testable learning. We formally define this model as follows.\n\n## Definition 1.1 (Universally Testable Learning)\n\nLet C be a concept class mapping Rd to {\u00b11}. Let D be a family of distributions over Rd. Let \u03f5, \u03b4 > 0 be parameters, and let \u03c8 : [0, 1] \u2192 [0, 1] be some function. We say C can be universally testably learned w.r.t. D up to error \u03c8(opt) + \u03f5 with failure probability \u03b4 if there exists a tester-learner A meeting the following specification. For any distribution DXY on Rd \u00d7 {\u00b11}, A takes in a large sample S drawn from DXY, and either rejects S or accepts and produces a hypothesis h : Rd \u2192 {\u00b11} such that:\n\n1. (a) (Soundness.) With probability at least 1 \u2212 \u03b4 over the sample S the following is true:\n- If A accepts, then the output h satisfies $P(x,y)\u223cDXY[h(x) \\neq y] \\leq \\psi(opt(C, DXY)) + \\epsilon$, where\n$opt(C, DXY) = \\inf_{f\\in C} P(x,y)\u223cDXY[h(x) \\neq y]$.\n2. (b) (Completeness.) Whenever the marginal of DXY lies within D, A accepts with probability at least 1 \u2212 \u03b4 over the sample S.\n\nIn this terminology, the original definition of testable learning reduces to the special case where D = {D\u2217}. We stress that while the prior work of [GKK23] allowed D\u2217 to be, say, any fixed strongly log-concave distribution, their tester-learners are still tailored to the particular D\u2217 that is selected. This is because their tests rely on checking that the unknown distribution closely matches moments with D\u2217. By contrast, a universal tester-learner must accept all marginals in a family D.\n\nOur main contribution in this paper is the first universal tester-learner for the class of half-spaces with respect to a broad family of structured continuous distributions. This family is the set of all distributions with bounded Poincar\u00e9 constant (see Definition 2.4) and some mild concentration and anti-concentration properties (see Definition 2.1). It captures all strongly log-concave distributions, and in fact, under the well-known Kannan\u2013Lov\u00e1sz\u2013Simonovits (KLS) conjecture (see Conjecture 2.6), it captures all log-concave distributions as well. Our universal tester-learner significantly generalizes the main result of [GKSV23], who showed comparable guarantees only for the case where the target marginal is the standard Gaussian.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Testable Learning", "md": "# Testable Learning"}, {"type": "heading", "lvl": 1, "value": "Introduction", "md": "# Introduction"}, {"type": "text", "value": "In this paper we study the recent model of testable learning, due to Rubinfeld and Vasilyan [RV23]. Testable learning addresses a key issue with essentially all known algorithms for the basic problem of agnostic learning, in which a learner is required to produce a hypothesis competitive with the best-fitting hypothesis in a concept class C. The issue is that these algorithms make distributional assumptions (such as Gaussianity or log-concavity) that are in general hard to verify. This means that in the absence of any prior information about the distribution or the optimal achievable error, it can be hard to check that the learner has even succeeded at meeting its guarantee.\n\nIn the testable learning model, the learning algorithm, or tester-learner, is given access to labeled examples from an unknown distribution and may either reject or accept the unknown distribution. If it accepts, it must successfully produce a near-optimal hypothesis. Moreover, it is also required to accept whenever the unknown distribution truly has a certain target marginal D\u2217. Work of [RV23, GKK23, GKSV23, DKK+23] provided tester-learners for a range of basic classes (including halfspaces, intersections of halfspaces, and more) with respect to particular target marginals D\u2217 (such as the standard Gaussian). All of these algorithms, however, have the shortcoming that they are closely tailored to the particular target marginal D\u2217 that is chosen. Indeed, their tests would reject many well-behaved distributions that are appreciably far from D\u2217. A highly natural question from both a theoretical and a practical perspective is: can we design tester-learners that accept a wide class of distributions simultaneously, without being tailored to any particular one?\n\nIn this work we answer this question in the affirmative by introducing and studying universally testable learning. We formally define this model as follows.", "md": "In this paper we study the recent model of testable learning, due to Rubinfeld and Vasilyan [RV23]. Testable learning addresses a key issue with essentially all known algorithms for the basic problem of agnostic learning, in which a learner is required to produce a hypothesis competitive with the best-fitting hypothesis in a concept class C. The issue is that these algorithms make distributional assumptions (such as Gaussianity or log-concavity) that are in general hard to verify. This means that in the absence of any prior information about the distribution or the optimal achievable error, it can be hard to check that the learner has even succeeded at meeting its guarantee.\n\nIn the testable learning model, the learning algorithm, or tester-learner, is given access to labeled examples from an unknown distribution and may either reject or accept the unknown distribution. If it accepts, it must successfully produce a near-optimal hypothesis. Moreover, it is also required to accept whenever the unknown distribution truly has a certain target marginal D\u2217. Work of [RV23, GKK23, GKSV23, DKK+23] provided tester-learners for a range of basic classes (including halfspaces, intersections of halfspaces, and more) with respect to particular target marginals D\u2217 (such as the standard Gaussian). All of these algorithms, however, have the shortcoming that they are closely tailored to the particular target marginal D\u2217 that is chosen. Indeed, their tests would reject many well-behaved distributions that are appreciably far from D\u2217. A highly natural question from both a theoretical and a practical perspective is: can we design tester-learners that accept a wide class of distributions simultaneously, without being tailored to any particular one?\n\nIn this work we answer this question in the affirmative by introducing and studying universally testable learning. We formally define this model as follows."}, {"type": "heading", "lvl": 2, "value": "Definition 1.1 (Universally Testable Learning)", "md": "## Definition 1.1 (Universally Testable Learning)"}, {"type": "text", "value": "Let C be a concept class mapping Rd to {\u00b11}. Let D be a family of distributions over Rd. Let \u03f5, \u03b4 > 0 be parameters, and let \u03c8 : [0, 1] \u2192 [0, 1] be some function. We say C can be universally testably learned w.r.t. D up to error \u03c8(opt) + \u03f5 with failure probability \u03b4 if there exists a tester-learner A meeting the following specification. For any distribution DXY on Rd \u00d7 {\u00b11}, A takes in a large sample S drawn from DXY, and either rejects S or accepts and produces a hypothesis h : Rd \u2192 {\u00b11} such that:\n\n1. (a) (Soundness.) With probability at least 1 \u2212 \u03b4 over the sample S the following is true:\n- If A accepts, then the output h satisfies $P(x,y)\u223cDXY[h(x) \\neq y] \\leq \\psi(opt(C, DXY)) + \\epsilon$, where\n$opt(C, DXY) = \\inf_{f\\in C} P(x,y)\u223cDXY[h(x) \\neq y]$.\n2. (b) (Completeness.) Whenever the marginal of DXY lies within D, A accepts with probability at least 1 \u2212 \u03b4 over the sample S.\n\nIn this terminology, the original definition of testable learning reduces to the special case where D = {D\u2217}. We stress that while the prior work of [GKK23] allowed D\u2217 to be, say, any fixed strongly log-concave distribution, their tester-learners are still tailored to the particular D\u2217 that is selected. This is because their tests rely on checking that the unknown distribution closely matches moments with D\u2217. By contrast, a universal tester-learner must accept all marginals in a family D.\n\nOur main contribution in this paper is the first universal tester-learner for the class of half-spaces with respect to a broad family of structured continuous distributions. This family is the set of all distributions with bounded Poincar\u00e9 constant (see Definition 2.4) and some mild concentration and anti-concentration properties (see Definition 2.1). It captures all strongly log-concave distributions, and in fact, under the well-known Kannan\u2013Lov\u00e1sz\u2013Simonovits (KLS) conjecture (see Conjecture 2.6), it captures all log-concave distributions as well. Our universal tester-learner significantly generalizes the main result of [GKSV23], who showed comparable guarantees only for the case where the target marginal is the standard Gaussian.", "md": "Let C be a concept class mapping Rd to {\u00b11}. Let D be a family of distributions over Rd. Let \u03f5, \u03b4 > 0 be parameters, and let \u03c8 : [0, 1] \u2192 [0, 1] be some function. We say C can be universally testably learned w.r.t. D up to error \u03c8(opt) + \u03f5 with failure probability \u03b4 if there exists a tester-learner A meeting the following specification. For any distribution DXY on Rd \u00d7 {\u00b11}, A takes in a large sample S drawn from DXY, and either rejects S or accepts and produces a hypothesis h : Rd \u2192 {\u00b11} such that:\n\n1. (a) (Soundness.) With probability at least 1 \u2212 \u03b4 over the sample S the following is true:\n- If A accepts, then the output h satisfies $P(x,y)\u223cDXY[h(x) \\neq y] \\leq \\psi(opt(C, DXY)) + \\epsilon$, where\n$opt(C, DXY) = \\inf_{f\\in C} P(x,y)\u223cDXY[h(x) \\neq y]$.\n2. (b) (Completeness.) Whenever the marginal of DXY lies within D, A accepts with probability at least 1 \u2212 \u03b4 over the sample S.\n\nIn this terminology, the original definition of testable learning reduces to the special case where D = {D\u2217}. We stress that while the prior work of [GKK23] allowed D\u2217 to be, say, any fixed strongly log-concave distribution, their tester-learners are still tailored to the particular D\u2217 that is selected. This is because their tests rely on checking that the unknown distribution closely matches moments with D\u2217. By contrast, a universal tester-learner must accept all marginals in a family D.\n\nOur main contribution in this paper is the first universal tester-learner for the class of half-spaces with respect to a broad family of structured continuous distributions. This family is the set of all distributions with bounded Poincar\u00e9 constant (see Definition 2.4) and some mild concentration and anti-concentration properties (see Definition 2.1). It captures all strongly log-concave distributions, and in fact, under the well-known Kannan\u2013Lov\u00e1sz\u2013Simonovits (KLS) conjecture (see Conjecture 2.6), it captures all log-concave distributions as well. Our universal tester-learner significantly generalizes the main result of [GKSV23], who showed comparable guarantees only for the case where the target marginal is the standard Gaussian."}]}, {"page": 3, "text": "Theorem 1.2 (Universal Tester-Learner for Halfspaces; formally stated as Theorem 4.1). Let C be\nthe class of origin-centered halfspaces over Rd. Let D be the class of \u0398(1)-nice and \u0398(1)-Poincar\u00b4e\ndistributions (see Definitions 2.1 and 2.4), which includes all isotropic strongly log-concave and,\nunder KLS, all isotropic log-concave distributions. Then C can be universally testably learned w.r.t.\nD up to error O(opt) + \u03f5 in poly(d, 1\u03f5) time and sample complexity.\n    A special and well-studied case of interest is when the label noise follows the Massart model,\ni.e. the label of every example is flipped by an adversary with probability at most \u03b7. In this case\nwe are able to handle a considerably larger class D while also providing a stronger guarantee.\nTheorem 1.3 (Universal Tester-Learner for Massart Halfspaces; formally stated as Theorem 4.1).\nLet C be the class of origin-centered halfspaces over Rd. Let D be the class of poly(d)-nice and\npoly(d)-Poincar\u00b4e distributions, which includes all isotropic log-concave distributions (uncondition-\nally). Suppose the label noise follows the Massart model with noise rate at most \u03b7 < 1       1           2. Then C\ncan be universally testably learned w.r.t. D up to error opt + \u03f5 in poly(d, 1\u03f5,            1\u22122\u03b7) time and sample\ncomplexity.\nTechnical Overview            We first describe the key reasons why prior tester-learners were tailored\nto a specific target D\u2217.       All known polynomial-time algorithms for agnostically learning halfs-\npaces up to error O(opt) + \u03f5 require some concentration and anti-concentration properties from\nthe input marginal distribution (encapsulated e.g. in Definition 2.1). While concentration is rel-\natively straightforward to check (e.g. by checking that the moments do not grow at too fast a\nrate), the key challenge in designing tester-learners for halfspaces is to check anti-concentration.\nAll prior tester-learners [RV23, GKK23, GKSV23, DKK+23] use the heavy machinery of moment-\nmatching to achieve this. This approach relies on establishing structural properties of the following\ntype: if D\u2217    is a well-behaved distribution (e.g. a strongly log-concave distribution), and D ap-\nproximately matches D\u2217         in its low-degree moments, then D is also well-behaved (in particular,\nanti-concentrated). A canonical statement of such a property is the main pseudorandomness result\nof [GKK23] (see Theorem 5.6 therein), which establishes that approximate moment-matching fools\nfunctions of a constant number of halfspaces. Applying this property inherently requires comparing\nthe low-degree moments of D with those of D\u2217. Such tests do (implicitly) succeed universally for\nthe class of all distributions that match low-degree moments with D\u2217                  (e.g., if D\u2217   is the uniform\ndistribution over the hypercube, moment matching would accept all k-wise independent distribu-\ntions). Definition 1.1, however, seeks a far broader kind of universality. Our tests are not tailored\nto a single target in any way, and are intended to succeed over practical classes of distributions\nthat are commonly considered in learning theory (e.g., log-concave distributions).1\n    We overcome this hurdle and design a conceptually simple way of checking anti-concentration\nwithout requiring the hammer of moment-matching. Our approach follows and improves on the\nroadmap used by [GKSV23] to design efficient tester-learners for halfspaces using non-convex SGD\n(building on [DKTZ20a, DKTZ20b]). Let us outline this approach at a high level (a more detailed\ntechnical overview may be found in [GKSV23, Sec 1.2]). The tester-learner first computes a sta-\ntionary point w of a certain smooth version of the ramp loss, a surrogate for the 0-1 loss. Let w\u2217\nbe any solution achieving 0-1 error opt. The tester-learner now checks distributional properties of\nthe1unknown marginal D that ensure that w is close in angular distance to w\u2217                     (specifically, they\n    One may wonder if it is possible to test whether the low-degree moments of the input marginal D match any\ndistribution in a family D (e.g., all strongly log-concave distributions) without directly comparing to a specific D\u2217.\nThis is a reduction to testing whether a given (approximate) low-degree moment tensor lies within a large set of\ntarget low-degree moment tensors, and would indeed suffice for universally testable learning. This general problem,\nhowever, seems highly challenging to solve directly.\n                                                          3", "md": "# Math Equations and Text\n\n## Theorem 1.2\n\nLet C be the class of origin-centered halfspaces over \\( \\mathbb{R}^d \\). Let D be the class of \\( \\Theta(1) \\)-nice and \\( \\Theta(1) \\)-Poincar\u00e9 distributions (see Definitions 2.1 and 2.4), which includes all isotropic strongly log-concave and, under KLS, all isotropic log-concave distributions. Then C can be universally testably learned w.r.t. D up to error \\( O(opt) + \\epsilon \\) in poly(d, \\( \\frac{1}{\\epsilon} \\)) time and sample complexity.\n\n## Theorem 1.3\n\nLet C be the class of origin-centered halfspaces over \\( \\mathbb{R}^d \\). Let D be the class of poly(d)-nice and poly(d)-Poincar\u00e9 distributions, which includes all isotropic log-concave distributions (unconditionally). Suppose the label noise follows the Massart model with noise rate at most \\( \\eta < \\frac{1}{2} \\). Then C can be universally testably learned w.r.t. D up to error opt + \\( \\epsilon \\) in poly(d, \\( \\frac{1}{\\epsilon}, 1-2\\eta \\)) time and sample complexity.\n\n## Technical Overview\n\nWe first describe the key reasons why prior tester-learners were tailored to a specific target \\( D^* \\). All known polynomial-time algorithms for agnostically learning halfspaces up to error \\( O(opt) + \\epsilon \\) require some concentration and anti-concentration properties from the input marginal distribution (encapsulated e.g. in Definition 2.1). While concentration is relatively straightforward to check (e.g. by checking that the moments do not grow at too fast a rate), the key challenge in designing tester-learners for halfspaces is to check anti-concentration. All prior tester-learners [RV23, GKK23, GKSV23, DKK+23] use the heavy machinery of moment-matching to achieve this. This approach relies on establishing structural properties of the following type: if \\( D^* \\) is a well-behaved distribution (e.g. a strongly log-concave distribution), and D approximately matches \\( D^* \\) in its low-degree moments, then D is also well-behaved (in particular, anti-concentrated). A canonical statement of such a property is the main pseudorandomness result of [GKK23] (see Theorem 5.6 therein), which establishes that approximate moment-matching fools functions of a constant number of halfspaces. Applying this property inherently requires comparing the low-degree moments of D with those of \\( D^* \\). Such tests do (implicitly) succeed universally for the class of all distributions that match low-degree moments with \\( D^* \\) (e.g., if \\( D^* \\) is the uniform distribution over the hypercube, moment matching would accept all k-wise independent distributions). Definition 1.1, however, seeks a far broader kind of universality. Our tests are not tailored to a single target in any way, and are intended to succeed over practical classes of distributions that are commonly considered in learning theory (e.g., log-concave distributions).\n\nWe overcome this hurdle and design a conceptually simple way of checking anti-concentration without requiring the hammer of moment-matching. Our approach follows and improves on the roadmap used by [GKSV23] to design efficient tester-learners for halfspaces using non-convex SGD (building on [DKTZ20a, DKTZ20b]). Let us outline this approach at a high level (a more detailed technical overview may be found in [GKSV23, Sec 1.2]). The tester-learner first computes a stationary point w of a certain smooth version of the ramp loss, a surrogate for the 0-1 loss. Let \\( w^* \\) be any solution achieving 0-1 error opt. The tester-learner now checks distributional properties of the unknown marginal D that ensure that w is close in angular distance to \\( w^* \\) (specifically, they", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Theorem 1.2", "md": "## Theorem 1.2"}, {"type": "text", "value": "Let C be the class of origin-centered halfspaces over \\( \\mathbb{R}^d \\). Let D be the class of \\( \\Theta(1) \\)-nice and \\( \\Theta(1) \\)-Poincar\u00e9 distributions (see Definitions 2.1 and 2.4), which includes all isotropic strongly log-concave and, under KLS, all isotropic log-concave distributions. Then C can be universally testably learned w.r.t. D up to error \\( O(opt) + \\epsilon \\) in poly(d, \\( \\frac{1}{\\epsilon} \\)) time and sample complexity.", "md": "Let C be the class of origin-centered halfspaces over \\( \\mathbb{R}^d \\). Let D be the class of \\( \\Theta(1) \\)-nice and \\( \\Theta(1) \\)-Poincar\u00e9 distributions (see Definitions 2.1 and 2.4), which includes all isotropic strongly log-concave and, under KLS, all isotropic log-concave distributions. Then C can be universally testably learned w.r.t. D up to error \\( O(opt) + \\epsilon \\) in poly(d, \\( \\frac{1}{\\epsilon} \\)) time and sample complexity."}, {"type": "heading", "lvl": 2, "value": "Theorem 1.3", "md": "## Theorem 1.3"}, {"type": "text", "value": "Let C be the class of origin-centered halfspaces over \\( \\mathbb{R}^d \\). Let D be the class of poly(d)-nice and poly(d)-Poincar\u00e9 distributions, which includes all isotropic log-concave distributions (unconditionally). Suppose the label noise follows the Massart model with noise rate at most \\( \\eta < \\frac{1}{2} \\). Then C can be universally testably learned w.r.t. D up to error opt + \\( \\epsilon \\) in poly(d, \\( \\frac{1}{\\epsilon}, 1-2\\eta \\)) time and sample complexity.", "md": "Let C be the class of origin-centered halfspaces over \\( \\mathbb{R}^d \\). Let D be the class of poly(d)-nice and poly(d)-Poincar\u00e9 distributions, which includes all isotropic log-concave distributions (unconditionally). Suppose the label noise follows the Massart model with noise rate at most \\( \\eta < \\frac{1}{2} \\). Then C can be universally testably learned w.r.t. D up to error opt + \\( \\epsilon \\) in poly(d, \\( \\frac{1}{\\epsilon}, 1-2\\eta \\)) time and sample complexity."}, {"type": "heading", "lvl": 2, "value": "Technical Overview", "md": "## Technical Overview"}, {"type": "text", "value": "We first describe the key reasons why prior tester-learners were tailored to a specific target \\( D^* \\). All known polynomial-time algorithms for agnostically learning halfspaces up to error \\( O(opt) + \\epsilon \\) require some concentration and anti-concentration properties from the input marginal distribution (encapsulated e.g. in Definition 2.1). While concentration is relatively straightforward to check (e.g. by checking that the moments do not grow at too fast a rate), the key challenge in designing tester-learners for halfspaces is to check anti-concentration. All prior tester-learners [RV23, GKK23, GKSV23, DKK+23] use the heavy machinery of moment-matching to achieve this. This approach relies on establishing structural properties of the following type: if \\( D^* \\) is a well-behaved distribution (e.g. a strongly log-concave distribution), and D approximately matches \\( D^* \\) in its low-degree moments, then D is also well-behaved (in particular, anti-concentrated). A canonical statement of such a property is the main pseudorandomness result of [GKK23] (see Theorem 5.6 therein), which establishes that approximate moment-matching fools functions of a constant number of halfspaces. Applying this property inherently requires comparing the low-degree moments of D with those of \\( D^* \\). Such tests do (implicitly) succeed universally for the class of all distributions that match low-degree moments with \\( D^* \\) (e.g., if \\( D^* \\) is the uniform distribution over the hypercube, moment matching would accept all k-wise independent distributions). Definition 1.1, however, seeks a far broader kind of universality. Our tests are not tailored to a single target in any way, and are intended to succeed over practical classes of distributions that are commonly considered in learning theory (e.g., log-concave distributions).\n\nWe overcome this hurdle and design a conceptually simple way of checking anti-concentration without requiring the hammer of moment-matching. Our approach follows and improves on the roadmap used by [GKSV23] to design efficient tester-learners for halfspaces using non-convex SGD (building on [DKTZ20a, DKTZ20b]). Let us outline this approach at a high level (a more detailed technical overview may be found in [GKSV23, Sec 1.2]). The tester-learner first computes a stationary point w of a certain smooth version of the ramp loss, a surrogate for the 0-1 loss. Let \\( w^* \\) be any solution achieving 0-1 error opt. The tester-learner now checks distributional properties of the unknown marginal D that ensure that w is close in angular distance to \\( w^* \\) (specifically, they", "md": "We first describe the key reasons why prior tester-learners were tailored to a specific target \\( D^* \\). All known polynomial-time algorithms for agnostically learning halfspaces up to error \\( O(opt) + \\epsilon \\) require some concentration and anti-concentration properties from the input marginal distribution (encapsulated e.g. in Definition 2.1). While concentration is relatively straightforward to check (e.g. by checking that the moments do not grow at too fast a rate), the key challenge in designing tester-learners for halfspaces is to check anti-concentration. All prior tester-learners [RV23, GKK23, GKSV23, DKK+23] use the heavy machinery of moment-matching to achieve this. This approach relies on establishing structural properties of the following type: if \\( D^* \\) is a well-behaved distribution (e.g. a strongly log-concave distribution), and D approximately matches \\( D^* \\) in its low-degree moments, then D is also well-behaved (in particular, anti-concentrated). A canonical statement of such a property is the main pseudorandomness result of [GKK23] (see Theorem 5.6 therein), which establishes that approximate moment-matching fools functions of a constant number of halfspaces. Applying this property inherently requires comparing the low-degree moments of D with those of \\( D^* \\). Such tests do (implicitly) succeed universally for the class of all distributions that match low-degree moments with \\( D^* \\) (e.g., if \\( D^* \\) is the uniform distribution over the hypercube, moment matching would accept all k-wise independent distributions). Definition 1.1, however, seeks a far broader kind of universality. Our tests are not tailored to a single target in any way, and are intended to succeed over practical classes of distributions that are commonly considered in learning theory (e.g., log-concave distributions).\n\nWe overcome this hurdle and design a conceptually simple way of checking anti-concentration without requiring the hammer of moment-matching. Our approach follows and improves on the roadmap used by [GKSV23] to design efficient tester-learners for halfspaces using non-convex SGD (building on [DKTZ20a, DKTZ20b]). Let us outline this approach at a high level (a more detailed technical overview may be found in [GKSV23, Sec 1.2]). The tester-learner first computes a stationary point w of a certain smooth version of the ramp loss, a surrogate for the 0-1 loss. Let \\( w^* \\) be any solution achieving 0-1 error opt. The tester-learner now checks distributional properties of the unknown marginal D that ensure that w is close in angular distance to \\( w^* \\) (specifically, they"}]}, {"page": 4, "text": "ensure the contrapositive, namely that any w that has large gradient norm must have large angle\nwith w\u2217). By a more careful analysis of the gradient norm than in [GKSV23] (see Proposition 4.3),\nwe are able to reduce to showing the following weak anti-concentration property. Let v denote any\nunit vector orthogonal to w, and let DT denote D restricted to the band T = {x | |\u27e8w, x\u27e9| \u2264          \u03c3}\n(where the width \u03c3 is carefully selected according to certain constraints). Then the property we\nneed is that\n                                      P\n                                    x\u223cDT[|\u27e8v, x\u27e9| \u2265  \u0398(1)] \u2265  \u0398(1).\nOur key observation is that the classical Paley\u2013Zygmund inequality applied to the random variable\nZ = \u27e8v, x\u27e92, where x \u223c   DT , already gives us the following type of anti-concentration:\n                                     P   Z > E[Z]    \u2265  1\n                                                2       4 \u00b7 E[Z]2\n                                                           E[Z2].\nThis turns out to suffice for our purposes\u2014provided we can show a hypercontractivity property for\nZ, namely that E[Z2] \u2264    \u0398(1) E[Z]2 (as well as that E[Z] = \u0398(1), which is just a second moment\nconstraint).\n   Our main algorithmic idea is to use a sum-of-squares (SOS) program to check hypercontractivity\nof the random variable Z. To do so, we crucially leverage a result due to [KS17] stating that any\nD that has bounded Poincar\u00b4e constant is certifiably hypercontractive in the SOS framework (and\nit turns out this extends to DT as well). This means that we can run a certain polynomial-time\nsemidefinite program that checks hypercontractivity of Z over the sample, and whenever D is in\nfact Poincar\u00b4e, we are guaranteed that the test will pass with high probability (see Proposition 3.5).\nThis is sufficient to ensure that the stationary point w we have computed is indeed close in angular\ndistance to w\u2217.\n   In order to finally arrive at our main results, we need to run further tests which ensure that the\ndisagreement between our computed w and any (unknown) optimum w\u2217              is bounded by the angle\nbetween them, i.e., Px\u223cD[sign(\u27e8w, x\u27e9    \u0338= sign(\u27e8w\u2217, x\u27e9)] \u2264  O(\u2221(w, w\u2217)) (see Lemma 3.1). This in\nturn guarantees that w has error O(opt) + \u03f5. We stress that while [GKSV23] introduced similar\ntesters for the special case of Gaussian marginals, our tests succeed universally with respect to a\nbroad family of distributions including some heavy-tailed distributions (see Definition 2.1). From a\ntechnical perspective, prior to our work, such tests either produced a suboptimal bound, or required\nestimating the operator norms of a polynomial number of random matrices formed using rejection\nsampling. We significantly simplify this approach by showing that it is sufficient to estimate the\noperator norm of a single random matrix. Finally, to obtain our improved results for the Massart\nsetting, it turns out that the proof admits certain simplifications that guarantee final error opt + \u03f5\nwhile also allowing a wider range of Poincar\u00b4e distributions.\nRelated Work       There is a large body of work on agnostic learning algorithms for halfspaces\nthat run in fully polynomial time.      We briefly mention only those that are most closely rele-\nvant to our work; please see [BH21] for a survey as well as [GKSV23, Sec 1.1] for further re-\nlated work. Following a long line of work on distribution-specific agnostic learners for halfspaces\n[KLS09, ABL17, Dan15, BZ17, YZ17, Zha18, ZSA20, ZL21], the work of [DKTZ20a] introduced a\nparticularly simple approach for the Massart setting, based solely on non-convex SGD. This work,\nwhich sets the template that our approach also follows, achieved the information-theoretically\noptimal error of opt + \u03f5 for origin-centered Massart halfspaces over a wide range of structured\ndistributions (and was later extended to general halfspaces by [DKK+22]). The non-convex SGD\napproach was then generalized by [DKTZ20b] to show an O(opt)+\u03f5 guarantee for the fully agnostic\nsetting. Other related work includes [DKS18, DKTZ22].\n                                                   4", "md": "Ensure the contrapositive, namely that any *w that has large gradient norm must have large angle with w*). By a more careful analysis of the gradient norm than in [GKSV23] (see Proposition 4.3), we are able to reduce to showing the following weak anti-concentration property. Let v denote any unit vector orthogonal to w, and let DT denote D restricted to the band T = {x | |\u27e8w, x*\u27e9| \u2264 \u03c3} (where the width \u03c3 is carefully selected according to certain constraints). Then the property we need is that\n\n$$\nP_{x\\sim DT}[|\u27e8v, x\u27e9| \\geq \\Theta(1)] \\geq \\Theta(1).\n$$\nOur key observation is that the classical Paley\u2013Zygmund inequality applied to the random variable *Z = \u27e8v, x\u27e92, where x \u223c DT*, already gives us the following type of anti-concentration:\n\n$$\nP(Z > E[Z]) \\geq \\frac{1}{2} \\geq \\frac{4 \\cdot E[Z]^2}{E[Z^2]}.\n$$\nThis turns out to suffice for our purposes\u2014provided we can show a hypercontractivity property for *Z*, namely that E[Z2] \u2264 \u0398(1) E[Z]2 (as well as that E[Z] = \u0398(1), which is just a second moment constraint).\n\nOur main algorithmic idea is to use a sum-of-squares (SOS) program to check hypercontractivity of the random variable *Z. To do so, we crucially leverage a result due to [KS17] stating that any D that has bounded Poincar\u00e9 constant is certifiably hypercontractive in the SOS framework (and it turns out this extends to DT as well). This means that we can run a certain polynomial-time semidefinite program that checks hypercontractivity of Z over the sample, and whenever D is in fact Poincar\u00e9, we are guaranteed that the test will pass with high probability (see Proposition 3.5). This is sufficient to ensure that the stationary point w we have computed is indeed close in angular distance to w**.\n\nIn order to finally arrive at our main results, we need to run further tests which ensure that the disagreement between our computed *w and any (unknown) optimum w* is bounded by the angle between them, i.e., Px\u223cD[sign(\u27e8w, x\u27e9 \u2260 sign(\u27e8w*, x\u27e9)] \u2264 O(\u2221(w, w*)) (see Lemma 3.1). This in turn guarantees that w* has error O(opt) + \u03f5. We stress that while [GKSV23] introduced similar testers for the special case of Gaussian marginals, our tests succeed universally with respect to a broad family of distributions including some heavy-tailed distributions (see Definition 2.1). From a technical perspective, prior to our work, such tests either produced a suboptimal bound, or required estimating the operator norms of a polynomial number of random matrices formed using rejection sampling. We significantly simplify this approach by showing that it is sufficient to estimate the operator norm of a single random matrix. Finally, to obtain our improved results for the Massart setting, it turns out that the proof admits certain simplifications that guarantee final error opt + \u03f5 while also allowing a wider range of Poincar\u00e9 distributions.\n\nRelated Work There is a large body of work on agnostic learning algorithms for halfspaces that run in fully polynomial time. We briefly mention only those that are most closely relevant to our work; please see [BH21] for a survey as well as [GKSV23, Sec 1.1] for further related work. Following a long line of work on distribution-specific agnostic learners for halfspaces [KLS09, ABL17, Dan15, BZ17, YZ17, Zha18, ZSA20, ZL21], the work of [DKTZ20a] introduced a particularly simple approach for the Massart setting, based solely on non-convex SGD. This work, which sets the template that our approach also follows, achieved the information-theoretically optimal error of opt + \u03f5 for origin-centered Massart halfspaces over a wide range of structured distributions (and was later extended to general halfspaces by [DKK+22]). The non-convex SGD approach was then generalized by [DKTZ20b] to show an O(opt)+\u03f5 guarantee for the fully agnostic setting. Other related work includes [DKS18, DKTZ22].", "images": [], "items": [{"type": "text", "value": "Ensure the contrapositive, namely that any *w that has large gradient norm must have large angle with w*). By a more careful analysis of the gradient norm than in [GKSV23] (see Proposition 4.3), we are able to reduce to showing the following weak anti-concentration property. Let v denote any unit vector orthogonal to w, and let DT denote D restricted to the band T = {x | |\u27e8w, x*\u27e9| \u2264 \u03c3} (where the width \u03c3 is carefully selected according to certain constraints). Then the property we need is that\n\n$$\nP_{x\\sim DT}[|\u27e8v, x\u27e9| \\geq \\Theta(1)] \\geq \\Theta(1).\n$$\nOur key observation is that the classical Paley\u2013Zygmund inequality applied to the random variable *Z = \u27e8v, x\u27e92, where x \u223c DT*, already gives us the following type of anti-concentration:\n\n$$\nP(Z > E[Z]) \\geq \\frac{1}{2} \\geq \\frac{4 \\cdot E[Z]^2}{E[Z^2]}.\n$$\nThis turns out to suffice for our purposes\u2014provided we can show a hypercontractivity property for *Z*, namely that E[Z2] \u2264 \u0398(1) E[Z]2 (as well as that E[Z] = \u0398(1), which is just a second moment constraint).\n\nOur main algorithmic idea is to use a sum-of-squares (SOS) program to check hypercontractivity of the random variable *Z. To do so, we crucially leverage a result due to [KS17] stating that any D that has bounded Poincar\u00e9 constant is certifiably hypercontractive in the SOS framework (and it turns out this extends to DT as well). This means that we can run a certain polynomial-time semidefinite program that checks hypercontractivity of Z over the sample, and whenever D is in fact Poincar\u00e9, we are guaranteed that the test will pass with high probability (see Proposition 3.5). This is sufficient to ensure that the stationary point w we have computed is indeed close in angular distance to w**.\n\nIn order to finally arrive at our main results, we need to run further tests which ensure that the disagreement between our computed *w and any (unknown) optimum w* is bounded by the angle between them, i.e., Px\u223cD[sign(\u27e8w, x\u27e9 \u2260 sign(\u27e8w*, x\u27e9)] \u2264 O(\u2221(w, w*)) (see Lemma 3.1). This in turn guarantees that w* has error O(opt) + \u03f5. We stress that while [GKSV23] introduced similar testers for the special case of Gaussian marginals, our tests succeed universally with respect to a broad family of distributions including some heavy-tailed distributions (see Definition 2.1). From a technical perspective, prior to our work, such tests either produced a suboptimal bound, or required estimating the operator norms of a polynomial number of random matrices formed using rejection sampling. We significantly simplify this approach by showing that it is sufficient to estimate the operator norm of a single random matrix. Finally, to obtain our improved results for the Massart setting, it turns out that the proof admits certain simplifications that guarantee final error opt + \u03f5 while also allowing a wider range of Poincar\u00e9 distributions.\n\nRelated Work There is a large body of work on agnostic learning algorithms for halfspaces that run in fully polynomial time. We briefly mention only those that are most closely relevant to our work; please see [BH21] for a survey as well as [GKSV23, Sec 1.1] for further related work. Following a long line of work on distribution-specific agnostic learners for halfspaces [KLS09, ABL17, Dan15, BZ17, YZ17, Zha18, ZSA20, ZL21], the work of [DKTZ20a] introduced a particularly simple approach for the Massart setting, based solely on non-convex SGD. This work, which sets the template that our approach also follows, achieved the information-theoretically optimal error of opt + \u03f5 for origin-centered Massart halfspaces over a wide range of structured distributions (and was later extended to general halfspaces by [DKK+22]). The non-convex SGD approach was then generalized by [DKTZ20b] to show an O(opt)+\u03f5 guarantee for the fully agnostic setting. Other related work includes [DKS18, DKTZ22].", "md": "Ensure the contrapositive, namely that any *w that has large gradient norm must have large angle with w*). By a more careful analysis of the gradient norm than in [GKSV23] (see Proposition 4.3), we are able to reduce to showing the following weak anti-concentration property. Let v denote any unit vector orthogonal to w, and let DT denote D restricted to the band T = {x | |\u27e8w, x*\u27e9| \u2264 \u03c3} (where the width \u03c3 is carefully selected according to certain constraints). Then the property we need is that\n\n$$\nP_{x\\sim DT}[|\u27e8v, x\u27e9| \\geq \\Theta(1)] \\geq \\Theta(1).\n$$\nOur key observation is that the classical Paley\u2013Zygmund inequality applied to the random variable *Z = \u27e8v, x\u27e92, where x \u223c DT*, already gives us the following type of anti-concentration:\n\n$$\nP(Z > E[Z]) \\geq \\frac{1}{2} \\geq \\frac{4 \\cdot E[Z]^2}{E[Z^2]}.\n$$\nThis turns out to suffice for our purposes\u2014provided we can show a hypercontractivity property for *Z*, namely that E[Z2] \u2264 \u0398(1) E[Z]2 (as well as that E[Z] = \u0398(1), which is just a second moment constraint).\n\nOur main algorithmic idea is to use a sum-of-squares (SOS) program to check hypercontractivity of the random variable *Z. To do so, we crucially leverage a result due to [KS17] stating that any D that has bounded Poincar\u00e9 constant is certifiably hypercontractive in the SOS framework (and it turns out this extends to DT as well). This means that we can run a certain polynomial-time semidefinite program that checks hypercontractivity of Z over the sample, and whenever D is in fact Poincar\u00e9, we are guaranteed that the test will pass with high probability (see Proposition 3.5). This is sufficient to ensure that the stationary point w we have computed is indeed close in angular distance to w**.\n\nIn order to finally arrive at our main results, we need to run further tests which ensure that the disagreement between our computed *w and any (unknown) optimum w* is bounded by the angle between them, i.e., Px\u223cD[sign(\u27e8w, x\u27e9 \u2260 sign(\u27e8w*, x\u27e9)] \u2264 O(\u2221(w, w*)) (see Lemma 3.1). This in turn guarantees that w* has error O(opt) + \u03f5. We stress that while [GKSV23] introduced similar testers for the special case of Gaussian marginals, our tests succeed universally with respect to a broad family of distributions including some heavy-tailed distributions (see Definition 2.1). From a technical perspective, prior to our work, such tests either produced a suboptimal bound, or required estimating the operator norms of a polynomial number of random matrices formed using rejection sampling. We significantly simplify this approach by showing that it is sufficient to estimate the operator norm of a single random matrix. Finally, to obtain our improved results for the Massart setting, it turns out that the proof admits certain simplifications that guarantee final error opt + \u03f5 while also allowing a wider range of Poincar\u00e9 distributions.\n\nRelated Work There is a large body of work on agnostic learning algorithms for halfspaces that run in fully polynomial time. We briefly mention only those that are most closely relevant to our work; please see [BH21] for a survey as well as [GKSV23, Sec 1.1] for further related work. Following a long line of work on distribution-specific agnostic learners for halfspaces [KLS09, ABL17, Dan15, BZ17, YZ17, Zha18, ZSA20, ZL21], the work of [DKTZ20a] introduced a particularly simple approach for the Massart setting, based solely on non-convex SGD. This work, which sets the template that our approach also follows, achieved the information-theoretically optimal error of opt + \u03f5 for origin-centered Massart halfspaces over a wide range of structured distributions (and was later extended to general halfspaces by [DKK+22]). The non-convex SGD approach was then generalized by [DKTZ20b] to show an O(opt)+\u03f5 guarantee for the fully agnostic setting. Other related work includes [DKS18, DKTZ22]."}]}, {"page": 5, "text": "     The testable learning model was introduced by the work of [RV23], who showed a tester-learner\nfor halfspaces achieving error opt + \u03f5 in time d                    O(1/\u03f54) for the case where the target marginal is\nGaussian. Subsequently, [GKK23] provided a general algorithmic framework based on moment-\nmatching for this problem, and showed a tester-learner for halfspaces only requiring time d                                     O(1/\u03f52)\nwith respect to any fixed strongly log-concave marginal (matching known lower bounds for ordinary\nagnostic learning over Gaussian marginals [GGK20, DKZ20, DKPZ21, DKR23]).\n     The most closely relevant work to the present one is that of [GKSV23] (see also [DKK+23]),\nwho showed fully polynomial-time tester-learners for halfspaces achieving error O(opt) + \u03f5 in the\nagnostic setting and opt + \u03f5 in the Massart setting for the case where the target marginal is the\nGaussian. As detailed in the technical overview, their tests rely crucially on moment-matching and\nare tailored to a specific target marginal. By contrast, our tests check hypercontractivity using an\nSOS program and succeed universally for a wide class of certifiably hypercontractive distributions.\n     Certifying distributional properties such as hypercontractivity is an important aspect of a large\nbody of work on robust algorithmic statistics using the SOS framework. We will not attempt to\nsummarize this literature here and direct the reader to [KS17, BK21] for overviews of related work,\nas well as to [FKP+19] for a textbook treatment. The notion of certifiable anti-concentration has\nalso been studied (see e.g. [KKK19, RY20, BK21]), but it turns out not to be directly useful for\nour purposes as it is only known to hold for distributions satisfying very strong conditions such as\nrotational symmetry.\n2       Preliminaries\nNotation and Terminology                      For what follows, we consider DXY to be an unknown joint distri-\nbution over X \u00d7Y from which we receive independent samples, and its marginal on X will be denoted\nby DX . In particular X = Rd, and labels will lie in Y = {\u00b11}. We will use C to denote a concept\nclass mapping Rd to {\u00b11}, which throughout this paper will be the class of halfspaces or functions\nof halfspaces over Rd. We use opt(C, DXY) to denote the optimal error inff\u2208C P(x,y)\u223cDXY[f(x) \u0338= y],\nor just opt when C and DXY are clear from context. We recall that in Massart noise model, the\nlabels satisfy Py\u223cDXY|x[y \u0338= sign(\u27e8w\u2217, x\u27e9) | x] = \u03b7(x), with \u03b7(x) \u2264                          \u03b7 < 1  2 for all x. When we have\nadversarial noise (i.e., when we are in the agnostic model), the labels can be completely arbitrary.\nIn both cases, the goal is to produce a hypothesis whose error is competitive with opt. We use E to\ndenote the expectation of a random variable in brackets (or, correspondingly, P for the probability\nof an event), either over the unknown joint distribution or over the empirical distribution with\nrespect to a sample S (e.g., EZ\u2208S[f(Z)] =                    1     Z\u2208S f(Z)).\nDefinitions and Distributional Assumptions                  |S|           For the problem of learning halfspaces in the\nagnostic and in Massart noise models, any of the known polynomial algorithms that achieve com-\nputationally optimal guarantees require that the marginal distribution has at least the following\nnice properties previously defined by, e.g., [DKTZ20b].\nDefinition 2.1 (Nice Distributions). For a given constant \u03bb \u2265                              1, we consider the class of \u03bb-nice\ndistributions over Rd to be the distributions that satisfy the following properties:\n    1. For any unit vector v in Rd the distribution satisfies E[\u27e8v, x\u27e92] \u2208                         [ 1\n                                                                                                     \u03bb, \u03bb]. (bounded spectrum)\n    2. For any two dimensional subspace V , the corresponding marginal density qV (x) satisfies\n        qV (x) \u2265     1/\u03bb for any \u2225x\u22252 \u2264         1/\u03bb.                                                   (anti-anti-concentration)\n                                                                    5", "md": "The testable learning model was introduced by the work of [RV23], who showed a tester-learner for halfspaces achieving error opt + \u03f5 in time $$d = O\\left(\\frac{1}{\\epsilon^4}\\right)$$ for the case where the target marginal is Gaussian. Subsequently, [GKK23] provided a general algorithmic framework based on moment-matching for this problem, and showed a tester-learner for halfspaces only requiring time $$d = O\\left(\\frac{1}{\\epsilon^2}\\right)$$ with respect to any fixed strongly log-concave marginal (matching known lower bounds for ordinary agnostic learning over Gaussian marginals [GGK20, DKZ20, DKPZ21, DKR23]).\n\nThe most closely relevant work to the present one is that of [GKSV23] (see also [DKK+23]), who showed fully polynomial-time tester-learners for halfspaces achieving error $$O(opt) + \\epsilon$$ in the agnostic setting and opt + \u03f5 in the Massart setting for the case where the target marginal is the Gaussian. As detailed in the technical overview, their tests rely crucially on moment-matching and are tailored to a specific target marginal. By contrast, our tests check hypercontractivity using an SOS program and succeed universally for a wide class of certifiably hypercontractive distributions.\n\nCertifying distributional properties such as hypercontractivity is an important aspect of a large body of work on robust algorithmic statistics using the SOS framework. We will not attempt to summarize this literature here and direct the reader to [KS17, BK21] for overviews of related work, as well as to [FKP+19] for a textbook treatment. The notion of certifiable anti-concentration has also been studied (see e.g. [KKK19, RY20, BK21]), but it turns out not to be directly useful for our purposes as it is only known to hold for distributions satisfying very strong conditions such as rotational symmetry.\n\n## Preliminaries\n### Notation and Terminology\nFor what follows, we consider $$D_{XY}$$ to be an unknown joint distribution over $$X \\times Y$$ from which we receive independent samples, and its marginal on $$X$$ will be denoted by $$D_X$$. In particular $$X = \\mathbb{R}^d$$, and labels will lie in $$Y = \\{ \\pm 1 \\}$$. We will use $$\\mathcal{C}$$ to denote a concept class mapping $$\\mathbb{R}^d$$ to $$\\{ \\pm 1 \\}$$, which throughout this paper will be the class of halfspaces or functions of halfspaces over $$\\mathbb{R}^d$$. We use $$\\text{opt}(\\mathcal{C}, D_{XY})$$ to denote the optimal error $$\\inf_{f \\in \\mathcal{C}} \\mathbb{P}(x,y) \\sim D_{XY}[f(x) \\neq y]$$, or just opt when $$\\mathcal{C}$$ and $$D_{XY}$$ are clear from context. We recall that in Massart noise model, the labels satisfy $$\\mathbb{P}(y \\sim D_{XY} | x)[y \\neq \\text{sign}(\\langle w^*, x \\rangle) | x] = \\eta(x)$$, with $$\\eta(x) \\leq \\eta < \\frac{1}{2}$$ for all $$x$$. When we have adversarial noise (i.e., when we are in the agnostic model), the labels can be completely arbitrary. In both cases, the goal is to produce a hypothesis whose error is competitive with opt. We use $$\\mathbb{E}$$ to denote the expectation of a random variable in brackets (or, correspondingly, $$\\mathbb{P}$$ for the probability of an event), either over the unknown joint distribution or over the empirical distribution with respect to a sample $$S$$ (e.g., $$\\mathbb{E}_{Z \\in S}[f(Z)] = \\frac{1}{|S|} \\sum_{Z \\in S} f(Z)$$).\n\n### Definitions and Distributional Assumptions\nFor the problem of learning halfspaces in the agnostic and in Massart noise models, any of the known polynomial algorithms that achieve computationally optimal guarantees require that the marginal distribution has at least the following nice properties previously defined by, e.g., [DKTZ20b].\n\n**Definition 2.1 (Nice Distributions)**. For a given constant $$\\lambda \\geq 1$$, we consider the class of $$\\lambda$$-nice distributions over $$\\mathbb{R}^d$$ to be the distributions that satisfy the following properties:\n1. For any unit vector $$v$$ in $$\\mathbb{R}^d$$ the distribution satisfies $$\\mathbb{E}[\\langle v, x \\rangle^2] \\in [\\frac{1}{\\lambda}, \\lambda]$$. (bounded spectrum)\n2. For any two-dimensional subspace $$V$$, the corresponding marginal density $$q_V(x)$$ satisfies $$q_V(x) \\geq \\frac{1}{\\lambda}$$ for any $$\\|x\\|_2 \\leq \\frac{1}{\\lambda}$$. (anti-anti-concentration)", "images": [], "items": [{"type": "text", "value": "The testable learning model was introduced by the work of [RV23], who showed a tester-learner for halfspaces achieving error opt + \u03f5 in time $$d = O\\left(\\frac{1}{\\epsilon^4}\\right)$$ for the case where the target marginal is Gaussian. Subsequently, [GKK23] provided a general algorithmic framework based on moment-matching for this problem, and showed a tester-learner for halfspaces only requiring time $$d = O\\left(\\frac{1}{\\epsilon^2}\\right)$$ with respect to any fixed strongly log-concave marginal (matching known lower bounds for ordinary agnostic learning over Gaussian marginals [GGK20, DKZ20, DKPZ21, DKR23]).\n\nThe most closely relevant work to the present one is that of [GKSV23] (see also [DKK+23]), who showed fully polynomial-time tester-learners for halfspaces achieving error $$O(opt) + \\epsilon$$ in the agnostic setting and opt + \u03f5 in the Massart setting for the case where the target marginal is the Gaussian. As detailed in the technical overview, their tests rely crucially on moment-matching and are tailored to a specific target marginal. By contrast, our tests check hypercontractivity using an SOS program and succeed universally for a wide class of certifiably hypercontractive distributions.\n\nCertifying distributional properties such as hypercontractivity is an important aspect of a large body of work on robust algorithmic statistics using the SOS framework. We will not attempt to summarize this literature here and direct the reader to [KS17, BK21] for overviews of related work, as well as to [FKP+19] for a textbook treatment. The notion of certifiable anti-concentration has also been studied (see e.g. [KKK19, RY20, BK21]), but it turns out not to be directly useful for our purposes as it is only known to hold for distributions satisfying very strong conditions such as rotational symmetry.", "md": "The testable learning model was introduced by the work of [RV23], who showed a tester-learner for halfspaces achieving error opt + \u03f5 in time $$d = O\\left(\\frac{1}{\\epsilon^4}\\right)$$ for the case where the target marginal is Gaussian. Subsequently, [GKK23] provided a general algorithmic framework based on moment-matching for this problem, and showed a tester-learner for halfspaces only requiring time $$d = O\\left(\\frac{1}{\\epsilon^2}\\right)$$ with respect to any fixed strongly log-concave marginal (matching known lower bounds for ordinary agnostic learning over Gaussian marginals [GGK20, DKZ20, DKPZ21, DKR23]).\n\nThe most closely relevant work to the present one is that of [GKSV23] (see also [DKK+23]), who showed fully polynomial-time tester-learners for halfspaces achieving error $$O(opt) + \\epsilon$$ in the agnostic setting and opt + \u03f5 in the Massart setting for the case where the target marginal is the Gaussian. As detailed in the technical overview, their tests rely crucially on moment-matching and are tailored to a specific target marginal. By contrast, our tests check hypercontractivity using an SOS program and succeed universally for a wide class of certifiably hypercontractive distributions.\n\nCertifying distributional properties such as hypercontractivity is an important aspect of a large body of work on robust algorithmic statistics using the SOS framework. We will not attempt to summarize this literature here and direct the reader to [KS17, BK21] for overviews of related work, as well as to [FKP+19] for a textbook treatment. The notion of certifiable anti-concentration has also been studied (see e.g. [KKK19, RY20, BK21]), but it turns out not to be directly useful for our purposes as it is only known to hold for distributions satisfying very strong conditions such as rotational symmetry."}, {"type": "heading", "lvl": 2, "value": "Preliminaries", "md": "## Preliminaries"}, {"type": "heading", "lvl": 3, "value": "Notation and Terminology", "md": "### Notation and Terminology"}, {"type": "text", "value": "For what follows, we consider $$D_{XY}$$ to be an unknown joint distribution over $$X \\times Y$$ from which we receive independent samples, and its marginal on $$X$$ will be denoted by $$D_X$$. In particular $$X = \\mathbb{R}^d$$, and labels will lie in $$Y = \\{ \\pm 1 \\}$$. We will use $$\\mathcal{C}$$ to denote a concept class mapping $$\\mathbb{R}^d$$ to $$\\{ \\pm 1 \\}$$, which throughout this paper will be the class of halfspaces or functions of halfspaces over $$\\mathbb{R}^d$$. We use $$\\text{opt}(\\mathcal{C}, D_{XY})$$ to denote the optimal error $$\\inf_{f \\in \\mathcal{C}} \\mathbb{P}(x,y) \\sim D_{XY}[f(x) \\neq y]$$, or just opt when $$\\mathcal{C}$$ and $$D_{XY}$$ are clear from context. We recall that in Massart noise model, the labels satisfy $$\\mathbb{P}(y \\sim D_{XY} | x)[y \\neq \\text{sign}(\\langle w^*, x \\rangle) | x] = \\eta(x)$$, with $$\\eta(x) \\leq \\eta < \\frac{1}{2}$$ for all $$x$$. When we have adversarial noise (i.e., when we are in the agnostic model), the labels can be completely arbitrary. In both cases, the goal is to produce a hypothesis whose error is competitive with opt. We use $$\\mathbb{E}$$ to denote the expectation of a random variable in brackets (or, correspondingly, $$\\mathbb{P}$$ for the probability of an event), either over the unknown joint distribution or over the empirical distribution with respect to a sample $$S$$ (e.g., $$\\mathbb{E}_{Z \\in S}[f(Z)] = \\frac{1}{|S|} \\sum_{Z \\in S} f(Z)$$).", "md": "For what follows, we consider $$D_{XY}$$ to be an unknown joint distribution over $$X \\times Y$$ from which we receive independent samples, and its marginal on $$X$$ will be denoted by $$D_X$$. In particular $$X = \\mathbb{R}^d$$, and labels will lie in $$Y = \\{ \\pm 1 \\}$$. We will use $$\\mathcal{C}$$ to denote a concept class mapping $$\\mathbb{R}^d$$ to $$\\{ \\pm 1 \\}$$, which throughout this paper will be the class of halfspaces or functions of halfspaces over $$\\mathbb{R}^d$$. We use $$\\text{opt}(\\mathcal{C}, D_{XY})$$ to denote the optimal error $$\\inf_{f \\in \\mathcal{C}} \\mathbb{P}(x,y) \\sim D_{XY}[f(x) \\neq y]$$, or just opt when $$\\mathcal{C}$$ and $$D_{XY}$$ are clear from context. We recall that in Massart noise model, the labels satisfy $$\\mathbb{P}(y \\sim D_{XY} | x)[y \\neq \\text{sign}(\\langle w^*, x \\rangle) | x] = \\eta(x)$$, with $$\\eta(x) \\leq \\eta < \\frac{1}{2}$$ for all $$x$$. When we have adversarial noise (i.e., when we are in the agnostic model), the labels can be completely arbitrary. In both cases, the goal is to produce a hypothesis whose error is competitive with opt. We use $$\\mathbb{E}$$ to denote the expectation of a random variable in brackets (or, correspondingly, $$\\mathbb{P}$$ for the probability of an event), either over the unknown joint distribution or over the empirical distribution with respect to a sample $$S$$ (e.g., $$\\mathbb{E}_{Z \\in S}[f(Z)] = \\frac{1}{|S|} \\sum_{Z \\in S} f(Z)$$)."}, {"type": "heading", "lvl": 3, "value": "Definitions and Distributional Assumptions", "md": "### Definitions and Distributional Assumptions"}, {"type": "text", "value": "For the problem of learning halfspaces in the agnostic and in Massart noise models, any of the known polynomial algorithms that achieve computationally optimal guarantees require that the marginal distribution has at least the following nice properties previously defined by, e.g., [DKTZ20b].\n\n**Definition 2.1 (Nice Distributions)**. For a given constant $$\\lambda \\geq 1$$, we consider the class of $$\\lambda$$-nice distributions over $$\\mathbb{R}^d$$ to be the distributions that satisfy the following properties:\n1. For any unit vector $$v$$ in $$\\mathbb{R}^d$$ the distribution satisfies $$\\mathbb{E}[\\langle v, x \\rangle^2] \\in [\\frac{1}{\\lambda}, \\lambda]$$. (bounded spectrum)\n2. For any two-dimensional subspace $$V$$, the corresponding marginal density $$q_V(x)$$ satisfies $$q_V(x) \\geq \\frac{1}{\\lambda}$$ for any $$\\|x\\|_2 \\leq \\frac{1}{\\lambda}$$. (anti-anti-concentration)", "md": "For the problem of learning halfspaces in the agnostic and in Massart noise models, any of the known polynomial algorithms that achieve computationally optimal guarantees require that the marginal distribution has at least the following nice properties previously defined by, e.g., [DKTZ20b].\n\n**Definition 2.1 (Nice Distributions)**. For a given constant $$\\lambda \\geq 1$$, we consider the class of $$\\lambda$$-nice distributions over $$\\mathbb{R}^d$$ to be the distributions that satisfy the following properties:\n1. For any unit vector $$v$$ in $$\\mathbb{R}^d$$ the distribution satisfies $$\\mathbb{E}[\\langle v, x \\rangle^2] \\in [\\frac{1}{\\lambda}, \\lambda]$$. (bounded spectrum)\n2. For any two-dimensional subspace $$V$$, the corresponding marginal density $$q_V(x)$$ satisfies $$q_V(x) \\geq \\frac{1}{\\lambda}$$ for any $$\\|x\\|_2 \\leq \\frac{1}{\\lambda}$$. (anti-anti-concentration)"}]}, {"page": 6, "text": "   3. For any two dimensional subspace V , the corresponding marginal density qV (x) satisfies\n      qV (x) \u2264   Q(\u2225x\u22252) for some function Q : R+ \u2192             R+ such that supr\u22650 Q(r) \u2264         \u03bb and also\n        \u221e                                                           (anti-concentration and concentration)\n       r=0 rkQ(r) dr \u2264    \u03bb, for any k = 1, 3, 5.\n    In the testable learning framework, however, corresponding results provide testable guarantees\nwith respect to target marginals that are isotropic strongly log-concave [GKSV23], which is a strictly\nstronger condition than the one of Definition 2.1 (see Proposition 2.3 below). We now provide the\nstandard definition of (strongly) log-concave distributions.\nDefinition 2.2 ((Strongly) Log-Concave Distributions [SW14]). We say that a distribution over\nRd is (\u03b2-strongly) log-concave, if its density can be written as e\u2212\u03d5, where \u03d5 is a (\u03b2-strongly) convex\nfunction on Rd (for some \u03b2 > 0).\nProposition 2.3 (Log-Concave Distributions are Nice [LV07]). There exists a universal constant\n\u03bb \u2265  1 such that any isotropic log-concave distribution is \u03bb-nice.\n    In this work, we provide universally testable guarantees with respect to the class of nice distri-\nbutions with bounded Poincar\u00b4e constant (see Definition 2.4 below).\nDefinition 2.4 (Poincar\u00b4e Distributions). For a given value \u03b3 > 0, we say that a distribution over\nRd is \u03b3-Poincar\u00b4e, if Var(f(x)) \u2264    \u03b3 \u00b7 E[\u2225\u2207f(x)\u22252  2] for any differentiable function f : Rd \u2192     R.\n    Although it is not clear whether one can efficiently obtain testable guarantees for the problem of\nlearning noisy halfspaces under nice marginals (which is known to be an efficiently solvable problem\nin the non-testable setting [DKTZ20a, DKTZ20b]), by restricting our attention to nice distributions\nthat, additionally, have bounded Poincar\u00b4e constant, we obtain efficient learning results, even in the\nuniversally testable setting. Our results are strictly stronger than the ones in [GKSV23], since we\ncapture isotropic strongly log-concave distributions universally, due to Proposition 2.3 and the fact\nthat strongly log-concave distributions are also Poincar\u00b4e, as per Proposition 2.5 below.\nProposition 2.5 (Strongly Log-Concave Distributions are Poincar\u00b4e, [SW14, Proposition 10.1]).\nAny 1 \u03b3 -strongly log-concave distribution is \u03b3-Poincar\u00b4   e.\n    Furthermore, under a long-standing conjecture about the geometry of convex bodies [KLS95],\nour results capture the family of all isotropic log-concave distributions.\nConjecture 2.6 (Kannan\u2013Lov\u00b4          asz\u2013Simonovits Conjecture [KLS95] reformulation from [LV18]).\nThere is a universal constant \u03b3 > 0 for which any isotropic log-concave distribution is \u03b3-Poincar\u00b4e.\n3     Universal Testers\nIn this section, we present two basic testers that constitute the basic building blocks of the universal\ntester-learners we provide in the next section. The testers in this section might be of independent\ninterest and their appeal is that they succeed even when the distribution in their input is unspecified\nup to certain bounds on a number of its statistics. In fact, the family of distributions for which\neach such tester succeeds is of infinite size, even non-parametric.\n3.1    Universal Tester for Bounding Local Halfspace Disagreement\nFirst, we present a universal tester that checks, given a parameter vector w, whether a set of\nsamples S is such that bounding the angular distance of w from an optimum parameter vector,\n                                                       6", "md": "# Math Equations and Text\n\n### 3. For any two dimensional subspace V, the corresponding marginal density qV(x) satisfies\n\n$$q_V(x) \\leq Q(\\|x\\|^2)$$ for some function $$Q : \\mathbb{R}^+ \\rightarrow \\mathbb{R}^+$$ such that $$\\sup_{r \\geq 0} Q(r) \\leq \\lambda$$ and also\n\n$$\\int_{0}^{\\infty} r^k Q(r) dr \\leq \\lambda$$, for any $$k = 1, 3, 5$$.\n\nIn the testable learning framework, however, corresponding results provide testable guarantees with respect to target marginals that are isotropic strongly log-concave [GKSV23], which is a strictly stronger condition than the one of Definition 2.1 (see Proposition 2.3 below). We now provide the standard definition of (strongly) log-concave distributions.\n\n#### Definition 2.2 ((Strongly) Log-Concave Distributions [SW14])\n\nWe say that a distribution over $$\\mathbb{R}^d$$ is (\u03b2-strongly) log-concave, if its density can be written as $$e^{-\\phi}$$, where $$\\phi$$ is a (\u03b2-strongly) convex function on $$\\mathbb{R}^d$$ (for some $$\\beta > 0$$).\n\n#### Proposition 2.3 (Log-Concave Distributions are Nice [LV07])\n\nThere exists a universal constant $$\\lambda \\geq 1$$ such that any isotropic log-concave distribution is $$\\lambda$$-nice.\n\nIn this work, we provide universally testable guarantees with respect to the class of nice distributions with bounded Poincar\u00e9 constant (see Definition 2.4 below).\n\n#### Definition 2.4 (Poincar\u00e9 Distributions)\n\nFor a given value $$\\gamma > 0$$, we say that a distribution over $$\\mathbb{R}^d$$ is $$\\gamma$$-Poincar\u00e9, if $$\\text{Var}(f(x)) \\leq \\gamma \\cdot \\mathbb{E}[\\|\\nabla f(x)\\|^2_2]$$ for any differentiable function $$f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$.\n\nAlthough it is not clear whether one can efficiently obtain testable guarantees for the problem of learning noisy halfspaces under nice marginals (which is known to be an efficiently solvable problem in the non-testable setting [DKTZ20a, DKTZ20b]), by restricting our attention to nice distributions that, additionally, have bounded Poincar\u00e9 constant, we obtain efficient learning results, even in the universally testable setting. Our results are strictly stronger than the ones in [GKSV23], since we capture isotropic strongly log-concave distributions universally, due to Proposition 2.3 and the fact that strongly log-concave distributions are also Poincar\u00e9, as per Proposition 2.5 below.\n\n#### Proposition 2.5 (Strongly Log-Concave Distributions are Poincar\u00e9, [SW14, Proposition 10.1])\n\nAny $$\\frac{1}{\\gamma}$$-strongly log-concave distribution is $$\\gamma$$-Poincar\u00e9.\n\nFurthermore, under a long-standing conjecture about the geometry of convex bodies [KLS95], our results capture the family of all isotropic log-concave distributions.\n\n#### Conjecture 2.6 (Kannan\u2013Lov\u00e1sz\u2013Simonovits Conjecture [KLS95] reformulation from [LV18])\n\nThere is a universal constant $$\\gamma > 0$$ for which any isotropic log-concave distribution is $$\\gamma$$-Poincar\u00e9.\n\n### 3 Universal Testers\n\nIn this section, we present two basic testers that constitute the basic building blocks of the universal tester-learners we provide in the next section. The testers in this section might be of independent interest and their appeal is that they succeed even when the distribution in their input is unspecified up to certain bounds on a number of its statistics. In fact, the family of distributions for which each such tester succeeds is of infinite size, even non-parametric.\n\n#### 3.1 Universal Tester for Bounding Local Halfspace Disagreement\n\nFirst, we present a universal tester that checks, given a parameter vector $$w$$, whether a set of samples $$S$$ is such that bounding the angular distance of $$w$$ from an optimum parameter vector,", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 3, "value": "3. For any two dimensional subspace V, the corresponding marginal density qV(x) satisfies", "md": "### 3. For any two dimensional subspace V, the corresponding marginal density qV(x) satisfies"}, {"type": "text", "value": "$$q_V(x) \\leq Q(\\|x\\|^2)$$ for some function $$Q : \\mathbb{R}^+ \\rightarrow \\mathbb{R}^+$$ such that $$\\sup_{r \\geq 0} Q(r) \\leq \\lambda$$ and also\n\n$$\\int_{0}^{\\infty} r^k Q(r) dr \\leq \\lambda$$, for any $$k = 1, 3, 5$$.\n\nIn the testable learning framework, however, corresponding results provide testable guarantees with respect to target marginals that are isotropic strongly log-concave [GKSV23], which is a strictly stronger condition than the one of Definition 2.1 (see Proposition 2.3 below). We now provide the standard definition of (strongly) log-concave distributions.", "md": "$$q_V(x) \\leq Q(\\|x\\|^2)$$ for some function $$Q : \\mathbb{R}^+ \\rightarrow \\mathbb{R}^+$$ such that $$\\sup_{r \\geq 0} Q(r) \\leq \\lambda$$ and also\n\n$$\\int_{0}^{\\infty} r^k Q(r) dr \\leq \\lambda$$, for any $$k = 1, 3, 5$$.\n\nIn the testable learning framework, however, corresponding results provide testable guarantees with respect to target marginals that are isotropic strongly log-concave [GKSV23], which is a strictly stronger condition than the one of Definition 2.1 (see Proposition 2.3 below). We now provide the standard definition of (strongly) log-concave distributions."}, {"type": "heading", "lvl": 4, "value": "Definition 2.2 ((Strongly) Log-Concave Distributions [SW14])", "md": "#### Definition 2.2 ((Strongly) Log-Concave Distributions [SW14])"}, {"type": "text", "value": "We say that a distribution over $$\\mathbb{R}^d$$ is (\u03b2-strongly) log-concave, if its density can be written as $$e^{-\\phi}$$, where $$\\phi$$ is a (\u03b2-strongly) convex function on $$\\mathbb{R}^d$$ (for some $$\\beta > 0$$).", "md": "We say that a distribution over $$\\mathbb{R}^d$$ is (\u03b2-strongly) log-concave, if its density can be written as $$e^{-\\phi}$$, where $$\\phi$$ is a (\u03b2-strongly) convex function on $$\\mathbb{R}^d$$ (for some $$\\beta > 0$$)."}, {"type": "heading", "lvl": 4, "value": "Proposition 2.3 (Log-Concave Distributions are Nice [LV07])", "md": "#### Proposition 2.3 (Log-Concave Distributions are Nice [LV07])"}, {"type": "text", "value": "There exists a universal constant $$\\lambda \\geq 1$$ such that any isotropic log-concave distribution is $$\\lambda$$-nice.\n\nIn this work, we provide universally testable guarantees with respect to the class of nice distributions with bounded Poincar\u00e9 constant (see Definition 2.4 below).", "md": "There exists a universal constant $$\\lambda \\geq 1$$ such that any isotropic log-concave distribution is $$\\lambda$$-nice.\n\nIn this work, we provide universally testable guarantees with respect to the class of nice distributions with bounded Poincar\u00e9 constant (see Definition 2.4 below)."}, {"type": "heading", "lvl": 4, "value": "Definition 2.4 (Poincar\u00e9 Distributions)", "md": "#### Definition 2.4 (Poincar\u00e9 Distributions)"}, {"type": "text", "value": "For a given value $$\\gamma > 0$$, we say that a distribution over $$\\mathbb{R}^d$$ is $$\\gamma$$-Poincar\u00e9, if $$\\text{Var}(f(x)) \\leq \\gamma \\cdot \\mathbb{E}[\\|\\nabla f(x)\\|^2_2]$$ for any differentiable function $$f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$.\n\nAlthough it is not clear whether one can efficiently obtain testable guarantees for the problem of learning noisy halfspaces under nice marginals (which is known to be an efficiently solvable problem in the non-testable setting [DKTZ20a, DKTZ20b]), by restricting our attention to nice distributions that, additionally, have bounded Poincar\u00e9 constant, we obtain efficient learning results, even in the universally testable setting. Our results are strictly stronger than the ones in [GKSV23], since we capture isotropic strongly log-concave distributions universally, due to Proposition 2.3 and the fact that strongly log-concave distributions are also Poincar\u00e9, as per Proposition 2.5 below.", "md": "For a given value $$\\gamma > 0$$, we say that a distribution over $$\\mathbb{R}^d$$ is $$\\gamma$$-Poincar\u00e9, if $$\\text{Var}(f(x)) \\leq \\gamma \\cdot \\mathbb{E}[\\|\\nabla f(x)\\|^2_2]$$ for any differentiable function $$f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$.\n\nAlthough it is not clear whether one can efficiently obtain testable guarantees for the problem of learning noisy halfspaces under nice marginals (which is known to be an efficiently solvable problem in the non-testable setting [DKTZ20a, DKTZ20b]), by restricting our attention to nice distributions that, additionally, have bounded Poincar\u00e9 constant, we obtain efficient learning results, even in the universally testable setting. Our results are strictly stronger than the ones in [GKSV23], since we capture isotropic strongly log-concave distributions universally, due to Proposition 2.3 and the fact that strongly log-concave distributions are also Poincar\u00e9, as per Proposition 2.5 below."}, {"type": "heading", "lvl": 4, "value": "Proposition 2.5 (Strongly Log-Concave Distributions are Poincar\u00e9, [SW14, Proposition 10.1])", "md": "#### Proposition 2.5 (Strongly Log-Concave Distributions are Poincar\u00e9, [SW14, Proposition 10.1])"}, {"type": "text", "value": "Any $$\\frac{1}{\\gamma}$$-strongly log-concave distribution is $$\\gamma$$-Poincar\u00e9.\n\nFurthermore, under a long-standing conjecture about the geometry of convex bodies [KLS95], our results capture the family of all isotropic log-concave distributions.", "md": "Any $$\\frac{1}{\\gamma}$$-strongly log-concave distribution is $$\\gamma$$-Poincar\u00e9.\n\nFurthermore, under a long-standing conjecture about the geometry of convex bodies [KLS95], our results capture the family of all isotropic log-concave distributions."}, {"type": "heading", "lvl": 4, "value": "Conjecture 2.6 (Kannan\u2013Lov\u00e1sz\u2013Simonovits Conjecture [KLS95] reformulation from [LV18])", "md": "#### Conjecture 2.6 (Kannan\u2013Lov\u00e1sz\u2013Simonovits Conjecture [KLS95] reformulation from [LV18])"}, {"type": "text", "value": "There is a universal constant $$\\gamma > 0$$ for which any isotropic log-concave distribution is $$\\gamma$$-Poincar\u00e9.", "md": "There is a universal constant $$\\gamma > 0$$ for which any isotropic log-concave distribution is $$\\gamma$$-Poincar\u00e9."}, {"type": "heading", "lvl": 3, "value": "3 Universal Testers", "md": "### 3 Universal Testers"}, {"type": "text", "value": "In this section, we present two basic testers that constitute the basic building blocks of the universal tester-learners we provide in the next section. The testers in this section might be of independent interest and their appeal is that they succeed even when the distribution in their input is unspecified up to certain bounds on a number of its statistics. In fact, the family of distributions for which each such tester succeeds is of infinite size, even non-parametric.", "md": "In this section, we present two basic testers that constitute the basic building blocks of the universal tester-learners we provide in the next section. The testers in this section might be of independent interest and their appeal is that they succeed even when the distribution in their input is unspecified up to certain bounds on a number of its statistics. In fact, the family of distributions for which each such tester succeeds is of infinite size, even non-parametric."}, {"type": "heading", "lvl": 4, "value": "3.1 Universal Tester for Bounding Local Halfspace Disagreement", "md": "#### 3.1 Universal Tester for Bounding Local Halfspace Disagreement"}, {"type": "text", "value": "First, we present a universal tester that checks, given a parameter vector $$w$$, whether a set of samples $$S$$ is such that bounding the angular distance of $$w$$ from an optimum parameter vector,", "md": "First, we present a universal tester that checks, given a parameter vector $$w$$, whether a set of samples $$S$$ is such that bounding the angular distance of $$w$$ from an optimum parameter vector,"}]}, {"page": 7, "text": "implies that the corresponding halfspace disagrees with the (unknown) optimum halfspace only\non a bounded fraction of points in S. This property ensures that if w is close to the optimum\nparameter vector, then it is also an approximate empirical risk minimizer. The tester universally\naccepts samples from nice distributions with high probability (Definition 2.1).\nLemma 3.1 (Universally Testable Bound for Local Halfspace Disagreement). Let DXY be a dis-\ntribution over Rd \u00d7 {\u00b11}, w \u2208              Sd\u22121, \u03b8 \u2208     (0, \u03c0/4], \u03bb \u2265     1 and \u03b4 \u2208     (0, 1). Then, for a sufficiently\nlarge constant C, there is a tester that given \u03b4, \u03b8, w and a set S of samples from DX with size at\nleast C \u00b7     d4   , runs in time poly        d, 1\u03b8, 1   and satisfies the following specifications:\n              \u03b82\u03b4                                    \u03b4\n  (a) If the tester accepts S, then for every unit vector w\u2032 \u2208                  Rn satisfying \u2221(w, w\u2032) \u2264          \u03b8 we have\n                                         P\n                                       x\u223cS[sign(\u27e8w\u2032, x\u27e9) \u0338= sign(\u27e8w, x\u27e9)] \u2264           C \u00b7 \u03b8 \u00b7 \u03bbC\n  (b) If the distribution DX is \u03bb-nice, the tester accepts S with probability 1 \u2212                       \u03b4.\n     The proof of Lemma 3.1 simplifies and improves the proof of a similar but weaker result in\n[GKSV23] (see their Proposition 4.5). The initial tester exploited the observation that the proba-\nbility of disagreement between two halfspaces can be upper bounded by a sum of products, where\neach product has two terms: one corresponding to the probability of falling in a (known) strip\northogonal to w and one corresponding to the probability of having large enough inner product\nwith some unknown vector orthogonal to w, conditioned in the (known) strip. The first term can\nbe controlled by estimating the probability of falling in a (known) strip, while the second follows\nby Chebyshev\u2019s inequality, after estimating the largest eigenvalue of the covariance matrix con-\nditioned in the known strip. This approach introduces a number of complications, including the\nfact that conditioning requires rejection sampling, which, in turn requires a lower bound on the\nprobability of falling inside each strip. We propose a simpler tester that controls all of the terms\nof the sum simultaneously by estimating the largest eigenvalue of a single covariance matrix (with-\nout conditioning). Upper and lower bounds on the eigenvalues of random symmetric matrices can\nbe universally tested with testers that are guaranteed to accept when the elements of the matrix\nhave bounded second moments (spectral tester of Proposition A.2). We present our full proof in\nAppendix B.\n3.2      Universally Testable Weak Anti-Concentration\nWe now provide an important universal tester, which ensures that for a given vector w, a sample set\nS and any unknown unit vector v orthogonal to w, among the samples falling within a (known) strip\northogonal to w, at least a constant fraction is absolutely correlated with v by a constant. In other\nwords, the tester ensures that the conditional empirical distribution is weakly anti-concentrated\nin every direction. The tester universally accepts nice distributions that have bounded Poincar\u00b4e\nconstant.\nLemma 3.2 (Universally Testable Weak Anti-Concentration). Let D be a distribution over Rd.\nThen, there is a universal1constant C > 0 and a tester that given a unit vector w \u2208                            Rd, \u03b4 \u2208    (0, 1),\n\u03b3 > 0, \u03bb \u2265     1, \u03c3 \u2264    2\u03bb and a set S of i.i.d. samples from D with size at least C \u00b7 d4               \u03c32\u03b4 log(d)\u03bbC, runs\nin time poly(d, \u03bb, 1    \u03c3, 1\n                           \u03b4) and satisfies the following specifications\n  (a) If the tester accepts S, then for any unit vector v \u2208                 Rd with \u27e8v, w\u27e9      = 0 we have\n                                        P    |\u27e8v, x\u27e9| \u2265      1     |\u27e8w, x\u27e9| \u2264    \u03c3   \u2265       1\n                                       x\u2208S                C\u03bbC                            C\u03bbC\u03b34\n                                                                7", "md": "# Math Equations and Text\n\nimplies that the corresponding halfspace disagrees with the (unknown) optimum halfspace only on a bounded fraction of points in S. This property ensures that if w is close to the optimum parameter vector, then it is also an approximate empirical risk minimizer. The tester universally accepts samples from nice distributions with high probability (Definition 2.1).\n\n$$Lemma\\ 3.1\\ (Universally\\ Testable\\ Bound\\ for\\ Local\\ Halfspace\\ Disagreement).$$ Let $$DXY$$ be a distribution over $$R^d \\times \\{\u00b11\\}$$, $$w \\in S^{d-1}$$, $$\u03b8 \\in (0, \\frac{\\pi}{4}]$$, $$\u03bb \\ge 1$$ and $$\u03b4 \\in (0, 1)$$. Then, for a sufficiently large constant C, there is a tester that given $$\u03b4, \u03b8, w$$ and a set S of samples from $$DX$$ with size at least $$C \\cdot d^4$$, runs in time $$poly(d, \\frac{1}{\u03b8}, 1)$$ and satisfies the following specifications:\n\n1. $\u03b8^2\u03b4$\n2.\nIf the tester accepts S, then for every unit vector $w' \\in R^n$ satisfying $\u2221(w, w') \\le \u03b8$ we have $P_{x\u223cS}[sign(\u27e8w', x\u27e9) \\neq sign(\u27e8w, x\u27e9)] \\le C \\cdot \u03b8 \\cdot \u03bb^C$\n3. If the distribution $DX$ is $\u03bb$-nice, the tester accepts S with probability $1 - \u03b4$.\n\nThe proof of Lemma 3.1 simplifies and improves the proof of a similar but weaker result in [GKSV23] (see their Proposition 4.5). The initial tester exploited the observation that the probability of disagreement between two halfspaces can be upper bounded by a sum of products, where each product has two terms: one corresponding to the probability of falling in a (known) strip orthogonal to $$w$$ and one corresponding to the probability of having large enough inner product with some unknown vector orthogonal to $$w$$, conditioned in the (known) strip. The first term can be controlled by estimating the probability of falling in a (known) strip, while the second follows by Chebyshev\u2019s inequality, after estimating the largest eigenvalue of the covariance matrix conditioned in the known strip. This approach introduces a number of complications, including the fact that conditioning requires rejection sampling, which, in turn requires a lower bound on the probability of falling inside each strip. We propose a simpler tester that controls all of the terms of the sum simultaneously by estimating the largest eigenvalue of a single covariance matrix (without conditioning). Upper and lower bounds on the eigenvalues of random symmetric matrices can be universally tested with testers that are guaranteed to accept when the elements of the matrix have bounded second moments (spectral tester of Proposition A.2). We present our full proof in Appendix B.\n\n$$Lemma\\ 3.2\\ (Universally\\ Testable\\ Weak\\ Anti-Concentration).$$ Let $$D$$ be a distribution over $$R^d$$. Then, there is a universal constant $$C > 0$$ and a tester that given a unit vector $$w \\in R^d$$, $$\u03b4 \\in (0, 1)$$, $$\u03b3 > 0$$, $$\u03bb \\ge 1$$, $$\u03c3 \\le 2\u03bb$$ and a set S of i.i.d. samples from $$D$$ with size at least $$C \\cdot d^4 \\cdot \u03c3^2\u03b4 \\cdot log(d)\u03bb^C$$, runs in time $$poly(d, \u03bb, \\frac{1}{\u03c3}, \\frac{1}{\u03b4})$$ and satisfies the following specifications:\n\n1. If the tester accepts S, then for any unit vector $v \\in R^d$ with $\u27e8v, w\u27e9 = 0$ we have $P_{x\u2208S}[|\u27e8v, x\u27e9| \\ge \\frac{1}{C\u03bb^C} \\cdot |\u27e8w, x\u27e9| \\le \u03c3 \\ge \\frac{1}{C\u03bb^C\u03b3^4}]$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "implies that the corresponding halfspace disagrees with the (unknown) optimum halfspace only on a bounded fraction of points in S. This property ensures that if w is close to the optimum parameter vector, then it is also an approximate empirical risk minimizer. The tester universally accepts samples from nice distributions with high probability (Definition 2.1).\n\n$$Lemma\\ 3.1\\ (Universally\\ Testable\\ Bound\\ for\\ Local\\ Halfspace\\ Disagreement).$$ Let $$DXY$$ be a distribution over $$R^d \\times \\{\u00b11\\}$$, $$w \\in S^{d-1}$$, $$\u03b8 \\in (0, \\frac{\\pi}{4}]$$, $$\u03bb \\ge 1$$ and $$\u03b4 \\in (0, 1)$$. Then, for a sufficiently large constant C, there is a tester that given $$\u03b4, \u03b8, w$$ and a set S of samples from $$DX$$ with size at least $$C \\cdot d^4$$, runs in time $$poly(d, \\frac{1}{\u03b8}, 1)$$ and satisfies the following specifications:\n\n1. $\u03b8^2\u03b4$\n2.\nIf the tester accepts S, then for every unit vector $w' \\in R^n$ satisfying $\u2221(w, w') \\le \u03b8$ we have $P_{x\u223cS}[sign(\u27e8w', x\u27e9) \\neq sign(\u27e8w, x\u27e9)] \\le C \\cdot \u03b8 \\cdot \u03bb^C$\n3. If the distribution $DX$ is $\u03bb$-nice, the tester accepts S with probability $1 - \u03b4$.\n\nThe proof of Lemma 3.1 simplifies and improves the proof of a similar but weaker result in [GKSV23] (see their Proposition 4.5). The initial tester exploited the observation that the probability of disagreement between two halfspaces can be upper bounded by a sum of products, where each product has two terms: one corresponding to the probability of falling in a (known) strip orthogonal to $$w$$ and one corresponding to the probability of having large enough inner product with some unknown vector orthogonal to $$w$$, conditioned in the (known) strip. The first term can be controlled by estimating the probability of falling in a (known) strip, while the second follows by Chebyshev\u2019s inequality, after estimating the largest eigenvalue of the covariance matrix conditioned in the known strip. This approach introduces a number of complications, including the fact that conditioning requires rejection sampling, which, in turn requires a lower bound on the probability of falling inside each strip. We propose a simpler tester that controls all of the terms of the sum simultaneously by estimating the largest eigenvalue of a single covariance matrix (without conditioning). Upper and lower bounds on the eigenvalues of random symmetric matrices can be universally tested with testers that are guaranteed to accept when the elements of the matrix have bounded second moments (spectral tester of Proposition A.2). We present our full proof in Appendix B.\n\n$$Lemma\\ 3.2\\ (Universally\\ Testable\\ Weak\\ Anti-Concentration).$$ Let $$D$$ be a distribution over $$R^d$$. Then, there is a universal constant $$C > 0$$ and a tester that given a unit vector $$w \\in R^d$$, $$\u03b4 \\in (0, 1)$$, $$\u03b3 > 0$$, $$\u03bb \\ge 1$$, $$\u03c3 \\le 2\u03bb$$ and a set S of i.i.d. samples from $$D$$ with size at least $$C \\cdot d^4 \\cdot \u03c3^2\u03b4 \\cdot log(d)\u03bb^C$$, runs in time $$poly(d, \u03bb, \\frac{1}{\u03c3}, \\frac{1}{\u03b4})$$ and satisfies the following specifications:\n\n1. If the tester accepts S, then for any unit vector $v \\in R^d$ with $\u27e8v, w\u27e9 = 0$ we have $P_{x\u2208S}[|\u27e8v, x\u27e9| \\ge \\frac{1}{C\u03bb^C} \\cdot |\u27e8w, x\u27e9| \\le \u03c3 \\ge \\frac{1}{C\u03bb^C\u03b3^4}]$", "md": "implies that the corresponding halfspace disagrees with the (unknown) optimum halfspace only on a bounded fraction of points in S. This property ensures that if w is close to the optimum parameter vector, then it is also an approximate empirical risk minimizer. The tester universally accepts samples from nice distributions with high probability (Definition 2.1).\n\n$$Lemma\\ 3.1\\ (Universally\\ Testable\\ Bound\\ for\\ Local\\ Halfspace\\ Disagreement).$$ Let $$DXY$$ be a distribution over $$R^d \\times \\{\u00b11\\}$$, $$w \\in S^{d-1}$$, $$\u03b8 \\in (0, \\frac{\\pi}{4}]$$, $$\u03bb \\ge 1$$ and $$\u03b4 \\in (0, 1)$$. Then, for a sufficiently large constant C, there is a tester that given $$\u03b4, \u03b8, w$$ and a set S of samples from $$DX$$ with size at least $$C \\cdot d^4$$, runs in time $$poly(d, \\frac{1}{\u03b8}, 1)$$ and satisfies the following specifications:\n\n1. $\u03b8^2\u03b4$\n2.\nIf the tester accepts S, then for every unit vector $w' \\in R^n$ satisfying $\u2221(w, w') \\le \u03b8$ we have $P_{x\u223cS}[sign(\u27e8w', x\u27e9) \\neq sign(\u27e8w, x\u27e9)] \\le C \\cdot \u03b8 \\cdot \u03bb^C$\n3. If the distribution $DX$ is $\u03bb$-nice, the tester accepts S with probability $1 - \u03b4$.\n\nThe proof of Lemma 3.1 simplifies and improves the proof of a similar but weaker result in [GKSV23] (see their Proposition 4.5). The initial tester exploited the observation that the probability of disagreement between two halfspaces can be upper bounded by a sum of products, where each product has two terms: one corresponding to the probability of falling in a (known) strip orthogonal to $$w$$ and one corresponding to the probability of having large enough inner product with some unknown vector orthogonal to $$w$$, conditioned in the (known) strip. The first term can be controlled by estimating the probability of falling in a (known) strip, while the second follows by Chebyshev\u2019s inequality, after estimating the largest eigenvalue of the covariance matrix conditioned in the known strip. This approach introduces a number of complications, including the fact that conditioning requires rejection sampling, which, in turn requires a lower bound on the probability of falling inside each strip. We propose a simpler tester that controls all of the terms of the sum simultaneously by estimating the largest eigenvalue of a single covariance matrix (without conditioning). Upper and lower bounds on the eigenvalues of random symmetric matrices can be universally tested with testers that are guaranteed to accept when the elements of the matrix have bounded second moments (spectral tester of Proposition A.2). We present our full proof in Appendix B.\n\n$$Lemma\\ 3.2\\ (Universally\\ Testable\\ Weak\\ Anti-Concentration).$$ Let $$D$$ be a distribution over $$R^d$$. Then, there is a universal constant $$C > 0$$ and a tester that given a unit vector $$w \\in R^d$$, $$\u03b4 \\in (0, 1)$$, $$\u03b3 > 0$$, $$\u03bb \\ge 1$$, $$\u03c3 \\le 2\u03bb$$ and a set S of i.i.d. samples from $$D$$ with size at least $$C \\cdot d^4 \\cdot \u03c3^2\u03b4 \\cdot log(d)\u03bb^C$$, runs in time $$poly(d, \u03bb, \\frac{1}{\u03c3}, \\frac{1}{\u03b4})$$ and satisfies the following specifications:\n\n1. If the tester accepts S, then for any unit vector $v \\in R^d$ with $\u27e8v, w\u27e9 = 0$ we have $P_{x\u2208S}[|\u27e8v, x\u27e9| \\ge \\frac{1}{C\u03bb^C} \\cdot |\u27e8w, x\u27e9| \\le \u03c3 \\ge \\frac{1}{C\u03bb^C\u03b3^4}]$"}]}, {"page": 8, "text": " (b) If D is \u03b3-Poincar\u00b4e and \u03bb-nice, then the tester accepts S with probability at least 1 \u2212    \u03b4.\n   The proof of Lemma 3.2 is based on a simple fact from probability that is true for any non-\nnegative random variable and ensures that the mass assigned to the tails is lower bounded by the\nratio of the square of its expectation to the second moment.\nProposition 3.3 (Paley\u2013Zygmund Inequality). For any non-negative random variable Z, we have\n                                      P[Z > E[Z]/2] \u2265   1\n                                                        4 \u00b7 E[Z]2\n                                                            E[Z2]\n   In the special case where Z follows the distribution of \u27e8v, x\u27e92 conditioned on |\u27e8w, x\u27e9| \u2264       \u03c3 for\nsome unitary orthogonal vectors v, w, some \u03c3 > 0 and some random variable x whose distribution\nis, say, 1-nice (see Definition 2.1), one can show that E[Z] is lower bounded by a constant and E[Z2]\nis upper bounded by another constant, so Z assigns a non-trivial mass to a set that is bounded\naway from zero. This property is useful in the context of learning noisy halfspaces, as we show\nin the following section (see Proposition 4.3 and Lemma 4.4). However, in order to the testing\nalgorithms that check whether such a property holds for given w and \u03c3, are guaranteed to succeed\nwhen the marginal distribution has, additionally, bounded Poincar\u00b4e constant. The main part of the\nproof that requires a bounded Poincar\u00b4e constant, is testing whether E[Z2] is bounded uniformly\nover v \u22a5  w, since Z2 = \u27e8v, x\u27e94, where v is unknown. We use the following result from [KS17].\nProposition 3.4 (Certifiable Hypercontractivity of Poincar\u00b4e Distributions, Theorem 4.1 in [KS17]).\nLet \u03b4 \u2208 (0, 1), \u03b3 > 0 and let D be a \u03b3-Poincar\u00b4e distribution over Rd. Let S be a set of independent\nsamples from D with size at least (2d log(4d/\u03b4))4. Consider the constrained maximization problem\n                                        arg max    x\u2208S[\u27e8v, x\u27e94]                                    (3.1)\n                                            \u2225v\u22252=1 E\nThen, the optimum solution of the degree-4 sum-of-squares relaxation of the problem (3.1) has value\nat most C\u03b34 for some universal constant C, with probability at least 1 \u2212     \u03b4 over the sample S.\n   Using Proposition 3.4, we are able to provide a universal tester for bounding the empirical fourth\nmoments. The tester solves an appropriate SDP relaxation of the (hard) problem of finding the\ndirection with maximum fourth moment and is guaranteed to succeed if x has Poincar\u00b4e parameter\nbounded by a known value.\nProposition 3.5 (Hypercontractivity Tester). Let D be a distribution over Rd. Then, there is\na tester that given \u03b4 \u2208   (0, 1), \u03b3 > 0 and a set S of i.i.d.      samples from D with size at least\n(2d log(4d/\u03b4))4, runs in time poly(d, log 1\u03b4) and satisfies the following specifications\n (a) If the tester accepts S, then for any unit vector v \u2208   Rd we have\n                                             E\n      where C is some universal constant.   x\u2208S[\u27e8v, x\u27e94] \u2264 C \u00b7 \u03b34 ,\n (b) If the distribution D is \u03b3-Poincar\u00b4e, then the tester accepts S with probability at least 1 \u2212   \u03b4.\nProof. The tester does the following:\n   1. Solves a degree-4 sum-of-squares relaxation of problem (3.1) up to accuracy \u03b34. (For a formal\n      definition of the relaxed problem, see Problem (2.3) in [KS17].)\n                                                   8", "md": "**(b) If D is \u03b3-Poincar\u00b4e and \u03bb-nice, then the tester accepts S with probability at least 1 \u2212 \u03b4.**\nThe proof of Lemma 3.2 is based on a simple fact from probability that is true for any non-negative random variable and ensures that the mass assigned to the tails is lower bounded by the ratio of the square of its expectation to the second moment.\n\n**Proposition 3.3 (Paley\u2013Zygmund Inequality).** For any non-negative random variable Z, we have\n\n$$\nP[Z > \\frac{E[Z]}{2}] \\geq \\frac{1}{4} \\cdot \\frac{E[Z]^2}{E[Z^2]}\n$$\n\nIn the special case where Z follows the distribution of $$\\langle v, x \\rangle^2$$ conditioned on $$|\\langle w, x \\rangle| \\leq \\sigma$$ for some unitary orthogonal vectors v, w, some $$\\sigma > 0$$ and some random variable x whose distribution is, say, 1-nice (see Definition 2.1), one can show that E[Z] is lower bounded by a constant and E[Z^2] is upper bounded by another constant, so Z assigns a non-trivial mass to a set that is bounded away from zero. This property is useful in the context of learning noisy halfspaces, as we show in the following section (see Proposition 4.3 and Lemma 4.4). However, in order for the testing algorithms that check whether such a property holds for given w and $$\\sigma$$, are guaranteed to succeed when the marginal distribution has, additionally, bounded Poincar\u00b4e constant. The main part of the proof that requires a bounded Poincar\u00b4e constant, is testing whether E[Z^2] is bounded uniformly over $$v \\perp w$$, since $$Z^2 = \\langle v, x \\rangle^4$$, where v is unknown. We use the following result from [KS17].\n\n**Proposition 3.4 (Certifiable Hypercontractivity of Poincar\u00b4e Distributions, Theorem 4.1 in [KS17]).** Let $$\\delta \\in (0, 1)$$, $$\\gamma > 0$$ and let D be a $$\\gamma$$-Poincar\u00b4e distribution over $$\\mathbb{R}^d$$. Let S be a set of independent samples from D with size at least $$(2d \\log(4d/\\delta))^4$$. Consider the constrained maximization problem\n\n$$\n\\underset{x \\in S}{\\text{arg max}} \\langle v, x \\rangle^4, \\quad \\text{s.t.} \\quad \\|v\\|_2 = 1\n$$\n$$(3.1)$$\n\nThen, the optimum solution of the degree-4 sum-of-squares relaxation of the problem (3.1) has value at most $$C\\gamma^4$$ for some universal constant C, with probability at least 1 \u2212 $$\\delta$$ over the sample S.\n\nUsing Proposition 3.4, we are able to provide a universal tester for bounding the empirical fourth moments. The tester solves an appropriate SDP relaxation of the (hard) problem of finding the direction with the maximum fourth moment and is guaranteed to succeed if x has Poincar\u00b4e parameter bounded by a known value.\n\n**Proposition 3.5 (Hypercontractivity Tester).** Let D be a distribution over $$\\mathbb{R}^d$$. Then, there is a tester that given $$\\delta \\in (0, 1)$$, $$\\gamma > 0$$ and a set S of i.i.d. samples from D with size at least $$(2d \\log(4d/\\delta))^4$$, runs in time poly(d, log 1/\u03b4) and satisfies the following specifications\n\n(a) If the tester accepts S, then for any unit vector v in $$\\mathbb{R}^d$$ we have\n\n$$\nE_{x \\in S}[\\langle v, x \\rangle^4] \\leq C \\cdot \\gamma^4\n$$\n\n(b) If the distribution D is $$\\gamma$$-Poincar\u00b4e, then the tester accepts S with probability at least 1 \u2212 $$\\delta$$.\n\n**Proof.** The tester does the following:\n\n1. Solves a degree-4 sum-of-squares relaxation of problem (3.1) up to accuracy $\\gamma^4$. (For a formal definition of the relaxed problem, see Problem (2.3) in [KS17].)", "images": [], "items": [{"type": "text", "value": "**(b) If D is \u03b3-Poincar\u00b4e and \u03bb-nice, then the tester accepts S with probability at least 1 \u2212 \u03b4.**\nThe proof of Lemma 3.2 is based on a simple fact from probability that is true for any non-negative random variable and ensures that the mass assigned to the tails is lower bounded by the ratio of the square of its expectation to the second moment.\n\n**Proposition 3.3 (Paley\u2013Zygmund Inequality).** For any non-negative random variable Z, we have\n\n$$\nP[Z > \\frac{E[Z]}{2}] \\geq \\frac{1}{4} \\cdot \\frac{E[Z]^2}{E[Z^2]}\n$$\n\nIn the special case where Z follows the distribution of $$\\langle v, x \\rangle^2$$ conditioned on $$|\\langle w, x \\rangle| \\leq \\sigma$$ for some unitary orthogonal vectors v, w, some $$\\sigma > 0$$ and some random variable x whose distribution is, say, 1-nice (see Definition 2.1), one can show that E[Z] is lower bounded by a constant and E[Z^2] is upper bounded by another constant, so Z assigns a non-trivial mass to a set that is bounded away from zero. This property is useful in the context of learning noisy halfspaces, as we show in the following section (see Proposition 4.3 and Lemma 4.4). However, in order for the testing algorithms that check whether such a property holds for given w and $$\\sigma$$, are guaranteed to succeed when the marginal distribution has, additionally, bounded Poincar\u00b4e constant. The main part of the proof that requires a bounded Poincar\u00b4e constant, is testing whether E[Z^2] is bounded uniformly over $$v \\perp w$$, since $$Z^2 = \\langle v, x \\rangle^4$$, where v is unknown. We use the following result from [KS17].\n\n**Proposition 3.4 (Certifiable Hypercontractivity of Poincar\u00b4e Distributions, Theorem 4.1 in [KS17]).** Let $$\\delta \\in (0, 1)$$, $$\\gamma > 0$$ and let D be a $$\\gamma$$-Poincar\u00b4e distribution over $$\\mathbb{R}^d$$. Let S be a set of independent samples from D with size at least $$(2d \\log(4d/\\delta))^4$$. Consider the constrained maximization problem\n\n$$\n\\underset{x \\in S}{\\text{arg max}} \\langle v, x \\rangle^4, \\quad \\text{s.t.} \\quad \\|v\\|_2 = 1\n$$\n$$(3.1)$$\n\nThen, the optimum solution of the degree-4 sum-of-squares relaxation of the problem (3.1) has value at most $$C\\gamma^4$$ for some universal constant C, with probability at least 1 \u2212 $$\\delta$$ over the sample S.\n\nUsing Proposition 3.4, we are able to provide a universal tester for bounding the empirical fourth moments. The tester solves an appropriate SDP relaxation of the (hard) problem of finding the direction with the maximum fourth moment and is guaranteed to succeed if x has Poincar\u00b4e parameter bounded by a known value.\n\n**Proposition 3.5 (Hypercontractivity Tester).** Let D be a distribution over $$\\mathbb{R}^d$$. Then, there is a tester that given $$\\delta \\in (0, 1)$$, $$\\gamma > 0$$ and a set S of i.i.d. samples from D with size at least $$(2d \\log(4d/\\delta))^4$$, runs in time poly(d, log 1/\u03b4) and satisfies the following specifications\n\n(a) If the tester accepts S, then for any unit vector v in $$\\mathbb{R}^d$$ we have\n\n$$\nE_{x \\in S}[\\langle v, x \\rangle^4] \\leq C \\cdot \\gamma^4\n$$\n\n(b) If the distribution D is $$\\gamma$$-Poincar\u00b4e, then the tester accepts S with probability at least 1 \u2212 $$\\delta$$.\n\n**Proof.** The tester does the following:\n\n1. Solves a degree-4 sum-of-squares relaxation of problem (3.1) up to accuracy $\\gamma^4$. (For a formal definition of the relaxed problem, see Problem (2.3) in [KS17].)", "md": "**(b) If D is \u03b3-Poincar\u00b4e and \u03bb-nice, then the tester accepts S with probability at least 1 \u2212 \u03b4.**\nThe proof of Lemma 3.2 is based on a simple fact from probability that is true for any non-negative random variable and ensures that the mass assigned to the tails is lower bounded by the ratio of the square of its expectation to the second moment.\n\n**Proposition 3.3 (Paley\u2013Zygmund Inequality).** For any non-negative random variable Z, we have\n\n$$\nP[Z > \\frac{E[Z]}{2}] \\geq \\frac{1}{4} \\cdot \\frac{E[Z]^2}{E[Z^2]}\n$$\n\nIn the special case where Z follows the distribution of $$\\langle v, x \\rangle^2$$ conditioned on $$|\\langle w, x \\rangle| \\leq \\sigma$$ for some unitary orthogonal vectors v, w, some $$\\sigma > 0$$ and some random variable x whose distribution is, say, 1-nice (see Definition 2.1), one can show that E[Z] is lower bounded by a constant and E[Z^2] is upper bounded by another constant, so Z assigns a non-trivial mass to a set that is bounded away from zero. This property is useful in the context of learning noisy halfspaces, as we show in the following section (see Proposition 4.3 and Lemma 4.4). However, in order for the testing algorithms that check whether such a property holds for given w and $$\\sigma$$, are guaranteed to succeed when the marginal distribution has, additionally, bounded Poincar\u00b4e constant. The main part of the proof that requires a bounded Poincar\u00b4e constant, is testing whether E[Z^2] is bounded uniformly over $$v \\perp w$$, since $$Z^2 = \\langle v, x \\rangle^4$$, where v is unknown. We use the following result from [KS17].\n\n**Proposition 3.4 (Certifiable Hypercontractivity of Poincar\u00b4e Distributions, Theorem 4.1 in [KS17]).** Let $$\\delta \\in (0, 1)$$, $$\\gamma > 0$$ and let D be a $$\\gamma$$-Poincar\u00b4e distribution over $$\\mathbb{R}^d$$. Let S be a set of independent samples from D with size at least $$(2d \\log(4d/\\delta))^4$$. Consider the constrained maximization problem\n\n$$\n\\underset{x \\in S}{\\text{arg max}} \\langle v, x \\rangle^4, \\quad \\text{s.t.} \\quad \\|v\\|_2 = 1\n$$\n$$(3.1)$$\n\nThen, the optimum solution of the degree-4 sum-of-squares relaxation of the problem (3.1) has value at most $$C\\gamma^4$$ for some universal constant C, with probability at least 1 \u2212 $$\\delta$$ over the sample S.\n\nUsing Proposition 3.4, we are able to provide a universal tester for bounding the empirical fourth moments. The tester solves an appropriate SDP relaxation of the (hard) problem of finding the direction with the maximum fourth moment and is guaranteed to succeed if x has Poincar\u00b4e parameter bounded by a known value.\n\n**Proposition 3.5 (Hypercontractivity Tester).** Let D be a distribution over $$\\mathbb{R}^d$$. Then, there is a tester that given $$\\delta \\in (0, 1)$$, $$\\gamma > 0$$ and a set S of i.i.d. samples from D with size at least $$(2d \\log(4d/\\delta))^4$$, runs in time poly(d, log 1/\u03b4) and satisfies the following specifications\n\n(a) If the tester accepts S, then for any unit vector v in $$\\mathbb{R}^d$$ we have\n\n$$\nE_{x \\in S}[\\langle v, x \\rangle^4] \\leq C \\cdot \\gamma^4\n$$\n\n(b) If the distribution D is $$\\gamma$$-Poincar\u00b4e, then the tester accepts S with probability at least 1 \u2212 $$\\delta$$.\n\n**Proof.** The tester does the following:\n\n1. Solves a degree-4 sum-of-squares relaxation of problem (3.1) up to accuracy $\\gamma^4$. (For a formal definition of the relaxed problem, see Problem (2.3) in [KS17].)"}]}, {"page": 9, "text": "    2. If the solution has value larger than (C \u2212              1)\u03b34, then reject. Otherwise accept.\n     The computational complexity of the tester is poly(|S|, d, log 1                \u03b3 ), since the problem it solves can\nbe written as a semidefinite program [Sho87, Par00, Nes00, Las01].\n     If the tester accepts S, then we know that the optimal solution of the relaxed problem is at\nmost C\u03b34 and we also know that any solution of the initial problem (3.1) has value at most equal\nto the value of the relaxation. Therefore E[\u27e8v, x\u27e94] \u2264                 C\u03b34, for any v \u2208       Sd\u22121.\n     On the other hand, if the true distribution D is \u03b3-Poincar\u00b4e, then, with probability at least 1\u2212\u03b4,\nwe have that the solution found in step 1 has, with probability at least 1\u2212\u03b4, value at most C\u2032\u03b34 for\nsome universal constant C\u2032, due to Proposition 3.4. In order to ensure that the tester will accept\nwith probability at least 1 \u2212         \u03b4, it suffices to pick C = C\u2032 + 1.\n     We are now ready to prove Lemma 3.2, by additionally making use of a spectral tester that\naccepts with high probability when the distribution of x is nice (similar to the spectral tester used\nfor Lemma 3.1).\nProof of Lemma 3.2. The testing algorithm receives a set S \u2282                      Rd, w \u2208    Sd\u22121, \u03b4 \u2208   (0, 1), \u03b3 > 0, \u03bb \u2265     1\n             1\nand \u03c3 \u2264     2\u03bb and does the following for some sufficiently large C1 > 0:\n    1. If Px\u2208S[|\u27e8w, x\u27e9| \u2264       \u03c3] > 2\u03c3 \u00b7 C1\u03bbC1, then reject.\n    2. Compute the (d \u2212         1) \u00d7 (d \u2212    1) matrix MS as follows:\n                                  MS = E   x\u2208S   (proj\u22a5w x)(proj\u22a5w x)T \u00b7 1{|\u27e8w, x\u27e9            \u2264  \u03c3|}                       2\u03c3\n    3. Run the spectral tester of Proposition A.2 on MS given \u03b4 \u2190                        \u03b4, \u03bb \u2190    C1\u03bbC1 and \u03b8 \u2190         C1\u03bbC1 ,\n                                                                                           2\u03c3\n        i.e., reject if the minimum singular value of MS is less than                   C1\u03bbC1 .\n    4. Run the hypercontractivity tester (Prop. 3.5) on S\u2032 = {proj\u22a5w x : x \u2208                          S and |\u27e8w, x\u27e9| \u2264       \u03c3},\n        i.e., solve an appropriate SDP and reject if the solution is larger than a specified threshold.\n        Otherwise, accept.\n     For part (a), we apply the Paley\u2013Zygmund inequality to the random variable Z = \u27e8v, x\u27e92\ncondtitioned on |\u27e8w, x\u27e9| \u2264          \u03c3 and obtain\n    P    \u27e8v, x\u27e92 \u2265    1 x\u2208S    \u27e8v, x\u27e92    |\u27e8w, x\u27e9| \u2264    \u03c3     |\u27e8w, x\u27e9| \u2264   \u03c3    \u2265  1\n   x\u2208S                2 E                                                          4 \u00b7 (Ex\u2208S[\u27e8v, x\u27e92 | |\u27e8w, x\u27e9| \u2264        \u03c3])2\n                                                                                         Ex\u2208S[\u27e8v, x\u27e94 | |\u27e8w, x\u27e9| \u2264        \u03c3]\nNote that since \u27e8v, w\u27e9         = 0, we have \u27e8v, x\u27e9        = \u27e8proj\u22a5w v, proj\u22a5w x\u27e9          (where \u2225v\u22252 = \u2225        proj\u22a5w v\u22252).\nTherefore, since S has passed the spectral tester as well as the tester for the probability of lying\nwithin the strip |\u27e8w, x\u27e9| \u2264         \u03c3, we have that\n                  E    \u27e8v, x\u27e92     |\u27e8w, x\u27e9| \u2264   \u03c3   = Ex\u2208S      \u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2264      \u03c3}    \u2265       1\n                 x\u2208S                                             Px\u2208S[|\u27e8w, x\u27e9| \u2264      \u03c3]             2C1\u03bb2C1\nMoreover, {x \u2208       S : |\u27e8w, x\u27e9| \u2264    \u03c3} has passed the hypercontractivity tester, and therefore, according\nto Proposition 3.5 we have\n                                          E    \u27e8v, x\u27e94     |\u27e8w, x\u27e9| \u2264    \u03c3   \u2264  C1 \u00b7 \u03b34\n                                         x\u2208S\nCombining the above inequalities we conclude the proof of part (a).\n                                                                9", "md": "2. If the solution has value larger than (C \u2212 1)\u03b34, then reject. Otherwise accept.\n\nThe computational complexity of the tester is poly(|S|, d, log 1 \u03b3), since the problem it solves can be written as a semidefinite program [Sho87, Par00, Nes00, Las01].\n\nIf the tester accepts S, then we know that the optimal solution of the relaxed problem is at most C\u03b34 and we also know that any solution of the initial problem (3.1) has value at most equal to the value of the relaxation. Therefore E[\u27e8v, x\u27e94] \u2264 C\u03b34, for any v \u2208 Sd\u22121.\n\nOn the other hand, if the true distribution D is \u03b3-Poincar\u00e9, then, with probability at least 1\u2212\u03b4, we have that the solution found in step 1 has, with probability at least 1\u2212\u03b4, value at most C\u2032\u03b34 for some universal constant C\u2032, due to Proposition 3.4. In order to ensure that the tester will accept with probability at least 1 \u2212 \u03b4, it suffices to pick C = C\u2032 + 1.\n\nWe are now ready to prove Lemma 3.2, by additionally making use of a spectral tester that accepts with high probability when the distribution of x is nice (similar to the spectral tester used for Lemma 3.1).\n\nProof of Lemma 3.2. The testing algorithm receives a set S \u2282 Rd, w \u2208 Sd\u22121, \u03b4 \u2208 (0, 1), \u03b3 > 0, \u03bb \u2265 1 and \u03c3 \u2264 2\u03bb and does the following for some sufficiently large C1 > 0:\n\n1. If Px\u2208S[|\u27e8w, x\u27e9| \u2264 \u03c3] > 2\u03c3 \u00b7 C1\u03bbC1, then reject.\n2. Compute the (d \u2212 1) \u00d7 (d \u2212 1) matrix MS as follows:\n\n$MS = E_{x\u2208S}[(proj\u22a5w x)(proj\u22a5w x)^T \u00b7 1\\{|\u27e8w, x\u27e9 \u2264 \u03c3|\\}]$\n3. Run the spectral tester of Proposition A.2 on MS given \u03b4 \u2190 \u03b4, \u03bb \u2190 C1\u03bbC1 and \u03b8 \u2190 C1\u03bbC1,\ni.e., reject if the minimum singular value of MS is less than C1\u03bbC1.\n4. Run the hypercontractivity tester (Prop. 3.5) on S' = {proj\u22a5w x : x \u2208 S and |\u27e8w, x\u27e9| \u2264 \u03c3},\ni.e., solve an appropriate SDP and reject if the solution is larger than a specified threshold. Otherwise, accept.\n\nFor part (a), we apply the Paley\u2013Zygmund inequality to the random variable Z = \u27e8v, x\u27e92 conditioned on |\u27e8w, x\u27e9| \u2264 \u03c3 and obtain\n\n$$P[\u27e8v, x\u27e92 \u2265 1 x\u2208S \u27e8v, x\u27e92 |\u27e8w, x\u27e9| \u2264 \u03c3 |\u27e8w, x\u27e9| \u2264 \u03c3] \u2265 \\frac{1}{4} \u00b7 (E_{x\u2208S}[\u27e8v, x\u27e92 | |\u27e8w, x\u27e9| \u2264 \u03c3])^2$$\n\nNote that since \u27e8v, w\u27e9 = 0, we have \u27e8v, x\u27e9 = \u27e8proj\u22a5w v, proj\u22a5w x\u27e9 (where \u2225v\u22252 = \u2225proj\u22a5w v\u22252). Therefore, since S has passed the spectral tester as well as the tester for the probability of lying within the strip |\u27e8w, x\u27e9| \u2264 \u03c3, we have that\n\n$$E[\u27e8v, x\u27e92 |\u27e8w, x\u27e9| \u2264 \u03c3] = E_{x\u2208S} [\u27e8v, x\u27e92 \u00b7 1\\{|\u27e8w, x\u27e9| \u2264 \u03c3}] \u2265 \\frac{1}{2C1\u03bb^2C1}$$\n\nMoreover, {x \u2208 S : |\u27e8w, x\u27e9| \u2264 \u03c3} has passed the hypercontractivity tester, and therefore, according to Proposition 3.5 we have\n\n$$E[\u27e8v, x\u27e94 |\u27e8w, x\u27e9| \u2264 \u03c3] \u2264 C1 \u00b7 \u03b34$$\n\nCombining the above inequalities we conclude the proof of part (a).\n\n9", "images": [], "items": [{"type": "text", "value": "2. If the solution has value larger than (C \u2212 1)\u03b34, then reject. Otherwise accept.\n\nThe computational complexity of the tester is poly(|S|, d, log 1 \u03b3), since the problem it solves can be written as a semidefinite program [Sho87, Par00, Nes00, Las01].\n\nIf the tester accepts S, then we know that the optimal solution of the relaxed problem is at most C\u03b34 and we also know that any solution of the initial problem (3.1) has value at most equal to the value of the relaxation. Therefore E[\u27e8v, x\u27e94] \u2264 C\u03b34, for any v \u2208 Sd\u22121.\n\nOn the other hand, if the true distribution D is \u03b3-Poincar\u00e9, then, with probability at least 1\u2212\u03b4, we have that the solution found in step 1 has, with probability at least 1\u2212\u03b4, value at most C\u2032\u03b34 for some universal constant C\u2032, due to Proposition 3.4. In order to ensure that the tester will accept with probability at least 1 \u2212 \u03b4, it suffices to pick C = C\u2032 + 1.\n\nWe are now ready to prove Lemma 3.2, by additionally making use of a spectral tester that accepts with high probability when the distribution of x is nice (similar to the spectral tester used for Lemma 3.1).\n\nProof of Lemma 3.2. The testing algorithm receives a set S \u2282 Rd, w \u2208 Sd\u22121, \u03b4 \u2208 (0, 1), \u03b3 > 0, \u03bb \u2265 1 and \u03c3 \u2264 2\u03bb and does the following for some sufficiently large C1 > 0:\n\n1. If Px\u2208S[|\u27e8w, x\u27e9| \u2264 \u03c3] > 2\u03c3 \u00b7 C1\u03bbC1, then reject.\n2. Compute the (d \u2212 1) \u00d7 (d \u2212 1) matrix MS as follows:\n\n$MS = E_{x\u2208S}[(proj\u22a5w x)(proj\u22a5w x)^T \u00b7 1\\{|\u27e8w, x\u27e9 \u2264 \u03c3|\\}]$\n3. Run the spectral tester of Proposition A.2 on MS given \u03b4 \u2190 \u03b4, \u03bb \u2190 C1\u03bbC1 and \u03b8 \u2190 C1\u03bbC1,\ni.e., reject if the minimum singular value of MS is less than C1\u03bbC1.\n4. Run the hypercontractivity tester (Prop. 3.5) on S' = {proj\u22a5w x : x \u2208 S and |\u27e8w, x\u27e9| \u2264 \u03c3},\ni.e., solve an appropriate SDP and reject if the solution is larger than a specified threshold. Otherwise, accept.\n\nFor part (a), we apply the Paley\u2013Zygmund inequality to the random variable Z = \u27e8v, x\u27e92 conditioned on |\u27e8w, x\u27e9| \u2264 \u03c3 and obtain\n\n$$P[\u27e8v, x\u27e92 \u2265 1 x\u2208S \u27e8v, x\u27e92 |\u27e8w, x\u27e9| \u2264 \u03c3 |\u27e8w, x\u27e9| \u2264 \u03c3] \u2265 \\frac{1}{4} \u00b7 (E_{x\u2208S}[\u27e8v, x\u27e92 | |\u27e8w, x\u27e9| \u2264 \u03c3])^2$$\n\nNote that since \u27e8v, w\u27e9 = 0, we have \u27e8v, x\u27e9 = \u27e8proj\u22a5w v, proj\u22a5w x\u27e9 (where \u2225v\u22252 = \u2225proj\u22a5w v\u22252). Therefore, since S has passed the spectral tester as well as the tester for the probability of lying within the strip |\u27e8w, x\u27e9| \u2264 \u03c3, we have that\n\n$$E[\u27e8v, x\u27e92 |\u27e8w, x\u27e9| \u2264 \u03c3] = E_{x\u2208S} [\u27e8v, x\u27e92 \u00b7 1\\{|\u27e8w, x\u27e9| \u2264 \u03c3}] \u2265 \\frac{1}{2C1\u03bb^2C1}$$\n\nMoreover, {x \u2208 S : |\u27e8w, x\u27e9| \u2264 \u03c3} has passed the hypercontractivity tester, and therefore, according to Proposition 3.5 we have\n\n$$E[\u27e8v, x\u27e94 |\u27e8w, x\u27e9| \u2264 \u03c3] \u2264 C1 \u00b7 \u03b34$$\n\nCombining the above inequalities we conclude the proof of part (a).\n\n9", "md": "2. If the solution has value larger than (C \u2212 1)\u03b34, then reject. Otherwise accept.\n\nThe computational complexity of the tester is poly(|S|, d, log 1 \u03b3), since the problem it solves can be written as a semidefinite program [Sho87, Par00, Nes00, Las01].\n\nIf the tester accepts S, then we know that the optimal solution of the relaxed problem is at most C\u03b34 and we also know that any solution of the initial problem (3.1) has value at most equal to the value of the relaxation. Therefore E[\u27e8v, x\u27e94] \u2264 C\u03b34, for any v \u2208 Sd\u22121.\n\nOn the other hand, if the true distribution D is \u03b3-Poincar\u00e9, then, with probability at least 1\u2212\u03b4, we have that the solution found in step 1 has, with probability at least 1\u2212\u03b4, value at most C\u2032\u03b34 for some universal constant C\u2032, due to Proposition 3.4. In order to ensure that the tester will accept with probability at least 1 \u2212 \u03b4, it suffices to pick C = C\u2032 + 1.\n\nWe are now ready to prove Lemma 3.2, by additionally making use of a spectral tester that accepts with high probability when the distribution of x is nice (similar to the spectral tester used for Lemma 3.1).\n\nProof of Lemma 3.2. The testing algorithm receives a set S \u2282 Rd, w \u2208 Sd\u22121, \u03b4 \u2208 (0, 1), \u03b3 > 0, \u03bb \u2265 1 and \u03c3 \u2264 2\u03bb and does the following for some sufficiently large C1 > 0:\n\n1. If Px\u2208S[|\u27e8w, x\u27e9| \u2264 \u03c3] > 2\u03c3 \u00b7 C1\u03bbC1, then reject.\n2. Compute the (d \u2212 1) \u00d7 (d \u2212 1) matrix MS as follows:\n\n$MS = E_{x\u2208S}[(proj\u22a5w x)(proj\u22a5w x)^T \u00b7 1\\{|\u27e8w, x\u27e9 \u2264 \u03c3|\\}]$\n3. Run the spectral tester of Proposition A.2 on MS given \u03b4 \u2190 \u03b4, \u03bb \u2190 C1\u03bbC1 and \u03b8 \u2190 C1\u03bbC1,\ni.e., reject if the minimum singular value of MS is less than C1\u03bbC1.\n4. Run the hypercontractivity tester (Prop. 3.5) on S' = {proj\u22a5w x : x \u2208 S and |\u27e8w, x\u27e9| \u2264 \u03c3},\ni.e., solve an appropriate SDP and reject if the solution is larger than a specified threshold. Otherwise, accept.\n\nFor part (a), we apply the Paley\u2013Zygmund inequality to the random variable Z = \u27e8v, x\u27e92 conditioned on |\u27e8w, x\u27e9| \u2264 \u03c3 and obtain\n\n$$P[\u27e8v, x\u27e92 \u2265 1 x\u2208S \u27e8v, x\u27e92 |\u27e8w, x\u27e9| \u2264 \u03c3 |\u27e8w, x\u27e9| \u2264 \u03c3] \u2265 \\frac{1}{4} \u00b7 (E_{x\u2208S}[\u27e8v, x\u27e92 | |\u27e8w, x\u27e9| \u2264 \u03c3])^2$$\n\nNote that since \u27e8v, w\u27e9 = 0, we have \u27e8v, x\u27e9 = \u27e8proj\u22a5w v, proj\u22a5w x\u27e9 (where \u2225v\u22252 = \u2225proj\u22a5w v\u22252). Therefore, since S has passed the spectral tester as well as the tester for the probability of lying within the strip |\u27e8w, x\u27e9| \u2264 \u03c3, we have that\n\n$$E[\u27e8v, x\u27e92 |\u27e8w, x\u27e9| \u2264 \u03c3] = E_{x\u2208S} [\u27e8v, x\u27e92 \u00b7 1\\{|\u27e8w, x\u27e9| \u2264 \u03c3}] \u2265 \\frac{1}{2C1\u03bb^2C1}$$\n\nMoreover, {x \u2208 S : |\u27e8w, x\u27e9| \u2264 \u03c3} has passed the hypercontractivity tester, and therefore, according to Proposition 3.5 we have\n\n$$E[\u27e8v, x\u27e94 |\u27e8w, x\u27e9| \u2264 \u03c3] \u2264 C1 \u00b7 \u03b34$$\n\nCombining the above inequalities we conclude the proof of part (a).\n\n9"}]}, {"page": 10, "text": "      For part (b), we assume that D is indeed \u03bb-nice and \u03b3-Poincar\u00b4e. We first use Proposition A.3 as\n                                                                                                        2\u03c3\nwell as a Hoeffding bound, to obtain that Px\u2208S[|\u27e8w, x\u27e9| \u2264                                    \u03c3] \u2208    [C\u2032\u03bbC\u2032 , 2\u03c3 \u00b7 C\u2032\u03bbC\u2032] with probability\nat least 1\u2212\u03b4/3 over S (since |S| is large enough), for some universal constant C\u2032 > 0. Then, we use                                                 4\u03c3\npart (ii) of Proposition A.3 to lower bound the minimum eigenvalue of MD = ED[MS] by                                                              C\u2032\u03bbC\u2032 .\nUsing part (iii) of Proposition A.3 to bound the second moment of each of the elements of MD,\n                                                                                     2\u03c3\nwe may use Proposition A.2 to obtain that MS \u2ab0                                     C\u2032\u03bbC\u2032 Id\u22121 (and our spectral test passes) with\nprobability at least 1 \u2212              \u03b4/3. It remains to show that the hypercontractivity tester will accept with\nprobability at least 1 \u2212              \u03b4/3 (since, then, the result follows from a union bound).\n      We acquire samples from the hypercontractivity tester through rejection sampling (we keep only\n                                                                                                                                              2\u03c3\nthe samples within the strip). Since the probability of falling inside the strip is at least                                                C\u2032\u03bbC\u2032 , the\n                                                                                  |S|\u03c3\nnumber of samples we will keep is at least |S\u2032| \u2265                                C\u2032\u2032\u03bbC\u2032 , for some large enough constant C\u2032\u2032 > 0\n(due to Chernoff bound) and with probability at least 1\u2212\u03b4/6. We now apply Lemma A.1 to obtain\nthat the distribution of proj                 \u22a5w x conditioned on the strip |\u27e8w, x\u27e9| \u2264                          \u03c3 is \u03b3-Poincar\u00b4e, since D is\nalso \u03b3-Poincar\u00b4e. Hence, the hypercontractivity tester accepts with probability at least 1 \u2212                                                   \u03b4/6 due\nto Proposition 3.5.\n4       Universal Tester-Learners for Halfspaces\nIn this section, we present our main result on universally testable learning of halfspaces.\nTheorem 4.1 (Efficient Universal Tester-Learner for Halfspaces). Let DXY be any distribution over\nRd \u00d7{\u00b11}. Let C be the class of origin centered halfspaces in Rd. Then, for any \u03bb \u2265                                                 1, \u03b3 > 0, \u03f5 > 0\nand \u03b4 \u2208      (0, 1), there exists an universal tester-learner for C w.r.t. the class of \u03bb-nice and \u03b3-Poincar\u00b4e\nmarginals up to error poly(\u03bb) \u00b7 (1 + \u03b34) \u00b7 opt + \u03f5, where opt = minw\u2208Sd\u22121 PDXY[y \u0338= sign(\u27e8w, x\u27e9)],\nand error probability at most \u03b4, using a number of samples and running time poly(d, \u03bb, \u03b3, 1                                                   \u03f5 , log 1\u03b4 ).\n      Moreover, if the noise is Massart with given rate \u03b7 < 1/2, then the algorithm achieves error\nopt + \u03f5 with time and sample complexity poly(d, \u03bb, \u03b3, 1                                    1\n                                                                                     \u03f5 , 1\u22122\u03b7, log 1   \u03b4 ).\n      Our proof follows a surrogate loss minimization approach that has been used for classical learn-\ning of noisy halfspaces [DKTZ20a, DKTZ20b] as well as classical (non-universal) testable learning\n[GKSV23]. In particular, the algorithm runs Projected Stochastic Gradient Descent (see A.4) on a\nsurrogate loss whose stationary points are shown to be close to optimum parameter vectors under\ncertain distributional assumptions. In the regular testable learning setting, given a stationary point,\nthe above property can be tested with respect to any (fixed and known) target strongly log-concave\nmarginal as shown by [GKSV23]. For such a stationary point, more tests are used in order to\nensure bounds on local halfspace disagreement. We provide some delicate refinements of the proofs\nin [GKSV23] that enable us to substitute their testers with the universal testers we designed in\nSection 3. We use the following surrogate loss function which was also used in [GKSV23].\n                                           L\u03c3(w; DXY) =                   E         \u2113\u03c3     \u2212  y  \u27e8w, x\u27e9        ,                                    (4.1)\n                                                                   (x,y)\u223cDXY                     \u2225w\u22252\nIn Equation (4.1), the function \u2113\u03c3 is a smoothed version of the step function defined as follows.\nProposition 4.2 (Proposition 4.2 of [GKSV23]). There is a universal constant C > 0, such that\nfor any \u03c3 > 0, there exists a continuously differentiable function \u2113\u03c3 : R \u2192                                            [0, 1] with the following\nproperties.\n     1. For any t \u2208          [\u2212\u03c3/6, \u03c3/6], \u2113\u03c3(t) = 1           2 + t \u03c3.      10", "md": "# Math Equations and Text\n\n## For part (b), we assume that D is indeed \u03bb-nice and \u03b3-Poincar&eacute;. We first use Proposition A.3 as well as a Hoeffding bound, to obtain that $$P_{x\\in S}[|\\langle w, x\\rangle| \\leq 2\\sigma] \\in [C'\\lambda C', 2\\sigma \\cdot C'\\lambda C']$$ with probability at least 1\u2212\u03b4/3 over S (since |S| is large enough), for some universal constant C' > 0. Then, we use $$4\\sigma$$ part (ii) of Proposition A.3 to lower bound the minimum eigenvalue of MD = ED[MS] by $$C'\\lambda C'$$. Using part (iii) of Proposition A.3 to bound the second moment of each of the elements of MD, we may use Proposition A.2 to obtain that MS \u2ab0 $$C'\\lambda C' Id-1$$ (and our spectral test passes) with probability at least 1 \u2212 $$\\delta/3$$. It remains to show that the hypercontractivity tester will accept with probability at least 1 \u2212 $$\\delta/3$$ (since, then, the result follows from a union bound).\n\n## We acquire samples from the hypercontractivity tester through rejection sampling (we keep only the samples within the strip). Since the probability of falling inside the strip is at least $$\\frac{2\\sigma}{C'\\lambda C'}$$, the number of samples we will keep is at least $$|S'|\\geq \\frac{|S|\\sigma}{C''\\lambda C'}$$, for some large enough constant C'' > 0 (due to Chernoff bound) and with probability at least 1\u2212\u03b4/6. We now apply Lemma A.1 to obtain that the distribution of $$\\text{proj}_{\\perp w} x$$ conditioned on the strip $$|\\langle w, x\\rangle| \\leq \\sigma$$ is \u03b3-Poincar&eacute;, since D is also \u03b3-Poincar&eacute;. Hence, the hypercontractivity tester accepts with probability at least 1 \u2212 $$\\delta/6$$ due to Proposition 3.5.\n\n## Universal Tester-Learners for Halfspaces\n\n## In this section, we present our main result on universally testable learning of halfspaces.\n\n## Theorem 4.1 (Efficient Universal Tester-Learner for Halfspaces). Let DXY be any distribution over $$\\mathbb{R}^d \\times \\{ \\pm 1 \\}$$. Let C be the class of origin centered halfspaces in $$\\mathbb{R}^d$$. Then, for any $$\\lambda \\geq 1, \\gamma > 0, \\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, there exists an universal tester-learner for C w.r.t. the class of \u03bb-nice and \u03b3-Poincar&eacute; marginals up to error $$\\text{poly}(\\lambda) \\cdot (1 + \\gamma^4) \\cdot \\text{opt} + \\epsilon$$, where $$\\text{opt} = \\min_{w\\in S^{d-1}} P_{DXY}[y \\neq \\text{sign}(\\langle w, x\\rangle)]$$, and error probability at most $$\\delta$$, using a number of samples and running time $$\\text{poly}(d, \\lambda, \\gamma, \\frac{1}{\\epsilon}, \\log \\frac{1}{\\delta})$$. Moreover, if the noise is Massart with given rate $$\\eta < \\frac{1}{2}$$, then the algorithm achieves error $$\\text{opt} + \\epsilon$$ with time and sample complexity $$\\text{poly}(d, \\lambda, \\gamma, \\frac{1}{\\epsilon}, 1-2\\eta, \\log \\frac{1}{\\delta})$$.\n\n## Our proof follows a surrogate loss minimization approach that has been used for classical learning of noisy halfspaces [DKTZ20a, DKTZ20b] as well as classical (non-universal) testable learning [GKSV23]. In particular, the algorithm runs Projected Stochastic Gradient Descent (see A.4) on a surrogate loss whose stationary points are shown to be close to optimum parameter vectors under certain distributional assumptions. In the regular testable learning setting, given a stationary point, the above property can be tested with respect to any (fixed and known) target strongly log-concave marginal as shown by [GKSV23]. For such a stationary point, more tests are used in order to ensure bounds on local halfspace disagreement. We provide some delicate refinements of the proofs in [GKSV23] that enable us to substitute their testers with the universal testers we designed in Section 3. We use the following surrogate loss function which was also used in [GKSV23].\n\n## $$L_{\\sigma}(w; DXY) = \\mathbb{E}_{(x,y)\\sim DXY} \\left[ \\ell_{\\sigma} - y \\langle w, x\\rangle \\right] / \\left\\| w \\right\\|^2$$\n\n## In Equation (4.1), the function $$\\ell_{\\sigma}$$ is a smoothed version of the step function defined as follows.\n\n## Proposition 4.2 (Proposition 4.2 of [GKSV23]). There is a universal constant C > 0, such that for any $$\\sigma > 0$$, there exists a continuously differentiable function $$\\ell_{\\sigma} : \\mathbb{R} \\rightarrow [0, 1]$$ with the following properties.\n\n1. For any $t \\in [-\\sigma/6, \\sigma/6]$, $\\ell_{\\sigma}(t) = 1 - 2 + t \\sigma$.\n2. 10", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "For part (b), we assume that D is indeed \u03bb-nice and \u03b3-Poincar&eacute;. We first use Proposition A.3 as well as a Hoeffding bound, to obtain that $$P_{x\\in S}[|\\langle w, x\\rangle| \\leq 2\\sigma] \\in [C'\\lambda C', 2\\sigma \\cdot C'\\lambda C']$$ with probability at least 1\u2212\u03b4/3 over S (since |S| is large enough), for some universal constant C' > 0. Then, we use $$4\\sigma$$ part (ii) of Proposition A.3 to lower bound the minimum eigenvalue of MD = ED[MS] by $$C'\\lambda C'$$. Using part (iii) of Proposition A.3 to bound the second moment of each of the elements of MD, we may use Proposition A.2 to obtain that MS \u2ab0 $$C'\\lambda C' Id-1$$ (and our spectral test passes) with probability at least 1 \u2212 $$\\delta/3$$. It remains to show that the hypercontractivity tester will accept with probability at least 1 \u2212 $$\\delta/3$$ (since, then, the result follows from a union bound).", "md": "## For part (b), we assume that D is indeed \u03bb-nice and \u03b3-Poincar&eacute;. We first use Proposition A.3 as well as a Hoeffding bound, to obtain that $$P_{x\\in S}[|\\langle w, x\\rangle| \\leq 2\\sigma] \\in [C'\\lambda C', 2\\sigma \\cdot C'\\lambda C']$$ with probability at least 1\u2212\u03b4/3 over S (since |S| is large enough), for some universal constant C' > 0. Then, we use $$4\\sigma$$ part (ii) of Proposition A.3 to lower bound the minimum eigenvalue of MD = ED[MS] by $$C'\\lambda C'$$. Using part (iii) of Proposition A.3 to bound the second moment of each of the elements of MD, we may use Proposition A.2 to obtain that MS \u2ab0 $$C'\\lambda C' Id-1$$ (and our spectral test passes) with probability at least 1 \u2212 $$\\delta/3$$. It remains to show that the hypercontractivity tester will accept with probability at least 1 \u2212 $$\\delta/3$$ (since, then, the result follows from a union bound)."}, {"type": "heading", "lvl": 2, "value": "We acquire samples from the hypercontractivity tester through rejection sampling (we keep only the samples within the strip). Since the probability of falling inside the strip is at least $$\\frac{2\\sigma}{C'\\lambda C'}$$, the number of samples we will keep is at least $$|S'|\\geq \\frac{|S|\\sigma}{C''\\lambda C'}$$, for some large enough constant C'' > 0 (due to Chernoff bound) and with probability at least 1\u2212\u03b4/6. We now apply Lemma A.1 to obtain that the distribution of $$\\text{proj}_{\\perp w} x$$ conditioned on the strip $$|\\langle w, x\\rangle| \\leq \\sigma$$ is \u03b3-Poincar&eacute;, since D is also \u03b3-Poincar&eacute;. Hence, the hypercontractivity tester accepts with probability at least 1 \u2212 $$\\delta/6$$ due to Proposition 3.5.", "md": "## We acquire samples from the hypercontractivity tester through rejection sampling (we keep only the samples within the strip). Since the probability of falling inside the strip is at least $$\\frac{2\\sigma}{C'\\lambda C'}$$, the number of samples we will keep is at least $$|S'|\\geq \\frac{|S|\\sigma}{C''\\lambda C'}$$, for some large enough constant C'' > 0 (due to Chernoff bound) and with probability at least 1\u2212\u03b4/6. We now apply Lemma A.1 to obtain that the distribution of $$\\text{proj}_{\\perp w} x$$ conditioned on the strip $$|\\langle w, x\\rangle| \\leq \\sigma$$ is \u03b3-Poincar&eacute;, since D is also \u03b3-Poincar&eacute;. Hence, the hypercontractivity tester accepts with probability at least 1 \u2212 $$\\delta/6$$ due to Proposition 3.5."}, {"type": "heading", "lvl": 2, "value": "Universal Tester-Learners for Halfspaces", "md": "## Universal Tester-Learners for Halfspaces"}, {"type": "heading", "lvl": 2, "value": "In this section, we present our main result on universally testable learning of halfspaces.", "md": "## In this section, we present our main result on universally testable learning of halfspaces."}, {"type": "heading", "lvl": 2, "value": "Theorem 4.1 (Efficient Universal Tester-Learner for Halfspaces). Let DXY be any distribution over $$\\mathbb{R}^d \\times \\{ \\pm 1 \\}$$. Let C be the class of origin centered halfspaces in $$\\mathbb{R}^d$$. Then, for any $$\\lambda \\geq 1, \\gamma > 0, \\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, there exists an universal tester-learner for C w.r.t. the class of \u03bb-nice and \u03b3-Poincar&eacute; marginals up to error $$\\text{poly}(\\lambda) \\cdot (1 + \\gamma^4) \\cdot \\text{opt} + \\epsilon$$, where $$\\text{opt} = \\min_{w\\in S^{d-1}} P_{DXY}[y \\neq \\text{sign}(\\langle w, x\\rangle)]$$, and error probability at most $$\\delta$$, using a number of samples and running time $$\\text{poly}(d, \\lambda, \\gamma, \\frac{1}{\\epsilon}, \\log \\frac{1}{\\delta})$$. Moreover, if the noise is Massart with given rate $$\\eta < \\frac{1}{2}$$, then the algorithm achieves error $$\\text{opt} + \\epsilon$$ with time and sample complexity $$\\text{poly}(d, \\lambda, \\gamma, \\frac{1}{\\epsilon}, 1-2\\eta, \\log \\frac{1}{\\delta})$$.", "md": "## Theorem 4.1 (Efficient Universal Tester-Learner for Halfspaces). Let DXY be any distribution over $$\\mathbb{R}^d \\times \\{ \\pm 1 \\}$$. Let C be the class of origin centered halfspaces in $$\\mathbb{R}^d$$. Then, for any $$\\lambda \\geq 1, \\gamma > 0, \\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, there exists an universal tester-learner for C w.r.t. the class of \u03bb-nice and \u03b3-Poincar&eacute; marginals up to error $$\\text{poly}(\\lambda) \\cdot (1 + \\gamma^4) \\cdot \\text{opt} + \\epsilon$$, where $$\\text{opt} = \\min_{w\\in S^{d-1}} P_{DXY}[y \\neq \\text{sign}(\\langle w, x\\rangle)]$$, and error probability at most $$\\delta$$, using a number of samples and running time $$\\text{poly}(d, \\lambda, \\gamma, \\frac{1}{\\epsilon}, \\log \\frac{1}{\\delta})$$. Moreover, if the noise is Massart with given rate $$\\eta < \\frac{1}{2}$$, then the algorithm achieves error $$\\text{opt} + \\epsilon$$ with time and sample complexity $$\\text{poly}(d, \\lambda, \\gamma, \\frac{1}{\\epsilon}, 1-2\\eta, \\log \\frac{1}{\\delta})$$."}, {"type": "heading", "lvl": 2, "value": "Our proof follows a surrogate loss minimization approach that has been used for classical learning of noisy halfspaces [DKTZ20a, DKTZ20b] as well as classical (non-universal) testable learning [GKSV23]. In particular, the algorithm runs Projected Stochastic Gradient Descent (see A.4) on a surrogate loss whose stationary points are shown to be close to optimum parameter vectors under certain distributional assumptions. In the regular testable learning setting, given a stationary point, the above property can be tested with respect to any (fixed and known) target strongly log-concave marginal as shown by [GKSV23]. For such a stationary point, more tests are used in order to ensure bounds on local halfspace disagreement. We provide some delicate refinements of the proofs in [GKSV23] that enable us to substitute their testers with the universal testers we designed in Section 3. We use the following surrogate loss function which was also used in [GKSV23].", "md": "## Our proof follows a surrogate loss minimization approach that has been used for classical learning of noisy halfspaces [DKTZ20a, DKTZ20b] as well as classical (non-universal) testable learning [GKSV23]. In particular, the algorithm runs Projected Stochastic Gradient Descent (see A.4) on a surrogate loss whose stationary points are shown to be close to optimum parameter vectors under certain distributional assumptions. In the regular testable learning setting, given a stationary point, the above property can be tested with respect to any (fixed and known) target strongly log-concave marginal as shown by [GKSV23]. For such a stationary point, more tests are used in order to ensure bounds on local halfspace disagreement. We provide some delicate refinements of the proofs in [GKSV23] that enable us to substitute their testers with the universal testers we designed in Section 3. We use the following surrogate loss function which was also used in [GKSV23]."}, {"type": "heading", "lvl": 2, "value": "$$L_{\\sigma}(w; DXY) = \\mathbb{E}_{(x,y)\\sim DXY} \\left[ \\ell_{\\sigma} - y \\langle w, x\\rangle \\right] / \\left\\| w \\right\\|^2$$", "md": "## $$L_{\\sigma}(w; DXY) = \\mathbb{E}_{(x,y)\\sim DXY} \\left[ \\ell_{\\sigma} - y \\langle w, x\\rangle \\right] / \\left\\| w \\right\\|^2$$"}, {"type": "heading", "lvl": 2, "value": "In Equation (4.1), the function $$\\ell_{\\sigma}$$ is a smoothed version of the step function defined as follows.", "md": "## In Equation (4.1), the function $$\\ell_{\\sigma}$$ is a smoothed version of the step function defined as follows."}, {"type": "heading", "lvl": 2, "value": "Proposition 4.2 (Proposition 4.2 of [GKSV23]). There is a universal constant C > 0, such that for any $$\\sigma > 0$$, there exists a continuously differentiable function $$\\ell_{\\sigma} : \\mathbb{R} \\rightarrow [0, 1]$$ with the following properties.", "md": "## Proposition 4.2 (Proposition 4.2 of [GKSV23]). There is a universal constant C > 0, such that for any $$\\sigma > 0$$, there exists a continuously differentiable function $$\\ell_{\\sigma} : \\mathbb{R} \\rightarrow [0, 1]$$ with the following properties."}, {"type": "text", "value": "1. For any $t \\in [-\\sigma/6, \\sigma/6]$, $\\ell_{\\sigma}(t) = 1 - 2 + t \\sigma$.\n2. 10", "md": "1. For any $t \\in [-\\sigma/6, \\sigma/6]$, $\\ell_{\\sigma}(t) = 1 - 2 + t \\sigma$.\n2. 10"}]}, {"page": 11, "text": "    2. For any t > \u03c3/2, \u2113\u03c3(t) = 1 and for any t < \u2212\u03c3/2, \u2113\u03c3(t) = 0.\n    3. For any t \u2208         R, \u2113\u2032  \u03c3(t) \u2208   [0, C/\u03c3], \u2113\u2032    \u03c3(t) = \u2113\u2032  \u03c3(\u2212t) and |\u2113\u2032\u2032     \u03c3(t)| \u2264    C/\u03c32.\n      In order to analyze the properties of the stationary points of the surrogate loss, we provide\nthe following refinement of results implicit in [GKSV23, DKTZ20a, DKTZ20b].                                                   We show that\nthe gradient of the surrogate loss is lower bounded by the difference between certain quantities\nthat are controlled by the marginal distribution (see Figure 1). We stress that we do not use any\nassumptions for the marginal distribution in this step. Prior work included similar bounds, but\nthe corresponding quantities were slightly different. We need to be more precise and provide the\nfollowing result, whose proof is based on two-dimensional geometry and is deferred to Appendix C.\nFigure 1: The Gaussian mass in each of the regions labelled A1 and A2 is proportional to the\ncorresponding term appearing in the statement of Proposition 4.3. As \u03c3 tends to 0, the Gaussian\nmass of region\u03c3A2 shrinks faster than the one of region A1, since both the height (\u03c3) and the\nwidth (     tan \u03b8) of A2 are proportional to \u03c3, while the width of A1 is not affected (the height is \u03c3/3).\nLemma 4.4 demonstrates that a similar property is universally testable under any nice Poincar\u00b4e\ndistribution.\nProposition 4.3 (Modification from [GKSV23, DKTZ20a, DKTZ20b]). For a distribution DXY\nover Rd \u00d7 {\u00b11} let opt be the minimum error achieved by some origin-centered halfspace and\nw\u2217    \u2208  Sd\u22121 a corresponding vector. Consider L\u03c3 as in Equation (4.1) for \u03c3 > 0 and let \u03b7 < 1/2. Let\nw \u2208    Sd\u22121 with \u2221(w, w\u2217) = \u03b8 < \u03c02 and v \u2208                     span(w, w\u2217) such that \u27e8v, w\u27e9                = 0 and \u27e8v, w\u2217\u27e9         < 0. Then,\n                                                                            \u03c3\nfor some universal constant C > 0 and any \u03b1 \u2265                            2 tan \u03b8 we have \u2225\u2207wL\u03c3(w; DXY)\u22252 \u2265                      A1 \u2212A2 \u2212A3,\nwhere\n               A1 =        \u03b1           |\u27e8v, x\u27e9| \u2265     \u03b1 and |\u27e8w, x\u27e9| \u2264            \u03c3\n               A2 =     C \u00b7C\u03c3 \u00b7 P      |\u27e8w, x\u27e9| \u2264      \u03c3      and A3 = C          6               E  \u27e8v, x\u27e92 \u00b7 1{|\u27e8w,x\u27e9|\u2264\u03c32 }\n                        tan \u03b8 \u00b7 P                      2                       \u03c3 \u00b7 \u221aopt \u00b7\nMoreover, if the noise is Massart with rate \u03b7, then \u2225\u2207wL\u03c3(w; DXY)\u22252 \u2265                                          (1 \u2212   2\u03b7)A1 \u2212      A2.\n      If the marginal distribution is nice, then the quantities A1, A2 and A3 are such that \u03c3 can\nbe chosen accordingly so that stationary points of the surrogate loss (or their inverses) are close\nto some optimum vector (see Proposition A.3 for properties of nice distributions). We use some\nsimple tests (e.g., estimate the probability of falling in a strip, P[|\u27e8w, x\u27e9| \u2264                                    \u03c3/2] and appropriate\nspectral testers) as well as our universal tester for weak anti-concentration (see 3.2) to establish\n                                                                        11", "md": "2. For any \\( t > \\frac{\\sigma}{2} \\), \\( \\ell_{\\sigma}(t) = 1 \\) and for any \\( t < -\\frac{\\sigma}{2} \\), \\( \\ell_{\\sigma}(t) = 0 \\).\n\n3. For any \\( t \\in \\mathbb{R} \\), \\( \\ell'_{\\sigma}(t) \\in [0, \\frac{C}{\\sigma}] \\), \\( \\ell'_{\\sigma}(t) = \\ell'_{\\sigma}(-t) \\) and \\( |\\ell''_{\\sigma}(t)| \\leq \\frac{C}{\\sigma^2} \\).\n\nIn order to analyze the properties of the stationary points of the surrogate loss, we provide the following refinement of results implicit in [GKSV23, DKTZ20a, DKTZ20b]. We show that the gradient of the surrogate loss is lower bounded by the difference between certain quantities that are controlled by the marginal distribution (see Figure 1). We stress that we do not use any assumptions for the marginal distribution in this step. Prior work included similar bounds, but the corresponding quantities were slightly different. We need to be more precise and provide the following result, whose proof is based on two-dimensional geometry and is deferred to Appendix C.\n\nFigure 1: The Gaussian mass in each of the regions labelled A1 and A2 is proportional to the corresponding term appearing in the statement of Proposition 4.3. As \\( \\sigma \\) tends to 0, the Gaussian mass of region A2 shrinks faster than the one of region A1, since both the height (\\( \\sigma \\)) and the width (\\( \\tan \\theta \\)) of A2 are proportional to \\( \\sigma \\), while the width of A1 is not affected (the height is \\( \\frac{\\sigma}{3} \\)). Lemma 4.4 demonstrates that a similar property is universally testable under any nice Poincar\u00e9 distribution.\n\nProposition 4.3 (Modification from [GKSV23, DKTZ20a, DKTZ20b]): For a distribution \\( D_{XY} \\) over \\( \\mathbb{R}^d \\times \\{ \\pm 1 \\} \\) let opt be the minimum error achieved by some origin-centered halfspace and \\( w^* \\in S^{d-1} \\) a corresponding vector. Consider \\( L_{\\sigma} \\) as in Equation (4.1) for \\( \\sigma > 0 \\) and let \\( \\eta < \\frac{1}{2} \\). Let \\( w \\in S^{d-1} \\) with \\( \\angle(w, w^*) = \\theta < \\frac{\\pi}{2} \\) and \\( v \\in \\text{span}(w, w^*) \\) such that \\( \\langle v, w \\rangle = 0 \\) and \\( \\langle v, w^* \\rangle < 0 \\). Then, for some universal constant \\( C > 0 \\) and any \\( \\alpha \\geq 2 \\tan \\theta \\) we have \\( \\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq A1 - A2 - A3 \\), where\n\n$$\n\\begin{align*}\nA1 & = \\alpha \\cdot \\mathbb{1} \\{ | \\langle v, x \\rangle | \\geq \\alpha \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\} \\\\\nA2 & = C \\cdot C_{\\sigma} \\cdot P \\{ | \\langle w, x \\rangle | \\leq \\sigma \\} \\text{ and } A3 = C \\cdot \\frac{6}{\\tan \\theta} \\cdot P \\{ | \\langle w, x \\rangle | \\leq \\sigma^2 \\} \\cdot \\sigma \\cdot \\sqrt{\\text{opt}}\n\\end{align*}\n$$\nMoreover, if the noise is Massart with rate \\( \\eta \\), then \\( \\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq (1 - 2\\eta)A1 - A2 \\).\n\nIf the marginal distribution is nice, then the quantities A1, A2 and A3 are such that \\( \\sigma \\) can be chosen accordingly so that stationary points of the surrogate loss (or their inverses) are close to some optimum vector (see Proposition A.3 for properties of nice distributions). We use some simple tests (e.g., estimate the probability of falling in a strip, \\( P[| \\langle w, x \\rangle | \\leq \\frac{\\sigma}{2}] \\) and appropriate spectral testers) as well as our universal tester for weak anti-concentration (see 3.2) to establish\n\n11", "images": [{"name": "page-11-0.jpg", "height": 246, "width": 281, "x": 165, "y": 186}], "items": [{"type": "text", "value": "2. For any \\( t > \\frac{\\sigma}{2} \\), \\( \\ell_{\\sigma}(t) = 1 \\) and for any \\( t < -\\frac{\\sigma}{2} \\), \\( \\ell_{\\sigma}(t) = 0 \\).\n\n3. For any \\( t \\in \\mathbb{R} \\), \\( \\ell'_{\\sigma}(t) \\in [0, \\frac{C}{\\sigma}] \\), \\( \\ell'_{\\sigma}(t) = \\ell'_{\\sigma}(-t) \\) and \\( |\\ell''_{\\sigma}(t)| \\leq \\frac{C}{\\sigma^2} \\).\n\nIn order to analyze the properties of the stationary points of the surrogate loss, we provide the following refinement of results implicit in [GKSV23, DKTZ20a, DKTZ20b]. We show that the gradient of the surrogate loss is lower bounded by the difference between certain quantities that are controlled by the marginal distribution (see Figure 1). We stress that we do not use any assumptions for the marginal distribution in this step. Prior work included similar bounds, but the corresponding quantities were slightly different. We need to be more precise and provide the following result, whose proof is based on two-dimensional geometry and is deferred to Appendix C.\n\nFigure 1: The Gaussian mass in each of the regions labelled A1 and A2 is proportional to the corresponding term appearing in the statement of Proposition 4.3. As \\( \\sigma \\) tends to 0, the Gaussian mass of region A2 shrinks faster than the one of region A1, since both the height (\\( \\sigma \\)) and the width (\\( \\tan \\theta \\)) of A2 are proportional to \\( \\sigma \\), while the width of A1 is not affected (the height is \\( \\frac{\\sigma}{3} \\)). Lemma 4.4 demonstrates that a similar property is universally testable under any nice Poincar\u00e9 distribution.\n\nProposition 4.3 (Modification from [GKSV23, DKTZ20a, DKTZ20b]): For a distribution \\( D_{XY} \\) over \\( \\mathbb{R}^d \\times \\{ \\pm 1 \\} \\) let opt be the minimum error achieved by some origin-centered halfspace and \\( w^* \\in S^{d-1} \\) a corresponding vector. Consider \\( L_{\\sigma} \\) as in Equation (4.1) for \\( \\sigma > 0 \\) and let \\( \\eta < \\frac{1}{2} \\). Let \\( w \\in S^{d-1} \\) with \\( \\angle(w, w^*) = \\theta < \\frac{\\pi}{2} \\) and \\( v \\in \\text{span}(w, w^*) \\) such that \\( \\langle v, w \\rangle = 0 \\) and \\( \\langle v, w^* \\rangle < 0 \\). Then, for some universal constant \\( C > 0 \\) and any \\( \\alpha \\geq 2 \\tan \\theta \\) we have \\( \\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq A1 - A2 - A3 \\), where\n\n$$\n\\begin{align*}\nA1 & = \\alpha \\cdot \\mathbb{1} \\{ | \\langle v, x \\rangle | \\geq \\alpha \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\} \\\\\nA2 & = C \\cdot C_{\\sigma} \\cdot P \\{ | \\langle w, x \\rangle | \\leq \\sigma \\} \\text{ and } A3 = C \\cdot \\frac{6}{\\tan \\theta} \\cdot P \\{ | \\langle w, x \\rangle | \\leq \\sigma^2 \\} \\cdot \\sigma \\cdot \\sqrt{\\text{opt}}\n\\end{align*}\n$$\nMoreover, if the noise is Massart with rate \\( \\eta \\), then \\( \\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq (1 - 2\\eta)A1 - A2 \\).\n\nIf the marginal distribution is nice, then the quantities A1, A2 and A3 are such that \\( \\sigma \\) can be chosen accordingly so that stationary points of the surrogate loss (or their inverses) are close to some optimum vector (see Proposition A.3 for properties of nice distributions). We use some simple tests (e.g., estimate the probability of falling in a strip, \\( P[| \\langle w, x \\rangle | \\leq \\frac{\\sigma}{2}] \\) and appropriate spectral testers) as well as our universal tester for weak anti-concentration (see 3.2) to establish\n\n11", "md": "2. For any \\( t > \\frac{\\sigma}{2} \\), \\( \\ell_{\\sigma}(t) = 1 \\) and for any \\( t < -\\frac{\\sigma}{2} \\), \\( \\ell_{\\sigma}(t) = 0 \\).\n\n3. For any \\( t \\in \\mathbb{R} \\), \\( \\ell'_{\\sigma}(t) \\in [0, \\frac{C}{\\sigma}] \\), \\( \\ell'_{\\sigma}(t) = \\ell'_{\\sigma}(-t) \\) and \\( |\\ell''_{\\sigma}(t)| \\leq \\frac{C}{\\sigma^2} \\).\n\nIn order to analyze the properties of the stationary points of the surrogate loss, we provide the following refinement of results implicit in [GKSV23, DKTZ20a, DKTZ20b]. We show that the gradient of the surrogate loss is lower bounded by the difference between certain quantities that are controlled by the marginal distribution (see Figure 1). We stress that we do not use any assumptions for the marginal distribution in this step. Prior work included similar bounds, but the corresponding quantities were slightly different. We need to be more precise and provide the following result, whose proof is based on two-dimensional geometry and is deferred to Appendix C.\n\nFigure 1: The Gaussian mass in each of the regions labelled A1 and A2 is proportional to the corresponding term appearing in the statement of Proposition 4.3. As \\( \\sigma \\) tends to 0, the Gaussian mass of region A2 shrinks faster than the one of region A1, since both the height (\\( \\sigma \\)) and the width (\\( \\tan \\theta \\)) of A2 are proportional to \\( \\sigma \\), while the width of A1 is not affected (the height is \\( \\frac{\\sigma}{3} \\)). Lemma 4.4 demonstrates that a similar property is universally testable under any nice Poincar\u00e9 distribution.\n\nProposition 4.3 (Modification from [GKSV23, DKTZ20a, DKTZ20b]): For a distribution \\( D_{XY} \\) over \\( \\mathbb{R}^d \\times \\{ \\pm 1 \\} \\) let opt be the minimum error achieved by some origin-centered halfspace and \\( w^* \\in S^{d-1} \\) a corresponding vector. Consider \\( L_{\\sigma} \\) as in Equation (4.1) for \\( \\sigma > 0 \\) and let \\( \\eta < \\frac{1}{2} \\). Let \\( w \\in S^{d-1} \\) with \\( \\angle(w, w^*) = \\theta < \\frac{\\pi}{2} \\) and \\( v \\in \\text{span}(w, w^*) \\) such that \\( \\langle v, w \\rangle = 0 \\) and \\( \\langle v, w^* \\rangle < 0 \\). Then, for some universal constant \\( C > 0 \\) and any \\( \\alpha \\geq 2 \\tan \\theta \\) we have \\( \\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq A1 - A2 - A3 \\), where\n\n$$\n\\begin{align*}\nA1 & = \\alpha \\cdot \\mathbb{1} \\{ | \\langle v, x \\rangle | \\geq \\alpha \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\} \\\\\nA2 & = C \\cdot C_{\\sigma} \\cdot P \\{ | \\langle w, x \\rangle | \\leq \\sigma \\} \\text{ and } A3 = C \\cdot \\frac{6}{\\tan \\theta} \\cdot P \\{ | \\langle w, x \\rangle | \\leq \\sigma^2 \\} \\cdot \\sigma \\cdot \\sqrt{\\text{opt}}\n\\end{align*}\n$$\nMoreover, if the noise is Massart with rate \\( \\eta \\), then \\( \\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq (1 - 2\\eta)A1 - A2 \\).\n\nIf the marginal distribution is nice, then the quantities A1, A2 and A3 are such that \\( \\sigma \\) can be chosen accordingly so that stationary points of the surrogate loss (or their inverses) are close to some optimum vector (see Proposition A.3 for properties of nice distributions). We use some simple tests (e.g., estimate the probability of falling in a strip, \\( P[| \\langle w, x \\rangle | \\leq \\frac{\\sigma}{2}] \\) and appropriate spectral testers) as well as our universal tester for weak anti-concentration (see 3.2) to establish\n\n11"}]}, {"page": 12, "text": "bounds on quantities A1, A2 and A3 which ensure that the desired property holds for a given vector\nw, under no distributional assumptions. The tester in the following result universally accepts nice\ndistributions with bounded Poincar\u00b4e parameter.\nLemma 4.4 (Universally Testable Structure of Surrogate Loss). Let DXY be any distribution over\nRd \u00d7 {\u00b11}. Consider L\u03c3 as in Equation (4.1). Then, there is a universal constant C > 0 and a                                    1\ntester that given a unit vector w \u2208                     Rd, \u03b4 \u2208      (0, 1), \u03b7 < 1/2, \u03b3 > 0, \u03bb \u2265                  1, \u03c3 \u2264      C\u03bbC and a set\nS of i.i.d. samples from DXY with size at least C \u00b7 d4                        \u03c32\u03b4 log(d)\u03bbC, runs in time poly(d, \u03bb, 1                 \u03c3, 1\u03b4 ) and\nsatisfies the following specifications\n  (a) If the tester accepts S, then, the following statements are true for the minimum error opt                                                  S\n         achieved by some origin-centered halfspace on S and the optimum vector w\u2217                                        S \u2208   Sd\u22121\n                                                                                                                           1\u22122\u03b7\n             \u2022 If the noise is Massart with associated rate \u03b7 and \u2225\u2207wL\u03c3(w; S)\u22252 \u2264                                         C\u03bbC\u03b34 then either\n                \u2221(w, w\u2217     S) \u2264    C\u03bbC(1+\u03b34)      \u00b7 \u03c3 or \u2221(\u2212w, w\u2217        S) \u2264    C\u03bbC(1+\u03b34)      \u00b7 \u03c3.\n                                        1\u22122\u03b7                                   \u03c3      1\u22122\u03b7                                   1\n             \u2022 If the noise is adversarial with opt                   S \u2264    C\u03bbC and \u2225\u2207wL\u03c3(w; S)\u22252 <                     C\u03bbC\u03b34 then either\n                \u2221(w, w\u2217     S) \u2264   C\u03bbC(1 + \u03b34) \u00b7 \u03c3 or \u2221(\u2212w, w\u2217                 S) \u2264   C\u03bbC(1 + \u03b34) \u00b7 \u03c3.\n   (b) If the marginal DX is \u03bb-nice and \u03b3-Poincar\u00b4e, then the tester accepts S with probability at\n         least 1 \u2212     \u03b4.                                                                                                          1\nProof. The testing algorithm receives w \u2208                        Sd\u22121, \u03b4 \u2208     (0, 1), \u03b7 < 1/2, \u03b3 > 0, \u03bb \u2265             1, \u03c3 \u2264     2\u03bb and a set\nS \u2282    Rd \u00d7 {\u00b11} and does the following for some sufficiently large C1 > 0\n    1. If P(x,y)\u2208S[|\u27e8w, x\u27e9| \u2264            \u03c36 ] \u2264     \u03c3                                      2 ] > \u03c3 \u00b7 C1\u03bbC1, then reject.\n                                                 C1\u03bbC1 or P(x,y)\u2208S[|\u27e8w, x\u27e9| \u2264              \u03c3\n    2. Compute the (d \u2212              1) \u00d7 (d \u2212      1) matrices M+       S and M\u2212      S as follows:\n                                       M+  S =        E       (proj\u22a5w x)(proj\u22a5w x)T \u00b7 1{|\u27e8w,x\u27e9|\u2264\u03c3                2 }\n                                                  (x,y)\u2208S\n                                       M\u2212  S =        E       (proj\u22a5w x)(proj\u22a5w x)T \u00b7 1{|\u27e8w,x\u27e9|\u2264\u03c3                6 }\n                                                  (x,y)\u2208S\n    3. Run the (maximum singular value) spectral tester of Proposition A.2 on M+                                            S given \u03b4 \u2190          \u03b4\n         \u03bb \u2190    C1\u03bbC1 and \u03b8 \u2190           C1\u03c3\u03bbC1    , i.e., reject if the maximum singular value of M+                       S is greater than     4,\n         \u03c3 \u00b7 C1\u03bbC1.                          2\n    4. Run the (minimum singular value) spectral tester of Proposition A.2 on M\u2212                                            S given \u03b4 \u2190          \u03b4\n         \u03bb \u2190     C1\u03bbC1 and \u03b8 \u2190               2\u03c3                                                                                S is less than    4,\n            \u03c3                              C1\u03bbC1 , i.e., reject if the minimum singular value of M\u2212\n         C1\u03bbC1 .\n    5. Run the hypercontractivity tester on S\u2032 = {proj\u22a5w x : (x, y) \u2208                                  S and |\u27e8w, x\u27e9| \u2264          \u03c3}, i.e., solve\n         an appropriate SDP (see Prop. 3.5 with \u03b3 \u2190                           \u03b3, \u03b4 \u2190     \u03b4/4) and reject if the solution is larger\n         than a specified threshold. Otherwise, accept.\n      For part (a), we suppose that the testing algorithm has accepted S. Therefore, S has passed all\nthe tests required for part (a) of Lemma 3.2 and there exists a universal constant C\u2032 > 0 such that\n                                        P      |\u27e8v, x\u27e9| \u2265         1        |\u27e8w, x\u27e9| \u2264      \u03c3   \u2265         1\n                                    (x,y)\u2208S                    C\u2032\u03bbC\u2032                               C\u2032\u03bbC\u2032\u03b34\n                                                                        12", "md": "Bounds on quantities \\(A1\\), \\(A2\\) and \\(A3\\) which ensure that the desired property holds for a given vector \\(w\\), under no distributional assumptions. The tester in the following result universally accepts nice distributions with bounded Poincar\u00e9 parameter.\n\nLemma 4.4 (Universally Testable Structure of Surrogate Loss). Let \\(DXY\\) be any distribution over \\(\\mathbb{R}^d \\times \\{ \\pm 1 \\}\\). Consider \\(L_\\sigma\\) as in Equation (4.1). Then, there is a universal constant \\(C > 0\\) and a tester that given a unit vector \\(w \\in \\mathbb{R}^d\\), \\(\\delta \\in (0, 1)\\), \\(\\eta < 1/2\\), \\(\\gamma > 0\\), \\(\\lambda \\geq 1\\), \\(\\sigma \\leq C\\lambda C\\) and a set \\(S\\) of i.i.d. samples from \\(DXY\\) with size at least \\(C \\cdot d^4 \\sigma^2 \\delta \\log(d) \\lambda C\\), runs in time poly(\\(d, \\lambda, 1, \\sigma, 1/\\delta\\)) and satisfies the following specifications:\n\n1. If the tester accepts \\(S\\), then, the following statements are true for the minimum error opt achieved by some origin-centered halfspace on \\(S\\) and the optimum vector \\(w^* \\in S_{d-1}^{1-2\\eta}\\):\n- If the noise is Massart with associated rate \\(\\eta\\) and \\(\\| \\nabla_w L_\\sigma(w; S) \\|_2 \\leq C\\lambda C\\gamma^4\\) then either \\(\\angle(w, w^* \\in S) \\leq C\\lambda C(1+\\gamma^4) \\cdot \\sigma\\) or \\(\\angle(-w, w^* \\in S) \\leq C\\lambda C(1+\\gamma^4) \\cdot \\sigma\\).\n- If the noise is adversarial with opt \\(S \\leq C\\lambda C\\) and \\(\\| \\nabla_w L_\\sigma(w; S) \\|_2 < C\\lambda C\\gamma^4\\) then either \\(\\angle(w, w^* \\in S) \\leq C\\lambda C(1 + \\gamma^4) \\cdot \\sigma\\) or \\(\\angle(-w, w^* \\in S) \\leq C\\lambda C(1 + \\gamma^4) \\cdot \\sigma\\).\n2. If the marginal \\(DX\\) is \\(\\lambda\\)-nice and \\(\\gamma\\)-Poincar\u00e9, then the tester accepts \\(S\\) with probability at least \\(1 - \\delta\\).\n\nProof. The testing algorithm receives \\(w \\in S_{d-1}\\), \\(\\delta \\in (0, 1)\\), \\(\\eta < 1/2\\), \\(\\gamma > 0\\), \\(\\lambda \\geq 1\\), \\(\\sigma \\leq 2\\lambda\\) and a set \\(S \\subset \\mathbb{R}^d \\times \\{ \\pm 1 \\}\\) and does the following for some sufficiently large \\(C_1 > 0\\):\n\n1. If \\(P(x,y) \\in S[| \\langle w, x \\rangle | \\leq \\sigma^6] \\leq \\sigma^2\\) or \\(P(x,y) \\in S[| \\langle w, x \\rangle | \\leq \\sigma^2] > \\sigma \\cdot C_1 \\lambda C_1\\), then reject.\n2. Compute the \\((d - 1) \\times (d - 1)\\) matrices \\(M^+_S\\) and \\(M^-_S\\) as follows:\n- \\(M^+_S = \\mathbb{E}[(\\text{proj}_{\\perp w} x)(\\text{proj}_{\\perp w} x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\leq \\sigma^2\\}\\) for \\((x,y) \\in S\\)\n- \\(M^-_S = \\mathbb{E}[(\\text{proj}_{\\perp w} x)(\\text{proj}_{\\perp w} x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\leq \\sigma^6\\}\\) for \\((x,y) \\in S\\)\n3. Run the (maximum singular value) spectral tester of Proposition A.2 on \\(M^+_S\\) given \\(\\delta \\leftarrow \\delta\\), \\(\\lambda \\leftarrow C_1 \\lambda C_1\\) and \\(\\theta \\leftarrow C_1 \\sigma \\lambda C_1\\), i.e., reject if the maximum singular value of \\(M^+_S\\) is greater than \\(4 \\sigma \\cdot C_1 \\lambda C_1\\).\n4. Run the (minimum singular value) spectral tester of Proposition A.2 on \\(M^-_S\\) given \\(\\delta \\leftarrow \\delta\\), \\(\\lambda \\leftarrow C_1 \\lambda C_1\\) and \\(\\theta \\leftarrow 2\\sigma \\cdot C_1 \\lambda C_1\\), i.e., reject if the minimum singular value of \\(M^-_S\\) is less than \\(4 \\sigma \\cdot C_1 \\lambda C_1\\).\n5. Run the hypercontractivity tester on \\(S' = \\{ \\text{proj}_{\\perp w} x : (x, y) \\in S \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\}\\), i.e., solve an appropriate SDP (see Prop. 3.5 with \\(\\gamma \\leftarrow \\gamma\\), \\(\\delta \\leftarrow \\delta/4\\)) and reject if the solution is larger than a specified threshold. Otherwise, accept.\n\nFor part (a), we suppose that the testing algorithm has accepted \\(S\\). Therefore, \\(S\\) has passed all the tests required for part (a) of Lemma 3.2 and there exists a universal constant \\(C' > 0\\) such that\n\n$$\n\\begin{align*}\nP \\left( | \\langle v, x \\rangle | \\geq \\frac{1}{C'\\lambda C'} \\Big| | \\langle w, x \\rangle | \\leq \\sigma \\right) \\geq \\frac{1}{12}\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Bounds on quantities \\(A1\\), \\(A2\\) and \\(A3\\) which ensure that the desired property holds for a given vector \\(w\\), under no distributional assumptions. The tester in the following result universally accepts nice distributions with bounded Poincar\u00e9 parameter.\n\nLemma 4.4 (Universally Testable Structure of Surrogate Loss). Let \\(DXY\\) be any distribution over \\(\\mathbb{R}^d \\times \\{ \\pm 1 \\}\\). Consider \\(L_\\sigma\\) as in Equation (4.1). Then, there is a universal constant \\(C > 0\\) and a tester that given a unit vector \\(w \\in \\mathbb{R}^d\\), \\(\\delta \\in (0, 1)\\), \\(\\eta < 1/2\\), \\(\\gamma > 0\\), \\(\\lambda \\geq 1\\), \\(\\sigma \\leq C\\lambda C\\) and a set \\(S\\) of i.i.d. samples from \\(DXY\\) with size at least \\(C \\cdot d^4 \\sigma^2 \\delta \\log(d) \\lambda C\\), runs in time poly(\\(d, \\lambda, 1, \\sigma, 1/\\delta\\)) and satisfies the following specifications:\n\n1. If the tester accepts \\(S\\), then, the following statements are true for the minimum error opt achieved by some origin-centered halfspace on \\(S\\) and the optimum vector \\(w^* \\in S_{d-1}^{1-2\\eta}\\):\n- If the noise is Massart with associated rate \\(\\eta\\) and \\(\\| \\nabla_w L_\\sigma(w; S) \\|_2 \\leq C\\lambda C\\gamma^4\\) then either \\(\\angle(w, w^* \\in S) \\leq C\\lambda C(1+\\gamma^4) \\cdot \\sigma\\) or \\(\\angle(-w, w^* \\in S) \\leq C\\lambda C(1+\\gamma^4) \\cdot \\sigma\\).\n- If the noise is adversarial with opt \\(S \\leq C\\lambda C\\) and \\(\\| \\nabla_w L_\\sigma(w; S) \\|_2 < C\\lambda C\\gamma^4\\) then either \\(\\angle(w, w^* \\in S) \\leq C\\lambda C(1 + \\gamma^4) \\cdot \\sigma\\) or \\(\\angle(-w, w^* \\in S) \\leq C\\lambda C(1 + \\gamma^4) \\cdot \\sigma\\).\n2. If the marginal \\(DX\\) is \\(\\lambda\\)-nice and \\(\\gamma\\)-Poincar\u00e9, then the tester accepts \\(S\\) with probability at least \\(1 - \\delta\\).\n\nProof. The testing algorithm receives \\(w \\in S_{d-1}\\), \\(\\delta \\in (0, 1)\\), \\(\\eta < 1/2\\), \\(\\gamma > 0\\), \\(\\lambda \\geq 1\\), \\(\\sigma \\leq 2\\lambda\\) and a set \\(S \\subset \\mathbb{R}^d \\times \\{ \\pm 1 \\}\\) and does the following for some sufficiently large \\(C_1 > 0\\):\n\n1. If \\(P(x,y) \\in S[| \\langle w, x \\rangle | \\leq \\sigma^6] \\leq \\sigma^2\\) or \\(P(x,y) \\in S[| \\langle w, x \\rangle | \\leq \\sigma^2] > \\sigma \\cdot C_1 \\lambda C_1\\), then reject.\n2. Compute the \\((d - 1) \\times (d - 1)\\) matrices \\(M^+_S\\) and \\(M^-_S\\) as follows:\n- \\(M^+_S = \\mathbb{E}[(\\text{proj}_{\\perp w} x)(\\text{proj}_{\\perp w} x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\leq \\sigma^2\\}\\) for \\((x,y) \\in S\\)\n- \\(M^-_S = \\mathbb{E}[(\\text{proj}_{\\perp w} x)(\\text{proj}_{\\perp w} x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\leq \\sigma^6\\}\\) for \\((x,y) \\in S\\)\n3. Run the (maximum singular value) spectral tester of Proposition A.2 on \\(M^+_S\\) given \\(\\delta \\leftarrow \\delta\\), \\(\\lambda \\leftarrow C_1 \\lambda C_1\\) and \\(\\theta \\leftarrow C_1 \\sigma \\lambda C_1\\), i.e., reject if the maximum singular value of \\(M^+_S\\) is greater than \\(4 \\sigma \\cdot C_1 \\lambda C_1\\).\n4. Run the (minimum singular value) spectral tester of Proposition A.2 on \\(M^-_S\\) given \\(\\delta \\leftarrow \\delta\\), \\(\\lambda \\leftarrow C_1 \\lambda C_1\\) and \\(\\theta \\leftarrow 2\\sigma \\cdot C_1 \\lambda C_1\\), i.e., reject if the minimum singular value of \\(M^-_S\\) is less than \\(4 \\sigma \\cdot C_1 \\lambda C_1\\).\n5. Run the hypercontractivity tester on \\(S' = \\{ \\text{proj}_{\\perp w} x : (x, y) \\in S \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\}\\), i.e., solve an appropriate SDP (see Prop. 3.5 with \\(\\gamma \\leftarrow \\gamma\\), \\(\\delta \\leftarrow \\delta/4\\)) and reject if the solution is larger than a specified threshold. Otherwise, accept.\n\nFor part (a), we suppose that the testing algorithm has accepted \\(S\\). Therefore, \\(S\\) has passed all the tests required for part (a) of Lemma 3.2 and there exists a universal constant \\(C' > 0\\) such that\n\n$$\n\\begin{align*}\nP \\left( | \\langle v, x \\rangle | \\geq \\frac{1}{C'\\lambda C'} \\Big| | \\langle w, x \\rangle | \\leq \\sigma \\right) \\geq \\frac{1}{12}\n\\end{align*}\n$$", "md": "Bounds on quantities \\(A1\\), \\(A2\\) and \\(A3\\) which ensure that the desired property holds for a given vector \\(w\\), under no distributional assumptions. The tester in the following result universally accepts nice distributions with bounded Poincar\u00e9 parameter.\n\nLemma 4.4 (Universally Testable Structure of Surrogate Loss). Let \\(DXY\\) be any distribution over \\(\\mathbb{R}^d \\times \\{ \\pm 1 \\}\\). Consider \\(L_\\sigma\\) as in Equation (4.1). Then, there is a universal constant \\(C > 0\\) and a tester that given a unit vector \\(w \\in \\mathbb{R}^d\\), \\(\\delta \\in (0, 1)\\), \\(\\eta < 1/2\\), \\(\\gamma > 0\\), \\(\\lambda \\geq 1\\), \\(\\sigma \\leq C\\lambda C\\) and a set \\(S\\) of i.i.d. samples from \\(DXY\\) with size at least \\(C \\cdot d^4 \\sigma^2 \\delta \\log(d) \\lambda C\\), runs in time poly(\\(d, \\lambda, 1, \\sigma, 1/\\delta\\)) and satisfies the following specifications:\n\n1. If the tester accepts \\(S\\), then, the following statements are true for the minimum error opt achieved by some origin-centered halfspace on \\(S\\) and the optimum vector \\(w^* \\in S_{d-1}^{1-2\\eta}\\):\n- If the noise is Massart with associated rate \\(\\eta\\) and \\(\\| \\nabla_w L_\\sigma(w; S) \\|_2 \\leq C\\lambda C\\gamma^4\\) then either \\(\\angle(w, w^* \\in S) \\leq C\\lambda C(1+\\gamma^4) \\cdot \\sigma\\) or \\(\\angle(-w, w^* \\in S) \\leq C\\lambda C(1+\\gamma^4) \\cdot \\sigma\\).\n- If the noise is adversarial with opt \\(S \\leq C\\lambda C\\) and \\(\\| \\nabla_w L_\\sigma(w; S) \\|_2 < C\\lambda C\\gamma^4\\) then either \\(\\angle(w, w^* \\in S) \\leq C\\lambda C(1 + \\gamma^4) \\cdot \\sigma\\) or \\(\\angle(-w, w^* \\in S) \\leq C\\lambda C(1 + \\gamma^4) \\cdot \\sigma\\).\n2. If the marginal \\(DX\\) is \\(\\lambda\\)-nice and \\(\\gamma\\)-Poincar\u00e9, then the tester accepts \\(S\\) with probability at least \\(1 - \\delta\\).\n\nProof. The testing algorithm receives \\(w \\in S_{d-1}\\), \\(\\delta \\in (0, 1)\\), \\(\\eta < 1/2\\), \\(\\gamma > 0\\), \\(\\lambda \\geq 1\\), \\(\\sigma \\leq 2\\lambda\\) and a set \\(S \\subset \\mathbb{R}^d \\times \\{ \\pm 1 \\}\\) and does the following for some sufficiently large \\(C_1 > 0\\):\n\n1. If \\(P(x,y) \\in S[| \\langle w, x \\rangle | \\leq \\sigma^6] \\leq \\sigma^2\\) or \\(P(x,y) \\in S[| \\langle w, x \\rangle | \\leq \\sigma^2] > \\sigma \\cdot C_1 \\lambda C_1\\), then reject.\n2. Compute the \\((d - 1) \\times (d - 1)\\) matrices \\(M^+_S\\) and \\(M^-_S\\) as follows:\n- \\(M^+_S = \\mathbb{E}[(\\text{proj}_{\\perp w} x)(\\text{proj}_{\\perp w} x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\leq \\sigma^2\\}\\) for \\((x,y) \\in S\\)\n- \\(M^-_S = \\mathbb{E}[(\\text{proj}_{\\perp w} x)(\\text{proj}_{\\perp w} x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\leq \\sigma^6\\}\\) for \\((x,y) \\in S\\)\n3. Run the (maximum singular value) spectral tester of Proposition A.2 on \\(M^+_S\\) given \\(\\delta \\leftarrow \\delta\\), \\(\\lambda \\leftarrow C_1 \\lambda C_1\\) and \\(\\theta \\leftarrow C_1 \\sigma \\lambda C_1\\), i.e., reject if the maximum singular value of \\(M^+_S\\) is greater than \\(4 \\sigma \\cdot C_1 \\lambda C_1\\).\n4. Run the (minimum singular value) spectral tester of Proposition A.2 on \\(M^-_S\\) given \\(\\delta \\leftarrow \\delta\\), \\(\\lambda \\leftarrow C_1 \\lambda C_1\\) and \\(\\theta \\leftarrow 2\\sigma \\cdot C_1 \\lambda C_1\\), i.e., reject if the minimum singular value of \\(M^-_S\\) is less than \\(4 \\sigma \\cdot C_1 \\lambda C_1\\).\n5. Run the hypercontractivity tester on \\(S' = \\{ \\text{proj}_{\\perp w} x : (x, y) \\in S \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\}\\), i.e., solve an appropriate SDP (see Prop. 3.5 with \\(\\gamma \\leftarrow \\gamma\\), \\(\\delta \\leftarrow \\delta/4\\)) and reject if the solution is larger than a specified threshold. Otherwise, accept.\n\nFor part (a), we suppose that the testing algorithm has accepted \\(S\\). Therefore, \\(S\\) has passed all the tests required for part (a) of Lemma 3.2 and there exists a universal constant \\(C' > 0\\) such that\n\n$$\n\\begin{align*}\nP \\left( | \\langle v, x \\rangle | \\geq \\frac{1}{C'\\lambda C'} \\Big| | \\langle w, x \\rangle | \\leq \\sigma \\right) \\geq \\frac{1}{12}\n\\end{align*}\n$$"}]}, {"page": 13, "text": "Moreover, we have            \u03c3                                   6 ] \u2264  P(x,y)\u2208S[|\u27e8w, x\u27e9| \u2264       \u03c32 ] < \u03c3 \u00b7 C\u2032\u03bbC\u2032 and\n                           C\u2032\u03bbC\u2032 < P(x,y)\u2208S[|\u27e8w, x\u27e9| \u2264           \u03c3\n                                            E      \u27e8v, x\u27e92      |\u27e8w, x\u27e9| \u2264    \u03c32   \u2264   C\u2032\u03bbC\u2032\n                                         (x,y)\u2208S   \u27e8v, x\u27e92                         \u2265      1\n                                            E                   |\u27e8w, x\u27e9| \u2264    \u03c36       C\u2032\u03bbC\u2032\n                                         (x,y)\u2208S\n     Since Proposition 4.3 holds for any distribution, it will also hold for the empirical distribution\n                                                                                1\n(uniform on S). We apply Proposition 4.3 with \u03b1 =                            C\u2032\u03bbC\u2032 to lower bound \u2225\u2207wL\u03c3(w; S)\u22252 (or\n\u2225\u2207wL\u03c3(\u2212w; S)\u22252) as follows\u2225\u2207wL\u03c3(w; S)\u22252 \u2265           A1(\u03b1) \u2212     A2 \u2212    A3                              (adversarial noise case)\n                          \u2225\u2207wL\u03c3(w; S)\u22252 \u2265           (1 \u2212   2\u03b7) \u00b7 A1(\u03b1) \u2212      A2                            (Massart noise case)\nCombining the above inequalities with the bounds implied by the fact that S has passed the tests,\nconcludes the proof of part (a), since (after observing that tan \u03b8 \u2265                         \u03b8) we obtain\n                                            3         \u221a C\u03c3\u03bbC/2      \u2212      opt \u00b7 C \u00b7 \u03bbC                 (adversarial noise case)\n             \u2225\u2207wL\u03c3(w; S)\u22252 \u2265            C\u03bbC\u03b34 \u2212             \u03b8                     \u03c3\n             \u2225\u2207wL\u03c3(w; S)\u22252 \u2265            3(1 \u2212   \u03b7)     \u221a  C\u03c3\u03bbC/2                                            (Massart noise case)\n                                        C\u03bbC\u03b34 \u2212              \u03b8\n     For part (b), we follow a similar recipe as the one used to prove part (b) of Lemma 3.2, i.e., we\nuse the following reasoning to show that the tests will pass with probability at least 1 \u2212                                 \u03b4\n   1. We assume that the marginal distribution DX is \u03bb-nice and \u03b3-Poincar\u00b4e.\n   2. We use Proposition A.3 to bound the values of the tested quantities under the true distribu-\n        tion.\n   3. We use appropriate concentration results (Hoeffding/Chernoff Bounds and Proposition A.2)\n        to show that, since |S| is large enough, each of the empirical quantities at hand does not\n        deviate a lot from its mean.\nThis concludes the proof of Lemma 4.4.\n     We are ready to prove Theorem 4.1.\nProof of Theorem 4.1. We will give the algorithm for \u03b4 \u2190                         \u03b4\u2032 = 1/3 since we can reduce the proba-\nbility of failure with repetition (repeat O(log 1              \u03b4) times, accept if the rate of acceptance is \u2126(1) and\noutput the halfspace achieving the minimum test error among the halfspaces returned).\n     The algorithm receives \u03bb \u2265             1, \u03b3 > 0, \u03f5 > 0 and \u03b7 \u2208            (0, 1/2) \u222a    {1} (say \u03b7 = 1 when we are in\nthe agnostic case) and does the following for some appropriately large universal constant C1 > 0.\n                                          \u03f5\n   1. First, initialize E =           C1\u03bbC1 , and let \u03a3 be a list of real numbers and A be a positive real\n                                                                                                                  E\n        number, where \u03a3 and A are defined as follows. If \u03b7 = 1, then \u03a3 is an                                   C1\u03bbC1 -cover of the\n        interval    0,     1      and A =          1                                      E\u00b7(1\u22122\u03b7)        and A =         1\u22122\u03b7\n                        C1\u03bbC1                  C 1\u03bbC1\u03b34 . Otherwise, let \u03a3 =            C1\u03bbC1(1+\u03b34)                     C1\u03bbC1\u03b34 .\n   2. Draw a set S1 of C1             \u03bbd\u03b3   C1 i.i.d. samples from DXY and run PSGD, as specified in Propo-\n                                        \u03f5\n                                                \u03b4\n        sition A.4 with \u03f5 \u2190         A, \u03b4 \u2190     C1 on the loss L\u03c3 for each \u03c3 \u2208             \u03a3.\n                                                                  13", "md": "Moreover, we have\n\n$$\\sigma \\leq P(x,y)\\in S[|\\langle w, x\\rangle| \\leq \\sigma^2] < \\sigma \\cdot C'\\lambda C'$$\n\nand\n\n$$C'\\lambda C' < P(x,y)\\in S[|\\langle w, x\\rangle| \\leq \\sigma E \\langle v, x\\rangle^2 | \\langle w, x\\rangle| \\leq \\sigma^2 \\leq C'\\lambda C' (x,y)\\in S \\langle v, x\\rangle^2 \\geq 1 E | \\langle w, x\\rangle| \\leq \\sigma^6 C'\\lambda C' (x,y)\\in S$$\n\nSince Proposition 4.3 holds for any distribution, it will also hold for the empirical distribution (uniform on S). We apply Proposition 4.3 with \u03b1 = $$\\frac{1}{C'\\lambda C'}$$ to lower bound $$\\|\\nabla_w L_\\sigma(w; S)\\|^2$$ (or $$\\|\\nabla_w L_\\sigma(-w; S)\\|^2$$) as follows:\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq A1(\\alpha) - A2 - A3 \\text{ (adversarial noise case)}$$\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq (1 - 2\\eta) \\cdot A1(\\alpha) - A2 \\text{ (Massart noise case)}$$\n\nCombining the above inequalities with the bounds implied by the fact that S has passed the tests, concludes the proof of part (a), since (after observing that $$\\tan \\theta \\geq \\theta$$) we obtain\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq C\\lambda C\\gamma^4 - \\frac{\\sqrt{C\\sigma\\lambda C/2} - \\text{opt} \\cdot C \\cdot \\lambda C}{\\sigma}$$\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq 3(1 - \\eta) \\sqrt{C\\sigma\\lambda C/2} / (C\\lambda C\\gamma^4 - \\theta)$$\n\nFor part (b), we follow a similar recipe as the one used to prove part (b) of Lemma 3.2, i.e., we use the following reasoning to show that the tests will pass with probability at least $$1 - \\delta$$\n\n1. We assume that the marginal distribution DX is \u03bb-nice and \u03b3-Poincar\u00e9.\n2. We use Proposition A.3 to bound the values of the tested quantities under the true distribution.\n3. We use appropriate concentration results (Hoeffding/Chernoff Bounds and Proposition A.2) to show that, since |S| is large enough, each of the empirical quantities at hand does not deviate a lot from its mean.\n\nThis concludes the proof of Lemma 4.4.\n\nWe are ready to prove Theorem 4.1.\n\nProof of Theorem 4.1. We will give the algorithm for $$\\delta \\leftarrow \\delta' = 1/3$$ since we can reduce the probability of failure with repetition (repeat $$O(\\log \\frac{1}{\\delta})$$ times, accept if the rate of acceptance is $$\\Omega(1)$$ and output the halfspace achieving the minimum test error among the halfspaces returned).\n\nThe algorithm receives $$\\lambda \\geq 1$$, $$\\gamma > 0$$, $$\\epsilon > 0$$ and $$\\eta \\in (0, 1/2) \\cup \\{1\\}$$ (say $$\\eta = 1$$ when we are in the agnostic case) and does the following for some appropriately large universal constant $$C1 > 0$$.\n\n1. First, initialize $$E = C1\\lambda C1$$, and let \u03a3 be a list of real numbers and A be a positive real number, where \u03a3 and A are defined as follows. If $$\\eta = 1$$, then \u03a3 is an $$C1\\lambda C1$$-cover of the interval $$[0, 1]$$ and $$A = \\frac{1}{E} \\cdot (1 - 2\\eta)$$ and $$A = 1 - 2\\eta \\cdot C1\\lambda C1$$. Otherwise, let $$\\Sigma = C1\\lambda C1(1+\\gamma^4)$$ and $$A = C1\\lambda C1\\gamma^4$$.\n\n2. Draw a set $$S1$$ of $$C1\\lambda d\\gamma C1$$ i.i.d. samples from DXY and run PSGD, as specified in Proposition A.4 with $$\\epsilon \\leftarrow A$$, $$\\delta \\leftarrow C1$$ on the loss $$L_\\sigma$$ for each $$\\sigma \\in \\Sigma$$.", "images": [], "items": [{"type": "text", "value": "Moreover, we have\n\n$$\\sigma \\leq P(x,y)\\in S[|\\langle w, x\\rangle| \\leq \\sigma^2] < \\sigma \\cdot C'\\lambda C'$$\n\nand\n\n$$C'\\lambda C' < P(x,y)\\in S[|\\langle w, x\\rangle| \\leq \\sigma E \\langle v, x\\rangle^2 | \\langle w, x\\rangle| \\leq \\sigma^2 \\leq C'\\lambda C' (x,y)\\in S \\langle v, x\\rangle^2 \\geq 1 E | \\langle w, x\\rangle| \\leq \\sigma^6 C'\\lambda C' (x,y)\\in S$$\n\nSince Proposition 4.3 holds for any distribution, it will also hold for the empirical distribution (uniform on S). We apply Proposition 4.3 with \u03b1 = $$\\frac{1}{C'\\lambda C'}$$ to lower bound $$\\|\\nabla_w L_\\sigma(w; S)\\|^2$$ (or $$\\|\\nabla_w L_\\sigma(-w; S)\\|^2$$) as follows:\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq A1(\\alpha) - A2 - A3 \\text{ (adversarial noise case)}$$\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq (1 - 2\\eta) \\cdot A1(\\alpha) - A2 \\text{ (Massart noise case)}$$\n\nCombining the above inequalities with the bounds implied by the fact that S has passed the tests, concludes the proof of part (a), since (after observing that $$\\tan \\theta \\geq \\theta$$) we obtain\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq C\\lambda C\\gamma^4 - \\frac{\\sqrt{C\\sigma\\lambda C/2} - \\text{opt} \\cdot C \\cdot \\lambda C}{\\sigma}$$\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq 3(1 - \\eta) \\sqrt{C\\sigma\\lambda C/2} / (C\\lambda C\\gamma^4 - \\theta)$$\n\nFor part (b), we follow a similar recipe as the one used to prove part (b) of Lemma 3.2, i.e., we use the following reasoning to show that the tests will pass with probability at least $$1 - \\delta$$\n\n1. We assume that the marginal distribution DX is \u03bb-nice and \u03b3-Poincar\u00e9.\n2. We use Proposition A.3 to bound the values of the tested quantities under the true distribution.\n3. We use appropriate concentration results (Hoeffding/Chernoff Bounds and Proposition A.2) to show that, since |S| is large enough, each of the empirical quantities at hand does not deviate a lot from its mean.\n\nThis concludes the proof of Lemma 4.4.\n\nWe are ready to prove Theorem 4.1.\n\nProof of Theorem 4.1. We will give the algorithm for $$\\delta \\leftarrow \\delta' = 1/3$$ since we can reduce the probability of failure with repetition (repeat $$O(\\log \\frac{1}{\\delta})$$ times, accept if the rate of acceptance is $$\\Omega(1)$$ and output the halfspace achieving the minimum test error among the halfspaces returned).\n\nThe algorithm receives $$\\lambda \\geq 1$$, $$\\gamma > 0$$, $$\\epsilon > 0$$ and $$\\eta \\in (0, 1/2) \\cup \\{1\\}$$ (say $$\\eta = 1$$ when we are in the agnostic case) and does the following for some appropriately large universal constant $$C1 > 0$$.\n\n1. First, initialize $$E = C1\\lambda C1$$, and let \u03a3 be a list of real numbers and A be a positive real number, where \u03a3 and A are defined as follows. If $$\\eta = 1$$, then \u03a3 is an $$C1\\lambda C1$$-cover of the interval $$[0, 1]$$ and $$A = \\frac{1}{E} \\cdot (1 - 2\\eta)$$ and $$A = 1 - 2\\eta \\cdot C1\\lambda C1$$. Otherwise, let $$\\Sigma = C1\\lambda C1(1+\\gamma^4)$$ and $$A = C1\\lambda C1\\gamma^4$$.\n\n2. Draw a set $$S1$$ of $$C1\\lambda d\\gamma C1$$ i.i.d. samples from DXY and run PSGD, as specified in Proposition A.4 with $$\\epsilon \\leftarrow A$$, $$\\delta \\leftarrow C1$$ on the loss $$L_\\sigma$$ for each $$\\sigma \\in \\Sigma$$.", "md": "Moreover, we have\n\n$$\\sigma \\leq P(x,y)\\in S[|\\langle w, x\\rangle| \\leq \\sigma^2] < \\sigma \\cdot C'\\lambda C'$$\n\nand\n\n$$C'\\lambda C' < P(x,y)\\in S[|\\langle w, x\\rangle| \\leq \\sigma E \\langle v, x\\rangle^2 | \\langle w, x\\rangle| \\leq \\sigma^2 \\leq C'\\lambda C' (x,y)\\in S \\langle v, x\\rangle^2 \\geq 1 E | \\langle w, x\\rangle| \\leq \\sigma^6 C'\\lambda C' (x,y)\\in S$$\n\nSince Proposition 4.3 holds for any distribution, it will also hold for the empirical distribution (uniform on S). We apply Proposition 4.3 with \u03b1 = $$\\frac{1}{C'\\lambda C'}$$ to lower bound $$\\|\\nabla_w L_\\sigma(w; S)\\|^2$$ (or $$\\|\\nabla_w L_\\sigma(-w; S)\\|^2$$) as follows:\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq A1(\\alpha) - A2 - A3 \\text{ (adversarial noise case)}$$\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq (1 - 2\\eta) \\cdot A1(\\alpha) - A2 \\text{ (Massart noise case)}$$\n\nCombining the above inequalities with the bounds implied by the fact that S has passed the tests, concludes the proof of part (a), since (after observing that $$\\tan \\theta \\geq \\theta$$) we obtain\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq C\\lambda C\\gamma^4 - \\frac{\\sqrt{C\\sigma\\lambda C/2} - \\text{opt} \\cdot C \\cdot \\lambda C}{\\sigma}$$\n\n$$\\|\\nabla_w L_\\sigma(w; S)\\|^2 \\geq 3(1 - \\eta) \\sqrt{C\\sigma\\lambda C/2} / (C\\lambda C\\gamma^4 - \\theta)$$\n\nFor part (b), we follow a similar recipe as the one used to prove part (b) of Lemma 3.2, i.e., we use the following reasoning to show that the tests will pass with probability at least $$1 - \\delta$$\n\n1. We assume that the marginal distribution DX is \u03bb-nice and \u03b3-Poincar\u00e9.\n2. We use Proposition A.3 to bound the values of the tested quantities under the true distribution.\n3. We use appropriate concentration results (Hoeffding/Chernoff Bounds and Proposition A.2) to show that, since |S| is large enough, each of the empirical quantities at hand does not deviate a lot from its mean.\n\nThis concludes the proof of Lemma 4.4.\n\nWe are ready to prove Theorem 4.1.\n\nProof of Theorem 4.1. We will give the algorithm for $$\\delta \\leftarrow \\delta' = 1/3$$ since we can reduce the probability of failure with repetition (repeat $$O(\\log \\frac{1}{\\delta})$$ times, accept if the rate of acceptance is $$\\Omega(1)$$ and output the halfspace achieving the minimum test error among the halfspaces returned).\n\nThe algorithm receives $$\\lambda \\geq 1$$, $$\\gamma > 0$$, $$\\epsilon > 0$$ and $$\\eta \\in (0, 1/2) \\cup \\{1\\}$$ (say $$\\eta = 1$$ when we are in the agnostic case) and does the following for some appropriately large universal constant $$C1 > 0$$.\n\n1. First, initialize $$E = C1\\lambda C1$$, and let \u03a3 be a list of real numbers and A be a positive real number, where \u03a3 and A are defined as follows. If $$\\eta = 1$$, then \u03a3 is an $$C1\\lambda C1$$-cover of the interval $$[0, 1]$$ and $$A = \\frac{1}{E} \\cdot (1 - 2\\eta)$$ and $$A = 1 - 2\\eta \\cdot C1\\lambda C1$$. Otherwise, let $$\\Sigma = C1\\lambda C1(1+\\gamma^4)$$ and $$A = C1\\lambda C1\\gamma^4$$.\n\n2. Draw a set $$S1$$ of $$C1\\lambda d\\gamma C1$$ i.i.d. samples from DXY and run PSGD, as specified in Proposition A.4 with $$\\epsilon \\leftarrow A$$, $$\\delta \\leftarrow C1$$ on the loss $$L_\\sigma$$ for each $$\\sigma \\in \\Sigma$$."}]}, {"page": 14, "text": "    3. Form a list L with all the pairs of the form (w, \u03c3) where w \u2208                                       Sd\u22121 is some iterate of the\n         PSGD subroutine performed on L\u03c3.\n    4. Draw a fresh set S2 of C1                  \u03bbd\u03b3   C1 i.i.d. samples from DXY and compute for each (w, \u03c3) \u2208                                  L\n                                                    \u03f5\n         the value \u2225\u2207wL\u03c3(w; S2)\u22252. If, for some \u03c3 \u2208                         \u03a3, \u2225\u2207wL\u03c3(w; S2)\u22252 > A for all (w, \u03c3) \u2208                       L, then\n         reject.\n    5. Update L by keeping for each \u03c3 \u2208                         \u03a3 only one pair of the form (w, \u03c3) for which we have\n         \u2225\u2207wL\u03c3(w; S2)\u22252 \u2264              A.\n    6. Run the following tests for each (w, \u03c3) \u2208                         L. (This will ensure that part (a) of Lemma 4.4\n         holds for each of the elements of L, i.e., that any stationary point of the loss L\u03c3 that lies in\n         L is angularly close to the empirical risk minimizer2.).\n             \u2022 If P(x,y)\u2208S2[|\u27e8w, x\u27e9| \u2264            \u03c36 ] \u2264     \u03c3                                       2 ] > \u03c3 \u00b7 C1\u03bbC1, then reject.\n                                                          C1\u03bbC1 or P(x,y)\u2208S2[|\u27e8w, x\u27e9| \u2264              \u03c3\n             \u2022 Compute the (d \u2212              1) \u00d7 (d \u2212     1) matrices M+        S2 and M\u2212     S2 as follows:3\n                                          M+  S2 =        E       (proj\u22a5w x)(proj\u22a5w x)T \u00b7 1{|\u27e8w,x\u27e9|\u2264\u03c3                2 }\n                                                     (x,y)\u2208S2\n                                          M\u2212  S2 =        E       (proj\u22a5w x)(proj\u22a5w x)T \u00b7 1{|\u27e8w,x\u27e9|\u2264\u03c3                6 }\n                                                     (x,y)\u2208S2\n             \u2022 Reject if the maximum singular value of M+                          S2 is greater than \u03c3 \u00b7 C1\u03bbC1.\n             \u2022 Reject if the minimum singular value of M\u2212                         S2 is less than           \u03c3\n                                                                                                         C1\u03bbC1 .\n             \u2022 Run the hypercontractivity tester on S\u2032 = {proj\u22a5w x : (x, y) \u2208                                      S2 and |\u27e8w, x\u27e9| \u2264           \u03c3},\n                i.e., solve an appropriate SDP (see Prop. 3.5 with \u03b3 \u2190                              \u03b3, \u03b4 \u2190      \u03b4\u2032/C1) and reject if the\n                solution is larger than a specified threshold.\n    7. Set \u03b8 = (1+\u03b34)\u03c3   A\u03b34     , and run the following tests for each pair of the form (w, \u03c3) and (\u2212w, \u03c3)\n         where (w, \u03c3) \u2208          L. (This will ensure that part (a) of Lemma 3.1 is activated, i.e., that the\n         distance of a vector from the empirical risk minimizer is an accurate proxy for the error of\n         the corresponding halfspace.)\n             \u2022 If P(x,y)\u2208S2[|\u27e8w, x\u27e9| \u2264           \u03b8] > C1\u03bbC1\u03b8 then reject.\n             \u2022 Compute the (d \u2212              1) \u00d7 (d \u2212     1) matrix MS2 as follows:4\n                             MS   2 =       E         \u221e    (proj\u22a5w x)(proj\u22a5w x)T              1{|\u27e8w, x\u27e9| \u2208       [(i \u2212   1)\u03b8, i\u03b8)}\n                                        (x,y)\u2208S2      i=2              (i \u2212  1)2                                                                \u03b4\n             \u2022 If \u2225MS\u2225op > C1\u03b8\u03bbC1, then reject.                           (Apply Proposition A.2 on MS given \u03b4 \u2190                               C1 ,\n                \u03bb \u2190     C1\u03bbC1 and \u03b8 \u2190           C1\n                                                 2 \u03b8\u03bbC1).\n    8. Otherwise, accept and output the vector w that achieves the smallest empirical error on S2\n         among the vectors in the list L.\n      For the following, let \u03b1 = 1 in the Massart noise case and \u03b1 = C1\u03bbC1\u03b34 in the adversarial\nnoise case. Consider also opt               S2 to be the error of the origin-centered halfspace with the minimum\nempirical error on S2 and w\u2217               S2 the corresponding optimum vector.\n    2Or the same holds for the inverse vector.\n    3The operator proj      \u22a5w : Rd \u2192      Rd\u22121 projects vectors on the hyperplane orthogonal to w.\n    4Note that only at most |S2| many terms below are non-zero, hence MS2 can be computed efficiently.\n                                                                        14", "md": "1. Form a list L with all the pairs of the form (w, \u03c3) where \\( w \\in S_{d-1} \\) is some iterate of the PSGD subroutine performed on L\u03c3.\n2. Draw a fresh set S2 of \\( C_1 \\lambda d\\gamma C_1 \\) i.i.d. samples from DXY and compute for each (w, \u03c3) in L the value \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 \\). If, for some \\( \\sigma \\) in \\( \\Sigma \\), \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 > A \\) for all (w, \u03c3) in L, then reject.\n3. Update L by keeping for each \\( \\sigma \\) in \\( \\Sigma \\) only one pair of the form (w, \u03c3) for which we have \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 \\leq A \\).\n4. Run the following tests for each (w, \u03c3) in L. (This will ensure that part (a) of Lemma 4.4 holds for each of the elements of L, i.e., that any stationary point of the loss L\u03c3 that lies in L is angularly close to the empirical risk minimizer):\n- If \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\sigma^6 ] \\leq \\sigma^2 \\) or \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\sigma \\] \\leq \\sigma^2 \\) > \\( \\sigma \\cdot C_1 \\lambda C_1 \\), then reject.\n- Compute the \\( (d-1) \\times (d-1) \\) matrices \\( M^+_S2 \\) and \\( M^-_S2 \\) as follows:\n$ M^+_S2 = E (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w,x \\rangle | \\leq \\sigma^2 \\} $\n$ M^-_S2 = E (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w,x \\rangle | \\leq \\sigma^6 \\} $\n- Reject if the maximum singular value of \\( M^+_S2 \\) is greater than \\( \\sigma \\cdot C_1 \\lambda C_1 \\).\n- Reject if the minimum singular value of \\( M^-_S2 \\) is less than \\( \\sigma \\cdot C_1 \\lambda C_1 \\).\n- Run the hypercontractivity tester on \\( S' = \\{proj^\\perp_w x : (x, y) \\in S2 \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\} \\), i.e., solve an appropriate SDP (see Prop. 3.5 with \\( \\gamma \\leftarrow \\gamma, \\delta \\leftarrow \\delta'/C_1 \\)) and reject if the solution is larger than a specified threshold.\n5. Set \\( \\theta = (1+\\gamma^4)\\sigma \\cdot A\\gamma^4 \\), and run the following tests for each pair of the form (w, \u03c3) and (-w, \u03c3) where (w, \u03c3) in L. (This will ensure that part (a) of Lemma 3.1 is activated, i.e., that the distance of a vector from the empirical risk minimizer is an accurate proxy for the error of the corresponding halfspace):\n- If \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\theta] > C_1 \\lambda C_1 \\theta \\) then reject.\n- Compute the \\( (d-1) \\times (d-1) \\) matrix \\( M_S2 \\) as follows:\n$ M_S2 = E \\sum_{i=2}^\\infty (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\in [(i-1)\\theta, i\\theta)\\} $\n- If \\( \\|M_S\\|_{op} > C_1 \\theta \\lambda C_1 \\), then reject. (Apply Proposition A.2 on \\( M_S \\) given \\( \\delta \\leftarrow C_1, \\lambda \\leftarrow C_1 \\lambda C_1 \\) and \\( \\theta \\leftarrow \\frac{C_1}{2} \\theta \\lambda C_1 \\)).\n6. Otherwise, accept and output the vector w that achieves the smallest empirical error on S2 among the vectors in the list L.\n\nFor the following, let \\( \\alpha = 1 \\) in the Massart noise case and \\( \\alpha = C_1 \\lambda C_1 \\gamma^4 \\) in the adversarial noise case. Consider also opt \\( S2 \\) to be the error of the origin-centered halfspace with the minimum empirical error on S2 and \\( w^* S2 \\) the corresponding optimum vector.\n\nOr the same holds for the inverse vector.\n\nThe operator \\( proj^\\perp_w : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d-1} \\) projects vectors on the hyperplane orthogonal to w.\n\nNote that only at most \\( |S2| \\) many terms below are non-zero, hence \\( M_S2 \\) can be computed efficiently.", "images": [], "items": [{"type": "text", "value": "1. Form a list L with all the pairs of the form (w, \u03c3) where \\( w \\in S_{d-1} \\) is some iterate of the PSGD subroutine performed on L\u03c3.\n2. Draw a fresh set S2 of \\( C_1 \\lambda d\\gamma C_1 \\) i.i.d. samples from DXY and compute for each (w, \u03c3) in L the value \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 \\). If, for some \\( \\sigma \\) in \\( \\Sigma \\), \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 > A \\) for all (w, \u03c3) in L, then reject.\n3. Update L by keeping for each \\( \\sigma \\) in \\( \\Sigma \\) only one pair of the form (w, \u03c3) for which we have \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 \\leq A \\).\n4. Run the following tests for each (w, \u03c3) in L. (This will ensure that part (a) of Lemma 4.4 holds for each of the elements of L, i.e., that any stationary point of the loss L\u03c3 that lies in L is angularly close to the empirical risk minimizer):\n- If \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\sigma^6 ] \\leq \\sigma^2 \\) or \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\sigma \\] \\leq \\sigma^2 \\) > \\( \\sigma \\cdot C_1 \\lambda C_1 \\), then reject.\n- Compute the \\( (d-1) \\times (d-1) \\) matrices \\( M^+_S2 \\) and \\( M^-_S2 \\) as follows:\n$ M^+_S2 = E (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w,x \\rangle | \\leq \\sigma^2 \\} $\n$ M^-_S2 = E (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w,x \\rangle | \\leq \\sigma^6 \\} $\n- Reject if the maximum singular value of \\( M^+_S2 \\) is greater than \\( \\sigma \\cdot C_1 \\lambda C_1 \\).\n- Reject if the minimum singular value of \\( M^-_S2 \\) is less than \\( \\sigma \\cdot C_1 \\lambda C_1 \\).\n- Run the hypercontractivity tester on \\( S' = \\{proj^\\perp_w x : (x, y) \\in S2 \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\} \\), i.e., solve an appropriate SDP (see Prop. 3.5 with \\( \\gamma \\leftarrow \\gamma, \\delta \\leftarrow \\delta'/C_1 \\)) and reject if the solution is larger than a specified threshold.\n5. Set \\( \\theta = (1+\\gamma^4)\\sigma \\cdot A\\gamma^4 \\), and run the following tests for each pair of the form (w, \u03c3) and (-w, \u03c3) where (w, \u03c3) in L. (This will ensure that part (a) of Lemma 3.1 is activated, i.e., that the distance of a vector from the empirical risk minimizer is an accurate proxy for the error of the corresponding halfspace):\n- If \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\theta] > C_1 \\lambda C_1 \\theta \\) then reject.\n- Compute the \\( (d-1) \\times (d-1) \\) matrix \\( M_S2 \\) as follows:\n$ M_S2 = E \\sum_{i=2}^\\infty (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\in [(i-1)\\theta, i\\theta)\\} $\n- If \\( \\|M_S\\|_{op} > C_1 \\theta \\lambda C_1 \\), then reject. (Apply Proposition A.2 on \\( M_S \\) given \\( \\delta \\leftarrow C_1, \\lambda \\leftarrow C_1 \\lambda C_1 \\) and \\( \\theta \\leftarrow \\frac{C_1}{2} \\theta \\lambda C_1 \\)).\n6. Otherwise, accept and output the vector w that achieves the smallest empirical error on S2 among the vectors in the list L.\n\nFor the following, let \\( \\alpha = 1 \\) in the Massart noise case and \\( \\alpha = C_1 \\lambda C_1 \\gamma^4 \\) in the adversarial noise case. Consider also opt \\( S2 \\) to be the error of the origin-centered halfspace with the minimum empirical error on S2 and \\( w^* S2 \\) the corresponding optimum vector.\n\nOr the same holds for the inverse vector.\n\nThe operator \\( proj^\\perp_w : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d-1} \\) projects vectors on the hyperplane orthogonal to w.\n\nNote that only at most \\( |S2| \\) many terms below are non-zero, hence \\( M_S2 \\) can be computed efficiently.", "md": "1. Form a list L with all the pairs of the form (w, \u03c3) where \\( w \\in S_{d-1} \\) is some iterate of the PSGD subroutine performed on L\u03c3.\n2. Draw a fresh set S2 of \\( C_1 \\lambda d\\gamma C_1 \\) i.i.d. samples from DXY and compute for each (w, \u03c3) in L the value \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 \\). If, for some \\( \\sigma \\) in \\( \\Sigma \\), \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 > A \\) for all (w, \u03c3) in L, then reject.\n3. Update L by keeping for each \\( \\sigma \\) in \\( \\Sigma \\) only one pair of the form (w, \u03c3) for which we have \\( \\|\\nabla_w L_\\sigma(w; S2)\\|^2 \\leq A \\).\n4. Run the following tests for each (w, \u03c3) in L. (This will ensure that part (a) of Lemma 4.4 holds for each of the elements of L, i.e., that any stationary point of the loss L\u03c3 that lies in L is angularly close to the empirical risk minimizer):\n- If \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\sigma^6 ] \\leq \\sigma^2 \\) or \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\sigma \\] \\leq \\sigma^2 \\) > \\( \\sigma \\cdot C_1 \\lambda C_1 \\), then reject.\n- Compute the \\( (d-1) \\times (d-1) \\) matrices \\( M^+_S2 \\) and \\( M^-_S2 \\) as follows:\n$ M^+_S2 = E (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w,x \\rangle | \\leq \\sigma^2 \\} $\n$ M^-_S2 = E (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w,x \\rangle | \\leq \\sigma^6 \\} $\n- Reject if the maximum singular value of \\( M^+_S2 \\) is greater than \\( \\sigma \\cdot C_1 \\lambda C_1 \\).\n- Reject if the minimum singular value of \\( M^-_S2 \\) is less than \\( \\sigma \\cdot C_1 \\lambda C_1 \\).\n- Run the hypercontractivity tester on \\( S' = \\{proj^\\perp_w x : (x, y) \\in S2 \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\} \\), i.e., solve an appropriate SDP (see Prop. 3.5 with \\( \\gamma \\leftarrow \\gamma, \\delta \\leftarrow \\delta'/C_1 \\)) and reject if the solution is larger than a specified threshold.\n5. Set \\( \\theta = (1+\\gamma^4)\\sigma \\cdot A\\gamma^4 \\), and run the following tests for each pair of the form (w, \u03c3) and (-w, \u03c3) where (w, \u03c3) in L. (This will ensure that part (a) of Lemma 3.1 is activated, i.e., that the distance of a vector from the empirical risk minimizer is an accurate proxy for the error of the corresponding halfspace):\n- If \\( P(x,y) \\in S2[| \\langle w, x \\rangle | \\leq \\theta] > C_1 \\lambda C_1 \\theta \\) then reject.\n- Compute the \\( (d-1) \\times (d-1) \\) matrix \\( M_S2 \\) as follows:\n$ M_S2 = E \\sum_{i=2}^\\infty (proj^\\perp_w x)(proj^\\perp_w x)^T \\cdot 1\\{| \\langle w, x \\rangle | \\in [(i-1)\\theta, i\\theta)\\} $\n- If \\( \\|M_S\\|_{op} > C_1 \\theta \\lambda C_1 \\), then reject. (Apply Proposition A.2 on \\( M_S \\) given \\( \\delta \\leftarrow C_1, \\lambda \\leftarrow C_1 \\lambda C_1 \\) and \\( \\theta \\leftarrow \\frac{C_1}{2} \\theta \\lambda C_1 \\)).\n6. Otherwise, accept and output the vector w that achieves the smallest empirical error on S2 among the vectors in the list L.\n\nFor the following, let \\( \\alpha = 1 \\) in the Massart noise case and \\( \\alpha = C_1 \\lambda C_1 \\gamma^4 \\) in the adversarial noise case. Consider also opt \\( S2 \\) to be the error of the origin-centered halfspace with the minimum empirical error on S2 and \\( w^* S2 \\) the corresponding optimum vector.\n\nOr the same holds for the inverse vector.\n\nThe operator \\( proj^\\perp_w : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d-1} \\) projects vectors on the hyperplane orthogonal to w.\n\nNote that only at most \\( |S2| \\) many terms below are non-zero, hence \\( M_S2 \\) can be computed efficiently."}]}, {"page": 15, "text": "Soundness.          We first prove the soundness condition, i.e., that the following implication holds\nwith probability at least 1 \u2212           \u03b4\u2032 over the samples:\n                         If the tester accepts, then             P\n                                                               DXY[y \u0338= sign(\u27e8w, x\u27e9)] \u2264          \u03b1 \u00b7 opt + \u03f5\nThe tester accepts only if for every \u03c3 \u2208              \u03a3, we have some w \u2208            L with \u2225\u2207wL\u03c3(w; S2)\u22252 \u2264              A (step 4)\nand for which part (a) of each of Lemmas 4.4 (step 6) and 3.1 (step 7) is activated. Therefore, in\nthe Massart noise case, for any \u03c3 \u2208              \u03a3, there is some w such that either (w, \u03c3) \u2208                   L or (\u2212w, \u03c3) \u2208        L\nand also\n                                                                                        def\n                                                     \u2221(w, w\u2217    S2) \u2264   1 + \u03b34    \u00b7 \u03c3   = \u03b8                                       (4.2)\n                                                                           \u03b34       A\n                                         P                                                                                        (4.3)\n                                         S2[y \u0338= sign(\u27e8w, x\u27e9)] \u2264        optS2 + C\u2032\u03bbC\u2032 \u00b7 \u03b8                                        \u03c3\nIn the adversarial noise case, the above are true conditional on \u03c3 being such that opt            E(1\u22122\u03b7)             S2 \u2264    C\u2032\u03bbC\u2032 .\n     Therefore, in the Massart noise case, the above are true for \u03c3 =                          C1\u03bbC1(1+\u03b34) which gives\n                                          P\n                                          S2[y \u0338= sign(\u27e8w, x\u27e9)] \u2264        optS2 + C\u2032\u03bbC\u2032E\n                                                                                    1\nIn the agnostic case, condition 4.3 is true for some \u03c3 \u2208                     [0, C1\u03bbC1 ] such that\n                                                \u03c3            1                        \u03c3\n                          1                  C\u2032\u03bbC\u2032 \u2212      C1\u03bbC1 \u2264      optS2 \u2264     C\u2032\u03bbC\u2032\nunless opt >       C1C\u2032\u03bbC1+C\u2032 , in which case any halfspace has error at most 1 = opt \u00b7 (C1C\u2032\u03bbC1+C\u2032).\nHence we obtain\n                             P\n                            S2[y \u0338= sign(\u27e8w, x\u27e9)] \u2264         poly(\u03bb) \u00b7 (1 + \u03b34) \u00b7 optS2 + C\u2032\u03bbC\u2032E\nSoundness follows from the fact that if |S2| is sufficiently large (but still polynomial in every\nparameter, since the VC dimension of the class of halfspaces in Rd is d + 1), then |optS2 \u2212                                     opt| \u2264\n       \u03f5\nC1\u03bbC1(1+\u03b3)4 with probability at least 1 \u2212                \u03b4\u2032.\nCompleteness.             Suppose now that the marginal is indeed \u03bb-nice and \u03b3-Poincar\u00b4e.                                  Then, for\nsufficiently large S1, after step 3, L will contain a stationary point of L\u03c3( \u00b7 ; DXY) for each \u03c3 \u2208                                  \u03a3,\ndue to Proposition A.4. If S2 is large enough, then steps 4, 6 and 7 will each accept with probability\nat least 1 \u2212    \u03b4\u2032/C1, due to part (b) of Lemmas 4.4 and 3.1, as well as the fact that each coordinate\nof \u2207wL\u03c3(w; S2) has bounded second moment (Proposition A.3) and therefore \u2207wL\u03c3(w; S2) is\nconcentrated around \u2207wL\u03c3(w; DXY) for any fixed w such that (w, \u03c3) \u2208                                   L (we also need a union\nbound over L). Hence, in total, the tester will accept with probability at least 1 \u2212                               \u03b4\u2032.\nReferences\n[ABL17]          Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization\n                 for efficiently learning linear separators with noise.                      Journal of the ACM (JACM),\n                 63(6):1\u201327, 2017. 1\n[BH21]           Maria-Florina Balcan and Nika Haghtalab. Noise in classification. Beyond the Worst-\n                 Case Analysis of Algorithms, page 361, 2021. 1\n                                                                  15", "md": "# Soundness and Completeness\n\n## Soundness\n\nWe first prove the soundness condition, i.e., that the following implication holds with probability at least 1 - \u03b4' over the samples:\n\nIf the tester accepts, then $$P_{DXY}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\alpha \\cdot \\text{opt} + \\epsilon$$\n\nThe tester accepts only if for every $$\\sigma \\in \\Sigma$$, we have some $$w \\in L$$ with $$\\| \\nabla_w L_\\sigma(w; S2) \\|_2 \\leq A$$ (step 4) and for which part (a) of each of Lemmas 4.4 (step 6) and 3.1 (step 7) is activated. Therefore, in the Massart noise case, for any $$\\sigma \\in \\Sigma$$, there is some $$w$$ such that either $$(w, \\sigma) \\in L$$ or $$(-w, \\sigma) \\in L$$ and also\n\n$$\\angle(w, w^*; S2) \\leq 1 + \\gamma_4 \\cdot \\sigma = \\theta$$ (4.2)\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{opt}_{S2} + C'\\lambda C' \\cdot \\theta$$ (4.3)\n\nIn the adversarial noise case, the above are true conditional on $$\\sigma$$ being such that $$\\text{opt} \\leq E(1-2\\eta) \\cdot \\text{opt}_{S2} \\leq C'\\lambda C'$$.\n\nTherefore, in the Massart noise case, the above are true for $$\\sigma = C1\\lambda C1(1+\\gamma_4)$$ which gives\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{opt}_{S2} + C'\\lambda C'E_1$$\n\nIn the agnostic case, condition 4.3 is true for some $$\\sigma \\in [0, C1\\lambda C1]$$ such that\n\n$$C'\\lambda C' - C1\\lambda C1 \\leq \\text{opt}_{S2} \\leq C'\\lambda C'$$\n\nunless $$\\text{opt} > C1C'\\lambda C1+C'$$, in which case any halfspace has error at most $$1 = \\text{opt} \\cdot (C1C'\\lambda C1+C')$$.\n\nHence we obtain\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{poly}(\\lambda) \\cdot (1 + \\gamma_4) \\cdot \\text{opt}_{S2} + C'\\lambda C'E$$\n\nSoundness follows from the fact that if $$|S2|$$ is sufficiently large (but still polynomial in every parameter, since the VC dimension of the class of halfspaces in $$\\mathbb{R}^d$$ is $$d + 1$$), then $$|\\text{opt}_{S2} - \\text{opt}| \\leq \\frac{\\epsilon}{C1\\lambda C1(1+\\gamma)4}$$ with probability at least $$1 - \\delta'$$.\n\n## Completeness\n\nSuppose now that the marginal is indeed $$\\lambda$$-nice and $$\\gamma$$-Poincar\u00e9. Then, for sufficiently large $$S1$$, after step 3, $$L$$ will contain a stationary point of $$L_\\sigma(\\cdot; DXY)$$ for each $$\\sigma \\in \\Sigma$$, due to Proposition A.4. If $$S2$$ is large enough, then steps 4, 6, and 7 will each accept with probability at least $$1 - \\frac{\\delta'}{C1}$$, due to part (b) of Lemmas 4.4 and 3.1, as well as the fact that each coordinate of $$\\nabla_w L_\\sigma(w; S2)$$ has bounded second moment (Proposition A.3) and therefore $$\\nabla_w L_\\sigma(w; S2)$$ is concentrated around $$\\nabla_w L_\\sigma(w; DXY)$$ for any fixed $$w$$ such that $$(w, \\sigma) \\in L$$ (we also need a union bound over $$L$$). Hence, in total, the tester will accept with probability at least $$1 - \\delta'$$.\n\n## References\n\n[ABL17] Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently learning linear separators with noise. Journal of the ACM (JACM), 63(6):1\u201327, 2017.\n\n[BH21] Maria-Florina Balcan and Nika Haghtalab. Noise in classification. Beyond the Worst-Case Analysis of Algorithms, page 361, 2021.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Soundness and Completeness", "md": "# Soundness and Completeness"}, {"type": "heading", "lvl": 2, "value": "Soundness", "md": "## Soundness"}, {"type": "text", "value": "We first prove the soundness condition, i.e., that the following implication holds with probability at least 1 - \u03b4' over the samples:\n\nIf the tester accepts, then $$P_{DXY}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\alpha \\cdot \\text{opt} + \\epsilon$$\n\nThe tester accepts only if for every $$\\sigma \\in \\Sigma$$, we have some $$w \\in L$$ with $$\\| \\nabla_w L_\\sigma(w; S2) \\|_2 \\leq A$$ (step 4) and for which part (a) of each of Lemmas 4.4 (step 6) and 3.1 (step 7) is activated. Therefore, in the Massart noise case, for any $$\\sigma \\in \\Sigma$$, there is some $$w$$ such that either $$(w, \\sigma) \\in L$$ or $$(-w, \\sigma) \\in L$$ and also\n\n$$\\angle(w, w^*; S2) \\leq 1 + \\gamma_4 \\cdot \\sigma = \\theta$$ (4.2)\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{opt}_{S2} + C'\\lambda C' \\cdot \\theta$$ (4.3)\n\nIn the adversarial noise case, the above are true conditional on $$\\sigma$$ being such that $$\\text{opt} \\leq E(1-2\\eta) \\cdot \\text{opt}_{S2} \\leq C'\\lambda C'$$.\n\nTherefore, in the Massart noise case, the above are true for $$\\sigma = C1\\lambda C1(1+\\gamma_4)$$ which gives\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{opt}_{S2} + C'\\lambda C'E_1$$\n\nIn the agnostic case, condition 4.3 is true for some $$\\sigma \\in [0, C1\\lambda C1]$$ such that\n\n$$C'\\lambda C' - C1\\lambda C1 \\leq \\text{opt}_{S2} \\leq C'\\lambda C'$$\n\nunless $$\\text{opt} > C1C'\\lambda C1+C'$$, in which case any halfspace has error at most $$1 = \\text{opt} \\cdot (C1C'\\lambda C1+C')$$.\n\nHence we obtain\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{poly}(\\lambda) \\cdot (1 + \\gamma_4) \\cdot \\text{opt}_{S2} + C'\\lambda C'E$$\n\nSoundness follows from the fact that if $$|S2|$$ is sufficiently large (but still polynomial in every parameter, since the VC dimension of the class of halfspaces in $$\\mathbb{R}^d$$ is $$d + 1$$), then $$|\\text{opt}_{S2} - \\text{opt}| \\leq \\frac{\\epsilon}{C1\\lambda C1(1+\\gamma)4}$$ with probability at least $$1 - \\delta'$$.", "md": "We first prove the soundness condition, i.e., that the following implication holds with probability at least 1 - \u03b4' over the samples:\n\nIf the tester accepts, then $$P_{DXY}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\alpha \\cdot \\text{opt} + \\epsilon$$\n\nThe tester accepts only if for every $$\\sigma \\in \\Sigma$$, we have some $$w \\in L$$ with $$\\| \\nabla_w L_\\sigma(w; S2) \\|_2 \\leq A$$ (step 4) and for which part (a) of each of Lemmas 4.4 (step 6) and 3.1 (step 7) is activated. Therefore, in the Massart noise case, for any $$\\sigma \\in \\Sigma$$, there is some $$w$$ such that either $$(w, \\sigma) \\in L$$ or $$(-w, \\sigma) \\in L$$ and also\n\n$$\\angle(w, w^*; S2) \\leq 1 + \\gamma_4 \\cdot \\sigma = \\theta$$ (4.2)\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{opt}_{S2} + C'\\lambda C' \\cdot \\theta$$ (4.3)\n\nIn the adversarial noise case, the above are true conditional on $$\\sigma$$ being such that $$\\text{opt} \\leq E(1-2\\eta) \\cdot \\text{opt}_{S2} \\leq C'\\lambda C'$$.\n\nTherefore, in the Massart noise case, the above are true for $$\\sigma = C1\\lambda C1(1+\\gamma_4)$$ which gives\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{opt}_{S2} + C'\\lambda C'E_1$$\n\nIn the agnostic case, condition 4.3 is true for some $$\\sigma \\in [0, C1\\lambda C1]$$ such that\n\n$$C'\\lambda C' - C1\\lambda C1 \\leq \\text{opt}_{S2} \\leq C'\\lambda C'$$\n\nunless $$\\text{opt} > C1C'\\lambda C1+C'$$, in which case any halfspace has error at most $$1 = \\text{opt} \\cdot (C1C'\\lambda C1+C')$$.\n\nHence we obtain\n\n$$P_{S2}[y \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq \\text{poly}(\\lambda) \\cdot (1 + \\gamma_4) \\cdot \\text{opt}_{S2} + C'\\lambda C'E$$\n\nSoundness follows from the fact that if $$|S2|$$ is sufficiently large (but still polynomial in every parameter, since the VC dimension of the class of halfspaces in $$\\mathbb{R}^d$$ is $$d + 1$$), then $$|\\text{opt}_{S2} - \\text{opt}| \\leq \\frac{\\epsilon}{C1\\lambda C1(1+\\gamma)4}$$ with probability at least $$1 - \\delta'$$."}, {"type": "heading", "lvl": 2, "value": "Completeness", "md": "## Completeness"}, {"type": "text", "value": "Suppose now that the marginal is indeed $$\\lambda$$-nice and $$\\gamma$$-Poincar\u00e9. Then, for sufficiently large $$S1$$, after step 3, $$L$$ will contain a stationary point of $$L_\\sigma(\\cdot; DXY)$$ for each $$\\sigma \\in \\Sigma$$, due to Proposition A.4. If $$S2$$ is large enough, then steps 4, 6, and 7 will each accept with probability at least $$1 - \\frac{\\delta'}{C1}$$, due to part (b) of Lemmas 4.4 and 3.1, as well as the fact that each coordinate of $$\\nabla_w L_\\sigma(w; S2)$$ has bounded second moment (Proposition A.3) and therefore $$\\nabla_w L_\\sigma(w; S2)$$ is concentrated around $$\\nabla_w L_\\sigma(w; DXY)$$ for any fixed $$w$$ such that $$(w, \\sigma) \\in L$$ (we also need a union bound over $$L$$). Hence, in total, the tester will accept with probability at least $$1 - \\delta'$$.", "md": "Suppose now that the marginal is indeed $$\\lambda$$-nice and $$\\gamma$$-Poincar\u00e9. Then, for sufficiently large $$S1$$, after step 3, $$L$$ will contain a stationary point of $$L_\\sigma(\\cdot; DXY)$$ for each $$\\sigma \\in \\Sigma$$, due to Proposition A.4. If $$S2$$ is large enough, then steps 4, 6, and 7 will each accept with probability at least $$1 - \\frac{\\delta'}{C1}$$, due to part (b) of Lemmas 4.4 and 3.1, as well as the fact that each coordinate of $$\\nabla_w L_\\sigma(w; S2)$$ has bounded second moment (Proposition A.3) and therefore $$\\nabla_w L_\\sigma(w; S2)$$ is concentrated around $$\\nabla_w L_\\sigma(w; DXY)$$ for any fixed $$w$$ such that $$(w, \\sigma) \\in L$$ (we also need a union bound over $$L$$). Hence, in total, the tester will accept with probability at least $$1 - \\delta'$$."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "[ABL17] Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently learning linear separators with noise. Journal of the ACM (JACM), 63(6):1\u201327, 2017.\n\n[BH21] Maria-Florina Balcan and Nika Haghtalab. Noise in classification. Beyond the Worst-Case Analysis of Algorithms, page 361, 2021.", "md": "[ABL17] Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently learning linear separators with noise. Journal of the ACM (JACM), 63(6):1\u201327, 2017.\n\n[BH21] Maria-Florina Balcan and Nika Haghtalab. Noise in classification. Beyond the Worst-Case Analysis of Algorithms, page 361, 2021."}]}, {"page": 16, "text": "[BK21]        Ainesh Bakshi and Pravesh K Kothari.        List-decodable subspace recovery: Dimen-\n              sion independent error in polynomial time. In Proceedings of the 2021 ACM-SIAM\n              Symposium on Discrete Algorithms (SODA), pages 1279\u20131297. SIAM, 2021. 1\n[BZ17]        Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient\n              learning algorithms under s-concave distributions. Advances in Neural Information\n              Processing Systems, 30, 2017. 1\n[Dan15]       Amit Daniely. A ptas for agnostically learning halfspaces. In Conference on Learning\n              Theory, pages 484\u2013502. PMLR, 2015. 1\n[DKK+22]      Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos\n              Zarifis. Learning general halfspaces with general massart noise under the gaussian\n              distribution. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory\n              of Computing, pages 874\u2013885, 2022. 1\n[DKK+23]      Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis.\n              Efficient testable learning of halfspaces with adversarial label noise. arXiv preprint\n              arXiv:2303.05485, 2023. 1, 1, 1\n[DKPZ21]      Ilias Diakonikolas, Daniel M Kane, Thanasis Pittas, and Nikos Zarifis. The optimality\n              of polynomial regression for agnostic learning under gaussian marginals in the sq model.\n              In Conference on Learning Theory, pages 1552\u20131584. PMLR, 2021. 1\n[DKR23]       Ilias Diakonikolas, Daniel M Kane, and Lisheng Ren. Near-optimal cryptographic hard-\n              ness of agnostically learning halfspaces and relu regression under gaussian marginals.\n              arXiv preprint arXiv:2302.06512, 2023. 1\n[DKS18]       Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts\n              with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on\n              Theory of Computing, pages 1061\u20131073, 2018. 1\n[DKTZ20a] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning half-\n              spaces with massart noise under structured distributions. In Conference on Learning\n              Theory, pages 1486\u20131513. PMLR, 2020. 1, 1, 2, 4, 4, 4.3, A.4, C.1\n[DKTZ20b] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex\n              sgd learns halfspaces with adversarial label noise. Advances in Neural Information\n              Processing Systems, 33:18540\u201318549, 2020. 1, 1, 2, 2, 4, 4, 4.3, C.1\n[DKTZ22]      Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning gen-\n              eral halfspaces with adversarial label noise via online gradient descent. In International\n              Conference on Machine Learning, pages 5118\u20135141. PMLR, 2022. 1\n[DKZ20]       Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis.      Near-optimal sq lower bounds\n              for agnostically learning halfspaces and relus under gaussian marginals. Advances in\n              Neural Information Processing Systems, 33:13586\u201313596, 2020. 1\n[FKP+19]      Noah Fleming, Pravesh Kothari, Toniann Pitassi, et al.         Semialgebraic proofs and\n              efficient algorithm design. Foundations and Trends\u00ae in Theoretical Computer Science,\n              14(1-2):1\u2013221, 2019. 1\n                                                   16", "md": "# References\n\n# List of References\n\n|[BK21]|Ainesh Bakshi and Pravesh K Kothari. List-decodable subspace recovery: Dimension independent error in polynomial time. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1279\u20131297. SIAM, 2021.|\n|---|---|\n|[BZ17]|Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient learning algorithms under s-concave distributions. Advances in Neural Information Processing Systems, 30, 2017.|\n|[Dan15]|Amit Daniely. A ptas for agnostically learning halfspaces. In Conference on Learning Theory, pages 484\u2013502. PMLR, 2015.|\n|[DKK+22]|Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with general massart noise under the gaussian distribution. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 874\u2013885, 2022.|\n|[DKK+23]|Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Efficient testable learning of halfspaces with adversarial label noise. arXiv preprint arXiv:2303.05485, 2023.|\n|[DKPZ21]|Ilias Diakonikolas, Daniel M Kane, Thanasis Pittas, and Nikos Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model. In Conference on Learning Theory, pages 1552\u20131584. PMLR, 2021.|\n|[DKR23]|Ilias Diakonikolas, Daniel M Kane, and Lisheng Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. arXiv preprint arXiv:2302.06512, 2023.|\n|[DKS18]|Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061\u20131073, 2018.|\n|[DKTZ20a]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning half-spaces with massart noise under structured distributions. In Conference on Learning Theory, pages 1486\u20131513. PMLR, 2020.|\n|[DKTZ20b]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex sgd learns halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 33:18540\u201318549, 2020.|\n|[DKTZ22]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with adversarial label noise via online gradient descent. In International Conference on Machine Learning, pages 5118\u20135141. PMLR, 2022.|\n|[DKZ20]|Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.|\n|[FKP+19]|Noah Fleming, Pravesh Kothari, Toniann Pitassi, et al. Semialgebraic proofs and efficient algorithm design. Foundations and Trends\u00ae in Theoretical Computer Science, 14(1-2):1\u2013221, 2019.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["[BK21]", "Ainesh Bakshi and Pravesh K Kothari. List-decodable subspace recovery: Dimension independent error in polynomial time. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1279\u20131297. SIAM, 2021."], ["[BZ17]", "Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient learning algorithms under s-concave distributions. Advances in Neural Information Processing Systems, 30, 2017."], ["[Dan15]", "Amit Daniely. A ptas for agnostically learning halfspaces. In Conference on Learning Theory, pages 484\u2013502. PMLR, 2015."], ["[DKK+22]", "Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with general massart noise under the gaussian distribution. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 874\u2013885, 2022."], ["[DKK+23]", "Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Efficient testable learning of halfspaces with adversarial label noise. arXiv preprint arXiv:2303.05485, 2023."], ["[DKPZ21]", "Ilias Diakonikolas, Daniel M Kane, Thanasis Pittas, and Nikos Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model. In Conference on Learning Theory, pages 1552\u20131584. PMLR, 2021."], ["[DKR23]", "Ilias Diakonikolas, Daniel M Kane, and Lisheng Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. arXiv preprint arXiv:2302.06512, 2023."], ["[DKS18]", "Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061\u20131073, 2018."], ["[DKTZ20a]", "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning half-spaces with massart noise under structured distributions. In Conference on Learning Theory, pages 1486\u20131513. PMLR, 2020."], ["[DKTZ20b]", "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex sgd learns halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 33:18540\u201318549, 2020."], ["[DKTZ22]", "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with adversarial label noise via online gradient descent. In International Conference on Machine Learning, pages 5118\u20135141. PMLR, 2022."], ["[DKZ20]", "Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020."], ["[FKP+19]", "Noah Fleming, Pravesh Kothari, Toniann Pitassi, et al. Semialgebraic proofs and efficient algorithm design. Foundations and Trends\u00ae in Theoretical Computer Science, 14(1-2):1\u2013221, 2019."]], "md": "|[BK21]|Ainesh Bakshi and Pravesh K Kothari. List-decodable subspace recovery: Dimension independent error in polynomial time. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1279\u20131297. SIAM, 2021.|\n|---|---|\n|[BZ17]|Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient learning algorithms under s-concave distributions. Advances in Neural Information Processing Systems, 30, 2017.|\n|[Dan15]|Amit Daniely. A ptas for agnostically learning halfspaces. In Conference on Learning Theory, pages 484\u2013502. PMLR, 2015.|\n|[DKK+22]|Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with general massart noise under the gaussian distribution. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 874\u2013885, 2022.|\n|[DKK+23]|Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Efficient testable learning of halfspaces with adversarial label noise. arXiv preprint arXiv:2303.05485, 2023.|\n|[DKPZ21]|Ilias Diakonikolas, Daniel M Kane, Thanasis Pittas, and Nikos Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model. In Conference on Learning Theory, pages 1552\u20131584. PMLR, 2021.|\n|[DKR23]|Ilias Diakonikolas, Daniel M Kane, and Lisheng Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. arXiv preprint arXiv:2302.06512, 2023.|\n|[DKS18]|Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061\u20131073, 2018.|\n|[DKTZ20a]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning half-spaces with massart noise under structured distributions. In Conference on Learning Theory, pages 1486\u20131513. PMLR, 2020.|\n|[DKTZ20b]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex sgd learns halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 33:18540\u201318549, 2020.|\n|[DKTZ22]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with adversarial label noise via online gradient descent. In International Conference on Machine Learning, pages 5118\u20135141. PMLR, 2022.|\n|[DKZ20]|Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.|\n|[FKP+19]|Noah Fleming, Pravesh Kothari, Toniann Pitassi, et al. Semialgebraic proofs and efficient algorithm design. Foundations and Trends\u00ae in Theoretical Computer Science, 14(1-2):1\u2013221, 2019.|", "isPerfectTable": true, "csv": "\"[BK21]\",\"Ainesh Bakshi and Pravesh K Kothari. List-decodable subspace recovery: Dimension independent error in polynomial time. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1279\u20131297. SIAM, 2021.\"\n\"[BZ17]\",\"Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient learning algorithms under s-concave distributions. Advances in Neural Information Processing Systems, 30, 2017.\"\n\"[Dan15]\",\"Amit Daniely. A ptas for agnostically learning halfspaces. In Conference on Learning Theory, pages 484\u2013502. PMLR, 2015.\"\n\"[DKK+22]\",\"Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with general massart noise under the gaussian distribution. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 874\u2013885, 2022.\"\n\"[DKK+23]\",\"Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Efficient testable learning of halfspaces with adversarial label noise. arXiv preprint arXiv:2303.05485, 2023.\"\n\"[DKPZ21]\",\"Ilias Diakonikolas, Daniel M Kane, Thanasis Pittas, and Nikos Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model. In Conference on Learning Theory, pages 1552\u20131584. PMLR, 2021.\"\n\"[DKR23]\",\"Ilias Diakonikolas, Daniel M Kane, and Lisheng Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. arXiv preprint arXiv:2302.06512, 2023.\"\n\"[DKS18]\",\"Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061\u20131073, 2018.\"\n\"[DKTZ20a]\",\"Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning half-spaces with massart noise under structured distributions. In Conference on Learning Theory, pages 1486\u20131513. PMLR, 2020.\"\n\"[DKTZ20b]\",\"Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex sgd learns halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 33:18540\u201318549, 2020.\"\n\"[DKTZ22]\",\"Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with adversarial label noise via online gradient descent. In International Conference on Machine Learning, pages 5118\u20135141. PMLR, 2022.\"\n\"[DKZ20]\",\"Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. Advances in Neural Information Processing Systems, 33:13586\u201313596, 2020.\"\n\"[FKP+19]\",\"Noah Fleming, Pravesh Kothari, Toniann Pitassi, et al. Semialgebraic proofs and efficient algorithm design. Foundations and Trends\u00ae in Theoretical Computer Science, 14(1-2):1\u2013221, 2019.\""}]}, {"page": 17, "text": "[GGK20]   Surbhi Goel, Aravind Gollakota, and Adam Klivans. Statistical-query lower bounds\n          via functional gradients. Advances in Neural Information Processing Systems, 33:2147\u2013\n          2158, 2020. 1\n[GKK23]   Aravind Gollakota, Adam R Klivans, and Pravesh K Kothari. A moment-matching\n          approach to testable learning and a new characterization of rademacher complexity.\n          Proceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023.\n          To appear. 1, 1, 1, 1\n[GKSV23]  Aravind Gollakota, Adam R Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan.\n          An efficient tester-learner for halfspaces. arXiv preprint arXiv:2302.14853, 2023. 1, 1,\n          1, 1, 2, 2, 3.1, 4, 4.2, 4, 4.3, A.4, C.1, C\n[KKK19]   Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari. List-decodable linear regres-\n          sion. Advances in neural information processing systems, 32, 2019. 1\n[KLS95]   Ravi Kannan, L\u00b4   aszl\u00b4\n                                o Lov\u00b4asz, and Mikl\u00b4 os Simonovits. Isoperimetric problems for con-\n          vex bodies and a localization lemma. Discrete & Computational Geometry, 13:541\u2013559,\n          1995. 2, 2.6\n[KLS09]   Adam R Klivans, Philip M Long, and Rocco A Servedio. Learning halfspaces with\n          malicious noise. Journal of Machine Learning Research, 10(12), 2009. 1\n[KS17]    Pravesh K Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor\n          norms. arXiv preprint arXiv:1711.07465, 2017. 1, 1, 3.2, 3.4, 1\n[Las01]   Jean B Lasserre. New positive semidefinite relaxations for nonconvex quadratic pro-\n          grams. Advances in Convex Analysis and Global Optimization: Honoring the Memory\n          of C. Caratheodory (1873\u20131950), pages 319\u2013331, 2001. 3.2\n[LV07]    L\u00b4aszl\u00b4\n                o Lov\u00b4asz and Santosh Vempala. The geometry of logconcave functions and sam-\n          pling algorithms. Random Structures & Algorithms, 30(3):307\u2013358, 2007. 2.3\n[LV18]    Yin Tat Lee and Santosh S Vempala.            The Kannan-Lov\u00b4     asz-Simonovits conjecture.\n          arXiv preprint arXiv:1807.03465, 2018. 2.6\n[Nes00]   Yurii Nesterov. Squared functional systems and optimization problems. High perfor-\n          mance optimization, pages 405\u2013440, 2000. 3.2\n[Par00]   Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods\n          in robustness and optimization. California Institute of Technology, 2000. 3.2\n[RV23]    Ronitt Rubinfeld and Arsen Vasilyan.          Testing distributional assumptions of learn-\n          ing algorithms. Proceedings of the fifty-fifth annual ACM Symposium on Theory of\n          Computing, 2023. To appear. 1, 1, 1\n[RY20]    Prasad Raghavendra and Morris Yau. List decodable learning via sum of squares. In\n          Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms,\n          pages 161\u2013180. SIAM, 2020. 1\n[Sho87]   N.Z. Shor. Quadratic optimization problems. Izv. Akad. Nauk SSSR Tekhn. Kibernet.,\n          1987(1):128\u2013139, 222, 1987. 3.2\n                                                  17", "md": "# References\n\n## List of References\n\n- [GGK20] Surbhi Goel, Aravind Gollakota, and Adam Klivans. Statistical-query lower bounds\nvia functional gradients. Advances in Neural Information Processing Systems, 33:2147\u20132158, 2020.\n- [GKK23] Aravind Gollakota, Adam R Klivans, and Pravesh K Kothari. A moment-matching\napproach to testable learning and a new characterization of rademacher complexity.\nProceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023. To appear.\n- [GKSV23] Aravind Gollakota, Adam R Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan.\nAn efficient tester-learner for halfspaces. arXiv preprint arXiv:2302.14853, 2023.\n- [KKK19] Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari. List-decodable linear regression.\nAdvances in neural information processing systems, 32, 2019.\n- [KLS95] Ravi Kannan, L\u00e1szl\u00f3 Lov\u00e1sz, and Mikl\u00f3s Simonovits. Isoperimetric problems for convex bodies\nand a localization lemma. Discrete & Computational Geometry, 13:541\u2013559, 1995.\n- [KLS09] Adam R Klivans, Philip M Long, and Rocco A Servedio. Learning halfspaces with malicious noise.\nJournal of Machine Learning Research, 10(12), 2009.\n- [KS17] Pravesh K Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms.\narXiv preprint arXiv:1711.07465, 2017.\n- [Las01] Jean B Lasserre. New positive semidefinite relaxations for nonconvex quadratic programs.\nAdvances in Convex Analysis and Global Optimization: Honoring the Memory of C. Caratheodory (1873\u20131950),\npages 319\u2013331, 2001.\n- [LV07] L\u00e1szl\u00f3 Lov\u00e1sz and Santosh Vempala. The geometry of logconcave functions and sampling algorithms.\nRandom Structures & Algorithms, 30(3):307\u2013358, 2007.\n- [LV18] Yin Tat Lee and Santosh S Vempala. The Kannan-Lov\u00e1sz-Simonovits conjecture.\narXiv preprint arXiv:1807.03465, 2018.\n- [Nes00] Yurii Nesterov. Squared functional systems and optimization problems. High performance optimization,\npages 405\u2013440, 2000.\n- [Par00] Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness\nand optimization. California Institute of Technology, 2000.\n- [RV23] Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learning algorithms.\nProceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023. To appear.\n- [RY20] Prasad Raghavendra and Morris Yau. List decodable learning via sum of squares. In Proceedings of\nthe Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 161\u2013180. SIAM, 2020.\n- [Sho87] N.Z. Shor. Quadratic optimization problems. Izv. Akad. Nauk SSSR Tekhn. Kibernet., 1987(1):128\u2013139, 222, 1987.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "List of References", "md": "## List of References"}, {"type": "text", "value": "- [GGK20] Surbhi Goel, Aravind Gollakota, and Adam Klivans. Statistical-query lower bounds\nvia functional gradients. Advances in Neural Information Processing Systems, 33:2147\u20132158, 2020.\n- [GKK23] Aravind Gollakota, Adam R Klivans, and Pravesh K Kothari. A moment-matching\napproach to testable learning and a new characterization of rademacher complexity.\nProceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023. To appear.\n- [GKSV23] Aravind Gollakota, Adam R Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan.\nAn efficient tester-learner for halfspaces. arXiv preprint arXiv:2302.14853, 2023.\n- [KKK19] Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari. List-decodable linear regression.\nAdvances in neural information processing systems, 32, 2019.\n- [KLS95] Ravi Kannan, L\u00e1szl\u00f3 Lov\u00e1sz, and Mikl\u00f3s Simonovits. Isoperimetric problems for convex bodies\nand a localization lemma. Discrete & Computational Geometry, 13:541\u2013559, 1995.\n- [KLS09] Adam R Klivans, Philip M Long, and Rocco A Servedio. Learning halfspaces with malicious noise.\nJournal of Machine Learning Research, 10(12), 2009.\n- [KS17] Pravesh K Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms.\narXiv preprint arXiv:1711.07465, 2017.\n- [Las01] Jean B Lasserre. New positive semidefinite relaxations for nonconvex quadratic programs.\nAdvances in Convex Analysis and Global Optimization: Honoring the Memory of C. Caratheodory (1873\u20131950),\npages 319\u2013331, 2001.\n- [LV07] L\u00e1szl\u00f3 Lov\u00e1sz and Santosh Vempala. The geometry of logconcave functions and sampling algorithms.\nRandom Structures & Algorithms, 30(3):307\u2013358, 2007.\n- [LV18] Yin Tat Lee and Santosh S Vempala. The Kannan-Lov\u00e1sz-Simonovits conjecture.\narXiv preprint arXiv:1807.03465, 2018.\n- [Nes00] Yurii Nesterov. Squared functional systems and optimization problems. High performance optimization,\npages 405\u2013440, 2000.\n- [Par00] Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness\nand optimization. California Institute of Technology, 2000.\n- [RV23] Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learning algorithms.\nProceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023. To appear.\n- [RY20] Prasad Raghavendra and Morris Yau. List decodable learning via sum of squares. In Proceedings of\nthe Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 161\u2013180. SIAM, 2020.\n- [Sho87] N.Z. Shor. Quadratic optimization problems. Izv. Akad. Nauk SSSR Tekhn. Kibernet., 1987(1):128\u2013139, 222, 1987.", "md": "- [GGK20] Surbhi Goel, Aravind Gollakota, and Adam Klivans. Statistical-query lower bounds\nvia functional gradients. Advances in Neural Information Processing Systems, 33:2147\u20132158, 2020.\n- [GKK23] Aravind Gollakota, Adam R Klivans, and Pravesh K Kothari. A moment-matching\napproach to testable learning and a new characterization of rademacher complexity.\nProceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023. To appear.\n- [GKSV23] Aravind Gollakota, Adam R Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan.\nAn efficient tester-learner for halfspaces. arXiv preprint arXiv:2302.14853, 2023.\n- [KKK19] Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari. List-decodable linear regression.\nAdvances in neural information processing systems, 32, 2019.\n- [KLS95] Ravi Kannan, L\u00e1szl\u00f3 Lov\u00e1sz, and Mikl\u00f3s Simonovits. Isoperimetric problems for convex bodies\nand a localization lemma. Discrete & Computational Geometry, 13:541\u2013559, 1995.\n- [KLS09] Adam R Klivans, Philip M Long, and Rocco A Servedio. Learning halfspaces with malicious noise.\nJournal of Machine Learning Research, 10(12), 2009.\n- [KS17] Pravesh K Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms.\narXiv preprint arXiv:1711.07465, 2017.\n- [Las01] Jean B Lasserre. New positive semidefinite relaxations for nonconvex quadratic programs.\nAdvances in Convex Analysis and Global Optimization: Honoring the Memory of C. Caratheodory (1873\u20131950),\npages 319\u2013331, 2001.\n- [LV07] L\u00e1szl\u00f3 Lov\u00e1sz and Santosh Vempala. The geometry of logconcave functions and sampling algorithms.\nRandom Structures & Algorithms, 30(3):307\u2013358, 2007.\n- [LV18] Yin Tat Lee and Santosh S Vempala. The Kannan-Lov\u00e1sz-Simonovits conjecture.\narXiv preprint arXiv:1807.03465, 2018.\n- [Nes00] Yurii Nesterov. Squared functional systems and optimization problems. High performance optimization,\npages 405\u2013440, 2000.\n- [Par00] Pablo A Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness\nand optimization. California Institute of Technology, 2000.\n- [RV23] Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learning algorithms.\nProceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023. To appear.\n- [RY20] Prasad Raghavendra and Morris Yau. List decodable learning via sum of squares. In Proceedings of\nthe Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 161\u2013180. SIAM, 2020.\n- [Sho87] N.Z. Shor. Quadratic optimization problems. Izv. Akad. Nauk SSSR Tekhn. Kibernet., 1987(1):128\u2013139, 222, 1987."}]}, {"page": 18, "text": "[SW14]             Adrien Saumard and Jon A Wellner. Log-concavity and strong log-concavity: a review.\n                   Statistics surveys, 8:45, 2014. 2.2, 2.5\n[YZ17]             Songbai Yan and Chicheng Zhang. Revisiting perceptron: Efficient and label-optimal\n                   learning of halfspaces. Advances in Neural Information Processing Systems, 30, 2017.\n                   1\n[Zha18]            Chicheng Zhang.              Efficient active learning of sparse halfspaces.                           In Conference on\n                   Learning Theory, pages 1856\u20131880. PMLR, 2018. 1\n[ZL21]             Chicheng Zhang and Yinan Li. Improved algorithms for efficient active learning half-\n                   spaces with massart and tsybakov noise. In Conference on Learning Theory, pages\n                   4526\u20134527. PMLR, 2021. 1\n[ZSA20]            Chicheng Zhang, Jie Shen, and Pranjal Awasthi. Efficient active learning of sparse\n                   halfspaces with arbitrary bounded noise. Advances in Neural Information Processing\n                   Systems, 33:7184\u20137197, 2020. 1\nA        Technical Lemmas\nIn this section, we provide a list of technical results that we use in our proofs.\nLemma A.1 (Preservation of Poincar\u00b4e constant). Let I be an open interval in R and q : Rd \u2192                                                      R+\nthe density of a \u03b3-Poincar\u00b4e distribution. Let v \u2208                           Sd\u22121 and q\u2032     v : Rd\u22121 \u2192         R+ be the density of the\ndistribution resulting from conditioning q to x \u00b7 v \u2208                         I and projecting on the subspace perpendicular\nto v. Then, the distribution corresponding to q\u2032                       v is \u03b3-Poincar\u00b4       e.\nProof. Assume, without loss of generality, that v = ed. We have that\n                                  q\u2032                xd\u2208I q(x<d, xd) dxd\n                                   v(x<d) =           x<d    xd\u2208I q(x) dx , for any x<d \u2208                 Rd\u22121 .\nLet f : Rd\u22121 \u2192            R be any differentiable function. In order to show that q\u2032                                 v is \u03b3-Poincar\u00b4       e, it is\nsufficient to show that under no further assumptions on f, the quantity Varq\u2032                                           v(f(x<d)) is upper\nbounded by the product of \u03b3 and Eq\u2032                       v[\u2225\u2207f(x<d)\u22252       2]. We expand the quantity Varq\u2032                     v(f(x<d)) as\nfollows          Varq\u2032 v(f(x<d)) = inf       \u03c4    x <d (f(x<d) \u2212       \u03c4)2q\u2032  v(x<d) dx<d\n                                        = inf\u03c4    x    (f(x<d) \u2212       \u03c4)2 \u00b7     xd\u2208I q(x<d, xd) dxd\n                                                    <d                            x <d   xd\u2208I q(x) dx dx<d\n                                        =   inf\u03c4    x<d    xd\u2208I(f(x<d) \u2212         \u03c4)2 \u00b7 q(x) dxd dx<d\n                                                                 x<d    xd\u2208I q(x) dx\n                                        \u2264   \u03b3 \u00b7   x<d    xd\u2208I \u2225\u2207xf(x<d)\u22252          2 \u00b7 q(x) dx                     (since q is \u03b3-Poincar\u00b4e)\n                                                           x <d   xd\u2208I q(x) dx\n                                        = \u03b3 \u00b7          \u2225\u2207x<df(x<d)\u22252         2 \u00b7 q\u2032v(x<d) dx<d                                (since     \u2202f\n                                                  x<d                                                                                    \u2202x d \u2261   0)\n                                        = \u03b3 \u00b7 E[\u2225\u2207f(x<d)\u22252          2] ,\n                                                q\u2032\n                                                 v\nwhich concludes the proof.\n                                                                         18", "md": "[SW14] Adrien Saumard and Jon A Wellner. Log-concavity and strong log-concavity: a review. Statistics surveys, 8:45, 2014. 2.2, 2.5\n\n[YZ17] Songbai Yan and Chicheng Zhang. Revisiting perceptron: Efficient and label-optimal learning of halfspaces. Advances in Neural Information Processing Systems, 30, 2017. 1\n\n[Zha18] Chicheng Zhang. Efficient active learning of sparse halfspaces. In Conference on Learning Theory, pages 1856\u20131880. PMLR, 2018. 1\n\n[ZL21] Chicheng Zhang and Yinan Li. Improved algorithms for efficient active learning halfspaces with massart and tsybakov noise. In Conference on Learning Theory, pages 4526\u20134527. PMLR, 2021. 1\n\n[ZSA20] Chicheng Zhang, Jie Shen, and Pranjal Awasthi. Efficient active learning of sparse halfspaces with arbitrary bounded noise. Advances in Neural Information Processing Systems, 33:7184\u20137197, 2020. 1\n\nA Technical Lemmas\n\nIn this section, we provide a list of technical results that we use in our proofs.\n\nLemma A.1 (Preservation of Poincar\u00e9 constant). Let I be an open interval in R and $$q : \\mathbb{R}^d \\rightarrow \\mathbb{R}^+$$ the density of a \u03b3-Poincar\u00e9 distribution. Let $$v \\in S^{d-1}$$ and $$q'_{v} : \\mathbb{R}^{d-1} \\rightarrow \\mathbb{R}^+$$ be the density of the distribution resulting from conditioning q to $$x \\cdot v \\in I$$ and projecting on the subspace perpendicular to v. Then, the distribution corresponding to $$q'_{v}$$ is \u03b3-Poincar\u00e9.\n\nProof. Assume, without loss of generality, that $$v = e_d$$. We have that\n\n$$\n\\begin{aligned}\nq'_{v}(x_{<d}) & = \\int_{x_d \\in I} q(x_{<d}, x_d) \\, dx_d \\\\\n& = \\int_{x_{<d}} \\left( \\int_{x_d \\in I} q(x) \\, dx \\right) \\, dx_{<d} \\quad \\text{for any } x_{<d} \\in \\mathbb{R}^{d-1}.\n\\end{aligned}\n$$\nLet $$f : \\mathbb{R}^{d-1} \\rightarrow \\mathbb{R}$$ be any differentiable function. In order to show that $$q'_{v}$$ is \u03b3-Poincar\u00e9, it is sufficient to show that under no further assumptions on f, the quantity $$\\text{Var}_{q'_{v}}(f(x_{<d}))$$ is upper bounded by the product of \u03b3 and $$\\mathbb{E}_{q'_{v}}[\\| \\nabla f(x_{<d}) \\|_2^2]$$. We expand the quantity $$\\text{Var}_{q'_{v}}(f(x_{<d}))$$ as follows\n\n$$\n\\begin{aligned}\n\\text{Var}_{q'_{v}}(f(x_{<d})) & = \\inf_{\\tau} \\int_{x_{<d}} (f(x_{<d}) - \\tau)^2 q'_{v}(x_{<d}) \\, dx_{<d} \\\\\n& = \\inf_{\\tau} \\int_{x_{<d}} (f(x_{<d}) - \\tau)^2 \\cdot \\left( \\int_{x_d \\in I} q(x_{<d}, x_d) \\, dx_d \\right) \\, dx_{<d} \\\\\n& = \\inf_{\\tau} \\int_{x_{<d}} \\left( f(x_{<d}) - \\tau \\right)^2 \\cdot q(x) \\, dx_{<d} \\, dx_{<d} \\\\\n& \\leq \\gamma \\cdot \\int_{x_{<d}} \\left( \\int_{x_d \\in I} \\| \\nabla_x f(x_{<d}) \\|_2^2 q(x) \\, dx_d \\right) \\, dx_{<d} \\quad \\text{(since q is \u03b3-Poincar\u00e9)} \\\\\n& = \\gamma \\cdot \\int_{x_{<d}} \\| \\nabla_{x_{<d}} f(x_{<d}) \\|_2^2 \\cdot q'_{v}(x_{<d}) \\, dx_{<d} \\quad \\text{(since } \\frac{\\partial f}{\\partial x_d} \\equiv 0) \\\\\n& = \\gamma \\cdot \\mathbb{E}_{q'_{v}}[\\| \\nabla f(x_{<d}) \\|_2^2],\n\\end{aligned}\n$$\nwhich concludes the proof.", "images": [], "items": [{"type": "text", "value": "[SW14] Adrien Saumard and Jon A Wellner. Log-concavity and strong log-concavity: a review. Statistics surveys, 8:45, 2014. 2.2, 2.5\n\n[YZ17] Songbai Yan and Chicheng Zhang. Revisiting perceptron: Efficient and label-optimal learning of halfspaces. Advances in Neural Information Processing Systems, 30, 2017. 1\n\n[Zha18] Chicheng Zhang. Efficient active learning of sparse halfspaces. In Conference on Learning Theory, pages 1856\u20131880. PMLR, 2018. 1\n\n[ZL21] Chicheng Zhang and Yinan Li. Improved algorithms for efficient active learning halfspaces with massart and tsybakov noise. In Conference on Learning Theory, pages 4526\u20134527. PMLR, 2021. 1\n\n[ZSA20] Chicheng Zhang, Jie Shen, and Pranjal Awasthi. Efficient active learning of sparse halfspaces with arbitrary bounded noise. Advances in Neural Information Processing Systems, 33:7184\u20137197, 2020. 1\n\nA Technical Lemmas\n\nIn this section, we provide a list of technical results that we use in our proofs.\n\nLemma A.1 (Preservation of Poincar\u00e9 constant). Let I be an open interval in R and $$q : \\mathbb{R}^d \\rightarrow \\mathbb{R}^+$$ the density of a \u03b3-Poincar\u00e9 distribution. Let $$v \\in S^{d-1}$$ and $$q'_{v} : \\mathbb{R}^{d-1} \\rightarrow \\mathbb{R}^+$$ be the density of the distribution resulting from conditioning q to $$x \\cdot v \\in I$$ and projecting on the subspace perpendicular to v. Then, the distribution corresponding to $$q'_{v}$$ is \u03b3-Poincar\u00e9.\n\nProof. Assume, without loss of generality, that $$v = e_d$$. We have that\n\n$$\n\\begin{aligned}\nq'_{v}(x_{<d}) & = \\int_{x_d \\in I} q(x_{<d}, x_d) \\, dx_d \\\\\n& = \\int_{x_{<d}} \\left( \\int_{x_d \\in I} q(x) \\, dx \\right) \\, dx_{<d} \\quad \\text{for any } x_{<d} \\in \\mathbb{R}^{d-1}.\n\\end{aligned}\n$$\nLet $$f : \\mathbb{R}^{d-1} \\rightarrow \\mathbb{R}$$ be any differentiable function. In order to show that $$q'_{v}$$ is \u03b3-Poincar\u00e9, it is sufficient to show that under no further assumptions on f, the quantity $$\\text{Var}_{q'_{v}}(f(x_{<d}))$$ is upper bounded by the product of \u03b3 and $$\\mathbb{E}_{q'_{v}}[\\| \\nabla f(x_{<d}) \\|_2^2]$$. We expand the quantity $$\\text{Var}_{q'_{v}}(f(x_{<d}))$$ as follows\n\n$$\n\\begin{aligned}\n\\text{Var}_{q'_{v}}(f(x_{<d})) & = \\inf_{\\tau} \\int_{x_{<d}} (f(x_{<d}) - \\tau)^2 q'_{v}(x_{<d}) \\, dx_{<d} \\\\\n& = \\inf_{\\tau} \\int_{x_{<d}} (f(x_{<d}) - \\tau)^2 \\cdot \\left( \\int_{x_d \\in I} q(x_{<d}, x_d) \\, dx_d \\right) \\, dx_{<d} \\\\\n& = \\inf_{\\tau} \\int_{x_{<d}} \\left( f(x_{<d}) - \\tau \\right)^2 \\cdot q(x) \\, dx_{<d} \\, dx_{<d} \\\\\n& \\leq \\gamma \\cdot \\int_{x_{<d}} \\left( \\int_{x_d \\in I} \\| \\nabla_x f(x_{<d}) \\|_2^2 q(x) \\, dx_d \\right) \\, dx_{<d} \\quad \\text{(since q is \u03b3-Poincar\u00e9)} \\\\\n& = \\gamma \\cdot \\int_{x_{<d}} \\| \\nabla_{x_{<d}} f(x_{<d}) \\|_2^2 \\cdot q'_{v}(x_{<d}) \\, dx_{<d} \\quad \\text{(since } \\frac{\\partial f}{\\partial x_d} \\equiv 0) \\\\\n& = \\gamma \\cdot \\mathbb{E}_{q'_{v}}[\\| \\nabla f(x_{<d}) \\|_2^2],\n\\end{aligned}\n$$\nwhich concludes the proof.", "md": "[SW14] Adrien Saumard and Jon A Wellner. Log-concavity and strong log-concavity: a review. Statistics surveys, 8:45, 2014. 2.2, 2.5\n\n[YZ17] Songbai Yan and Chicheng Zhang. Revisiting perceptron: Efficient and label-optimal learning of halfspaces. Advances in Neural Information Processing Systems, 30, 2017. 1\n\n[Zha18] Chicheng Zhang. Efficient active learning of sparse halfspaces. In Conference on Learning Theory, pages 1856\u20131880. PMLR, 2018. 1\n\n[ZL21] Chicheng Zhang and Yinan Li. Improved algorithms for efficient active learning halfspaces with massart and tsybakov noise. In Conference on Learning Theory, pages 4526\u20134527. PMLR, 2021. 1\n\n[ZSA20] Chicheng Zhang, Jie Shen, and Pranjal Awasthi. Efficient active learning of sparse halfspaces with arbitrary bounded noise. Advances in Neural Information Processing Systems, 33:7184\u20137197, 2020. 1\n\nA Technical Lemmas\n\nIn this section, we provide a list of technical results that we use in our proofs.\n\nLemma A.1 (Preservation of Poincar\u00e9 constant). Let I be an open interval in R and $$q : \\mathbb{R}^d \\rightarrow \\mathbb{R}^+$$ the density of a \u03b3-Poincar\u00e9 distribution. Let $$v \\in S^{d-1}$$ and $$q'_{v} : \\mathbb{R}^{d-1} \\rightarrow \\mathbb{R}^+$$ be the density of the distribution resulting from conditioning q to $$x \\cdot v \\in I$$ and projecting on the subspace perpendicular to v. Then, the distribution corresponding to $$q'_{v}$$ is \u03b3-Poincar\u00e9.\n\nProof. Assume, without loss of generality, that $$v = e_d$$. We have that\n\n$$\n\\begin{aligned}\nq'_{v}(x_{<d}) & = \\int_{x_d \\in I} q(x_{<d}, x_d) \\, dx_d \\\\\n& = \\int_{x_{<d}} \\left( \\int_{x_d \\in I} q(x) \\, dx \\right) \\, dx_{<d} \\quad \\text{for any } x_{<d} \\in \\mathbb{R}^{d-1}.\n\\end{aligned}\n$$\nLet $$f : \\mathbb{R}^{d-1} \\rightarrow \\mathbb{R}$$ be any differentiable function. In order to show that $$q'_{v}$$ is \u03b3-Poincar\u00e9, it is sufficient to show that under no further assumptions on f, the quantity $$\\text{Var}_{q'_{v}}(f(x_{<d}))$$ is upper bounded by the product of \u03b3 and $$\\mathbb{E}_{q'_{v}}[\\| \\nabla f(x_{<d}) \\|_2^2]$$. We expand the quantity $$\\text{Var}_{q'_{v}}(f(x_{<d}))$$ as follows\n\n$$\n\\begin{aligned}\n\\text{Var}_{q'_{v}}(f(x_{<d})) & = \\inf_{\\tau} \\int_{x_{<d}} (f(x_{<d}) - \\tau)^2 q'_{v}(x_{<d}) \\, dx_{<d} \\\\\n& = \\inf_{\\tau} \\int_{x_{<d}} (f(x_{<d}) - \\tau)^2 \\cdot \\left( \\int_{x_d \\in I} q(x_{<d}, x_d) \\, dx_d \\right) \\, dx_{<d} \\\\\n& = \\inf_{\\tau} \\int_{x_{<d}} \\left( f(x_{<d}) - \\tau \\right)^2 \\cdot q(x) \\, dx_{<d} \\, dx_{<d} \\\\\n& \\leq \\gamma \\cdot \\int_{x_{<d}} \\left( \\int_{x_d \\in I} \\| \\nabla_x f(x_{<d}) \\|_2^2 q(x) \\, dx_d \\right) \\, dx_{<d} \\quad \\text{(since q is \u03b3-Poincar\u00e9)} \\\\\n& = \\gamma \\cdot \\int_{x_{<d}} \\| \\nabla_{x_{<d}} f(x_{<d}) \\|_2^2 \\cdot q'_{v}(x_{<d}) \\, dx_{<d} \\quad \\text{(since } \\frac{\\partial f}{\\partial x_d} \\equiv 0) \\\\\n& = \\gamma \\cdot \\mathbb{E}_{q'_{v}}[\\| \\nabla f(x_{<d}) \\|_2^2],\n\\end{aligned}\n$$\nwhich concludes the proof."}]}, {"page": 19, "text": "Proposition A.2 (Spectral Tester). Let D be a distribution over Rd. Then, there is a tester that\ngiven \u03b4 \u2208   (0, 1), \u03bb \u2265  1, \u03b8 > 0 and a set S of i.i.d. samples from D with size at least 2\u03bbd4          \u03b82\u03b4 , runs in\ntime poly(d, 1 \u03b8, |S|) and satisfies the following specifications\n  (a) If the tester accepts, then, for z \u223c       S, ES[zzT ] \u2ab0    \u03b8\n                                                                  2Id (resp. ES[zzT ] \u2aaf     2\u03b8Id).\n  (b) If, for z \u223c    D, ED[(zizj)2] \u2264      \u03bb and ED[zzT ] \u2ab0       \u03b8Id (resp. ED[zzT ] \u2aaf       \u03b8Id), then the tester\n       accepts with probability at least 1 \u2212      \u03b4.\nProof. The tester receives \u03bb, a set S and \u03b4 \u2208          (0, 1) and does the following:\n   1. Compute the matrix MS = ES[zzT ].\n   2. If the minimum (resp. maximum) eigenvalue of MS is larger than \u03b8                 2 (resp. smaller than 2\u03b8),\n       then accept. Otherwise reject.\nClearly, if the tester accepts, then the desired property is satisfied by construction. If the distribu-\ntion D satisfies the conditions of part (b), we can show that for MD = Ez\u223cD[zzT ] we have\n                             MS \u2212    MD    op \u2264   \u03b8\nwhich implies that MS \u2ab0        \u03b8                  2, with probability at least 1 \u2212       \u03b4\n                               2Id (and MS \u2aaf       (\u03b8 + \u03b82)Id \u2aaf  2\u03b8Id). In particular, we have that (MS)ij =\nES[zizj], and by Chebyshev\u2019s inequality we have\n                                                                                               \u03b4\n                     P  |(MS)ij \u2212   (MD)ij| > \u03b8        \u2264   4d2   z\u223cD[(zizj)2] \u2264     4\u03bbd2       d\n                                                  2d      \u03b82|S| E \u03b8                \u03b82|S| \u2264     2\nBy a union bound, we obtain that \u2225MS \u2212             MD\u2225max \u2264      2d with probability at least 1 \u2212       \u03b4 and hence\n\u2225MS \u2212    MD\u2225op \u2264     d\u2225MS \u2212    MD\u2225max \u2264      \u03b8\n                                             2, which concludes the proof.\n                                                   1\nProposition A.3. Let c \u2265         0, \u03bb \u2265   1, \u03c3 \u2264  2\u03bb and D be a \u03bb-nice distribution over Rd. Then, for any\nunit vectors w, v, v\u2032, u, u\u2032 \u2208    Rd with \u27e8w, v\u27e9     = \u27e8w, v\u2032\u27e9   = 0 and for some universal constant C > 0\nwe have\n  (i) P[|\u27e8w, x\u27e9| \u2264    \u03c3] = 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2208       [ 1\n                                                        C\u03bb, C\u03bb].\n (ii) E[\u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2264     \u03c3}] = 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2208        [ 1\n                                                                       C\u03bb, C\u03bb].\n(iii) E[\u27e8x, u\u27e92\u27e8x, u\u2032\u27e92] = \u03b1C, for some \u03b1 \u2264          C\u03bb.\n (iv) E[\u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2208     [c, c + \u03c3]}] \u2264  2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2264      C\u03bb.\nProof. We start by deriving property (i). Recall the function Q from the definition of a \u03bb-nice distri-\nbution, which upper-bounds the density of any two-dimensional projection of a \u03bb-nice distribution\nwe see that:\n                         P[|\u27e8w, x\u27e9| \u2264   \u03c3] =     \u03c3        \u221e      qspan(v,w)(x1, x2) dx1dx2\n                                               x1=\u2212\u03c3    x2=\u2212\u221e\n                                           \u2264     \u03c3        \u221e      Q      x21 + x22   dx1dx2\n                                               x1=\u2212\u03c3    x2=\u2212\u221e\nNow, note that the region {(x1, x2) : |x1| \u2264         \u03c3} is a subset of the set\n                         {(x1, x2) : |x2| \u2264   \u03c3|x1|} \u222a  {(x1, x2) : |x1| \u2264   \u03c3 & |x2| \u2264   1}.\n                                                          19", "md": "## Proposition A.2 (Spectral Tester)\n\nLet D be a distribution over \u211d\u1d48. Then, there is a tester that given \u03b4 \u2208 (0, 1), \u03bb \u2265 1, \u03b8 > 0 and a set S of i.i.d. samples from D with size at least 2\u03bbd\u2074\u03b8\u00b2\u03b4, runs in time poly(d, 1/\u03b8, |S|) and satisfies the following specifications:\n\n1. If the tester accepts, then, for z \u223c S, $\\mathbb{E}[zz^T] \\succeq \\theta/2I_d$ (resp. $\\mathbb{E}[zz^T] \\preceq 2\\theta I_d$).\n2. If, for z \u223c D, $\\mathbb{E}[(zi zj)\u00b2] \\leq \\lambda$ and $\\mathbb{E}[zz^T] \\succeq \\theta I_d$ (resp. $\\mathbb{E}[zz^T] \\preceq \\theta I_d$), then the tester accepts with probability at least 1 - \u03b4.\n\nProof. The tester receives \u03bb, a set S, and \u03b4 \u2208 (0, 1) and does the following:\n\n1. Compute the matrix MS = $\\mathbb{E}[zz^T]$.\n2. If the minimum (resp. maximum) eigenvalue of MS is larger than \u03b8/2 (resp. smaller than 2\u03b8), then accept. Otherwise reject.\n\nClearly, if the tester accepts, then the desired property is satisfied by construction. If the distribution D satisfies the conditions of part (b), we can show that for MD = Ez\u223cD[zz^T] we have $$\\|MS - MD\\|_{op} \\leq \\theta$$ which implies that $$MS \\succeq \\theta/2$$, with probability at least 1 - \u03b4 (and $$MS \\preceq (\\theta + \\theta\u00b2)I_d \\preceq 2\\theta I_d$$). In particular, we have that $$(MS)_{ij} = \\mathbb{E}[z_i z_j]$$, and by Chebyshev\u2019s inequality we have $$P\\left(|(MS)_{ij} - (MD)_{ij}| > \\theta\\right) \\leq 4d\u00b2 \\mathbb{E}_{z\u223cD}[(zi zj)\u00b2] \\leq 4\u03bbd\u00b2 \\leq 2\\delta$$.\n\nBy a union bound, we obtain that $$\\|MS - MD\\|_{\\text{max}} \\leq 2d$$ with probability at least 1 - \u03b4 and hence $$\\|MS - MD\\|_{op} \\leq d\\|MS - MD\\|_{\\text{max}} \\leq \\theta/2$$, which concludes the proof.\n\n## Proposition A.3\n\nLet c \u2265 0, \u03bb \u2265 1, \u03c3 \u2264 2\u03bb and D be a \u03bb-nice distribution over \u211d\u1d48. Then, for any unit vectors w, v, v', u, u' \u2208 \u211d\u1d48 with \u27e8w, v\u27e9 = \u27e8w, v'\u27e9 = 0 and for some universal constant C > 0 we have:\n\n1. P[|\u27e8w, x\u27e9| \u2264 \u03c3] = 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2208 [C\u03bb, C\u03bb].\n2. E[\u27e8v, x\u27e9\u00b2 \u00b7 1{|\u27e8w, x\u27e9| \u2264 \u03c3}] = 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2208 [C\u03bb, C\u03bb].\n3. E[\u27e8x, u\u27e9\u00b2\u27e8x, u'\u27e9\u00b2] = \u03b1C, for some \u03b1 \u2264 C\u03bb.\n4. E[\u27e8v, x\u27e9\u00b2 \u00b7 1{|\u27e8w, x\u27e9| \u2208 [c, c + \u03c3]}] \u2264 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2264 C\u03bb.\n\nProof. We start by deriving property (i). Recall the function Q from the definition of a \u03bb-nice distribution, which upper-bounds the density of any two-dimensional projection of a \u03bb-nice distribution we see that:\n\n$$\n\\begin{align*}\nP[|\u27e8w, x\u27e9| \u2264 \u03c3] & = \\int_{x_1=-\u03c3}^{x_1=\u03c3} \\int_{x_2=-\u221e}^{x_2=\u221e} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2 \\\\\n& \\leq \\int_{x_1=-\u03c3}^{x_1=\u03c3} \\int_{x_2=-\u221e}^{x_2=\u221e} Q(x_1\u00b2 + x_2\u00b2) dx_1 dx_2\n\\end{align*}\n$$\nNow, note that the region {(x\u2081, x\u2082) : |x\u2081| \u2264 \u03c3} is a subset of the set {(x\u2081, x\u2082) : |x\u2082| \u2264 \u03c3|x\u2081|} \u222a {(x\u2081, x\u2082) : |x\u2081| \u2264 \u03c3 & |x\u2082| \u2264 1}.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Proposition A.2 (Spectral Tester)", "md": "## Proposition A.2 (Spectral Tester)"}, {"type": "text", "value": "Let D be a distribution over \u211d\u1d48. Then, there is a tester that given \u03b4 \u2208 (0, 1), \u03bb \u2265 1, \u03b8 > 0 and a set S of i.i.d. samples from D with size at least 2\u03bbd\u2074\u03b8\u00b2\u03b4, runs in time poly(d, 1/\u03b8, |S|) and satisfies the following specifications:\n\n1. If the tester accepts, then, for z \u223c S, $\\mathbb{E}[zz^T] \\succeq \\theta/2I_d$ (resp. $\\mathbb{E}[zz^T] \\preceq 2\\theta I_d$).\n2. If, for z \u223c D, $\\mathbb{E}[(zi zj)\u00b2] \\leq \\lambda$ and $\\mathbb{E}[zz^T] \\succeq \\theta I_d$ (resp. $\\mathbb{E}[zz^T] \\preceq \\theta I_d$), then the tester accepts with probability at least 1 - \u03b4.\n\nProof. The tester receives \u03bb, a set S, and \u03b4 \u2208 (0, 1) and does the following:\n\n1. Compute the matrix MS = $\\mathbb{E}[zz^T]$.\n2. If the minimum (resp. maximum) eigenvalue of MS is larger than \u03b8/2 (resp. smaller than 2\u03b8), then accept. Otherwise reject.\n\nClearly, if the tester accepts, then the desired property is satisfied by construction. If the distribution D satisfies the conditions of part (b), we can show that for MD = Ez\u223cD[zz^T] we have $$\\|MS - MD\\|_{op} \\leq \\theta$$ which implies that $$MS \\succeq \\theta/2$$, with probability at least 1 - \u03b4 (and $$MS \\preceq (\\theta + \\theta\u00b2)I_d \\preceq 2\\theta I_d$$). In particular, we have that $$(MS)_{ij} = \\mathbb{E}[z_i z_j]$$, and by Chebyshev\u2019s inequality we have $$P\\left(|(MS)_{ij} - (MD)_{ij}| > \\theta\\right) \\leq 4d\u00b2 \\mathbb{E}_{z\u223cD}[(zi zj)\u00b2] \\leq 4\u03bbd\u00b2 \\leq 2\\delta$$.\n\nBy a union bound, we obtain that $$\\|MS - MD\\|_{\\text{max}} \\leq 2d$$ with probability at least 1 - \u03b4 and hence $$\\|MS - MD\\|_{op} \\leq d\\|MS - MD\\|_{\\text{max}} \\leq \\theta/2$$, which concludes the proof.", "md": "Let D be a distribution over \u211d\u1d48. Then, there is a tester that given \u03b4 \u2208 (0, 1), \u03bb \u2265 1, \u03b8 > 0 and a set S of i.i.d. samples from D with size at least 2\u03bbd\u2074\u03b8\u00b2\u03b4, runs in time poly(d, 1/\u03b8, |S|) and satisfies the following specifications:\n\n1. If the tester accepts, then, for z \u223c S, $\\mathbb{E}[zz^T] \\succeq \\theta/2I_d$ (resp. $\\mathbb{E}[zz^T] \\preceq 2\\theta I_d$).\n2. If, for z \u223c D, $\\mathbb{E}[(zi zj)\u00b2] \\leq \\lambda$ and $\\mathbb{E}[zz^T] \\succeq \\theta I_d$ (resp. $\\mathbb{E}[zz^T] \\preceq \\theta I_d$), then the tester accepts with probability at least 1 - \u03b4.\n\nProof. The tester receives \u03bb, a set S, and \u03b4 \u2208 (0, 1) and does the following:\n\n1. Compute the matrix MS = $\\mathbb{E}[zz^T]$.\n2. If the minimum (resp. maximum) eigenvalue of MS is larger than \u03b8/2 (resp. smaller than 2\u03b8), then accept. Otherwise reject.\n\nClearly, if the tester accepts, then the desired property is satisfied by construction. If the distribution D satisfies the conditions of part (b), we can show that for MD = Ez\u223cD[zz^T] we have $$\\|MS - MD\\|_{op} \\leq \\theta$$ which implies that $$MS \\succeq \\theta/2$$, with probability at least 1 - \u03b4 (and $$MS \\preceq (\\theta + \\theta\u00b2)I_d \\preceq 2\\theta I_d$$). In particular, we have that $$(MS)_{ij} = \\mathbb{E}[z_i z_j]$$, and by Chebyshev\u2019s inequality we have $$P\\left(|(MS)_{ij} - (MD)_{ij}| > \\theta\\right) \\leq 4d\u00b2 \\mathbb{E}_{z\u223cD}[(zi zj)\u00b2] \\leq 4\u03bbd\u00b2 \\leq 2\\delta$$.\n\nBy a union bound, we obtain that $$\\|MS - MD\\|_{\\text{max}} \\leq 2d$$ with probability at least 1 - \u03b4 and hence $$\\|MS - MD\\|_{op} \\leq d\\|MS - MD\\|_{\\text{max}} \\leq \\theta/2$$, which concludes the proof."}, {"type": "heading", "lvl": 2, "value": "Proposition A.3", "md": "## Proposition A.3"}, {"type": "text", "value": "Let c \u2265 0, \u03bb \u2265 1, \u03c3 \u2264 2\u03bb and D be a \u03bb-nice distribution over \u211d\u1d48. Then, for any unit vectors w, v, v', u, u' \u2208 \u211d\u1d48 with \u27e8w, v\u27e9 = \u27e8w, v'\u27e9 = 0 and for some universal constant C > 0 we have:\n\n1. P[|\u27e8w, x\u27e9| \u2264 \u03c3] = 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2208 [C\u03bb, C\u03bb].\n2. E[\u27e8v, x\u27e9\u00b2 \u00b7 1{|\u27e8w, x\u27e9| \u2264 \u03c3}] = 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2208 [C\u03bb, C\u03bb].\n3. E[\u27e8x, u\u27e9\u00b2\u27e8x, u'\u27e9\u00b2] = \u03b1C, for some \u03b1 \u2264 C\u03bb.\n4. E[\u27e8v, x\u27e9\u00b2 \u00b7 1{|\u27e8w, x\u27e9| \u2208 [c, c + \u03c3]}] \u2264 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2264 C\u03bb.\n\nProof. We start by deriving property (i). Recall the function Q from the definition of a \u03bb-nice distribution, which upper-bounds the density of any two-dimensional projection of a \u03bb-nice distribution we see that:\n\n$$\n\\begin{align*}\nP[|\u27e8w, x\u27e9| \u2264 \u03c3] & = \\int_{x_1=-\u03c3}^{x_1=\u03c3} \\int_{x_2=-\u221e}^{x_2=\u221e} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2 \\\\\n& \\leq \\int_{x_1=-\u03c3}^{x_1=\u03c3} \\int_{x_2=-\u221e}^{x_2=\u221e} Q(x_1\u00b2 + x_2\u00b2) dx_1 dx_2\n\\end{align*}\n$$\nNow, note that the region {(x\u2081, x\u2082) : |x\u2081| \u2264 \u03c3} is a subset of the set {(x\u2081, x\u2082) : |x\u2082| \u2264 \u03c3|x\u2081|} \u222a {(x\u2081, x\u2082) : |x\u2081| \u2264 \u03c3 & |x\u2082| \u2264 1}.", "md": "Let c \u2265 0, \u03bb \u2265 1, \u03c3 \u2264 2\u03bb and D be a \u03bb-nice distribution over \u211d\u1d48. Then, for any unit vectors w, v, v', u, u' \u2208 \u211d\u1d48 with \u27e8w, v\u27e9 = \u27e8w, v'\u27e9 = 0 and for some universal constant C > 0 we have:\n\n1. P[|\u27e8w, x\u27e9| \u2264 \u03c3] = 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2208 [C\u03bb, C\u03bb].\n2. E[\u27e8v, x\u27e9\u00b2 \u00b7 1{|\u27e8w, x\u27e9| \u2264 \u03c3}] = 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2208 [C\u03bb, C\u03bb].\n3. E[\u27e8x, u\u27e9\u00b2\u27e8x, u'\u27e9\u00b2] = \u03b1C, for some \u03b1 \u2264 C\u03bb.\n4. E[\u27e8v, x\u27e9\u00b2 \u00b7 1{|\u27e8w, x\u27e9| \u2208 [c, c + \u03c3]}] \u2264 2\u03c3 \u00b7 \u03b1C, for some \u03b1 \u2264 C\u03bb.\n\nProof. We start by deriving property (i). Recall the function Q from the definition of a \u03bb-nice distribution, which upper-bounds the density of any two-dimensional projection of a \u03bb-nice distribution we see that:\n\n$$\n\\begin{align*}\nP[|\u27e8w, x\u27e9| \u2264 \u03c3] & = \\int_{x_1=-\u03c3}^{x_1=\u03c3} \\int_{x_2=-\u221e}^{x_2=\u221e} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2 \\\\\n& \\leq \\int_{x_1=-\u03c3}^{x_1=\u03c3} \\int_{x_2=-\u221e}^{x_2=\u221e} Q(x_1\u00b2 + x_2\u00b2) dx_1 dx_2\n\\end{align*}\n$$\nNow, note that the region {(x\u2081, x\u2082) : |x\u2081| \u2264 \u03c3} is a subset of the set {(x\u2081, x\u2082) : |x\u2082| \u2264 \u03c3|x\u2081|} \u222a {(x\u2081, x\u2082) : |x\u2081| \u2264 \u03c3 & |x\u2082| \u2264 1}."}]}, {"page": 20, "text": "Therefore:\n      \u03c3         \u221e       Q       x21 + x22    dx1dx2 \u2264\n     x1=\u2212\u03c3     x2=\u2212\u221e   4 arcsin(\u03c3) \u00b7      \u221e  2\u03c0rQ (r) dr +         \u03c3         1     Q       x21 + x2 2   dx1dx2 \u2264     O(\u03c3\u03bb)\n                                        r=0                       x1=\u2212\u03c3     x2=\u22121\nNote that in the last line above, we bounded the first term via the bound                          \u221e\nthe definition of \u03bb-nice distributions. Likewise, we bounded the second term via the inequality    r=0 rQ (r) dr \u2264      \u03bb from\nQ(r) \u2264    \u03bb from the definition of \u03bb-nice distributions. Overall, we obtain\n                                         E[\u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2264      \u03c3}] \u2264   O(\u03c3\u03bb)\nNow, we shall lower-bound the same quantity. We have\n                           P[|\u27e8w, x\u27e9| \u2264    \u03c3] =      \u03c3        \u221e       qspan(v,w)(x1, x2) dx1dx2\n                                                   x1=\u2212\u03c3     x2=\u2212\u221e\n                                                               1\n                                              \u2265      \u03c3         2\u03bb      qspan(v,w)(x1, x2) dx1dx2\n                                                             x2=\u2212   1\n                       1                           x1=\u2212\u03c3           2\u03bb\nNow, since \u03c3 \u2264        2\u03bb via the premise of the lemma, we see that the whole region of integration on\nthe right side of the set {(x1, x2) :            x2\n                                                  1 + x2 2 \u2264   1\n                                                              \u03bb}. From the definition of \u03bb-nice distributions, the\ndensity q   span(v,w) is lower-bounded by 1/\u03bb in this region. Therefore, we have\n                                            P[|\u27e8w, x\u27e9| \u2264    \u03c3] \u2265   2\u03c3\nwhich finishes the proof of property (i).                           \u03bb \u00b7 1 \u03bb = 2\u03c3\u03bb2 ,\n     Now, we derive property (ii). Recall the function Q from the definition of a \u03bb-nice distribution,\nwhich upper-bounds the density of any two-dimensional projection of a \u03bb-nice distribution we see\nthat:\n                 E[\u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2264      \u03c3}] =      \u03c3         \u221e       x2\n                                                         x1=\u2212\u03c3     x2=\u2212\u221e     2 \u00b7 qspan(v,w)(x1, x2) dx1dx2\n                                                    \u2264     \u03c3         \u221e       x2           x2           dx1dx2\n                                                         x1=\u2212\u03c3     x2=\u2212\u221e     2 \u00b7 Q         1 + x22\nNow, note that the region {(x1, x2) : |x1| \u2264             \u03c3} is a subset of the set\nTherefore:                 {(x1, x2) : |x2| \u2264     \u03c3|x1|} \u222a   {(x1, x2) : |x1| \u2264    \u03c3 & |x2| \u2264    1}.\n      \u03c3         \u221e       x22 \u00b7 Q      x21 + x22    dx1dx2 \u2264\n     x1=\u2212\u03c3     x2=\u2212\u221e\n                4 arcsin(\u03c3) \u00b7      \u221e   2\u03c0r3Q (r) dr +         \u03c3         1     x22 \u00b7 Q      x21 + x2 2   dx1dx2 \u2264     O(\u03c3\u03bb)\n                                  r=0                        x1=\u2212\u03c3    x2=\u22121\nNote that in the last line above, we bounded the first term via the bound on                            \u221e\n                                                                                                       r=0 r3Q (r) dr from\nthe definition of \u03bb-nice distributions. Likewise, we bounded the second term via the inequality\nQ(r) \u2264    \u03bb from the definition of \u03bb-nice distributions. Therefore, we obtain\n                                         E[\u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2264      \u03c3}] \u2264   O(\u03c3\u03bb)\n                                                               20", "md": "# Math Equations\n\nTherefore:\n\n$$\n\\int_{x_1=-\\sigma}^{\\infty} \\int_{x_2=-\\infty} Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq 4 \\arcsin(\\sigma) \\cdot \\int_{r=0}^{\\infty} \\frac{2\\pi rQ(r) dr}{\\sigma} + \\int_{x_1=-\\sigma}^{1} \\int_{x_2=-1} Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq O(\\sigma\\lambda)\n$$\nNote that in the last line above, we bounded the first term via the bound on the definition of \u03bb-nice distributions. Likewise, we bounded the second term via the inequality $\\int_{r=0}^{\\infty} rQ(r) dr \\leq \\lambda$ from $Q(r) \\leq \\lambda$ from the definition of \u03bb-nice distributions. Overall, we obtain\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] \\leq O(\\sigma\\lambda)\n$$\nNow, we shall lower-bound the same quantity. We have\n\n$$\nP[|\\langle w, x \\rangle| \\leq \\sigma] = \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\n$$\n\\geq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\frac{1}{2\\lambda}}^{\\frac{1}{2\\lambda}} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\nNow, since $\\sigma \\leq \\frac{1}{2\\lambda}$ via the premise of the lemma, we see that the whole region of integration on the right side of the set $\\{(x_1, x_2) : x_1 + x_2^2 \\leq \\frac{1}{\\lambda}\\}$. From the definition of \u03bb-nice distributions, the density $q_{\\text{span}(v,w)}$ is lower-bounded by $\\frac{1}{\\lambda}$ in this region. Therefore, we have\n\n$$\nP[|\\langle w, x \\rangle| \\leq \\sigma] \\geq \\frac{2\\sigma}{\\lambda} \\cdot \\frac{1}{\\lambda} = \\frac{2\\sigma}{\\lambda^2}\n$$\nwhich finishes the proof of property (i).\n\nNow, we derive property (ii). Recall the function $Q$ from the definition of a \u03bb-nice distribution, which upper-bounds the density of any two-dimensional projection of a \u03bb-nice distribution we see that:\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] = \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} 2 \\cdot q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\n$$\n\\leq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} 2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2\n$$\nNow, note that the region $\\{(x_1, x_2) : |x_1| \\leq \\sigma\\}$ is a subset of the set\n\nTherefore:\n\n$$\n\\int_{x_1=-\\sigma}^{\\infty} \\int_{x_2=-\\infty} x_2^2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq 4 \\arcsin(\\sigma) \\cdot \\int_{r=0}^{\\infty} 2\\pi r^3Q(r) dr + \\int_{x_1=-\\sigma}^{1} x_2^2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq O(\\sigma\\lambda)\n$$\nNote that in the last line above, we bounded the first term via the bound on $\\int_{r=0}^{\\infty} r^3Q(r) dr$ from the definition of \u03bb-nice distributions. Likewise, we bounded the second term via the inequality $Q(r) \\leq \\lambda$ from the definition of \u03bb-nice distributions. Therefore, we obtain\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] \\leq O(\\sigma\\lambda)\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Therefore:\n\n$$\n\\int_{x_1=-\\sigma}^{\\infty} \\int_{x_2=-\\infty} Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq 4 \\arcsin(\\sigma) \\cdot \\int_{r=0}^{\\infty} \\frac{2\\pi rQ(r) dr}{\\sigma} + \\int_{x_1=-\\sigma}^{1} \\int_{x_2=-1} Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq O(\\sigma\\lambda)\n$$\nNote that in the last line above, we bounded the first term via the bound on the definition of \u03bb-nice distributions. Likewise, we bounded the second term via the inequality $\\int_{r=0}^{\\infty} rQ(r) dr \\leq \\lambda$ from $Q(r) \\leq \\lambda$ from the definition of \u03bb-nice distributions. Overall, we obtain\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] \\leq O(\\sigma\\lambda)\n$$\nNow, we shall lower-bound the same quantity. We have\n\n$$\nP[|\\langle w, x \\rangle| \\leq \\sigma] = \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\n$$\n\\geq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\frac{1}{2\\lambda}}^{\\frac{1}{2\\lambda}} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\nNow, since $\\sigma \\leq \\frac{1}{2\\lambda}$ via the premise of the lemma, we see that the whole region of integration on the right side of the set $\\{(x_1, x_2) : x_1 + x_2^2 \\leq \\frac{1}{\\lambda}\\}$. From the definition of \u03bb-nice distributions, the density $q_{\\text{span}(v,w)}$ is lower-bounded by $\\frac{1}{\\lambda}$ in this region. Therefore, we have\n\n$$\nP[|\\langle w, x \\rangle| \\leq \\sigma] \\geq \\frac{2\\sigma}{\\lambda} \\cdot \\frac{1}{\\lambda} = \\frac{2\\sigma}{\\lambda^2}\n$$\nwhich finishes the proof of property (i).\n\nNow, we derive property (ii). Recall the function $Q$ from the definition of a \u03bb-nice distribution, which upper-bounds the density of any two-dimensional projection of a \u03bb-nice distribution we see that:\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] = \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} 2 \\cdot q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\n$$\n\\leq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} 2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2\n$$\nNow, note that the region $\\{(x_1, x_2) : |x_1| \\leq \\sigma\\}$ is a subset of the set\n\nTherefore:\n\n$$\n\\int_{x_1=-\\sigma}^{\\infty} \\int_{x_2=-\\infty} x_2^2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq 4 \\arcsin(\\sigma) \\cdot \\int_{r=0}^{\\infty} 2\\pi r^3Q(r) dr + \\int_{x_1=-\\sigma}^{1} x_2^2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq O(\\sigma\\lambda)\n$$\nNote that in the last line above, we bounded the first term via the bound on $\\int_{r=0}^{\\infty} r^3Q(r) dr$ from the definition of \u03bb-nice distributions. Likewise, we bounded the second term via the inequality $Q(r) \\leq \\lambda$ from the definition of \u03bb-nice distributions. Therefore, we obtain\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] \\leq O(\\sigma\\lambda)\n$$", "md": "Therefore:\n\n$$\n\\int_{x_1=-\\sigma}^{\\infty} \\int_{x_2=-\\infty} Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq 4 \\arcsin(\\sigma) \\cdot \\int_{r=0}^{\\infty} \\frac{2\\pi rQ(r) dr}{\\sigma} + \\int_{x_1=-\\sigma}^{1} \\int_{x_2=-1} Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq O(\\sigma\\lambda)\n$$\nNote that in the last line above, we bounded the first term via the bound on the definition of \u03bb-nice distributions. Likewise, we bounded the second term via the inequality $\\int_{r=0}^{\\infty} rQ(r) dr \\leq \\lambda$ from $Q(r) \\leq \\lambda$ from the definition of \u03bb-nice distributions. Overall, we obtain\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] \\leq O(\\sigma\\lambda)\n$$\nNow, we shall lower-bound the same quantity. We have\n\n$$\nP[|\\langle w, x \\rangle| \\leq \\sigma] = \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\n$$\n\\geq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\frac{1}{2\\lambda}}^{\\frac{1}{2\\lambda}} q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\nNow, since $\\sigma \\leq \\frac{1}{2\\lambda}$ via the premise of the lemma, we see that the whole region of integration on the right side of the set $\\{(x_1, x_2) : x_1 + x_2^2 \\leq \\frac{1}{\\lambda}\\}$. From the definition of \u03bb-nice distributions, the density $q_{\\text{span}(v,w)}$ is lower-bounded by $\\frac{1}{\\lambda}$ in this region. Therefore, we have\n\n$$\nP[|\\langle w, x \\rangle| \\leq \\sigma] \\geq \\frac{2\\sigma}{\\lambda} \\cdot \\frac{1}{\\lambda} = \\frac{2\\sigma}{\\lambda^2}\n$$\nwhich finishes the proof of property (i).\n\nNow, we derive property (ii). Recall the function $Q$ from the definition of a \u03bb-nice distribution, which upper-bounds the density of any two-dimensional projection of a \u03bb-nice distribution we see that:\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] = \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} 2 \\cdot q_{\\text{span}(v,w)}(x_1, x_2) dx_1 dx_2\n$$\n$$\n\\leq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} 2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2\n$$\nNow, note that the region $\\{(x_1, x_2) : |x_1| \\leq \\sigma\\}$ is a subset of the set\n\nTherefore:\n\n$$\n\\int_{x_1=-\\sigma}^{\\infty} \\int_{x_2=-\\infty} x_2^2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq 4 \\arcsin(\\sigma) \\cdot \\int_{r=0}^{\\infty} 2\\pi r^3Q(r) dr + \\int_{x_1=-\\sigma}^{1} x_2^2 \\cdot Q(x_1^2 + x_2^2) dx_1 dx_2 \\leq O(\\sigma\\lambda)\n$$\nNote that in the last line above, we bounded the first term via the bound on $\\int_{r=0}^{\\infty} r^3Q(r) dr$ from the definition of \u03bb-nice distributions. Likewise, we bounded the second term via the inequality $Q(r) \\leq \\lambda$ from the definition of \u03bb-nice distributions. Therefore, we obtain\n\n$$\nE[\\langle v, x \\rangle^2 \\cdot 1\\{|\\langle w, x \\rangle| \\leq \\sigma\\}] \\leq O(\\sigma\\lambda)\n$$"}]}, {"page": 21, "text": "Now, we shall lower-bound the same quantity. We have\n                E[\u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2264       \u03c3}] =      \u03c3         \u221e      x22 \u00b7 qspan(v,w)(x1, x2) dx1dx2\n                                                         x1=\u2212\u03c3    x2=\u2212\u221e\n                                                                     1\n                                                    \u2265     \u03c3         2\u03bb      x22 \u00b7 qspan(v,w)(x1, x2) dx1dx2\n                                                         x1=\u2212\u03c3    x2=\u2212   1\n                                                                         2\u03bb\n                       1\nNow, since \u03c3 \u2264        2\u03bb via the premise of the lemma, we see that the whole region of integration on\nthe right side of the set {(x1, x2) :            x2\n                                                  1 + x2 2 \u2264   1\n                                                              \u03bb}. From the definition of \u03bb-nice distributions, the\ndensity q   span(v,w) is lower-bounded by 1/\u03bb in this region. Therefore, we have\n                                E[\u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2264       \u03c3}] \u2265   2\u03c3      1            \u03c3\nwhich finishes the proof of property (ii).                              \u03bb \u00b7  4\u03bb2 \u00b7 1\u03bb =    2\u03bb4 ,\n     We proceed to property (iii). We will denote the angle between v and v\u2032 as \u03b2, which allows us\nto write\n          E[\u27e8x, v\u27e92\u27e8x, v\u2032\u27e92] =         \u221e         \u221e       x21(x1 cos \u03b2 + x2 sin \u03b2)2q     span(v,w)(x1, x2) dx1dx2\n                                     x1=\u2212\u221e      x2=\u2212\u221e\n                                \u2264      \u221e         \u221e       x2                                      x2           dx1dx2\n                                     x1=\u2212\u221e      x2=\u2212\u221e      1(x1 cos \u03b2 + x2 sin \u03b2)2 \u00b7 Q            1 + x2 2\n                                \u2264      \u221e         \u221e      (x21 + x2 2)2 \u00b7 Q       x21 + x22    dx1dx2\n                                     x1=\u2212\u221e      x2=\u2212\u221e\n                                =      \u221e  2\u03c0r5Q(r) dr \u2264       2\u03c0\u03bb,\n                                     r=0\nwhich finishes the proof of property (iii).\n     Finally, we prove property (iv). For \u03b2 \u2265             0 we have\n             \u221e  r2 Q      r2 + \u03b2    dr =      1  r2Q       r2 + \u03b2    dr +      \u221e  r2Q       r2 + \u03b2    dr\n            r=0                              r=0                              r=1\n                                        \u2264  \u03bb +     \u221e   r3Q      r2 + \u03b2     dr                     (since supr\u22650 Q(r) \u2264       \u03bb)\n                                                  r=1\n                                        \u2264  \u03bb +     \u221e        (r\u20323 \u2212  \u03b2r\u2032)Q(r\u2032) dr\u2032              (by setting r\u2032 =        r2 + \u03b2)\n                                                  r\u2032=\u221a1+\u03b2\n                                        \u2264  \u03bb +     \u221e   r3Q(r) dr                      (since \u03b2rQ(r) \u2265       0 for any r \u2265     0)\n                                                  r=0\n                                        \u2264  2\u03bb\nApplying the above inequality to the quantity of property (iv), we obtain the desired result.\n           E[\u27e8v, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2208      [c, c + \u03c3]}] =                     \u221e       x22 \u00b7 qspan(v,w) dx1dx2\n                                                            |x1|\u2208[c,c+\u03c3]   x2=\u2212\u221e\n                                                        \u2264                   \u221e       x22 \u00b7 Q     x21 + x22   dx1dx2\n                                                            |x1|\u2208[c,c+\u03c3]   x2=\u2212\u221e\n                                                        =                   2    \u221e  r2 \u00b7 Q   x2  1 + r2    dr    dx1\n                                                            |x1|\u2208[c,c+\u03c3]       r=0\n                                                        \u2264   |x1|\u2208[c,c+\u03c3] (4\u03bb) dx1 \u2264     8\u03bb\u03c3\n                                                               21", "md": "Now, we shall lower-bound the same quantity. We have\n\n$$\n\\begin{aligned}\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\leq \\sigma\\}] &= \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\geq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\frac{1}{2\\lambda}}^{\\frac{1}{2\\lambda}} x_2^2 \\cdot q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\geq \\frac{1}{2\\lambda} \\cdot \\sigma^2.\n\\end{aligned}\n$$\n\nNow, since $\u03c3 \\leq \\frac{1}{2\\lambda}$ via the premise of the lemma, we see that the whole region of integration on the right side of the set $\\{(x_1, x_2) : 1 + x_2^2 \\leq \\frac{1}{\\lambda}\\}$. From the definition of \u03bb-nice distributions, the density $q_{\\text{span}}(v,w)$ is lower-bounded by $\\frac{1}{\\lambda}$ in this region. Therefore, we have\n\n$$\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\leq \\sigma\\}] \\geq 2\\sigma \\cdot \\frac{1}{4\\lambda^2} \\cdot \\frac{1}{\\lambda} = 2\\lambda^4,\n$$\n\nwhich finishes the proof of property (ii).\n\nWe proceed to property (iii). We will denote the angle between $v$ and $v'$ as \u03b2, which allows us to write\n\n$$\n\\begin{aligned}\nE[\u27e8x, v\u27e9^2\u27e8x, v'\u27e9^2] &= \\int_{x_1=-\\infty}^{\\infty} \\int_{x_2=-\\infty}^{\\infty} x_1^2(x_1 \\cos \\beta + x_2 \\sin \\beta)^2q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\leq \\int_{x_1=-\\infty}^{\\infty} \\int_{x_2=-\\infty}^{\\infty} (x_1^2 + x_2^2)^2 \\cdot Q(x_1^2 + x_2^2) \\, dx_1dx_2 \\\\\n&= \\int_{r=0}^{\\infty} 2\\pi r^5Q(r) \\, dr \\leq 2\\pi\\lambda,\n\\end{aligned}\n$$\n\nwhich finishes the proof of property (iii).\n\nFinally, we prove property (iv). For $\u03b2 \\geq 0$ we have\n\n$$\n\\begin{aligned}\n\\int_{r=0}^{\\infty} r^2 Q(r^2 + \\beta) \\, dr &= \\int_{r=0}^{1} \\frac{1}{r^2}Q(r^2 + \\beta) \\, dr + \\int_{r=1}^{\\infty} r^2Q(r^2 + \\beta) \\, dr \\\\\n&\\leq \\lambda + \\int_{r=1}^{\\infty} (r^3 - \\beta r)Q(r) \\, dr \\\\\n&\\leq \\lambda + \\int_{r'=\\sqrt{1+\\beta}}^{\\infty} r^3Q(r) \\, dr' \\quad (\\text{by setting } r' = r^2 + \\beta) \\\\\n&\\leq \\lambda + \\int_{r=0}^{\\infty} r^3Q(r) \\, dr \\quad (\\text{since } \\beta rQ(r) \\geq 0 \\text{ for any } r \\geq 0) \\\\\n&\\leq 2\\lambda.\n\\end{aligned}\n$$\n\nApplying the above inequality to the quantity of property (iv), we obtain the desired result.\n\n$$\n\\begin{aligned}\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\in [c, c + \\sigma]\\}] &= \\int_{|x_1| \\in [c,c+\\sigma]} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot q_{\\text{span}}(v,w) \\, dx_1dx_2 \\\\\n&\\leq \\int_{|x_1| \\in [c,c+\\sigma]} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot Q(x_1^2 + x_2^2) \\, dx_1dx_2 \\\\\n&= 2 \\int_{r=0}^{\\infty} r^2 \\cdot Q(x^2 + x^2) \\, dr \\, dx_1 \\\\\n&\\leq \\int_{|x_1| \\in [c,c+\\sigma]} (4\\lambda) \\, dx_1 \\leq 8\\lambda\\sigma.\n\\end{aligned}\n$$", "images": [], "items": [{"type": "text", "value": "Now, we shall lower-bound the same quantity. We have\n\n$$\n\\begin{aligned}\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\leq \\sigma\\}] &= \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\geq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\frac{1}{2\\lambda}}^{\\frac{1}{2\\lambda}} x_2^2 \\cdot q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\geq \\frac{1}{2\\lambda} \\cdot \\sigma^2.\n\\end{aligned}\n$$\n\nNow, since $\u03c3 \\leq \\frac{1}{2\\lambda}$ via the premise of the lemma, we see that the whole region of integration on the right side of the set $\\{(x_1, x_2) : 1 + x_2^2 \\leq \\frac{1}{\\lambda}\\}$. From the definition of \u03bb-nice distributions, the density $q_{\\text{span}}(v,w)$ is lower-bounded by $\\frac{1}{\\lambda}$ in this region. Therefore, we have\n\n$$\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\leq \\sigma\\}] \\geq 2\\sigma \\cdot \\frac{1}{4\\lambda^2} \\cdot \\frac{1}{\\lambda} = 2\\lambda^4,\n$$\n\nwhich finishes the proof of property (ii).\n\nWe proceed to property (iii). We will denote the angle between $v$ and $v'$ as \u03b2, which allows us to write\n\n$$\n\\begin{aligned}\nE[\u27e8x, v\u27e9^2\u27e8x, v'\u27e9^2] &= \\int_{x_1=-\\infty}^{\\infty} \\int_{x_2=-\\infty}^{\\infty} x_1^2(x_1 \\cos \\beta + x_2 \\sin \\beta)^2q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\leq \\int_{x_1=-\\infty}^{\\infty} \\int_{x_2=-\\infty}^{\\infty} (x_1^2 + x_2^2)^2 \\cdot Q(x_1^2 + x_2^2) \\, dx_1dx_2 \\\\\n&= \\int_{r=0}^{\\infty} 2\\pi r^5Q(r) \\, dr \\leq 2\\pi\\lambda,\n\\end{aligned}\n$$\n\nwhich finishes the proof of property (iii).\n\nFinally, we prove property (iv). For $\u03b2 \\geq 0$ we have\n\n$$\n\\begin{aligned}\n\\int_{r=0}^{\\infty} r^2 Q(r^2 + \\beta) \\, dr &= \\int_{r=0}^{1} \\frac{1}{r^2}Q(r^2 + \\beta) \\, dr + \\int_{r=1}^{\\infty} r^2Q(r^2 + \\beta) \\, dr \\\\\n&\\leq \\lambda + \\int_{r=1}^{\\infty} (r^3 - \\beta r)Q(r) \\, dr \\\\\n&\\leq \\lambda + \\int_{r'=\\sqrt{1+\\beta}}^{\\infty} r^3Q(r) \\, dr' \\quad (\\text{by setting } r' = r^2 + \\beta) \\\\\n&\\leq \\lambda + \\int_{r=0}^{\\infty} r^3Q(r) \\, dr \\quad (\\text{since } \\beta rQ(r) \\geq 0 \\text{ for any } r \\geq 0) \\\\\n&\\leq 2\\lambda.\n\\end{aligned}\n$$\n\nApplying the above inequality to the quantity of property (iv), we obtain the desired result.\n\n$$\n\\begin{aligned}\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\in [c, c + \\sigma]\\}] &= \\int_{|x_1| \\in [c,c+\\sigma]} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot q_{\\text{span}}(v,w) \\, dx_1dx_2 \\\\\n&\\leq \\int_{|x_1| \\in [c,c+\\sigma]} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot Q(x_1^2 + x_2^2) \\, dx_1dx_2 \\\\\n&= 2 \\int_{r=0}^{\\infty} r^2 \\cdot Q(x^2 + x^2) \\, dr \\, dx_1 \\\\\n&\\leq \\int_{|x_1| \\in [c,c+\\sigma]} (4\\lambda) \\, dx_1 \\leq 8\\lambda\\sigma.\n\\end{aligned}\n$$", "md": "Now, we shall lower-bound the same quantity. We have\n\n$$\n\\begin{aligned}\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\leq \\sigma\\}] &= \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\geq \\int_{x_1=-\\sigma}^{\\sigma} \\int_{x_2=-\\frac{1}{2\\lambda}}^{\\frac{1}{2\\lambda}} x_2^2 \\cdot q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\geq \\frac{1}{2\\lambda} \\cdot \\sigma^2.\n\\end{aligned}\n$$\n\nNow, since $\u03c3 \\leq \\frac{1}{2\\lambda}$ via the premise of the lemma, we see that the whole region of integration on the right side of the set $\\{(x_1, x_2) : 1 + x_2^2 \\leq \\frac{1}{\\lambda}\\}$. From the definition of \u03bb-nice distributions, the density $q_{\\text{span}}(v,w)$ is lower-bounded by $\\frac{1}{\\lambda}$ in this region. Therefore, we have\n\n$$\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\leq \\sigma\\}] \\geq 2\\sigma \\cdot \\frac{1}{4\\lambda^2} \\cdot \\frac{1}{\\lambda} = 2\\lambda^4,\n$$\n\nwhich finishes the proof of property (ii).\n\nWe proceed to property (iii). We will denote the angle between $v$ and $v'$ as \u03b2, which allows us to write\n\n$$\n\\begin{aligned}\nE[\u27e8x, v\u27e9^2\u27e8x, v'\u27e9^2] &= \\int_{x_1=-\\infty}^{\\infty} \\int_{x_2=-\\infty}^{\\infty} x_1^2(x_1 \\cos \\beta + x_2 \\sin \\beta)^2q_{\\text{span}}(v,w)(x_1, x_2) \\, dx_1dx_2 \\\\\n&\\leq \\int_{x_1=-\\infty}^{\\infty} \\int_{x_2=-\\infty}^{\\infty} (x_1^2 + x_2^2)^2 \\cdot Q(x_1^2 + x_2^2) \\, dx_1dx_2 \\\\\n&= \\int_{r=0}^{\\infty} 2\\pi r^5Q(r) \\, dr \\leq 2\\pi\\lambda,\n\\end{aligned}\n$$\n\nwhich finishes the proof of property (iii).\n\nFinally, we prove property (iv). For $\u03b2 \\geq 0$ we have\n\n$$\n\\begin{aligned}\n\\int_{r=0}^{\\infty} r^2 Q(r^2 + \\beta) \\, dr &= \\int_{r=0}^{1} \\frac{1}{r^2}Q(r^2 + \\beta) \\, dr + \\int_{r=1}^{\\infty} r^2Q(r^2 + \\beta) \\, dr \\\\\n&\\leq \\lambda + \\int_{r=1}^{\\infty} (r^3 - \\beta r)Q(r) \\, dr \\\\\n&\\leq \\lambda + \\int_{r'=\\sqrt{1+\\beta}}^{\\infty} r^3Q(r) \\, dr' \\quad (\\text{by setting } r' = r^2 + \\beta) \\\\\n&\\leq \\lambda + \\int_{r=0}^{\\infty} r^3Q(r) \\, dr \\quad (\\text{since } \\beta rQ(r) \\geq 0 \\text{ for any } r \\geq 0) \\\\\n&\\leq 2\\lambda.\n\\end{aligned}\n$$\n\nApplying the above inequality to the quantity of property (iv), we obtain the desired result.\n\n$$\n\\begin{aligned}\nE[\u27e8v, x\u27e9^2 \\cdot 1\\{|\u27e8w, x\u27e9| \\in [c, c + \\sigma]\\}] &= \\int_{|x_1| \\in [c,c+\\sigma]} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot q_{\\text{span}}(v,w) \\, dx_1dx_2 \\\\\n&\\leq \\int_{|x_1| \\in [c,c+\\sigma]} \\int_{x_2=-\\infty}^{\\infty} x_2^2 \\cdot Q(x_1^2 + x_2^2) \\, dx_1dx_2 \\\\\n&= 2 \\int_{r=0}^{\\infty} r^2 \\cdot Q(x^2 + x^2) \\, dr \\, dx_1 \\\\\n&\\leq \\int_{|x_1| \\in [c,c+\\sigma]} (4\\lambda) \\, dx_1 \\leq 8\\lambda\\sigma.\n\\end{aligned}\n$$"}]}, {"page": 22, "text": "This concludes the proof of Proposition A.3.\nProposition A.4 (PSGD Convergence [DKTZ20a], restated in [GKSV23]). Let L\u03c3 be as in Equa-\ntion (4.1) with \u03c3 \u2208     (0, 1], \u2113\u03c3 as described in Proposition 4.2, \u03bb \u2265           1 and DXY such that the marginal\nDX on Rd is \u03bb-nice. Then for some universal constant C > 0 and for any \u03f5 > 0 and \u03b4 \u2208                                 (0, 1),\nthere is an algorithm whose time and sample complexity is O( \u03bbCd              \u03c34 + \u03bbC log(1/\u03b4)  ), which, having access\n                                                                                        \u03f54\u03c34\nto samples from DXY, outputs a list L of vectors w \u2208                 Sd\u22121 with |L| = O( \u03bbCd    \u03c34 + \u03bbC log(1/\u03b4)   ) so that\n                                                                                                          \u03f54\u03c34\nthere exists w \u2208     L with\u2225\u2207wL\u03c3(w; DXY)\u22252 \u2264           \u03f5 , with probability at least 1 \u2212       \u03b4 .\nIn particular, the algorithm performs Stochastic Gradient Descent on L\u03c3 Projected on Sd\u22121 (PSGD).\nB      Proof of Lemma 3.1\nWe restate Lemma 3.1 here for convenience.\nLemma B.1 (Lemma 3.1). Let DXY be a distribution over Rd \u00d7 {\u00b11}, w \u2208                                 Sd\u22121, \u03b8 \u2208    (0, \u03c0/4],\n\u03bb \u2265  1 and \u03b4 \u2208     (0, 1). Then, for a sufficiently large constant C, there is a tester that given \u03b4, \u03b8, w\nand a set S of samples from DX with size at least C \u00b7                d4   , runs in time poly      d, 1      and satisfies\n                                                                    \u03b82\u03b4                               \u03b8, 1\u03b4\nthe following specifications:\n  (a) If the tester accepts S, then forPevery unit vector w\u2032 \u2208               Rn satisfying \u2221(w, w\u2032) \u2264         \u03b8 we have\n                                      x\u223cS[sign(\u27e8w\u2032, x\u27e9) \u0338= sign(\u27e8w, x\u27e9)] \u2264         C \u00b7 \u03b8 \u00b7 \u03bbC\n  (b) If the distribution DX is \u03bb-nice, the tester accepts S with probability 1 \u2212                   \u03b4.\nProof. The testing algorithm receives integer d, set S \u2282                  Rd, w \u2208     Sd\u22121, \u03b8 \u2208     (0, \u03c0/4], \u03bb \u2265    1 and\n\u03b4 \u2208  (0, 1) and does the following for some sufficiently large universal constant C1 > 0:\n   1. If Px\u2208S [|\u27e8w, x\u27e9| \u2208     [0, \u03b8]] > C1\u03b8\u03bbC1, then reject.\n   2. Let proj    \u22a5w : Rd \u2192       Rd\u22121 denote the operator that given any vector in Rd, it outputs its\n       projection into the (d \u2212       1)-dimensional subspace of Rd that is orthogonal to w.\n   3. Compute the (d \u2212         1) \u00d7 (d \u2212   1) matrix MS as follows5:\n                        MS = E         \u221e    (proj\u22a5w x)(proj\u22a5w x)T        1{|\u27e8w, x\u27e9| \u2208    [(i \u2212  1)\u03b8, i\u03b8)}\n                                x\u2208S    i=2           (i \u2212  1)2\n   4. Run the spectral tester of Proposition A.2 on MS given \u03b4 \u2190                     \u03b4, \u03bb \u2190   C1\u03bbC1 and \u03b8 \u2190        C1\n   5   i.e., compute \u2225MS\u2225op and if \u2225MS\u2225op > C1\u03b8\u03bbC1, then reject. Otherwise, accept.                                 2 \u03b8\u03bbC1,\n    Note that only at most |S| many terms below are non-zero, hence MS can be computed efficiently.\n                                                             22", "md": "# Math Equations and Text\n\nThis concludes the proof of Proposition A.3.\n\n$$\\text{Proposition A.4 (PSGD Convergence [DKTZ20a], restated in [GKSV23])}.$$ Let $$L_{\\sigma}$$ be as in Equation (4.1) with $$\\sigma \\in (0, 1]$$, $$\\ell_{\\sigma}$$ as described in Proposition 4.2, $$\\lambda \\geq 1$$ and $$D_{XY}$$ such that the marginal $$D_X$$ on $$\\mathbb{R}^d$$ is $$\\lambda$$-nice. Then for some universal constant $$C > 0$$ and for any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, there is an algorithm whose time and sample complexity is $$O(\\lambda C d^{\\sigma 4} + \\lambda C \\log(1/\\delta))$$, which, having access to samples from $$D_{XY}$$, outputs a list $$L$$ of vectors $$w \\in S^{d-1}$$ with $$|L| = O(\\lambda C d^{\\sigma 4} + \\lambda C \\log(1/\\delta))$$ so that there exists $$w \\in L$$ with $$\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\leq \\epsilon$$, with probability at least $$1 - \\delta$$. In particular, the algorithm performs Stochastic Gradient Descent on $$L_{\\sigma}$$ Projected on $$S^{d-1}$$ (PSGD).\n\n$$\\text{Proof of Lemma 3.1}$$\n\nWe restate Lemma 3.1 here for convenience.\n\n$$\\text{Lemma B.1 (Lemma 3.1).}$$ Let $$D_{XY}$$ be a distribution over $$\\mathbb{R}^d \\times \\{ \\pm 1 \\}$$, $$w \\in S^{d-1}$$, $$\\theta \\in (0, \\pi/4]$$, $$\\lambda \\geq 1$$ and $$\\delta \\in (0, 1)$$. Then, for a sufficiently large constant $$C$$, there is a tester that given $$\\delta, \\theta, w$$ and a set $$S$$ of samples from $$D_X$$ with size at least $$C \\cdot d^4$$, runs in time $$\\text{poly}(d, 1/\\theta, 1/\\delta)$$ and satisfies the following specifications:\n\n1. If the tester accepts $S$, then for every unit vector $w' \\in \\mathbb{R}^n$ satisfying $\\angle(w, w') \\leq \\theta$ we have $\\mathbb{E}_{x \\sim S}[ \\text{sign}(\\langle w', x \\rangle) \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq C \\cdot \\theta \\cdot \\lambda^C$\n2. If the distribution $D_X$ is $\\lambda$-nice, the tester accepts $S$ with probability $1 - \\delta$.\n\n$$\\text{Proof.}$$ The testing algorithm receives integer $$d$$, set $$S \\subset \\mathbb{R}^d$$, $$w \\in S^{d-1}$$, $$\\theta \\in (0, \\pi/4]$$, $$\\lambda \\geq 1$$ and $$\\delta \\in (0, 1)$$ and does the following for some sufficiently large universal constant $$C_1 > 0$$:\n\n1. If $\\mathbb{P}_{x \\in S}[|\\langle w, x \\rangle| \\in [0, \\theta]] > C_1 \\theta \\lambda^{C_1}$, then reject.\n2. Let $\\text{proj}_{\\perp w} : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d-1}$ denote the operator that given any vector in $\\mathbb{R}^d$, it outputs its projection into the $(d - 1)$-dimensional subspace of $\\mathbb{R}^d$ that is orthogonal to $w$.\n3. Compute the $(d - 1) \\times (d - 1)$ matrix $M_S$ as follows:\n\n$$\nM_S = \\sum_{x \\in S} \\sum_{i=2}^{\\infty} \\left( \\text{proj}_{\\perp w} x \\right) \\left( \\text{proj}_{\\perp w} x \\right)^T \\cdot 1\\{ |\\langle w, x \\rangle| \\in [(i - 1)\\theta, i\\theta) \\} / (i - 1)^2\n$$\n\nRun the spectral tester of Proposition A.2 on $$M_S$$ given $$\\delta \\leftarrow \\delta$$, $$\\lambda \\leftarrow C_1 \\lambda^{C_1}$$ and $$\\theta \\leftarrow C_1$$.\ni.e., compute $$\\| M_S \\|_{\\text{op}}$$ and if $$\\| M_S \\|_{\\text{op}} > C_1 \\theta \\lambda^{C_1}$$, then reject. Otherwise, accept.\n\nNote that only at most $$|S|$$ many terms below are non-zero, hence $$M_S$$ can be computed efficiently.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "This concludes the proof of Proposition A.3.\n\n$$\\text{Proposition A.4 (PSGD Convergence [DKTZ20a], restated in [GKSV23])}.$$ Let $$L_{\\sigma}$$ be as in Equation (4.1) with $$\\sigma \\in (0, 1]$$, $$\\ell_{\\sigma}$$ as described in Proposition 4.2, $$\\lambda \\geq 1$$ and $$D_{XY}$$ such that the marginal $$D_X$$ on $$\\mathbb{R}^d$$ is $$\\lambda$$-nice. Then for some universal constant $$C > 0$$ and for any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, there is an algorithm whose time and sample complexity is $$O(\\lambda C d^{\\sigma 4} + \\lambda C \\log(1/\\delta))$$, which, having access to samples from $$D_{XY}$$, outputs a list $$L$$ of vectors $$w \\in S^{d-1}$$ with $$|L| = O(\\lambda C d^{\\sigma 4} + \\lambda C \\log(1/\\delta))$$ so that there exists $$w \\in L$$ with $$\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\leq \\epsilon$$, with probability at least $$1 - \\delta$$. In particular, the algorithm performs Stochastic Gradient Descent on $$L_{\\sigma}$$ Projected on $$S^{d-1}$$ (PSGD).\n\n$$\\text{Proof of Lemma 3.1}$$\n\nWe restate Lemma 3.1 here for convenience.\n\n$$\\text{Lemma B.1 (Lemma 3.1).}$$ Let $$D_{XY}$$ be a distribution over $$\\mathbb{R}^d \\times \\{ \\pm 1 \\}$$, $$w \\in S^{d-1}$$, $$\\theta \\in (0, \\pi/4]$$, $$\\lambda \\geq 1$$ and $$\\delta \\in (0, 1)$$. Then, for a sufficiently large constant $$C$$, there is a tester that given $$\\delta, \\theta, w$$ and a set $$S$$ of samples from $$D_X$$ with size at least $$C \\cdot d^4$$, runs in time $$\\text{poly}(d, 1/\\theta, 1/\\delta)$$ and satisfies the following specifications:\n\n1. If the tester accepts $S$, then for every unit vector $w' \\in \\mathbb{R}^n$ satisfying $\\angle(w, w') \\leq \\theta$ we have $\\mathbb{E}_{x \\sim S}[ \\text{sign}(\\langle w', x \\rangle) \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq C \\cdot \\theta \\cdot \\lambda^C$\n2. If the distribution $D_X$ is $\\lambda$-nice, the tester accepts $S$ with probability $1 - \\delta$.\n\n$$\\text{Proof.}$$ The testing algorithm receives integer $$d$$, set $$S \\subset \\mathbb{R}^d$$, $$w \\in S^{d-1}$$, $$\\theta \\in (0, \\pi/4]$$, $$\\lambda \\geq 1$$ and $$\\delta \\in (0, 1)$$ and does the following for some sufficiently large universal constant $$C_1 > 0$$:\n\n1. If $\\mathbb{P}_{x \\in S}[|\\langle w, x \\rangle| \\in [0, \\theta]] > C_1 \\theta \\lambda^{C_1}$, then reject.\n2. Let $\\text{proj}_{\\perp w} : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d-1}$ denote the operator that given any vector in $\\mathbb{R}^d$, it outputs its projection into the $(d - 1)$-dimensional subspace of $\\mathbb{R}^d$ that is orthogonal to $w$.\n3. Compute the $(d - 1) \\times (d - 1)$ matrix $M_S$ as follows:\n\n$$\nM_S = \\sum_{x \\in S} \\sum_{i=2}^{\\infty} \\left( \\text{proj}_{\\perp w} x \\right) \\left( \\text{proj}_{\\perp w} x \\right)^T \\cdot 1\\{ |\\langle w, x \\rangle| \\in [(i - 1)\\theta, i\\theta) \\} / (i - 1)^2\n$$\n\nRun the spectral tester of Proposition A.2 on $$M_S$$ given $$\\delta \\leftarrow \\delta$$, $$\\lambda \\leftarrow C_1 \\lambda^{C_1}$$ and $$\\theta \\leftarrow C_1$$.\ni.e., compute $$\\| M_S \\|_{\\text{op}}$$ and if $$\\| M_S \\|_{\\text{op}} > C_1 \\theta \\lambda^{C_1}$$, then reject. Otherwise, accept.\n\nNote that only at most $$|S|$$ many terms below are non-zero, hence $$M_S$$ can be computed efficiently.", "md": "This concludes the proof of Proposition A.3.\n\n$$\\text{Proposition A.4 (PSGD Convergence [DKTZ20a], restated in [GKSV23])}.$$ Let $$L_{\\sigma}$$ be as in Equation (4.1) with $$\\sigma \\in (0, 1]$$, $$\\ell_{\\sigma}$$ as described in Proposition 4.2, $$\\lambda \\geq 1$$ and $$D_{XY}$$ such that the marginal $$D_X$$ on $$\\mathbb{R}^d$$ is $$\\lambda$$-nice. Then for some universal constant $$C > 0$$ and for any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, there is an algorithm whose time and sample complexity is $$O(\\lambda C d^{\\sigma 4} + \\lambda C \\log(1/\\delta))$$, which, having access to samples from $$D_{XY}$$, outputs a list $$L$$ of vectors $$w \\in S^{d-1}$$ with $$|L| = O(\\lambda C d^{\\sigma 4} + \\lambda C \\log(1/\\delta))$$ so that there exists $$w \\in L$$ with $$\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\leq \\epsilon$$, with probability at least $$1 - \\delta$$. In particular, the algorithm performs Stochastic Gradient Descent on $$L_{\\sigma}$$ Projected on $$S^{d-1}$$ (PSGD).\n\n$$\\text{Proof of Lemma 3.1}$$\n\nWe restate Lemma 3.1 here for convenience.\n\n$$\\text{Lemma B.1 (Lemma 3.1).}$$ Let $$D_{XY}$$ be a distribution over $$\\mathbb{R}^d \\times \\{ \\pm 1 \\}$$, $$w \\in S^{d-1}$$, $$\\theta \\in (0, \\pi/4]$$, $$\\lambda \\geq 1$$ and $$\\delta \\in (0, 1)$$. Then, for a sufficiently large constant $$C$$, there is a tester that given $$\\delta, \\theta, w$$ and a set $$S$$ of samples from $$D_X$$ with size at least $$C \\cdot d^4$$, runs in time $$\\text{poly}(d, 1/\\theta, 1/\\delta)$$ and satisfies the following specifications:\n\n1. If the tester accepts $S$, then for every unit vector $w' \\in \\mathbb{R}^n$ satisfying $\\angle(w, w') \\leq \\theta$ we have $\\mathbb{E}_{x \\sim S}[ \\text{sign}(\\langle w', x \\rangle) \\neq \\text{sign}(\\langle w, x \\rangle)] \\leq C \\cdot \\theta \\cdot \\lambda^C$\n2. If the distribution $D_X$ is $\\lambda$-nice, the tester accepts $S$ with probability $1 - \\delta$.\n\n$$\\text{Proof.}$$ The testing algorithm receives integer $$d$$, set $$S \\subset \\mathbb{R}^d$$, $$w \\in S^{d-1}$$, $$\\theta \\in (0, \\pi/4]$$, $$\\lambda \\geq 1$$ and $$\\delta \\in (0, 1)$$ and does the following for some sufficiently large universal constant $$C_1 > 0$$:\n\n1. If $\\mathbb{P}_{x \\in S}[|\\langle w, x \\rangle| \\in [0, \\theta]] > C_1 \\theta \\lambda^{C_1}$, then reject.\n2. Let $\\text{proj}_{\\perp w} : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d-1}$ denote the operator that given any vector in $\\mathbb{R}^d$, it outputs its projection into the $(d - 1)$-dimensional subspace of $\\mathbb{R}^d$ that is orthogonal to $w$.\n3. Compute the $(d - 1) \\times (d - 1)$ matrix $M_S$ as follows:\n\n$$\nM_S = \\sum_{x \\in S} \\sum_{i=2}^{\\infty} \\left( \\text{proj}_{\\perp w} x \\right) \\left( \\text{proj}_{\\perp w} x \\right)^T \\cdot 1\\{ |\\langle w, x \\rangle| \\in [(i - 1)\\theta, i\\theta) \\} / (i - 1)^2\n$$\n\nRun the spectral tester of Proposition A.2 on $$M_S$$ given $$\\delta \\leftarrow \\delta$$, $$\\lambda \\leftarrow C_1 \\lambda^{C_1}$$ and $$\\theta \\leftarrow C_1$$.\ni.e., compute $$\\| M_S \\|_{\\text{op}}$$ and if $$\\| M_S \\|_{\\text{op}} > C_1 \\theta \\lambda^{C_1}$$, then reject. Otherwise, accept.\n\nNote that only at most $$|S|$$ many terms below are non-zero, hence $$M_S$$ can be computed efficiently."}]}, {"page": 23, "text": "     First, suppose the test accepts. For the following, consider the vector w\u2032 \u2208               Rd to be an arbitrary\nunit vector and v \u2208      Rd to be the unit vector that is perpendicular to w, lies within the plane defined\nby w and w\u2032 and \u27e8v, w\u2032\u27e9         \u2264  0. Then we have:\n   P                                           \u221e    P    |\u27e8v, x\u27e9| >    \u03b8               & |\u27e8w, x\u27e9| \u2208    [(i \u2212  1)\u03b8, i\u03b8]\n  x\u223cS[sign(\u27e8w\u2032, x\u27e9) \u0338= sign(\u27e8w, x\u27e9)] \u2264        i=1 x\u223cS                tan \u03b8 \u00b7 (i \u2212  1)\n                                                           Implies |\u27e8v,x\u27e9|>(i\u22121)/2\n                                           \u2264   P                       +4   \u221e   Ex\u223cS    \u27e8v, x\u27e921|\u27e8w,x\u27e9|\u2208[(i\u22121)\u03b8,i\u03b8]\n                                              x\u2208S [|\u27e8w, x\u27e9| \u2208   [0, \u03b8]]    i=2                (i \u2212 1)2\n                                           \u2264  5C1\u03b8\u03bbC1\u2264C1\u03b8\u03bbC1               \u27e8proj\u22a5w v, M proj\u22a5w v\u27e9\u2264\u2225M\u2225op\u2264C1\u03b8\u03bbC1\n     For part (b), we suppose that the distribution DX is indeed \u03bb-nice. We will show that with\nprobability at least 1 \u2212     \u03b4, the tester will accept, i.e., that\n                                         P                                                                         (B.1)\n                                       x\u2208S [|\u27e8w, x\u27e9| \u2208   [0, \u03b8]] \u2264  C1\u03b8\u03bbC1 and\n                                                      \u2225MS\u2225op \u2264      C1\u03b8\u03bbC1                                         (B.2)\nWe first observe that the corresponding quantities under distribution DX due to Proposition A.3.\nIn particular, we have that for some universal constant C\u2032 > 0\n                                    P                                                                              (B.3)\n                E                x\u2208DX [|\u27e8w, x\u27e9| \u2208     [0, \u03b8]] \u2264 C\u2032\u03b8\u03bbC\u2032 and                                         (B.4)\n              x\u2208DX[\u27e8v\u2032, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2208      [c, c + \u03b8]}] \u2264  C\u2032\u03b8\u03bbC\u2032 for any v\u2032 \u2208      Sd\u22121 and c \u2265     0\nIf we let MD    X = EDX [MS], we have that\n              \u2225MDX \u2225op =       sup                          sup        (proj \u22a5w v\u2032)T MDX (proj\u22a5w v\u2032)\n                              u\u2208Sd\u22122 uT MDX u =      v\u2032\u2208Sd\u22121:\u27e8v\u2032,w\u27e9=0\n                           \u2264   \u221e      1        sup      E\n                              i=2  (i \u2212 1)2  v\u2032\u2208Sd\u22121 x\u2208DX[\u27e8v\u2032, x\u27e92 \u00b7 1{|\u27e8w, x\u27e9| \u2208      [c, c + \u03b8]}]\n                           \u2264   \u221e      1        sup                      \u03b8\u03bbC\u2032\n                                   (i \u2212 1)2                         6\n                              i=2            v\u2032\u2208Sd\u22121 C\u2032\u03b8\u03bbC\u2032 \u2264     C\u2032\u03c02\nBy Proposition A.2, in order to satisfy expression (B.2), it remains to show that Ez\u223cD[(z\u2113zj)2] \u2264\nC1\u03bbC1 for any \u2113, j \u2208      [d], where z is defined as follows\n                                       z =    \u221e   proj\u22a5w x\n                                             i=2   (i \u2212  1) 1|\u27e8w,x\u27e9|\u2208[(i\u22121)\u03b8,i\u03b8).\nSince Ez\u223cD[(z\u2113zj)2] \u2264       Ez\u223cD[\u27e8u, x\u27e92\u27e8u\u2032, x\u27e92], for some unit vectors u, u\u2032 \u2208            Sd\u22121 (orthogonal to w),\nthe desired bound follows from Proposition A.3.\n     It remains to bound the absolute distance between the quantities of the left hand side of ex-\npressions (B.1) and (B.3). This can be achieved by an application of the Hoeffding bound, since\nthe empirical version of the quantity is the average of independent Bernoulli random variables.\n                                                           23", "md": "# Math Equations\n\nFirst, suppose the test accepts. For the following, consider the vector \\( w' \\in \\mathbb{R}^d \\) to be an arbitrary unit vector and \\( v \\in \\mathbb{R}^d \\) to be the unit vector that is perpendicular to \\( w \\), lies within the plane defined by \\( w \\) and \\( w' \\) and \\( \\langle v, w' \\rangle \\leq 0 \\). Then we have:\n\n$$\n\\begin{array}{|c|c|}\n\\hline\nP(x \\sim S[sign(\\langle w', x \\rangle) \\neq sign(\\langle w, x \\rangle)]) \\leq \\sum_{i=1}^{\\infty} P\\left(|\\langle v, x \\rangle| > \\theta \\text{ and } |\\langle w, x \\rangle| \\in [(i-1)\\theta, i\\theta]\\right) \\leq \\sum_{i=1}^{\\infty} x \\sim S \\tan \\theta \\cdot (i-1) \\\\\n\\leq \\sum_{i=1}^{\\infty} P\\left(|\\langle v, x \\rangle| > \\frac{i-1}{2}\\right) \\leq \\sum_{i=2}^{\\infty} P\\left(|\\langle v, x \\rangle|^2 \\cdot | \\langle w, x \\rangle| \\in [(i-1)\\theta, i\\theta] \\text{ and } |\\langle w, x \\rangle| \\in [0, \\theta]\\right) \\frac{(i-1)^2}{5} \\\\\n\\leq 5C_1 \\theta \\lambda C_1 \\leq C_1 \\theta \\lambda C_1 \\leq \\langle \\text{proj}_{\\perp w} v, M \\text{proj}_{\\perp w} v \\rangle \\leq \\|M\\|_{\\text{op}} \\leq C_1 \\theta \\lambda C_1 \\\\\n\\hline\n\\end{array}\n$$\nFor part (b), we suppose that the distribution \\( DX \\) is indeed \\( \\lambda \\)-nice. We will show that with probability at least \\( 1 - \\delta \\), the tester will accept, i.e., that\n\n$$\n\\begin{array}{|c|}\n\\hline\nP(x \\in S [| \\langle w, x \\rangle | \\in [0, \\theta]]) \\leq C_1 \\theta \\lambda C_1 \\text{ and } \\|MS\\|_{\\text{op}} \\leq C_1 \\theta \\lambda C_1 \\\\\n\\hline\n\\end{array}\n$$\nWe first observe that the corresponding quantities under distribution \\( DX \\) due to Proposition A.3. In particular, we have that for some universal constant \\( C' > 0 \\)\n\n$$\n\\begin{array}{|c|}\n\\hline\nE[x \\in DX [| \\langle w, x \\rangle | \\in [0, \\theta]] \\leq C'\\theta \\lambda C' \\text{ and } x \\in DX[\\langle v', x \\rangle^2 \\cdot 1\\{ | \\langle w, x \\rangle | \\in [c, c + \\theta] \\}] \\leq C'\\theta \\lambda C' \\text{ for any } v' \\in \\mathbb{S}^{d-1} \\text{ and } c \\geq 0 \\\\\n\\hline\n\\end{array}\n$$\nIf we let \\( MD_X = E[DX \\cdot MS] \\), we have that\n\n$$\n\\begin{array}{|c|}\n\\hline\n\\|MD_X \\|_{\\text{op}} = \\sup_{u \\in \\mathbb{S}^{d-2}} u^T MD_X u = \\sup_{v' \\in \\mathbb{S}^{d-1}: \\langle v', w \\rangle = 0} \\leq \\sum_{i=2}^{\\infty} \\frac{(i-1)^2}{6} v' \\in \\mathbb{S}^{d-1} C'\\theta \\lambda C' \\leq C'\\pi^2 \\\\\n\\hline\n\\end{array}\n$$\nBy Proposition A.2, in order to satisfy expression (B.2), it remains to show that \\( E[z_{\\ell} z_j)^2] \\leq C_1 \\lambda C_1 \\) for any \\( \\ell, j \\in [d] \\), where \\( z \\) is defined as follows\n\n$$\nz = \\sum_{i=2}^{\\infty} (i-1) 1\\{ | \\langle w, x \\rangle | \\in [(i-1)\\theta, i\\theta] \\}.\n$$\nSince \\( E[z_{\\ell} z_j)^2] \\leq E[\\langle u, x \\rangle^2 \\langle u', x \\rangle^2] \\), for some unit vectors \\( u, u' \\in \\mathbb{S}^{d-1} \\) (orthogonal to \\( w \\)), the desired bound follows from Proposition A.3.\n\nIt remains to bound the absolute distance between the quantities of the left hand side of expressions (B.1) and (B.3). This can be achieved by an application of the Hoeffding bound, since the empirical version of the quantity is the average of independent Bernoulli random variables.\n\n23", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "First, suppose the test accepts. For the following, consider the vector \\( w' \\in \\mathbb{R}^d \\) to be an arbitrary unit vector and \\( v \\in \\mathbb{R}^d \\) to be the unit vector that is perpendicular to \\( w \\), lies within the plane defined by \\( w \\) and \\( w' \\) and \\( \\langle v, w' \\rangle \\leq 0 \\). Then we have:\n\n$$\n\\begin{array}{|c|c|}\n\\hline\nP(x \\sim S[sign(\\langle w', x \\rangle) \\neq sign(\\langle w, x \\rangle)]) \\leq \\sum_{i=1}^{\\infty} P\\left(|\\langle v, x \\rangle| > \\theta \\text{ and } |\\langle w, x \\rangle| \\in [(i-1)\\theta, i\\theta]\\right) \\leq \\sum_{i=1}^{\\infty} x \\sim S \\tan \\theta \\cdot (i-1) \\\\\n\\leq \\sum_{i=1}^{\\infty} P\\left(|\\langle v, x \\rangle| > \\frac{i-1}{2}\\right) \\leq \\sum_{i=2}^{\\infty} P\\left(|\\langle v, x \\rangle|^2 \\cdot | \\langle w, x \\rangle| \\in [(i-1)\\theta, i\\theta] \\text{ and } |\\langle w, x \\rangle| \\in [0, \\theta]\\right) \\frac{(i-1)^2}{5} \\\\\n\\leq 5C_1 \\theta \\lambda C_1 \\leq C_1 \\theta \\lambda C_1 \\leq \\langle \\text{proj}_{\\perp w} v, M \\text{proj}_{\\perp w} v \\rangle \\leq \\|M\\|_{\\text{op}} \\leq C_1 \\theta \\lambda C_1 \\\\\n\\hline\n\\end{array}\n$$\nFor part (b), we suppose that the distribution \\( DX \\) is indeed \\( \\lambda \\)-nice. We will show that with probability at least \\( 1 - \\delta \\), the tester will accept, i.e., that\n\n$$\n\\begin{array}{|c|}\n\\hline\nP(x \\in S [| \\langle w, x \\rangle | \\in [0, \\theta]]) \\leq C_1 \\theta \\lambda C_1 \\text{ and } \\|MS\\|_{\\text{op}} \\leq C_1 \\theta \\lambda C_1 \\\\\n\\hline\n\\end{array}\n$$\nWe first observe that the corresponding quantities under distribution \\( DX \\) due to Proposition A.3. In particular, we have that for some universal constant \\( C' > 0 \\)\n\n$$\n\\begin{array}{|c|}\n\\hline\nE[x \\in DX [| \\langle w, x \\rangle | \\in [0, \\theta]] \\leq C'\\theta \\lambda C' \\text{ and } x \\in DX[\\langle v', x \\rangle^2 \\cdot 1\\{ | \\langle w, x \\rangle | \\in [c, c + \\theta] \\}] \\leq C'\\theta \\lambda C' \\text{ for any } v' \\in \\mathbb{S}^{d-1} \\text{ and } c \\geq 0 \\\\\n\\hline\n\\end{array}\n$$\nIf we let \\( MD_X = E[DX \\cdot MS] \\), we have that\n\n$$\n\\begin{array}{|c|}\n\\hline\n\\|MD_X \\|_{\\text{op}} = \\sup_{u \\in \\mathbb{S}^{d-2}} u^T MD_X u = \\sup_{v' \\in \\mathbb{S}^{d-1}: \\langle v', w \\rangle = 0} \\leq \\sum_{i=2}^{\\infty} \\frac{(i-1)^2}{6} v' \\in \\mathbb{S}^{d-1} C'\\theta \\lambda C' \\leq C'\\pi^2 \\\\\n\\hline\n\\end{array}\n$$\nBy Proposition A.2, in order to satisfy expression (B.2), it remains to show that \\( E[z_{\\ell} z_j)^2] \\leq C_1 \\lambda C_1 \\) for any \\( \\ell, j \\in [d] \\), where \\( z \\) is defined as follows\n\n$$\nz = \\sum_{i=2}^{\\infty} (i-1) 1\\{ | \\langle w, x \\rangle | \\in [(i-1)\\theta, i\\theta] \\}.\n$$\nSince \\( E[z_{\\ell} z_j)^2] \\leq E[\\langle u, x \\rangle^2 \\langle u', x \\rangle^2] \\), for some unit vectors \\( u, u' \\in \\mathbb{S}^{d-1} \\) (orthogonal to \\( w \\)), the desired bound follows from Proposition A.3.\n\nIt remains to bound the absolute distance between the quantities of the left hand side of expressions (B.1) and (B.3). This can be achieved by an application of the Hoeffding bound, since the empirical version of the quantity is the average of independent Bernoulli random variables.\n\n23", "md": "First, suppose the test accepts. For the following, consider the vector \\( w' \\in \\mathbb{R}^d \\) to be an arbitrary unit vector and \\( v \\in \\mathbb{R}^d \\) to be the unit vector that is perpendicular to \\( w \\), lies within the plane defined by \\( w \\) and \\( w' \\) and \\( \\langle v, w' \\rangle \\leq 0 \\). Then we have:\n\n$$\n\\begin{array}{|c|c|}\n\\hline\nP(x \\sim S[sign(\\langle w', x \\rangle) \\neq sign(\\langle w, x \\rangle)]) \\leq \\sum_{i=1}^{\\infty} P\\left(|\\langle v, x \\rangle| > \\theta \\text{ and } |\\langle w, x \\rangle| \\in [(i-1)\\theta, i\\theta]\\right) \\leq \\sum_{i=1}^{\\infty} x \\sim S \\tan \\theta \\cdot (i-1) \\\\\n\\leq \\sum_{i=1}^{\\infty} P\\left(|\\langle v, x \\rangle| > \\frac{i-1}{2}\\right) \\leq \\sum_{i=2}^{\\infty} P\\left(|\\langle v, x \\rangle|^2 \\cdot | \\langle w, x \\rangle| \\in [(i-1)\\theta, i\\theta] \\text{ and } |\\langle w, x \\rangle| \\in [0, \\theta]\\right) \\frac{(i-1)^2}{5} \\\\\n\\leq 5C_1 \\theta \\lambda C_1 \\leq C_1 \\theta \\lambda C_1 \\leq \\langle \\text{proj}_{\\perp w} v, M \\text{proj}_{\\perp w} v \\rangle \\leq \\|M\\|_{\\text{op}} \\leq C_1 \\theta \\lambda C_1 \\\\\n\\hline\n\\end{array}\n$$\nFor part (b), we suppose that the distribution \\( DX \\) is indeed \\( \\lambda \\)-nice. We will show that with probability at least \\( 1 - \\delta \\), the tester will accept, i.e., that\n\n$$\n\\begin{array}{|c|}\n\\hline\nP(x \\in S [| \\langle w, x \\rangle | \\in [0, \\theta]]) \\leq C_1 \\theta \\lambda C_1 \\text{ and } \\|MS\\|_{\\text{op}} \\leq C_1 \\theta \\lambda C_1 \\\\\n\\hline\n\\end{array}\n$$\nWe first observe that the corresponding quantities under distribution \\( DX \\) due to Proposition A.3. In particular, we have that for some universal constant \\( C' > 0 \\)\n\n$$\n\\begin{array}{|c|}\n\\hline\nE[x \\in DX [| \\langle w, x \\rangle | \\in [0, \\theta]] \\leq C'\\theta \\lambda C' \\text{ and } x \\in DX[\\langle v', x \\rangle^2 \\cdot 1\\{ | \\langle w, x \\rangle | \\in [c, c + \\theta] \\}] \\leq C'\\theta \\lambda C' \\text{ for any } v' \\in \\mathbb{S}^{d-1} \\text{ and } c \\geq 0 \\\\\n\\hline\n\\end{array}\n$$\nIf we let \\( MD_X = E[DX \\cdot MS] \\), we have that\n\n$$\n\\begin{array}{|c|}\n\\hline\n\\|MD_X \\|_{\\text{op}} = \\sup_{u \\in \\mathbb{S}^{d-2}} u^T MD_X u = \\sup_{v' \\in \\mathbb{S}^{d-1}: \\langle v', w \\rangle = 0} \\leq \\sum_{i=2}^{\\infty} \\frac{(i-1)^2}{6} v' \\in \\mathbb{S}^{d-1} C'\\theta \\lambda C' \\leq C'\\pi^2 \\\\\n\\hline\n\\end{array}\n$$\nBy Proposition A.2, in order to satisfy expression (B.2), it remains to show that \\( E[z_{\\ell} z_j)^2] \\leq C_1 \\lambda C_1 \\) for any \\( \\ell, j \\in [d] \\), where \\( z \\) is defined as follows\n\n$$\nz = \\sum_{i=2}^{\\infty} (i-1) 1\\{ | \\langle w, x \\rangle | \\in [(i-1)\\theta, i\\theta] \\}.\n$$\nSince \\( E[z_{\\ell} z_j)^2] \\leq E[\\langle u, x \\rangle^2 \\langle u', x \\rangle^2] \\), for some unit vectors \\( u, u' \\in \\mathbb{S}^{d-1} \\) (orthogonal to \\( w \\)), the desired bound follows from Proposition A.3.\n\nIt remains to bound the absolute distance between the quantities of the left hand side of expressions (B.1) and (B.3). This can be achieved by an application of the Hoeffding bound, since the empirical version of the quantity is the average of independent Bernoulli random variables.\n\n23"}]}, {"page": 24, "text": "C        Proof of Proposition 4.3\nWe restate Proposition 4.3 here for completeness.\nProposition C.1 (Modification from [GKSV23, DKTZ20a, DKTZ20b]). For a distribution DXY\nover Rd \u00d7 {\u00b11} let opt be the minimum error achieved by some origin-centered halfspace and\nw\u2217    \u2208  Sd\u22121 a corresponding vector. Consider L\u03c3 as in Equation (4.1) for \u03c3 > 0 and let \u03b7 < 1/2. Let\nw \u2208    Sd\u22121 with \u2221(w, w\u2217) = \u03b8 < \u03c02 and v \u2208                     span(w, w\u2217) such that \u27e8v, w\u27e9                = 0 and \u27e8v, w\u2217\u27e9         < 0. Then,\n                                                                            \u03c3\nfor some universal constant C > 0 and any \u03b1 \u2265                            2 tan \u03b8 we have \u2225\u2207wL\u03c3(w; DXY)\u22252 \u2265                      A1 \u2212A2 \u2212A3,\nwhere\n               A1 =        \u03b1           |\u27e8v, x\u27e9| \u2265     \u03b1 and |\u27e8w, x\u27e9| \u2264            \u03c3\n               A2 =     C \u00b7C\u03c3 \u00b7 P      |\u27e8w, x\u27e9| \u2264      \u03c3      and A3 = C          6               E  \u27e8v, x\u27e92 \u00b7 1{|\u27e8w,x\u27e9|\u2264\u03c32 }\n                        tan \u03b8 \u00b7 P                      2                       \u03c3 \u00b7 \u221aopt \u00b7\nMoreover, if the noise is Massart with rate \u03b7, then \u2225\u2207wL\u03c3(w; DXY)\u22252 \u2265                                          (1 \u2212   2\u03b7)A1 \u2212      A2.\nProof. The proof is a slight modification of a part of the proof of Lemma 4.4 in [GKSV23], but we\npresent it here for completeness.\n      For any vector x \u2208            Rd, let: xw = \u27e8w, x\u27e9             and xv = \u27e8v, x\u27e9. It follows that projV (x) = xve1 +\nxwe2, where proj           V is the operator that orthogonally projects vectors on V . Using the fact that\n\u2207w(\u27e8w, x\u27e9/\u2225w\u22252) = x \u2212                   \u27e8w, x\u27e9w = x \u2212            xww for any w \u2208               Sd\u22121, the interchangeability of the\ngradient and expectation operators and the fact that \u2113\u2032                            \u03c3 is an even function we have that\n                                       \u2207wL\u03c3(w) = E              \u2212  \u2113\u2032\n                                                                    \u03c3(|\u27e8w, x\u27e9|) \u00b7 y \u00b7 (x \u2212         xww)\nSince the projection operator proj                  V is a contraction, we have \u2225\u2207wL\u03c3(w)\u22252 \u2265                          \u2225projV \u2207wL\u03c3(w)\u22252,\nand we can therefore restrict our attention to a simpler, two dimensional problem. In particular,\nsince proj     V (x) = xve1 + xwe2, we obtain\n         \u2225projV \u2207wL\u03c3(w)\u22252 =                E    \u2212  \u2113\u2032\u03c3(|xw|) \u00b7 y \u00b7 xv\n                                       =   E    \u2212  \u2113\u2032\u03c3(|xw|) \u00b7 sign(\u27e8w\u2217, x\u27e9) \u00b7 (1 \u2212            2 1{y \u0338= sign(\u27e8w\u2217, x\u27e9)}) \u00b7 xv\nLet F(y, x) denote 1\u22122 1{y \u0338= sign(\u27e8w\u2217, x\u27e9)}. We may write xv as |xv|\u00b7sign(xv) and let G \u2286                                               R2 such\nthat sign(xv) \u00b7 sign(\u27e8w\u2217, x\u27e9) = \u22121 iff x \u2208                    G. Then, sign(xv) \u00b7 sign(\u27e8w\u2217, x\u27e9) = 1{x \u0338\u2208                     G} \u2212    1{x \u2208     G}.\nWe obtain\n             \u2225 projV \u2207wL\u03c3(w)\u22252 =\n               =    E   \u2113\u2032\n                         \u03c3(|xw|) \u00b7 (1{x \u2208         G} \u2212    1{x \u0338\u2208    G}) \u00b7 F(y, x) \u00b7 |xv|\u00b7\n               \u2265   E   \u2113\u2032                                                   \u2212  E   \u2113\u2032\n                        \u03c3(|xw|) \u00b7 1{x \u2208         G} \u00b7 F(y, x) \u00b7 |xv|                  \u03c3(|xw|) \u00b7 1{x \u0338\u2208       G} \u00b7 F(y, x) \u00b7 |xv|\nLet A\u2032   1 = E[\u2113\u2032   \u03c3(|xw|) \u00b7 1{x \u2208        G} \u00b7 F(y, x) \u00b7 |xv|] and A\u2032          2 = E[\u2113\u2032   \u03c3(|xw|) \u00b7 1{x \u0338\u2208       G} \u00b7 F(y, x) \u00b7 |xv|].\n      In the Massart noise case Ey|x[F(y, x)] = 1 \u2212                        2\u03b7(x) \u2208     [1 \u2212   2\u03b7, 1], where 1 \u2212         2\u03b7 > 0. Therefore,\nwe have that A\u2032        1 \u2265    (1 \u2212   2\u03b7) \u00b7 E[\u2113\u2032  \u03c3(|xw|) \u00b7 1{x \u2208         G} \u00b7 |xv|]. When the noise is adversarial, we have\nA\u2032 1 \u2265   E[\u2113\u2032 \u03c3(|xw|) \u00b7 1{x \u2208        G} \u00b7 |xv|] \u2212      2 E[\u2113\u2032\u03c3(|xw|) \u00b7 1{x \u2208         G} \u00b7 1{y \u0338= sign(\u27e8w\u2217, x\u27e9)} \u00b7 |xv|].\n                                                                        24", "md": "Proof of Proposition 4.3\n\nWe restate Proposition 4.3 here for completeness.\n\nProposition C.1 (Modification from [GKSV23, DKTZ20a, DKTZ20b]). For a distribution \\(D_{XY}\\) over \\(\\mathbb{R}^d \\times \\{ \\pm 1 \\}\\) let opt be the minimum error achieved by some origin-centered halfspace and \\(w^* \\in S^{d-1}\\) a corresponding vector. Consider \\(L_{\\sigma}\\) as in Equation (4.1) for \\(\\sigma > 0\\) and let \\(\\eta < \\frac{1}{2}\\). Let \\(w \\in S^{d-1}\\) with \\(\\angle(w, w^*) = \\theta < \\frac{\\pi}{2}\\) and \\(v \\in \\text{span}(w, w^*)\\) such that \\(\\langle v, w \\rangle = 0\\) and \\(\\langle v, w^* \\rangle < 0\\). Then, for some universal constant \\(C > 0\\) and any \\(\\alpha \\geq 2 \\tan \\theta\\) we have \\(\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq A_1 - A_2 - A_3\\), where\n\n$$\n\\begin{align*}\nA_1 & = \\alpha \\quad | \\langle v, x \\rangle | \\geq \\alpha \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\\\\nA_2 & = C \\cdot C_{\\sigma} \\cdot P \\quad | \\langle w, x \\rangle | \\leq \\sigma \\text{ and } A_3 = C \\cdot \\frac{6}{\\tan \\theta} \\cdot P^2 \\cdot \\sigma \\cdot \\sqrt{\\text{opt}} \\cdot 1\\{ | \\langle w, x \\rangle | \\leq \\sigma^2 \\}\n\\end{align*}\n$$\nMoreover, if the noise is Massart with rate \\(\\eta\\), then \\(\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq (1 - 2\\eta)A_1 - A_2\\).\n\nProof. The proof is a slight modification of a part of the proof of Lemma 4.4 in [GKSV23], but we present it here for completeness.\n\nFor any vector \\(x \\in \\mathbb{R}^d\\), let: \\(x_w = \\langle w, x \\rangle\\) and \\(x_v = \\langle v, x \\)\\(. It follows that \\(\\text{proj}_V(x) = x_v e_1 + x_w e_2\\), where \\(\\text{proj}_V\\) is the operator that orthogonally projects vectors on \\(V\\). Using the fact that \\(\\nabla_w(\\langle w, x \\rangle / \\| w \\|_2) = x - \\langle w, x \\rangle w = x - x w w\\) for any \\(w \\in S^{d-1}\\), the interchangeability of the gradient and expectation operators and the fact that \\(\\ell'_{\\sigma}\\) is an even function we have that\n\n$$\n\\nabla_w L_{\\sigma}(w) = E - \\ell'_{\\sigma}(| \\langle w, x \\rangle |) \\cdot y \\cdot (x - x w w)\n$$\nSince the projection operator \\(\\text{proj}_V\\) is a contraction, we have \\(\\| \\nabla_w L_{\\sigma}(w) \\|_2 \\geq \\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2\\), and we can therefore restrict our attention to a simpler, two-dimensional problem. In particular, since \\(\\text{proj}_V(x) = x_v e_1 + x_w e_2\\), we obtain\n\n$$\n\\begin{align*}\n\\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2 & = E - \\ell'_{\\sigma}(|x_w|) \\cdot y \\cdot x_v \\\\\n& = E - \\ell'_{\\sigma}(|x_w|) \\cdot \\text{sign}(\\langle w^*, x \\rangle) \\cdot (1 - 2 \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\}) \\cdot x_v\n\\end{align*}\n$$\nLet \\(F(y, x)\\) denote \\(1 - 2 \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\}\\). We may write \\(x_v\\) as \\(|x_v| \\cdot \\text{sign}(x_v)\\) and let \\(G \\subseteq \\mathbb{R}^2\\) such that \\(\\text{sign}(x_v) \\cdot \\text{sign}(\\langle w^*, x \\rangle) = -1\\) if \\(x \\in G\\). Then, \\(\\text{sign}(x_v) \\cdot \\text{sign}(\\langle w^*, x \\rangle) = 1\\{ x \\notin G \\} - 1\\{ x \\in G \\}\\). We obtain\n\n$$\n\\begin{align*}\n\\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2 & = E \\ell'_{\\sigma}(|x_w|) \\cdot (1\\{ x \\in G \\} - 1\\{ x \\notin G \\}) \\cdot F(y, x) \\cdot |x_v| \\\\\n& \\geq E \\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot F(y, x) \\cdot |x_v| - E \\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\notin G \\} \\cdot F(y, x) \\cdot |x_v|\n\\end{align*}\n$$\nLet \\(A'_1 = E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot F(y, x) \\cdot |x_v|]\\) and \\(A'_2 = E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\notin G \\} \\cdot F(y, x) \\cdot |x_v|]\\).\n\nIn the Massart noise case \\(E_y|x[F(y, x)] = 1 - 2\\eta(x) \\in [1 - 2\\eta, 1]\\), where \\(1 - 2\\eta > 0\\). Therefore, we have that \\(A'_1 \\geq (1 - 2\\eta) \\cdot E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot |x_v|]\\). When the noise is adversarial, we have \\(A'_1 \\geq E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot |x_v|] - 2 \\cdot E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\} \\cdot |x_v|]\\).", "images": [], "items": [{"type": "text", "value": "Proof of Proposition 4.3\n\nWe restate Proposition 4.3 here for completeness.\n\nProposition C.1 (Modification from [GKSV23, DKTZ20a, DKTZ20b]). For a distribution \\(D_{XY}\\) over \\(\\mathbb{R}^d \\times \\{ \\pm 1 \\}\\) let opt be the minimum error achieved by some origin-centered halfspace and \\(w^* \\in S^{d-1}\\) a corresponding vector. Consider \\(L_{\\sigma}\\) as in Equation (4.1) for \\(\\sigma > 0\\) and let \\(\\eta < \\frac{1}{2}\\). Let \\(w \\in S^{d-1}\\) with \\(\\angle(w, w^*) = \\theta < \\frac{\\pi}{2}\\) and \\(v \\in \\text{span}(w, w^*)\\) such that \\(\\langle v, w \\rangle = 0\\) and \\(\\langle v, w^* \\rangle < 0\\). Then, for some universal constant \\(C > 0\\) and any \\(\\alpha \\geq 2 \\tan \\theta\\) we have \\(\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq A_1 - A_2 - A_3\\), where\n\n$$\n\\begin{align*}\nA_1 & = \\alpha \\quad | \\langle v, x \\rangle | \\geq \\alpha \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\\\\nA_2 & = C \\cdot C_{\\sigma} \\cdot P \\quad | \\langle w, x \\rangle | \\leq \\sigma \\text{ and } A_3 = C \\cdot \\frac{6}{\\tan \\theta} \\cdot P^2 \\cdot \\sigma \\cdot \\sqrt{\\text{opt}} \\cdot 1\\{ | \\langle w, x \\rangle | \\leq \\sigma^2 \\}\n\\end{align*}\n$$\nMoreover, if the noise is Massart with rate \\(\\eta\\), then \\(\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq (1 - 2\\eta)A_1 - A_2\\).\n\nProof. The proof is a slight modification of a part of the proof of Lemma 4.4 in [GKSV23], but we present it here for completeness.\n\nFor any vector \\(x \\in \\mathbb{R}^d\\), let: \\(x_w = \\langle w, x \\rangle\\) and \\(x_v = \\langle v, x \\)\\(. It follows that \\(\\text{proj}_V(x) = x_v e_1 + x_w e_2\\), where \\(\\text{proj}_V\\) is the operator that orthogonally projects vectors on \\(V\\). Using the fact that \\(\\nabla_w(\\langle w, x \\rangle / \\| w \\|_2) = x - \\langle w, x \\rangle w = x - x w w\\) for any \\(w \\in S^{d-1}\\), the interchangeability of the gradient and expectation operators and the fact that \\(\\ell'_{\\sigma}\\) is an even function we have that\n\n$$\n\\nabla_w L_{\\sigma}(w) = E - \\ell'_{\\sigma}(| \\langle w, x \\rangle |) \\cdot y \\cdot (x - x w w)\n$$\nSince the projection operator \\(\\text{proj}_V\\) is a contraction, we have \\(\\| \\nabla_w L_{\\sigma}(w) \\|_2 \\geq \\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2\\), and we can therefore restrict our attention to a simpler, two-dimensional problem. In particular, since \\(\\text{proj}_V(x) = x_v e_1 + x_w e_2\\), we obtain\n\n$$\n\\begin{align*}\n\\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2 & = E - \\ell'_{\\sigma}(|x_w|) \\cdot y \\cdot x_v \\\\\n& = E - \\ell'_{\\sigma}(|x_w|) \\cdot \\text{sign}(\\langle w^*, x \\rangle) \\cdot (1 - 2 \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\}) \\cdot x_v\n\\end{align*}\n$$\nLet \\(F(y, x)\\) denote \\(1 - 2 \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\}\\). We may write \\(x_v\\) as \\(|x_v| \\cdot \\text{sign}(x_v)\\) and let \\(G \\subseteq \\mathbb{R}^2\\) such that \\(\\text{sign}(x_v) \\cdot \\text{sign}(\\langle w^*, x \\rangle) = -1\\) if \\(x \\in G\\). Then, \\(\\text{sign}(x_v) \\cdot \\text{sign}(\\langle w^*, x \\rangle) = 1\\{ x \\notin G \\} - 1\\{ x \\in G \\}\\). We obtain\n\n$$\n\\begin{align*}\n\\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2 & = E \\ell'_{\\sigma}(|x_w|) \\cdot (1\\{ x \\in G \\} - 1\\{ x \\notin G \\}) \\cdot F(y, x) \\cdot |x_v| \\\\\n& \\geq E \\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot F(y, x) \\cdot |x_v| - E \\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\notin G \\} \\cdot F(y, x) \\cdot |x_v|\n\\end{align*}\n$$\nLet \\(A'_1 = E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot F(y, x) \\cdot |x_v|]\\) and \\(A'_2 = E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\notin G \\} \\cdot F(y, x) \\cdot |x_v|]\\).\n\nIn the Massart noise case \\(E_y|x[F(y, x)] = 1 - 2\\eta(x) \\in [1 - 2\\eta, 1]\\), where \\(1 - 2\\eta > 0\\). Therefore, we have that \\(A'_1 \\geq (1 - 2\\eta) \\cdot E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot |x_v|]\\). When the noise is adversarial, we have \\(A'_1 \\geq E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot |x_v|] - 2 \\cdot E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\} \\cdot |x_v|]\\).", "md": "Proof of Proposition 4.3\n\nWe restate Proposition 4.3 here for completeness.\n\nProposition C.1 (Modification from [GKSV23, DKTZ20a, DKTZ20b]). For a distribution \\(D_{XY}\\) over \\(\\mathbb{R}^d \\times \\{ \\pm 1 \\}\\) let opt be the minimum error achieved by some origin-centered halfspace and \\(w^* \\in S^{d-1}\\) a corresponding vector. Consider \\(L_{\\sigma}\\) as in Equation (4.1) for \\(\\sigma > 0\\) and let \\(\\eta < \\frac{1}{2}\\). Let \\(w \\in S^{d-1}\\) with \\(\\angle(w, w^*) = \\theta < \\frac{\\pi}{2}\\) and \\(v \\in \\text{span}(w, w^*)\\) such that \\(\\langle v, w \\rangle = 0\\) and \\(\\langle v, w^* \\rangle < 0\\). Then, for some universal constant \\(C > 0\\) and any \\(\\alpha \\geq 2 \\tan \\theta\\) we have \\(\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq A_1 - A_2 - A_3\\), where\n\n$$\n\\begin{align*}\nA_1 & = \\alpha \\quad | \\langle v, x \\rangle | \\geq \\alpha \\text{ and } | \\langle w, x \\rangle | \\leq \\sigma \\\\\nA_2 & = C \\cdot C_{\\sigma} \\cdot P \\quad | \\langle w, x \\rangle | \\leq \\sigma \\text{ and } A_3 = C \\cdot \\frac{6}{\\tan \\theta} \\cdot P^2 \\cdot \\sigma \\cdot \\sqrt{\\text{opt}} \\cdot 1\\{ | \\langle w, x \\rangle | \\leq \\sigma^2 \\}\n\\end{align*}\n$$\nMoreover, if the noise is Massart with rate \\(\\eta\\), then \\(\\| \\nabla_w L_{\\sigma}(w; D_{XY}) \\|_2 \\geq (1 - 2\\eta)A_1 - A_2\\).\n\nProof. The proof is a slight modification of a part of the proof of Lemma 4.4 in [GKSV23], but we present it here for completeness.\n\nFor any vector \\(x \\in \\mathbb{R}^d\\), let: \\(x_w = \\langle w, x \\rangle\\) and \\(x_v = \\langle v, x \\)\\(. It follows that \\(\\text{proj}_V(x) = x_v e_1 + x_w e_2\\), where \\(\\text{proj}_V\\) is the operator that orthogonally projects vectors on \\(V\\). Using the fact that \\(\\nabla_w(\\langle w, x \\rangle / \\| w \\|_2) = x - \\langle w, x \\rangle w = x - x w w\\) for any \\(w \\in S^{d-1}\\), the interchangeability of the gradient and expectation operators and the fact that \\(\\ell'_{\\sigma}\\) is an even function we have that\n\n$$\n\\nabla_w L_{\\sigma}(w) = E - \\ell'_{\\sigma}(| \\langle w, x \\rangle |) \\cdot y \\cdot (x - x w w)\n$$\nSince the projection operator \\(\\text{proj}_V\\) is a contraction, we have \\(\\| \\nabla_w L_{\\sigma}(w) \\|_2 \\geq \\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2\\), and we can therefore restrict our attention to a simpler, two-dimensional problem. In particular, since \\(\\text{proj}_V(x) = x_v e_1 + x_w e_2\\), we obtain\n\n$$\n\\begin{align*}\n\\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2 & = E - \\ell'_{\\sigma}(|x_w|) \\cdot y \\cdot x_v \\\\\n& = E - \\ell'_{\\sigma}(|x_w|) \\cdot \\text{sign}(\\langle w^*, x \\rangle) \\cdot (1 - 2 \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\}) \\cdot x_v\n\\end{align*}\n$$\nLet \\(F(y, x)\\) denote \\(1 - 2 \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\}\\). We may write \\(x_v\\) as \\(|x_v| \\cdot \\text{sign}(x_v)\\) and let \\(G \\subseteq \\mathbb{R}^2\\) such that \\(\\text{sign}(x_v) \\cdot \\text{sign}(\\langle w^*, x \\rangle) = -1\\) if \\(x \\in G\\). Then, \\(\\text{sign}(x_v) \\cdot \\text{sign}(\\langle w^*, x \\rangle) = 1\\{ x \\notin G \\} - 1\\{ x \\in G \\}\\). We obtain\n\n$$\n\\begin{align*}\n\\| \\text{proj}_V \\nabla_w L_{\\sigma}(w) \\|_2 & = E \\ell'_{\\sigma}(|x_w|) \\cdot (1\\{ x \\in G \\} - 1\\{ x \\notin G \\}) \\cdot F(y, x) \\cdot |x_v| \\\\\n& \\geq E \\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot F(y, x) \\cdot |x_v| - E \\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\notin G \\} \\cdot F(y, x) \\cdot |x_v|\n\\end{align*}\n$$\nLet \\(A'_1 = E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot F(y, x) \\cdot |x_v|]\\) and \\(A'_2 = E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\notin G \\} \\cdot F(y, x) \\cdot |x_v|]\\).\n\nIn the Massart noise case \\(E_y|x[F(y, x)] = 1 - 2\\eta(x) \\in [1 - 2\\eta, 1]\\), where \\(1 - 2\\eta > 0\\). Therefore, we have that \\(A'_1 \\geq (1 - 2\\eta) \\cdot E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot |x_v|]\\). When the noise is adversarial, we have \\(A'_1 \\geq E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot |x_v|] - 2 \\cdot E[\\ell'_{\\sigma}(|x_w|) \\cdot 1\\{ x \\in G \\} \\cdot 1\\{ y \\neq \\text{sign}(\\langle w^*, x \\rangle) \\} \\cdot |x_v|]\\)."}]}, {"page": 25, "text": "Figure 2: The Gaussian mass in each of the regions labelled A1 and A2 is proportional to the\ncorresponding term appearing in the statement of Proposition 4.3. As \u03c3 tends to 0, the Gaussian\nmass of region\u03c3A2 shrinks faster than the one of region A1, since both the height (\u03c3) and the\nwidth (     tan \u03b8) of A2 are proportional to \u03c3, while the width of A1 is not affected (the height is \u03c3/3).\nLemma 4.4 demonstrates that a similar property is universally testable under any nice Poincar\u00b4e\ndistribution.\n                              \u03c3\n     For any \u03b1 \u2265           2 tan \u03b8, we have that\n  E   \u2113\u2032                                          \u2265   E   \u2113\u2032                                                            (since terms are positive)\n       \u03c3(|xw|) \u00b7 1{x \u2208           G} \u00b7 |xv|                  \u03c3(|xw|) \u00b7 1{x \u2208          G} \u00b7 1{|xw|\u2264\u03c36 } \u00b7|xv|\n                                                  \u2265   E    1                            |xw| \u2264       \u03c3     \u00b7 |xv|                (by Proposition 4.2)\n                                                           \u03c3 \u00b7 1{x \u2208        G} \u00b7 1                   6\n                                                  \u2265   \u03b1          1{x \u2208      G} \u00b7 1{|xw|\u2264\u03c36 } \u00b7 1{|xv|\u2265\u03b1}\n                                                      \u03c3 \u00b7 E                                                                                (see Figure 2)\n                                                  \u2265   \u03b1          1{|xw|\u2264\u03c36 } \u00b7 1{|xv|\u2265\u03b1}\n                                                  = \u03b1 \u03c3 \u00b7 E                                                def\n                                                                |xw| \u2264       \u03c3                              = A1\n                                                      \u03c3 \u00b7 P                  6 and |xv| \u2265             \u03b1\nMoreover, for some universal constant C\u2032 > 0, we similarly have\n  E   \u2113\u2032\u03c3(|xw|) \u00b7 1{x \u0338\u2208          G} \u00b7 F    (y, x) \u00b7 |xv|        \u2264   E   \u2113\u2032\u03c3(|xw|) \u00b7 1{x \u0338\u2208         G} \u00b7 |xv|                      (since F      (y, x) \u2264      1)\n                                                                 \u2264   E   C\u2032\u03c3 \u00b7 1{|xw|\u2264\u03c3       2 } \u00b7 1{x \u0338\u2208     G} \u00b7 |xv|         (by Proposition 4.2)\n                                                                 \u2264   C\u2032          1{|xw|\u2264\u03c3                         \u03c3                        (see Figure 2)\n                                                                      \u03c3 \u00b7 E                  2 } \u00b7 1{|xv|\u22642 tan \u03b8 } \u00b7|xv|\n                                                                 \u2264       C\u2032               1{|xw|\u2264\u03c32 } \u00b7 1{|xv|\u22642 tan \u03b8 }  \u03c3\n                                                                     2 \u00b7 tan \u03b8 \u00b7 E\n                                                                         C\u2032                                 def\n                                                                 \u2264                        |xw| \u2264      \u03c3     = A2\n                                                                     2 \u00b7 tan \u03b8 \u00b7 P                    2\nHence, we have shown that, in the Massart noise case, we have \u2225\u2207wL\u03c3(w)\u22252 \u2265                                                         (1 \u2212    2\u03b7)A1 \u2212       A2 as\n                                                                               25", "md": "# Math Equations and Text\n\nFigure 2: The Gaussian mass in each of the regions labelled A1 and A2 is proportional to the corresponding term appearing in the statement of Proposition 4.3. As \u03c3 tends to 0, the Gaussian mass of region \u03c3A2 shrinks faster than the one of region A1, since both the height (\u03c3) and the width ($$tan \\theta$$) of A2 are proportional to \u03c3, while the width of A1 is not affected (the height is \u03c3/3). Lemma 4.4 demonstrates that a similar property is universally testable under any nice Poincar\u00e9 distribution.\n\n$$\n\\begin{align*}\n&\\text{For any } \\alpha \\geq 2 \\tan \\theta, \\text{ we have that} \\\\\n&E_{\\ell'}\\left[\\frac{\\sigma(|xw|) \\cdot 1\\{x \\in G\\} \\cdot |xv|}{\\sigma(|xw|) \\cdot 1\\{x \\in G\\} \\cdot 1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot |xv|}\\right] \\\\\n&\\geq E\\left[\\frac{1\\{|xw| \\leq \\sigma\\} \\cdot |xv|}{\\sigma} \\right] \\text{ (by Proposition 4.2)} \\\\\n&\\geq \\alpha \\frac{1\\{x \\in G\\} \\cdot 1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot 1\\{|xv| \\geq \\alpha\\}}{\\sigma} \\text{ (see Figure 2)} \\\\\n&= \\alpha \\frac{\\sigma \\cdot E[1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot 1\\{|xv| \\geq \\alpha\\}]}{\\sigma} \\\\\n&= \\alpha \\frac{P\\{ |xw| \\leq \\sigma \\text{ and } |xv| \\geq \\alpha \\}}{\\sigma} = A1\n\\end{align*}\n$$\n\nMoreover, for some universal constant C' > 0, we similarly have:\n\n$$\n\\begin{align*}\n&E_{\\ell'}\\left[\\sigma(|xw|) \\cdot 1\\{x \\not\\in G\\} \\cdot F(y, x) \\cdot |xv|\\right] \\\\\n&\\leq E_{\\ell'}\\left[\\sigma(|xw|) \\cdot 1\\{x \\not\\in G\\} \\cdot |xv|\\right] \\text{ (since } F(y, x) \\leq 1) \\\\\n&\\leq E\\left[C' \\sigma \\cdot 1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{x \\not\\in G\\} \\cdot |xv|\\right] \\text{ (by Proposition 4.2)} \\\\\n&\\leq C' \\frac{1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{|xv| \\leq 2 \\tan \\theta\\} \\sigma}{\\sigma} \\\\\n&\\leq C' \\frac{1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{|xv| \\leq 2 \\tan \\theta\\} \\sigma}{2 \\tan \\theta} = A2\n\\end{align*}\n$$\n\nHence, we have shown that, in the Massart noise case, we have $$\\| \\nabla w L_\\sigma(w) \\|_2 \\geq (1 - 2\\eta)A1 - A2$$", "images": [{"name": "page-25-0.jpg", "height": 246, "width": 281, "x": 165, "y": 35}], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Figure 2: The Gaussian mass in each of the regions labelled A1 and A2 is proportional to the corresponding term appearing in the statement of Proposition 4.3. As \u03c3 tends to 0, the Gaussian mass of region \u03c3A2 shrinks faster than the one of region A1, since both the height (\u03c3) and the width ($$tan \\theta$$) of A2 are proportional to \u03c3, while the width of A1 is not affected (the height is \u03c3/3). Lemma 4.4 demonstrates that a similar property is universally testable under any nice Poincar\u00e9 distribution.\n\n$$\n\\begin{align*}\n&\\text{For any } \\alpha \\geq 2 \\tan \\theta, \\text{ we have that} \\\\\n&E_{\\ell'}\\left[\\frac{\\sigma(|xw|) \\cdot 1\\{x \\in G\\} \\cdot |xv|}{\\sigma(|xw|) \\cdot 1\\{x \\in G\\} \\cdot 1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot |xv|}\\right] \\\\\n&\\geq E\\left[\\frac{1\\{|xw| \\leq \\sigma\\} \\cdot |xv|}{\\sigma} \\right] \\text{ (by Proposition 4.2)} \\\\\n&\\geq \\alpha \\frac{1\\{x \\in G\\} \\cdot 1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot 1\\{|xv| \\geq \\alpha\\}}{\\sigma} \\text{ (see Figure 2)} \\\\\n&= \\alpha \\frac{\\sigma \\cdot E[1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot 1\\{|xv| \\geq \\alpha\\}]}{\\sigma} \\\\\n&= \\alpha \\frac{P\\{ |xw| \\leq \\sigma \\text{ and } |xv| \\geq \\alpha \\}}{\\sigma} = A1\n\\end{align*}\n$$\n\nMoreover, for some universal constant C' > 0, we similarly have:\n\n$$\n\\begin{align*}\n&E_{\\ell'}\\left[\\sigma(|xw|) \\cdot 1\\{x \\not\\in G\\} \\cdot F(y, x) \\cdot |xv|\\right] \\\\\n&\\leq E_{\\ell'}\\left[\\sigma(|xw|) \\cdot 1\\{x \\not\\in G\\} \\cdot |xv|\\right] \\text{ (since } F(y, x) \\leq 1) \\\\\n&\\leq E\\left[C' \\sigma \\cdot 1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{x \\not\\in G\\} \\cdot |xv|\\right] \\text{ (by Proposition 4.2)} \\\\\n&\\leq C' \\frac{1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{|xv| \\leq 2 \\tan \\theta\\} \\sigma}{\\sigma} \\\\\n&\\leq C' \\frac{1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{|xv| \\leq 2 \\tan \\theta\\} \\sigma}{2 \\tan \\theta} = A2\n\\end{align*}\n$$\n\nHence, we have shown that, in the Massart noise case, we have $$\\| \\nabla w L_\\sigma(w) \\|_2 \\geq (1 - 2\\eta)A1 - A2$$", "md": "Figure 2: The Gaussian mass in each of the regions labelled A1 and A2 is proportional to the corresponding term appearing in the statement of Proposition 4.3. As \u03c3 tends to 0, the Gaussian mass of region \u03c3A2 shrinks faster than the one of region A1, since both the height (\u03c3) and the width ($$tan \\theta$$) of A2 are proportional to \u03c3, while the width of A1 is not affected (the height is \u03c3/3). Lemma 4.4 demonstrates that a similar property is universally testable under any nice Poincar\u00e9 distribution.\n\n$$\n\\begin{align*}\n&\\text{For any } \\alpha \\geq 2 \\tan \\theta, \\text{ we have that} \\\\\n&E_{\\ell'}\\left[\\frac{\\sigma(|xw|) \\cdot 1\\{x \\in G\\} \\cdot |xv|}{\\sigma(|xw|) \\cdot 1\\{x \\in G\\} \\cdot 1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot |xv|}\\right] \\\\\n&\\geq E\\left[\\frac{1\\{|xw| \\leq \\sigma\\} \\cdot |xv|}{\\sigma} \\right] \\text{ (by Proposition 4.2)} \\\\\n&\\geq \\alpha \\frac{1\\{x \\in G\\} \\cdot 1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot 1\\{|xv| \\geq \\alpha\\}}{\\sigma} \\text{ (see Figure 2)} \\\\\n&= \\alpha \\frac{\\sigma \\cdot E[1\\{|xw| \\leq \\frac{\\sigma}{6}\\} \\cdot 1\\{|xv| \\geq \\alpha\\}]}{\\sigma} \\\\\n&= \\alpha \\frac{P\\{ |xw| \\leq \\sigma \\text{ and } |xv| \\geq \\alpha \\}}{\\sigma} = A1\n\\end{align*}\n$$\n\nMoreover, for some universal constant C' > 0, we similarly have:\n\n$$\n\\begin{align*}\n&E_{\\ell'}\\left[\\sigma(|xw|) \\cdot 1\\{x \\not\\in G\\} \\cdot F(y, x) \\cdot |xv|\\right] \\\\\n&\\leq E_{\\ell'}\\left[\\sigma(|xw|) \\cdot 1\\{x \\not\\in G\\} \\cdot |xv|\\right] \\text{ (since } F(y, x) \\leq 1) \\\\\n&\\leq E\\left[C' \\sigma \\cdot 1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{x \\not\\in G\\} \\cdot |xv|\\right] \\text{ (by Proposition 4.2)} \\\\\n&\\leq C' \\frac{1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{|xv| \\leq 2 \\tan \\theta\\} \\sigma}{\\sigma} \\\\\n&\\leq C' \\frac{1\\{|xw| \\leq \\sigma^2\\} \\cdot 1\\{|xv| \\leq 2 \\tan \\theta\\} \\sigma}{2 \\tan \\theta} = A2\n\\end{align*}\n$$\n\nHence, we have shown that, in the Massart noise case, we have $$\\| \\nabla w L_\\sigma(w) \\|_2 \\geq (1 - 2\\eta)A1 - A2$$"}]}, {"page": 26, "text": "desired. For the adversarial noise case, it remains to bound the following quantity\n  2 E   \u2113\u2032\u03c3(|xw|) \u00b7 1{x\u2208G} \u00b7 1{y\u0338=sign(\u27e8w\u2217,x\u27e9)} \u00b7|xv|                \u2264   2C\u2032   \u00b7 E   1{x\u2208G} \u00b7 1{|xw|\u2264\u03c3       2 } \u00b7 1{y\u0338=sign(\u27e8w\u2217,x\u27e9)} \u00b7|xv|\n                                                                          \u03c3\n                                                                     \u2264   2C\u2032   \u00b7 E   1{|xw|\u2264\u03c32 } \u00b7 1{y\u0338=sign(\u27e8w\u2217,x\u27e9)} \u00b7|xv|\n                                                                          \u03c3                                                   def\n                                                                     \u2264   2C\u2032   \u00b7 \u221aopt \u00b7       E    |xv|2 \u00b7 1{|xw|\u2264\u03c3     2 }   = A3\n                                                                          \u03c3\nwhere the final inequality follows from Cauchy-Schwarz inequality.\n                                                                        26", "md": "desired. For the adversarial noise case, it remains to bound the following quantity\n\n$$\n2E \\ell' \\sigma(|xw|) \\cdot 1\\{x\\in G\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv| \\leq 2C' \\cdot E 1\\{x\\in G\\} \\cdot 1\\{|xw|\\leq \\sigma^2\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv|\n$$\n$$\n\\leq 2C' \\cdot E 1\\{|xw|\\leq \\sigma^2\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv|\n$$\n$$\n\\leq 2C' \\cdot \\sqrt{\\text{opt}} \\cdot E |xv|^2 \\cdot 1\\{|xw|\\leq \\sigma^2\\} = A3\n$$\nwhere the final inequality follows from Cauchy-Schwarz inequality.\n\n26", "images": [], "items": [{"type": "text", "value": "desired. For the adversarial noise case, it remains to bound the following quantity\n\n$$\n2E \\ell' \\sigma(|xw|) \\cdot 1\\{x\\in G\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv| \\leq 2C' \\cdot E 1\\{x\\in G\\} \\cdot 1\\{|xw|\\leq \\sigma^2\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv|\n$$\n$$\n\\leq 2C' \\cdot E 1\\{|xw|\\leq \\sigma^2\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv|\n$$\n$$\n\\leq 2C' \\cdot \\sqrt{\\text{opt}} \\cdot E |xv|^2 \\cdot 1\\{|xw|\\leq \\sigma^2\\} = A3\n$$\nwhere the final inequality follows from Cauchy-Schwarz inequality.\n\n26", "md": "desired. For the adversarial noise case, it remains to bound the following quantity\n\n$$\n2E \\ell' \\sigma(|xw|) \\cdot 1\\{x\\in G\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv| \\leq 2C' \\cdot E 1\\{x\\in G\\} \\cdot 1\\{|xw|\\leq \\sigma^2\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv|\n$$\n$$\n\\leq 2C' \\cdot E 1\\{|xw|\\leq \\sigma^2\\} \\cdot 1\\{y\\neq \\text{sign}(\\langle w^*,x \\rangle)\\} \\cdot |xv|\n$$\n$$\n\\leq 2C' \\cdot \\sqrt{\\text{opt}} \\cdot E |xv|^2 \\cdot 1\\{|xw|\\leq \\sigma^2\\} = A3\n$$\nwhere the final inequality follows from Cauchy-Schwarz inequality.\n\n26"}]}], "job_id": "648f0cbc-6f67-4471-a228-740baf407708", "file_path": "./corpus/2305.11765.pdf"}