question,contexts,ground_truth,evolution_type,metadata,episode_done
How is OOD accuracy measured in the context of evaluating effective robustness in machine learning models?,"[' performed single-dimensional\nlinear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\net al., 2020). And they then evaluate the actual OOD accuracy of the models beyond the expected\nvalue that can be predicted from the fitting line, as the effective robustness. We expand on this\ndefinition by allowing for multiple ID test sets, and perform multi-dimensional linear regression to fit\na plane to predict OOD accuracy from the accuracy on multiple ID test sets.\n\nIn summary, we make the following contributions:\n- We reveal a limitation in the existing effective robustness evaluation when used to compare models\ntrained on different data distributions.\n- We then propose a new effective robustness evaluation which uses multiple ID test sets to more\nprecisely compare the effective robustness of models trained on different data.\n- We show that the OOD accuracy of various models including zero-shot CLIP models can usually\nbe better predicted from accuracies on multiple ID test sets compared to using only one ID test set.\n- Our results provide new understandings on the effective robustness gains of CLIP-like models\nobserved in prior works only using ImageNet as the ID test set, while the gains diminish under our\nnew evaluation.\n\n## Background of Effective Robustness\n\nUnder natural distribution shifts, the OOD accuracy of a model is often correlated with the ID\naccuracy. After applying a logit transformation on the accuracy, a linear trend between the transformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from\nImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky\net al., 2009) to CIFAR-10.2 (Hendrycks & Dietterich, 2018)) and models with various architectures\nand training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most\nmodels showing higher OOD accuracies naturally resulted from better ID performance.\n\nTo eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed\neffective robustness that measures the OOD performance beyond the expected OOD accuracy given\nthe ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend\nof baseline models. Since they only use a single ID test set, we refer to this version of effective\nrobustness as single-ID effective robustness.\n\nSuppose there are n baseline models \\( f_1, f_2, \\ldots, f_n \\). A baseline function \\( \\tilde{\\beta}(x) \\) is constructed to predict\nthe OOD accuracy of each baseline model, \\( acc_{ood}(f_i) \\) (\\( 1 \\leq i \\leq n \\)), given the single ID accuracy of\nthe model \\( x = acc_{id}(f_i) \\). The baseline function is instantiated as:\n\n$$\n\\tilde{\\beta}(x) = \\text{expit}(w \\text{logit}(x) + b) \\quad (1)\n$$\n\nwhere \\( w \\) and \\( b \\) are parameters, \\( \\text{logit}(x) = \\ln\\left(\\frac{1}{1-x}\\right) \\) is the logit transformation, and \\( \\text{expit}(x) \\) is the\ninverse of \\( \\text{logit}(x) \\). Since \\( \\text{logit}(\\tilde{\\beta}(x)) = w \\text{logit}(x) + b \\), the baseline function is essentially a linear\n\n# Document\n\n## ImageNet models\n\n| |80|ImageNet models|\n|---|---|---|\n|95|YFCC models|YFCC models|\n| |70| |\n\n## Linear fit for ImageNet models\n\n80\n\n70\n\n50\n\n25\n\n10\n\n## Linear fit for YFCC models\n\n50\n\n25\n\n10\n\n## ImageNet-V2 accuracy (css., %)\n\n10 25 50 70 80 90 95\n\n## ImageNet accuracy (css., %)\n\nYFCC accuracy (css., %)\n\n### (a) ImageNet-V2 accuracy against ImageNet accuracy.\n\n### (b) ImageNet-V2 accuracy against YFCC accuracy.\n\nFigure 1: Class-subsampled (""css."" for short) ImageNet-V2 accuracy against ImageNet accuracy and YFCC accuracy, respectively, for 36 ImageNet models and 13 YFCC models that are also used']",OOD accuracy is measured in the context of evaluating effective robustness in machine learning models by comparing the actual OOD accuracy of models beyond the expected value predicted from fitting a line. This evaluation aims to eliminate the confounding effect of ID accuracy on OOD performance.,simple,"[{'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0', 'pdf_path': './corpus/2302.01381.pdf'}]",True
What is the role of ImageNet-R as the OOD test set in the evaluation process?,"[' (see Section 5.1).\n\n(b) Using ImageNet-R as the OOD test set.\n\nFigure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.\n\nRetain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models.\n\n### 5.2 Evaluation on CIFAR-like OOD Test Sets\n\n| |CIFAR-10|ImageNet|\n|---|---|---|\n|95|CIFAR-10|ImageNet|\n|90|CIFAR-10+ImageNet|CIFAR-10+ImageNet|\n|80| | |\n|70| | |\n|50| | |\n|25| | |\n|CIFAR-10.2 accuracy (%)| | |\n\n(a) CIFAR-10.2 accuracy against CIFAR-10 accuracy. ImageNet models have higher CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for CIFAR-10 accuracy only.\n\n(b) CIFAR-10.2 accuracy against ImageNet accuracy. ImageNet models have lower CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for ImageNet accuracy only.\n\nFigure 4: Projected views of Figure 3a. Figure 4a and Figure 4b correspond to single-ID evaluations using different ID test sets and yield contradictory conclusions on the effective robustness. Our multi-ID evaluation provides a more holistic view where all these models are approximately on a same plane and thus have similar effective robustness.\n\nWe first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets. We show the fitting quality in Table 1a and the effective robustness of various models in Table 1b.\n\nCompared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE), and thus provides a more precise understanding on the effective robustness. Specifically, while both single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms of the effective robustness, under the single-ID evaluation, ImageNet models achieve $$3.91\\pm2.20\\%$$ and $$2.77\\pm1.25\\%$$ effective robustness on CIFAR-10.2 and CINIC-10, respectively.\n\n# Results on CIFAR-like OOD test sets\n\n## Results on CIFAR-like OOD test sets\n\n148 models are included, including CIFAR-10 models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models and ImageNet models also become closer to 0.\n\n### (a) Fitting quality evaluated by R2 and mean absolute error (MAE)\n\n|Test set|R2 (↑)|MAE (% ↓)|\n|---|---|---|\n|CIFAR-10.1|0.996|0.997|1.07|0.93|\n|CIFAR-10.2|0.981|0.996|2.22|0.95|\n|CINIC-10|0.978|0.990|2.41|1.49|\n\n### (b) Effective robustness values (%)\n\nWe report the mean and standard deviation for three groups of models with different training data, respectively.\n\n|Test set|Evaluation|CIFAR-10|ImageNet|CIFAR+IN|\n|---|---|---|---|---|\n|CIFAR-10.1|Single-ID|-1.68±0.92|1.05±1.27|0.02±1.10|\n|CIFAR-10.1|Multi-ID|-1.43±0.92|0.10±1.12|0.19±1.01|\n|CIFAR-10.2|Single-ID|-1.']",ImageNet-R serves as the OOD test set in the evaluation process.,simple,"[{'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0', 'pdf_path': './corpus/2302.01381.pdf'}]",True
