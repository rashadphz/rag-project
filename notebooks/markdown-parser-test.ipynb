{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# So it can see the scripts folder\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashad/Library/Caches/pypoetry/virtualenvs/rag-project-RXD1v8WZ-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from scripts.constants import GPT3_MODEL\n",
    "from scripts.parse_json import PDFJson\n",
    "from llama_index.core import Document\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = MarkdownElementNodeParser(\n",
    "    llm=OpenAI(model=GPT3_MODEL), num_workers=8\n",
    ")\n",
    "pdf_json = PDFJson.from_json_file(\"../parse-output/2302.01381.json\")\n",
    "document = Document(metadata={\"pdf_path\": pdf_json.file_path, \"pdf_id\": pdf_json.job_id}, text=pdf_json.full_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 37718.56it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='29c4684f-4fee-4e11-8d86-32847f1a9a08', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cd7e92c1-330d-43bc-af36-0c9e26a2ec76', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='20b357706ccba394bb1a1199bf2f75fd36aafd245d0c7dadd0fe2e1e68c2199b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='87e48e4d-fe81-4a00-b441-5d70584bbfbc', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='a1c1425721076dfc42ba055a4bd9f2632f99b27236bc562f5517754c7f383b5e')}, text='Effective Robustness against Natural Distribution Shifts\\n\\n Effective Robustness against Natural Distribution Shifts for Models with Different Training Data\\n\\nZhouxing Shi* &emsp; Nicholas Carlini &emsp; Ananth Balashankar\\n\\nUCLA &emsp; Google Research &emsp; Google Research\\n\\nzshi@cs.ucla.edu &emsp; ncarlini@google.com &emsp; ananthbshankar@google.com\\n\\nLudwig Schmidt &emsp; Cho-Jui Hsieh\\n\\nUniversity of Washington &emsp; Google, UCLA\\n\\nschmidt@cs.washington.edu &emsp; chohsieh@cs.ucla.edu\\n\\nAlex Beutel* &emsp; Yao Qin\\n\\nOpenAI &emsp; UCSB, Google Research\\n\\nalexb@openai.com &emsp; yaoqin@ucsb.edu\\n\\n Abstract\\n\\n\"Effective robustness\" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageNet as the only ID test set, while the gains diminish under our new evaluation. Additional artifacts including interactive visualizations are provided at https://shizhouxing.github.io/effective-robustness.\\n\\n 1 Introduction\\n\\nRobustness against distribution shifts is important for machine learning models to work reliably across various environments. For natural distribution shifts on image classification datasets, Taori et al. (2020) proposed the notion of effective robustness to control for in-distribution (ID) accuracy when evaluating out-of-distribution (OOD) accuracy. Following a long line of work that has found a strong correlation between ID and OOD accuracy on many test sets (Recht et al., 2019; Yadav & Bottou, 2019), effective robustness allows researchers to assess whether an apparently improved OOD accuracy is a result of effectively improved robustness or is simply an expected outcome of enhanced ID accuracy.\\n\\nUnfortunately, the current definition of effective robustness has a subtle limitation: it requires a fixed ID test set, which is typically ImageNet (Deng et al., 2009) when using ImageNet-like OOD test sets. *Work done while at Google.\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\n\\nin Taori et al. (2020) or CIFAR-10 (Krizhevsky et al., 2009) when using CIFAR-like OOD test sets\\nin Miller et al. (2021). It is acceptable when models are trained predominately on only one dataset.\\nHowever, the emergence of many large-scale models trained on significantly different datasets makes\\nit necessary to evaluate and compare models trained on different data distributions, under which it\\nbecomes unclear which ID test set should be used.\\nIn particular, models from Contrastive Language-Image Pre-training, such as CLIP (Radford et al.,\\n2021) and ALIGN (Jia et al., 2021) have recently exhibited unprecedented effective robustness gains\\nduring zero-shot inference (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022). However\\nthese previous works simply take ImageNet as the single ID test set, even though the models are not\\ntrained on ImageNet. We demonstrate that the results of evaluating effective robustness using a single\\nID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise\\ntreatment on the ID test set in existing works could end up exaggerating the effective robustness of\\nzero-shot CLIP models compared to models that are exactly trained on ImageNet.\\nIn this paper, we propose to more precisely evaluate and compare the effective robustness of models\\ntrained on different datasets. Instead of controlling for a single ID accuracy that may bias towards\\nmodels from a particular training distribution, we propose to use multiple ID test sets that cover the\\ntraining distributions of all the models. In particular, previous works performed single-dimensional\\nlinear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\\net al., 2020).', start_char_idx=1, end_char_idx=4608, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='87e48e4d-fe81-4a00-b441-5d70584bbfbc', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cd7e92c1-330d-43bc-af36-0c9e26a2ec76', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='20b357706ccba394bb1a1199bf2f75fd36aafd245d0c7dadd0fe2e1e68c2199b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='29c4684f-4fee-4e11-8d86-32847f1a9a08', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='91b0ce454c9e75de87c458b56656a87e4508ec70e2ea2ae86d6c190a0a852594'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_14_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: 80\\nType: ImageNet models\\nSummary: None\\n\\nColumn: YFCC models\\nType: YFCC models\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='1f8ebbbc96476ce68d5202126f3079eb3deac2d989f24992e78c81078a007c9c')}, text='We demonstrate that the results of evaluating effective robustness using a single\\nID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise\\ntreatment on the ID test set in existing works could end up exaggerating the effective robustness of\\nzero-shot CLIP models compared to models that are exactly trained on ImageNet.\\nIn this paper, we propose to more precisely evaluate and compare the effective robustness of models\\ntrained on different datasets. Instead of controlling for a single ID accuracy that may bias towards\\nmodels from a particular training distribution, we propose to use multiple ID test sets that cover the\\ntraining distributions of all the models. In particular, previous works performed single-dimensional\\nlinear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\\net al., 2020). And they then evaluate the actual OOD accuracy of the models beyond the expected\\nvalue that can be predicted from the fitting line, as the effective robustness. We expand on this\\ndefinition by allowing for multiple ID test sets, and perform multi-dimensional linear regression to fit\\na plane to predict OOD accuracy from the accuracy on multiple ID test sets.\\n\\nIn summary, we make the following contributions:\\n- We reveal a limitation in the existing effective robustness evaluation when used to compare models\\ntrained on different data distributions.\\n- We then propose a new effective robustness evaluation which uses multiple ID test sets to more\\nprecisely compare the effective robustness of models trained on different data.\\n- We show that the OOD accuracy of various models including zero-shot CLIP models can usually\\nbe better predicted from accuracies on multiple ID test sets compared to using only one ID test set.\\n- Our results provide new understandings on the effective robustness gains of CLIP-like models\\nobserved in prior works only using ImageNet as the ID test set, while the gains diminish under our\\nnew evaluation.\\n\\n Background of Effective Robustness\\n\\nUnder natural distribution shifts, the OOD accuracy of a model is often correlated with the ID\\naccuracy. After applying a logit transformation on the accuracy, a linear trend between the transformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from\\nImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky\\net al., 2009) to CIFAR-10.2 (Hendrycks & Dietterich, 2018)) and models with various architectures\\nand training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most\\nmodels showing higher OOD accuracies naturally resulted from better ID performance.\\n\\nTo eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed\\neffective robustness that measures the OOD performance beyond the expected OOD accuracy given\\nthe ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend\\nof baseline models. Since they only use a single ID test set, we refer to this version of effective\\nrobustness as single-ID effective robustness.\\n\\nSuppose there are n baseline models \\\\( f_1, f_2, \\\\ldots, f_n \\\\). A baseline function \\\\( \\\\tilde{\\\\beta}(x) \\\\) is constructed to predict\\nthe OOD accuracy of each baseline model, \\\\( acc_{ood}(f_i) \\\\) (\\\\( 1 \\\\leq i \\\\leq n \\\\)), given the single ID accuracy of\\nthe model \\\\( x = acc_{id}(f_i) \\\\). The baseline function is instantiated as:\\n\\n$$\\n\\\\tilde{\\\\beta}(x) = \\\\text{expit}(w \\\\text{logit}(x) + b) \\\\quad (1)\\n$$\\n\\nwhere \\\\( w \\\\) and \\\\( b \\\\) are parameters, \\\\( \\\\text{logit}(x) = \\\\ln\\\\left(\\\\frac{1}{1-x}\\\\right) \\\\) is the logit transformation, and \\\\( \\\\text{expit}(x) \\\\) is the\\ninverse of \\\\( \\\\text{logit}(x) \\\\). Since \\\\( \\\\text{logit}(\\\\tilde{\\\\beta}(x)) = w \\\\text{logit}(x) + b \\\\), the baseline function is essentially a linear\\n\\n Document\\n\\n ImageNet models', start_char_idx=3718, end_char_idx=7621, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_14_table_ref', embedding=None, metadata={'col_schema': 'Column: 80\\nType: ImageNet models\\nSummary: None\\n\\nColumn: YFCC models\\nType: YFCC models\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='87e48e4d-fe81-4a00-b441-5d70584bbfbc', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='a1c1425721076dfc42ba055a4bd9f2632f99b27236bc562f5517754c7f383b5e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_14_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{' ': {0: '95', 1: ' '}, '80': {0: 'YFCC models', 1: '70'}, 'ImageNet models': {0: 'YFCC models', 1: ' '}}\", 'table_summary': 'This table compares ImageNet models with YFCC models.,\\nwith the following table title:\\nImageNet models vs YFCC models,\\nwith the following columns:\\n- 80: None\\n- YFCC models: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='00876e125769708adb16e77e2764ddbd680afb662217071bba6ff76b234b6f5c')}, text='This table compares ImageNet models with YFCC models.,\\nwith the following table title:\\nImageNet models vs YFCC models,\\nwith the following columns:\\n- 80: None\\n- YFCC models: None\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_14_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_14_table', embedding=None, metadata={'table_df': \"{' ': {0: '95', 1: ' '}, '80': {0: 'YFCC models', 1: '70'}, 'ImageNet models': {0: 'YFCC models', 1: ' '}}\", 'table_summary': 'This table compares ImageNet models with YFCC models.,\\nwith the following table title:\\nImageNet models vs YFCC models,\\nwith the following columns:\\n- 80: None\\n- YFCC models: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_14_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: 80\\nType: ImageNet models\\nSummary: None\\n\\nColumn: YFCC models\\nType: YFCC models\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='1f8ebbbc96476ce68d5202126f3079eb3deac2d989f24992e78c81078a007c9c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4135f811-73c2-4b56-a2fa-6b251ea3e2b5', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='4a5eaef060b6306a1c73d65c993b8dcd96cdb058252902c5e32b1be4d16f8ce5')}, text='This table compares ImageNet models with YFCC models.,\\nwith the following table title:\\nImageNet models vs YFCC models,\\nwith the following columns:\\n- 80: None\\n- YFCC models: None\\n\\n| |80|ImageNet models|\\n|---|---|---|\\n|95|YFCC models|YFCC models|\\n| |70| |\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='4135f811-73c2-4b56-a2fa-6b251ea3e2b5', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='591665ea-74cc-472c-976e-499f1e4be60c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='dfc9e7ca5cbf052b970aab13024b933de70bae18e669c820cb618b0704773d3e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='559437c3-70f5-4e0f-9958-d6dea3952c59', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='579f63b783d410e50b6c676a04a1e77ee95f2f882b2abe78d72965f24517b457'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_14_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{' ': {0: '95', 1: ' '}, '80': {0: 'YFCC models', 1: '70'}, 'ImageNet models': {0: 'YFCC models', 1: ' '}}\", 'table_summary': 'This table compares ImageNet models with YFCC models.,\\nwith the following table title:\\nImageNet models vs YFCC models,\\nwith the following columns:\\n- 80: None\\n- YFCC models: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='00876e125769708adb16e77e2764ddbd680afb662217071bba6ff76b234b6f5c')}, text='Linear fit for ImageNet models\\n\\n80\\n\\n70\\n\\n50\\n\\n25\\n\\n10\\n\\n Linear fit for YFCC models\\n\\n50\\n\\n25\\n\\n10\\n\\n ImageNet-V2 accuracy (css., %)\\n\\n10 25 50 70 80 90 95\\n\\n ImageNet accuracy (css., %)\\n\\nYFCC accuracy (css., %)\\n\\n (a) ImageNet-V2 accuracy against ImageNet accuracy.\\n\\n (b) ImageNet-V2 accuracy against YFCC accuracy.\\n\\nFigure 1: Class-subsampled (\"css.\" for short) ImageNet-V2 accuracy against ImageNet accuracy and YFCC accuracy, respectively, for 36 ImageNet models and 13 YFCC models that are also used in Table 3a. A linear fit is generated for ImageNet models and YFCC-15M models, respectively. Accuracies and linear fits are under the logit scale. Class-subsampling is used to only include classes that appear in all the involved test sets (see Section 5.1).\\n\\nFunction after applying a logit transformation on the accuracies, and it can be determined by solving a linear regression. Then the single-ID effective robustness of a model f is evaluated as:\\n\\n$$\\\\tilde{\\\\rho}(f) = acc_{ood}(f) - \\\\beta\\\\tilde{}(acc_{id}(f))$$\\n\\nfrom the actual OOD accuracy $acc_{ood}(f)$.\\n\\n Limitation of the Single ID Test Set\\n\\nThe existing effective robustness evaluation in Section 2 fixes a single ID test set for all the models, which is reflective of the ID performance only if all the models are trained on the same dataset that matches the ID test set. However, as large-scale pre-trained models emerge, it becomes necessary to compare models trained on different datasets, in order to know if the latest pre-training techniques can yield effective robustness gains. In this section, we use the comparison between zero-shot CLIP models and standard ImageNet models as an example to show the limitation of using a single ID test set: when only one ID test set is used, using different ID test sets leads to contradictory conclusions. Following Fang et al. (2022), we compare models trained on ImageNet (Deng et al., 2009) and YFCC-15M (Thomee et al., 2016), respectively. On ImageNet, we include standard classifiers, and we also train CLIP models using templates filled with an ImageNet class name as the caption in a format of \"A photo of a {class name}\". We also train CLIP models on YFCC-15M, a dataset with image-text pairs. And we use ImageNet-V2 (Recht et al., 2019) as the OOD test set. We consider two different ID test sets. One ID test set is simply ImageNet. The other ID test set is constructed from YFCC-15M, since we have CLIP models trained on YFCC. We refer to this test set as \"YFCC test set\", and we refer to the accuracy on this test set as \"YFCC accuracy\". We discuss its details in Section 5.1 and Appendix B.2. Both ID test sets we consider here match the training distribution of some of the models (ImageNet models and YFCC models respectively) but not all the models.\\n\\nWe then plot the ImageNet-V2 accuracy of the models against their ImageNet accuracy and YFCC accuracy, respectively. There is a strong linear trend between the scaled ID accuracy and OOD accuracy for ImageNet models and YFCC models, respectively, and we plot fitting lines for these two sets of models, respectively. When the ID test set is ImageNet, Fig. 1a shows that the fitting line for YFCC models is generally above the fitting line for ImageNet models (except for the regime with extremely low accuracies), which appears to suggest that YFCC models have effective robustness gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 1b which uses YFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models, which instead appears to suggest that ImageNet models have greater effective robustness than YFCC models. We observe that when there is a mismatch in the training data and the ID test data.\\n\\nmodels appear to have greater effective robustness (YFCC models in Figure 1a and ImageNet models\\nin Figure 1b), as their performance on the ID test data and the OOD performance predicted from the\\nsingle ID accuracy tend to be lower.', start_char_idx=2, end_char_idx=3973, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='559437c3-70f5-4e0f-9958-d6dea3952c59', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='591665ea-74cc-472c-976e-499f1e4be60c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='dfc9e7ca5cbf052b970aab13024b933de70bae18e669c820cb618b0704773d3e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4135f811-73c2-4b56-a2fa-6b251ea3e2b5', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='4a5eaef060b6306a1c73d65c993b8dcd96cdb058252902c5e32b1be4d16f8ce5'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c801165b-e055-4ac2-9c0a-6c7a409e68e9', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='d18fdcbac197039934661985782be64c5adcc650947a64ebcd1344d73292fa4d')}, text='1a shows that the fitting line for YFCC models is generally above the fitting line for ImageNet models (except for the regime with extremely low accuracies), which appears to suggest that YFCC models have effective robustness gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 1b which uses YFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models, which instead appears to suggest that ImageNet models have greater effective robustness than YFCC models. We observe that when there is a mismatch in the training data and the ID test data.\\n\\nmodels appear to have greater effective robustness (YFCC models in Figure 1a and ImageNet models\\nin Figure 1b), as their performance on the ID test data and the OOD performance predicted from the\\nsingle ID accuracy tend to be lower. This makes it difficult to compare models trained on different\\ndata and leads to imprecise conclusions on effective robustness if only one ID test set is used.\\n\\n Multi-ID Effective Robustness\\n\\nConsidering the limitations of using a single ID test set, we propose a new way for effective robustness\\nevaluation using multiple ID test sets that cover the training data distributions of all the involved\\nmodels. We name it multi-ID effective robustness. Specifically, for each training distribution, we\\npropose to prepare an ID test set that matches the training distribution, respectively. In particular, we\\nfocus on comparing models trained on two different datasets at a time in this paper, and we thereby\\nuse two ID test sets, where each of them corresponds to one of the training datasets.\\n\\nWhile we refer to them as ID test sets, each of them is only the exact ID test set for some of the\\nconsidered models that are trained on the distribution matching the test set, and it is not exactly an\\nID test set for all the considered models. However, we assume that the training distributions of all\\nthe models are still relatively close compared to the OOD test distributions (e.g., images normally\\ncollected from social medias in ImageNet (Deng et al., 2009), YFCC (Thomee et al., 2016), and\\nLAION (Schuhmann et al., 2021) are relatively close compared to the OOD images in ImageNet-\\nSketch (Wang et al., 2019) that consists of sketch images specifically). In this way, both ID test sets\\nare relatively ID for all the models compared to the OOD test sets, and it can be meaningful to control\\nfor the performance on these ID test sets when comparing the OOD performance.\\n\\nWe still use $$acc_{ood}(·)$$ to denote the OOD accuracy, and we use $$acc_1(·)$$ and $$acc_2(·)$$ to denote the\\naccuracy on the two ID test sets, respectively. In contrast to the previous baseline function $$\\\\tilde{\\\\beta}(x)$$ in\\nEq. (1), we propose a new baseline function $$\\\\beta(x, y)$$ that predicts the OOD accuracy based on the\\naccuracies x and y on the two ID test sets, respectively.\\n\\nAll the models in Figure 1 are trained on either ImageNet or YFCC. Thus, to compare their effective ImageNet Models YFCC Models\\nrobustness under our new evaluation, we use two ID\\ntest sets for ImageNet and YFCC at the same time, in 1\\ncontrast to Figure 1a and 1b which use one ID test set\\nseparately at each time and results on the two different\\nID test sets lead to contradictory conclusions. As\\nshown in Figure 2, we plot the OOD accuracy against\\nthe two ID accuracies on both two ID test sets in a 3D\\nspace. We observe that the data points approximately\\nlie on a plane when plotted on the logit scale. This motivates us to instantiate $$\\\\beta(x, y)$$ as:\\n\\n$$\\\\beta(x, y) = expit(wx logit(x) + wy logit(y) + b),$$\\n\\nwhere wx, wy, b are parameters. $$\\\\beta(x, y)$$, which is the plane in Figure 2, is also a linear function w.r.t. x and\\ny under the logit scale, and thus it is a reasonable extension from $$\\\\tilde{\\\\beta}(x)$$ by using a multi-dimensional linear function on the logit scale. We determine the parameters by solving an ordinary least squares regression to fit the accuracies. Metrics for linear regression\\nsuch as the coefficient of determination, a.k.a. R2,\\ncan be used to evaluate the fitting quality of the baseline function.', start_char_idx=3129, end_char_idx=7277, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='c801165b-e055-4ac2-9c0a-6c7a409e68e9', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='591665ea-74cc-472c-976e-499f1e4be60c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='dfc9e7ca5cbf052b970aab13024b933de70bae18e669c820cb618b0704773d3e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='559437c3-70f5-4e0f-9958-d6dea3952c59', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='579f63b783d410e50b6c676a04a1e77ee95f2f882b2abe78d72965f24517b457'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='79a5cfcd-4eed-4681-bd81-2128d8870821', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='f03cca6b24596d06c718d4eaadfa9190d3c1e2ce977643df433fea13bf9b97a3')}, text='We observe that the data points approximately\\nlie on a plane when plotted on the logit scale. This motivates us to instantiate $$\\\\beta(x, y)$$ as:\\n\\n$$\\\\beta(x, y) = expit(wx logit(x) + wy logit(y) + b),$$\\n\\nwhere wx, wy, b are parameters. $$\\\\beta(x, y)$$, which is the plane in Figure 2, is also a linear function w.r.t. x and\\ny under the logit scale, and thus it is a reasonable extension from $$\\\\tilde{\\\\beta}(x)$$ by using a multi-dimensional linear function on the logit scale. We determine the parameters by solving an ordinary least squares regression to fit the accuracies. Metrics for linear regression\\nsuch as the coefficient of determination, a.k.a. R2,\\ncan be used to evaluate the fitting quality of the baseline function. A high R2 value indicates that\\nthe OOD accuracy is accurately predicted by the baseline function from the ID accuracies, and thus\\nthe evaluated models have similar effective robustness. And our multi-ID effective robustness for a\\nmodel f is defined as\\n\\n$$\\\\rho(f) = acc_{ood}(f) - \\\\beta(acc_1(f), acc_2(f)).$$\\n\\nCompared to the existing definition for effective robustness in Eq. (2), the major difference is the\\ninclusion of two ID accuracies $$acc_1(f)$$ and $$acc_2(f)$$ in the baseline function, compared to using a\\nsingle ID accuracy $$acc_{id}(f)$$.\\n\\n Generalizing to more than two training datasets\\n\\n Generalizing to more than two training datasets\\n\\nAlthough we focus on handling two training datasets at a time, our method may be generalized to more than two datasets in principle, by defining a baseline function based on multiple ID accuracies $acc_1(\\\\cdot), \\\\ldots, acc_k(\\\\cdot)$. However, it could be costly as it would require training more models to fit a high-quality baseline function. We leave it for future works to reduce the cost when dealing with a larger number of datasets.\\n\\n Experiments\\n\\n Settings\\n\\nModels: In order to fit a baseline function, we need a large amount of models with diverse accuracies. To this end, we follow Taori et al. (2020) to train models with various proportions of data by subsampling from the entire training set (namely dataset subsampling), which effectively produces models with diverse accuracies. Moreover, we also combine examples from two datasets with different sampling ratios and train models on these combined datasets. This produces models with training distributions varying between the two training datasets and it is supposed to yield different combinations of the two ID accuracies. We use models trained on each single dataset as well as the combined datasets in the same fitting process, so that the baseline functions do not bias towards models trained on certain data. Our experiments include the following models:\\n\\n- Standard classifiers on CIFAR-10 and ImageNet. We train standard classifiers on CIFAR-10 and ImageNet (Deng et al., 2009). We use ResNet-18, ResNet-50, and ResNet-101 (He et al., 2016). Additionally, we train models by combining CIFAR-10 and ImageNet at various ratios, where we upsample CIFAR-10 images from 32x32 to 224x224. Furthermore, we include ViT-S/16, ViT-B/16, ViT-L/16 models (Dosovitskiy et al., 2021) pre-trained on the whole ImageNet.\\n- CLIP models. On YFCC-15M (Thomee et al., 2016) and LAION-15M (Schuhmann et al., 2021) which consist of image-text pairs, we train CLIP models using ResNet-50 and ResNet-101. We also train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide additional details in Appendix B.1.\\n\\nWe use \"{Name_of_dataset} models\" to denote models trained only on the dataset, e.g., \"CIFAR-10 models\". And we use \"{Name_of_dataset_A}+{Name_of_dataset_B} models\" to represent models trained on a combination of two datasets, e.g., \"CIFAR-10+ImageNet models\".', start_char_idx=6547, end_char_idx=10563, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='79a5cfcd-4eed-4681-bd81-2128d8870821', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='591665ea-74cc-472c-976e-499f1e4be60c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='dfc9e7ca5cbf052b970aab13024b933de70bae18e669c820cb618b0704773d3e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c801165b-e055-4ac2-9c0a-6c7a409e68e9', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='d18fdcbac197039934661985782be64c5adcc650947a64ebcd1344d73292fa4d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_56_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: CIFAR-10\\nType: string\\nSummary: Accuracy percentages for CIFAR-10 dataset\\n\\nColumn: ImageNet\\nType: string\\nSummary: Accuracy percentages for ImageNet dataset', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='1c0d9d9932f3077c6cb563825a1ed78ff1e8fb0c133f50b2b4a1964c615a2b08')}, text='We also train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide additional details in Appendix B.1.\\n\\nWe use \"{Name_of_dataset} models\" to denote models trained only on the dataset, e.g., \"CIFAR-10 models\". And we use \"{Name_of_dataset_A}+{Name_of_dataset_B} models\" to represent models trained on a combination of two datasets, e.g., \"CIFAR-10+ImageNet models\".\\n\\nID test sets: We focus on image classification. Labeled image classification datasets such as ImageNet can be directly used for evaluating ID accuracy. For datasets that consist of image-text pairs for language-image pre-training without original labels, including YFCC and LAION, we automatically generate classification labels by matching captions with ImageNet classes, which has been similarly performed in Fang et al. (2022) for training classifiers using caption data, and we then hold out a balanced test set from the original dataset. More details are reported in Appendix B.2. Although it is possible to obtain a higher-quality test set by human labeling, we will show that using the automatically labeled test sets can already produce reasonable results.\\n\\nOOD test sets: To compare the effectiveness robustness of models trained on CIFAR-10 and ImageNet, we use 3 CIFAR-like OOD test sets with natural distribution shifts, including CIFAR-10.1 (Recht et al., 2019), CIFAR-10.2 (Lu et al., 2020), and CINIC-10 (Darlow et al., 2018). We use 4 ImageNet-like OOD test sets to compare models trained on ImageNet with models trained on YFCC and LAION: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019). We do not use ImageNet-A (Hendrycks et al., 2021b) which involves adversarial filtering and has a different behavior in effective robustness evaluation (Taori et al., 2020; Fang et al., 2022).\\n\\nClass subsampling and mapping: Considering that different test sets may not have the same classes, we follow prior works (Taori et al., 2020; Fang et al., 2022) to use class subsampling to\\n\\nWe reuse the term \"class subsampling\" from prior works (Taori et al., 2020; Fang et al., 2022), although it is not a random sampling.\\n\\n Document\\n\\n CIFAR-10 Models\\n\\n ImageNet Models\\n\\n CIFAR-10+ImageNet Models\\n\\n ImageNet Models\\n\\n YFCC Models\\n\\n ImageNet+YFCC Models\\n\\n1. Cipa4-I0\\n\\n(a) Using CIFAR-10.2 as the OOD test set. The ImageNet accuracy is mapped to CIFAR-10 classes (see Section 5.1).\\n\\n(b) Using ImageNet-R as the OOD test set.\\n\\nFigure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.\\n\\nRetain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models.\\n\\n 5.2 Evaluation on CIFAR-like OOD Test Sets', start_char_idx=9888, end_char_idx=13210, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_56_table_ref', embedding=None, metadata={'col_schema': 'Column: CIFAR-10\\nType: string\\nSummary: Accuracy percentages for CIFAR-10 dataset\\n\\nColumn: ImageNet\\nType: string\\nSummary: Accuracy percentages for ImageNet dataset', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='79a5cfcd-4eed-4681-bd81-2128d8870821', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='f03cca6b24596d06c718d4eaadfa9190d3c1e2ce977643df433fea13bf9b97a3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_56_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{' ': {0: '95', 1: '90', 2: '80', 3: '70', 4: '50', 5: '25', 6: 'CIFAR-10.2 accuracy (%)'}, 'CIFAR-10': {0: 'CIFAR-10', 1: 'CIFAR-10+ImageNet', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' '}, 'ImageNet': {0: 'ImageNet', 1: 'CIFAR-10+ImageNet', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' '}}\", 'table_summary': 'This table compares accuracy percentages for CIFAR-10 and ImageNet datasets.,\\nwith the following table title:\\nAccuracy Comparison of CIFAR-10 and ImageNet Datasets,\\nwith the following columns:\\n- CIFAR-10: Accuracy percentages for CIFAR-10 dataset\\n- ImageNet: Accuracy percentages for ImageNet dataset\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='aa8901da2e1c0945a56156ffdbeb63c950ea85db2b95a6c1e28b0ac048fce6e3')}, text='This table compares accuracy percentages for CIFAR-10 and ImageNet datasets.,\\nwith the following table title:\\nAccuracy Comparison of CIFAR-10 and ImageNet Datasets,\\nwith the following columns:\\n- CIFAR-10: Accuracy percentages for CIFAR-10 dataset\\n- ImageNet: Accuracy percentages for ImageNet dataset\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_56_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_56_table', embedding=None, metadata={'table_df': \"{' ': {0: '95', 1: '90', 2: '80', 3: '70', 4: '50', 5: '25', 6: 'CIFAR-10.2 accuracy (%)'}, 'CIFAR-10': {0: 'CIFAR-10', 1: 'CIFAR-10+ImageNet', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' '}, 'ImageNet': {0: 'ImageNet', 1: 'CIFAR-10+ImageNet', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' '}}\", 'table_summary': 'This table compares accuracy percentages for CIFAR-10 and ImageNet datasets.,\\nwith the following table title:\\nAccuracy Comparison of CIFAR-10 and ImageNet Datasets,\\nwith the following columns:\\n- CIFAR-10: Accuracy percentages for CIFAR-10 dataset\\n- ImageNet: Accuracy percentages for ImageNet dataset\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_56_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: CIFAR-10\\nType: string\\nSummary: Accuracy percentages for CIFAR-10 dataset\\n\\nColumn: ImageNet\\nType: string\\nSummary: Accuracy percentages for ImageNet dataset', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='1c0d9d9932f3077c6cb563825a1ed78ff1e8fb0c133f50b2b4a1964c615a2b08'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6efadafc-4893-4a0b-85c9-c96bd1972bb8', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='91304257a1ca5e81572805107785acbd55063b9d7f12dca01308f55a746a4aaa')}, text='This table compares accuracy percentages for CIFAR-10 and ImageNet datasets.,\\nwith the following table title:\\nAccuracy Comparison of CIFAR-10 and ImageNet Datasets,\\nwith the following columns:\\n- CIFAR-10: Accuracy percentages for CIFAR-10 dataset\\n- ImageNet: Accuracy percentages for ImageNet dataset\\n\\n| |CIFAR-10|ImageNet|\\n|---|---|---|\\n|95|CIFAR-10|ImageNet|\\n|90|CIFAR-10+ImageNet|CIFAR-10+ImageNet|\\n|80| | |\\n|70| | |\\n|50| | |\\n|25| | |\\n|CIFAR-10.2 accuracy (%)| | |\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='6efadafc-4893-4a0b-85c9-c96bd1972bb8', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ea4c8f81-9f7f-4ac8-9f5b-6616dc009bc4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='c9d4bf73cf541e670d6987cd062fd947c7f0a68590f5ecd18dfcae96c85263c3'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_56_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{' ': {0: '95', 1: '90', 2: '80', 3: '70', 4: '50', 5: '25', 6: 'CIFAR-10.2 accuracy (%)'}, 'CIFAR-10': {0: 'CIFAR-10', 1: 'CIFAR-10+ImageNet', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' '}, 'ImageNet': {0: 'ImageNet', 1: 'CIFAR-10+ImageNet', 2: ' ', 3: ' ', 4: ' ', 5: ' ', 6: ' '}}\", 'table_summary': 'This table compares accuracy percentages for CIFAR-10 and ImageNet datasets.,\\nwith the following table title:\\nAccuracy Comparison of CIFAR-10 and ImageNet Datasets,\\nwith the following columns:\\n- CIFAR-10: Accuracy percentages for CIFAR-10 dataset\\n- ImageNet: Accuracy percentages for ImageNet dataset\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='aa8901da2e1c0945a56156ffdbeb63c950ea85db2b95a6c1e28b0ac048fce6e3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_64_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: Name of the test set\\n\\nColumn: R2 (↑)\\nType: float\\nSummary: R-squared metric indicating the goodness of fit of the model (higher is better)\\n\\nColumn: MAE (% ↓)\\nType: float\\nSummary: Mean Absolute Error percentage (lower is better)', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='9c77bc0db7f32948e8ee7c7b4bfad91f80d21c58b39b860326cc5bc0fd086cd9')}, text='(a) CIFAR-10.2 accuracy against CIFAR-10 accuracy. ImageNet models have higher CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for CIFAR-10 accuracy only.\\n\\n(b) CIFAR-10.2 accuracy against ImageNet accuracy. ImageNet models have lower CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for ImageNet accuracy only.\\n\\nFigure 4: Projected views of Figure 3a. Figure 4a and Figure 4b correspond to single-ID evaluations using different ID test sets and yield contradictory conclusions on the effective robustness. Our multi-ID evaluation provides a more holistic view where all these models are approximately on a same plane and thus have similar effective robustness.\\n\\nWe first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets. We show the fitting quality in Table 1a and the effective robustness of various models in Table 1b.\\n\\nCompared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE), and thus provides a more precise understanding on the effective robustness. Specifically, while both single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms of the effective robustness, under the single-ID evaluation, ImageNet models achieve $$3.91\\\\pm2.20\\\\%$$ and $$2.77\\\\pm1.25\\\\%$$ effective robustness on CIFAR-10.2 and CINIC-10, respectively.\\n\\n Results on CIFAR-like OOD test sets\\n\\n Results on CIFAR-like OOD test sets\\n\\n148 models are included, including CIFAR-10 models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models and ImageNet models also become closer to 0.\\n\\n (a) Fitting quality evaluated by R2 and mean absolute error (MAE)', start_char_idx=1, end_char_idx=2006, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_64_table_ref', embedding=None, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: Name of the test set\\n\\nColumn: R2 (↑)\\nType: float\\nSummary: R-squared metric indicating the goodness of fit of the model (higher is better)\\n\\nColumn: MAE (% ↓)\\nType: float\\nSummary: Mean Absolute Error percentage (lower is better)', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6efadafc-4893-4a0b-85c9-c96bd1972bb8', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='91304257a1ca5e81572805107785acbd55063b9d7f12dca01308f55a746a4aaa'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_64_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': '|Test set|R2 (↑)|MAE (% ↓)|\\n|---|---|---|\\n|CIFAR-10.1|0.996|0.997|1.07|0.93|\\n|CIFAR-10.2|0.981|0.996|2.22|0.95|\\n|CINIC-10|0.978|0.990|2.41|1.49|', 'table_summary': 'Comparison of performance metrics R2 and MAE for different test sets including CIFAR-10.1, CIFAR-10.2, and CINIC-10.,\\nwith the following table title:\\nPerformance Metrics Comparison for Different Test Sets,\\nwith the following columns:\\n- Test set: Name of the test set\\n- R2 (↑): R-squared metric indicating the goodness of fit of the model (higher is better)\\n- MAE (% ↓): Mean Absolute Error percentage (lower is better)\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='84f5e5587b205a31e92f311859ba49394dd3def6843b2eb5bcc9e049c5746712')}, text='Comparison of performance metrics R2 and MAE for different test sets including CIFAR-10.1, CIFAR-10.2, and CINIC-10.,\\nwith the following table title:\\nPerformance Metrics Comparison for Different Test Sets,\\nwith the following columns:\\n- Test set: Name of the test set\\n- R2 (↑): R-squared metric indicating the goodness of fit of the model (higher is better)\\n- MAE (% ↓): Mean Absolute Error percentage (lower is better)\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_64_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_64_table', embedding=None, metadata={'table_df': '|Test set|R2 (↑)|MAE (% ↓)|\\n|---|---|---|\\n|CIFAR-10.1|0.996|0.997|1.07|0.93|\\n|CIFAR-10.2|0.981|0.996|2.22|0.95|\\n|CINIC-10|0.978|0.990|2.41|1.49|', 'table_summary': 'Comparison of performance metrics R2 and MAE for different test sets including CIFAR-10.1, CIFAR-10.2, and CINIC-10.,\\nwith the following table title:\\nPerformance Metrics Comparison for Different Test Sets,\\nwith the following columns:\\n- Test set: Name of the test set\\n- R2 (↑): R-squared metric indicating the goodness of fit of the model (higher is better)\\n- MAE (% ↓): Mean Absolute Error percentage (lower is better)\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_64_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: Name of the test set\\n\\nColumn: R2 (↑)\\nType: float\\nSummary: R-squared metric indicating the goodness of fit of the model (higher is better)\\n\\nColumn: MAE (% ↓)\\nType: float\\nSummary: Mean Absolute Error percentage (lower is better)', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='9c77bc0db7f32948e8ee7c7b4bfad91f80d21c58b39b860326cc5bc0fd086cd9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='33c7bb68-e673-4f1a-aaa1-b8a071169154', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='979d0146db717736bee8c16b4e27fe737e6a9e7b55c60fb934897171dd576fd6')}, text='Comparison of performance metrics R2 and MAE for different test sets including CIFAR-10.1, CIFAR-10.2, and CINIC-10.,\\nwith the following table title:\\nPerformance Metrics Comparison for Different Test Sets,\\nwith the following columns:\\n- Test set: Name of the test set\\n- R2 (↑): R-squared metric indicating the goodness of fit of the model (higher is better)\\n- MAE (% ↓): Mean Absolute Error percentage (lower is better)\\n\\n|Test set|R2 (↑)|MAE (% ↓)|\\n|---|---|---|\\n|CIFAR-10.1|0.996|0.997|1.07|0.93|\\n|CIFAR-10.2|0.981|0.996|2.22|0.95|\\n|CINIC-10|0.978|0.990|2.41|1.49|', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='33c7bb68-e673-4f1a-aaa1-b8a071169154', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='36d3c587-2abc-4053-9bea-7697bf465763', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='ebec80ca0bcac045e5d33fb6fb529cf8a0795948457264f0d2727b9c6009930d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_64_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': '|Test set|R2 (↑)|MAE (% ↓)|\\n|---|---|---|\\n|CIFAR-10.1|0.996|0.997|1.07|0.93|\\n|CIFAR-10.2|0.981|0.996|2.22|0.95|\\n|CINIC-10|0.978|0.990|2.41|1.49|', 'table_summary': 'Comparison of performance metrics R2 and MAE for different test sets including CIFAR-10.1, CIFAR-10.2, and CINIC-10.,\\nwith the following table title:\\nPerformance Metrics Comparison for Different Test Sets,\\nwith the following columns:\\n- Test set: Name of the test set\\n- R2 (↑): R-squared metric indicating the goodness of fit of the model (higher is better)\\n- MAE (% ↓): Mean Absolute Error percentage (lower is better)\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='84f5e5587b205a31e92f311859ba49394dd3def6843b2eb5bcc9e049c5746712'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_68_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: Evaluation\\nType: string\\nSummary: None\\n\\nColumn: CIFAR-10\\nType: string\\nSummary: None\\n\\nColumn: ImageNet\\nType: string\\nSummary: None\\n\\nColumn: CIFAR+IN\\nType: string\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='a30be48770042d3b2378a8f52d3db1af8afb840c3a9bd60536eea4f23e346874')}, text='(b) Effective robustness values (%)\\n\\nWe report the mean and standard deviation for three groups of models with different training data, respectively.', start_char_idx=2, end_char_idx=151, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_68_table_ref', embedding=None, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: Evaluation\\nType: string\\nSummary: None\\n\\nColumn: CIFAR-10\\nType: string\\nSummary: None\\n\\nColumn: ImageNet\\nType: string\\nSummary: None\\n\\nColumn: CIFAR+IN\\nType: string\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='33c7bb68-e673-4f1a-aaa1-b8a071169154', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='979d0146db717736bee8c16b4e27fe737e6a9e7b55c60fb934897171dd576fd6'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_68_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: 'CIFAR-10.1', 1: 'CIFAR-10.1', 2: 'CIFAR-10.2', 3: 'CIFAR-10.2', 4: 'CINIC-10', 5: 'CINIC-10', 6: 'Average', 7: 'Average'}, 'Evaluation': {0: 'Single-ID', 1: 'Multi-ID', 2: 'Single-ID', 3: 'Multi-ID', 4: 'Single-ID', 5: 'Multi-ID', 6: 'Single-ID', 7: 'Multi-ID'}, 'CIFAR-10': {0: '-1.68±0.92', 1: '-1.43±0.92', 2: '-1.65±0.70', 3: '-0.76±0.77', 4: '-0.96±1.43', 5: '-0.08±1.52', 6: '-1.43±0.53', 7: '-0.76±0.63'}, 'ImageNet': {0: '1.05±1.27', 1: '0.10±1.12', 2: '3.91±2.20', 3: '0.56±1.27', 4: '2.77±1.25', 5: '-0.52±0.98', 6: '2.58±1.32', 7: '0.04±0.67'}, 'CIFAR+IN': {0: '0.02±1.10', 1: '0.19±1.01', 2: '-0.64±1.79', 3: '0.03±1.29', 4: '-0.10±2.81', 5: '0.63±2.10', 6: '-0.24±1.58', 7: '0.28±1.04'}}\", 'table_summary': 'This table presents evaluation results for different datasets and models, comparing Single-ID and Multi-ID performance on CIFAR-10, ImageNet, and CIFAR+IN datasets.,\\nwith the following table title:\\nEvaluation Results for Different Datasets and Models,\\nwith the following columns:\\n- Test set: None\\n- Evaluation: None\\n- CIFAR-10: None\\n- ImageNet: None\\n- CIFAR+IN: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='8e8066fe4bb7aae1ebe9ae5b85e031ae761d156e7efa7cd7f534c495f494e835')}, text='This table presents evaluation results for different datasets and models, comparing Single-ID and Multi-ID performance on CIFAR-10, ImageNet, and CIFAR+IN datasets.,\\nwith the following table title:\\nEvaluation Results for Different Datasets and Models,\\nwith the following columns:\\n- Test set: None\\n- Evaluation: None\\n- CIFAR-10: None\\n- ImageNet: None\\n- CIFAR+IN: None\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_68_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_68_table', embedding=None, metadata={'table_df': \"{'Test set': {0: 'CIFAR-10.1', 1: 'CIFAR-10.1', 2: 'CIFAR-10.2', 3: 'CIFAR-10.2', 4: 'CINIC-10', 5: 'CINIC-10', 6: 'Average', 7: 'Average'}, 'Evaluation': {0: 'Single-ID', 1: 'Multi-ID', 2: 'Single-ID', 3: 'Multi-ID', 4: 'Single-ID', 5: 'Multi-ID', 6: 'Single-ID', 7: 'Multi-ID'}, 'CIFAR-10': {0: '-1.68±0.92', 1: '-1.43±0.92', 2: '-1.65±0.70', 3: '-0.76±0.77', 4: '-0.96±1.43', 5: '-0.08±1.52', 6: '-1.43±0.53', 7: '-0.76±0.63'}, 'ImageNet': {0: '1.05±1.27', 1: '0.10±1.12', 2: '3.91±2.20', 3: '0.56±1.27', 4: '2.77±1.25', 5: '-0.52±0.98', 6: '2.58±1.32', 7: '0.04±0.67'}, 'CIFAR+IN': {0: '0.02±1.10', 1: '0.19±1.01', 2: '-0.64±1.79', 3: '0.03±1.29', 4: '-0.10±2.81', 5: '0.63±2.10', 6: '-0.24±1.58', 7: '0.28±1.04'}}\", 'table_summary': 'This table presents evaluation results for different datasets and models, comparing Single-ID and Multi-ID performance on CIFAR-10, ImageNet, and CIFAR+IN datasets.,\\nwith the following table title:\\nEvaluation Results for Different Datasets and Models,\\nwith the following columns:\\n- Test set: None\\n- Evaluation: None\\n- CIFAR-10: None\\n- ImageNet: None\\n- CIFAR+IN: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_68_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: Evaluation\\nType: string\\nSummary: None\\n\\nColumn: CIFAR-10\\nType: string\\nSummary: None\\n\\nColumn: ImageNet\\nType: string\\nSummary: None\\n\\nColumn: CIFAR+IN\\nType: string\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='a30be48770042d3b2378a8f52d3db1af8afb840c3a9bd60536eea4f23e346874'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='baa2d904-aa97-47f9-a393-daef5a0aa05f', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='0d5c54126d062eb8ad2226d1784aa4a156c1c9f52cb734c916aef881603bd881')}, text='This table presents evaluation results for different datasets and models, comparing Single-ID and Multi-ID performance on CIFAR-10, ImageNet, and CIFAR+IN datasets.,\\nwith the following table title:\\nEvaluation Results for Different Datasets and Models,\\nwith the following columns:\\n- Test set: None\\n- Evaluation: None\\n- CIFAR-10: None\\n- ImageNet: None\\n- CIFAR+IN: None\\n\\n|Test set|Evaluation|CIFAR-10|ImageNet|CIFAR+IN|\\n|---|---|---|---|---|\\n|CIFAR-10.1|Single-ID|-1.68±0.92|1.05±1.27|0.02±1.10|\\n|CIFAR-10.1|Multi-ID|-1.43±0.92|0.10±1.12|0.19±1.01|\\n|CIFAR-10.2|Single-ID|-1.65±0.70|3.91±2.20|-0.64±1.79|\\n|CIFAR-10.2|Multi-ID|-0.76±0.77|0.56±1.27|0.03±1.29|\\n|CINIC-10|Single-ID|-0.96±1.43|2.77±1.25|-0.10±2.81|\\n|CINIC-10|Multi-ID|-0.08±1.52|-0.52±0.98|0.63±2.10|\\n|Average|Single-ID|-1.43±0.53|2.58±1.32|-0.24±1.58|\\n|Average|Multi-ID|-0.76±0.63|0.04±0.67|0.28±1.04|\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='baa2d904-aa97-47f9-a393-daef5a0aa05f', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fc54b008-9e63-4bd3-bb5b-5fdbd497d856', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='76f8d97355ccd9b9ab6d6c1ab3109d5f626141ca135e0bfef4799d1f8b218070'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_68_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: 'CIFAR-10.1', 1: 'CIFAR-10.1', 2: 'CIFAR-10.2', 3: 'CIFAR-10.2', 4: 'CINIC-10', 5: 'CINIC-10', 6: 'Average', 7: 'Average'}, 'Evaluation': {0: 'Single-ID', 1: 'Multi-ID', 2: 'Single-ID', 3: 'Multi-ID', 4: 'Single-ID', 5: 'Multi-ID', 6: 'Single-ID', 7: 'Multi-ID'}, 'CIFAR-10': {0: '-1.68±0.92', 1: '-1.43±0.92', 2: '-1.65±0.70', 3: '-0.76±0.77', 4: '-0.96±1.43', 5: '-0.08±1.52', 6: '-1.43±0.53', 7: '-0.76±0.63'}, 'ImageNet': {0: '1.05±1.27', 1: '0.10±1.12', 2: '3.91±2.20', 3: '0.56±1.27', 4: '2.77±1.25', 5: '-0.52±0.98', 6: '2.58±1.32', 7: '0.04±0.67'}, 'CIFAR+IN': {0: '0.02±1.10', 1: '0.19±1.01', 2: '-0.64±1.79', 3: '0.03±1.29', 4: '-0.10±2.81', 5: '0.63±2.10', 6: '-0.24±1.58', 7: '0.28±1.04'}}\", 'table_summary': 'This table presents evaluation results for different datasets and models, comparing Single-ID and Multi-ID performance on CIFAR-10, ImageNet, and CIFAR+IN datasets.,\\nwith the following table title:\\nEvaluation Results for Different Datasets and Models,\\nwith the following columns:\\n- Test set: None\\n- Evaluation: None\\n- CIFAR-10: None\\n- ImageNet: None\\n- CIFAR+IN: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='8e8066fe4bb7aae1ebe9ae5b85e031ae761d156e7efa7cd7f534c495f494e835'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_72_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: Different datasets used for testing\\n\\nColumn: R2 (↑)\\nType: float\\nSummary: R2 values for Single-ID and Multi-ID models\\n\\nColumn: MAE (% ↓)\\nType: float\\nSummary: MAE values for Single-ID and Multi-ID models', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='e4e5d38753f7648b26ede44925558dc6a43e1894816d9d2e13ed7b93061db2fa')}, text='Effective robustness values seem to suggest an advantage of ImageNet models compared to CIFAR-10 models, which is consistent with the findings in Miller et al. (2021). However, under the multi-ID evaluation, the advantage of ImageNet models diminishes, and the effective robustness values of both CIFAR-10 models and ImageNet models are much closer to 0. Therefore, the apparent advantage reported by prior works can be explained as the effect of training data on the single-ID evaluation, and our multi-ID evaluation resolves this confounder to provide a more precise understanding.\\n\\nIn Figure 3a, we visualize the multi-ID effective robustness on CIFAR-10.2, where the accuracies of all the models approximately lie on a plane (the baseline function) on the logit scale, and thus these models have similar effective robustness as the OOD accuracy of all the models can be approximately predicted using a simple plane. We also show projected views of Figure 3a in Figure 4, where Figure 4a and Figure 4b correspond to the single-ID evaluation taking different ID test sets with contradictory conclusions. In contrast, our new evaluation provides a more holistic view.\\n\\n Evaluation on ImageNet-like OOD Test Sets\\n\\nWe then conduct evaluation on ImageNet-like OOD test sets, and we compare ImageNet models with models trained on YFCC and LAION, respectively. We show the fitting quality in Table 2 and the effective robustness in Tables 3a and 3b. Consistent with results in Section 5.2, our multi-ID evaluation improves the fitting quality over the single-ID evaluation to better predict and understand the OOD accuracies from ID accuracies. On effective robustness, single-ID evaluation leads to a perception of an effective robustness gain when there is a mismatch between the training data and the single ID test set. Our multi-ID evaluation enjoys a holistic view of all the training distributions and suggests that all the models evaluated here have similar effective robustness.\\n\\nSpecifically, the improvement of fitting quality is particularly significant for models involving LAION on ImageNet-R (R2 improved from 0.216 to 0.982 and MAE reduced from 9.23% to 1.32%) and ImageNet-Sketch (R2 improved from 0.281 to 0.937 and MAE reduced from 7.90% to 2.10%). On the effective robustness values, under the single-ID evaluation, YFCC and LAION models have positive effective robustness values (2.59±2.43 (%) on average for YFCC models and 5.96±4.96 (%) on average for LAION models), which is consistent with the findings in Fang et al. (2022); Nguyen et al. (2022). In contrast, under our multi-ID evaluation, the average effective robustness becomes 0.77±0.85 (%) for YFCC models, and -0.00±0.52 (%) for LAION models, much closer to 0. While single-ID evaluation used by prior works (Fang et al., 2022; Nguyen et al., 2022) suggests effective', start_char_idx=1, end_char_idx=2847, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_72_table_ref', embedding=None, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: Different datasets used for testing\\n\\nColumn: R2 (↑)\\nType: float\\nSummary: R2 values for Single-ID and Multi-ID models\\n\\nColumn: MAE (% ↓)\\nType: float\\nSummary: MAE values for Single-ID and Multi-ID models', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='baa2d904-aa97-47f9-a393-daef5a0aa05f', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='0d5c54126d062eb8ad2226d1784aa4a156c1c9f52cb734c916aef881603bd881'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_72_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': '| |Test set|R2 (↑)|MAE (% ↓)|\\n|---|---|---|---|\\n| |Single-ID|Multi-ID|Single-ID|Multi-ID|\\n|ImageNet-V2| |0.990|0.999|1.44|0.54|\\n|YFCC| |0.879|0.965|2.55|1.30|\\n|ImageNet-Sketch| |0.928|0.945|1.56|1.31|\\n| |ObjectNet|0.903|0.936|2.60|1.98|\\n|ImageNet-V2| |0.992|0.999|1.33|0.51|\\n|LAION| |0.216|0.982|9.23|1.32|\\n|ImageNet-Sketch| |0.281|0.937|7.90|2.10|\\n| |ObjectNet|0.849|0.906|2.88|2.38|', 'table_summary': 'Comparison of performance metrics R2 and MAE for different datasets and models,\\nwith the following table title:\\nPerformance Metrics Comparison,\\nwith the following columns:\\n- Test set: Different datasets used for testing\\n- R2 (↑): R2 values for Single-ID and Multi-ID models\\n- MAE (% ↓): MAE values for Single-ID and Multi-ID models\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='e93ec74350d1198916b8188b11e7095e5677a4ac90b26065a32a4a23bb1bb828')}, text='Comparison of performance metrics R2 and MAE for different datasets and models,\\nwith the following table title:\\nPerformance Metrics Comparison,\\nwith the following columns:\\n- Test set: Different datasets used for testing\\n- R2 (↑): R2 values for Single-ID and Multi-ID models\\n- MAE (% ↓): MAE values for Single-ID and Multi-ID models\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_72_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_72_table', embedding=None, metadata={'table_df': '| |Test set|R2 (↑)|MAE (% ↓)|\\n|---|---|---|---|\\n| |Single-ID|Multi-ID|Single-ID|Multi-ID|\\n|ImageNet-V2| |0.990|0.999|1.44|0.54|\\n|YFCC| |0.879|0.965|2.55|1.30|\\n|ImageNet-Sketch| |0.928|0.945|1.56|1.31|\\n| |ObjectNet|0.903|0.936|2.60|1.98|\\n|ImageNet-V2| |0.992|0.999|1.33|0.51|\\n|LAION| |0.216|0.982|9.23|1.32|\\n|ImageNet-Sketch| |0.281|0.937|7.90|2.10|\\n| |ObjectNet|0.849|0.906|2.88|2.38|', 'table_summary': 'Comparison of performance metrics R2 and MAE for different datasets and models,\\nwith the following table title:\\nPerformance Metrics Comparison,\\nwith the following columns:\\n- Test set: Different datasets used for testing\\n- R2 (↑): R2 values for Single-ID and Multi-ID models\\n- MAE (% ↓): MAE values for Single-ID and Multi-ID models\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_72_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: Different datasets used for testing\\n\\nColumn: R2 (↑)\\nType: float\\nSummary: R2 values for Single-ID and Multi-ID models\\n\\nColumn: MAE (% ↓)\\nType: float\\nSummary: MAE values for Single-ID and Multi-ID models', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='e4e5d38753f7648b26ede44925558dc6a43e1894816d9d2e13ed7b93061db2fa'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_74_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: ImageNet\\nType: string\\nSummary: None\\n\\nColumn: YFCC\\nType: string\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='12b48c45ec82dfad71ba05b1fbc7fc494f02752652188164fd8f4d0c91e80809')}, text='Comparison of performance metrics R2 and MAE for different datasets and models,\\nwith the following table title:\\nPerformance Metrics Comparison,\\nwith the following columns:\\n- Test set: Different datasets used for testing\\n- R2 (↑): R2 values for Single-ID and Multi-ID models\\n- MAE (% ↓): MAE values for Single-ID and Multi-ID models\\n\\n| |Test set|R2 (↑)|MAE (% ↓)|\\n|---|---|---|---|\\n| |Single-ID|Multi-ID|Single-ID|Multi-ID|\\n|ImageNet-V2| |0.990|0.999|1.44|0.54|\\n|YFCC| |0.879|0.965|2.55|1.30|\\n|ImageNet-Sketch| |0.928|0.945|1.56|1.31|\\n| |ObjectNet|0.903|0.936|2.60|1.98|\\n|ImageNet-V2| |0.992|0.999|1.33|0.51|\\n|LAION| |0.216|0.982|9.23|1.32|\\n|ImageNet-Sketch| |0.281|0.937|7.90|2.10|\\n| |ObjectNet|0.849|0.906|2.88|2.38|', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_74_table_ref', embedding=None, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: ImageNet\\nType: string\\nSummary: None\\n\\nColumn: YFCC\\nType: string\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_72_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': '| |Test set|R2 (↑)|MAE (% ↓)|\\n|---|---|---|---|\\n| |Single-ID|Multi-ID|Single-ID|Multi-ID|\\n|ImageNet-V2| |0.990|0.999|1.44|0.54|\\n|YFCC| |0.879|0.965|2.55|1.30|\\n|ImageNet-Sketch| |0.928|0.945|1.56|1.31|\\n| |ObjectNet|0.903|0.936|2.60|1.98|\\n|ImageNet-V2| |0.992|0.999|1.33|0.51|\\n|LAION| |0.216|0.982|9.23|1.32|\\n|ImageNet-Sketch| |0.281|0.937|7.90|2.10|\\n| |ObjectNet|0.849|0.906|2.88|2.38|', 'table_summary': 'Comparison of performance metrics R2 and MAE for different datasets and models,\\nwith the following table title:\\nPerformance Metrics Comparison,\\nwith the following columns:\\n- Test set: Different datasets used for testing\\n- R2 (↑): R2 values for Single-ID and Multi-ID models\\n- MAE (% ↓): MAE values for Single-ID and Multi-ID models\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='e93ec74350d1198916b8188b11e7095e5677a4ac90b26065a32a4a23bb1bb828'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_74_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': '|Test set|ImageNet|YFCC|\\n|---|---|---|\\n|Single-ID|Multi-ID|Single-ID|Multi-ID|\\n|ImageNet-V2|-1.23±0.46|-0.19±0.50|1.69±1.84|-0.16±0.57|\\n|ImageNet-R|-2.80±1.34|-0.41±1.83|3.44±3.25|1.07±1.17|\\n|ImageNet-Sketch|-1.25±1.90|0.14±2.57|1.90±2.25|0.89±1.29|\\n|ObjectNet|-0.99±4.23|0.74±4.14|3.32±2.55|1.27±0.85|\\n|Average|-1.57±1.20|0.07±1.68|2.59±2.43|0.77±0.85|\\n|ImageNet-V2| |-1.21±0.56|0.05±0.65|1.42±1.73|-0.03±0.57|\\n|ImageNet-R| |-9.45±2.79|-0.54±1.90|9.48±8.84|-0.65±1.05|\\n|ImageNet-Sketch| |-7.63±3.40|-0.72±3.03|8.71±7.15|-1.10±1.98|\\n|ObjectNet| |-1.90±4.48|1.14±4.18|4.24±2.39|1.77±1.20|\\n|Average| |-5.05±2.21|-0.02±1.53|5.96±4.96|-0.00±0.52|', 'table_summary': 'This table compares performance metrics across different test sets and identification types for various datasets like ImageNet, YFCC, ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nComparison of Performance Metrics Across Test Sets and Identification Types,\\nwith the following columns:\\n- Test set: None\\n- ImageNet: None\\n- YFCC: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='bc840931746b7b5914606b2ad24b8d8761a5e2dcbbde56c650dbe765187b0261')}, text='This table compares performance metrics across different test sets and identification types for various datasets like ImageNet, YFCC, ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nComparison of Performance Metrics Across Test Sets and Identification Types,\\nwith the following columns:\\n- Test set: None\\n- ImageNet: None\\n- YFCC: None\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_74_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_74_table', embedding=None, metadata={'table_df': '|Test set|ImageNet|YFCC|\\n|---|---|---|\\n|Single-ID|Multi-ID|Single-ID|Multi-ID|\\n|ImageNet-V2|-1.23±0.46|-0.19±0.50|1.69±1.84|-0.16±0.57|\\n|ImageNet-R|-2.80±1.34|-0.41±1.83|3.44±3.25|1.07±1.17|\\n|ImageNet-Sketch|-1.25±1.90|0.14±2.57|1.90±2.25|0.89±1.29|\\n|ObjectNet|-0.99±4.23|0.74±4.14|3.32±2.55|1.27±0.85|\\n|Average|-1.57±1.20|0.07±1.68|2.59±2.43|0.77±0.85|\\n|ImageNet-V2| |-1.21±0.56|0.05±0.65|1.42±1.73|-0.03±0.57|\\n|ImageNet-R| |-9.45±2.79|-0.54±1.90|9.48±8.84|-0.65±1.05|\\n|ImageNet-Sketch| |-7.63±3.40|-0.72±3.03|8.71±7.15|-1.10±1.98|\\n|ObjectNet| |-1.90±4.48|1.14±4.18|4.24±2.39|1.77±1.20|\\n|Average| |-5.05±2.21|-0.02±1.53|5.96±4.96|-0.00±0.52|', 'table_summary': 'This table compares performance metrics across different test sets and identification types for various datasets like ImageNet, YFCC, ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nComparison of Performance Metrics Across Test Sets and Identification Types,\\nwith the following columns:\\n- Test set: None\\n- ImageNet: None\\n- YFCC: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_74_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: ImageNet\\nType: string\\nSummary: None\\n\\nColumn: YFCC\\nType: string\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='12b48c45ec82dfad71ba05b1fbc7fc494f02752652188164fd8f4d0c91e80809'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c2e1828a-e33c-4f3c-b150-088ec5f94a46', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='f78cf7f743e7ac37b5316ad7d051e41283c66f00f5629683427d727c1c37c0d3')}, text='This table compares performance metrics across different test sets and identification types for various datasets like ImageNet, YFCC, ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nComparison of Performance Metrics Across Test Sets and Identification Types,\\nwith the following columns:\\n- Test set: None\\n- ImageNet: None\\n- YFCC: None\\n\\n|Test set|ImageNet|YFCC|\\n|---|---|---|\\n|Single-ID|Multi-ID|Single-ID|Multi-ID|\\n|ImageNet-V2|-1.23±0.46|-0.19±0.50|1.69±1.84|-0.16±0.57|\\n|ImageNet-R|-2.80±1.34|-0.41±1.83|3.44±3.25|1.07±1.17|\\n|ImageNet-Sketch|-1.25±1.90|0.14±2.57|1.90±2.25|0.89±1.29|\\n|ObjectNet|-0.99±4.23|0.74±4.14|3.32±2.55|1.27±0.85|\\n|Average|-1.57±1.20|0.07±1.68|2.59±2.43|0.77±0.85|\\n|ImageNet-V2| |-1.21±0.56|0.05±0.65|1.42±1.73|-0.03±0.57|\\n|ImageNet-R| |-9.45±2.79|-0.54±1.90|9.48±8.84|-0.65±1.05|\\n|ImageNet-Sketch| |-7.63±3.40|-0.72±3.03|8.71±7.15|-1.10±1.98|\\n|ObjectNet| |-1.90±4.48|1.14±4.18|4.24±2.39|1.77±1.20|\\n|Average| |-5.05±2.21|-0.02±1.53|5.96±4.96|-0.00±0.52|', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='c2e1828a-e33c-4f3c-b150-088ec5f94a46', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='084e073d-abc3-4853-a2a8-53af4f6afe02', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='fe69aba36a912e7f344674d6d2cfe2670595102d14a77fd70df9f34fe3adb0af'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6a0c8242-1b5e-42df-806f-f6f733c05582', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='5e1ea7cf5123ac5bce1c932cc9f601af5a4d099a4488b449ddfd29c35622aad3'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_74_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': '|Test set|ImageNet|YFCC|\\n|---|---|---|\\n|Single-ID|Multi-ID|Single-ID|Multi-ID|\\n|ImageNet-V2|-1.23±0.46|-0.19±0.50|1.69±1.84|-0.16±0.57|\\n|ImageNet-R|-2.80±1.34|-0.41±1.83|3.44±3.25|1.07±1.17|\\n|ImageNet-Sketch|-1.25±1.90|0.14±2.57|1.90±2.25|0.89±1.29|\\n|ObjectNet|-0.99±4.23|0.74±4.14|3.32±2.55|1.27±0.85|\\n|Average|-1.57±1.20|0.07±1.68|2.59±2.43|0.77±0.85|\\n|ImageNet-V2| |-1.21±0.56|0.05±0.65|1.42±1.73|-0.03±0.57|\\n|ImageNet-R| |-9.45±2.79|-0.54±1.90|9.48±8.84|-0.65±1.05|\\n|ImageNet-Sketch| |-7.63±3.40|-0.72±3.03|8.71±7.15|-1.10±1.98|\\n|ObjectNet| |-1.90±4.48|1.14±4.18|4.24±2.39|1.77±1.20|\\n|Average| |-5.05±2.21|-0.02±1.53|5.96±4.96|-0.00±0.52|', 'table_summary': 'This table compares performance metrics across different test sets and identification types for various datasets like ImageNet, YFCC, ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nComparison of Performance Metrics Across Test Sets and Identification Types,\\nwith the following columns:\\n- Test set: None\\n- ImageNet: None\\n- YFCC: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='bc840931746b7b5914606b2ad24b8d8761a5e2dcbbde56c650dbe765187b0261')}, text='Robustness gains of YFCC models compared to ImageNet models (Figure 5a), all the models have similar effective robustness under our multi-ID evaluation. Additionally, we provide an ablation study on using models with mixed training data in Appendix A.3 and additional interactive visualization on our website at https://shizhouxing.github.io/effective-robustness.\\n\\n Evaluation on Additional Models\\n\\nWe also evaluate additional models that are not used in fitting the baseline functions. We download models pre-trained by existing works, including OpenCLIP (Ilharco et al., 2021) and SLIP (Mu et al., 2022). OpenCLIP provides CLIP models trained on YFCC and LAION, and SLIP provides YFCC models trained using CLIP and also a combination of self-supervised learning (Chen et al., 2020a,b) and CLIP (SimCLR+CLIP namely SLIP). And we also fine-tune CLIP models on ImageNet for models we pre-train on YFCC and LAION. We use both vanilla fine-tuning and also WiSE-FT (Wortsman et al., 2022b) which aims to improve the robustness after fine-tuning, using a weight-space ensembling of the pre-trained model and the fine-tuned model. Details are in Appendix B.1.\\n\\nIn Table 4, we show results involving YFCC and LAION, respectively. Since these models are not used in the fitting, we do not use R2, but we use MAE to measure the fitting quality. Our multi-ID evaluation generally reduces MAE compared to the single-ID evaluation, and thus the multi-ID evaluation can still more accurately predict the OOD accuracy from the ID accuracies for these models that are not used in the fitting. The effective robustness values of the models also generally become closer to 0, especially for the zero-shot CLIP models. The results further validate that zero-shot CLIP models, although may achieve high OOD performance if pre-trained with large-scale data (Radford et al., 2021), generally do not improve effective robustness if we control for all the ID accuracies. Among the models evaluated here, SLIP models on YFCC and WiSE-FT models from LAION achieve relatively higher average effective robustness compared to other models, under our multi-ID evaluation, although the gains are not consistently significant on all the test sets and become much smaller than those reflected in the single-ID evaluation. However, we are not certain.\\n\\n Table and Text\\n\\n Table 4: Fitting quality and effective robustness for downloaded and fine-tuned models\\n\\nThe models are not used in the fitting and directly evaluated. Note that MAE and effective robustness are different, where MAE takes absolute values but not effective robustness. For CLIP by Mu et al. (2022) and SLIP, only models pre-trained on YFCC are available.\\n\\nModel\\nTest set\\nModels with pre-training on YFCC\\nModels with pre-training on LAION\\n\\nMAE (%, ↓)\\nEffective Robustness (%)\\n\\nSingle-ID\\nMulti-ID\\nSingle-ID\\nMulti-ID\\n\\nImageNetN-V2\\nImageNetN-R\\n3.95\\n0.45\\n3.95±0.70\\n-0.33±0.45\\n4.70\\n0.38\\n4.70±0.01\\n0.38±0.01\\n\\n$$\\n\\\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\\n\\\\hline\\n\\\\text{Model} & \\\\text{Test set} & \\\\text{Models with pre-training on YFCC} & \\\\text{Models with pre-training on LAION} \\\\\\\\\\n\\\\hline\\n&  & \\\\text{MAE (\\\\%, $\\\\downarrow$)} &  &  & \\\\text{Effective Robustness (\\\\%)} &  &  &  &  \\\\\\\\\\n\\\\hline\\n&  & \\\\text{Single-ID} & \\\\text{Multi-ID} & \\\\text{Single-ID} & \\\\text{Multi-ID} &  &  &  &  \\\\\\\\\\n\\\\hline\\n\\\\text{ImageNetN-V2} & \\\\text{ImageNetN-R} & 3.95 & 0.45 & 3.95\\\\pm0.70 & -0.33\\\\pm0.45 & 4.70 & 0.38 & 4.70\\\\pm0.01 & 0.38\\\\pm0.01 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n$$\\n\\n...\\n\\net al. (2022b) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al.', start_char_idx=1, end_char_idx=3557, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='6a0c8242-1b5e-42df-806f-f6f733c05582', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='084e073d-abc3-4853-a2a8-53af4f6afe02', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='fe69aba36a912e7f344674d6d2cfe2670595102d14a77fd70df9f34fe3adb0af'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c2e1828a-e33c-4f3c-b150-088ec5f94a46', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='f78cf7f743e7ac37b5316ad7d051e41283c66f00f5629683427d727c1c37c0d3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='694f9461-7986-4317-a8a5-0b2e7fdf0cdd', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='af9970eab0f3492813972a300b005fb31b6e5797f2263468429f76d3c4b7357f')}, text='(2022b) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al. (2021); Santurkar et al. (2022) studied the transfer performance of CLIP models, which is out of our scope on the robustness against natural distribution shifts.\\n\\n Conclusion\\n\\nTo conclude, we propose a new and more precise effective robustness evaluation for models with different training data. In our evaluation, the OOD accuracy can generally be better predicted from multiple ID accuracies compared to previous effective robustness evaluation with a single ID test. We find that zero-shot CLIP models pre-trained on language-image data do not have better effective robustness compared to standard image classifiers, and we provide a new understanding of the apparently significant effective robustness gains observed by prior works.\\n\\n Limitations and Future Work\\n\\nThere remain several limitations that may be addressed in future works:\\n\\n- Our method requires fully knowing the training distributions of all the evaluated models, which is not directly applicable for large-scale pre-trained models with private training data. And this also requires the training methods not to significantly alter the training distribution beyond basic data augmentation, while some methods such as SLIP may alter training distributions more significantly, and it is unclear how we can precisely define the training distribution for a model with post-training processing, such as WiSE-FT (Wortsman et al., 2022b) and Model Soups (Wortsman et al., 2022a) with weight-space ensembling. Future work may study how these techniques may impact the ID performance evaluation (Section 5.4).\\n- We assume that the two ID test sets have relatively close distributions compared to the OOD test sets. We have not considered how the difference between the multiple ID test sets may affect the evaluation, and how the effective robustness should be compared if models are trained on highly different distributions.\\n- We use fixed OOD test sets to evaluate the OOD performance, following previous works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022; Schuhmann et al., 2021). When models are pre-trained on large-scale data, it becomes unclear if these “OOD” test sets are still OOD, or if these test sets could be less OOD for the pre-trained models compared to standard classifiers. There may also be some distribution overlap between these test sets and the pre-training datasets, even though Radford et al. (2021) mentioned that the likelihood of direct data overlapping is low.\\n- We focus on distribution shifts where at least “accuracy-on-the-line” from existing works is known to hold for models trained on the same data (Taori et al., 2020; Miller et al., 2021), yet there are counterexamples where “accuracy-on-the-line” does not hold (Section 6) and requires further study. We may set a threshold on the fitting quality and only adopt our method when the fitting quality is sufficiently good. And there are also other OOD test sets (Singla & Feizi, 2021; Rusak et al., 2021; Singla et al., 2022; Idrissi et al., 2022; Li et al., 2023; Moayeri et al., 2022; Vasudevan et al., 2022) that have not been studied in the effective robustness works yet.\\n- While we mostly focus on comparing CLIP-like models with standard image classifiers, due to the notable OOD robustness of CLIP-like models studied in prior works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022), this work may further be extended to cover other types of models (Goyal et al., 2022; Singh et al., 2022) as well as other modalities such as distribution shifts on language data (Miller et al., 2020; Awadalla et al., 2022).\\n- Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively.\\n\\n Acknowledgments & Funding Disclosure\\n\\n Acknowledgments & Funding Disclosure\\n\\nWe thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive feedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001.', start_char_idx=3473, end_char_idx=7940, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='694f9461-7986-4317-a8a5-0b2e7fdf0cdd', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='084e073d-abc3-4853-a2a8-53af4f6afe02', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='fe69aba36a912e7f344674d6d2cfe2670595102d14a77fd70df9f34fe3adb0af'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6a0c8242-1b5e-42df-806f-f6f733c05582', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='5e1ea7cf5123ac5bce1c932cc9f601af5a4d099a4488b449ddfd29c35622aad3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a5eaa7c0-0287-4225-ba3e-5d68abf6edfa', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='19fe4d03aed866c4207dba8490f0533238eb2f3c6c03108f25fb9680e08ff35d')}, text='- Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively.\\n\\n Acknowledgments & Funding Disclosure\\n\\n Acknowledgments & Funding Disclosure\\n\\nWe thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive feedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001.\\n\\n References\\n\\n- Andreassen, A. J., Bahri, Y., Neyshabur, B., and Roelofs, R. The evolution of out-of-distribution robustness throughout fine-tuning. Transactions on Machine Learning Research, 2022.\\n- Awadalla, A., Wortsman, M., Ilharco, G., Min, S., Magnusson, I., Hajishirzi, H., and Schmidt, L. Exploring the landscape of distributional robustness for question answering models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5971–5987, 2022.\\n- Baek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. Advances in Neural Information Processing Systems, 35:19274–19289, 2022.\\n- Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.\\n- Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a.\\n- Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33: 22243–22255, 2020b.\\n- Darlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505, 2018.\\n- Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. Imagenet: A large-scale hierarchical image database. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248–255, 2009.\\n- Desai, K. and Johnson, J. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11162–11173, 2021.\\n- Devillers, B., Choksi, B., Bielawski, R., and VanRullen, R. Does language help generalization in vision models? In Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 171–182, 2021.\\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\\n- Fang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pp. 6216–6234. PMLR, 2022.', start_char_idx=7153, end_char_idx=10652, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='a5eaa7c0-0287-4225-ba3e-5d68abf6edfa', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='084e073d-abc3-4853-a2a8-53af4f6afe02', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='fe69aba36a912e7f344674d6d2cfe2670595102d14a77fd70df9f34fe3adb0af'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='694f9461-7986-4317-a8a5-0b2e7fdf0cdd', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='af9970eab0f3492813972a300b005fb31b6e5797f2263468429f76d3c4b7357f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='eb1c0287-10a5-48cf-bbc6-10453796d6ca', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='7f358b123a2b55734450cbd4f285261ef5b72bba59a94e48d6f8647d3be02e65')}, text='171–182, 2021.\\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\\n- Fang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pp. 6216–6234. PMLR, 2022.\\n- Goyal, P., Duval, Q., Seessel, I., Caron, M., Singh, M., Misra, I., Sagun, L., Joulin, A., and Bojanowski, P. Vision models are more robust and fair when pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360, 2022.\\n- He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.\\n\\n References\\n\\n References\\n\\n- Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions\\nand perturbations. In International Conference on Learning Representations, 2018.\\n- Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli,\\nS., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical\\nanalysis of out-of-distribution generalization. ICCV, 2021a.\\n- Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\n15262–15271, 2021b.\\n- Idrissi, B. Y., Bouchacourt, D., Balestriero, R., Evtimov, I., Hazirbas, C., Ballas, N., Vincent, P.,\\nDrozdzal, M., Lopez-Paz, D., and Ibrahim, M. Imagenet-x: Understanding model mistakes\\nwith factor of variation annotations. In The Eleventh International Conference on Learning\\nRepresentations, 2022.\\n- Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar,\\nV., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. Openclip, 2021. URL\\nhttps://doi.org/10.5281/zenodo.5143773.\\n- Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig,\\nT. Scaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational Conference on Machine Learning, pp. 4904–4916. PMLR, 2021.\\n- Kendall, M. Rank correlation methods. 1948.\\n- Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical\\nReport TR-2009, 2009.\\n- Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. Fine-tuning can distort pre-\\ntrained features and underperform out-of-distribution. In International Conference on Learning\\nRepresentations, 2021.\\n- Li, Z., Evtimov, I., Gordo, A., Hazirbas, C., Hassner, T., Ferrer, C. C., Xu, C., and Ibrahim, M.\\nA whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.', start_char_idx=10070, end_char_idx=13256, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='eb1c0287-10a5-48cf-bbc6-10453796d6ca', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='084e073d-abc3-4853-a2a8-53af4f6afe02', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='fe69aba36a912e7f344674d6d2cfe2670595102d14a77fd70df9f34fe3adb0af'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a5eaa7c0-0287-4225-ba3e-5d68abf6edfa', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='19fe4d03aed866c4207dba8490f0533238eb2f3c6c03108f25fb9680e08ff35d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6d0b67d5-d44a-4104-ace7-d94e30dab688', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='126e4cc655e5136650376bdd010dcfe6b55258f5a7f147e2735cf38856bbbeb9')}, text='- Kendall, M. Rank correlation methods. 1948.\\n- Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical\\nReport TR-2009, 2009.\\n- Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. Fine-tuning can distort pre-\\ntrained features and underperform out-of-distribution. In International Conference on Learning\\nRepresentations, 2021.\\n- Li, Z., Evtimov, I., Gordo, A., Hazirbas, C., Hassner, T., Ferrer, C. C., Xu, C., and Ibrahim, M.\\nA whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\n20071–20082, 2023.\\n- Lu, S., Nott, B., Olson, A., Todeschini, A., Vahabi, H., Carmon, Y., and Schmidt, L. Harder or\\ndifferent? a closer look at distribution shift in dataset reproduction. In ICML Workshop on\\nUncertainty and Robustness in Deep Learning, 2020.\\n- Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question\\nanswering models. In International Conference on Machine Learning, pp. 6905–6916. PMLR,\\n2020.\\n- Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., Liang, P., Carmon, Y.,\\nand Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and\\nin-distribution generalization. In International Conference on Machine Learning, pp. 7721–7735.\\nPMLR, 2021.\\n- Moayeri, M., Singla, S., and Feizi, S. Hard imagenet: Segmentations for objects with strong\\nspurious cues. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and\\nBenchmarks Track, 2022.\\n- Mu, N., Kirillov, A., Wagner, D., and Xie, S. Slip: Self-supervision meets language-image pre-\\ntraining. In European Conference on Computer Vision, pp. 529–544. Springer, 2022.\\n- Nguyen, T., Ilharco, G., Wortsman, M., Oh, S., and Schmidt, L. Quality not quantity: On the\\ninteraction between dataset design and robustness of clip. Advances in Neural Information\\nProcessing Systems, 35:21455–21469, 2022.\\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,\\nMishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision.\\nIn International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.\\n\\n References\\n\\n List of References\\n\\n- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do cifar-10 classifiers generalize to cifar-10?*\\n\\narXiv preprint arXiv:1806.00451, 2018.\\n- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do imagenet classifiers generalize to imagenet?*\\n\\nIn International Conference on Machine Learning, pp. 5389–5400. PMLR, 2019.\\n- Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and Bethge, M.\\n*If your data distribution shifts, use self-learning.*\\n\\nTransactions on Machine Learning Research, 2021.\\n- Santurkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T. *Is a caption worth a thousand images? a controlled study for representation learning.*\\n\\narXiv preprint arXiv:2207.07635, 2022.\\n- Sariyildiz, M. B., Perez, J., and Larlus, D. *Learning visual representations with caption annotations.*\\n\\nIn European Conference on Computer Vision, pp. 153–170. Springer, 2020.', start_char_idx=12594, end_char_idx=15874, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='6d0b67d5-d44a-4104-ace7-d94e30dab688', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='084e073d-abc3-4853-a2a8-53af4f6afe02', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='fe69aba36a912e7f344674d6d2cfe2670595102d14a77fd70df9f34fe3adb0af'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='eb1c0287-10a5-48cf-bbc6-10453796d6ca', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='7f358b123a2b55734450cbd4f285261ef5b72bba59a94e48d6f8647d3be02e65'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='88d6ce97-6786-4b70-8433-e748bf71768c', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='4356a865b8e0c57b4f09b3315c4c6b19d6659eef81f003e81c9567ae6215ff8d')}, text='PMLR, 2019.\\n- Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and Bethge, M.\\n*If your data distribution shifts, use self-learning.*\\n\\nTransactions on Machine Learning Research, 2021.\\n- Santurkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T. *Is a caption worth a thousand images? a controlled study for representation learning.*\\n\\narXiv preprint arXiv:2207.07635, 2022.\\n- Sariyildiz, M. B., Perez, J., and Larlus, D. *Learning visual representations with caption annotations.*\\n\\nIn European Conference on Computer Vision, pp. 153–170. Springer, 2020.\\n- Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A.\\n*Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.*\\n\\narXiv preprint arXiv:2111.02114, 2021.\\n- Singh, M., Gustafson, L., Adcock, A., de Freitas Reis, V., Gedik, B., Kosaraju, R. P., Mahajan, D., Girshick, R., Dollár, P., and Van Der Maaten, L.\\n*Revisiting weakly supervised pre-training of visual perception models.*\\n\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 804–814, 2022.\\n- Singla, S. and Feizi, S. *Salient imagenet: How to discover spurious features in deep learning?*\\n\\nIn International Conference on Learning Representations, 2021.\\n- Singla, S., Moayeri, M., and Feizi, S. *Core risk minimization using salient imagenet.*\\n\\narXiv preprint arXiv:2203.15566, 2022.\\n- Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L.\\n*Measuring robustness to natural distribution shifts in image classification.*\\n\\nAdvances in Neural Information Processing Systems, 33:18583–18599, 2020.\\n- Teney, D., Oh, S. J., and Abbasnejad, E. *Id and ood performance are sometimes inversely correlated on real-world datasets.*\\n\\narXiv preprint arXiv:2209.00613, 2022.\\n- Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J.\\n*Yfcc100m: The new data in multimedia research.*\\n\\nCommunications of the ACM, 59(2):64–73, 2016.\\n- Vasudevan, V., Caine, B., Gontijo Lopes, R., Fridovich-Keil, S., and Roelofs, R.\\n*When does dough become a bagel? analyzing the remaining mistakes on imagenet.*\\n\\nAdvances in Neural Information Processing Systems, 35:6720–6734, 2022.\\n- Wang, H., Ge, S., Lipton, Z., and Xing, E. P.\\n*Learning robust global representations by penalizing local predictive power.*\\n\\nAdvances in Neural Information Processing Systems, 32, 2019.\\n- Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al.\\n*Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.*\\n\\nIn International Conference on Machine Learning, pp. 23965–23998. PMLR, 2022a.\\n- Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.\\n*Robust fine-tuning of zero-shot models.*\\n\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.', start_char_idx=15270, end_char_idx=18378, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='88d6ce97-6786-4b70-8433-e748bf71768c', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='084e073d-abc3-4853-a2a8-53af4f6afe02', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='fe69aba36a912e7f344674d6d2cfe2670595102d14a77fd70df9f34fe3adb0af'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6d0b67d5-d44a-4104-ace7-d94e30dab688', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='126e4cc655e5136650376bdd010dcfe6b55258f5a7f147e2735cf38856bbbeb9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7fb7f3df-155f-4660-9ef4-23f4674fda62', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='242fda125248e270e818e33f6d507198697c1add15012525a82338c652923fb3')}, text='*Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.*\\n\\nIn International Conference on Machine Learning, pp. 23965–23998. PMLR, 2022a.\\n- Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.\\n*Robust fine-tuning of zero-shot models.*\\n\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959–7971, 2022b.\\n- Yadav, C. and Bottou, L. *Cold case: The lost mnist digits.*\\n\\nAdvances in neural information processing systems, 32, 2019.\\n- Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Langlotz, C. P.\\n*Contrastive learning of medical visual representations from paired images and text.*\\n\\nIn Machine Learning for Healthcare Conference, pp. 2–25. PMLR, 2022.\\n\\n Additional Results\\n\\n Projected Views\\n\\nIn Figure 5, we show projected views of Figure 3b.\\n\\n$$\\n\\\\begin{array}{|c|c|c|c|c|c|c|c|}\\n\\\\hline\\n\\\\text{ImageNet} & 95 & \\\\text{YFCC} & 50 & \\\\text{ImageNet+YFCC} \\\\\\\\\\n\\\\hline\\n90 & & & & 25 \\\\\\\\\\n80 & & & & 10 \\\\\\\\\\n70 & & & & \\\\\\\\\\n50 & & & & \\\\\\\\\\n25 & & & & \\\\\\\\\\n10 & & & & \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n$$\\n\\n$$\\n\\\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}\\n\\\\hline\\n10 & 25 & 50 & 70 & 80 & 90 & 95 & & & & \\\\\\\\\\n\\\\text{ImageNet accuracy (css., %)} & & & & & & & \\\\text{YFCC accuracy (css., %)} & & & \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n$$\\n\\n(a) ImageNet-R accuracy against ImageNet accuracy. YFCC models have higher ImageNet-R accuracy compared to ImageNet models when controlling for ImageNet accuracy only.\\n\\n(b) ImageNet-R accuracy against YFCC accuracy. YFCC models have similar ImageNet-R accuracy compared to ImageNet models when controlling for YFCC accuracy only.\\n\\nFigure 5: Projected views of Figure 3b. Figure 5a and Figure 5b correspond to single-ID evaluation using different ID test sets, Figure 5a suggests effective robustness gains of YFCC models but the gains diminish in Figure 5b. Our multi-ID evaluation shows a holistic view where all the models have a similar effective robustness.\\n\\n Agreement between Single-ID and Multi-ID Evaluation\\n\\nWe conduct an experiment to check the correlation between the single-ID evaluation and our new multi-ID evaluation, in terms of the relative ranking between different models trained on the same data. We use Kendall’s rank correlation test (Kendall, 1948) and we report the τ statistic computed by scipy.stats.kendalltau in Tables 5 to 7. Results show that the rankings on the single-ID effective robustness and multi-ID effective robustness are positively correlated for CIFAR-10 and ImageNet models. There is also a weaker positive correlation for YFCC models. For LAION models, there is sometimes a negative correlation. Overall, while there is often a positive correlation between the rankings provided by the single-ID evaluation and the multi-ID evaluation, respectively, it is not necessarily consistent on all the datasets. Thus, when comparing models trained on the same data, our multi-ID evaluation is not intended to replace the single-ID evaluation. Our multi-ID evaluation is mainly for comparing models trained on different data, and may be used as a supplementary evaluation if all the models are trained on a single dataset.', start_char_idx=17909, end_char_idx=21132, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='7fb7f3df-155f-4660-9ef4-23f4674fda62', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='084e073d-abc3-4853-a2a8-53af4f6afe02', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='fe69aba36a912e7f344674d6d2cfe2670595102d14a77fd70df9f34fe3adb0af'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='88d6ce97-6786-4b70-8433-e748bf71768c', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='4356a865b8e0c57b4f09b3315c4c6b19d6659eef81f003e81c9567ae6215ff8d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_112_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: The different datasets being compared\\n\\nColumn: ImageNet models\\nType: float\\nSummary: Performance scores of ImageNet models on each dataset\\n\\nColumn: YFCC models\\nType: float\\nSummary: Performance scores of YFCC models on each dataset', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='16714d015eb76f937f0d2daf456190f52c4a04897d8c2cc4ddea09a8b15dc936')}, text='We use Kendall’s rank correlation test (Kendall, 1948) and we report the τ statistic computed by scipy.stats.kendalltau in Tables 5 to 7. Results show that the rankings on the single-ID effective robustness and multi-ID effective robustness are positively correlated for CIFAR-10 and ImageNet models. There is also a weaker positive correlation for YFCC models. For LAION models, there is sometimes a negative correlation. Overall, while there is often a positive correlation between the rankings provided by the single-ID evaluation and the multi-ID evaluation, respectively, it is not necessarily consistent on all the datasets. Thus, when comparing models trained on the same data, our multi-ID evaluation is not intended to replace the single-ID evaluation. Our multi-ID evaluation is mainly for comparing models trained on different data, and may be used as a supplementary evaluation if all the models are trained on a single dataset.\\n\\n$$\\n\\\\begin{array}{|c|c|c|c|}\\n\\\\hline\\n\\\\text{Test set} & \\\\text{CIFAR-10.1} & \\\\text{CIFAR-10.2} & \\\\text{CINIC-10} \\\\\\\\\\n\\\\hline\\n\\\\text{CIFAR-10 models} & 0.9619 & 0.6667 & 0.8095 \\\\\\\\\\n\\\\text{ImageNet models} & 0.1664 & 0.1522 & 0.4907 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n$$\\n\\n Models with Mixed Training Data in the Fitting\\n\\nAs mentioned in Section 5.1, we train models with mixed data to obtain models with diverse accuracies. In Tables 8 and 9, we show that if we do not include models with mixed training data in the fitting, the MAE for these models can become higher, although the difference is not large.\\n\\n Research Results\\n\\n Table 6: τ statistics for ImageNet models and YFCC models on ImageNet-like OOD test sets', start_char_idx=20192, end_char_idx=21826, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_112_table_ref', embedding=None, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: The different datasets being compared\\n\\nColumn: ImageNet models\\nType: float\\nSummary: Performance scores of ImageNet models on each dataset\\n\\nColumn: YFCC models\\nType: float\\nSummary: Performance scores of YFCC models on each dataset', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7fb7f3df-155f-4660-9ef4-23f4674fda62', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='242fda125248e270e818e33f6d507198697c1add15012525a82338c652923fb3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_112_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: 'ImageNet models', 1: 'YFCC models'}, 'ImageNet-V2': {0: 0.5142, 1: 0.0256}, 'ImageNet-R': {0: 0.4412, 1: 0.8461}, 'ImageNet-Sketch': {0: 0.8031, 1: 0.7179}, 'ObjectNet': {0: 0.8063, 1: 0.2564}}\", 'table_summary': 'This table compares the performance of different models on various datasets including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet. The models are categorized as ImageNet models and YFCC models, with corresponding performance scores provided for each dataset.,\\nwith the following table title:\\nPerformance Comparison of ImageNet and YFCC Models on Different Datasets,\\nwith the following columns:\\n- Test set: The different datasets being compared\\n- ImageNet models: Performance scores of ImageNet models on each dataset\\n- YFCC models: Performance scores of YFCC models on each dataset\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='38ee6218afa812e8d0d7351222de0a891edcd91ccc23e82ffc50f2e635be9c05')}, text='This table compares the performance of different models on various datasets including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet. The models are categorized as ImageNet models and YFCC models, with corresponding performance scores provided for each dataset.,\\nwith the following table title:\\nPerformance Comparison of ImageNet and YFCC Models on Different Datasets,\\nwith the following columns:\\n- Test set: The different datasets being compared\\n- ImageNet models: Performance scores of ImageNet models on each dataset\\n- YFCC models: Performance scores of YFCC models on each dataset\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_112_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_112_table', embedding=None, metadata={'table_df': \"{'Test set': {0: 'ImageNet models', 1: 'YFCC models'}, 'ImageNet-V2': {0: 0.5142, 1: 0.0256}, 'ImageNet-R': {0: 0.4412, 1: 0.8461}, 'ImageNet-Sketch': {0: 0.8031, 1: 0.7179}, 'ObjectNet': {0: 0.8063, 1: 0.2564}}\", 'table_summary': 'This table compares the performance of different models on various datasets including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet. The models are categorized as ImageNet models and YFCC models, with corresponding performance scores provided for each dataset.,\\nwith the following table title:\\nPerformance Comparison of ImageNet and YFCC Models on Different Datasets,\\nwith the following columns:\\n- Test set: The different datasets being compared\\n- ImageNet models: Performance scores of ImageNet models on each dataset\\n- YFCC models: Performance scores of YFCC models on each dataset\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_112_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: The different datasets being compared\\n\\nColumn: ImageNet models\\nType: float\\nSummary: Performance scores of ImageNet models on each dataset\\n\\nColumn: YFCC models\\nType: float\\nSummary: Performance scores of YFCC models on each dataset', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='16714d015eb76f937f0d2daf456190f52c4a04897d8c2cc4ddea09a8b15dc936'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='06ba96b5-2309-4119-895a-a7ea5dfdd6a9', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='af626c9c08d3086ad834cedc198c04a0641e8337d9994f5989aeede8fbab7206')}, text='This table compares the performance of different models on various datasets including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet. The models are categorized as ImageNet models and YFCC models, with corresponding performance scores provided for each dataset.,\\nwith the following table title:\\nPerformance Comparison of ImageNet and YFCC Models on Different Datasets,\\nwith the following columns:\\n- Test set: The different datasets being compared\\n- ImageNet models: Performance scores of ImageNet models on each dataset\\n- YFCC models: Performance scores of YFCC models on each dataset\\n\\n|Test set|ImageNet-V2|ImageNet-R|ImageNet-Sketch|ObjectNet|\\n|---|---|---|---|---|\\n|ImageNet models|0.5142|0.4412|0.8031|0.8063|\\n|YFCC models|0.0256|0.8461|0.7179|0.2564|\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='06ba96b5-2309-4119-895a-a7ea5dfdd6a9', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5f53d20e-9eb0-4159-854e-06c8586de7e1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='3d160120a63f6f6c145d1acef5bae47982587ff3cca1e5478d8076504956f487'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_112_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: 'ImageNet models', 1: 'YFCC models'}, 'ImageNet-V2': {0: 0.5142, 1: 0.0256}, 'ImageNet-R': {0: 0.4412, 1: 0.8461}, 'ImageNet-Sketch': {0: 0.8031, 1: 0.7179}, 'ObjectNet': {0: 0.8063, 1: 0.2564}}\", 'table_summary': 'This table compares the performance of different models on various datasets including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet. The models are categorized as ImageNet models and YFCC models, with corresponding performance scores provided for each dataset.,\\nwith the following table title:\\nPerformance Comparison of ImageNet and YFCC Models on Different Datasets,\\nwith the following columns:\\n- Test set: The different datasets being compared\\n- ImageNet models: Performance scores of ImageNet models on each dataset\\n- YFCC models: Performance scores of YFCC models on each dataset\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='38ee6218afa812e8d0d7351222de0a891edcd91ccc23e82ffc50f2e635be9c05'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_116_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: ImageNet-V2\\nType: float\\nSummary: None\\n\\nColumn: ImageNet-R\\nType: float\\nSummary: None\\n\\nColumn: ImageNet-Sketch\\nType: float\\nSummary: None\\n\\nColumn: ObjectNet\\nType: float\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='36010f23f4c20c64dc36af0cd4e007e91c5b6ebb0e0f667f33f2f7052f2c5756')}, text='Table 7: τ statistics for ImageNet models and LAION models on ImageNet-like OOD test sets', start_char_idx=2, end_char_idx=91, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_116_table_ref', embedding=None, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: ImageNet-V2\\nType: float\\nSummary: None\\n\\nColumn: ImageNet-R\\nType: float\\nSummary: None\\n\\nColumn: ImageNet-Sketch\\nType: float\\nSummary: None\\n\\nColumn: ObjectNet\\nType: float\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='06ba96b5-2309-4119-895a-a7ea5dfdd6a9', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='af626c9c08d3086ad834cedc198c04a0641e8337d9994f5989aeede8fbab7206'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_116_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: 'ImageNet models', 1: 'LAION models'}, 'ImageNet-V2': {0: 0.3123, 1: -0.4945}, 'ImageNet-R': {0: 0.4384, 1: 0.6483}, 'ImageNet-Sketch': {0: 0.6216, 1: -0.2747}, 'ObjectNet': {0: 0.9729, 1: 0.6483}}\", 'table_summary': 'This table compares performance metrics of ImageNet and LAION models on different datasets.,\\nwith the following table title:\\nComparison of ImageNet and LAION Model Performance,\\nwith the following columns:\\n- Test set: None\\n- ImageNet-V2: None\\n- ImageNet-R: None\\n- ImageNet-Sketch: None\\n- ObjectNet: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='ce1a8b20fe64ac1ec85bf0d7fcd17e039614efa5ee0ac771550823eafb9d7528')}, text='This table compares performance metrics of ImageNet and LAION models on different datasets.,\\nwith the following table title:\\nComparison of ImageNet and LAION Model Performance,\\nwith the following columns:\\n- Test set: None\\n- ImageNet-V2: None\\n- ImageNet-R: None\\n- ImageNet-Sketch: None\\n- ObjectNet: None\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_116_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_116_table', embedding=None, metadata={'table_df': \"{'Test set': {0: 'ImageNet models', 1: 'LAION models'}, 'ImageNet-V2': {0: 0.3123, 1: -0.4945}, 'ImageNet-R': {0: 0.4384, 1: 0.6483}, 'ImageNet-Sketch': {0: 0.6216, 1: -0.2747}, 'ObjectNet': {0: 0.9729, 1: 0.6483}}\", 'table_summary': 'This table compares performance metrics of ImageNet and LAION models on different datasets.,\\nwith the following table title:\\nComparison of ImageNet and LAION Model Performance,\\nwith the following columns:\\n- Test set: None\\n- ImageNet-V2: None\\n- ImageNet-R: None\\n- ImageNet-Sketch: None\\n- ObjectNet: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_116_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: ImageNet-V2\\nType: float\\nSummary: None\\n\\nColumn: ImageNet-R\\nType: float\\nSummary: None\\n\\nColumn: ImageNet-Sketch\\nType: float\\nSummary: None\\n\\nColumn: ObjectNet\\nType: float\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='36010f23f4c20c64dc36af0cd4e007e91c5b6ebb0e0f667f33f2f7052f2c5756'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='104e6794-ee5a-4799-aad7-a0bf4666b3ad', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='bd7eb4180c074de207fdf845ad93eeb245e77f857189f04e34475244e615f5c5')}, text='This table compares performance metrics of ImageNet and LAION models on different datasets.,\\nwith the following table title:\\nComparison of ImageNet and LAION Model Performance,\\nwith the following columns:\\n- Test set: None\\n- ImageNet-V2: None\\n- ImageNet-R: None\\n- ImageNet-Sketch: None\\n- ObjectNet: None\\n\\n|Test set|ImageNet-V2|ImageNet-R|ImageNet-Sketch|ObjectNet|\\n|---|---|---|---|---|\\n|ImageNet models|0.3123|0.4384|0.6216|0.9729|\\n|LAION models|-0.4945|0.6483|-0.2747|0.6483|\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='104e6794-ee5a-4799-aad7-a0bf4666b3ad', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2133b0a7-1d1d-4f30-9cc0-7d46ac2f3733', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='2326f0c33fd2efcb1a5da1f8b789921e527653f609c4adacc051db9a0a63ab0b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_116_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: 'ImageNet models', 1: 'LAION models'}, 'ImageNet-V2': {0: 0.3123, 1: -0.4945}, 'ImageNet-R': {0: 0.4384, 1: 0.6483}, 'ImageNet-Sketch': {0: 0.6216, 1: -0.2747}, 'ObjectNet': {0: 0.9729, 1: 0.6483}}\", 'table_summary': 'This table compares performance metrics of ImageNet and LAION models on different datasets.,\\nwith the following table title:\\nComparison of ImageNet and LAION Model Performance,\\nwith the following columns:\\n- Test set: None\\n- ImageNet-V2: None\\n- ImageNet-R: None\\n- ImageNet-Sketch: None\\n- ObjectNet: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='ce1a8b20fe64ac1ec85bf0d7fcd17e039614efa5ee0ac771550823eafb9d7528'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_120_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: Excluded\\nType: float\\nSummary: None\\n\\nColumn: Included\\nType: float\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='6f716a5531d721f7e347038abe3bc96fdf133865b9421c8d8bae79f67fc43a2f')}, text='Table 8: MAE (%) for ImageNet+YFCC models\\n\\nWhen they are excluded and included in the fitting, respectively, comparing the effective robustness of ImageNet models and YFCC models.', start_char_idx=2, end_char_idx=181, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_120_table_ref', embedding=None, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: Excluded\\nType: float\\nSummary: None\\n\\nColumn: Included\\nType: float\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='104e6794-ee5a-4799-aad7-a0bf4666b3ad', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='bd7eb4180c074de207fdf845ad93eeb245e77f857189f04e34475244e615f5c5'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_120_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: 'ImageNet-V2', 1: 'ImageNet-R', 2: 'ImageNet-Sketch', 3: 'ObjectNet'}, 'ImageNet+YFCC models in the fitting': {0: ' ', 1: ' ', 2: ' ', 3: ' '}, 'Excluded': {0: 0.71, 1: 1.3, 2: 0.98, 3: 1.73}, 'Included': {0: 0.64, 1: 1.21, 2: 0.94, 3: 0.95}}\", 'table_summary': 'This table compares the performance of different models on ImageNet and YFCC datasets, showing the results for both excluded and included categories.,\\nwith the following table title:\\nImageNet+YFCC models in the fitting,\\nwith the following columns:\\n- Test set: None\\n- Excluded: None\\n- Included: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='11bca0e33b8ec92f57827a29bbf2ef9de0b40d0475ebff60eac5276f5bb43088')}, text='This table compares the performance of different models on ImageNet and YFCC datasets, showing the results for both excluded and included categories.,\\nwith the following table title:\\nImageNet+YFCC models in the fitting,\\nwith the following columns:\\n- Test set: None\\n- Excluded: None\\n- Included: None\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_120_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_120_table', embedding=None, metadata={'table_df': \"{'Test set': {0: 'ImageNet-V2', 1: 'ImageNet-R', 2: 'ImageNet-Sketch', 3: 'ObjectNet'}, 'ImageNet+YFCC models in the fitting': {0: ' ', 1: ' ', 2: ' ', 3: ' '}, 'Excluded': {0: 0.71, 1: 1.3, 2: 0.98, 3: 1.73}, 'Included': {0: 0.64, 1: 1.21, 2: 0.94, 3: 0.95}}\", 'table_summary': 'This table compares the performance of different models on ImageNet and YFCC datasets, showing the results for both excluded and included categories.,\\nwith the following table title:\\nImageNet+YFCC models in the fitting,\\nwith the following columns:\\n- Test set: None\\n- Excluded: None\\n- Included: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_120_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: None\\n\\nColumn: Excluded\\nType: float\\nSummary: None\\n\\nColumn: Included\\nType: float\\nSummary: None', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='6f716a5531d721f7e347038abe3bc96fdf133865b9421c8d8bae79f67fc43a2f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='69f17b27-05a7-4467-b5e2-a886171ef701', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='c2995f6426510a40ecff526d2021964e542e2752f9f13ad0f10184d66a29608e')}, text='This table compares the performance of different models on ImageNet and YFCC datasets, showing the results for both excluded and included categories.,\\nwith the following table title:\\nImageNet+YFCC models in the fitting,\\nwith the following columns:\\n- Test set: None\\n- Excluded: None\\n- Included: None\\n\\n|Test set|ImageNet+YFCC models in the fitting|Excluded|Included|\\n|---|---|---|---|\\n|ImageNet-V2| |0.71|0.64|\\n|ImageNet-R| |1.3|1.21|\\n|ImageNet-Sketch| |0.98|0.94|\\n|ObjectNet| |1.73|0.95|\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='69f17b27-05a7-4467-b5e2-a886171ef701', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2f0ab3b8-f92c-498a-a30a-bea2c4ab99d2', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='7ae0d4fd28770971605e98a7d2f4be4b448fcea1a653937a6e4b16d3ae1cd36c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_120_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: 'ImageNet-V2', 1: 'ImageNet-R', 2: 'ImageNet-Sketch', 3: 'ObjectNet'}, 'ImageNet+YFCC models in the fitting': {0: ' ', 1: ' ', 2: ' ', 3: ' '}, 'Excluded': {0: 0.71, 1: 1.3, 2: 0.98, 3: 1.73}, 'Included': {0: 0.64, 1: 1.21, 2: 0.94, 3: 0.95}}\", 'table_summary': 'This table compares the performance of different models on ImageNet and YFCC datasets, showing the results for both excluded and included categories.,\\nwith the following table title:\\nImageNet+YFCC models in the fitting,\\nwith the following columns:\\n- Test set: None\\n- Excluded: None\\n- Included: None\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='11bca0e33b8ec92f57827a29bbf2ef9de0b40d0475ebff60eac5276f5bb43088'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_126_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: The different test sets being compared\\n\\nColumn: Excluded\\nType: number\\nSummary: Performance values for the excluded models\\n\\nColumn: Included\\nType: number\\nSummary: Performance values for the included models', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='2488a868ed4f58ba137b9f6502034e5418acc9186cce84c0593be35506fb58ad')}, text='Experimental Details\\n\\n Details of Models\\n\\nWe use TF-Vision3 under the Apache License Version 2.0 to train standard classifiers on CIFAR-10 and ImageNet. We follow the configurations provided in TF-Vision for vanilla ResNet training on ImageNet and we train ResNet-18, ResNet-50 and ResNet-101 models. We reuse the configurations to train models on CIFAR-10, where we only change the dataset, number of classes, and image size, without tuning hyperparameters for the training. And we load checkpoints of ViT-S/16, ViT-B/16, and ViT-L/16 models pre-trained on ImageNet, provided by TF-Vision.\\n\\nFor training CLIP models, we mostly follow hyperparameters provided in Fang et al. (2022) and the implementation in Open-CLIP (Ilharco et al., 2021). While Fang et al. (2022) used a batch size of 1024, we use 2048 for more parallelism. We use YFCC-15M in Radford et al. (2021), which is a subset of YFCC-100M (Thomee et al., 2016). And we use LAION-15M which we uniformly sample from LAION-400M (Schuhmann et al., 2021). For fine-tuning CLIP models, we fine-tune for 50,000 steps, using learning rates $$3 \\\\times 10^{-5}$$ and $$1 \\\\times 10^{-4}$$, respectively. For WiSE-FT, we take $$\\\\alpha = 0.5$$ which is the coefficient for weight-space ensembling. For OpenCLIP models, we use ViT-B/32 models trained on LAION-400M. For SLIP, we use all the CLIP and SLIP models trained on YFCC-15M.\\n\\nFor data subsampling, we uniformly sample a proportion of training examples from the entire dataset, at ratios of {5%, 10%, 20%, 30%, 40%, 50%}, respectively. For combining two training datasets at various ratios, given a coefficient $$\\\\lambda$$ (0 < $$\\\\lambda$$ < 1), we uniformly sample a proportion of data from the two datasets at ratios of $$\\\\lambda$$ and $$(1 - \\\\lambda)$$, respectively, and then we combine the two subsets. When combining ImageNet and CIFAR-10, we take $$\\\\lambda \\\\in \\\\{0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.995\\\\}$$; when combining ImageNet with YFCC and LAION, respectively, we take $$\\\\lambda \\\\in \\\\{0.01, 0.1, 0.25, 0.5\\\\}$$.\\n\\nSources: TF-Vision, Open-CLIP, SLIP', start_char_idx=2, end_char_idx=2067, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " IndexNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_126_table_ref', embedding=None, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: The different test sets being compared\\n\\nColumn: Excluded\\nType: number\\nSummary: Performance values for the excluded models\\n\\nColumn: Included\\nType: number\\nSummary: Performance values for the included models', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['col_schema'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='69f17b27-05a7-4467-b5e2-a886171ef701', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='c2995f6426510a40ecff526d2021964e542e2752f9f13ad0f10184d66a29608e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_126_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: ' ', 1: ' ', 2: 'ImageNet-Sketch', 3: ' '}, 'ImageNet+LAION models in the fitting': {0: 'ImageNet-V2', 1: 'ImageNet-R', 2: '2.62', 3: 'ObjectNet'}, ' ': {0: 0.54, 1: 1.56, 2: 2.03, 3: 2.91}, 'Excluded': {0: '0.51', 1: '1.19', 2: ' ', 3: '1.47'}, 'Included': {0: ' ', 1: ' ', 2: ' ', 3: ' '}}\", 'table_summary': 'This table compares the performance of different ImageNet+LAION models in fitting, including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nImageNet+LAION models in the fitting,\\nwith the following columns:\\n- Test set: The different test sets being compared\\n- Excluded: Performance values for the excluded models\\n- Included: Performance values for the included models\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='5b834c079528f0aa1a412b875c535731879a770c74763c86996a7c3e90702d23')}, text='This table compares the performance of different ImageNet+LAION models in fitting, including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nImageNet+LAION models in the fitting,\\nwith the following columns:\\n- Test set: The different test sets being compared\\n- Excluded: Performance values for the excluded models\\n- Included: Performance values for the included models\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_126_table', obj=None),\n",
       " TextNode(id_='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_126_table', embedding=None, metadata={'table_df': \"{'Test set': {0: ' ', 1: ' ', 2: 'ImageNet-Sketch', 3: ' '}, 'ImageNet+LAION models in the fitting': {0: 'ImageNet-V2', 1: 'ImageNet-R', 2: '2.62', 3: 'ObjectNet'}, ' ': {0: 0.54, 1: 1.56, 2: 2.03, 3: 2.91}, 'Excluded': {0: '0.51', 1: '1.19', 2: ' ', 3: '1.47'}, 'Included': {0: ' ', 1: ' ', 2: ' ', 3: ' '}}\", 'table_summary': 'This table compares the performance of different ImageNet+LAION models in fitting, including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nImageNet+LAION models in the fitting,\\nwith the following columns:\\n- Test set: The different test sets being compared\\n- Excluded: Performance values for the excluded models\\n- Included: Performance values for the included models\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=['table_df', 'table_summary'], excluded_llm_metadata_keys=['table_df', 'table_summary'], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_126_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Test set\\nType: string\\nSummary: The different test sets being compared\\n\\nColumn: Excluded\\nType: number\\nSummary: Performance values for the excluded models\\n\\nColumn: Included\\nType: number\\nSummary: Performance values for the included models', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='2488a868ed4f58ba137b9f6502034e5418acc9186cce84c0593be35506fb58ad'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b42b2530-19e9-48d9-9e80-67a02685f035', node_type=<ObjectType.TEXT: '1'>, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='c917eb943757829c166ef0bf79c31f7b5715d0b2cc75b3338212c7b3b25b21e1')}, text='This table compares the performance of different ImageNet+LAION models in fitting, including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nImageNet+LAION models in the fitting,\\nwith the following columns:\\n- Test set: The different test sets being compared\\n- Excluded: Performance values for the excluded models\\n- Included: Performance values for the included models\\n\\n|Test set|ImageNet+LAION models in the fitting| |Excluded|Included|\\n|---|---|---|---|---|\\n| |ImageNet-V2|0.54|0.51| |\\n| |ImageNet-R|1.56|1.19| |\\n|ImageNet-Sketch|2.62|2.03| | |\\n| |ObjectNet|2.91|1.47| |\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='b42b2530-19e9-48d9-9e80-67a02685f035', embedding=None, metadata={'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='87965b81-8ed4-4c39-8966-220f8141e898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='10a2165b3330a9f358747fd57cc0112d70f8792fa6198638ed3376ff43b561f6'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_419849e5-5c6d-4d17-a967-8f5569e7bf79_126_table', node_type=<ObjectType.TEXT: '1'>, metadata={'table_df': \"{'Test set': {0: ' ', 1: ' ', 2: 'ImageNet-Sketch', 3: ' '}, 'ImageNet+LAION models in the fitting': {0: 'ImageNet-V2', 1: 'ImageNet-R', 2: '2.62', 3: 'ObjectNet'}, ' ': {0: 0.54, 1: 1.56, 2: 2.03, 3: 2.91}, 'Excluded': {0: '0.51', 1: '1.19', 2: ' ', 3: '1.47'}, 'Included': {0: ' ', 1: ' ', 2: ' ', 3: ' '}}\", 'table_summary': 'This table compares the performance of different ImageNet+LAION models in fitting, including ImageNet-V2, ImageNet-R, ImageNet-Sketch, and ObjectNet.,\\nwith the following table title:\\nImageNet+LAION models in the fitting,\\nwith the following columns:\\n- Test set: The different test sets being compared\\n- Excluded: Performance values for the excluded models\\n- Included: Performance values for the included models\\n', 'pdf_path': './corpus/2302.01381.pdf', 'pdf_id': 'ef725e87-9f67-4a94-859d-ec9285017cf0'}, hash='5b834c079528f0aa1a412b875c535731879a770c74763c86996a7c3e90702d23')}, text='Models are trained using 4x4 or 4x8 TPU v2 Pods, and they are evaluated using NVIDIA V100 GPUs, on the cloud.\\n\\n Details of ID Test Sets\\n\\nWe construct an ID test set from YFCC-15M and LAION-15M, respectively, and we automatically generate classification labels by matching text with ImageNet class names, which has also been similarly performed in Fang et al. (2022) for training classifiers using caption data. On YFCC-15M which contains metadata, we use tags for matching. On LAION-15M which does not provide metadata such as tags, we simply use the entire text for matching. We adopt a label only if a unique ImageNet class can be determined by matching for the involved image. We then construct a balanced and labelled test set by keeping 50 examples for each class that has at least 100 examples in the labelled images. The test examples are then held out from the training data. For the YFCC test set, there are 22550 examples for 451 classes; and for the LAION test set, there are 20400 examples 408 classes.\\n\\n Licenses of the Datasets\\n\\nWe have used the following datasets:\\n\\n- CIFAR-10 (Krizhevsky et al., 2009). License is not clearly known.\\n- YFCC6 under various Creative Commons licenses.\\n- LAION7 under the Creative Common CC-BY 4.0 license for the metadata, and the images are under the copyright of the original authors.\\n- CIFAR-10.18 under the MIT license.\\n- CIFAR-10.29. License is not clearly known.\\n- CINIC-1010 under the MIT license.\\n- ImageNet-V211 under the MIT license.\\n- ImageNet-R12 under the MIT license.\\n- ImageNet-Sketch13 under the MIT license.\\n- ObjectNet14 with a license provided on the webpage.', start_char_idx=1, end_char_idx=1625, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = node_parser.get_nodes_from_documents([document])\n",
    "nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(path=\"../qdrant-vector-store\")\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test-collection\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes=base_nodes, storage_context=storage_context)\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "# reranker = FlagEmbeddingReranker(\n",
    "#     top_n=5,\n",
    "#     model=\"BAAI/bge-reranker-large\",\n",
    "# )\n",
    "query_engine = index.as_query_engine(similarity_top_k=10, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"What is the role of ImageNet-R as the OOD test set in the evaluation process?\t\"\n",
    ")\n",
    "response = query_engine.query(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ImageNet-R serves as an out-of-distribution (OOD) test set in the evaluation process.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We also train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide additional details in Appendix B.1.\n",
      "\n",
      "We use \"{Name_of_dataset} models\" to denote models trained only on the dataset, e.g., \"CIFAR-10 models\". And we use \"{Name_of_dataset_A}+{Name_of_dataset_B} models\" to represent models trained on a combination of two datasets, e.g., \"CIFAR-10+ImageNet models\".\n",
      "\n",
      "ID test sets: We focus on image classification. Labeled image classification datasets such as ImageNet can be directly used for evaluating ID accuracy. For datasets that consist of image-text pairs for language-image pre-training without original labels, including YFCC and LAION, we automatically generate classification labels by matching captions with ImageNet classes, which has been similarly performed in Fang et al. (2022) for training classifiers using caption data, and we then hold out a balanced test set from the original dataset. More details are reported in Appendix B.2. Although it is possible to obtain a higher-quality test set by human labeling, we will show that using the automatically labeled test sets can already produce reasonable results.\n",
      "\n",
      "OOD test sets: To compare the effectiveness robustness of models trained on CIFAR-10 and ImageNet, we use 3 CIFAR-like OOD test sets with natural distribution shifts, including CIFAR-10.1 (Recht et al., 2019), CIFAR-10.2 (Lu et al., 2020), and CINIC-10 (Darlow et al., 2018). We use 4 ImageNet-like OOD test sets to compare models trained on ImageNet with models trained on YFCC and LAION: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019). We do not use ImageNet-A (Hendrycks et al., 2021b) which involves adversarial filtering and has a different behavior in effective robustness evaluation (Taori et al., 2020; Fang et al., 2022).\n",
      "\n",
      "Class subsampling and mapping: Considering that different test sets may not have the same classes, we follow prior works (Taori et al., 2020; Fang et al., 2022) to use class subsampling to\n",
      "\n",
      "We reuse the term \"class subsampling\" from prior works (Taori et al., 2020; Fang et al., 2022), although it is not a random sampling.\n",
      "\n",
      " Document\n",
      "\n",
      " CIFAR-10 Models\n",
      "\n",
      " ImageNet Models\n",
      "\n",
      " CIFAR-10+ImageNet Models\n",
      "\n",
      " ImageNet Models\n",
      "\n",
      " YFCC Models\n",
      "\n",
      " ImageNet+YFCC Models\n",
      "\n",
      "1. Cipa4-I0\n",
      "\n",
      "(a) Using CIFAR-10.2 as the OOD test set. The ImageNet accuracy is mapped to CIFAR-10 classes (see Section 5.1).\n",
      "\n",
      "(b) Using ImageNet-R as the OOD test set.\n",
      "\n",
      "Figure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.\n",
      "\n",
      "Retain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models.\n",
      "\n",
      " 5.2 Evaluation on CIFAR-like OOD Test Sets\n",
      "Effective robustness values seem to suggest an advantage of ImageNet models compared to CIFAR-10 models, which is consistent with the findings in Miller et al. (2021). However, under the multi-ID evaluation, the advantage of ImageNet models diminishes, and the effective robustness values of both CIFAR-10 models and ImageNet models are much closer to 0. Therefore, the apparent advantage reported by prior works can be explained as the effect of training data on the single-ID evaluation, and our multi-ID evaluation resolves this confounder to provide a more precise understanding.\n",
      "\n",
      "In Figure 3a, we visualize the multi-ID effective robustness on CIFAR-10.2, where the accuracies of all the models approximately lie on a plane (the baseline function) on the logit scale, and thus these models have similar effective robustness as the OOD accuracy of all the models can be approximately predicted using a simple plane. We also show projected views of Figure 3a in Figure 4, where Figure 4a and Figure 4b correspond to the single-ID evaluation taking different ID test sets with contradictory conclusions. In contrast, our new evaluation provides a more holistic view.\n",
      "\n",
      " Evaluation on ImageNet-like OOD Test Sets\n",
      "\n",
      "We then conduct evaluation on ImageNet-like OOD test sets, and we compare ImageNet models with models trained on YFCC and LAION, respectively. We show the fitting quality in Table 2 and the effective robustness in Tables 3a and 3b. Consistent with results in Section 5.2, our multi-ID evaluation improves the fitting quality over the single-ID evaluation to better predict and understand the OOD accuracies from ID accuracies. On effective robustness, single-ID evaluation leads to a perception of an effective robustness gain when there is a mismatch between the training data and the single ID test set. Our multi-ID evaluation enjoys a holistic view of all the training distributions and suggests that all the models evaluated here have similar effective robustness.\n",
      "\n",
      "Specifically, the improvement of fitting quality is particularly significant for models involving LAION on ImageNet-R (R2 improved from 0.216 to 0.982 and MAE reduced from 9.23% to 1.32%) and ImageNet-Sketch (R2 improved from 0.281 to 0.937 and MAE reduced from 7.90% to 2.10%). On the effective robustness values, under the single-ID evaluation, YFCC and LAION models have positive effective robustness values (2.59±2.43 (%) on average for YFCC models and 5.96±4.96 (%) on average for LAION models), which is consistent with the findings in Fang et al. (2022); Nguyen et al. (2022). In contrast, under our multi-ID evaluation, the average effective robustness becomes 0.77±0.85 (%) for YFCC models, and -0.00±0.52 (%) for LAION models, much closer to 0. While single-ID evaluation used by prior works (Fang et al., 2022; Nguyen et al., 2022) suggests effective\n",
      "We demonstrate that the results of evaluating effective robustness using a single\n",
      "ID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise\n",
      "treatment on the ID test set in existing works could end up exaggerating the effective robustness of\n",
      "zero-shot CLIP models compared to models that are exactly trained on ImageNet.\n",
      "In this paper, we propose to more precisely evaluate and compare the effective robustness of models\n",
      "trained on different datasets. Instead of controlling for a single ID accuracy that may bias towards\n",
      "models from a particular training distribution, we propose to use multiple ID test sets that cover the\n",
      "training distributions of all the models. In particular, previous works performed single-dimensional\n",
      "linear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\n",
      "et al., 2020). And they then evaluate the actual OOD accuracy of the models beyond the expected\n",
      "value that can be predicted from the fitting line, as the effective robustness. We expand on this\n",
      "definition by allowing for multiple ID test sets, and perform multi-dimensional linear regression to fit\n",
      "a plane to predict OOD accuracy from the accuracy on multiple ID test sets.\n",
      "\n",
      "In summary, we make the following contributions:\n",
      "- We reveal a limitation in the existing effective robustness evaluation when used to compare models\n",
      "trained on different data distributions.\n",
      "- We then propose a new effective robustness evaluation which uses multiple ID test sets to more\n",
      "precisely compare the effective robustness of models trained on different data.\n",
      "- We show that the OOD accuracy of various models including zero-shot CLIP models can usually\n",
      "be better predicted from accuracies on multiple ID test sets compared to using only one ID test set.\n",
      "- Our results provide new understandings on the effective robustness gains of CLIP-like models\n",
      "observed in prior works only using ImageNet as the ID test set, while the gains diminish under our\n",
      "new evaluation.\n",
      "\n",
      " Background of Effective Robustness\n",
      "\n",
      "Under natural distribution shifts, the OOD accuracy of a model is often correlated with the ID\n",
      "accuracy. After applying a logit transformation on the accuracy, a linear trend between the transformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from\n",
      "ImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky\n",
      "et al., 2009) to CIFAR-10.2 (Hendrycks & Dietterich, 2018)) and models with various architectures\n",
      "and training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most\n",
      "models showing higher OOD accuracies naturally resulted from better ID performance.\n",
      "\n",
      "To eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed\n",
      "effective robustness that measures the OOD performance beyond the expected OOD accuracy given\n",
      "the ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend\n",
      "of baseline models. Since they only use a single ID test set, we refer to this version of effective\n",
      "robustness as single-ID effective robustness.\n",
      "\n",
      "Suppose there are n baseline models \\( f_1, f_2, \\ldots, f_n \\). A baseline function \\( \\tilde{\\beta}(x) \\) is constructed to predict\n",
      "the OOD accuracy of each baseline model, \\( acc_{ood}(f_i) \\) (\\( 1 \\leq i \\leq n \\)), given the single ID accuracy of\n",
      "the model \\( x = acc_{id}(f_i) \\). The baseline function is instantiated as:\n",
      "\n",
      "$$\n",
      "\\tilde{\\beta}(x) = \\text{expit}(w \\text{logit}(x) + b) \\quad (1)\n",
      "$$\n",
      "\n",
      "where \\( w \\) and \\( b \\) are parameters, \\( \\text{logit}(x) = \\ln\\left(\\frac{1}{1-x}\\right) \\) is the logit transformation, and \\( \\text{expit}(x) \\) is the\n",
      "inverse of \\( \\text{logit}(x) \\). Since \\( \\text{logit}(\\tilde{\\beta}(x)) = w \\text{logit}(x) + b \\), the baseline function is essentially a linear\n",
      "\n",
      " Document\n",
      "\n",
      " ImageNet models\n",
      "Effective Robustness against Natural Distribution Shifts\n",
      "\n",
      " Effective Robustness against Natural Distribution Shifts for Models with Different Training Data\n",
      "\n",
      "Zhouxing Shi* &emsp; Nicholas Carlini &emsp; Ananth Balashankar\n",
      "\n",
      "UCLA &emsp; Google Research &emsp; Google Research\n",
      "\n",
      "zshi@cs.ucla.edu &emsp; ncarlini@google.com &emsp; ananthbshankar@google.com\n",
      "\n",
      "Ludwig Schmidt &emsp; Cho-Jui Hsieh\n",
      "\n",
      "University of Washington &emsp; Google, UCLA\n",
      "\n",
      "schmidt@cs.washington.edu &emsp; chohsieh@cs.ucla.edu\n",
      "\n",
      "Alex Beutel* &emsp; Yao Qin\n",
      "\n",
      "OpenAI &emsp; UCSB, Google Research\n",
      "\n",
      "alexb@openai.com &emsp; yaoqin@ucsb.edu\n",
      "\n",
      " Abstract\n",
      "\n",
      "\"Effective robustness\" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageNet as the only ID test set, while the gains diminish under our new evaluation. Additional artifacts including interactive visualizations are provided at https://shizhouxing.github.io/effective-robustness.\n",
      "\n",
      " 1 Introduction\n",
      "\n",
      "Robustness against distribution shifts is important for machine learning models to work reliably across various environments. For natural distribution shifts on image classification datasets, Taori et al. (2020) proposed the notion of effective robustness to control for in-distribution (ID) accuracy when evaluating out-of-distribution (OOD) accuracy. Following a long line of work that has found a strong correlation between ID and OOD accuracy on many test sets (Recht et al., 2019; Yadav & Bottou, 2019), effective robustness allows researchers to assess whether an apparently improved OOD accuracy is a result of effectively improved robustness or is simply an expected outcome of enhanced ID accuracy.\n",
      "\n",
      "Unfortunately, the current definition of effective robustness has a subtle limitation: it requires a fixed ID test set, which is typically ImageNet (Deng et al., 2009) when using ImageNet-like OOD test sets. *Work done while at Google.\n",
      "\n",
      "37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
      "\n",
      "in Taori et al. (2020) or CIFAR-10 (Krizhevsky et al., 2009) when using CIFAR-like OOD test sets\n",
      "in Miller et al. (2021). It is acceptable when models are trained predominately on only one dataset.\n",
      "However, the emergence of many large-scale models trained on significantly different datasets makes\n",
      "it necessary to evaluate and compare models trained on different data distributions, under which it\n",
      "becomes unclear which ID test set should be used.\n",
      "In particular, models from Contrastive Language-Image Pre-training, such as CLIP (Radford et al.,\n",
      "2021) and ALIGN (Jia et al., 2021) have recently exhibited unprecedented effective robustness gains\n",
      "during zero-shot inference (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022). However\n",
      "these previous works simply take ImageNet as the single ID test set, even though the models are not\n",
      "trained on ImageNet. We demonstrate that the results of evaluating effective robustness using a single\n",
      "ID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise\n",
      "treatment on the ID test set in existing works could end up exaggerating the effective robustness of\n",
      "zero-shot CLIP models compared to models that are exactly trained on ImageNet.\n",
      "In this paper, we propose to more precisely evaluate and compare the effective robustness of models\n",
      "trained on different datasets. Instead of controlling for a single ID accuracy that may bias towards\n",
      "models from a particular training distribution, we propose to use multiple ID test sets that cover the\n",
      "training distributions of all the models. In particular, previous works performed single-dimensional\n",
      "linear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori\n",
      "et al., 2020).\n",
      "(a) CIFAR-10.2 accuracy against CIFAR-10 accuracy. ImageNet models have higher CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for CIFAR-10 accuracy only.\n",
      "\n",
      "(b) CIFAR-10.2 accuracy against ImageNet accuracy. ImageNet models have lower CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for ImageNet accuracy only.\n",
      "\n",
      "Figure 4: Projected views of Figure 3a. Figure 4a and Figure 4b correspond to single-ID evaluations using different ID test sets and yield contradictory conclusions on the effective robustness. Our multi-ID evaluation provides a more holistic view where all these models are approximately on a same plane and thus have similar effective robustness.\n",
      "\n",
      "We first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets. We show the fitting quality in Table 1a and the effective robustness of various models in Table 1b.\n",
      "\n",
      "Compared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE), and thus provides a more precise understanding on the effective robustness. Specifically, while both single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms of the effective robustness, under the single-ID evaluation, ImageNet models achieve $$3.91\\pm2.20\\%$$ and $$2.77\\pm1.25\\%$$ effective robustness on CIFAR-10.2 and CINIC-10, respectively.\n",
      "\n",
      " Results on CIFAR-like OOD test sets\n",
      "\n",
      " Results on CIFAR-like OOD test sets\n",
      "\n",
      "148 models are included, including CIFAR-10 models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models and ImageNet models also become closer to 0.\n",
      "\n",
      " (a) Fitting quality evaluated by R2 and mean absolute error (MAE)\n",
      "(2022b) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al. (2021); Santurkar et al. (2022) studied the transfer performance of CLIP models, which is out of our scope on the robustness against natural distribution shifts.\n",
      "\n",
      " Conclusion\n",
      "\n",
      "To conclude, we propose a new and more precise effective robustness evaluation for models with different training data. In our evaluation, the OOD accuracy can generally be better predicted from multiple ID accuracies compared to previous effective robustness evaluation with a single ID test. We find that zero-shot CLIP models pre-trained on language-image data do not have better effective robustness compared to standard image classifiers, and we provide a new understanding of the apparently significant effective robustness gains observed by prior works.\n",
      "\n",
      " Limitations and Future Work\n",
      "\n",
      "There remain several limitations that may be addressed in future works:\n",
      "\n",
      "- Our method requires fully knowing the training distributions of all the evaluated models, which is not directly applicable for large-scale pre-trained models with private training data. And this also requires the training methods not to significantly alter the training distribution beyond basic data augmentation, while some methods such as SLIP may alter training distributions more significantly, and it is unclear how we can precisely define the training distribution for a model with post-training processing, such as WiSE-FT (Wortsman et al., 2022b) and Model Soups (Wortsman et al., 2022a) with weight-space ensembling. Future work may study how these techniques may impact the ID performance evaluation (Section 5.4).\n",
      "- We assume that the two ID test sets have relatively close distributions compared to the OOD test sets. We have not considered how the difference between the multiple ID test sets may affect the evaluation, and how the effective robustness should be compared if models are trained on highly different distributions.\n",
      "- We use fixed OOD test sets to evaluate the OOD performance, following previous works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022; Schuhmann et al., 2021). When models are pre-trained on large-scale data, it becomes unclear if these “OOD” test sets are still OOD, or if these test sets could be less OOD for the pre-trained models compared to standard classifiers. There may also be some distribution overlap between these test sets and the pre-training datasets, even though Radford et al. (2021) mentioned that the likelihood of direct data overlapping is low.\n",
      "- We focus on distribution shifts where at least “accuracy-on-the-line” from existing works is known to hold for models trained on the same data (Taori et al., 2020; Miller et al., 2021), yet there are counterexamples where “accuracy-on-the-line” does not hold (Section 6) and requires further study. We may set a threshold on the fitting quality and only adopt our method when the fitting quality is sufficiently good. And there are also other OOD test sets (Singla & Feizi, 2021; Rusak et al., 2021; Singla et al., 2022; Idrissi et al., 2022; Li et al., 2023; Moayeri et al., 2022; Vasudevan et al., 2022) that have not been studied in the effective robustness works yet.\n",
      "- While we mostly focus on comparing CLIP-like models with standard image classifiers, due to the notable OOD robustness of CLIP-like models studied in prior works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022), this work may further be extended to cover other types of models (Goyal et al., 2022; Singh et al., 2022) as well as other modalities such as distribution shifts on language data (Miller et al., 2020; Awadalla et al., 2022).\n",
      "- Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively.\n",
      "\n",
      " Acknowledgments & Funding Disclosure\n",
      "\n",
      " Acknowledgments & Funding Disclosure\n",
      "\n",
      "We thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive feedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001.\n",
      "Table 7: τ statistics for ImageNet models and LAION models on ImageNet-like OOD test sets\n",
      "1a shows that the fitting line for YFCC models is generally above the fitting line for ImageNet models (except for the regime with extremely low accuracies), which appears to suggest that YFCC models have effective robustness gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 1b which uses YFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models, which instead appears to suggest that ImageNet models have greater effective robustness than YFCC models. We observe that when there is a mismatch in the training data and the ID test data.\n",
      "\n",
      "models appear to have greater effective robustness (YFCC models in Figure 1a and ImageNet models\n",
      "in Figure 1b), as their performance on the ID test data and the OOD performance predicted from the\n",
      "single ID accuracy tend to be lower. This makes it difficult to compare models trained on different\n",
      "data and leads to imprecise conclusions on effective robustness if only one ID test set is used.\n",
      "\n",
      " Multi-ID Effective Robustness\n",
      "\n",
      "Considering the limitations of using a single ID test set, we propose a new way for effective robustness\n",
      "evaluation using multiple ID test sets that cover the training data distributions of all the involved\n",
      "models. We name it multi-ID effective robustness. Specifically, for each training distribution, we\n",
      "propose to prepare an ID test set that matches the training distribution, respectively. In particular, we\n",
      "focus on comparing models trained on two different datasets at a time in this paper, and we thereby\n",
      "use two ID test sets, where each of them corresponds to one of the training datasets.\n",
      "\n",
      "While we refer to them as ID test sets, each of them is only the exact ID test set for some of the\n",
      "considered models that are trained on the distribution matching the test set, and it is not exactly an\n",
      "ID test set for all the considered models. However, we assume that the training distributions of all\n",
      "the models are still relatively close compared to the OOD test distributions (e.g., images normally\n",
      "collected from social medias in ImageNet (Deng et al., 2009), YFCC (Thomee et al., 2016), and\n",
      "LAION (Schuhmann et al., 2021) are relatively close compared to the OOD images in ImageNet-\n",
      "Sketch (Wang et al., 2019) that consists of sketch images specifically). In this way, both ID test sets\n",
      "are relatively ID for all the models compared to the OOD test sets, and it can be meaningful to control\n",
      "for the performance on these ID test sets when comparing the OOD performance.\n",
      "\n",
      "We still use $$acc_{ood}(·)$$ to denote the OOD accuracy, and we use $$acc_1(·)$$ and $$acc_2(·)$$ to denote the\n",
      "accuracy on the two ID test sets, respectively. In contrast to the previous baseline function $$\\tilde{\\beta}(x)$$ in\n",
      "Eq. (1), we propose a new baseline function $$\\beta(x, y)$$ that predicts the OOD accuracy based on the\n",
      "accuracies x and y on the two ID test sets, respectively.\n",
      "\n",
      "All the models in Figure 1 are trained on either ImageNet or YFCC. Thus, to compare their effective ImageNet Models YFCC Models\n",
      "robustness under our new evaluation, we use two ID\n",
      "test sets for ImageNet and YFCC at the same time, in 1\n",
      "contrast to Figure 1a and 1b which use one ID test set\n",
      "separately at each time and results on the two different\n",
      "ID test sets lead to contradictory conclusions. As\n",
      "shown in Figure 2, we plot the OOD accuracy against\n",
      "the two ID accuracies on both two ID test sets in a 3D\n",
      "space. We observe that the data points approximately\n",
      "lie on a plane when plotted on the logit scale. This motivates us to instantiate $$\\beta(x, y)$$ as:\n",
      "\n",
      "$$\\beta(x, y) = expit(wx logit(x) + wy logit(y) + b),$$\n",
      "\n",
      "where wx, wy, b are parameters. $$\\beta(x, y)$$, which is the plane in Figure 2, is also a linear function w.r.t. x and\n",
      "y under the logit scale, and thus it is a reasonable extension from $$\\tilde{\\beta}(x)$$ by using a multi-dimensional linear function on the logit scale. We determine the parameters by solving an ordinary least squares regression to fit the accuracies. Metrics for linear regression\n",
      "such as the coefficient of determination, a.k.a. R2,\n",
      "can be used to evaluate the fitting quality of the baseline function.\n",
      "- Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively.\n",
      "\n",
      " Acknowledgments & Funding Disclosure\n",
      "\n",
      " Acknowledgments & Funding Disclosure\n",
      "\n",
      "We thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive feedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001.\n",
      "\n",
      " References\n",
      "\n",
      "- Andreassen, A. J., Bahri, Y., Neyshabur, B., and Roelofs, R. The evolution of out-of-distribution robustness throughout fine-tuning. Transactions on Machine Learning Research, 2022.\n",
      "- Awadalla, A., Wortsman, M., Ilharco, G., Min, S., Magnusson, I., Hajishirzi, H., and Schmidt, L. Exploring the landscape of distributional robustness for question answering models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5971–5987, 2022.\n",
      "- Baek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. Advances in Neural Information Processing Systems, 35:19274–19289, 2022.\n",
      "- Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.\n",
      "- Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a.\n",
      "- Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33: 22243–22255, 2020b.\n",
      "- Darlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505, 2018.\n",
      "- Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. Imagenet: A large-scale hierarchical image database. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248–255, 2009.\n",
      "- Desai, K. and Johnson, J. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11162–11173, 2021.\n",
      "- Devillers, B., Choksi, B., Bielawski, R., and VanRullen, R. Does language help generalization in vision models? In Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 171–182, 2021.\n",
      "- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n",
      "- Fang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pp. 6216–6234. PMLR, 2022.\n",
      "Models are trained using 4x4 or 4x8 TPU v2 Pods, and they are evaluated using NVIDIA V100 GPUs, on the cloud.\n",
      "\n",
      " Details of ID Test Sets\n",
      "\n",
      "We construct an ID test set from YFCC-15M and LAION-15M, respectively, and we automatically generate classification labels by matching text with ImageNet class names, which has also been similarly performed in Fang et al. (2022) for training classifiers using caption data. On YFCC-15M which contains metadata, we use tags for matching. On LAION-15M which does not provide metadata such as tags, we simply use the entire text for matching. We adopt a label only if a unique ImageNet class can be determined by matching for the involved image. We then construct a balanced and labelled test set by keeping 50 examples for each class that has at least 100 examples in the labelled images. The test examples are then held out from the training data. For the YFCC test set, there are 22550 examples for 451 classes; and for the LAION test set, there are 20400 examples 408 classes.\n",
      "\n",
      " Licenses of the Datasets\n",
      "\n",
      "We have used the following datasets:\n",
      "\n",
      "- CIFAR-10 (Krizhevsky et al., 2009). License is not clearly known.\n",
      "- YFCC6 under various Creative Commons licenses.\n",
      "- LAION7 under the Creative Common CC-BY 4.0 license for the metadata, and the images are under the copyright of the original authors.\n",
      "- CIFAR-10.18 under the MIT license.\n",
      "- CIFAR-10.29. License is not clearly known.\n",
      "- CINIC-1010 under the MIT license.\n",
      "- ImageNet-V211 under the MIT license.\n",
      "- ImageNet-R12 under the MIT license.\n",
      "- ImageNet-Sketch13 under the MIT license.\n",
      "- ObjectNet14 with a license provided on the webpage.\n"
     ]
    }
   ],
   "source": [
    "for source in response.source_nodes:\n",
    "    print(source.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-project-RXD1v8WZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
